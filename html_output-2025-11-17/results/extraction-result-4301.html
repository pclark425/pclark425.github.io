<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4301 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4301</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4301</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-99.html">extraction-schema-99</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <p><strong>Paper ID:</strong> paper-be84ef4a6e67fa5e9784f575a1b2e2385fa06aa5</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/be84ef4a6e67fa5e9784f575a1b2e2385fa06aa5" target="_blank">A Survey of Scientific Large Language Models: From Data Foundations to Agent Frontiers</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> A comprehensive, data-centric synthesis that reframes the development of Sci-LLMs as a co-evolution between models and their underlying data substrate, and outlines a paradigm shift toward closed-loop systems where autonomous agents based on Sci-LLMs actively experiment, validate, and contribute to a living, evolving knowledge base.</p>
                <p><strong>Paper Abstract:</strong> Scientific Large Language Models (Sci-LLMs) are transforming how knowledge is represented, integrated, and applied in scientific research, yet their progress is shaped by the complex nature of scientific data. This survey presents a comprehensive, data-centric synthesis that reframes the development of Sci-LLMs as a co-evolution between models and their underlying data substrate. We formulate a unified taxonomy of scientific data and a hierarchical model of scientific knowledge, emphasizing the multimodal, cross-scale, and domain-specific challenges that differentiate scientific corpora from general natural language processing datasets. We systematically review recent Sci-LLMs, from general-purpose foundations to specialized models across diverse scientific disciplines, alongside an extensive analysis of over 270 pre-/post-training datasets, showing why Sci-LLMs pose distinct demands -- heterogeneous, multi-scale, uncertainty-laden corpora that require representations preserving domain invariance and enabling cross-modal reasoning. On evaluation, we examine over 190 benchmark datasets and trace a shift from static exams toward process- and discovery-oriented assessments with advanced evaluation protocols. These data-centric analyses highlight persistent issues in scientific data development and discuss emerging solutions involving semi-automated annotation pipelines and expert validation. Finally, we outline a paradigm shift toward closed-loop systems where autonomous agents based on Sci-LLMs actively experiment, validate, and contribute to a living, evolving knowledge base. Collectively, this work provides a roadmap for building trustworthy, continually evolving artificial intelligence (AI) systems that function as a true partner in accelerating scientific discovery.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4301.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4301.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dual Reasoning Symbolic Regression</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dual Reasoning Symbolic Regression</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-powered approach that couples language-model reasoning with reflective optimization to extract symbolic mathematical expressions (equations) from data or textual descriptions in scientific contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Dual Reasoning Symbolic Regression</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>A hybrid pipeline in which an LLM is used to propose or reason about candidate symbolic forms (e.g., functional relationships, equation templates) and a separate symbolic-regression/optimization module fits parameters and evaluates candidates; iterations between the LLM (for proposing/refining structure) and the optimizer (for parameter estimation and validation) produce extracted equations.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>physics / astronomy (general scientific equation discovery)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>mathematical equations / symbolic relationships</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>symbolic expressions / mathematical equations</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Not specified in this survey; general challenges likely include correctly constraining search space, ensuring dimensional consistency, avoiding overfitting to noisy or partial textual descriptions, and requiring reliable iterative coordination between LLM proposals and numeric fitting.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Scientific Large Language Models: From Data Foundations to Agent Frontiers', 'publication_date_yy_mm': '2025-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4301.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4301.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PhyE2E</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PhyE2E (end-to-end neural symbolic regression)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An end-to-end neural symbolic regression system that generates dimensionally consistent formulas from diverse data sources, combining neural components with symbolic regression constraints to produce interpretable equations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>PhyE2E neural symbolic regression</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>A neural-symbolic pipeline that learns mappings from input data (including scientific observational streams and benchmark datasets) to symbolic mathematical formulas, explicitly enforcing dimensional consistency and leveraging neural modules to propose candidate symbolic structures which are then converted into closed-form equations.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>not an LLM (neural symbolic regression model)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>astronomy / solar and space physics (applied to mission and observational datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>dimensionally consistent mathematical formulas / symbolic equations</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>mathematical equations / symbolic expressions</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Applied to mission and benchmark datasets (e.g., THEMIS mission data, AI Feynman benchmarks) as reported in the survey</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Survey notes dimensional consistency is a design goal; specific limitations not detailed here but likely include sensitivity to noise, model bias toward particular functional forms, and generalization across domains.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Scientific Large Language Models: From Data Foundations to Agent Frontiers', 'publication_date_yy_mm': '2025-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4301.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4301.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AI Feynman</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AI Feynman (physics-inspired symbolic regression)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A physics-inspired symbolic regression method/dataset used as a benchmark for discovering closed-form equations from data, leveraging physics priors to simplify the search for analytic expressions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>AI Feynman symbolic regression / benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>A symbolic-regression approach (and set of benchmark problems) that exploits physics-motivated transformations and separability heuristics to simplify and identify analytic expressions that fit data; often used as a ground for testing neural or hybrid symbolic discovery systems.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>not an LLM (symbolic regression algorithm / benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>physics (general symbolic equation discovery benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>closed-form analytic equations / symbolic laws</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>symbolic expressions / mathematical formulas</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Benchmark evaluation on synthetic physics-inspired datasets (AI Feynman benchmark problems) and comparison of recovered symbolic forms to ground-truth expressions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>As a benchmark/algorithm, known limitations include sensitivity to noise and to high-dimensional inputs; survey only cites AI Feynman as a data source for other methods.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Scientific Large Language Models: From Data Foundations to Agent Frontiers', 'publication_date_yy_mm': '2025-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4301.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4301.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Graph networks for force-law discovery</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph neural networks / equation-graph based symbolic regression for force-law recovery</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Graph-based neural methods (message-passing networks and equation graphs) trained on physical interaction data that can recover functional force laws (e.g., Newton's law) by learning relational patterns between entities and mapping them to symbolic relationships.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Graph network-based symbolic discovery / equation graphs</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Message-passing graph neural networks are trained on data of interacting particles/objects to learn relational functions; learned representations or explicit equation-graph structures (variables and operators as nodes) are then used to infer or reconstruct symbolic force laws or equations describing the interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>not an LLM (graph neural networks / equation-graph architectures)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>physics (mechanics / force-law discovery)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>force laws (e.g., Newton's law) / mathematical relationships between physical variables</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>functional relationships / symbolic or parametric laws</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Recovered laws compared to known physical laws (e.g., Newtonian force law) as described in the survey</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Survey-level mention only; potential limitations include needing well-specified interaction datasets, difficulty extracting compact symbolic forms directly from learned message-passing weights, and sensitivity to training regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Scientific Large Language Models: From Data Foundations to Agent Frontiers', 'publication_date_yy_mm': '2025-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4301.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4301.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM symbolic derivation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based symbolic derivation (symbolic-first reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting/usage pattern where LLMs perform symbolic derivations by keeping variables symbolic (rather than substituting numeric values early), which has been observed to improve accuracy on physics problem solving.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Symbolic-first LLM derivation / reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Prompting or training LLMs to carry out symbolic manipulations and derivations (e.g., algebraic manipulation, symbolic calculus) by preserving symbolic variables through reasoning steps, enabling the model to output derivations and final analytic expressions rather than numeric-first solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>physics / mathematical problem solving</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>derived equations, symbolic derivations, analytic expressions</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>symbolic derivations / mathematical equations</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Reported via improved accuracy on physics problem solving tasks (survey cites observation but does not provide numeric results here).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Survey notes symbolic-first LLM derivations tend to outperform numeric-first approaches on physics problems, but gives no quantitative comparisons in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Survey notes need for maintaining symbolic integrity and domain-appropriate manipulation; specific failure modes not detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Scientific Large Language Models: From Data Foundations to Agent Frontiers', 'publication_date_yy_mm': '2025-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>AI Feynman: A Physics-Inspired Method for Symbolic Regression <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4301",
    "paper_id": "paper-be84ef4a6e67fa5e9784f575a1b2e2385fa06aa5",
    "extraction_schema_id": "extraction-schema-99",
    "extracted_data": [
        {
            "name_short": "Dual Reasoning Symbolic Regression",
            "name_full": "Dual Reasoning Symbolic Regression",
            "brief_description": "An LLM-powered approach that couples language-model reasoning with reflective optimization to extract symbolic mathematical expressions (equations) from data or textual descriptions in scientific contexts.",
            "citation_title": "",
            "mention_or_use": "mention",
            "method_name": "Dual Reasoning Symbolic Regression",
            "method_description": "A hybrid pipeline in which an LLM is used to propose or reason about candidate symbolic forms (e.g., functional relationships, equation templates) and a separate symbolic-regression/optimization module fits parameters and evaluates candidates; iterations between the LLM (for proposing/refining structure) and the optimizer (for parameter estimation and validation) produce extracted equations.",
            "llm_model_used": null,
            "scientific_domain": "physics / astronomy (general scientific equation discovery)",
            "number_of_papers": null,
            "type_of_quantitative_law": "mathematical equations / symbolic relationships",
            "extraction_output_format": "symbolic expressions / mathematical equations",
            "validation_method": null,
            "performance_metrics": null,
            "baseline_comparison": null,
            "challenges_limitations": "Not specified in this survey; general challenges likely include correctly constraining search space, ensuring dimensional consistency, avoiding overfitting to noisy or partial textual descriptions, and requiring reliable iterative coordination between LLM proposals and numeric fitting.",
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4301.0",
            "source_info": {
                "paper_title": "A Survey of Scientific Large Language Models: From Data Foundations to Agent Frontiers",
                "publication_date_yy_mm": "2025-08"
            }
        },
        {
            "name_short": "PhyE2E",
            "name_full": "PhyE2E (end-to-end neural symbolic regression)",
            "brief_description": "An end-to-end neural symbolic regression system that generates dimensionally consistent formulas from diverse data sources, combining neural components with symbolic regression constraints to produce interpretable equations.",
            "citation_title": "",
            "mention_or_use": "mention",
            "method_name": "PhyE2E neural symbolic regression",
            "method_description": "A neural-symbolic pipeline that learns mappings from input data (including scientific observational streams and benchmark datasets) to symbolic mathematical formulas, explicitly enforcing dimensional consistency and leveraging neural modules to propose candidate symbolic structures which are then converted into closed-form equations.",
            "llm_model_used": "not an LLM (neural symbolic regression model)",
            "scientific_domain": "astronomy / solar and space physics (applied to mission and observational datasets)",
            "number_of_papers": null,
            "type_of_quantitative_law": "dimensionally consistent mathematical formulas / symbolic equations",
            "extraction_output_format": "mathematical equations / symbolic expressions",
            "validation_method": "Applied to mission and benchmark datasets (e.g., THEMIS mission data, AI Feynman benchmarks) as reported in the survey",
            "performance_metrics": null,
            "baseline_comparison": null,
            "challenges_limitations": "Survey notes dimensional consistency is a design goal; specific limitations not detailed here but likely include sensitivity to noise, model bias toward particular functional forms, and generalization across domains.",
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4301.1",
            "source_info": {
                "paper_title": "A Survey of Scientific Large Language Models: From Data Foundations to Agent Frontiers",
                "publication_date_yy_mm": "2025-08"
            }
        },
        {
            "name_short": "AI Feynman",
            "name_full": "AI Feynman (physics-inspired symbolic regression)",
            "brief_description": "A physics-inspired symbolic regression method/dataset used as a benchmark for discovering closed-form equations from data, leveraging physics priors to simplify the search for analytic expressions.",
            "citation_title": "",
            "mention_or_use": "mention",
            "method_name": "AI Feynman symbolic regression / benchmark",
            "method_description": "A symbolic-regression approach (and set of benchmark problems) that exploits physics-motivated transformations and separability heuristics to simplify and identify analytic expressions that fit data; often used as a ground for testing neural or hybrid symbolic discovery systems.",
            "llm_model_used": "not an LLM (symbolic regression algorithm / benchmark)",
            "scientific_domain": "physics (general symbolic equation discovery benchmarks)",
            "number_of_papers": null,
            "type_of_quantitative_law": "closed-form analytic equations / symbolic laws",
            "extraction_output_format": "symbolic expressions / mathematical formulas",
            "validation_method": "Benchmark evaluation on synthetic physics-inspired datasets (AI Feynman benchmark problems) and comparison of recovered symbolic forms to ground-truth expressions.",
            "performance_metrics": null,
            "baseline_comparison": null,
            "challenges_limitations": "As a benchmark/algorithm, known limitations include sensitivity to noise and to high-dimensional inputs; survey only cites AI Feynman as a data source for other methods.",
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4301.2",
            "source_info": {
                "paper_title": "A Survey of Scientific Large Language Models: From Data Foundations to Agent Frontiers",
                "publication_date_yy_mm": "2025-08"
            }
        },
        {
            "name_short": "Graph networks for force-law discovery",
            "name_full": "Graph neural networks / equation-graph based symbolic regression for force-law recovery",
            "brief_description": "Graph-based neural methods (message-passing networks and equation graphs) trained on physical interaction data that can recover functional force laws (e.g., Newton's law) by learning relational patterns between entities and mapping them to symbolic relationships.",
            "citation_title": "",
            "mention_or_use": "mention",
            "method_name": "Graph network-based symbolic discovery / equation graphs",
            "method_description": "Message-passing graph neural networks are trained on data of interacting particles/objects to learn relational functions; learned representations or explicit equation-graph structures (variables and operators as nodes) are then used to infer or reconstruct symbolic force laws or equations describing the interactions.",
            "llm_model_used": "not an LLM (graph neural networks / equation-graph architectures)",
            "scientific_domain": "physics (mechanics / force-law discovery)",
            "number_of_papers": null,
            "type_of_quantitative_law": "force laws (e.g., Newton's law) / mathematical relationships between physical variables",
            "extraction_output_format": "functional relationships / symbolic or parametric laws",
            "validation_method": "Recovered laws compared to known physical laws (e.g., Newtonian force law) as described in the survey",
            "performance_metrics": null,
            "baseline_comparison": null,
            "challenges_limitations": "Survey-level mention only; potential limitations include needing well-specified interaction datasets, difficulty extracting compact symbolic forms directly from learned message-passing weights, and sensitivity to training regimes.",
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4301.3",
            "source_info": {
                "paper_title": "A Survey of Scientific Large Language Models: From Data Foundations to Agent Frontiers",
                "publication_date_yy_mm": "2025-08"
            }
        },
        {
            "name_short": "LLM symbolic derivation",
            "name_full": "LLM-based symbolic derivation (symbolic-first reasoning)",
            "brief_description": "A prompting/usage pattern where LLMs perform symbolic derivations by keeping variables symbolic (rather than substituting numeric values early), which has been observed to improve accuracy on physics problem solving.",
            "citation_title": "",
            "mention_or_use": "mention",
            "method_name": "Symbolic-first LLM derivation / reasoning",
            "method_description": "Prompting or training LLMs to carry out symbolic manipulations and derivations (e.g., algebraic manipulation, symbolic calculus) by preserving symbolic variables through reasoning steps, enabling the model to output derivations and final analytic expressions rather than numeric-first solutions.",
            "llm_model_used": null,
            "scientific_domain": "physics / mathematical problem solving",
            "number_of_papers": null,
            "type_of_quantitative_law": "derived equations, symbolic derivations, analytic expressions",
            "extraction_output_format": "symbolic derivations / mathematical equations",
            "validation_method": "Reported via improved accuracy on physics problem solving tasks (survey cites observation but does not provide numeric results here).",
            "performance_metrics": null,
            "baseline_comparison": "Survey notes symbolic-first LLM derivations tend to outperform numeric-first approaches on physics problems, but gives no quantitative comparisons in this paper.",
            "challenges_limitations": "Survey notes need for maintaining symbolic integrity and domain-appropriate manipulation; specific failure modes not detailed here.",
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4301.4",
            "source_info": {
                "paper_title": "A Survey of Scientific Large Language Models: From Data Foundations to Agent Frontiers",
                "publication_date_yy_mm": "2025-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "AI Feynman: A Physics-Inspired Method for Symbolic Regression",
            "rating": 2,
            "sanitized_title": "ai_feynman_a_physicsinspired_method_for_symbolic_regression"
        }
    ],
    "cost": 0.014561749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>A Survey of Scientific Large Language Models: From Data Foundations to Agent Frontiers</h1>
<p>Ming $\mathrm{Hu}^{1,2}$ Chenglong $\mathrm{Ma}^{1,3}$ Wei $\mathrm{Li}^{1,4}$ Wanghan $\mathrm{Xu}^{1,4}$ Jiamin $\mathrm{Wu}^{1,5}$ Jucheng $\mathrm{Hu}^{1,6}$ Tianbin $\mathrm{Li}^{1}$ Guohang Zhuang ${ }^{1}$ Jiaqi Liu ${ }^{1,7}$ Yingzhou Lu ${ }^{8}$ Ying Chen ${ }^{1}$ Chaoyang Zhang ${ }^{1}$ Cheng Tan ${ }^{1}$ Jie Ying ${ }^{1}$ Guocheng Wu ${ }^{1}$ Shujian Gao ${ }^{1}$ Pengcheng Chen ${ }^{1}$ Jiashi Lin ${ }^{1}$ Haitao Wu ${ }^{1}$ Lulu Chen ${ }^{9}$ Fengxiang Wang ${ }^{1}$ Yuanyuan Zhang ${ }^{10}$ Xiangyu Zhao ${ }^{1}$ Feilong Tang ${ }^{1,2}$ Encheng Su ${ }^{1}$ Junzhi Ning ${ }^{1}$ Xinyao Liu ${ }^{1}$ Ye Du ${ }^{1}$ Changkai Ji ${ }^{1}$ Pengfei Jiang ${ }^{1}$ Cheng Tang ${ }^{1}$ Ziyan Huang ${ }^{1}$ Jiyao Liu ${ }^{1,3}$ Jiaqi Wei ${ }^{1}$ Yuejin Yang ${ }^{1}$ Xiang Zhang ${ }^{1}$ Guangshuai Wang ${ }^{1}$ Yue Yang ${ }^{1}$ Huihui Xu ${ }^{1}$ Ziyang Chen ${ }^{1}$ Yizhou Wang ${ }^{1}$ Chen Tang ${ }^{1}$ Jianyu Wu ${ }^{1}$ Yuchen Ren ${ }^{1}$ Siyuan Yan ${ }^{2}$ Zhonghua Wang ${ }^{2}$ Zhongxing Xu ${ }^{2}$ Shiyan Su ${ }^{2}$ Shangquan Sun ${ }^{1}$ Runkai Zhao ${ }^{1}$ Zhisheng Zhang ${ }^{11}$ Dingkang Yang ${ }^{3}$ Jinjie Wei ${ }^{3}$ Jiaqi Wang ${ }^{1}$ Jiahao Xu ${ }^{1}$ Jiangtao Yan ${ }^{1}$ Wenhao Tang ${ }^{1}$ Hongze Zhu ${ }^{1}$ Yu Liu ${ }^{12}$ Fudi Wang ${ }^{13}$ Yiqing Shen ${ }^{14}$ Yuanfeng Ji ${ }^{8}$ Yanzhou Su ${ }^{15}$ Tong Xie ${ }^{16}$ Hongming Shan ${ }^{3}$ Chun-Mei Feng ${ }^{17}$ Zhi Hou ${ }^{1}$ Diping Song ${ }^{1}$ Lihao Liu ${ }^{1}$ Yanyan Huang ${ }^{18}$ Lequan Yu ${ }^{18}$ Bin Fu ${ }^{1}$ Shujun Wang ${ }^{19}$ Xiaomeng $\mathrm{Li}^{20}$ Xiaowei $\mathrm{Hu}^{21}$ Yun Gu ${ }^{4}$ Ben Fei ${ }^{5}$ Benyou Wang ${ }^{22}$ Yuewen Cao ${ }^{1}$ Minjie Shen ${ }^{9}$ Jie Xu ${ }^{1}$ Haodong Duan ${ }^{1}$ Fang Yan ${ }^{1}$ Hongxia Hao ${ }^{1}$ Jielan Li ${ }^{1}$ Jiajun Du ${ }^{23}$ Yanbo Wang ${ }^{24}$ Imran Razzak ${ }^{25}$ Zhongying Deng ${ }^{26}$ Chi Zhang ${ }^{1}$ Lijun Wu ${ }^{1}$ Conghui He ${ }^{1}$ Zhaohui Lu ${ }^{4}$ Jinhai Huang ${ }^{3}$ Wenqi Shao ${ }^{1}$ Yihao Liu ${ }^{1}$ Siqi Luo ${ }^{1}$ Yi Xin ${ }^{1}$ Xiaohong Liu ${ }^{4}$ Fenghua Ling ${ }^{1}$ Yuqiang $\mathrm{Li}^{1}$ Aoran Wang ${ }^{1}$ Siqi Sun ${ }^{1}$ Qihao Zheng ${ }^{1}$ Nanqing Dong ${ }^{1}$ Tianfan Fu ${ }^{27,1}$ Dongzhan Zhou ${ }^{1}$ Yan Lu ${ }^{1}$ Wenlong Zhang ${ }^{1}$ Jin Ye ${ }^{1,2}$ Jianfei Cai ${ }^{2}$ Yirong Chen ${ }^{1}$ Wanli Ouyang ${ }^{1,5}$ Yu Qiao ${ }^{1}$ Zongyuan Ge ${ }^{21}$ Shixiang Tang ${ }^{1,5 \dagger \ddagger}$ Junjun $\mathrm{He}^{1 \dagger \ddagger}$ Chunfeng Song ${ }^{1 \dagger \ddagger}$ Lei Bai ${ }^{1 \dagger \S}$ Bowen Zhou ${ }^{1 \dagger \S}$
${ }^{1}$ Shanghai Artificial Intelligence Laboratory ${ }^{2}$ Monash University ${ }^{3}$ Fudan University
${ }^{4}$ Shanghai Jiao Tong University ${ }^{5}$ The Chinese University of Hong Kong
${ }^{6}$ University College London ${ }^{7}$ UNC-Chapel Hill ${ }^{8}$ Stanford University ${ }^{9}$ Virginia Tech
${ }^{10}$ Purdue University ${ }^{11}$ China Pharmaceutical University
${ }^{12}$ Beijing Institute of Heart, Lung and Blood Vessel Diseases ${ }^{13}$ Chinese Academy of Sciences
${ }^{14}$ Johns Hopkins University ${ }^{15}$ Fuzhou University ${ }^{16}$ University of New South Wales ${ }^{17}$ University College Dublin
${ }^{18}$ The University of Hong Kong ${ }^{19}$ The Hong Kong Polytechnic University
${ }^{20}$ The Hong Kong University of Science and Technology
${ }^{21}$ South China University of Technology ${ }^{22}$ The Chinese University of Hong Kong, Shenzhen
${ }^{23}$ Caltech ${ }^{24}$ North University of China ${ }^{25}$ MBZUAI ${ }^{26}$ University of Cambridge ${ }^{27}$ Nanjing University
(7) Github Repository: https://github.com/open-sciencelab/Awesome-Scientific-Datasets-and-LLMs</p>
<h4>Abstract</h4>
<p>Scientific Large Language Models (Sci-LLMs) are transforming how knowledge is represented, integrated, and applied in scientific research, yet their progress is shaped by the complex nature of scientific data. This survey presents a comprehensive, data-centric synthesis that reframes the development of Sci-LLMs as a co-evolution between models and their underlying data substrate. We formulate a unified taxonomy of scientific data and a hierarchical model of scientific knowledge, emphasizing the multimodal, cross-scale, and domain-specific challenges that differentiate scientific corpora from general natural language processing datasets. We systematically review recent Sci-LLMs, from general-purpose foundations to specialized models across diverse scientific disciplines, alongside an extensive analysis of over 270 pre-/post-training datasets, showing why Sci-LLMs pose distinct demands-heterogeneous, multi-scale, uncertainty-laden corpora that require representations preserving domain invariance and enabling cross-modal reasoning. On evaluation, we examine over 190 benchmark datasets and trace a shift from static exams toward process- and discovery-oriented assessments with advanced evaluation protocols. These data-centric analyses highlight persistent issues in scientific data development and discuss emerging solutions involving semi-automated annotation pipelines and expert validation. Finally, we outline a paradigm shift toward closed-loop systems where autonomous agents based on Sci-LLMs actively experiment, validate, and contribute to a living, evolving knowledge base. Collectively, this work provides a roadmap for building trustworthy, continually evolving artificial intelligence (AI) systems that function as a true partner in accelerating scientific discovery.</p>
<p>Keywords: Large Language Model; AI for Science; Scientific Data; Data4LLM
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1: The song of humanity is a song of courage. The diagram depicts the continuum of scientific inquiry spanning from subatomic particles through atomic and molecular structures, cellular and organismal biology, ecological systems, planetary sciences, to cosmological phenomena. Each tier represents distinct yet interconnected domains of investigation, illustrating the nested hierarchy of natural phenomena and the corresponding disciplinary frameworks employed in their study. This visualization encapsulates the expansion of scientific understanding from micro to macro dimensions, symbolizing humanity's persistent pursuit of knowledge across all scales of nature.</p>
<h1>CONTENTS</h1>
<p>I Introduction ..... 6
II Background ..... 9
II-A Taxonomy of Scientific Data ..... 9
II-A1 Textual Formats ..... 9
II-A2 Visual Data ..... 10
II-A3 Symbolic Representations ..... 12
II-A4 Structured Data ..... 13
II-A5 Time-Series Data ..... 14
II-A6 Multi-omics Integration ..... 14
II-B Hierarchical Structure of Scientific Knowledge ..... 16
II-B1 Factual Level ..... 16
II-B2 Theoretical Level ..... 17
II-B3 Methodological and Technological Level ..... 17
II-B4 Modeling and Simulation Level ..... 18
II-B5 Insight Level ..... 18
II-B6 Dynamic Interactions and Evolution ..... 18
II-B7 Implications for Sci-LLMs ..... 19
II-C Key Challenges in Scientific AI ..... 19
II-C1 Interpretability in Scientific AI ..... 19
II-C2 Cross-scale and Multimodal Integration ..... 19
II-C3 Dynamic Knowledge Evolvement ..... 20
II-D Quality Standards for Scientific Datasets ..... 20
II-D1 Accuracy ..... 20
II-D2 Completeness ..... 20
II-D3 Timeliness ..... 20
II-D4 Traceability ..... 21
II-E Dimensions for Evaluating Scientific AI ..... 21
II-E1 Expert-Level Scientific Knowledge Comprehension and Retrieval ..... 21
II-E2 Scientific Reasoning and Problem Solving ..... 21
II-E3 Multimodal Scientific Data ..... 21
III Scientific Large Language Models ..... 21
III-A Introduction of Large Language Models ..... 22
III-B General-purpose Sci-LLMs ..... 22
III-C Domain-specific Sci-LLMs ..... 24
III-C1 Physics ..... 24
III-C2 Chemistry ..... 25
III-C3 Materials Science ..... 25
III-C4 Life Sciences ..... 26
III-C5 Astronomy ..... 29
III-C6 Earth Science ..... 29
III-D Sci-LLMs Analysis ..... 30
IV Scientific Data for Pre-training ..... 31
IV-A Physics, Chemistry and Material Sciences: the Foundation for Understanding the Material World ..... 32
IV-A1 Physics ..... 32
IV-A2 Chemistry ..... 33
IV-A3 Materials Science ..... 33
IV-B Life Sciences: Complexity from Molecules to Systems ..... 33
IV-B1 Molecular and Cell Biology ..... 33
IV-B2 Multi-Omics ..... 34
IV-B3 Neuroscience ..... 34
IV-B4 Healthcare and Medical Science ..... 34
IV-B5 Agriculture ..... 35
IV-C Astronomy and Earth Science: Understanding Our Planet ..... 35
IV-C1 Astronomy ..... 35</p>
<p>IV-C2 Earth Science ..... 35
IV-D Pre-training Data Analysis ..... 36
V Scientific Data for Post-training ..... 36
V-A Current Landscape Across Scientific Domains ..... 37
V-A1 Physics ..... 37
V-A2 Chemistry ..... 37
V-A3 Materials Science ..... 37
V-A4 Life Sciences ..... 37
V-A5 Astronomy ..... 38
V-A6 Earth Science ..... 38
V-B Post-training Data Analysis ..... 39
VI Evaluation of Sci-LLMs ..... 40
VI-A Current Landscape Across Scientific Domains ..... 40
VI-A1 Physics ..... 40
VI-A2 Chemistry ..... 40
VI-A3 Materials Science ..... 41
VI-A4 Life Sciences ..... 41
VI-A5 Astronomy ..... 41
VI-A6 Earth Science ..... 41
VI-A7 General Science ..... 42
VI-B Evaluation Data Analysis ..... 42
VI-B1 Tiered Regime in Data Generation and Annotation ..... 43
VI-B2 Skewed Knowledge Level with Increasing Difficulty ..... 43
VI-B3 Shift towards Domain-Specific Metrics ..... 44
VI-C LLM / Agent as a Judge ..... 44
VI-D Inspiration from Test-Time Learning ..... 45
VII Scientific Data Development ..... 45
VII-A Data Collection and Labeling ..... 45
VII-A1 Data Source Heterogeneity and Acquisition Strategies ..... 46
VII-A2 Annotation Methodologies and Quality Control ..... 46
VII-A3 Cross-Domain Patterns and Domain-Specific Considerations ..... 47
VII-B Limitations of Current Scientific Datasets ..... 47
VII-B1 Scarcity of Experimental Data ..... 47
VII-B2 Over-reliance on Text Modality Data ..... 48
VII-B3 Representation Gap between Static Knowledge and Dynamic Processes ..... 48
VII-B4 Multi-level Biases in Scientific Datasets ..... 48
VII-C Systematic Issues in Data Quality ..... 48
VII-C1 Data Traceability Crisis ..... 49
VII-C2 Scientific Data Latency ..... 49
VII-C3 The Lack of AI-readiness ..... 49
VIII New Paradigms for Data-Driven Sci-LLMs ..... 49
VIII-A Scientific Agent ..... 49
VIII-A1 LLMs as Scientific Agents ..... 49
VIII-A2 Multi-Agent Collaboration ..... 50
VIII-A3 Tool Use ..... 50
VIII-A4 Self-evolving Agents ..... 50
VIII-A5 Evaluation Frameworks and Benchmarking ..... 51
VIII-A6 Autonomous Scientific Discovery ..... 51
VIII-B Data Ecosystems for Sci-LLMs ..... 51
VIII-B1 The Data Bottleneck Behind the Rise of Scientific Agents ..... 52
VIII-B2 Building an Operating System-level Interaction Protocol ..... 52
VIII-B3 Design Principles for Next-Generation Scientific Data Architecture ..... 53
VIII-B4 Sustainable Data Sharing Mechanism ..... 53
VIII-B5 Data Safety and Privacy ..... 53</p>
<p>IX Challenges and Outlook ..... 54
IX-A Challenges ..... 54
IX-A1 Scientific Data Selection for Efficient Pretraining ..... 54
IX-A2 Optimizing Data Processing Pipelines ..... 54
IX-A3 Representing Non-Sequential and Non-Textual Data ..... 54
IX-A4 LLM Knowledge Update and Version Control ..... 54
IX-B Future Work ..... 55
IX-B1 Integrated Scientific Data Ecosystems ..... 55
IX-B2 Automated Scientific Data Standardization Pipeline ..... 55
IX-B3 Comprehensive Evaluation System ..... 55
IX-B4 Advanced Scientific Reasoning ..... 55
IX-B5 Autonomous Scientific Agents ..... 55
IX-B6 From Sci-LLMs to Scientific Discovery ..... 55
IX-B7 Ethical Governance for Responsible Scientific AI Innovation ..... 55
X Conclusion ..... 55
References ..... 69</p>
<h2>I. INTRODUCTION</h2>
<p>"Science is built up with facts, as a house is with stones. But a collection of facts is no more a science than a heap of stones is a house."</p>
<ul>
<li>Henri Poincar</li>
</ul>
<p>The rapid advancement of large language models (LLMs) has sparked a paradigm shift across numerous domains, demonstrating unprecedented transformative potential through task automation, productivity enhancement, and breakthrough innovations [1]-[5] (Fig. 2). These models have fundamentally transformed scientific research by introducing a unified approach that replaces traditional task-specific methods, extending beyond natural language processing to encompass diverse scientific data types, including molecules [6], proteins [7], tables [8], and complex metadata. LLMs have already revolutionized fields such as software engineering [2], [9], [10], law [11], [12], materials science [13], [14], healthcare [15][17], and biomedical research [18], and have been applied across disciplines from mathematics [19] and physics to chemistry [20], biology [21], and geoscience [22].</p>
<p>The evolution of scientific LLMs (Sci-LLMs) has undergone a paradigm shift through four distinct data-driven phases from 2018 to 2025 (Fig. 3). The initial transfer learning phase (2018-2020) witnessed domain-specific adaptations of BERT [23] architecture, with models like SciBERT [24], BioBERT [25], and PubMedBERT [26] trained on largescale scientific corpora, showing that continued pre-training on domain literature yields sizable gains in downstream tasks that require scientific text understanding. These models provided reliable, static concept representations for specific downstream uses, but struggled to synthesize or generate novel scientific content at scale. The subsequent scaling phase (2020-2022) embraced parameter and token-count expansion, marking a critical transition. Models like GPT-3 [27] with 175 billion parameters, along with later data/compute-optimal training rules [28], [29] demonstrated that massive parameter scaling with diverse training data could achieve emergent knowledge integration capabilities, fundamentally altering the landscape of scientific AI. Galactica [30] extended this lesson to science, with 120 billion parameters trained on more than 48 million scientific papers, textbooks, and encyclopedias, designing specialized tokenization schemes for mathematical formulas, chemical structures, and citations. MedPaLM-2 [31], further instruction-tuned on multiple medical-domain datasets and achieved over $85 \%$ accuracy on USMLE-style questions, becoming the first AI system to exhibit expert-level medical reasoning capabilities comparable to those of licensed physicians. However, scaling ran into a data wall for Sci-LLMs: unlike general-domain crawls with hundreds of billions to trillions of tokens, high-quality scientific text corpora were orders of magnitude smaller, with abundant scientific raw data underutilized in early large-scale attempts.</p>
<p>The instruction-following phase (2022-2024) shifted focus from capacity to alignment, introducing task adaptation via reinforcement learning from human feedback (RLHF). Examples include InstructGPT [32] and ChatGPT [33], enabling
more precise scientific task execution. Subsequently, foundational architectures represented by open-source LLMs (e.g., LLaMA [34], Qwen [35], ChatGLM [36], and Mistral [37]) have enabled unprecedented diversity in scientific applications. Concurrently, the unprecedented expansion of instruction datasets has given rise to a series of milestone Sci-LLMs. Specifically, in the biomedical field, Meditron [38], pre-trained on 48.1 billion tokens from medical literature, demonstrates the potential of open-source models in professional medical reasoning. ProteinChat [39], trained on 1.5 million protein-prompt-answer triplets, facilitates protein research; LLaMAGene [40] integrates gigabytes of DNA, protein, and text data and 500 millions of instruction examples in DNA/protein tasks for training, achieving cross-modal biological sequence understanding. The multidisciplinary model SciGLM [41] leverages the efficient architecture of ChatGLM, fine-tuned on 254,000 carefully constructed instruction examples, achieving cross-disciplinary knowledge integration capabilities. Notably, several works demonstrate a strong correlation between data scale and model performance: HuatuoGPT-II [42] utilizes an 11 TB medical corpus with million-scale documents for pretraining, while NatureLM [43] is pre-trained on 143 billion tokens and fine-tuned using 45.1 million instruction-response pairs. This dual-drive paradigm of "architectural diversity + data scaling" has become the core framework for current scientific large language model development.</p>
<p>Beyond excelling at analyzing existing scientific data, these models demonstrate remarkable potential in accelerating scientific discovery via hypothesis generation, theorem proving, experiment design, drug discovery, and weather forecasting, fundamentally reshaping how complex challenges are approached and solved in the era of AI-driven research [44][46]. As a prominent example of this trend, Intern-S1 [47] is a scientific multimodal Mixture-of-Experts (MoE) [48] foundation model with general understanding and reasoning capabilities alongside specialized expertise in scientific data analysis. Continually pre-trained on massive scientific data with 2.5 trillion tokens and enhanced with a Mixture-ofRewards reinforcement learning, it surpasses existing closedsource state-of-the-art models in professional tasks such as molecular synthesis, reaction condition prediction, and crystalline thermodynamic stability prediction, while maintaining leading performance on general reasoning tasks.</p>
<p>The latest paradigm of agentic science (2023-now) is enabling AI systems with scientific agency, able to plan, act, and iterate across stages of discovery. Many works demonstrate end-to-end scientific workflows [44], [49], with increasing focus on multi-agent [50], [51] and tool ecosystems [18], [52]. Multi-agent designs emulate laboratory hierarchies from principal investigators to domain specialists, coordinating through formalized meeting protocols and critique-iteration loops [53], [54]. Such systems generate scientific ideas with improved novelty and feasibility by explicitly modeling research teamwork [55] and scientific law constraints [56]. At scale, cooperative frameworks manage entire research lifecycles (problem scoping, manuscript drafting, etc.), preserving persistent artifacts and audit trails [57], while embodied variants integrate robotic execution with adaptive planning [58]. Parallel</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 2: Cumulative trend of publications on major preprint platforms whose titles or abstracts mention the keyword "language model" or the combination "language model + scientific domain" (e.g., chemistry, physics, multi-omics, medicine, etc.). Left: Results from January 2018 to August 2025, from arXiv and PubMed. For arXiv, the matching includes "language model" in combination with additional science-related keywords; PubMed results are limited to occurrences in titles and abstracts. Both platforms show rapid growth. Right: Results from 2020 to August 2025, from bioRxiv, medRxiv, and ChemRxiv, all based on direct matches of "language model" in titles and abstracts. While the overall volumes are smaller than arXiv and PubMed, all three platforms, especially bioRxiv, show rapid acceleration, reflecting growing interdisciplinary interest in large language models across biomedical, chemical, and computational sciences.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 3: Evolution of Sci-LLMs reveals four paradigm shifts from 2018 to 2025, including (1) the progression from transfer learning approaches, (2) through the scaling era marked by knowledge integration in larger models, (3) instruction-following capabilities enabling flexible task adaptation, to (4) the latest paradigm introduces scientific agentsAI systems capable of autonomously conducting scientific research, from hypothesis generation and experimental design to data analysis and discovery. <strong>Note:</strong> Model positions reflect their release dates (x-axis) rather than strict paradigm classification. The four paradigms represent evolving trends in Sci-LLM development with overlaps and continuities, not mutually exclusive categories.</p>
<p>advances in tool integration center on knowledge-graphdriven orchestration [59] and domain-scale agents interfacing with hundreds of software tools, databases, and instruments with provenance tracking [18].</p>
<p>Despite these promising results, Sci-LLMs encounter fundamental challenges stemming from the <em>unique characteristics of scientific data and knowledge representation</em>. Unlike the relatively homogeneous text corpora for general-purpose LLM development, scientific datasets exhibit extreme heterogeneity across modalities and formats. For instance, in</p>
<p>chemistry alone, models must reconcile molecular strings, 3D molecular coordinates, spectroscopic data, and reaction mechanisms, each requiring distinct processing strategies [60]. This heterogeneity extends beyond chemistry to encompass the full spectrum of scientific disciplines. In life sciences, models must simultaneously process genomic sequences, protein structures, multi-omics data, and clinical imaging [61][63], while astronomical applications demand integration of time-series photometry, spectroscopic observations, and multiwavelength imaging across vastly different spatial and temporal scales [64], [65].
The challenge is further compounded by the hierarchical nature of scientific knowledge itself, which spans from raw observational data to abstract theoretical frameworks, each with its own representational requirements [66], [67]. Moreover, scientific data often embodies domain-specific semantics that resist straightforward tokenization or embedding. Mathematical equations carry precise symbolic relationships that must be preserved during processing [68], [69], while crystallographic information files encode 3D structural constraints essential for materials science applications [70], [71]. Time-series data from instruments like Laser Interferometer Gravitational-Wave Observatory (LIGO) contain subtle signals buried in noise, requiring specialized preprocessing for physical interpretability [65], [72]. These diverse data types cannot be adequately represented through conventional text-based approaches, necessitating novel architectures that preserve domain-specific invariance while enabling cross-modal reasoning [73]-[75]. The integration of such heterogeneous data sources poses additional computational and methodological challenges. Crossscale modeling, from quantum mechanical calculations to macroscopic phenomena, demands architectures capable of capturing multi-resolution dependencies [76]. Furthermore, the uncertainty in experimental measurements require models to propagate error bounds and maintain scientific rigor throughout the reasoning process [77]-[79]. These constraints fundamentally distinguish scientific AI from general-purpose language modeling, requiring specialized solutions that respect the unique epistemological foundations of scientific inquiry.
The inherent complexity of scientific data and reasoning naturally extends to the evaluation of Sci-LLMs, where conventional natural language processing benchmarks prove insufficient for capturing domain-specific competencies. Recent efforts have produced comprehensive evaluation suites such as ScienceQA [80], which tests multimodal scientific understanding across elementary to graduate levels, and MMLU-Pro [81], which includes rigorous assessments in specialized fields like quantum physics and molecular biology. However, these benchmarks often fail to capture the nuanced requirements of scientific discovery, e.g., the ability to generate novel hypotheses, identify non-obvious connections between disparate findings, or design experiments that test theoretical predictions. To address this gap, Liu et al. propose ResearchBench [82], a large-scale scientific discovery benchmark spanning 12 disciplines to systemically evaluate the hypothesis generation capabilities of LLMs. Furthermore, researchers have also begun developing process-oriented evaluations that assess intermediate reasoning steps rather than just final answers, exemplified by
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig. 4: Six main scientific domains covered in this survey. The figure illustrates the primary disciplines investigated in our study on science-oriented large language models, encompassing Chemistry, Materials Science, Physics, Life Sciences, Astronomy, and Earth Science, along with representative subfields within each domain.
frameworks like ScienceAgentBench [83] that evaluate models on complex scientific workflows, including literature review, experimental design, and result interpretation. Benchmarks such as MultiAgentBench [84] and WorkflowBench [85] now quantify collaboration, coordination, and workflow synthesis skills, marking a shift toward measurable, safety-aware, and reproducible science automation. The community has also recognized that scientific validity requires more than linguistic fluency; models must respect fundamental constraints such as physical laws, chemical valence rules, and biological feasibility [21], [86], [87]. This has led to the integration of symbolic reasoning modules and constraint satisfaction systems that act as guardrails during generation, ensuring that model outputs remain within scientifically plausible bounds while still allowing for creative exploration at the frontiers of knowledge.</p>
<p>To address these gaps, several survey papers look into adjacent facets of the problem. A few works [88], [89] focused on models and tasks for biomedical data; Zhang et al. [21] examined Sci-LLMs under a broader perspective that involves both biological and chemical domains. Other works [60] explored the application of Sci-LLMs in scientific discovery. Wei et al. [90] and Wang et al. [91] reviewed scientific agent paradigms and system designs for autonomous research and scientific discovery. Ni et al. [92] conducted a survey on existing benchmarks for LLMs involving several science fields. Chen et al. [93] provided a comprehensive survey on AI for autonomous scientific research, offering a systematic taxonomy and compiling resources across multiple disciplines. However, these reviews are theme-specific and</p>
<p>limited to models with only a cursory touch on the underlying substrate-scientific datasets, throughout pre-training, posttraining and evaluation. Complementing these perspectives, our survey contributes a unified, cross-disciplinary synthesis that explicitly links data foundations to agent frontiers. We summarize the contributions as follows:</p>
<ul>
<li>By introducing a unified taxonomy of scientific data and a hierarchical model of scientific knowledge, we provide a novel epistemological framework for analyzing the challenges in representing scientific information, from raw observational data and symbolic notations to abstract theoretical insights.</li>
<li>We deliver a comprehensive and structured account of the rapidly evolving landscape of scientific large language models across six main scientific domains (i.e., physics, chemistry, life sciences, Earth Science, astronomy, and materials science; as in Fig. 4).</li>
<li>By systematically analyzing over 270 pre- and posttraining datasets, we provide a comprehensive panorama of current scientific datasets for Sci-LLM development, distilling the multimodal, cross-scale, and domainspecific challenges that distinguish Sci-LLMs from their general-purpose counterpart.</li>
<li>We conduct a comprehensive review of over 190 evaluation datasets for Sci-LLMs, discussing the shift of evaluation from static exams to research-level scientific discovery, the increasing employment and combination of domain-specific metrics, and the emergence of advanced evaluation methodologies.</li>
<li>We identify structural failures in scientific data curation and translate them into a forward-looking data development agenda that supports advanced scientific intelligence, advocating for a closed-loop feedback between autonomous scientific discovery and scientific data infrastructure.
Collectively, these contributions establish a consolidated reference and a clear roadmap for building trustworthy, continually evolving Sci-LLMs capable of accelerating data-driven scientific discovery.
The paper is organized as follows: Sec. II formulates a unified taxonomy of scientific data grounded in a hierarchical model of scientific knowledge. Sec. III shows the landscape of Sci-LLMs across six main scientific domains. Secs IV, V, and VI provide an extensive catalog and analysis of existing pre-training, post-training, and evaluation datasets for SciLLMs. Sec. VII analyzes how scientific data shapes LLM development and identify systemic issues that impede AIreadable corpora. Sec. VIII outlines forward directions for scientific discovery empowered by advanced scientific agents and data ecosystems. Secs. IX and X summarize challenges, outlook, and conclusion distilled from the paper.</li>
</ul>
<h2>II. BACKGROUND</h2>
<p>This section provides the foundations for understanding scientific AI systems. We first examine the diverse taxonomy of scientific data across disciplines (Sec. II-A), followed by an analysis of the hierarchical structure of scientific knowledge
(Sec. II-B), which reveals that scientific understanding forms a sophisticated multilevel system rather than a simple information repository. Then, we identify critical challenges unique to scientific AI (Sec. II-C), including knowledge consistency, interpretability, and the integration of cross-scale multimodal data. We conclude by establishing frameworks for evaluating both data quality standards (Sec. II-D) and AI system capabilities specific to scientific domains (Sec. II-E). These elements collectively define the requirements for AI systems designed to support rigorous scientific discovery and reasoning.</p>
<h2>A. Taxonomy of Scientific Data</h2>
<p>Scientific data manifests in striking diversity across disciplines, shaped by the fundamental questions and methodological paradigms unique to each field. In this subsection, we review and summarize the primary data types and modalities across scientific domains, examining how they appear and function within different scientific contexts, including: textual formats (papers, experimental reports) in Sec. II-A1, visual data (medical scans, astronomical observations) in Sec. II-A2, symbolic representations (formulas, chemical structures) in Sec. II-A3, structured data (databases, knowledge graphs) in Sec. II-A4, and time-series data (neurophysiological recordings, astronomical light curves) in Sec. II-A5. In addition to these general types, we also discuss multi-omics integration in Sec. II-A6 as a special case, as it represents an emerging paradigm that requires combining heterogeneous data across multiple biological layers (e.g., genomics, transcriptomics, proteomics). This taxonomy sets the stage for understanding how scientific data collectively support AI-driven scientific discovery across domains, and also establishes the foundation for developing multimodal large language models (MLLMs) which aim to process and integrate heterogeneous scientific data within a unified framework.</p>
<p>1) Textual Formats: Scientific textual data forms the foundational substrate for knowledge representation across disciplines, encompassing a rich hierarchy from primary experimental documentation to synthesized knowledge repositories. At the most granular level, laboratory notebooks, experimental protocols, and field observations capture the raw process of scientific discovery, documenting not only successful experiments but also failed attempts and methodological refinements that prove invaluable for reproducibility and knowledge transfer [94]. This primary documentation feeds into specialized databases and repositories that have become central to modern scientific practice: genomic sequences in GenBank [95], protein structures in RCSB [96], chemical compounds in PubChem [97], [98], and astronomical observations in NASA's Astrophysics Data System (ADS) [99], collectively housing petabytes of structured information linked to their textual descriptions and metadata.</p>
<p>The scholarly communication layer builds upon this foundation through peer-reviewed journals, comprehensive textbooks, and increasingly, preprint repositories that accelerate knowledge dissemination. Traditional venues like Physical Review Letters, The Astrophysical Journal, and Monthly Notices of the Royal Astronomical Society maintain rigorous standards while</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Fig. 5: Examples of visual data across typical medical imaging modalities, involving radiology (PET, CT, mammography, X-ray, MRI, and ultrasound), dermatology, ophthalmology (CFP, FFA, UWF-SLO, and OCT), endoscopy, histopathology, and cellular microscopy. The figure is sourced from open-source medical datasets.
platforms such as arXiv [100] and ChemRxiv [101] enable rapid sharing of emerging findings across physics, astronomy, chemistry, and interdisciplinary domains. This academic corpus is complemented by educational resources ranging from open-access textbooks like OpenStax series [102], [103] and The Feynman Lectures [104] to specialized training materials including agricultural extension question-answering (QA) records [105], examination questions, and curated datasets for AI model evaluation such as ScholarChemQA [106], ScienceQA [107], and materials science benchmarks [108][110].</p>
<p>Beyond traditional academic outputs, scientific textual data increasingly encompasses regulatory documentation, real-time observational streams, and computational artifacts that reflect the evolving nature of modern research. Clinical trial registries [111], institutional review protocols [112], and biosafety guidelines [113] ensure responsible research conduct, while electronic health records [114], [115], citizen science annotations from projects like Galaxy Zoo [116], and realtime environmental monitoring data [117] bridge laboratory findings with societal applications. The integration of computational approaches has spawned new textual categories, including bioinformatics pipelines [118], systems biology models [119], synthesis planning frameworks [120], and code generation benchmarks [121], [122], all requiring extensive documentation for reproducibility. This diverse textual ecosystem not only archives scientific progress but enables metaanalyses [123], knowledge synthesis efforts, and increasingly sophisticated AI-driven discovery across the full spectrum of scientific inquiry.
2) Visual Data: Visual data in scientific domains broadly fall into two categories: instrumental imaging that directly captures physical subjects through various sensing technologies, and diagrammatic representations that abstract and visualize concepts, relationships, and analytical results. These visual data span an extraordinary range of scales and modalities, from sub-atomic particle interactions to cosmic structures, providing essential foundations for multimodal AI systems to understand scientific phenomena.</p>
<p>At the smallest scales, as shown in Fig. 6, advanced microscopy techniques, including scanning and transmission
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Fig. 6: Examples of visual data in physics. SEM of epoxy with/without AlN [124]; TEM of W-doped $\mathrm{Cu}-\mathrm{Pt}$ nanoalloys [125]; AFM topography of hyper-stoichiometric $\mathrm{UO}<em 2="2">{2}$ [126]; STM of $\mathrm{Si}(111)-(7 \times 7)$ at multiple scan sizes [127]; UV/Vis contour map (500-680 nm) [128]; Infrared thermographs of a directional emitter [129]; Raman helicity-resolved maps of $1 \mathrm{~T}-\mathrm{TaS}</em>$ [130]; NMR of yttrium hydrides [131]. All panels are reused or adapted under the stated licenses (CC-BY-4.0 or CC-BY), with minor cropping only.
electron microscopy (SEM/TEM) [132], [133], atomic force microscopy (AFM) [134], and scanning tunneling microscopy (STM) [135], reveal atomic structures and molecular arrangements critical for physics, materials science and chemistry. Visual spectrum data, including ultraviolet-visible spectrophotometry (UV/Vis) [136], infrared [137], Raman [138], and nuclear magnetic resonance (NMR) [139] spectroscopy, serve as molecular "fingerprints" across chemistry, materials science, and physics, with visual representations proven effective for spectrum learning [140], [141].</p>
<p>In life sciences, light microscopy (brightfield, confocal) and fluorescence microscopy capture cellular structures and protein localizations, with datasets like the Human Protein Atlas [142] and Broad Bioimage Benchmark Collection [143] supporting cell segmentation and phenotype classification tasks. These microscopy images, typically stored in formats like TIFF [144] or ND2 [145], have been increasingly leveraged for training visual-language models [146], [147]. Moving up in scale, whole-slide digital pathology produces gigapixel images stored</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Fig. 7: Data from Earth science's six major domains, including the lithosphere, anthroposphere, biosphere, cryosphere, hydrosphere, and atmosphere. Each panel consists of geospatial data, maps, satellite imagery, charts, etc. These data sources are highly diverse, encompassing a wide range of spatial and temporal resolutions, as detailed in Sec. II-B1. The figure is sourced from MSEarth [153], and authorization for its use has been obtained from the original author.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Fig. 8: Examples of astronomical data, demonstrating the application of radio signals, optical signals, and infrared signals in imaging different astronomical objects. The image is sourced from NASA.
in SVS format, essential for cancer diagnosis, with large cohorts like TCGA [148] and CPTAC [149] providing thousands of images paired with diagnostic reports [150]-[152].</p>
<p>At tissue and organ scales, radiological imaging encompasses multiple modalities including X-rays [154], [155], computed tomography (CT) [156]-[158], histopathology [159], magnetic resonance imaging (MRI) [160], [161], ultrasound [162], [163], positron emission tomography (PET) [164], [165], and mammography [166], each revealing different aspects of internal anatomy and function. These images, commonly stored in DICOM [167] or NIfTI [168] formats with rich metadata, can be processed using specialized viewers like RadiAnt [169] and MRIcroGL [170] or program-
matic libraries such as pydicom [171] and SimpleITK [172]. Clinical imaging extends to specialized domains like ophthalmology with color fundus photography (CFP) [173][175], fundus fluorescein angiography (FFA) [176], ophthalmology [177] and optical coherence tomography (OCT) [178], [179], dermatology for skin lesion analysis [180], [181] ophthalmic surgical microscopy for high-resolution intraoperative visualization in ophthalmic procedures [182]-[185], and endoscopy for surgical guidance [186]-[188]. These visual data, once paired with their descriptions and reports, hold great potential in developing healthcare MLLMs; visualization examples are shown in Fig. 5.</p>
<p>At macroscopic scales, natural photographs capture biodiversity through datasets like iNaturalist [189], while agricultural visual data span from micro-level plant imaging to macrolevel UAV and satellite imagery for crop monitoring [190][192]. Earth science leverages satellite remote sensing [193], [194] and atmospheric datasets [195], [196] for climate modeling and environmental monitoring. As shown in Fig. 7, due to the diversity of their collection sources, earth observation data exhibit significant variability. For instance, some data are obtained from ground-based observation stations, offering long-term and continuous records at specific locations. Other datasets are derived from multispectral remote sensing technologies, which provide comprehensive information on surface and atmospheric characteristics across larger spatial scales. Additionally, reanalysis data [195] integrate observational records with numerical models, resulting in meteorological and environmental parameters with enhanced temporal and spatial consistency. These various types of data each possess</p>
<p>unique features in terms of spatial coverage, temporal resolution, and observational content, offering a multi-dimensional information foundation for research in earth system science. Beyond Earth, astronomical observations across the radio interferometry [197] to optical [64], [198] and infrared [199], capture celestial phenomena, complemented by spectroscopic data from instruments like Large sky Area Multi-Object fiber Spectroscopic Telescope (LAMOST) [200] that reveal chemical compositions and stellar dynamics, as illustrated in Fig. 8.</p>
<p>Complementing direct imaging, diagrammatic figures and spectroscopic visualizations provide crucial abstractions of scientific knowledge that cannot be captured through photography alone. Molecular structure diagrams, increasingly recognized as natural interfaces for chemical AI systems [201], have been curated into large-scale datasets for tasks ranging from image captioning to property prediction [97], [202], [203]. Schematic diagrams and conceptual illustrations from scientific literature [204]-[207] distill complex processes and experimental setups into accessible forms, essential for both human understanding and AI interpretation. These diverse visual modalities from atomic-resolution microscopy to cosmic surveys, and from molecular diagrams to climate visualizations, collectively form a rich multimodal foundation for scientific AI systems. The integration of these varied visual elements into comprehensive datasets like MaCBench [208] and MMSci [75] enables models to synthesize knowledge across disciplines, though challenges remain in aligning dense visual information with semantic textual descriptions, particularly for complex phenomena in molecular biology, materials science, and mathematical physics that require advanced multimodal learning techniques.
3) Symbolic Representations: Symbolic representations constitute a fundamental data modality in scientific computing, providing abstract, non-numeric encodings of scientific entities, relationships, and laws that are both humaninterpretable and machine-processable. These representations include molecular structures encoded as string notations, such as Simplified Molecular-Input Line-Entry System (SMILES) strings [209], International Chemical Identifier (InChI) codes [210], Self-Referencing Embedded Strings (SELFIES) [211]), Crystallographic Information Files (CIF) for material structures, and parameterized equations for physics and Earth system modeling. The significance of symbolic data lies in its ability to encode complex scientific knowledge in compact, manipulable forms that preserve semantic meaning while enabling automated reasoning, transformation, and discovery operations critical for modern scientific computing.</p>
<p>The most prevalent symbolic representations in chemistry and materials science are string-based molecular encodings, with SMILES [209] being the de facto standard since the 1980s. SMILES is a specification in the form of a line notation for describing the structure of chemical species using short ASCII strings, encoding molecular structures using ASCII strings with specific rules: atoms are represented by their chemical element symbols (often with brackets omitted), bonds by symbols including "-" (single), "=" (double), "#" (triple), ";" (aromatic), rings by breaking cycles and adding
matching numbers (e.g., "O1CCOCC1" for 1,4-Dioxane), aromatic rings using lowercase letters or alternating bonds (e.g., "c1ccccc1" for benzene), and branches using parentheses (e.g., "CCC(=O)O" for propionic acid). An extension of SMILES for polymers is BigSMILES [212], which represents polymers as stochastic objects with monomers enclosed in curly brackets, as illustrated in Fig. 9. However, SMILES suffers from syntactic fragility-small perturbations can render strings invalid. To address this, SELFIES (SELF-referencing Embedded Strings) [213] was introduced in 2020, guaranteeing $100 \%$ validity through formal grammar rules. SELFIES uses a vocabulary of tokens like "[C]", "[=O]", "[Branch]", "[Ring]" with localized markers for branches and rings, enabling robust left-to-right parsing that gracefully handles errors. Fig. 10 shows examples of Formaldehyde and Phenol's molecular graphs and corresponding SMILES and SELFIES strings. The difference between SMILES, BigSMILES, and SELFIES is demonstrated in Table I. Beyond strings, molecular graphs provide more intuitive representations where nodes correspond to atoms and edges to bonds, with adjacency matrices encoding connectivity and bond types [214]. Recent benchmark [215] reveals that SMILES remains most expressive for molecular optimization tasks, while SELFIES often underperforms due to redundancy.</p>
<p>For crystalline materials, the CIF format serves as the standard, encoding unit cell parameters (lattice constants $a, b, c$, angles $\alpha, \beta, \gamma$ ), atomic positions in fractional coordinates, space group symmetries, and experimental metadata in a structured key-value format readable by tools like pymatgen and VESTA. These representations underpin major databases including ZINC [216], ChEMBL [217], USPTO [218], ICSD, and the Materials Project [70], as well as benchmarks like MoleculeNet [219] and MatBench [71].</p>
<p>In physics and astronomy, symbolic representations extend beyond structural encodings to encompass mathematical expressions, differential equations, and theoretical frameworks that enable automated scientific discovery. At the core are algebraic equations, differential/integral forms, and probability distributions, with recent work demonstrating that LLMs performing symbolic derivation, i.e., keeping variables symbolic before late-stage numerical substitution, tend to achieve higher accuracy on physics problem solving compared with numericfirst approaches [68]. Equation graphs represent variables and operators as nodes, enabling graph-based symbolic regression; for instance, graph networks trained on force-law data successfully recover Newton's law through messagepassing outputs [220]. Building on this foundation, LLMpowered methods like Dual Reasoning Symbolic Regression integrate language model reasoning with reflective optimization for equation extraction [69]. In astronomy, systems like PhyE2E [221] demonstrate end-to-end neural symbolic regression, generating dimensionally consistent formulas from diverse sources including NASA's THEMIS mission data [222], AI Feynman datasets [223], [224], and solar observation data (SILSO) [225]. Similarly, Earth science employ symbolic representations through mathematical formula fitting and regression for modeling complex phenomena governed by partially understood physics, such as the Navier-Stokes equations [226]</p>
<p>TABLE I: Comparison of SMILES, BigSMILES, and SELFIES representations.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Feature</th>
<th style="text-align: center;">SMILES [209]</th>
<th style="text-align: center;">BigSMILES [212]</th>
<th style="text-align: center;">SELFIES [213]</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Primary domain</td>
<td style="text-align: center;">Small molecules</td>
<td style="text-align: center;">Polymers and macromolecules</td>
<td style="text-align: center;">Small molecules</td>
</tr>
<tr>
<td style="text-align: center;">Syntax basis</td>
<td style="text-align: center;">ASCII strings with chemical rules</td>
<td style="text-align: center;">SMILES syntax + curly bracket extensions</td>
<td style="text-align: center;">Tokenized grammar rules</td>
</tr>
<tr>
<td style="text-align: center;">Connectivity encoding</td>
<td style="text-align: center;">Explicit bonds, rings, branches</td>
<td style="text-align: center;">Bonds, rings, branches + bonding descriptors ( $[*]$ )</td>
<td style="text-align: center;">Encoded via grammar tokens</td>
</tr>
<tr>
<td style="text-align: center;">Stochastic representation</td>
<td style="text-align: center;">Not supported</td>
<td style="text-align: center;">Supported via curly brackets</td>
<td style="text-align: center;">Not supported</td>
</tr>
<tr>
<td style="text-align: center;">Polymer architecture</td>
<td style="text-align: center;">Not supported</td>
<td style="text-align: center;">Supports block, random, graft, branched</td>
<td style="text-align: center;">Not supported</td>
</tr>
<tr>
<td style="text-align: center;">Error tolerance</td>
<td style="text-align: center;">Fragile-small changes can break validity</td>
<td style="text-align: center;">Same as SMILES for monomers</td>
<td style="text-align: center;">Guaranteed 100\% valid</td>
</tr>
<tr>
<td style="text-align: center;">Typical example</td>
<td style="text-align: center;">CCO (ethanol)</td>
<td style="text-align: center;">$\left{[<em>] \mathrm{CC}[</em>]\right}$ (polyethylene)</td>
<td style="text-align: center;">Guaranteed 100\% valid</td>
</tr>
<tr>
<td style="text-align: center;">Advantages</td>
<td style="text-align: center;">Compact, widely supported</td>
<td style="text-align: center;">Encodes polymer connectivity</td>
<td style="text-align: center;">Robust to syntax errors</td>
</tr>
<tr>
<td style="text-align: center;">Limitations</td>
<td style="text-align: center;">Syntactic fragility</td>
<td style="text-align: center;">Still fragile at monomer level</td>
<td style="text-align: center;">Redundancy, longer strings</td>
</tr>
</tbody>
</table>
<p>SMILES Representation for Organic Molecules
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>BigSMILES Supports a Wide Range of Structures
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Fig. 9: Schematic of BigSMILES representations from Lin et al. [212]. Polymers are represented as monomers (repeating units) enclosed within curly brackets; the curly brackets indicate that the molecule is a stochastic object. The monomers are represented as SMILES strings, with additional information expressing the connectivity between monomeric units.
in atmospheric motion, wave equations in seismology [227], and shallow-water equations in oceanography [228]. These models utilize parameterization schemes and regression analysis (least squares, Bayesian inference) to align theoretical predictions with observational data, demonstrating how symbolic representations serve as a bridge between empirical observations and theoretical understanding across scientific disciplines.
4) Structured Data: Structured data in scientific domains refers to information systematically organized through explicit, formal models that enable efficient querying, storage, and computational reasoning. Across disciplines, structured data follows a progression from simple tabular formats to complex knowledge representations. At the foundational level, data tables $T$ consisting of columns $\left{c_{i}\right}<em j="j">{i=1}^{n^{2}}$ and rows $\left{l</em>\right}<em i="i" j="j">{j=1}^{R}$ serve as the basic organizational unit, with each cell $v</em>\right)$ connect columns across tables, enabling complex queries over diverse entities as seen in Ensembl [232] and UniProtKB [233].}$ representing measurements or annotations. These tables, prevalent in resources like GEO [229], dbSNP [230], and weather station datasets such as WEATHER-5K [231], provide straightforward data organization but lack explicit semantics or inter-attribute relationships. Building upon this foundation, relational databases $D=\left{T_{1}, T_{2}, \ldots, T_{N}\right}$ extend tables with schema-level constraints and referential integrity, where foreign key pairs $\left(c_{i}^{(k)}, c_{j}^{(k)</p>
<p>The evolution toward more expressive representations includes ontologies and knowledge graphs that capture domainspecific semantics and relationships. Ontologies formally</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Name</th>
<th style="text-align: center;">Formaldehyde</th>
<th style="text-align: center;">Phenol</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Molecular graph</td>
<td style="text-align: center;">$\stackrel{\mathrm{O}}{\mathrm{H}^{\prime}}$</td>
<td style="text-align: center;">$\stackrel{\mathrm{OH}}{\mathrm{H}^{\prime}}$</td>
</tr>
<tr>
<td style="text-align: center;">SMILES string</td>
<td style="text-align: center;">C=O</td>
<td style="text-align: center;">Oc1ccccc1</td>
</tr>
<tr>
<td style="text-align: center;">SELFIES string</td>
<td style="text-align: center;">$[\mathrm{C}][\times 0]$</td>
<td style="text-align: center;">$[\mathrm{C}][\times \mathrm{C}][\mathrm{C}][\times \mathrm{C}][\mathrm{C}][\times \mathrm{C}][$ Ring $1][$ Branch1 $][0]$</td>
</tr>
<tr>
<td style="text-align: center;">Node identity</td>
<td style="text-align: center;">${\mathrm{C}, \mathrm{O}, \mathrm{H} 1, \mathrm{H} 2}$</td>
<td style="text-align: center;">${\mathrm{C} 1, \mathrm{C} 2, \mathrm{C} 3, \mathrm{C} 4, \mathrm{C} 5, \mathrm{C} 6, \mathrm{O}}$</td>
</tr>
<tr>
<td style="text-align: center;">Adjacency matrix</td>
<td style="text-align: center;">$\left(\begin{array}{llll}0 &amp; 2 &amp; 1 &amp; 1 \ 2 &amp; 0 &amp; 0 &amp; 0 \ 1 &amp; 0 &amp; 0 &amp; 0 \ 1 &amp; 0 &amp; 0 &amp; 0\end{array}\right)$</td>
<td style="text-align: center;">$\begin{gathered} 0100021 \ 1020000 \ 0201000 \ 0010200 \ 0002010 \ 2000100 \ 1000000 \end{gathered}$</td>
</tr>
</tbody>
</table>
<p>Fig. 10: Exemplified symbolic representations (cheminformatics) of formaldehyde and phenol: molecular graph, SMILES and SELFIES string, node identity, and adjacency matrix. Hydrogens are typically omitted in SMILES and SELFIES strings. In the adjacency matrix, edge weights reflect bond types: 1 for single bonds, 2 for double bonds, and 3 for bonds in the aromatic ring.
represent concepts and their relationships using languages like Web Ontology Language [234] or Open Biological and Biomedical Ontologies [235], defining classes, properties, and hierarchies for semantic interoperability and logical inference, exemplified by the Gene Ontology [236] and Human Phenotype Ontology [237]. A knowledge graph is a collection of relational facts $G \subseteq \mathcal{E} \times \mathcal{R} \times \mathcal{E}$, where $\mathcal{E}$ denotes the set of entities and $\mathcal{R}$ the set of semantic relations. By integrating heterogeneous data into a unified semantic representation, knowledge graphs facilitate knowledge reasoning and discovery [238], [239], as exemplified by UMLS [240] and PrimeKG [241]; similarly, CLLMate [242] aligns meteorological records with climate events. Taken together, these developments form a structured data ecosystem supported by standardized exchange formats-including CSV, XML, JSON, YAML, HDF5, ROOT, FITS, and NetCDF-that ensure traceability and interoperability across disciplines. Large-scale repositories have emerged as critical infrastructure, from molecular libraries like ZINC [216] and ChEMBL [217] storing compounds in SMILES format [243], [244], to physics archives like CODATA [245] and particle physics databases [246], astronomical catalogs including SIMBAD [247] and VizieR [248], materials databases such as the Materials Project [70] and</p>
<p><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Fig. 11: Five-channel EEG recording setup and corresponding time series data. Horizontal axis: time (T); Vertical axis: individual EEG channels showing brain electrical activity patterns recorded from scalp electrodes. Figure is adapted from CSBrain [272].</p>
<h2>MatBench [71].</h2>
<p>The sophistication of structured data extends to specialized property datasets that enable targeted scientific investigations. In chemistry, ADMET (Absorption, Distribution, Metabolism, Excretion, and Toxicity) databases [244], [249] provide comprehensive pharmacokinetic properties including absorption (Bioavailability [250], HIA [251]), distribution (BBB [252], FreeSolv [253]), metabolism (Clearance-AstraZeneca [254]), excretion (VDss [62], [255]), and toxicity (ClinTox [219], ToxCast [256], Tox21 [257]) measurements crucial for drug discovery. Similarly, gravitational-wave catalogs like GWTC [65] document events with detailed source parameters in machinereadable formats, while materials databases provide multiproperty coverage including electronic, thermodynamic, and mechanical behaviors computed under standardized protocols. These structured resources leverage persistent identifiers and metadata standards, facilitating rich scholarly analyses through bibliographic knowledge graphs like INSPIRE-HEP [258] and NASA ADS [99], ultimately enabling robust predictive modeling and efficient exploration of vast scientific spaces across all disciplines.
5) Time-Series Data: Time series data, characterized by sequences of temporal data points collected at certain intervals [259]-[261], constitutes a fundamental data modality across scientific disciplines, capturing dynamic phenomena from nanoseconds to decades. These data enable the analysis of temporal patterns, periodicity, and system evolution across vastly different scales-from molecular dynamics tracking atomic positions $\left{\mathbf{X}^{(t)} \in \mathbb{R}^{N \times 3}\right}<em t="0">{t=0}^{T}$, velocities $\left{\mathbf{V}^{(t)} \in\right.$ $\left.\mathbb{R}^{N \times 3}\right}</em>$ in datasets like MD17 [262] and ISO17 [263], [264], to astronomical observations monitoring stellar brightness variations for exoplanet detection in missions like Kepler [265] and Five-hundred-meter Aperture Spherical Telescope (FAST) [266]. The temporal resolution spans milliseconds in neurophysiological recordings such as electroencephalogram (EEG) [267] capturing brain oscillations [268] and event-related potentials [269] (Fig. 11), to hourly meteorological variables in the ERA5 dataset [195] with 0.25 -degree spatial resolution, and continuous seismic waveforms from Incorporate Research Institutions for Seismology [270] and United States Geological Survey networks [271] for earthquake monitoring.}^{T}$, and forces $\left{\mathbf{F}^{(t)} \in \mathbb{R}^{N \times 3}\right}_{t=0}^{T</p>
<p>The diversity of time-series modalities reflects the mul-
<img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Fig. 12: Multi-omics data landscape.
tiscale nature of scientific phenomena. In biological systems, time-series data capture dynamics from molecular-level gene expression patterns revealing temporal responses [273][275] to clinical monitoring through electrocardiogram (ECG) [276] for cardiac rhythm analysis [277], electromyogram (EMG) [278] for muscle activity [279], and continuous glucose monitoring [280], [281]. Neuroimaging modalities provide complementary temporal and spatial resolutions: functional magnetic resonance imaging (fMRI) detects blood-oxygen-level-dependent (BOLD) signals [282] for mapping brain networks [283], while magnetoencephalography (MEG) measures magnetic fields from neuronal activity [284], [285]. In chemistry, molecular spectrum data mainly include Raman, infrared (IR), ultraviolet (UV), ${ }^{1} \mathrm{H}$ nuclear magnetic resonance (NMR), and ${ }^{13} \mathrm{C}$ NMR spectroscopy [286], revealing structural and compositional information enabling AIdriven representation learning [140]. Physics leverages highfrequency strain data from LIGO/Virgo at $16,384 \mathrm{~Hz}$ for gravitational wave detection [65], while SDO [287] provides Atmospheric Imaging Assembly Extreme Ultraviolet images every 12 seconds and Helioseismic and Magnetic Imager (HMI) vector-magnetogram-derived Space-weather HMI Active Region Patches features at 12-minute cadence to forecast space weather [288].</p>
<p>These temporal datasets serve critical roles in understanding system dynamics, enabling predictive modeling, and monitoring critical events. Longitudinal clinical studies utilize serial MRI, CT, and clinical report data [289] to model disease trajectories [290], [291], while synoptic astronomical surveys like The Zwicky Transient Facility [292] and Legacy Survey of Space and Time [293] generate calibrated image sequences for transient detection. Earth science integrates atmospheric data from WeatherBench [196] and WEATHER-5K [231], oceanic measurements from the Hybrid Coordinate Ocean Model (HYCOM) [294] and NOAA Tides [295], and geophysical recordings for comprehensive Earth system monitoring. The standardization of these diverse time-series formats facilitates cross-disciplinary AI applications [296]-[298], establishing time-series analysis as a cornerstone methodology for extracting insights from dynamic scientific phenomena across all scales.
6) Multi-omics Integration: Driven by rapid advances in high-throughput technologies, multi-omics has emerged as a powerful approach for capturing the complexity of living</p>
<p><img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Fig. 13: Symbolic representations and 3D structure visualizations across different scientific domains: DNA, RNA and Protein. The DNA structure is split into chain I and chain J from PDB 1KX5 [299] and visualized by UCSF Chimera [300]. The RNA structure is from the RNAsolo with ID 7ELQ [301], [302]. The protein snapshot is from the PDB bank with ID 7CAM [303]. The DNA and protein are adapted from NatureLM [43].
systems through the integrated analysis of multiple layers of biological data [61]. As illustrated in Fig. 12, the multiomics landscape encompasses seven major data modalities: genomics (capturing genetic sequences and variations), epigenomics (mapping regulatory modifications), transcriptomics (profiling gene expression), proteomics (analyzing protein abundance and function), metabolomics (measuring small molecule metabolites), microbiome (characterizing microbial communities and their functions/interactions), and exposome (tracking environmental effects). These omics layers are interconnected through biological processes, from transcription and translation at the molecular level to environmental interactions at the systems level, offering complementary insights that together enable a more comprehensive understanding of biological processes than any single layer alone [304], [305]. At the molecular core of this framework, biological information flows from DNA to RNA to proteins, with each biomolecule existing in both symbolic sequence representations and threedimensional structural forms (Fig. 13).</p>
<p>Multi-omics technologies have continued to advance, offering improved resolution, accuracy, and scalability, along with enhanced methods for integrating data across different biological domains [62], [306]-[308]. As a result, multi-omics has emerged as a cornerstone of modern scientific research, providing deeper insights into the molecular mechanisms underlying health and disease, unraveling complex regulatory networks, and driving data-informed discoveries across diverse biological domains [309].</p>
<p>Genomics encompasses a vast and evolving ecosystem of structured, symbolic and sequence-based representations. (i) Reference genomes, such as those hosted by Ensembl [310] and UCSC Genome Browser [311], provide curated nucleotide sequences and annotated genomic elements across thousands of species. (ii) Genetic variation, arising from differences in DNA sequences across individuals or populations, is a central focus of genomics. Population-scale resources such
as GWAS Catalog [312], dbSNP [230] and gnomAD [313] catalog common and rare variants, providing estimates of allele frequencies across diverse cohorts, while ClinVar connects specific variants to clinical phenotypes and pathogenicity interpretations [314]. (iii) Functional genomics maps, such as those from ENCODE and Roadmap Epigenomics [315], layer chromatin accessibility, histone marks, DNA methylation, and transcription factor binding profiles onto the genome to reveal regulatory landscapes. (iv) Spatial genome resources [316], [317], including Hi-C datasets and 3D genome browsers, reconstruct chromatin topology to explore long-range regulatory interactions. Genomic data are inherently symbolic and sequential, with rich metadata and controlled vocabularies [237]-features that make them well-suited for conversion into prompt-based representations for language models [318], [319]. Emerging methods already leverage large-scale variant catalogs [313] and knowledge graphs [241] to train foundation models for genotype-phenotype reasoning, while multiresolution integration with imaging or epigenetics supports causal inference at cellular and organismal scales.</p>
<p>Transcriptomics captures the dynamic and context-specific landscape of gene expression, linking genome to phenotype in time and space. Its data ecosystem spans multiple layers that together provide a comprehensive view of transcriptional activity. (i) Transcript annotations from sources like GENCODE [320] and RefSeq [321] define exon-intron structures, splice variants, and isoform-level expression. (ii) At the foundational level, bulk RNA-seq and single-cell RNA-seq repositories such as GEO [229], and ArrayExpress [322] house millions of transcriptomic profiles across tissues, conditions, and perturbations. (iii) Expression atlases, such as the Human Cell Atlas or GTEx [323], enable comparative and tissuespecific analyses of transcriptional activity. (iv) Spatial transcriptomics platforms, including 10x Genomics Visium [324], Slide-seq [325], and Stereo-seq [326], link gene expression profiles to precise tissue coordinates, enabling spatially resolved analyses of cell-cell interactions, microenvironmental heterogeneity, and histopathological context. Public repositories like SpatialDB [327] aggregate thousands of such datasets across diverse species and conditions, facilitating cross-study comparisons and integration with histology images. (v) Gene co-expression networks, such as STRING [328] co-expression edges, provide functional grouping of genes based on correlated activity. These transcriptomic resources form a rich, structured, and temporally resolved representation of cellular states, readily convertible into graph-, token-, or prompt-based formats for integration with other omics layers in large-scale modeling.</p>
<p>Proteomics is often described as multimodal, but, strictly speaking, the field rarely couples images with free text in the way vision-language benchmarks do. Instead, it juggles molecular representations drawn from distinct information channels: (i) structured knowledge bases such as UniProtKB deliver expertly curated sequences, domains and post-translational modifications for more than 250 million proteins [233], among them, the reviewed subset UniProtKB/Swiss-Prot ( 0.57 million entries, as of August 2025) is the most widely used; while the Protein Data Bank (PDB) stores atomic coordinates</p>
<p>for experimentally determined folds [329] ; (ii) interaction networks fuse biochemical and genetic evidence-STRING merges literature, co-expression and synteny to build genomewide association graphs [330], whereas BioGRID [331] and IntAct [332] record bench-validated contacts; (iii) symbolic ontologies provide a shared semantic layer, with the Gene Ontology defining controlled terms for function, process and localization [333]; (iv) image resources such as the Human Protein Atlas place thousands of proteins into tissue and cellular context by immunohistochemistry and fluorescence microscopy [334]; (v) computational structure repositories, notably the AlphaFold Protein Structure Database, extend empirical coverage with high-confidence models for millions of previously unsolved proteins [335]; and (vi) time-resolved quantitative datasets from mass-spectrometry pipelines are shared through the ProteomeXchange consortium [336], with PRIDE as its flagship archive [337]. Seamlessly combining these heterogeneous modalities yields synergistic insight, e.g., PDB experimental structures and AlphaFold DB predicted models (surfaced via PDBe-KB) jointly constrain interaction graphs from STRING, BioGRID, and IntAct; ontology-aware statistics translate large-scale microscopy screens into testable biological hypotheses; and longitudinal mass spectrometry experiments connect dynamic post-translational regulation to spatial relocalization inferred from imaging. Although corpora already formatted as dialogue for LLM training remain scarce, the underlying repositories constitute machine-readable graphs, tables and sequences that can be converted into textual prompts or retrieval-augmented contexts with minimal templating. Emerging pipelines therefore marry graph databases with transformer representation learning, reconcile identifiers across formats, and propagate uncertainty, all under FAIR standards [338] (Findable, Accessible, Interoperable, Reusable) such as MIAPE [339] and ProteomeXchange-XML [336]. As these resources expand and model architectures mature, a genuinely integrative, causally grounded "digital proteome" becomes feasible, where each protein is simultaneously encoded as sequence, structure, dynamic profile, network node and spatial image, ready for LLM-driven reasoning across the molecular landscape.</p>
<p>Beyond the molecular central dogma, additional omics layers provide complementary biochemical and environmental perspectives. Metabolomics profiles small-molecule metabolites to capture biochemical activity and phenotypic state, with repositories such as the Human Metabolome Database [340] and MetaboLights [341] supporting pathway-level integration with other omics. Microbiome studies characterize the composition and functional potential of microbial communities through metagenomic and metatranscriptomic sequencing, with resources like the Human Microbiome Project [342] and MGnify [343] enabling host-microbe interaction analyses. Exposome research examines the totality of environmental exposures, including diet, pollutants, and lifestyle factors, using chemical assays, wearable sensors, and curated biomarker databases such as Exposome-Explorer [344]. These layers extend multi-omics frameworks by linking molecular phenotypes to ecological and environmental contexts.</p>
<p>From precision medicine and cancer research to environ-
mental science and agriculture, multi-omics data now empower researchers to tackle complex, interdisciplinary problems and generate holistic models of biological and ecological systems [345]-[347].</p>
<h2>B. Hierarchical Structure of Scientific Knowledge</h2>
<p>Scientific knowledge fundamentally differs from a flat collection of information. Instead, it manifests as a sophisticated hierarchical system that mirrors the progressive nature of human cognition and the evolutionary path of scientific discovery from phenomena to essence, from the concrete to the abstract. This inherent stratification resonates with established knowledge hierarchy models, most notably the DIKW (Data-Information-Knowledge-Wisdom) pyramid articulated by Ackoff [348] and systematically analyzed by Rowley [349], which posits that knowledge emerges through qualitative transformations rather than mere accumulation. However, as Zeleny [350] observed in mapping knowledge forms from "knownothing" through "know-what" and "know-how" to "knowwhy," scientific inquiry demands a more nuanced taxonomy that captures both procedural and explanatory dimensions. Building upon these theoretical foundations while addressing the unique epistemological requirements of scientific practice, we propose a five-tiered framework encompassing factual, theoretical, methodological-technological, modeling-simulation, and insight levels. This stratification reflects what Baskarada and Koronios [351] characterize as the need to contextualize knowledge hierarchies within specific domains, incorporating the computational and instrumental dimensions essential to contemporary science. Each level represents not merely a repository of information but a distinct mode of understanding, exhibiting emergent properties that reflect the transformative nature of scientific knowledge construction. The following sections will systematically examine each stratum, revealing how this hierarchical architecture facilitates both the organization of existing knowledge and the generation of novel scientific insights.</p>
<p>To this end, we organize this subsection into five interconnected components, each representing a distinct level of scientific knowledge, as shown in Fig. 14. These levels include: the Factual Level (Sec. II-B1), the Theoretical Level (Sec. II-B2), the Methodological and Technological Level (Sec. II-B3), the Modeling and Simulation Level (Sec. II-B4), and the Insight Level (Sec. II-B5). In addition, we discuss Dynamic Interactions and Evolution (Sec. II-B6) which highlights the iterative feedback loops across levels that collectively drive scientific progress. Finally, we conclude this subsection with the implication of such hierarchy (Sec. II-B7), which not only underscores the progressive deepening from data to discovery but also provides a structured foundation for developing SciLLMs that can effectively capture and utilize the multifaceted nature of scientific data.</p>
<p>1) Factual Level: At the foundation of scientific knowledge lies the factual level-direct observational data, experimental measurements, and empirical evidence that constitute our primary interface with the physical world. This raw, unprocessed information serves as the bedrock for all subsequent scientific understanding.</p>
<p><img alt="img-13.jpeg" src="img-13.jpeg" /></p>
<p>Fig. 14: Hierarchical structure of scientific knowledge. The framework comprises five levels: factual (raw data), theoretical (laws and principles), methodological/technological (methods and tools), modeling/simulation (computational models), and insight (discoveries). The bottom panel illustrates the iterative cycle linking these levels through data collection, pattern recognition, hypothesis testing, and theory development.</p>
<p>Factual data is characterized by its objectivity and minimal human intervention. When astronomers collect astronomical imaging data, such as multi-band images [352], and additional light curves and spectra from distant galaxies, particle physicists capture collision events at the Large Hadron Collider [72], gravitational-wave detectors measure strain signals [353], or biologists sequence genetic material [354], they obtain direct representations of nature's state. Despite instrumental limitations, these data fundamentally reflect objective reality independent of theoretical frameworks.</p>
<p>Modern experiments generate data of unprecedented dimensionality and structural complexity. High-energy physics experiments like A Toroidal LHC Apparatus (ATLAS) and Compact Muon Solenoid (CMS) produce order-of-tens of terabytes of collision data per second [72], while LIGO and Virgo release strain data sampled at $16,384 \mathrm{~Hz}$ [65]. This heterogeneity spans all domains: multi-channel neural recordings capture brain dynamics at millisecond resolution [355], single-cell RNA sequencing reveals cellular heterogeneity with millions of transcripts [356], multi-omics platforms integrate genomic, proteomic, and metabolomic data [61], agricultural sensors monitor crop phenotypes across spatial and temporal scales [357], and Earth observation satellites generate multispectral imagery for climate monitoring [358].</p>
<p>Critical to scientific data is its spatiotemporal context. Astronomical observations acquire meaning only when anchored by precise coordinates and timestamps, enabling crossinstrument calibration and transient detection. Self-supervised models that jointly encode images, spectra, and light curves demonstrate that meaningful representations emerge through multimodal fusion [359]. Similarly, seismic wave arrivals at distributed stations enable earthquake triangulation and Earth structure probing [360], [361], while drug discovery relies on temporal pharmacokinetic profiles [362] and agricultural yield predictions depend on phenological timing [363]. In the field
of Earth science, the spatiotemporal characteristics of data are particularly prominent. This is primarily reflected in the fact that spatial scales of Earth science data often need to be mapped to specific geographic resolutions. For example, in [364], global meteorological variables are represented using a $128 \times 256$ tensor, providing a spatial discretization suitable for modeling over the entire globe. Regarding temporal resolution, different tasks require data at distinct time intervals. For some short-term nowcasting tasks [365], [366], data are typically recorded at 10-minute intervals, enabling the capture of rapidly evolving atmospheric phenomena. In contrast, for medium-range forecasting tasks [367], [368], data are usually sampled every 6 hours to balance data volume with the relevant timescales for prediction.</p>
<p>Inherent uncertainties and noise are integral to factual data. Quantum experiments face fundamental measurement limits [77], biological studies contend with individual variation and technical noise [78], astronomical observations are severely degraded by atmospheric turbulence [369], and clinical trials must account for patient heterogeneity [370]. These uncertainties inform confidence bounds and guide robust analytical methods across all scientific disciplines.
2) Theoretical Level: The theoretical level transcends empirical observations through diverse forms of abstraction and formalization. Beyond mathematical equations such as Newton's mechanics [371], Maxwell's electromagnetism [372], Schrdinger's quantum mechanics [373], and Hodgkin-Huxley neural dynamics [374], scientific theories employ multiple representational frameworks.</p>
<p>Conceptual models capture fundamental principles: the central dogma in molecular biology [375], plate tectonics in geoscience [376], and the Standard Model in particle physics [377]. Classification systems organize knowledge hierarchically: Linnaean taxonomy [378], the periodic table [379], Gene Ontology [236], and astronomical object catalogs [248]. Network representations reveal systemic relationships: protein interaction networks [380], metabolic pathways [381], ecological food webs [382], and brain connectomes [383]. Computational models bridge theory and prediction: climate circulation models [384], molecular dynamics simulations [385], population genetics algorithms [386], and pharmacokinetic compartmental models [387]. Statistical frameworks quantify uncertainty: Bayesian inference in phylogenetics [388], machine learning in multi-omics integration [61], and cosmological parameter estimation [389].</p>
<p>These diverse theoretical representations exhibit hierarchical organization and domain-specific validity. Mathematical formalisms enable precise predictions; conceptual models provide intuitive understanding; classification systems facilitate knowledge organization; network models reveal emergent properties; computational approaches handle complexity. Together, they transform raw data into actionable scientific knowledge, creating a multi-layered theoretical infrastructure that supports discovery, prediction, and technological innovation across disciplines [67].
3) Methodological and Technological Level: Between raw facts and abstract theories lies a crucial intermediate layer of methods and tools that transform theoretical predictions into</p>
<p>testable hypotheses and raw data into theoretical insights.
Scientific methodology has evolved from simple comparative studies to sophisticated experimental designs across disciplines. Revolutionary techniques open new frontiers: CRISPRCas9 enables precise genomic editing [390], ultracold atom Bose-Einstein condensation paved the way for quantum simulation [391], and high-throughput sequencing enables multiomics profiling [61].</p>
<p>Computational methods bridge theory and experiment. Monte Carlo algorithms [392] underpin simulations from protein folding to climate modeling. Machine learning extracts patterns from massive datasets, e.g., AlphaFold [393] predicts protein structures, while algorithms identify astronomical objects and reconstruct neural circuits [394]. Statistical frameworks ensure rigorous inference: particle physics commonly adopts a five-sigma threshold for discovery [395], while Bayesian approaches provide principled uncertainty quantification across fields [79].</p>
<p>Instrumental technologies extend observation into new realms. From Ruska's electron microscope [396] to modern cryo-electron microscopes (cryo-EM), from LIGO's detection of $10^{-21}$-level spacetime strains [65] to single-cell sequencing [397], these tools fundamentally alter what questions we can ask. This creates feedback loops where better instruments enable deeper theories, which guide development of more sophisticated technologies.
4) Modeling and Simulation Level: This level involves utilizing numerical simulations to replicate complex systems. Virtual experiments enable researchers to test hypotheses and predict phenomena otherwise difficult or costly to study.</p>
<p>Contemporary modeling emphasizes multi-scale integration. Materials science connects quantum calculations at atomic scales to macro-level material behaviors [76]. Climate modeling integrates short-term atmospheric processes with longterm ocean dynamics, bridging local weather and global climate change [398]. Astronomy links transient events like supernovae to long-term galaxy evolution spanning billions of years [399]. Physics-informed neural networks merge physical laws and data-driven approaches, enabling effective data-physics fusion for fluid dynamics simulations with notable demonstrations from aerospace to biomedical applications [400], [401]. Life sciences employ multi-scale models to explore molecular interactions and biological systems [402]. Computational simulations accelerate drug discovery by predicting molecular interactions [403]. Multi-omics approaches integrate genomic, proteomic, and metabolomic data to decipher disease mechanisms and guide personalized treatment [61]. Neuroscience simulations range from synaptic processes to brain-wide activity [404], while agronomic models forecast crop performance under varying environmental conditions [405]. Rigorous verification and validation processes ensure model reliability, confirming computational accuracy and predictive validity against experimental data, which is critical in nuclear engineering, aerospace, and medical certifications [406].</p>
<p>Thus, the modeling and simulation level serves as a foundational tool, supporting modern scientific exploration and informed decision-making.
5) Insight Level: At the apex of the scientific hierarchy, the insight level represents transformative moments when disparate knowledge coalesces into revolutionary understanding. Cross-disciplinary fusion has repeatedly catalyzed such breakthroughs: Shannon's information theory meeting molecular biology birthed bioinformatics, revealing life as an information processing system [407], [408]; neuroscience converging with physics produced brain imaging technologies that decode neural activity patterns [409]; astronomical spectroscopy combined with quantum mechanics unveiled stellar nucleosynthesis, explaining element formation across the cosmos [410]. These interdisciplinary insights demand intellectual flexibility to recognize patterns across traditional boundaries, from protein folding dynamics mirroring energy landscape theory in physics [411], to agricultural genomics borrowing population genetics models to enhance crop resilience [412].</p>
<p>Scientific revolutions often emerge from careful attention to anomalies that challenge existing frameworks. Classical physics predicted unbounded ultraviolet radiance at short wavelengths under the Rayleigh-Jeans law; Planck's quantization of energy in 1900 resolved this "ultraviolet catastrophe" and birthed quantum theory [413]. Similarly, the discovery of reverse transcriptase shattered the central dogma of molecular biology [414], while anomalous galactic rotation curves revealed dark matter's existence [415]. In pharmacology, unexpected drug side effects have led to therapeutic breakthroughs: sildenafil's transition from angina treatment to erectile dysfunction exemplifies serendipitous discovery through anomaly recognition [416]. True conceptual innovation transcends problem-solving to introduce novel frameworks: Darwin's natural selection fundamentally altered our view of life's relationship to time [417]; plate tectonics unified previously disparate geological phenomena [418]; systems biology's emergence revealed that biological function arises from network interactions rather than isolated components [402].</p>
<p>In the era of multi-omics and big data, extracting genuine insight requires navigating information overload through humanAI collaboration. Machine learning excels at pattern recognition across genomic, proteomic, and metabolomic datasets, uncovering disease signatures invisible to traditional analysis [61]. Yet human judgment remains essential for distinguishing correlation from causation, contextualizing discoveries within theoretical frameworks, and recognizing which patterns reflect fundamental principles. The future of scientific insight lies in this synergy, where computational power amplifies human creativity to reveal nature's hidden connections across scales from quantum to cosmic, from molecular to ecological.
6) Dynamic Interactions and Evolution: Scientific progress emerges from dynamic interactions between hierarchical levels of knowledge, creating intricate feedback loops that drive discovery forward. This process manifests through three primary mechanisms: bottom-up induction, top-down deduction, and horizontal method transfer.</p>
<p>Inductive processes transform observations into theoretical understanding across disciplines. In astronomy, Kepler's analysis of Brahe's observations yielded planetary motion laws, later unified by Newton's gravitational theory. Modern</p>
<p>life sciences follow similar trajectories: genomic sequencing reveals patterns explained through molecular and evolutionary models; neuroimaging data drives theories of brain function; agricultural field trials inform crop optimization strategies; and multi-omics integration uncovers systems-level biological principles. In physics, deduction channels theoretical insights into experimental design. Einstein's 1916 prediction of gravitational waves guided decades of detector development, culminating in LIGO and Virgo's detection of spacetime strains ( $\sim 10^{-21} \mathrm{~m}$ ) from binary black-hole mergers in 2015, confirming century-old predictions and inaugurating gravitationalwave astronomy [419].</p>
<p>Horizontal method transfer catalyzes unexpected advances. X-ray crystallography transitioned from mineralogy to revealing biomolecular structures; machine learning algorithms developed for image recognition now predict protein folding and drug-target interactions; network analysis from sociology illuminates ecological interactions and neural connectivity; spectroscopic techniques from physics enable remote sensing in Earth science and metabolomics profiling. This evolution follows a spiral pattern where theories transcend and include predecessors, i.e., classical mechanics subsumed within relativity and quantum mechanics, Mendelian genetics integrated with molecular biology, revealing why earlier frameworks succeeded within their domains while pointing toward a more comprehensive understanding. Such dynamic interactions are essential for developing AI systems that capture science's creative essence beyond pattern matching.
7) Implications for Sci-LLMs: This hierarchical framework carries profound implications for the development and deployment of Sci-LLMs. Each level offers distinct computational challenges and opportunities for language model integration. At the factual level, LLMs must learn to parse heterogeneous data formats, extract patterns from high-dimensional observations, and maintain spatiotemporal context, which is essential for tasks like automated literature mining and experimental data interpretation. The theoretical level demands that models internalize mathematical formalisms, causal relationships, and domain-specific ontologies, enabling them to reason about scientific laws and generate testable hypotheses. The methodological level requires LLMs to understand experimental protocols, computational workflows, and instrumental constraints, facilitating automated experiment design and method recommendation. At the modeling and simulation level, language models can serve as interfaces between natural language queries and complex computational engines, translating scientific questions into simulation parameters and interpreting results. Finally, the insight level challenges LLMs to perform cross-domain synthesis and creative hypothesis generation, capabilities that emerge from training on the full spectrum of scientific knowledge rather than isolated datasets. By incorporating data from all five levels, Sci-LLMs can transcend simple information retrieval to become active participants in the scientific discovery process, bridging human intuition with computational power.</p>
<h2>C. Key Challenges in Scientific AI</h2>
<p>In the field of scientific AI, especially within LLMs and MLLMs, several key challenges must be addressed to enable meaningful scientific understanding and reasoning. These challenges include interpretability (Sec. II-C1), cross-scale and multimodal integration (Sec. II-C2), as well as dynamic knowledge evolvement (Sec. II-C3), all of which are essential for enhancing the effectiveness of these models in scientific applications.</p>
<p>1) Interpretability in Scientific AI: Interpretability remains a major bottleneck. Scientific reasoning is inherently logical, based on clear explanations and justifications. However, LLMs and MLLMs are typically perceived as "black-box" models, making it difficult to understand the rationale behind a model's reasoning or output. This challenge is particularly acute in scientific domains, where understanding the "why" and "how" behind an answer is just as important as the answer itself. Interpretability is crucial for building trust in Sci-LLMs, especially in high-stakes fields such as drug discovery and climate modeling. In LLM/MLLM area, prompting or training the model with chain-of-thought (CoT) [420], [421] emerges as an effective technique to elicit explicit, natural-language reasoning capability of LLMs. CoT enables the model to write a step-by-step reasoning trace, breaking down complex tasks before giving the final answer. This makes the reasoning path more transparent and provides clearer insights into its decision-making. The recent work, BioReason [422], introduces this multi-step reasoning strategy into DNA foundation models, enabling deep, interpretable biological reasoning from complex genomic data. By integrating a DNA foundation model with an LLM and constructing a biological CoT, BioReason empowers the LLM to directly process and reason with genomic information, fostering multimodal biological understanding. Through reinforcement learning, the model refines its multi-step reasoning capabilities, leading to biologically coherent deductions and outperforming traditional singlemodality models on biological reasoning benchmarks. Overall, conducting CoT reasoning in scientific AI models is particularly challenging due to the complexity and domain-specific nature of scientific knowledge. Unlike generalist models, scientific reasoning involves hypothesis-driven logic grounded in empirical evidence, requiring a precise understanding across disciplines such as biology, chemistry, and physics. Therefore, more work is needed to develop transparent models that can offer both scientific accuracy and explainable reasoning.
2) Cross-scale and Multimodal Integration: Another major hurdle in the application of LLMs and MLLMs to scientific reasoning is their ability to handle cross-scale and multimodal integration. Scientific data is often characterized by hierarchical structures that span multiple scales, from microscopic phenomena (e.g., molecular dynamics in chemistry) to macroscopic phenomena (e.g., weather patterns or ecosystem behavior). For example, in computational biology, understanding the behavior of a cell involves integrating data from individual molecules to entire tissues, which can require models to simultaneously process both fine-grained details and large-scale systems. Traditional LLMs excel at processing textual data but</p>
<p>struggle to model spatiotemporal dependencies across scales. Moreover, scientific reasoning frequently involves multimodal data, typically combining text, images, numerical data, and experimental results. This requires models to seamlessly integrate heterogeneous data sources [73], [74]. The challenge is further exacerbated when the information comes from different experimental setups or different measurement modalities, each requiring tailored processing pipelines that preserve important domain-specific features. For instance, bioinformatics deals with an extensive variety of data, including DNA, RNA, protein sequences, and drug molecules [63]. MLLMs have the potential to address this complexity by integrating text, images, audio, and other modalities. They offer promising opportunities to enhance scientific understanding by connecting disparate data points and inferring relationships across these varied modalities. Initiatives such as the National Institutes of Health's "Advancing Health Research through Multimodal AI" [423] exemplify this trend, aiming to develop data-driven multimodal AI approaches to model, interpret, and predict complex biological, behavioral, and health systems. However, significant challenges persist in achieving seamless multimodal integration. MLLMs frequently struggle with complex multimodal and multi-step reasoning tasks, often relying on shallow multimodal cues or defaulting to text-dominant reasoning rather than truly integrated understanding. A major bottleneck in their development is the scarcity of appropriate, high-quality multimodal scientific datasets.</p>
<p>To address these challenges, models need to move beyond isolated data streams and embrace a holistic integration of cross-scale and multimodal information to create truly unified frameworks that can seamlessly integrate complex scientific data and perform rigor scientific reasoning.
3) Dynamic Knowledge Evolvement: One of the most prominent challenges in applying LLMs and MLLMs to scientific domains is ensuring knowledge update and evolvement. In scientific research, knowledge evolves dynamically, with new discoveries constantly challenging existing theories. This makes it difficult for models trained on static datasets to maintain consistency with the most current body of scientific knowledge. Models that fail to continuously update their knowledge bases risk generating outdated or conflicting information, which can undermine their utility in domains like medical research, physics, or environmental science. To fix this, we need to explore new methods like automated knowledge injection and model adaptation. These approaches would allow models to continuously integrate new research findings, ensuring they remain coherent and aligned with the rapidly changing world of scientific discovery.</p>
<h2>D. Quality Standards for Scientific Datasets</h2>
<p>Assessing the quality of scientific data is essential for developing robust scientific AI models. In this subsection, we outline four complementary dimensions that together characterize data quality in scientific contexts. First, accuracy (Sec. II-D1) assesses how faithfully data represent the underlying phenomena. Second, completeness (Sec. II-D2) concerns the extent to which datasets capture all relevant elements
across content, structure, and temporal coverage. Third, timeliness (Sec. II-D3) measures the update frequency and responsiveness of datasets to real-world changes. Finally, traceability (Sec. II-D4) ensures transparency and reproducibility by documenting provenance, metadata, and version histories. Together, these aspects provide a systematic framework for evaluating the reliability, usability, and long-term value of scientific datasets, standardizing data management practices and guiding optimal AI deployment.</p>
<p>1) Accuracy: Accuracy is one of the fundamental dimensions of scientific data quality, reflecting how closely data represent the real world in terms of spatial positioning, temporal annotation, and signal fidelity. High-accuracy data not only enhances the training efficiency and inference precision of AI models, but also directly impact the credibility of scientific conclusions. For example, in geospatial datasets, Landsat 8 satellite imagery, after ground control point correction, achieves a geolocation error of 15 to 30 meters, indicating high spatial precision [424]. In contrast, location information from some social media platforms is often only annotated at the city level, offering coarse granularity that hinders fine-grained modeling [425]. In the physical sciences, the Materials Project provides data generated via first-principles calculations, controlling model errors, and ensuring reliable accuracy in band structure and lattice constants [70]. Common methods for assessing accuracy include mean squared error (MSE), root mean square error (RMSE), temporal alignment deviation, and signal-to-noise ratio (SNR), typically quantified by comparing with ground truth or high-quality benchmark datasets [426], [427].
2) Completeness: Completeness refers to the extent to which a scientific data set adequately covers content, structural fields, and temporal span, whether it contains all the data elements that should have been collected. It serves as a foundation for systematic and logical data analysis. In genomics, completeness is often evaluated by sequencing depth; coverage below $10 \times$ is generally insufficient to accurately detect mutations, and modern whole genome sequencing standards typically require an average coverage of $30 \times$ or more [428], [429]. In the field of materials science, data integrity directly determines the success or failure of data-driven discovery of new materials [430]. Methods for assessing completeness include missing value statistics, field coverage analysis, breakpoint detection, and time series gap identification. For example, in Earth science, SCDNA [431] filled in missing data for precipitation, minimum temperature, and maximum temperature to ensure the data integrity across all weather stations, which improved the accuracy of spatial interpolation. Tools such as OpenRefine [432] and DataCleaner [433] can automatically detect missing entries, structural anomalies, and null fields, thus improving the overall quality of datasets.
3) Timeliness: Timeliness measures data update frequency, the latency between data collection and release, and the speed at which data respond to real-world changes. This is crucial for applications like emergency response, trend forecasting, and dynamic modeling. For instance, during the COVID-19 pandemic, the Johns Hopkins University dataset was released at daily intervals, enabling rapid epidemic mod-</p>
<p>eling and policy decision-making on a global scale [434]. In remote sensing, NASA's MODIS satellite products are updated daily, supporting timely environmental monitoring and disaster assessment [435]. In contrast, traditional datasets like ImageNet [436] and MNIST have not been updated for years, making them suitable for algorithm benchmarking but less relevant for contemporary applications. Meanwhile, open knowledge bases like Wikidata allow real-time user editing and provide API-based updates, representing a higher level of "interactive timeliness" [437]. Timeliness can be systematically quantified using indicators such as collection-to-release time lag, average update interval, event response delay, and timestamp consistency [426], [438].
4) Traceability: Data traceability refers to the ability to track the complete journey of data from its origin and transformations to its final use. Traceability has increasingly become a critical supplementary metric for evaluating scientific data security and trustworthiness, especially in the context of open science and data reuse. Highly traceable data should include complete metadata, change logs, version control records, and accountability information, meeting the "Findability" and "Reusability" criteria of the FAIR principles [338]. For example, each record on the OpenAIRE platform [439] includes a unique DOI, data acquisition description, and license details, significantly enhancing verifiability and reuse credibility. Moderately traceable data may provide basic metadata but often lack processing chains, revision histories, or algorithmic documentation, limiting users' ability to assess reliability. Low-traceability data typically lack source documentation and coherent annotation, rendering them difficult to verify. For instance, web-scraped research images or code snippets without provenance or revision records pose considerable risks in academic usage [440]. Recently, technologies such as blockchain and cryptographic hash signatures are being explored to build traceability chains and verifiable records for scientific data [441].</p>
<h2>E. Dimensions for Evaluating Scientific AI</h2>
<p>General-purpose LLM benchmarks primarily assess core natural language processing and general reasoning abilities. Key evaluation dimensions typically include language understanding, fluency, factual knowledge recall, reasoning and problem-solving. These benchmarks are designed to evaluate broad linguistic competence and general cognitive skills across everyday or non-specialized domains. Even when covering technical subjects (e.g., STEM topics in MMLU), they often assume only basic computational skills and high school-level science knowledge. Evaluations of factuality and alignment are typically grounded in general content. In contrast, sciencefocused LLM benchmarks require mastery of the depth, precision, and rigor characteristic of academic research. Beyond the general dimensions listed above, scientific LLMs must be evaluated on their ability to engage with domain-specific scientific knowledge, reason with formal systems (e.g., equations, symbolic logic), retrieve and synthesize scholarly information, and support hypothesis generation or experimental design.</p>
<p>1) Expert-Level Scientific Knowledge Comprehension and Retrieval: Unlike general-purpose language models, scientific AI models must retrieve, comprehend, and apply cutting-edge research knowledge across diverse scientific disciplines with domain-level expertise. This knowledge extends beyond general encyclopedic facts to include domain-specific equations, physical constants, technical terminology, and theoretical constructs. A model's ability to access, interpret, and reason over external academic knowledge is a critical dimension of evaluation, serving as a cornerstone for enabling automated scientific discovery. Key evaluation aspects include information retrieval, literature-based fact verification, and the integration of heterogeneous scientific knowledge. For example, SciBench [442] introduces benchmark tasks requiring the retrieval of mathematical equations, chemical laws, and physical theorems; SciKnowEval [443] spans domains from biology to materials science, assessing tasks such as molecule identification and reaction prediction; and SciQA [108] leverages the Open Research Knowledge Graph to support complex crossdomain scientific questions. This dimension challenges models on both the breadth and depth of scientific understanding, emphasizing accuracy, completeness, and the ability to engage with knowledge beyond surface-level facts.
2) Scientific Reasoning and Problem Solving: Scientific problems often require multi-step reasoning rooted in the principles of the scientific method. Effective models must be capable of formulating and decomposing complex problems, applying relevant scientific laws and theories, and performing precise numerical computations. SFE [444], for example, emphasizes advanced reasoning skills of Sci-LLMs, including the evaluation of scientific attribute understanding and comparative analysis. Error analyses of science-focused benchmarks reveal that key reasoning capabilities include logical decomposition, causal inference, deductive problem solving, and abstract reasoning. These tasks extend beyond the scope of general mathematical puzzles found in standard LLM benchmarks, demanding the ability to reason about experimental procedures, derive theoretical formulas, and interpret results within a scientific framework.
3) Multimodal Scientific Data: Science AI models should incorporate various modalities other than language. The ability to understand data diagrams, including figures and tables, and to conduct quantitative and statistical analysis to identify scientific trends, is crucial. Furthermore, expert AI models need to comprehend specialized scientific data that requires domain-specific knowledge, such as chemical structures and laboratory images, for high-level reasoning. SciBench [442] notably includes a multimodal subset with figures and graphs, highlighting that assessing the ability to interpret visual scientific information is a dimension beyond typical LLMs and even MLLMs. On the other hand, it remains to be seen whether current science AI models can fully incorporate and leverage all these diverse data types effectively for truly advanced scientific discovery.</p>
<h2>III. SCIENTIFIC LARGE LANGUAGE MODELS</h2>
<p>Sci-LLMs are emerging as powerful tools for modeling, understanding, and reasoning across diverse scientific domains.</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-290 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-290</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-290</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-14.html">extraction-schema-14</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <p><strong>Paper ID:</strong> paper-932b6353204e56f20917edadda2fa636ace21090</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/932b6353204e56f20917edadda2fa636ace21090" target="_blank">Sub-Task Decomposition Enables Learning in Sequence to Sequence Tasks</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> It is shown that when concatenating intermediate supervision to the input and training a sequence-to-sequence model on this modified input, unlearnable composite problems can become learnable.</p>
                <p><strong>Paper Abstract:</strong> The field of Natural Language Processing has experienced a dramatic leap in capabilities with the recent introduction of huge Language Models. Despite this success, natural language problems that involve several compounded steps are still practically unlearnable, even by the largest LMs. This complies with experimental failures for end-to-end learning of composite problems that were demonstrated in a variety of domains. An effective mitigation is to introduce intermediate supervision for solving sub-tasks of the compounded problem. Recently, several works have demonstrated high gains by taking a straightforward approach for incorporating intermediate supervision in compounded natural language problems: the sequence-to-sequence LM is fed with an augmented input, in which the decomposed tasks' labels are simply concatenated to the original input. In this paper, we prove a positive learning result that motivates these recent efforts. We show that when concatenating intermediate supervision to the input and training a sequence-to-sequence model on this modified input, unlearnable composite problems can become learnable. We show that this is true for any family of tasks which on the one hand, are unlearnable, and on the other hand, can be decomposed into a polynomial number of simple sub-tasks, each of which depends only on O(1) previous sub-task results. Beyond motivating contemporary empirical efforts for incorporating intermediate supervision in sequence-to-sequence language models, our positive theoretical result is the first of its kind in the landscape of results on the benefits of intermediate supervision for neural-network learning: Until now, all theoretical results on the subject are negative, i.e., show cases where learning is impossible without intermediate supervision, while our result is positive, showing that learning is facilitated in the presence of intermediate supervision.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e290.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e290.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Concat-Subtask Supervision</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Concatenating intermediate sub-task labels to the input (sequence-to-sequence teacher-forcing)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A training procedure for sequence-to-sequence language models in which labels for intermediate sub-tasks are appended to the input sequence during training (teacher forcing), so the model is trained to predict the intermediate results and final answer autoregressively.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>sequence-to-sequence model (analyzed)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>Elman RNN (theoretical analysis) and Transformer (empirical discussion); sequence-to-sequence autoregressive setup</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>general multi-step arithmetic / algorithmic operations (parity decomposition used as concrete example)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>works on bit-strings of length d (parity over d/2 bits); general statement covers polynomial-time functions with polynomial-size circuit decompositions</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Concatenate intermediate sub-task labels to input; teacher forcing at training time (model sees ground-truth intermediate results during training; at test time it autoregressively predicts them).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Theoretical result: when intermediate supervision is concatenated, a simple sequence-to-sequence model (Elman RNN) can reach arbitrarily low zero-one loss with polynomial network size, sample complexity and number of gradient updates for tasks that decompose into O(1)-dependent subtasks. Without supervision some tasks (e.g., bit-subset parity) require exponentially many updates and achieve near-random accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Intermediate supervision converts a hard global function into many local single-hop functions each depending on O(1) inputs; each intermediate output can be represented as a low-degree polynomial of a small number of inputs, putting the overall task into a low-complexity hypothesis class (H_phi). Teacher forcing bounds test-time error (union bound over subtask errors). This allows gradient-based training to find solutions efficiently, whereas end-to-end gradients lack sufficient signal for such compositional tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Provably polynomial in problem size d (and 1/epsilon) when intermediate supervision is provided; without intermediate supervision performance requires exponentially many updates for certain tasks (e.g., parity).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>End-to-end training (no intermediate labels) can fail catastrophically on compounded algorithmic tasks (e.g., bit-subset parity), showing essentially random accuracy unless exponential compute/updates are used.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared with end-to-end training without intermediate supervision (same architecture but without concatenated subtasks / teacher forcing).</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Appending intermediate sub-task labels (teacher forcing / scratchpad) converts many otherwise-unlearnable compositional arithmetic/algorithmic tasks into efficiently learnable ones by enabling the model to learn local low-degree computations and avoiding poor end-to-end gradient signal.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sub-Task Decomposition Enables Learning in Sequence to Sequence Tasks', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e290.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e290.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bit-Subset Parity (Transformer experiment)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Empirical evaluation: BERT-base sized Transformer on bit-subset parity with and without concatenated intermediate supervision</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An experiment training a BERT-base sized Transformer to predict parity over a subset of input bits, comparing learning with concatenated intermediate sub-task labels versus vanilla end-to-end training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT-base sized Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>Transformer (BERT-base sized architecture used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>parity computation over subsets of bits (bit-subset parity)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>input sizes from 8 bits up to 256 bits; parity defined over d/2 randomly chosen indices (so parity complexity grows with d)</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Train with concatenated intermediate sub-task labels (inverse-BFS traversal of binary-tree parity decomposition) vs standard training without intermediate labels; training runs up to 100k iterations (some larger runs up to 300k or 2M in specific settings).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Qualitative empirical gap: with intermediate supervision the Transformer learned tasks easily (validation accuracy passes >60% and then rapidly reaches >95%), whereas without intermediate supervision learning fails for modest sizes (e.g., 32-bit parity showed no learning even after >2M steps). Exact per-size numeric accuracies are not tabulated in the text, but figure indicates a rapidly growing gap as input size increases.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Providing intermediate computations (scratchpad) prevents the need for the model to discover a global parity algorithm end-to-end, letting it learn local pairwise parity computations (degree-2 polynomial behavior) and compose them; without those intermediate signals the optimization does not find the algorithmic solution.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>With intermediate supervision the model scales to larger bit widths (learns efficiently as d increases within tested range); without supervision performance rapidly degrades as number of bits increases (32-bit already failed in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>End-to-end training gets stuck / fails to learn algorithmic parity structure; no progress observed for larger bit widths even with very long training in some runs (grokking not observed without supervision).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Comparison is explicitly with vs without concatenated intermediate supervision (same Transformer architecture and training regime).</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Empirically, Transformers show a large practical gap: concatenated intermediate supervision (scratchpad) enables successful learning of parity tasks that are otherwise not learned end-to-end, corroborating the theoretical results.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sub-Task Decomposition Enables Learning in Sequence to Sequence Tasks', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e290.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e290.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Scratchpads / Show-Your-Work (Nye et al. 2022)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Show your work: Scratchpads for intermediate computation with language models (scratchpad / intermediate computation concatenation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior empirical work demonstrating that providing intermediate calculations (scratchpads) to language models greatly improves performance on algorithmic and arithmetic tasks such as multi-digit addition.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Show your work: Scratchpads for intermediate computation with language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>multi-digit addition (8-digit addition example reported)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>8-digit addition (example cited)</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Concatenate intermediate calculations / scratchpad (intermediate supervision) to the model input during training/inference.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>>99% accuracy on 8-digit addition when concatenating intermediate calculations; vanilla (no intermediate supervision) accuracy ~35% (as reported in the cited work and referenced in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Shows empirically that exposing intermediate computation traces dramatically improves arithmetic performance, implying that models can use scratchpad-style supervision to learn multi-step algorithms rather than relying on weak end-to-end signals.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Without intermediate computation traces, models perform poorly on multi-digit addition (low accuracy ~35% in cited example).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>With vs without concatenated intermediate calculations (scratchpad).</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Providing scratchpad-style intermediate calculations turns a poor-performing end-to-end arithmetic learner into a near-perfect solver for 8-digit addition, demonstrating the practical value of intermediate supervision for arithmetic.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sub-Task Decomposition Enables Learning in Sequence to Sequence Tasks', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Show your work: Scratchpads for intermediate computation with language models. <em>(Rating: 2)</em></li>
                <li>Training verifiers to solve math word problems <em>(Rating: 1)</em></li>
                <li>Program induction by rationale generation: Learning to solve and explain algebraic word problems <em>(Rating: 1)</em></li>
                <li>Measuring and improving BERT's mathematical abilities by predicting the order of reasoning <em>(Rating: 2)</em></li>
                <li>Chain of thought prompting elicits reasoning in large language models. <em>(Rating: 1)</em></li>
                <li>Failures of gradient-based deep learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-290",
    "paper_id": "paper-932b6353204e56f20917edadda2fa636ace21090",
    "extraction_schema_id": "extraction-schema-14",
    "extracted_data": [
        {
            "name_short": "Concat-Subtask Supervision",
            "name_full": "Concatenating intermediate sub-task labels to the input (sequence-to-sequence teacher-forcing)",
            "brief_description": "A training procedure for sequence-to-sequence language models in which labels for intermediate sub-tasks are appended to the input sequence during training (teacher forcing), so the model is trained to predict the intermediate results and final answer autoregressively.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "sequence-to-sequence model (analyzed)",
            "model_size": null,
            "model_architecture": "Elman RNN (theoretical analysis) and Transformer (empirical discussion); sequence-to-sequence autoregressive setup",
            "arithmetic_operation_type": "general multi-step arithmetic / algorithmic operations (parity decomposition used as concrete example)",
            "number_range_or_complexity": "works on bit-strings of length d (parity over d/2 bits); general statement covers polynomial-time functions with polynomial-size circuit decompositions",
            "method_or_intervention": "Concatenate intermediate sub-task labels to input; teacher forcing at training time (model sees ground-truth intermediate results during training; at test time it autoregressively predicts them).",
            "performance_result": "Theoretical result: when intermediate supervision is concatenated, a simple sequence-to-sequence model (Elman RNN) can reach arbitrarily low zero-one loss with polynomial network size, sample complexity and number of gradient updates for tasks that decompose into O(1)-dependent subtasks. Without supervision some tasks (e.g., bit-subset parity) require exponentially many updates and achieve near-random accuracy.",
            "mechanistic_insight": "Intermediate supervision converts a hard global function into many local single-hop functions each depending on O(1) inputs; each intermediate output can be represented as a low-degree polynomial of a small number of inputs, putting the overall task into a low-complexity hypothesis class (H_phi). Teacher forcing bounds test-time error (union bound over subtask errors). This allows gradient-based training to find solutions efficiently, whereas end-to-end gradients lack sufficient signal for such compositional tasks.",
            "performance_scaling": "Provably polynomial in problem size d (and 1/epsilon) when intermediate supervision is provided; without intermediate supervision performance requires exponentially many updates for certain tasks (e.g., parity).",
            "failure_modes": "End-to-end training (no intermediate labels) can fail catastrophically on compounded algorithmic tasks (e.g., bit-subset parity), showing essentially random accuracy unless exponential compute/updates are used.",
            "comparison_baseline": "Compared with end-to-end training without intermediate supervision (same architecture but without concatenated subtasks / teacher forcing).",
            "key_finding": "Appending intermediate sub-task labels (teacher forcing / scratchpad) converts many otherwise-unlearnable compositional arithmetic/algorithmic tasks into efficiently learnable ones by enabling the model to learn local low-degree computations and avoiding poor end-to-end gradient signal.",
            "uuid": "e290.0",
            "source_info": {
                "paper_title": "Sub-Task Decomposition Enables Learning in Sequence to Sequence Tasks",
                "publication_date_yy_mm": "2022-04"
            }
        },
        {
            "name_short": "Bit-Subset Parity (Transformer experiment)",
            "name_full": "Empirical evaluation: BERT-base sized Transformer on bit-subset parity with and without concatenated intermediate supervision",
            "brief_description": "An experiment training a BERT-base sized Transformer to predict parity over a subset of input bits, comparing learning with concatenated intermediate sub-task labels versus vanilla end-to-end training.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "BERT-base sized Transformer",
            "model_size": null,
            "model_architecture": "Transformer (BERT-base sized architecture used in experiments)",
            "arithmetic_operation_type": "parity computation over subsets of bits (bit-subset parity)",
            "number_range_or_complexity": "input sizes from 8 bits up to 256 bits; parity defined over d/2 randomly chosen indices (so parity complexity grows with d)",
            "method_or_intervention": "Train with concatenated intermediate sub-task labels (inverse-BFS traversal of binary-tree parity decomposition) vs standard training without intermediate labels; training runs up to 100k iterations (some larger runs up to 300k or 2M in specific settings).",
            "performance_result": "Qualitative empirical gap: with intermediate supervision the Transformer learned tasks easily (validation accuracy passes &gt;60% and then rapidly reaches &gt;95%), whereas without intermediate supervision learning fails for modest sizes (e.g., 32-bit parity showed no learning even after &gt;2M steps). Exact per-size numeric accuracies are not tabulated in the text, but figure indicates a rapidly growing gap as input size increases.",
            "mechanistic_insight": "Providing intermediate computations (scratchpad) prevents the need for the model to discover a global parity algorithm end-to-end, letting it learn local pairwise parity computations (degree-2 polynomial behavior) and compose them; without those intermediate signals the optimization does not find the algorithmic solution.",
            "performance_scaling": "With intermediate supervision the model scales to larger bit widths (learns efficiently as d increases within tested range); without supervision performance rapidly degrades as number of bits increases (32-bit already failed in experiments).",
            "failure_modes": "End-to-end training gets stuck / fails to learn algorithmic parity structure; no progress observed for larger bit widths even with very long training in some runs (grokking not observed without supervision).",
            "comparison_baseline": "Comparison is explicitly with vs without concatenated intermediate supervision (same Transformer architecture and training regime).",
            "key_finding": "Empirically, Transformers show a large practical gap: concatenated intermediate supervision (scratchpad) enables successful learning of parity tasks that are otherwise not learned end-to-end, corroborating the theoretical results.",
            "uuid": "e290.1",
            "source_info": {
                "paper_title": "Sub-Task Decomposition Enables Learning in Sequence to Sequence Tasks",
                "publication_date_yy_mm": "2022-04"
            }
        },
        {
            "name_short": "Scratchpads / Show-Your-Work (Nye et al. 2022)",
            "name_full": "Show your work: Scratchpads for intermediate computation with language models (scratchpad / intermediate computation concatenation)",
            "brief_description": "Prior empirical work demonstrating that providing intermediate calculations (scratchpads) to language models greatly improves performance on algorithmic and arithmetic tasks such as multi-digit addition.",
            "citation_title": "Show your work: Scratchpads for intermediate computation with language models.",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "model_architecture": null,
            "arithmetic_operation_type": "multi-digit addition (8-digit addition example reported)",
            "number_range_or_complexity": "8-digit addition (example cited)",
            "method_or_intervention": "Concatenate intermediate calculations / scratchpad (intermediate supervision) to the model input during training/inference.",
            "performance_result": "&gt;99% accuracy on 8-digit addition when concatenating intermediate calculations; vanilla (no intermediate supervision) accuracy ~35% (as reported in the cited work and referenced in this paper).",
            "mechanistic_insight": "Shows empirically that exposing intermediate computation traces dramatically improves arithmetic performance, implying that models can use scratchpad-style supervision to learn multi-step algorithms rather than relying on weak end-to-end signals.",
            "performance_scaling": null,
            "failure_modes": "Without intermediate computation traces, models perform poorly on multi-digit addition (low accuracy ~35% in cited example).",
            "comparison_baseline": "With vs without concatenated intermediate calculations (scratchpad).",
            "key_finding": "Providing scratchpad-style intermediate calculations turns a poor-performing end-to-end arithmetic learner into a near-perfect solver for 8-digit addition, demonstrating the practical value of intermediate supervision for arithmetic.",
            "uuid": "e290.2",
            "source_info": {
                "paper_title": "Sub-Task Decomposition Enables Learning in Sequence to Sequence Tasks",
                "publication_date_yy_mm": "2022-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Show your work: Scratchpads for intermediate computation with language models.",
            "rating": 2
        },
        {
            "paper_title": "Training verifiers to solve math word problems",
            "rating": 1
        },
        {
            "paper_title": "Program induction by rationale generation: Learning to solve and explain algebraic word problems",
            "rating": 1
        },
        {
            "paper_title": "Measuring and improving BERT's mathematical abilities by predicting the order of reasoning",
            "rating": 2
        },
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models.",
            "rating": 1
        },
        {
            "paper_title": "Failures of gradient-based deep learning",
            "rating": 1
        }
    ],
    "cost": 0.01425625,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Sub-Task Decomposition Enables Learning in Sequence to Sequence Tasks</h1>
<p>Noam Wies, Yoav Levine \&amp; Amnon Shashua<br>The Hebrew University of Jerusalem<br>{noam.wies, yoav.levine, shashua}@cs.huji.ac.il</p>
<h4>Abstract</h4>
<p>The field of Natural Language Processing (NLP) has experienced a dramatic leap in capabilities with the recent introduction of huge Language Models (LMs). Despite this success, natural language problems that involve several compounded steps are still practically unlearnable, even by the largest LMs. This complies with experimental failures for end-to-end learning of composite problems that were demonstrated in a variety of domains. An effective mitigation is to introduce intermediate supervision for solving sub-tasks of the compounded problem. Recently, several works have demonstrated high gains by taking a straightforward approach for incorporating intermediate supervision in compounded natural language problems: the sequence-to-sequence LM is fed with an augmented input, in which the decomposed tasks' labels are simply concatenated to the original input (see figure 1). In this paper, we prove a positive learning result that motivates these recent efforts. We show that when concatenating intermediate supervision to the input and training a sequence-to-sequence model on this modified input, unlearnable composite problems can become learnable. We show that this is true for any family of tasks which on the one hand, are unlearnable, and on the other hand, can be decomposed into a polynomial number of simple sub-tasks, each of which depends only on $O(1)$ previous sub-task results. Beyond motivating contemporary empirical efforts for incorporating intermediate supervision in sequence-to-sequence language models, our positive theoretical result is the first of its kind in the landscape of results on the benefits of intermediate supervision for neural-network learning: Until now, all theoretical results on the subject are negative, i.e., show cases where learning is impossible without intermediate supervision, while our result is positive, showing that learning is facilitated in the presence of intermediate supervision.</p>
<h2>1 INTRODUCTION</h2>
<p>Large-scale language models such as BERT (Devlin et al., 2019), T5 (Raffel et al., 2020), and GPT3 (Brown et al., 2020) have recently pushed the envelope in many NLP tasks. Nevertheless, there are some problem-families that even the largest models do not seem to be capable of solving. One such family is that of "multi-hop" reasoning problems (see, e.g., Geva et al. (2021); Kalyan et al. (2021); Press et al. (2022)) that require compounding operations in order to produce an answer. For example, Gopher (Rae et al., 2021), one of the largest available language models, achieves $61 \%$ accuracy in the StrategyQA benchmark (Geva et al., 2021) that requires implicit decomposition into reasoning steps, while human level performance is around $87 \%$ accuracy.</p>
<p>The limitations of learning compounded tasks with neural networks in an end-to-end manner have been observed in a variety of non-linguistic domains. A leading experimental approach for addressing these is to first explicitly break the compounded operations into more basic "single-hop" operations and then combine the results. Gülçehre \&amp; Bengio (2016), one of the earliest works on this subject, propose that supervision for the single-hop intermediate steps is crucial for avoiding bad local minima in the optimization of neural networks. Afterward, Glasmachers (2017) demonstrated that gradient-based end-to-end multi-hop learning is inefficient for solving complex problems that are easily solved by a divide-and-conquer strategy. Beyond position papers, specific examples were</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: An illustrative example of the prominent method for introducing sub-task decomposition and intermediate supervision for math word problems (Ling et al., 2017; Cobbe et al., 2021). Intermediate sub-tasks and their labels are concatenated to the original task's input to form a new input sequence. At training time, the likelihood of the entire sequence following the original input is maximized conditioned on the input, and at test time only the original input is given to the model.
shown, e.g., Chang et al. (2020) showed that SATNet (Wang et al., 2019) could not solve visual Sudoku without using intermediate labels to identify individual Sudoku digit images.</p>
<p>Similar limitations were observed in language related compounded tasks, including commonsense reasoning (Liu et al., 2022; Wei et al., 2022; Zelikman et al., 2022), math word problems (Piękos et al., 2021; Wei et al., 2022), and programs execution (Nye et al., 2022). The go-to architectures in this domain are powerful language models, which are trained as sequence-to-sequence models over text. In this setting, a particular form of introducing intermediate supervision for compounded tasks has emerged: intermediate sub-tasks and their labels are concatenated to the original task's input to form a new input sequence, on which the sequence-to-sequence LM is trained. This approach has recently been widely adopted, e.g., by Rajani et al. (2019); Cobbe et al. (2021); Piękos et al. (2021); Recchia (2021); Nye et al. (2022); Wei et al. (2022); Zelikman et al. (2022). Figure 1 illustrates this approach for math problems, as done in Ling et al. (2017); Cobbe et al. (2021). These works show that training sequence-to-sequence models with concatenated sub-task decomposition supervision significantly improves the results when compared to training the same model without the intermediate supervision. For example, Nye et al. (2022) show $&gt;99 \%$ accuracy for 8 digits addition when concatenating intermediate calculations to the input, while the vanilla accuracy without intermediate supervision is around $\sim 35 \%$.
While such decomposition based approaches are intuitive, we are not aware of theoretical results that motivate and formulate their benefits for learning composite problems with neural-networks. In this paper, we provide positive theoretical results in this domain, which are in fact the first of their kind (see related work in section 2). We show our results for sequential models, integrating the intermediate supervision in a manner that mimics the above cited successful empirical approaches in the language domain. In this formulation, a learner learns to predict a sequence composed of the task inputs $\mathbf{x}$, followed by the single-hop reasoning steps referred to as the evidence, and finally, the final answer $y$. We extend provable guarantees for the convergence of overparameterized recurrent neural networks (Wang et al., 2021) and prove that with intermediate sub-task supervision, even a simple sequence-to-sequence model provably learns any task that obeys an efficient decomposition into simpler subtasks that depend only on a small fraction of the input. Importantly, both the sample complexity and the required number of gradient updates are polynomial. In contrast, we rely on existing works (Valiant, 1984; Goldreich et al., 1986; Daniely \&amp; Shalev-Shwartz, 2016) to show that in the absence of intermediate supervision, there exist efficiently decomposable tasks that are unlearnable with polynomial time learning algorithms.</p>
<p>Our results apply to a broad family of tasks. As a first exemplifying step, we show a positive result for learning bit subset parity, a setting that is notoriously not amenable to gradient-based algorithms in an efficient way without intermediate supervision (Kearns, 1998; Shalev-Shwartz et al., 2017; Abbe \&amp; Sandon, 2020; Abbe et al., 2021). In this setting, the family of target functions consists of parities over subsets of unknown input bits. Specifically, the input is $d$ bits and the task is to predict whether the number of 1 's in certain unknown subset of $\forall / 2$ bits is odd or even. The corresponding sub-tasks we consider are the parities of subsets of the unknown input subset. We prove a theorem guaranteeing that, when intermediate supervision is available, efficient neural network learning is</p>
<p>made possible. As a result, we show an exponential gap between the end-to-end and decompositionbased neural network learnability of the bit subset parity problem.</p>
<p>Next, we generalize the above result, and show that when sufficient intermediate supervision is available, any family of functions with a polynomial time complexity, i.e., functions that belong to the $\mathbf{P}$ time complexity class, are efficiently learnable by neural networks. Accordingly, based on either standard cryptographic assumptions (Valiant, 1984; Goldreich et al., 1986) or computational complexity hardness assumptions (Daniely \&amp; Shalev-Shwartz, 2016) we prove that there exist tasks that, on the one hand, cannot be learned by any polynomial time algorithm and, on the other hand, can be efficiently learned by neural networks when intermediate supervision is present.</p>
<p>Our main result can be stated as follows:
Theorem 1. (Informal) There exists a binary classification problem parameterized by size d, such that the following holds:</p>
<ul>
<li>On one hand, when equipped with sub-task decomposition supervision, a simple sequence-to-sequence model can get arbitrarily low $\epsilon&gt;0$ zero-one loss with number of gradient updates that is polynomial in $d, \epsilon^{-1}$.</li>
<li>On the other hand, when supervision regarding sub-task is missing, then for any polynomial time learning algorithm and (constant) $\epsilon&gt;0$, the zero-one loss will be higher than $1 / 2-\epsilon$.</li>
</ul>
<p>To summarize, the main contributions of this paper are:</p>
<ol>
<li>We show the first positive result that guarantees neural networks learnability in the presence of intermediate supervision for a problem that is unlearnable without it.</li>
<li>We do so in the sequence-to-sequence setting that is currently used for applying state-of-the-art language models on complex multi-hop tasks in NLP.</li>
<li>We show that with sufficient intermediate supervision this sequence-to-sequence setting allows learning any function in the $\mathbf{P}$ time complexity class.</li>
</ol>
<p>The remainder of this paper is organized as follows. Section 3 presents the sequence to sequence model we analyzed. Section 4 presents a hypothesis class and proves that it can be learned with our sequence to sequence model. Section 5 presents a concrete example, and demonstrates that the task of learning bit-subset parity with sequence-to-sequence models can be learned with sub-task decomposition and the corresponding intermediate supervision. Finally, in section 6 we generalize the positive results to any function in the $\mathbf{P}$ time complexity class, thus establishing our main result.</p>
<h1>2 Related Work</h1>
<p>The concept of how learning can be enhanced by guiding a learner through intermediate tasks is an old one, dating back to animal training by shaping (Skinner, 1958; Peterson, 2004; Krueger \&amp; Dayan, 2009). Since then, a large body of work has shown its practical benefits for various machine learning tasks. For example, there exists a rich line of work on the importance of shaping rewards and adding sub-goals in reinforcement learning tasks. Karlsson (1994) introduced the methodology of using knowledge in the reward function, in order to decompose a holistic task into several subtasks. Ng et al. (1999) established necessary and sufficient conditions for reward shaping to reserved optimal policies. Marthi (2007) investigate the problem of automatically learning a decomposition of the reward function. All these work intuitively rely on benefits of adding intermediate supervision. Recently, Zhai et al. (2022) showed that adding sub-goal rewards provably reduces the complexity of the synchronous value iteration algorithm. However, this reduction is linear in the number of the sub-goals, unlike our work that proves exponential gap in the supervised learning setting. Moreover, several of the notions in their analysis are unique to the reinforcement leaning setup and cannot be easily translated into the supervised learning setting (e.g., One-Way Intermediate States).
Negative theoretical results exist, showing that end-to-end learning of multi-hop problems is unfeasible without decomposition. Shalev-Shwartz et al. (2017) explored the theoretical limitations of end-to-end gradients based learning, studying learnability of tasks that are composed of classification and parity tasks, proving that the end-to-end approach does not converge in a polynomial</p>
<p>number of gradient updates. They do show that when intermediate supervision is provided, the gradients have a much higher signal-to-noise ratio. However, they provide no guarantees that in this case learning is possible in a polynomial number of gradient updates. In addition, Shalev-Shwartz \&amp; Shashua (2016) proved an exponential gap between end-to-end-based verification sample complexity and the decomposition-based verification sample complexity. However, again, an explicit setting in which providing intermediate supervision for training actually improves the situation to a point that learning is feasible, is lacking. We provide the first theoretical result proving that neural networks also benefit from sub-task decomposition, while earlier theoretical works in this space only prove that end-to-end learning is unfeasible in some compounded cases.</p>
<h1>3 THE ANALYZED SEQUENCE-TO-SEQUENCE LEARNING ALGORITHM</h1>
<p>A recent successful empirical approach for solving compounded natural language problems (Ling et al., 2017; Rajani et al., 2019; Piękos et al., 2021; Recchia, 2021; Cobbe et al., 2021; Nye et al., 2022; Wei et al., 2022; Zelikman et al., 2022) is to concatenate intermediate supervision labels to the input. This way, the language model receives a sequence composed of the input followed by the labels of the intermediate tasks, before emitting the final compounded answer. For a compounded binary classification task which consists of a $d$-bit input string $\mathbf{x}$, with $\mathcal{S}$ denoting the string of intermediate step results, we denote the combined input sequence as $\mathbf{z}=\operatorname{Concat}{\mathbf{x} ; \mathcal{S}}$, and the combined output sequence as $\mathbf{y}$, defined in a standard autoregressive fashion by ${ }^{1} y_{t}=z_{t+1}$ (see figure 2 for a $d=8$ example). Training and testing follow conventional sequence-to-sequence model protocol: At training time, $z_{t}$ for $t&gt;d$ will be the ground-truth sub-task result $y_{t-1}$ (a practice sometimes referred to as "teacher forcing" (Williams \&amp; Zipser, 1989)), and at test time, $z_{t}$ for $t&gt;d$ will be the model's prediction at time $t-1$.</p>
<p>We analyze the classical Elman recurrent neural networks (Elman, 1990) with ReLU activations as our sequence-to-sequence model. Given an input sequence $\mathbf{z}$ of length $T=d+\operatorname{len}(\mathcal{S})$ as defined above, the architecture $f^{\mathrm{RNN}}$ computes:</p>
<p>$$
\begin{aligned}
&amp; \forall t \in[T] \quad h_{t}(\mathbf{z})=\operatorname{ReLU}\left(W h_{t-1}+A \mathbf{e}<em t="t">{z</em>\right) \
&amp; \forall t \in[T] \quad f_{t}^{\mathrm{RNN}}(\mathbf{z})=B^{T} h_{t}(\mathbf{z}) \
&amp; h_{0}(\mathbf{z})=\operatorname{ReLU}\left(M_{0}\right)
\end{aligned}
$$}</p>
<p>where $\mathbf{e}<em 1="1">{0}, \mathbf{e}</em>$ is the initialization of the hidden state.} \in \mathbb{R}^{2}$ are one-hot vectors, $A \in \mathbb{R}^{m \times 2}$ translates the input to the hidden dimension $m, W \in \mathbb{R}^{m \times m}$ is the learned hidden weights matrix, $B \in \mathbb{R}^{m}$ is the output weights vector and $M_{0} \in \mathbb{R}^{m</p>
<p>We will use the binary cross-entropy loss over output locations for $t \geq d$, i.e., our loss ignores the architecture's prediction of $\mathbf{x}$ and depends on its prediction of intermediate labels and final outcome:</p>
<p>$$
l(\mathbf{y}, \mathbf{s})=\left(\frac{1}{T-d}\right) \sum_{t=d}^{T} \log \left(1+e^{-y_{t} \cdot s_{t}}\right)
$$</p>
<p>Algorithm 1 below describes the analyzed training procedure of our sequence-to-sequence model. This algorithm describes a straightforward SGD training procedure where, for simplicity, we analyze a variant that updates only the hidden $W$ weights while keeping $A, B, M_{0}$ frozen at initialization. This amounts to keeping the input, output and the $t=0$ hidden state untrained, and training only the core recurrence operation to perform the task while complying with these frozen components.</p>
<h2>4 COMPOUNDED SEQUENCE TO SEQUENCE LEARNABILITY</h2>
<p>In this section, we present a hypothesis class and prove for it that the above described "teacher forcing" (Williams \&amp; Zipser, 1989) of intermediate supervision at training time with algorithm 1 provably leads to generalization in polynomial sample complexity and gradient updates. This guarantee will allow us to prove our positive results in the following sections, as we will show that interesting function families belong to this hypothesis class.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<div class="codehilite"><pre><span></span><code>Algorithm 1: Training \(f^{\mathrm{RNN}}\) with SGD
Data: Data set \(\mathcal{D}\), learning rate \(\eta\).
Initialization: The entries of \(W^{(0)}, A, M_{0}\) are i.i.d. generated from \(N\left(0, \frac{2}{m}\right)\). The entries of \(B\)
    are i.i.d. generated from \(N\left(0, \frac{1}{m}\right)\).
for \(i=1,2,3 \ldots n\) do
    Randomly sample \(\left(\mathbf{x}_{i}, \mathbf{y}_{i}\right)\) from the data set \(\mathcal{D}\).
    \(W^{(i)}=W^{(i-1)}-\eta \nabla_{W^{(i-1)}} \ell\left(\mathbf{y}_{i}, f^{\mathrm{RNN}, W^{(i-1)}}\left(\mathbf{z}_{i}\right)\right)\).
end
</code></pre></div>

<p>In order to analyze the teacher forcing technique, we begin with an important observation. Essentially, we show that when the zero-one loss of all the single-hop sub-tasks is low, then it implies that also at test time, when the model does not have the ground truth results of the previous sub-tasks and the errors might accumulate, the zero-one loss on the final answer is still low:
Lemma 1. Denote by $z_{t}^{\text {train }}:=y_{t-1}$ the ground truth input at training time, and by $z_{t}^{\text {test }}:=$ $f_{t-1}^{R N N, W}\left(\mathbf{z}^{\text {test }}\right)$ the iteratively predicted input at test time. Then, for any $W$ the following holds:</p>
<p>$$
\mathbb{E}<em 0-1="0-1">{\mathbf{x}}\left[l</em>}\left(y, f_{T}^{R N N, W}\left(\mathbf{z}^{\text {test }}\right)\right)\right] \leq \mathbb{E<em t="d">{\mathbf{x}}\left[\sum</em>\right)\right)\right]
$$}^{T} l_{0-1}\left(y_{t}, f_{t}^{R N N, W}\left(\mathbf{z}^{\text {train }</p>
<p>Proof. Clearly, for any $\mathbf{x}$ when $f^{\mathrm{RNN}, W}\left(\mathbf{z}^{\text {train }}\right)$ solves all of the sub-tasks correctly we have that $\mathbf{z}^{\text {test }}=\mathbf{z}^{\text {train }}$ and therefore they have the same zero zero-one loss. So it is enough to upper bound the probability that $f^{\mathrm{RNN}, W}\left(\mathbf{z}^{\text {train }}\right)$ is erroneous in any sub-task. Now by definition, for any $t$ the zero-one loss at the $t$ 'th location is equal to the probability of wrong prediction at this location. Therefore, by the union bound, we get that the sum of the zero-one loss over all the locations is upper bounding the probability of $f^{\mathrm{RNN}, W}\left(\mathbf{z}^{\text {train }}\right)$ make an error in any sub-task. See full details in section A at the appendix.</p>
<p>As expected, due to a union bound, when the model does not have the ground truth results of the previous sub-task the error can increase by a factor of $T-d$ but this increase is relatively modest as long as $T$ is polynomial in $d$.</p>
<p>Lemma 1 above assures us that it is enough to find an hypothesis class for which algorithm 1 converges and generalizes when we do have the ground truth results of the previous sub-tasks, in order to prove that the teacher forcing technique works. As a candidate for such a hypothesis class, we consider tasks for which the output at each location $d \leq t \leq T$ can be written as sign of composition of linear functions (represented by $\mathbf{w}$ below) of at most $N&lt;T$ input locations $j_{1}, \ldots, j_{N} \leq t$, with polynomials activations $\psi_{t}(x)=\sum_{i=0}^{\operatorname{deg}\left(\psi_{t}\right)} a_{t, i} x^{i}$ :</p>
<p>$$
\forall d \leq t \leq T \quad h_{t}(\mathbf{z})=\operatorname{sign}\left(\psi_{t}\left(\left\langle\frac{\mathbf{w}^{(t)}}{\left|\mathbf{w}^{(t)}\right|},\left(\begin{array}{c}
\mathbf{e}<em j__1="j_{1">{z</em> \
\vdots \
\mathbf{e}}}<em j__N="j_{N">{z</em>
\end{array}\right)\right\rangle\right)\right)
$$}}</p>
<p>In order to prove convergence and generalization results, we will measure the complexity of functions in the above hypothesis class by a function $\phi(T, \psi, N)$, described formally in appendix A. Importantly, $\phi(T, \psi, N)$ is polynomial in both $T$ and $\max <em i="i" t_="t,">{t, i}\left|a</em>\right|$, while exponential in both $\max <em t="t">{t} \operatorname{deg}\left(\psi</em>$ the hypothesis class described in eq 6.
Now, with this hypothesis class, we can combine lemma 1 with theorem 2 in Wang et al. (2021). They study the learnability of RNNs for binary classification tasks ${ }^{2}$ without intermediate supervision, and prove that algorithm 1 is capable of learning function where the final answer $y$ have low complexity $\phi(T, \psi, N)$.}\right)$ and $N$. We will denote by $\mathcal{H}_{\phi(T, \psi, N)</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Illustration of the proposed input and output for learning the $d=8$ bit-subset parity problem with sequence-to-sequence models.</p>
<p>Theorem 2. Denote by $z_{t}^{\text {test }}:=\int_{t-1}^{R N N, W}\left(\mathbf{z}^{\text {test }}\right)$ the iteratively predicted input at test time, and let $\epsilon, \delta&gt;0$. Assume we run Algorithm 1 for $n&gt;\tilde{O}\left(\left(\frac{\phi(T, \psi, N)+\log \left(\frac{1}{\delta}\right)}{\epsilon}\right)^{2}\right)$ iterations with learning rate $\eta=\frac{1}{m \sqrt{n}}$, then there exists $m^{<em>}=$ poly $\left(n, \delta^{-1}, T\right)$ such that if $m&gt;m^{</em>}$ then for any $h \in$ $\mathcal{H}_{\phi(T, \psi, N)}$ with probability at least $1-\delta$ over the randomness in Algorithm 1, the following holds:</p>
<p>$$
\frac{1}{n} \sum_{i=1}^{n} \mathbb{E}<em 0-1="0-1">{\mathbf{x}}\left[l</em>\right)\right)\right]&lt;\epsilon
$$}\left(y, f_{T}^{R N N, W^{(i)}}\left(\mathbf{z}^{t e x t</p>
<p>where $W^{(i)}$ denotes the output of Algorithm 1 at the $i^{\prime}$ th iteration and $l_{0-1}$ is the zero-one loss.
Note that sections C,D of the appendix extends theorem 2 for both SGD and GD with finite precision. In the next sections, we will prove our positive results by showing the intermediate single-hop subtasks of the analyzed tasks belong to $\mathcal{H}_{\phi(T, \psi, N)}$ with low complexity $\phi(T, \psi, N)$.</p>
<h1>5 Learning Bit-Subset Parity With Sequence to Sequence Models</h1>
<p>As a concrete demonstration, in this section we show that unlike the end-to-end case, bit-subset parity can be learned with neural networks when intermediate supervision is provided. We begin by defining the challenging task of learning parities over unknown subsets of input bits. Specifically, for a $d$-bit string with a subset of $d / 2$ randomly predefined unique indices $i_{1}, \ldots, i_{d / 2}$, our goal is to train a predictor mapping $\mathbf{x} \in{0,1}^{d}$ to $y=(-1)^{\sum_{j=1}^{d / 2} x_{i_{j}}}$ where $\mathbf{x}$ is uniformly distributed. In words, $y$ indicates whether the number of 1 's in the given subset of coordinates of $\mathbf{x}$ is odd or even.
We analyze this task as a "multi-hop" task by decomposing it into natural intermediate sub-tasks: parities of subsets of the predefined input subset $x_{i_{1}}, \ldots, x_{i_{d / 2}}$. Concretely, assuming for simplicity that $d / 2$ is a power of 2 , and beginning with only two adjacently indexed input bits at a time, we recursively treat the parity of every two adjacently indexed subgroups as an intermediate task. Figure 2(a) illustrates this binary tree sub-task decomposition of our parity problem. The leaves of the tree of intermediate labels $\mathcal{T}$ are $-1^{x_{i_{1}}+x_{i_{2}}}, \ldots,-1^{x_{i_{d / 2-1}}+x_{i_{d / 2}}}$ and each node in the tree represents the sub-task of calculating the parity function over its descendants.
In order to fit into the sequence-to-sequence setting of section 3, we translate our imaginary tree of intermediate labels $\mathcal{T}$ into a sequence of intermediate labels $\mathcal{S}$ by inverse-BFS like tree traversal, and then concatenate the sequence $\mathcal{S}$ after the input $\mathbf{x}$. An exact formulation of the mapping from tree $\mathcal{T}$ to sequence of intermediate labels $\mathcal{S}$ is given in appendix B.</p>
<h1>5.1 Learnability of Bit-Subset Parity With and Without Intermediate SUPERVISION</h1>
<p>In this section we show that the sub-tasks of learning bit-subset parity are simple enough to be covered by the result in theorem 2, i.e., we prove that our sequence-to-sequence formulation of the bit-subset parity target function, which includes the intermediate labels sequence $\mathcal{S}$ defined above, can be written as a multivariate function where each of its outputs is a simple low degree polynomial of at most $O(1)$ inputs bits. We show that this is indeed the case, i.e., that all the parity target functions comprising the intermediate supervision to our problem belong to $\mathcal{H}<em t="t">{\phi(T, \psi, N)}$ (see section 4) with $N, \max </em>\right), \max } \operatorname{deg}\left(\psi_{t<em i="i" t_="t,">{t, i}\left|a</em>\right)$. Thus, no efficient learning is guaranteed for the original compounded task.
We begin by showing that our single-hop tasks of parities over two bits (see illustration in figure 2(a)) are simple degree-2 polynomials:
Lemma 2. There exists degree two polynomial $\psi(x)=a_{2} x^{2}+a_{1} x+a_{0}$ with bounded coefficients $\forall i \quad\left|a_{i}\right|&lt;10$ as well as $\mathbf{w} \in \mathbb{R}^{4}$ such that:}\right|$ that do not grow with $d$. Importantly, when defining the input sequence to be only the original input, without the concatenated sub-task decomposition labels, then the function $h_{T}(\mathbf{x})$ clearly depends on $\nicefrac{{d}}{{2}}$ bits, and therefore will require $N=\nicefrac{{d}}{{2}}$, that leads to exponential complexity $\phi(T, \psi, N)=\Omega\left(e^{d</p>
<p>$$
\forall z_{1}, z_{2} \in{0,1} \quad \psi\left(\left\langle\frac{\mathbf{w}}{|\mathbf{w}|},\binom{\mathbf{e}<em 1="1">{z</em>}}}{\mathbf{e<em 2="2">{z</em>
1 &amp; z_{1}=z_{2} \
-1 &amp; z_{1} \neq z_{2}
\end{array}\right.
$$}}}\right\rangle\right\rangle=\left{\begin{array}{ll</p>
<p>Proof. We will use $\mathbf{w}$ to sum the first coordinates of $\mathbf{e}<em 1="1">{z</em>}}, \mathbf{e<em 2="2">{z</em>=1$ points, see full details in appendix B.}}$, and use polynomial interpolation to find a degree two polynomial $\psi$ that interpolates the $z_{1}=z_{2}=0, z_{1} \neq z_{2}, z_{1}=z_{2</p>
<p>The above lemma implies that all of the target functions in our defined intermediate supervision belong to $\mathcal{H}_{\phi(T, \psi, N)}$ for $\phi(T, \psi, N)=O(d)$. Therefore, together with theorem 2, it assures us that when intermediate supervision is available, Algorithm 1 can learn bit-subset parities with polynomial network size, sample complexity and number of gradient updates.
Now, after we showed that when incorporating intermediate supervision bit-subset parities can be learned by a neural network, we will use the results of Shalev-Shwartz et al. (2017) to establish an exponential gap between the end-to-end and decomposition-based neural network learnability ${ }^{3}$ :
Corollary 1. When learning bit-subset parities using neural networks, the following holds:</p>
<ul>
<li>On one hand, when equipped with sub-task decomposition supervision, a simple sequence-to-sequence model can get arbitrarily low $\epsilon&gt;0$ zero-one loss with number of gradient updates that is polynomial in $d, \epsilon^{-1}$.</li>
<li>On the other hand, when supervision regarding sub-task is missing, then for any (constant) $\epsilon&gt;0$ with high probability over the target parity, the zero-one loss will be higher than $\nicefrac{{1}}{{2}}-\epsilon$ unless the number of gradient updates is exponential in $d$.</li>
</ul>
<p>Proof. Follows directly by combining theorem 2 and lemma 2 with the the negative results in Shalev-Shwartz et al. (2017). See full details in section F at the appendix.</p>
<h3>5.2 Bit-Subset Parity Experiments</h3>
<p>In section 5.1 we proved an exponential gap when using Elman RNNs (Elman, 1990) to learn bitsubset parity with and without sub-task decomposition. This section empirically demonstrates that the same gap exists with the commonly used Transformer (Vaswani et al., 2017) architecture. We trained a series of models while varying the input sizes from 8 bits to 256 bits. For each input size,</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>we trained a BERT-base sized Transformer model for $100 k$ iterations ${ }^{4}$ with and without intermediate supervision. The intermediate supervision was introduced exactly as described in the previous subsection, see Figure 2 for an illustration. See full technical details of the training apparatus in appendix G.</p>
<p>Figure 3 clearly shows that in a practical setting, using common Transformer networks, a very large gap quickly opens between the settings with and without intermediate supervision. The employed BERT base sized Transformer architecture is a strong network that pushed the envelope on very challenging NLP tasks, and is much stronger than the theoretically analyzed RNN. Still, learning even the 32 bit subset parity task without supervision proved to be too challenging even for this network (no learning after over 2 M steps), while it easily learned the task in the presence of intermediate supervision. Overall this experiment, performed on the same task on which we prove our theoretical results, reinforces their relevance to common Transformer architectures.</p>
<h1>6 UNIVERSALITY OF DECOMPOSITION BASED SEQUENCE-TO-SEQUENCE LEARNING</h1>
<p>In this section, we prove our main result (outlined in the introductory theorem 1). On the one hand, we generalize the positive results of section 5.1 by showing that when sufficient intermediate supervision is available, a neural network can efficiently learn any function in the $\mathbf{P}$ time complexity class. On the other hand, we rely on existing works (Valiant, 1984; Goldreich et al., 1986; Daniely \&amp; Shalev-Shwartz, 2016) to show that under either standard cryptographic assumptions or computational complexity hardness assumptions, there exist functions in the $\mathbf{P}$ time complexity class that cannot be learned by any polynomial time learning algorithm without intermediate supervision.</p>
<p>We begin by defining the decomposition of any function $f$ in the $\mathbf{P}$ time complexity class into sub tasks. For that, we will use the fact that any such $f$ has polynomial circuit complexity (see for example theorem 9.30 in Sipser (2013)), and therefore can be computed by a boolean circuit with polynomial size. We will denote by $G=(V, E)$ the directed acyclic graph associated with such a circuit, and by $l_{v}$ the logic gates of each vertex $v$. Furthermore, since both the"AND" and "OR" logical gates can be decomposed into a boolean circuit with binary-tree like structure, we may assume that the input degree of each vertex is $O(1)$.</p>
<p>Now, in order to fit into the sequence-tosequence setting of section 3, we define the intermediate labels sequence $\mathcal{S}$ for any $f$. Basically, each non-leaf vertex $v \in V$ will represent an intermediate task with its ground-truth label determined by $l_{v}$, and we will use a topological sorting of $G$ in order to translate $G$ into a sequence of intermediate labels $\mathcal{S}$ with length $T:=|V|$ (see figure 2 for a concrete example of this abstract construction strategy). Importantly, as in the bit-subsets parity task, $T$ is polynomial in $d$.</p>
<p>In order to show our generalized positive result, theorem 2 motivates us to prove that our sequence-to-sequence formulation of any function $f$ in the $\mathbf{P}$ time complexity class, which includes the intermediate labels sequence $\mathcal{S}$ defined above, can be written as a multivariate function where each of its outputs is a simple low degree polynomial of at most $O(1)$ input bits. Lemma 3 below shows that this is indeed the case, i.e., that all the target functions comprising the intermediate supervision to our problem belong to $\mathcal{H}<em t="t">{\psi(T, \psi, N)}$ (see section 3) with $N, \max </em>\right), \max } \operatorname{deg}\left(\psi_{t<em i="i" t_="t,">{t, i}\left|a</em>\right|$ that do not grow with $d$.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: The number of steps until a BERTbase sized Transformer learns bit-subset parities with and without intermediate supervision. By learning we mean validation accuracy higher than $60 \%$. While this definition is somehow arbitrary, in practice we observed a grokking phenomenon (Power et al., 2021) where very soon after the accuracy became higher than random level it also became almost perfect (accuracy $&gt;95 \%$ ).</p>
<p>Lemma 3. For any logical gate $l_{v}:{0,1}^{N} \rightarrow{0,1}$ with $N=O(1)$, there exists $O(1)$ degree polynomial $\psi(x)=\sum_{i=0}^{\operatorname{deg}(\psi)} a_{i} x^{i}$ with bounded coefficients $\max <em i="i">{i}\left|a</em>$ such that:}\right|=O(1)$ as well as $\mathbf{w} \in \mathbb{R}^{2 N</p>
<p>$$
\forall z_{1}, \ldots, z_{N} \in{0,1} \quad \psi\left(\left\langle\frac{\mathbf{w}}{|\mathbf{w}|},\left(\begin{array}{c}
\mathbf{e}<em 1="1">{z</em> \
\vdots \
\mathbf{e}}<em N="N">{z</em>
\end{array}\right)\right\rangle\right)=l_{v}\left(z_{1}, \ldots, z_{N}\right)
$$}</p>
<p>Proof. We will use $\mathbf{w}$ to uniquely represent each possible combination of $z_{0}, \ldots, z_{N}$ as an $N$ bit real value number, and use polynomial interpolation to find a $2^{N}$-degree polynomial $\psi$ that interpolates the $z_{1}=\cdots=z_{N}=0, \ldots, z_{1}=\cdots=z_{N}=1$ points. Finally the $O(1)$ coefficients boundedness follow from taking maximum ${ }^{5}$ over all possible $l_{v}$ logical gates. see full details in appendix B.</p>
<p>The above lemma implies that all of the target functions in our defined intermediate supervision belong to $\mathcal{H}_{\phi(T, \psi, N)}$ for $\phi(T, \psi, N)=O(d)$. Therefore, together with theorem 2, it assures us that when intermediate supervision is available, Algorithm 1 can learn any function in the $\mathbf{P}$ time complexity class with polynomial network size, sample complexity and number of gradient updates.
Now, after we showed that when incorporating intermediate supervision any function in the $\mathbf{P}$ time complexity class can be learned by a neural network, our main results is a simple corollary of the above results:
Corollary 2. Under either standard cryptographic assumptions or computational complexity hardness assumptions, there exists a binary classification problem parameterized by size $d$, such that the following holds:</p>
<ul>
<li>On one hand, when equipped with sub-task decomposition supervision, a simple sequence-to-sequence model can get arbitrarily low $\epsilon&gt;0$ zero-one loss with number of gradient updates that is polynomial in $d, \epsilon^{-1}$.</li>
<li>On the other hand, when supervision regarding sub-task is missing, then for any polynomial time learning algorithm and (constant) $\epsilon&gt;0$, the zero-one loss will be higher than $1 / 2-\epsilon$.</li>
</ul>
<p>Proof. Follows directly by combining theorem 2 and lemma 3 with either the negative results in Valiant (1984); Goldreich et al. (1986) or in Daniely \&amp; Shalev-Shwartz (2016).</p>
<h1>7 DISCUSSION</h1>
<p>In this paper, we show for a broad family of functions an exponential gap between learning algorithms that rely on intermediate supervision and algorithms that do not rely on intermediate supervision. Across domains and architectures, there has been a wide range of proposed methods for introducing intermediate supervision. Some design specialized architectures, some add relevant loss terms, etc. The method that is taking over in the NLP domain is straightforward, and is particularly natural for this domain in which the core architectures are strong sequence-to-sequence Language Models: Concatenate the intermediate supervision to the input, and thus jointly train the model to maximize the likelihood of all the intermediate labels as well as the overall output. Our analysis is framed in this space, and motivates this intuitive incorporation of intermediate supervision in the framework of sequence-to-sequence models. We show that even with a simple sequence-to-sequence architecture it is feasible to expect such simultaneous compounded learning to be useful. In this regard, we view our work as providing timely theoretical feedback to the rapid empirical advances in this field.</p>
<p>Limitations: We proved universal learnability results when sufficient intermediate supervision was provided. A fundamental question is what happens when we limit the amount of sub-task supervision. For the task of bit-subset parity, we demonstrated that supervision regarding $O(d)$ sub-tasks can yield an exponential advantage. An interesting question that we leave open for future work is whether there exists a similar advantage with only $O(1)$ sub-tasks.
In addition, while our results show an exponential gain, it is still unclear which sub-tasks are solvable by end-to-end methods, and which tasks require decomposition? Interestingly, a recent study (Abbe et al., 2022) addressed exactly this question for one-layer hidden networks in the mean-field regime. However, our understanding of this question for practical architectures is still very limited.</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>REPRODUCIbILITY STATEMENT</h1>
<p>A complete proof of all the theoretical claims was included in the appendix. We also provide the source code for the bit-subset parity experiment in https://github.com/HUJIDeep/sub_task_decomposition.</p>
<h2>ACKNOWLEDGMENTS AND DISCLOSURE OF FUNDING</h2>
<p>We thank Eran Malach and Shai Shalev-Shwartz for a helpful discussion on our stronger negative results, as well as Lifu Wang for clarifying Wang et al. (2021). This research was supported by the ERC (European Research Council) and the ISF (Israel Science Foundation). Yoav Levine was supported by the Israel Academy of Sciences Adams fellowship.</p>
<h2>REFERENCES</h2>
<p>Emmanuel Abbe and Colin Sandon. On the universality of deep learning. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 20061-20072. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/ e7e8f8e5982b3298c8addedf6811d500-Paper.pdf.</p>
<p>Emmanuel Abbe, Pritish Kamath, eran malach, Colin Sandon, and Nathan Srebro. On the power of differentiable learning versus PAC and SQ learning. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/forum?id=TZPidZS3r_z.</p>
<p>Emmanuel Abbe, Enric Boix Adsera, and Theodor Misiakiewicz. The merged-staircase property: a necessary and nearly sufficient condition for sgd learning of sparse functions on two-layer neural networks. In Conference on Learning Theory, pp. 4782-4887. PMLR, 2022.</p>
<p>Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. On the convergence rate of training recurrent neural networks. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/ 0ee8b85a85a49346fdff9665312a5cc4-Paper.pdf.</p>
<p>Afonso S Bandeira and Ramon Van Handel. Sharp nonasymptotic bounds on the norm of random matrices with independent entries. The Annals of Probability, 44(4):2479-2506, 2016.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 1876-1900. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/ 1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.</p>
<p>Oscar Chang, Lampros Flokas, Hod Lipson, and Michael Spranger. Assessing satnet's ability to solve the symbol grounding problem. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 1428-1439. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/ paper/2020/file/0ff8033cf9437c213ee13937b1c4c455-Paper.pdf.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.</p>
<p>Amit Daniely and Shai Shalev-Shwartz. Complexity theoretic limitations on learning dnf's. In Vitaly Feldman, Alexander Rakhlin, and Ohad Shamir (eds.), 29th Annual Conference on Learning Theory, volume 49 of Proceedings of Machine Learning Research, pp. 815-830, Columbia University, New York, New York, USA, 23-26 Jun 2016. PMLR. URL https://proceedings. mlr.press/v49/daniely16.html.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. In Jill Burstein, Christy Doran, and Thamar Solorio (eds.), Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pp. 41714186. Association for Computational Linguistics, 2019. doi: 10.18653/v1/n19-1423. URL https://doi.org/10.18653/v1/n19-1423.</p>
<p>Jeffrey L Elman. Finding structure in time. Cognitive science, 14(2):179-211, 1990.
Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies. Transactions of the Association for Computational Linguistics (TACL), 2021.</p>
<p>Tobias Glasmachers. Limits of end-to-end learning. In Asian Conference on Machine Learning, pp. 17-32. PMLR, 2017.</p>
<p>Oded Goldreich, Shafi Goldwasser, and Silvio Micali. How to construct random functions. Journal of the ACM (JACM), 33(4):792-807, 1986.</p>
<p>Çağlar Gülçehre and Yoshua Bengio. Knowledge matters: Importance of prior information for optimization. Journal of Machine Learning Research, 17(8):1-32, 2016. URL http://jmlr. org/papers/v17/gulchere16a.html.</p>
<p>Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence and generalization in neural networks. Advances in neural information processing systems, 31, 2018.</p>
<p>Ziwei Ji and Matus Telgarsky. Polylogarithmic width suffices for gradient descent to achieve arbitrarily small test error with shallow relu networks. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=HygegyrYwH.</p>
<p>Ashwin Kalyan, Abhinav Kumar, Arjun Chandrasekaran, Ashish Sabharwal, and Peter Clark. How much coffee was consumed during EMNLP 2019? fermi problems: A new reasoning challenge for AI. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 7318-7328, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.582. URL https://aclanthology.org/2021.emnlp-main.582.</p>
<p>Jonas Karlsson. Task decomposition in reinforcement learning. In Proceedings of the AAAI Spring Symposium on Goal-Driven Learning, Stanford, CA, 1994.</p>
<p>Michael Kearns. Efficient noise-tolerant learning from statistical queries. Journal of the ACM (JACM), 45(6):983-1006, 1998.</p>
<p>Kai A Krueger and Peter Dayan. Flexible shaping: How learning in small steps helps. Cognition, 110(3):380-394, 2009.</p>
<p>Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction by rationale generation: Learning to solve and explain algebraic word problems. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 158-167, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1015. URL https://aclanthology.org/P17-1015.</p>
<p>Jiacheng Liu, Alisa Liu, Ximing Lu, Sean Welleck, Peter West, Ronan Le Bras, Yejin Choi, and Hannaneh Hajishirzi. Generated knowledge prompting for commonsense reasoning. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 3154-3169, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.225. URL https://aclanthology.org/2022. acl-long. 225 .</p>
<p>Bhaskara Marthi. Automatic shaping and decomposition of reward functions. In Proceedings of the 24th International Conference on Machine learning, pp. 601-608, 2007.</p>
<p>Andrew Y Ng, Daishi Harada, and Stuart Russell. Policy invariance under reward transformations: Theory and application to reward shaping. In Icml, volume 99, pp. 278-287, 1999.</p>
<p>Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Augustus Odena. Show your work: Scratchpads for intermediate computation with language models. In Deep Learning for Code Workshop, 2022. URL https://openreview.net/forum? id=HBlx2idbkbq.</p>
<p>Gail B Peterson. A day of great illumination: Bf skinner's discovery of shaping. Journal of the experimental analysis of behavior, 82(3):317-328, 2004.</p>
<p>Piotr Piękos, Mateusz Malinowski, and Henryk Michalewski. Measuring and improving BERT's mathematical abilities by predicting the order of reasoning. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pp. 383-394, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-short.49. URL https://aclanthology.org/2021.acl-short.49.</p>
<p>Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, and Vedant Misra. Grokking: Generalization beyond overfitting on small algorithmic datasets. In ICLR MATH-AI Workshop, 2021.</p>
<p>Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A. Smith, and Mike Lewis. Measuring and narrowing the compositionality gap in language models, 2022. URL https://arxiv. org/abs/2210.03350.</p>
<p>Jack Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d’Autume, Yujia Li, Tayfun Terzi, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, James Bradbury, Matthew Johnson, Laura Weidinger, Iason Gabriel, William Isaac, Ed Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving. Scaling language models: Methods, analysis \&amp; insights from training Gopher. arXiv submission, 2021.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-totext transformer. Journal of Machine Learning Research, 21(140):1-67, 2020. URL http: //jmlr.org/papers/v21/20-074.html.</p>
<p>Nazneen Fatema Rajani, Bryan McCann, Caiming Xiong, and Richard Socher. Explain yourself! leveraging language models for commonsense reasoning. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 4932-4942, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1487. URL https: //aclanthology.org/P19-1487.</p>
<p>Gabriel Recchia. Teaching autoregressive language models complex tasks by demonstration. arXiv preprint arXiv:2109.02102, 2021.</p>
<p>Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algorithms. Cambridge university press, 2014.</p>
<p>Shai Shalev-Shwartz and Amnon Shashua. On the sample complexity of end-to-end training vs. semantic abstraction training. arXiv preprint arXiv:1604.06915, 2016.</p>
<p>Shai Shalev-Shwartz, Ohad Shamir, and Shaked Shammah. Failures of gradient-based deep learning. In International Conference on Machine Learning, pp. 3067-3075. PMLR, 2017.</p>
<p>Ohad Shamir. Distribution-specific hardness of learning neural networks. The Journal of Machine Learning Research, 19(1):1135-1163, 2018.</p>
<p>M Sipser. Introduction to the theory of computation. 3th. Cengage Learning, 2013.
Burrhus F Skinner. Reinforcement today. American Psychologist, 13(3):94, 1958.
Leslie G Valiant. A theory of the learnable. Communications of the ACM, 27(11):1134-1142, 1984.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.</p>
<p>Lifu Wang, Bo Shen, Bo Hu, and Xing Cao. On the provable generalization of recurrent neural networks. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, volume 34, pp. 20258-20269. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper/2021/file/ a928731e103dfc64c0027fa84709689e-Paper.pdf.</p>
<p>Po-Wei Wang, Priya Donti, Bryan Wilder, and Zico Kolter. Satnet: Bridging deep learning and logical reasoning using a differentiable satisfiability solver. In International Conference on Machine Learning, pp. 6545-6554. PMLR, 2019.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/ forum?id=_VjQlMeSB_J.</p>
<p>Noam Wies, Yoav Levine, Daniel Jannai, and Amnon Shashua. Which transformer architecture fits my data? a vocabulary bottleneck in self-attention. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 11170-11181. PMLR, 18-24 Jul 2021. URL https://proceedings.mlr.press/v139/wies21a.html.</p>
<p>Ronald J Williams and David Zipser. A learning algorithm for continually running fully recurrent neural networks. Neural computation, 1(2):270-280, 1989.</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 38-45, Online, October 2020. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/ 2020.emnlp-demos.6.</p>
<p>Eric Zelikman, Yuhuai Wu, and Noah D. Goodman. Star: Bootstrapping reasoning with reasoning, 2022. URL https://arxiv.org/abs/2203.14465.</p>
<p>Yuexiang Zhai, Christina Baek, Zhengyuan Zhou, Jiantao Jiao, and Yi Ma. Computational benefits of intermediate rewards for goal-reaching policy learning. Journal of Artificial Intelligence Research, 73:847-896, 2022.</p>
<h1>A COMPOUNDED SEQUENCE TO SEQUENCE LEARNABILITY DETAILS</h1>
<p>We start by formally defining our sequence-to-sequence functions complexity measure as:</p>
<p>$$
\phi(T, \psi, N):=\tilde{O}\left(T^{16+3 N+\max <em t="t">{t} \operatorname{deg}\left(\psi</em> \max }\right)} C^{2 N<em t="t">{t} \operatorname{deg}\left(\psi</em> \max }\right)^{3 N<em i="i" t_="t,">{t, i}\left|a</em>\right)
$$}\right|^{2</p>
<p>Where $C&gt;0$ is some constant.
Now we prove lemma 1 from the main text. Essentially this lemma applies the union bound to show that when the zero-one loss of all the single-hop sub-tasks is low, then also at test time - when the model does not have the ground truth results of the previous sub-task and errors may accumulate the zero-one loss on the final answer is still low.</p>
<p>Proof. Denote by $\epsilon$ the zero-one loss for $\mathbf{z}^{\text {train }}$, i.e., the right hand side in eq 5. Clearly, for any $\mathbf{x}$ when $f^{\mathrm{RNN}, W}\left(\mathbf{z}^{\text {train }}\right)$ solves all the sub-tasks correctly we have that $\mathbf{z}^{\text {test }}=\mathbf{z}^{\text {train }}$ and therefore $l_{0-1}\left(y, f_{T}^{\mathrm{RNN}, W}\left(\mathbf{z}^{\text {test }}\right)\right)=0$. So it is enough to upper bound the probability that $f^{\mathrm{RNN}, W}\left(\mathbf{z}^{\text {train }}\right)$ makes an error in any sub-task by $\epsilon$. But by the zero-one loss definition, for any $t$ we have that:</p>
<p>$$
P_{\mathbf{x}}\left(f_{t}^{\mathrm{RNN}, W}\left(\mathbf{z}^{\text {train }}\right) \neq y_{t}\right)=\mathbb{E}<em 0-1="0-1">{\mathbf{x}}\left[l</em>\right)\right)\right]
$$}\left(y_{t}, f_{t}^{\mathrm{RNN}, W}\left(\mathbf{z}^{\text {train }</p>
<p>And therefore $\sum_{t=d}^{T} P_{\mathbf{x}}\left(f_{t}^{\mathrm{RNN}, W}\left(\mathbf{z}^{\text {train }}\right) \neq y_{t}\right)=\epsilon$. Finally, by the union bound we got that</p>
<p>$$
P_{\mathbf{x}}\left(\exists d \leq t \leq T \quad f_{t}^{\mathrm{RNN}, W}\left(\mathbf{z}^{\text {train }}\right) \neq y_{t}\right) \leq \sum_{t=d}^{T} P_{\mathbf{x}}\left(f_{t}^{\mathrm{RNN}, W}\left(\mathbf{z}^{\text {train }}\right) \neq y_{t}\right)=\epsilon
$$</p>
<h2>B Sub Tasks Learnability Proofs</h2>
<p>In this section we prove lemmas 2,3 from the main text, i.e., we prove that our intermediate steps are simple enough to be covered by theorem 2 .</p>
<p>We begin by formally describing the details of learning parities with sequence-to-sequence models. Since sequence-to-sequence models expect their inputs to be sequences, we will translate the tree described in section 5 into a sequence by inverse BFS like tree traversal, and concatenate the result sequence after $\mathbf{x}$. Therefore, our inputs sequence includes all the $d$ variables in $\mathbf{x}$, together with all the sub-tasks decomposition nodes in the binary tree except the root (that represent $y$ ). So we will have an input sequence length $T$ that is equal to:</p>
<p>$$
T:=d+\text { nodes in full binary tree with } \frac{d}{4} \text { leaves }-1=\frac{3}{2} d-2
$$</p>
<p>And the ground-truth sub-task results are recursively defined by:</p>
<p>$$
\forall t \geq d \quad y_{t}= \begin{cases}(-1)^{x_{t_{2(t-d)+1}}+x_{t_{2(t-d)+2}}} &amp; t&lt;\frac{5}{4} d \ (-1)^{y_{2}\left(\epsilon-\frac{5 d}{4}\right)+d}+y_{2\left(\epsilon-\frac{5 d}{4}\right)+d+1}} &amp; \text { else }\end{cases}
$$</p>
<p>For $t&gt;d$, at training time, $z_{t}$ will be the ground-truth sub-task result $y_{t-1}$. At test time $z_{t}$ will be the model prediction at time $t-1$ :</p>
<p>$$
\forall t \in[T] \quad z_{t}= \begin{cases}x_{t} &amp; t \leq d \ \frac{1}{2}+\frac{1}{2} \operatorname{sign}\left(f_{t-1}^{\mathrm{RNN}}\left(z_{1}, \cdots, z_{t-1}\right)\right) &amp; t&gt;d \wedge \text { test } \ \frac{1+y_{t-1}}{2} &amp; t&gt;d \wedge \text { training }\end{cases}
$$</p>
<p>Note that $f^{\mathrm{RNN}}$ is causal model, i.e. $f_{t_{1}}^{\mathrm{RNN}}$ does not depend on $\mathbf{z}<em 2="2">{t</em>$, and therefore eq 15 is well defined.}}$ for any $t_{2}&gt;t_{1</p>
<p>Now, we prove lemma 2 from the main text. Essentially this lemma shows that the intermediate steps of length 2 parities belong to the hypothesis class define in eq 6 .</p>
<p>Proof. Define $\mathbf{w}:=\frac{1}{\sqrt{2}}\left(\begin{array}{c}1 \ 0 \ 1 \ 0\end{array}\right)$, then</p>
<p>$$
\left\langle\frac{\mathbf{w}}{|\mathbf{w}|},\binom{\mathbf{e}<em 1="1">{z</em>}}}{\mathbf{e<em 2="2">{z</em>
$$}}}\right\rangle= \begin{cases}\sqrt{2} &amp; z_{1}=0 \wedge z_{2}=0 \ \frac{1}{\sqrt{2}} &amp; z_{1} \neq z_{2} \ 0 &amp; z_{1}=1 \wedge z_{2}=1\end{cases</p>
<p>Therefore, it is enough to find $\psi$ such that</p>
<p>$$
\psi(0)=\psi(\sqrt{2})=1 \wedge \psi\left(\frac{1}{\sqrt{2}}\right)=-1
$$</p>
<p>Finally, we will use Lagrange basis functions to find the required polynomial and get:</p>
<p>$$
\begin{aligned}
\psi(z) &amp; =\left(\frac{z-\frac{1}{\sqrt{2}}}{0-\frac{1}{\sqrt{2}}}\right)\left(\frac{z-\sqrt{2}}{0-\sqrt{2}}\right)-\left(\frac{z-0}{\frac{1}{\sqrt{2}}-0}\right)\left(\frac{z-\sqrt{2}}{\frac{1}{\sqrt{2}}-\sqrt{2}}\right)+\left(\frac{z-0}{\sqrt{2}-0}\right)\left(\frac{z-\frac{1}{\sqrt{2}}}{\sqrt{2}-\frac{1}{\sqrt{2}}}\right) \
&amp; =1 \cdot\left(z-\frac{1}{\sqrt{2}}\right)(z-\sqrt{2})+\frac{z}{2}(z-\sqrt{2})+3 z\left(z-\frac{1}{\sqrt{2}}\right) \
&amp; =\left(z^{2}-\frac{3}{\sqrt{2}} z+1\right)+\left(\frac{z^{2}}{2}-\frac{1}{\sqrt{2}} z\right)+\left(3 z^{2}-\frac{3 z}{\sqrt{2}}\right) \
&amp; =\frac{9}{2} z^{2}-\frac{7}{\sqrt{2}} z+1
\end{aligned}
$$</p>
<p>Now we prove lemma 3 from the main text. Essentially this lemma shows that also our intermediate steps for any functions in the $\mathbf{P}$ time complexity class belongs to the hypothesis class define in eq 6 .</p>
<p>Proof. Denote $\alpha\left(z_{1}, \ldots, z_{N}\right):=\sum_{i=0}^{N-1} 2^{i} \cdot 1_{z_{i}=0}$ the function that converts $N$ bits to their binary string, and define $\mathbf{w}:=\sqrt{\frac{3}{4^{N}-1}}\left(\begin{array}{c}2^{0} \ 0 \ 2^{1} \ 0 \ \vdots \ 2^{N-1} \ 0\end{array}\right)$, then $\mathbf{w}$ is a unit vector that represents $z_{1}, \ldots, z_{N}$ as $N$ bit numbers $\left\langle\frac{\mathbf{w}}{|\mathbf{w}|},\left(\begin{array}{c}\mathbf{e}<em 1="1">{z</em>}} \ \vdots \ \mathbf{e<em N="N">{z</em>\right)$. Now, we can use the Lagrange basis functions to find the required polynomial:}}\end{array}\right)\right\rangle=\alpha\left(z_{1}, \ldots, z_{N</p>
<p>$$
\psi(x)=\sum_{z_{1}, \ldots, z_{N}=0}^{1}\left(f_{v}\left(z_{1}, \ldots, z_{N}\right) \prod_{\left(\tilde{z}<em N="N">{1}, \ldots, \tilde{z}</em>}\right) \neq\left(z_{1}, \ldots, z_{N}\right)}\left(\frac{x-\alpha\left(\tilde{z<em N="N">{1}, \ldots, \tilde{z}</em>}\right)}{\alpha\left(z_{1}, \ldots, z_{N}\right)-\alpha\left(\tilde{z<em N="N">{1}, \ldots, \tilde{z}</em>\right)\right)
$$}\right)</p>
<p>Finally the $O(1)$ coefficients boundedness follows from taking the maximum ${ }^{6}$ over all possible $f_{v}$ functions.</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>C EXTENSION FOR SGD WITH FINITE PRECISION</h1>
<p>In this section, we prove theorem 2 from the main text holds also for algorithm 2 which is a finiteprecision variant of $\mathrm{SGD}^{2} \ldots$ We will follow the proof in Wang et al. (2021) while taking into account the finite precision gradients.</p>
<div class="codehilite"><pre><span></span><code><span class="n">Algorithm</span><span class="w"> </span><span class="mi">2</span><span class="p">:</span><span class="w"> </span><span class="n">Training</span><span class="w"> </span>\<span class="p">(</span><span class="n">f</span><span class="o">^</span><span class="p">{</span>\<span class="n">mathrm</span><span class="p">{</span><span class="n">RNN</span><span class="p">}}</span>\<span class="p">)</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">finite</span><span class="w"> </span><span class="n">precision</span><span class="w"> </span><span class="n">SGD</span><span class="w"> </span><span class="p">(</span><span class="n">an</span><span class="w"> </span><span class="n">finite</span><span class="w"> </span><span class="n">precisio</span><span class="w"> </span><span class="n">variant</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">algorithm</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span>
<span class="n">Data</span><span class="p">:</span><span class="w"> </span><span class="n">Data</span><span class="w"> </span><span class="n">set</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">D</span><span class="p">}</span>\<span class="p">),</span><span class="w"> </span><span class="n">learning</span><span class="w"> </span><span class="n">rate</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">eta</span>\<span class="p">),</span><span class="w"> </span><span class="n">finite</span><span class="w"> </span><span class="n">precision</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">sigma</span>\<span class="p">)</span><span class="o">.</span>
<span class="n">Initialization</span><span class="p">:</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">entries</span><span class="w"> </span><span class="n">of</span><span class="w"> </span>\<span class="p">(</span><span class="n">W</span><span class="o">^</span><span class="p">{(</span><span class="mi">0</span><span class="p">)},</span><span class="w"> </span><span class="n">A</span><span class="p">,</span><span class="w"> </span><span class="n">M_</span><span class="p">{</span><span class="mi">0</span><span class="p">}</span>\<span class="p">)</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">generated</span><span class="w"> </span><span class="n">i</span><span class="o">.</span><span class="n">i</span><span class="o">.</span><span class="n">d</span><span class="o">.</span><span class="w"> </span><span class="n">from</span><span class="w"> </span>\<span class="p">(</span><span class="n">N</span>\<span class="n">left</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span>\<span class="n">frac</span><span class="p">{</span><span class="mi">2</span><span class="p">}{</span><span class="n">m</span><span class="p">}</span>\<span class="n">right</span><span class="p">)</span>\<span class="p">)</span><span class="o">.</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">entries</span><span class="w"> </span><span class="n">of</span><span class="w"> </span>\<span class="p">(</span><span class="n">B</span>\<span class="p">)</span>
<span class="n">are</span><span class="w"> </span><span class="n">generated</span><span class="w"> </span><span class="n">i</span><span class="o">.</span><span class="n">i</span><span class="o">.</span><span class="n">d</span><span class="o">.</span><span class="w"> </span><span class="n">from</span><span class="w"> </span>\<span class="p">(</span><span class="n">N</span>\<span class="n">left</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span>\<span class="n">frac</span><span class="p">{</span><span class="mi">1</span><span class="p">}{</span><span class="n">m</span><span class="p">}</span>\<span class="n">right</span><span class="p">)</span>\<span class="p">)</span><span class="o">.</span>
<span class="k">for</span><span class="w"> </span>\<span class="p">(</span><span class="n">i</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="w"> </span>\<span class="n">ldots</span><span class="w"> </span><span class="n">n</span>\<span class="p">)</span><span class="w"> </span><span class="n">do</span>
<span class="w">    </span><span class="n">Randomly</span><span class="w"> </span><span class="n">sample</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">left</span><span class="p">(</span>\<span class="n">mathbf</span><span class="p">{</span><span class="n">x</span><span class="p">}</span><span class="n">_</span><span class="p">{</span><span class="n">i</span><span class="p">},</span><span class="w"> </span>\<span class="n">mathbf</span><span class="p">{</span><span class="n">y</span><span class="p">}</span><span class="n">_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span>\<span class="n">right</span><span class="p">)</span>\<span class="p">)</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="n">set</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">D</span><span class="p">}</span>\<span class="p">)</span><span class="o">.</span>
<span class="w">    </span><span class="n">Get</span><span class="w"> </span><span class="n">arbitrary</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">sigma</span>\<span class="p">)</span><span class="o">-</span><span class="n">approximation</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">gradient</span><span class="p">:</span>
<span class="w">    </span>\<span class="p">(</span><span class="n">G</span><span class="o">^</span><span class="p">{(</span><span class="n">i</span><span class="p">)}</span><span class="w"> </span>\<span class="ow">in</span><span class="w"> </span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">B</span><span class="p">}</span><span class="n">_</span><span class="p">{</span>\<span class="n">infty</span><span class="p">}{</span><span class="w"> </span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="n">k</span><span class="p">}</span>\<span class="n">left</span><span class="p">(</span>\<span class="n">nabla_</span><span class="p">{</span><span class="n">W</span><span class="o">^</span><span class="p">{(</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">)}}</span><span class="w"> </span>\<span class="n">ell</span>\<span class="n">left</span><span class="p">(</span>\<span class="n">mathbf</span><span class="p">{</span><span class="n">y</span><span class="p">}</span><span class="n">_</span><span class="p">{</span><span class="n">i</span><span class="p">},</span><span class="w"> </span><span class="n">f</span><span class="o">^</span><span class="p">{</span>\<span class="n">mathrm</span><span class="p">{</span><span class="n">RNN</span><span class="p">},</span><span class="w"> </span><span class="n">W</span><span class="o">^</span><span class="p">{(</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">)}}</span>\<span class="n">left</span><span class="p">(</span>\<span class="n">mathbf</span><span class="p">{</span><span class="n">z</span><span class="p">}</span><span class="n">_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span>\<span class="n">right</span><span class="p">)</span>\<span class="n">right</span><span class="p">),</span><span class="w"> </span>\<span class="n">sigma</span>\<span class="n">right</span><span class="p">)</span>\<span class="p">)</span><span class="o">.</span>
<span class="w">    </span><span class="n">Update</span><span class="w"> </span><span class="n">weights</span><span class="p">:</span>
<span class="w">    </span>\<span class="p">(</span><span class="n">W</span><span class="o">^</span><span class="p">{(</span><span class="n">i</span><span class="p">)}</span><span class="o">=</span><span class="n">W</span><span class="o">^</span><span class="p">{(</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">)}</span><span class="o">-</span>\<span class="n">eta</span><span class="w"> </span><span class="n">G</span><span class="o">^</span><span class="p">{(</span><span class="n">i</span><span class="p">)}</span>\<span class="p">)</span><span class="o">.</span>
<span class="n">end</span>
</code></pre></div>

<p>We begin by stating theorem $2^{9}$ in Wang et al. (2021) with our notations:
Theorem 3. Let $\delta&gt;0$, and assume we run algorithm 1 for $n$ iterations with learning rate $\eta=\frac{1}{m \sqrt{n}}$. Then there exists $m^{\star}=\operatorname{poly}\left(n, \delta^{-1}, T\right)$ such that if $m&gt;m^{\star}$ then for any $h \in \mathcal{H}_{\phi(T, \psi, N)}$ with probability at least $1-\delta$ over the randomness in algorithm 1, the following hold:</p>
<p>$$
\mathbb{E}<em t="d">{\mathbf{x}}\left[\left(\frac{1}{n(T-d)}\right) \sum</em>\right)
$$}^{T} \sum_{i=1}^{n} l_{0-1}\left(y_{t}, f_{t}^{R N N, W^{(i)}}(\mathbf{z})\right)\right]&lt;\tilde{O}\left(\frac{\phi(T, \psi, N)}{\sqrt{n}}\right)+O\left(\frac{\log \frac{1}{\delta}}{\sqrt{n}</p>
<p>where $W^{[i]}$ denote the output of algorithm 1 at the $i$ 'th iteration and $l_{0-1}$ is the zero-one loss.
Clearly when using infinite precision SGD theorem 2 follows from theorem 3 and lemma 1 by simple algebraic manipulations. Therefore, for proving theorem 2 with finite precision gradient based optimization, it is enough to modify theorem's 3 proof to analyze algorithm 2 that uses finite precision gradients, instead of algorithm 1 that uses full precision gradients.
While complicated, Wang et al. (2021)'s proof for theorem 3 can be divided into two high-level arguments. The first argument measure the complexity of the learned hypothesis class with respect to random initialization of $f^{\mathrm{RNN}}$ (lemma 6). And the second argument is a generalization bound for algorithm 1 with networks that are overparameterized enough. Since the first argument is independent of the gradients, the proof of the first argument still holds and we only need to prove a generalization bound for algorithm 2. More specifically we only need to prove a lemma that is equivalent to lemma 14 in Wang et al. (2021) and the rest of the second argument (lemma 7 in Wang et al. (2021)) remain unchanged.
Lemma 4. Let $n \in \mathbb{N}$, and denote by $L_{i}(W):=l\left(\mathbf{y}^{(i)}, f^{R N N, W}\left(\mathbf{z}^{(i)}\right)\right)$ the training loss. Suppose there exists $W^{\star} \in \mathcal{B}^{10}\left(W^{(0)}, \frac{R}{\sqrt{m}}\right)$ such that $R=O(\operatorname{poly}(T))=\Omega\left(T^{16}\right)$ and $L_{i}\left(W^{\star}\right) \leq \frac{1+R^{2}}{n}$. Then for any $\delta&gt;0$ there exists $m^{\star}=\operatorname{poly}\left(n, R, T, \delta^{-1}\right)$ such that if $m&gt;m^{\star}$ then with probability at least $1-\delta$ algorithm 2 with $\eta=\frac{1}{m \sqrt{n}}$ and finite precision $\sigma=O\left(\frac{1}{m}\right)$ will output:</p>
<p>$$
\mathbb{E}<em t="d">{\mathbf{x}}\left[\left(\frac{1}{n(T-d)}\right) \sum</em>\right)
$$}^{T} \sum_{i=1}^{n} l_{0-1}\left(y_{t}, f_{t}^{R N N, W^{(i)}}(\mathbf{z})\right)\right]&lt;O\left(\frac{R^{2}+\log \frac{1}{\delta}}{\sqrt{n}</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Proof. We begin by showing that with high probability over the initialization of $f^{\mathrm{RNN}}$, at any iteration $i \leq n$ of algorithm 2 , the distance of the learned hidden weights matrix $W^{(i)}$ from its initialization point $W^{(0)}$ is not too large. As a results we will get that the assumption of lemma 8 uphold, and therefore its upper bound of the deviation from linearization is valid.
By the triangle inequality for any $0 \leq i&lt;n$ we have that:</p>
<p>$$
\left|W^{(i+1)}-W^{(0)}\right|<em k="0">{F} \leq \sum</em>
$$}^{i}\left|W^{(k+1)}-W^{(k)}\right|_{F</p>
<p>Substituting algorithm 2 update rule for $W^{(k+1)}$, we get that there exist $\left|\sigma_{i}\right|_{\infty}&lt;\sigma$ such that:</p>
<p>$$
W^{(k+1)}-W^{(k)}=-\eta\left(\nabla_{W^{(k)}} \ell\left(\mathbf{y}<em k="k">{k}, f^{\mathrm{RNN}, W^{(k)}}\left(\mathbf{z}</em>\right)
$$}\right)\right)+\sigma_{k</p>
<p>Now, explicitly writing $\nabla_{W^{(k)}} \ell\left(\mathbf{y}<em k="k">{k}, f^{\mathrm{RNN}, W^{(k)}}\left(\mathbf{z}</em>\right)\right)$ with the chain rule we have that:</p>
<p>$$
\nabla_{W^{(k)}} \ell\left(\mathbf{y}<em k="k">{k}, f^{\mathrm{RNN}, W^{(k)}}\left(\mathbf{z}</em>}\right)\right)=\frac{1}{T-d}\left(\sum_{t=d}^{T}\left(\frac{-y_{t}^{(k)} \cdot e^{-y_{t}^{(k)} f_{t}^{\mathrm{RNN}, W}\left(\mathbf{z<em t="t">{k}\right)}}{1+e^{-y</em>}^{(k)} f_{t}^{\mathrm{RNN}, W}\left(\mathbf{z<em W_k_="W^{(k)">{k}\right)}}\right) \cdot\left(\nabla</em>\right)\right)\right)
$$}} f_{t}^{\mathrm{RNN}, W^{(k)}}\left(\mathbf{z}_{k</p>
<p>and since $0 \leq \frac{x}{1+x} \leq 1$ for any $x \geq 0$, we conclude that:</p>
<p>$$
\left|\nabla_{W^{(k)}} \ell\left(\mathbf{y}<em k="k">{k}, f^{\mathrm{RNN}, W^{(k)}}\left(\mathbf{z}</em>\right)\right)\right|<em T="T" _leq="\leq" d="d" t="t">{F} \leq \max </em>}\left|\nabla_{W^{(k)}} f_{t}^{\mathrm{RNN}, W^{(k)}}\left(\mathbf{z<em F="F">{k}\right)\right|</em>
$$</p>
<p>Now we will use an induction over $i$, to show that $\left|W^{(i+1)}-W^{(0)}\right|<em T="T" _leq="\leq" d="d" t="t">{F}=O\left(\frac{(i+1) T^{8}}{m \cdot \sqrt{n}}\right)$ for any $0 \leq i&lt;n$. By the induction hypothesis, lemma 10 assure us that for wide enough networks $m^{\star}=\Omega\left(\max \left{T^{6} \log ^{3}\left(\frac{n \cdot T}{2}\right), \sqrt{n} T^{8}\right}\right)$, with probability of at least $1-\delta$ over the initialization of $f^{\mathrm{RNN}}, \max </em>}\left|\nabla_{W^{(k)}} f_{t}^{\mathrm{RNN}, W^{(k)}}\left(\mathbf{z<em F="F">{k}\right)\right|</em>\right)$. Therefore, in this case}=O\left(T^{8</p>
<p>$$
\left|W^{(k+1)}-W^{(k)}\right|_{F}=\eta O\left(T^{8}+\frac{1}{m}\right)=O\left(\frac{T^{8}}{m \cdot \sqrt{n}}+\frac{1}{m^{2} \sqrt{n}}\right)
$$</p>
<p>and hence $\left|W^{(i+1)}-W^{(0)}\right|_{F}=O\left(\frac{(i+1) T^{8}}{m \cdot \sqrt{n}}\right)$ as required.
Now after we showed the assumptions of lemma 8 upholds, we can use it to obtain first order Taylor approximation of the training loss:</p>
<p>$$
\begin{aligned}
L_{i}\left(W^{(i)}\right)-L_{i}\left(W^{\star}\right) &amp; \leq\left\langle\nabla_{W^{(i)}} \ell\left(\mathbf{y}<em i="i">{i}, f^{\mathrm{RNN}, W^{(i)}}\left(\mathbf{z}</em>\right\rangle \
&amp; +\max }\right)\right), W^{(i)}-W^{\star<em t="t">{d \leq t \leq T} \underbrace{\left|\frac{y</em>} \cdot e^{-y_{t} f^{\mathrm{RNN}, W}\left(\mathbf{z<em t="t">{i}\right)}}{1+e^{-y</em>} f^{\mathrm{RNN}, W}\left(\mathbf{z<em 1="1" _leq="\leq">{i}\right)}}\right|}</em>
\end{aligned}
$$} \cdot O\left(\left(\frac{R}{\sqrt{m}}\right)^{\frac{1}{3}} T^{10} \sqrt{m}(\log m)\right)\left|W^{(i)}-W^{\star}\right|_{F</p>
<p>Where we assumed that $m^{\star}&gt;n$ and therefore $\left|W^{(i)}-W^{(0)}\right|_{F}&lt;O\left(\frac{R}{\sqrt{m}}\right)$.
Using algorithm 2 update rule for $W^{(i+1)}$ again (see eq 26), we can use an inequality from in ShalevShwartz \&amp; Ben-David (2014)'s lemma 14.1 to get that:</p>
<p>$$
\begin{array}{r}
\sum_{i=1}^{n}\left\langle\nabla_{W^{(i)}} \ell\left(\mathbf{y}<em i="i">{i}, f^{\mathrm{RNN}, W^{(i)}}\left(\mathbf{z}</em>\right\rangle \
\leq \frac{\left|W^{(1)}-W^{\star}\right|}\right)\right)+\sigma_{i}, W^{(i)}-W^{\star<em i="1">{F}^{2}}{2 \eta}+\frac{\eta}{2} \sum</em>}^{n}\left|\nabla_{W^{(i)}} \ell\left(\mathbf{y<em i="i">{i}, f^{\mathrm{RNN}, W^{(i)}}\left(\mathbf{z}</em>
\end{array}
$$}\right)\right)+\sigma_{i}\right|_{F}^{2</p>
<p>Now combing with Cauchy-Schwarz inequality we have that:</p>
<p>$$
\begin{aligned}
\sum_{i=1}^{n}\left(L_{i}\left(W^{(i)}\right)-L_{i}\left(W^{\star}\right)\right) &amp; \leq \frac{\left|W^{(1)}-W^{\star}\right|<em i="1">{F}^{2}}{2 \eta}+\frac{\eta}{2} \sum</em>}^{n}\left|\nabla_{W^{(k)}} \ell\left(\mathbf{y<em k="k">{k}, f^{\mathrm{RNN}, W^{(k)}}\left(\mathbf{z}</em>\right|}\right)\right)+\sigma_{k<em i="1">{F}^{2} \
&amp; +O\left(\left(\frac{R}{\sqrt{m}}\right)^{\frac{1}{3}} T^{10} \sqrt{m}(\log m)\right)\left(\sum</em>\right|}^{n}\left|W^{(i)}-W^{\star<em i="1">{F}\right) \
&amp; -\sum</em>\right\rangle
\end{aligned}
$$}^{n}\left\langle\sigma_{i}, W^{(i)}-W^{\star</p>
<p>Substituting the upper bounds from eqs 28,29 and using the assumption that $R=\Omega\left(T^{16}\right)$ we get that:</p>
<p>$$
\begin{gathered}
\sum_{i=1}^{n}\left(L_{i}\left(W^{(i)}\right)-L_{i}\left(W^{\star}\right)\right) \leq \frac{O\left(R^{2}+T^{16}\right)}{2 \eta m}+\frac{\eta \cdot n}{2} O\left(T^{8}+1\right)^{2} \
+O\left(\left(\frac{R}{\sqrt{m}}\right)^{\frac{1}{3}} T^{10} n(\log m)\right)\left(R+n T^{8}\right)+O\left(m^{-\frac{3}{2}}\left(n^{2} T^{8}+n R\right)\right) \
\leq O\left(R^{2} \sqrt{n}\right)+O\left(\left(\frac{R}{\sqrt{m}}\right)^{\frac{1}{3}} T^{10} n(\log m)\right)\left(R+n T^{8}\right)+O\left(m^{-\frac{3}{2}} n^{2} R\right)
\end{gathered}
$$</p>
<p>To ensure the left hand side is upper bounded by $O\left(R^{2} \sqrt{n}\right)$ we will chose $m^{\star}$ such that $\frac{m^{\star}}{\left(\log ^{3} m^{\star}\right)}&gt;n^{\frac{3}{2}}$, note that, as required, $m^{\star}$ is polynomial in $n, T$. Then for $m&gt;m^{\star}$ we have that $\sum_{i=1}^{n} L_{i}\left(W^{(i)}\right) \leq O\left(R^{2} \sqrt{n}\right)$. Therefore,</p>
<p>$$
\frac{1}{n} \sum_{i=1}^{n} L_{i}\left(W^{(i)}\right) \leq O\left(\frac{R^{2}}{\sqrt{n}}\right)
$$</p>
<p>Now, to prove generalization bound we will follow lemma 4.3 in Ji \&amp; Telgarsky (2020) and use a martingale Bernstein bound argument. We begin by showing that during the whole training process, our binary cross entropy loss is bounded. Indeed lemma 9 assure us that :</p>
<p>$$
\max <em T="T" _leq="\leq" d_t="d&lt;t">{\mathbf{x} \in{0,1}^{d}} \max </em>\right)
$$}\left|f_{t}^{\mathrm{RNN}, W^{(i)}}(\mathbf{z})\right|=O\left(T^{14} \cdot \sqrt{\frac{n}{m}}+T\right) \leq O\left(T^{14</p>
<p>Therefore, their exist a constant $C&gt;0$ such that the binary cross entropy loss is bounded by $\log \left(1+e^{O\left(T^{14}\right)}\right) \leq C \cdot T^{14}$.
Now, we will define a bounded martingle. For any $i \geq 0$, let $s_{i}$ denote $\left(\mathbf{x}<em i="i">{i}, \mathbf{y}</em>\right)$. Importantly, the quantity}\right)$ and $s_{0, i}$ denote $\left(s_{0}, \ldots, s_{i</p>
<p>$$
\frac{1}{C \cdot T^{14}}\left(\sum_{t&lt;i}\left(\mathbb{E}<em t="t">{\mathbf{x}}\left[l\left(\mathbf{y}, f^{\mathrm{RNN}, W^{(t)}}(\mathbf{z})\right)\right]-l\left(\mathbf{y}</em>\right)\right)\right)\right)
$$}, f^{\mathrm{RNN}, W^{(t)}}\left(\mathbf{z}_{t</p>
<p>is a martingal w.r.t the filration $\sigma\left(s_{0, i-1}\right)$. This martingal difference sequence is given by</p>
<p>$$
\frac{1}{C \cdot T^{14}}\left(\mathbb{E}<em t="t">{\mathbf{x}}\left[l\left(\mathbf{y}, f^{\mathrm{RNN}, W^{(t)}}(\mathbf{z})\right)\right]-l\left(\mathbf{y}</em>\right)\right)\right) \leq 1
$$}, f^{\mathrm{RNN}, W^{(t)}}\left(\mathbf{z}_{t</p>
<p>Moreover, we have</p>
<p>$$
\begin{gathered}
\mathbb{E}<em 0_="0," t="t">{s</em>}}\left[\frac{1}{C^{2} \cdot T^{28}}\left(\mathbb{E<em t="t">{\mathbf{x}}\left[l\left(\mathbf{y}, f^{\mathrm{RNN}, W^{(t)}}(\mathbf{z})\right)\right]-l\left(\mathbf{y}</em>}, f^{\mathrm{RNN}, W^{(t)}}\left(\mathbf{z<em 0_="0," i-1="i-1">{t}\right)\right)\right)^{2} \mid \sigma\left(s</em>\right)\right] \
=\frac{1}{C^{2} \cdot T^{28}}\left(\mathbb{E}<em 0_="0," t="t">{s</em>}}\left[l\left(\mathbf{y<em t="t">{t}, f^{\mathrm{RNN}, W^{(t)}}\left(\mathbf{z}</em>}\right)\right)^{2} \mid \sigma\left(s_{0, i-1}\right)\right]-\mathbb{E<em s__0_="s_{0," t="t">{\mathbf{x}}\left[l\left(\mathbf{y}, f^{\mathrm{RNN}, W^{(t)}}(\mathbf{z})\right)\right]^{2}\right) \
\leq \mathbb{E}</em>}}\left[\frac{1}{C \cdot T^{14}} l\left(\mathbf{y<em t="t">{t}, f^{\mathrm{RNN}, W^{(t)}}\left(\mathbf{z}</em>\right)\right] \
=\frac{1}{C \cdot T^{14}} \cdot \mathbb{E}_{\mathbf{x}}\left[l\left(\mathbf{y}, f^{\mathrm{RNN}, W^{(t)}}(\mathbf{z})\right)\right]
\end{gathered}
$$}\right)\right) \mid \sigma\left(s_{0, i-1</p>
<p>Therefore, by lemma C. 2 in Ji \&amp; Telgarsky (2020) we have that with probability $1-\delta$</p>
<p>$$
\begin{gathered}
\frac{1}{C \cdot T^{14}} \sum_{i=1}^{n}\left(\mathbb{E}<em i="i">{\mathbf{x}}\left[l\left(\mathbf{y}, f^{\mathrm{RNN}, W^{(i)}}(\mathbf{z})\right)\right]-l\left(\mathbf{y}</em>}, f^{\mathrm{RNN}, W^{(i)}}\left(\mathbf{z<em i="1">{i}\right)\right)\right) \
\leq \frac{1}{C \cdot T^{14}}(e-2) \cdot \sum</em>\right)
\end{gathered}
$$}^{n} \mathbb{E}_{\mathbf{x}}\left[l\left(\mathbf{y}, f^{\mathrm{RNN}, W^{(i)}}(\mathbf{z})\right)\right]+\ln \left(\frac{1}{\delta</p>
<p>And hence</p>
<p>$$
\begin{aligned}
\sum_{i=1}^{n} \mathbb{E}<em i="1">{\mathbf{x}}\left[l\left(\mathbf{y}, f^{\mathrm{RNN}, W^{(i)}}(\mathbf{z})\right)\right] &amp; \leq \frac{1}{(3-e)} \sum</em>}^{n} l\left(\mathbf{y<em i="i">{i}, f^{\mathrm{RNN}, W^{(i)}}\left(\mathbf{z}</em>\right) \
&amp; =O\left(R^{2} \sqrt{n}\right)+O\left(R \ln \left(\frac{1}{\delta}\right)\right)
\end{aligned}
$$}\right)\right)+O\left(T^{14}\right) \ln \left(\frac{1}{\delta</p>
<p>Finally, for $y_{t} \cdot f_{t}^{\mathrm{RNN}, W}(\mathbf{z})&lt;0$ we have that $\log \left(1+e^{-y_{t} \cdot f_{t}^{\mathrm{RNN}, W}(\mathbf{z})}\right)&gt;\log 2$. In addition, clearly $\log \left(1+e^{-y_{t} \cdot f_{t}^{\mathrm{RNN}, W}(\mathbf{z})}\right)&gt;0$. Therefore, we conclude that $\frac{1}{T-d} \sum_{t=d}^{T} l_{o-1}\left(y_{t}, f_{t}^{\mathrm{RNN}, W^{(i)}}(\mathbf{z})\right)&lt;$ $\frac{1}{\log 2} l\left(\mathbf{y}, f^{\mathrm{RNN}, W^{(i)}}(\mathbf{z})\right)$ and thus eq 24 uphold.</p>
<h1>D EXTENSION FOR GD WITH FINITE PRECISION</h1>
<p>In this section, we prove theorem 2 from the main text still holds when using the gradient descent based algorithm 3, instead of the stochastic gradient descent based algorithm 2. . Establishing our positive results that the parities task is efficiently learnable with sub-task decomposition supervision in the exact same setting of the negative results that show that learning is impossible without intermediate supervision, presented in section F. We will follow the proof in section C while taking into account the full non-stochastic gradients.</p>
<div class="codehilite"><pre><span></span><code>Algorithm 3: Training \(f^{\mathrm{RNN}}\) with finite precision GD
Data: Data set \(\mathcal{D}\), learning rate \(\eta\), finite precision \(\sigma\).
Initialization: The entries of \(W^{(0)}, A, M_{0}\) are generated i.i.d. from \(N\left(0, \frac{2}{m}\right)\). The entries of \(B\)
    are generated i.i.d. from \(N\left(0, \frac{1}{m}\right)\).
for \(i=1,2,3 \ldots n\) do
    Get arbitrary \(\sigma\)-approximation of the gradient:
    \(G^{(i)} \in \mathcal{B}_{\infty}{ }^{11}\left(\mathbb{E}_{\mathbf{x}}\left[\nabla_{W^{(i-1)}} \ell\left(\mathbf{y}, f^{\mathrm{RNN}, W^{(i-1)}}(\mathbf{z})\right)\right], \sigma\right)\).
    Update weights:
    \(W^{(i)}=W^{(i-1)}-\eta G^{(i)}\).
end
</code></pre></div>

<p>We begin by sampling a fake training set denoted by $\mathbf{x}^{(1)}, \ldots, \mathbf{x}^{(n)}, \mathbf{y}^{(1)}, \ldots, \mathbf{y}^{(n)}$. Essentially, we will apply the same reasoning as in section $C$ with this fake training points. As in the finite precision SGD case it is enough to prove a lemma that is equivalent to lemma 4 in section C.
Lemma 5. Let $n \in \mathbb{N}$, and denote by $L_{i}(W):=l\left(\mathbf{y}^{(i)}, f^{\text {RNN }, W}\left(\mathbf{z}^{(i)}\right)\right)$ the training loss. Suppose there exists $W^{\star} \in \mathcal{B}^{12}\left(W^{(0)}, \frac{R}{\sqrt{m}}\right)$ such that $R=O(\operatorname{poly}(T))=\Omega\left(T^{16}\right)$ and $L_{i}\left(W^{\star}\right) \leq \frac{1+R^{2}}{n}$. Then for any $\delta&gt;0$ there exists $m^{\star}=\operatorname{poly}\left(n, R, T, \delta^{-1}\right)$ such that if $m&gt;m^{\star}$ then with probability at least $1-\delta$ algorithm 3 with $\eta=\frac{1}{m \sqrt{n}}$ and finite precision $\sigma=O\left(\frac{1}{m}\right)$ will output:</p>
<p>$$
\mathbb{E}<em _ell="d">{\mathbf{x}}\left[\left(\frac{1}{n(T-d)}\right) \sum</em>\right)
$$}^{T} \sum_{i=1}^{n} l_{0-1}\left(y_{t}, f_{t}^{\text {RNN }, W^{(i)}}(\mathbf{z})\right)\right]&lt;O\left(\frac{R^{2}+\log \frac{1}{\delta}}{\sqrt{n}</p>
<p><sup id="fnref8:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Proof. We begin by showing that there exists $\tilde{W} \in \mathcal{B}\left(W^{(0)}, \frac{T^{8}}{\sqrt{m}}\right)$ such that:</p>
<p>$$
\mathbb{E}_{\mathbf{x}}\left[l\left(\mathbf{y}, f^{\mathrm{RNN}, \tilde{W}}(\mathbf{z})\right)\right] \leq O\left(\frac{R^{2}}{\sqrt{n}}\right)
$$</p>
<p>Indeed, since the minimum can not be larger than the mean, eq'50 in the proof of lemma 4 assure us that under the assumptions of lemma 5 , for $m&gt;\max \left{n^{2}, \ln ^{4}\left(\frac{1}{\delta}\right)\right}$, with probability of at least $1-\delta$ algorithm 2 will reach such $\tilde{W}$ during the first $\max \left{n, \ln ^{2}\left(\frac{1}{\delta}\right)\right}$ SGD iteration.
Now we shows that that with high probability over the initialization of $f^{\mathrm{RNN}}$, at any iteration $i \leq n$ of algorithm 3, the distance of the learned hidden weights matrix $W^{(i)}$ from its initialization point $W^{(0)}$ is not too large. As a results we will get that the assumption of lemma 8 uphold, and therefore its upper bound of the deviation from linearization is valid.
By the triangle inequality for any $0 \leq i&lt;n$ we have that:</p>
<p>$$
\left|W^{(i+1)}-W^{(0)}\right|<em k="0">{F} \leq \sum</em>
$$}^{i}\left|W^{(k+1)}-W^{(k)}\right|_{F</p>
<p>Substituting algorithm 3 update rule for $W^{(k+1)}$, we get that there exist $\left|\sigma_{i}\right|_{\infty}&lt;\sigma$ such that:</p>
<p>$$
W^{(k+1)}-W^{(k)}=-\eta\left(\mathbb{E}<em W_k_="W^{(k)">{\mathbf{x}}\left[\nabla</em>\right)
$$}} \ell\left(\mathbf{y}, f^{\mathrm{RNN}, W^{(k)}}(\mathbf{z})\right)\right]+\sigma_{k</p>
<p>Now, explicitly writing $\nabla_{W^{(k)}} \ell\left(\mathbf{y}, f^{\mathrm{RNN}, W^{(k)}}(\mathbf{z})\right)$ with the chain rule we have that:</p>
<p>$$
\nabla_{W^{(k)}} \ell\left(\mathbf{y}, f^{\mathrm{RNN}, W^{(k)}}(\mathbf{z})\right)=\frac{1}{T-d}\left(\sum_{t=d}^{T}\left(\frac{-y_{t} \cdot e^{-y_{t} f_{t}^{\mathrm{RNN}, W}(\mathbf{z})}}{1+e^{-y_{t} f_{t}^{\mathrm{RNN}, W}(\mathbf{z})}}\right) \cdot\left(\nabla_{W^{(k)}} f_{t}^{\mathrm{RNN}, W^{(k)}}(\mathbf{z})\right)\right)
$$</p>
<p>and since $0 \leq \frac{x}{1+x} \leq 1$ for any $x \geq 0$, we conclude by Jensen's inequality that:</p>
<p>$$
\left|\mathbb{E}<em W_k_="W^{(k)">{\mathbf{x}}\left[\nabla</em>)\right)\right]\right|}} \ell\left(\mathbf{y}, f^{\mathrm{RNN}, W^{(k)}}(\mathbf{z<em t="d">{F} \leq \frac{1}{T-d} \sum</em>}^{T} \mathbb{E<em W_k_="W^{(k)">{\mathbf{x}}\left[\left|\nabla</em>\right]
$$}} f_{t}^{\mathrm{RNN}, W^{(k)}}(\mathbf{z})\right|_{F</p>
<p>Now we will use an induction over $i$, to show that $\left|W^{(i+1)}-W^{(0)}\right|<em _mathbf_x="\mathbf{x">{F}=O\left(\frac{(i+1) T^{8}}{m \cdot \sqrt{n}}\right)$ for any $0 \leq i&lt;n$. By the induction hypothesis, lemma 10 assure us that for wide enough networks $m^{\star}=\Omega\left(\max \left{T^{7} \log ^{3}\left(\frac{n \cdot T}{\delta}\right), \sqrt{n} T^{8}\right}\right)$, with probability of at least $1-\delta$ over the initialization of $f^{\mathrm{RNN}}, \mathbb{E}</em>\right)$ for any $d \leq t \leq T$. Therefore, in this case}}\left[\left|\nabla_{W^{(k)}} f_{t}^{\mathrm{RNN}, W^{(k)}}(\mathbf{z})\right|_{F}\right]=O\left(T^{8</p>
<p>$$
\left|W^{(k+1)}-W^{(k)}\right|_{F}=\eta O\left(T^{8}+1\right)=O\left(\frac{T^{8}}{m \cdot \sqrt{n}}+\frac{1}{m^{2} \sqrt{n}}\right)
$$</p>
<p>and hence $\left|W^{(i+1)}-W^{(0)}\right|_{F}=O\left(\frac{(i+1) T^{8}}{m \cdot \sqrt{n}}\right)$ as required.
Now after we showed the assumptions of lemma 11 upholds, we can use it to obtain first order Taylor approximation of the training loss for any $\mathbf{x}$ :</p>
<p>$$
\begin{aligned}
&amp; l\left(\mathbf{y}, f^{\mathrm{RNN}, W^{(i)}}(\mathbf{z})\right)-l\left(\mathbf{y}, f^{\mathrm{RNN}, \tilde{W}}(\mathbf{z})\right) \leq\left\langle\nabla_{W^{(i)}} \ell\left(\mathbf{y}, f^{\mathrm{RNN}, W^{(i)}}(\mathbf{z})\right), W^{(i)}-\tilde{W}\right\rangle \
&amp; \quad+\max <em t="t">{d \leq t \leq T} \underbrace{\left|\frac{y</em>} \cdot e^{-y_{t} f^{\mathrm{RNN}, W}(\mathbf{z})}}{1+e^{-y_{t} f^{\mathrm{RNN}, W}(\mathbf{z})}}\right|<em F="F">{\leq 1} \cdot O\left(\left(\frac{T^{8}}{\sqrt{m}}\right)^{\frac{1}{3}} T^{10} \sqrt{m}(\log m)\right)\left|W^{(i)}-\tilde{W}\right|</em>
\end{aligned}
$$</p>
<p>Where we assumed that $m^{\star}&gt;n$ and therefore $\left|W^{(i)}-W^{(0)}\right|_{F}&lt;O\left(\frac{T^{8}}{\sqrt{m}}\right)$.</p>
<p>Using algorithm 3 update rule for $W^{(k+1)}$ again (see eq 55), we can use an inequality from ShalevShwartz \&amp; Ben-David (2014)'s section 14.1.1 to get that</p>
<p>$$
\begin{array}{r}
\sum_{i=1}^{n}\left\langle\mathbb{E}<em W_i_="W^{(i)">{\mathbf{x}}\left[\nabla</em>\right\rangle \
\frac{\left|W^{(1)}-\tilde{W}\right|}} \ell\left(\mathbf{y}, f^{\mathrm{RNN}, W^{(i)}}(\mathbf{z})\right)\right]+\sigma_{i}, W^{(i)}-\tilde{W<em i="1">{F}^{2}}{2 \eta}+\frac{\eta}{2} \sum</em>}^{n}\left|\mathbb{E<em W_i_="W^{(i)">{\mathbf{x}}\left[\nabla</em>
\end{array}
$$}} \ell\left(\mathbf{y}, f^{\mathrm{RNN}, W^{(i)}}(\mathbf{z})\right)\right]+\sigma_{i}\right|_{F}^{2</p>
<p>Now, we can take expectation over eq 59, and combine the Cauchy-Schwarz inequality with the above bound and get that:</p>
<p>$$
\begin{aligned}
&amp; \sum_{i=1}^{n} \mathbb{E}<em F="F">{\mathbf{x}}\left[l\left(\mathbf{y}, f^{\mathrm{RNN}, W^{(i)}}(\mathbf{z})\right)-l\left(\mathbf{y}, f^{\mathrm{RNN}, \tilde{W}}(\mathbf{z})\right)\right] \leq \frac{\left|W^{(1)}-\tilde{W}\right|</em> \
&amp; \quad+\frac{\eta}{2} \sum_{i=1}^{n}\left|\mathbb{E}}^{2}}{2 \eta<em W_i_="W^{(i)">{\mathbf{x}}\left[\nabla</em>\right|}} \ell\left(\mathbf{y}, f^{\mathrm{RNN}, W^{(i)}}(\mathbf{z})\right)\right]+\sigma_{i<em i="1">{F}^{2}+O\left(\left(\frac{T^{8}}{\sqrt{m}}\right)^{\frac{1}{3}} T^{10} \sqrt{m}(\log m)\right)\left(\sum</em>\right)
\end{aligned}
$$}^{n}\left|W^{(i)}-\tilde{W}\right|_{F</p>
<p>Substituting the upper bounds from eqs 57, 58 we get that:</p>
<p>$$
\begin{aligned}
\sum_{i=1}^{n} \mathbb{E}_{\mathbf{x}}\left[l\left(\mathbf{y}, f^{\mathrm{RNN}, W^{(i)}}(\mathbf{z})\right)\right. &amp; \left.-l\left(\mathbf{y}, f^{\mathrm{RNN}, W^{*}}(\mathbf{z})\right)\right] \leq \frac{O\left(T^{16}\right)}{2 \eta m}+\frac{\eta \cdot n}{2} O\left(T^{8}+1\right)^{2} \
&amp; +O\left(\left(\frac{T^{8}}{\sqrt{m}}\right)^{\frac{1}{3}} T^{10} n(\log m)\right)\left(2 n T^{8}\right)+O\left(m^{-\frac{3}{2}}\left(n^{2} T^{8}\right)\right) \
&amp; \leq O\left(T^{16} \sqrt{n}\right)+O\left(m^{-\frac{1}{6}} T^{\frac{n 2}{3}} n^{2}(\log m)\right)+O\left(m^{-\frac{3}{2}} n^{2} T^{8}\right)
\end{aligned}
$$</p>
<p>To ensure the left hand side is upper bounded by $O\left(R^{2} \sqrt{n}\right)$ we will chose $m^{\star}$ such that $\frac{m^{\star}}{\left(\log ^{2} m^{\star}\right)}&gt;n^{\frac{6}{2}}$, note that, as required, $m^{\star}$ is polynomial in $n, T$. Then since eq 53 assure us that $\mathbb{E}<em i="1">{\mathbf{x}}\left[l\left(\mathbf{y}, f^{\mathrm{RNN}, \tilde{W}}(\mathbf{z})\right)\right] \leq O\left(\frac{R^{2}}{\sqrt{n}}\right)$, we have that $\sum</em>$. Therefore,}^{n} \mathbb{E}_{\mathbf{x}}\left[l\left(\mathbf{y}, f^{\mathrm{RNN}, W^{(i)}}(\mathbf{z})\right)\right] \leq O\left(R^{2} \sqrt{n}\right)$ for $m&gt;m^{\star</p>
<p>$$
\frac{1}{n} \sum_{i=1}^{n} \mathbb{E}_{\mathbf{x}}\left[l\left(\mathbf{y}, f^{\mathrm{RNN}, W^{(i)}}(\mathbf{z})\right)\right] \leq O\left(\frac{R^{2}}{\sqrt{n}}\right)
$$</p>
<p>Finally, for $y_{t} \cdot f_{t}^{\mathrm{RNN}, W}(\mathbf{z})&lt;0$ we have that $\log \left(1+e^{-y_{t} \cdot f_{t}^{\mathrm{RNN}, W}(\mathbf{z})}\right)&gt;\log 2$. In addition, clearly $\log \left(1+e^{-y_{t} \cdot f_{t}^{\mathrm{RNN}, W}(\mathbf{z})}\right)&gt;0$. Therefore, we conclude that $\frac{1}{T-d} \sum_{t=d}^{T} l_{o-1}\left(y_{t}, f_{t}^{\mathrm{RNN}, W^{(i)}}(\mathbf{z})\right)&lt;$ $\frac{1}{\log 2} l\left(\mathbf{y}, f^{\mathrm{RNN}, W^{(i)}}(\mathbf{z})\right)$ and thus eq 52 uphold.</p>
<h1>E Learnability Of RNNs</h1>
<p>In this section we state and extend several lemmas from Wang et al. (2021) and Allen-Zhu et al. (2019). We will use this lemmas in sections C,D for extending theorem 2 in the main text for finite precision SGD and GD.</p>
<p>Following Wang et al. (2021)'s notations, for any target function $h \in \mathcal{H}<em i="1">{\phi(T, \psi, N)}$ and $n$ samples $\left(\mathbf{z}^{(i)}\right)</em>$ we will denote:}^{n</p>
<p>$$
\mathbf{H}<em W_0_="W^{(0)">{i, j}^{t}:=\frac{1}{m}\left\langle\nabla</em>\right)\right\rangle
$$}} f_{t}^{\mathrm{RNN}, W^{(0)}}\left(\mathbf{z}^{(i)}\right), \nabla_{W^{(0)}} f_{t}^{\mathrm{RNN}, W^{(0)}}\left(\mathbf{z}^{(j)</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{11}$ The ball is with respect to the distance that defined by the max matrix norm, i.e. the elementwise distances are at most $\sigma$.
${ }^{12}$ The ball is with respect to the distance that defined by the Frobenius matrix norm.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref8:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
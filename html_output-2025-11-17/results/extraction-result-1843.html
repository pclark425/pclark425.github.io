<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1843 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1843</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1843</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-33.html">extraction-schema-33</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <p><strong>Paper ID:</strong> paper-38939304bb760473141c2aca0305e44fbe04e6e8</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/38939304bb760473141c2aca0305e44fbe04e6e8" target="_blank">RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control</a></p>
                <p><strong>Paper Venue:</strong> Conference on Robot Learning</p>
                <p><strong>Paper TL;DR:</strong> This work proposes a simple, general recipe to enable a single end-to-end trained model to both learn to map robot observations to actions and enjoy the benefits of large-scale pretraining on language and vision-language data from the web.</p>
                <p><strong>Paper Abstract:</strong> We study how vision-language models trained on Internet-scale data can be incorporated directly into end-to-end robotic control to boost generalization and enable emergent semantic reasoning. Our goal is to enable a single end-to-end trained model to both learn to map robot observations to actions and enjoy the benefits of large-scale pretraining on language and vision-language data from the web. To this end, we propose to co-fine-tune state-of-the-art vision-language models on both robotic trajectory data and Internet-scale vision-language tasks, such as visual question answering. In contrast to other approaches, we propose a simple, general recipe to achieve this goal: in order to fit both natural language responses and robotic actions into the same format, we express the actions as text tokens and incorporate them directly into the training set of the model in the same way as natural language tokens. We refer to such category of models as vision-language-action models (VLA) and instantiate an example of such a model, which we call RT-2. Our extensive evaluation (6k evaluation trials) shows that our approach leads to performant robotic policies and enables RT-2 to obtain a range of emergent capabilities from Internet-scale training. This includes significantly improved generalization to novel objects, the ability to interpret commands not present in the robot training data (such as placing an object onto a particular number or icon), and the ability to perform rudimentary reasoning in response to user commands (such as picking up the smallest or largest object, or the one closest to another object). We further show that incorporating chain of thought reasoning allows RT-2 to perform multi-stage semantic reasoning, for example figuring out which object to pick up for use as an improvised hammer (a rock), or which type of drink is best suited for someone who is tired (an energy drink).</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1843.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1843.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RT-2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RT-2 (Vision-Language-Action models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of vision-language-action models obtained by co-fine-tuning large pretrained vision-language models on Internet-scale vision-language data together with robot trajectory (behavior cloning) data, where robot actions are tokenized as text and produced by the model at inference for closed-loop control.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>RT-2 (family: RT-2-PaLI-X, RT-2-PaLM-E, RT-2-PaLI-3B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>VLA models created by taking large pretrained vision-language models (PaLI-X and PaLM-E variants) and co-fine-tuning them on robot trajectory data while retaining VLM tasks; models output action tokens (discretized motor deltas + gripper + terminate) as text in the same token space as natural language.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Vision-language web data (image-text pairs, captioning, VQA) and language tasks</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Based on the original VLM training mixtures: WebLI (approx. 10B image-text pairs filtered to ~1B high-quality examples), plus many captioning and VQA datasets used in Chen et al. (2023a) and Driess et al. (2023). Models used: PaLI-X (multilingual VLM) and PaLM-E (embodied multimodal LLM) pretrained on those mixtures; sizes evaluated include PaLI-X at 5B and 55B parameters, PaLM-E at 12B, and PaLI 3B for a simulation experiment.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Real-world robotic manipulation (RT-1 robot dataset) and Language-Table simulated manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Real robot tasks use trajectories collected with a 7DoF mobile manipulator across 13 robots over 17 months (Brohan et al., 2022) covering skills: Pick Object, Move Object Near Object, Place Object Upright, Knock Object Over, Open/Close Drawer, Place Into Receptacle, Pick from Receptacle then place on counter; evaluation includes seen tasks and generalization tests (unseen objects, backgrounds, environments). Also evaluated in the Language-Table 2D simulated environment for discrete 2D delta actions.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Pretraining tasks produce natural language tokens (captions, VQA answers); action outputs are expressed as sequences of text tokens representing discretized numeric bins (i.e. actions encoded as tokenized integers or overwritten tokens).</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>6-DoF end-effector delta position and rotation + gripper extension + discrete termination command; continuous dimensions discretized into 256 bins per dimension (encoded as 8 integer tokens concatenated into a string).</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Continuous robot action dimensions uniformly discretized into 256 bins; action bins mapped into model tokens by (a) associating integer tokens (PaLI-X tokenizers provide integer tokens) or (b) overwriting 256 least-frequent vocabulary tokens (PaLM-E) to represent action bins; actions concatenated into a single string (e.g. "terminate Δpos_x Δpos_y Δpos_z Δrot_x Δrot_y Δrot_z gripper") and produced autoregressively. During decoding for robot tasks, output vocabulary is constrained so only action tokens are sampled.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>RGB images processed by ViT-based image encoders (ViT variants used inside PaLI-X and PaLM-E); models accept one or more images as input; no requirement for calibrated depth in the primary presented setup.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>RT-2 models substantially improved generalization: on average ≈2× improvement over the next-best baselines (RT-1 and MOO) on held-out object/background/environment generalization tasks, and ≈6× over other baselines; in the Language-Table simulated benchmark RT-2-PaLI-3B achieved 90 ± 10% success vs baselines in the 72–77% range; RT-2-PaLI-X reported >3× average success on emergent semantic skill evaluations compared to RT-1.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>Baselines without VLM-based co-fine-tuning (e.g., RT-1) had similar in-distribution (seen task) performance but markedly worse out-of-distribution generalization; exact baseline numbers vary by split but RT-2 shows roughly 2× better generalization success than RT-1/MOO and much larger gains versus other representation baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>The paper gives training step counts used for co-fine-tuning (RT-2-PaLI-X-55B: 80K gradient steps with batch size 2048; RT-2-PaLI-X-5B: 270K steps; RT-2-PaLM-E-12B: 1M steps; RT-2-PaLI-3B for Language-Table: 300K steps). The robotics dataset proportion in the co-finetuning mixture: ~50% for RT-2-PaLI-X, ~66% for RT-2-PaLM-E. The paper does not report an explicit number of robot trajectories/episodes needed to reach a given success threshold, so episode/sample counts to convergence are not specified.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td>Not specified as explicit episode/sample counts in the paper. Baseline RT-1 and other baselines were trained on the same robotic data; the paper reports relative performance but does not report per-method sample-efficiency curves or episode counts to threshold.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td>No numeric sample-efficiency multiplier is reported; gains are described qualitatively as improved generalization and emergent capabilities from VLM pretraining rather than acquisition of new motion skills. Co-fine-tuning (keeping web data during finetuning) was shown to be more effective than naive finetuning on robot data alone.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Key contributors: (1) shared token space enabling actions to be expressed in the same output modality as language (symbol tuning), (2) co-fine-tuning on both web VLM data and robot trajectories which prevents forgetting and preserves semantic knowledge, (3) large-scale VLM pretraining (WebLI and diverse VQA/caption data) that imparts open-vocabulary and semantic reasoning, and (4) larger model capacity (bigger PaLI-X improved generalization).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Limits include: (1) the model does not acquire new physical motions beyond the distribution in the robot data (pretraining provides semantics but not new motor skills), (2) compute/inference cost and latency for very large models (55B runs at 1–3 Hz; 5B at ~5 Hz), (3) small number of publicly available open VLMs and fine-tuning APIs, and (4) no explicit depth or proprioceptive modalities required by pretraining, potentially leaving spatial/physical gaps.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Large vision-language pretraining can be directly leveraged for closed-loop robotic control by representing low-level actions as text tokens and co-fine-tuning VLMs with robot trajectories; this yields major gains in semantic generalization and emergent reasoning capabilities (symbol understanding, basic relational and math reasoning, person recognition) while preserving in-distribution task competence — however, physical skill breadth remains constrained by robot data and significant compute/latency costs persist.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1843.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1843.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RT-2-PaLI-X</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RT-2-PaLI-X (RT-2 built on PaLI-X)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An instantiation of RT-2 that fine-tunes PaLI-X VLM weights (available at multiple sizes) to output tokenized robot actions while co-training on original PaLI-X web-scale tasks and robot trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>RT-2-PaLI-X (5B and 55B variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Uses PaLI-X vision-language backbone (ViT + encoder-decoder UL2-like backbone) at multiple parameter counts (5B and 55B); action tokens encoded using PaLI-X's tokenizer where integers up to 1000 each have unique tokens, mapping discretized action bins to integer tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Multilingual vision-language web data (image-text pairs, VQA, captioning)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Built on PaLI-X pretraining mixtures (WebLI ~10B raw image-text pairs filtered to ~1B plus other captioning/VQA datasets). Co-fine-tuning used the PaLI-X web mixture (except Episodic WebLI) plus robotics demonstrations; robotics portion weighted ≈50% in the co-fine-tuning mixture.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Real-world manipulation tasks (RT-1 dataset) and out-of-distribution generalization evaluations</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Same set of robot skills as RT-2 overall: pick, place, move near, knock, open/close drawers, pick from/in receptacle; extensive real-world generalization tests on unseen objects, backgrounds, and environments separated into easy/hard splits.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>VLM output tokens (natural language) and integer tokens representing discretized action bins (PaLI-X tokenizer supports integer tokens up to 1000).</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>6-DoF positional and rotational deltas + gripper extension + terminate command; continuous dims discretized into 256 bins each, represented as 8 integer tokens forming the action string.</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Direct mapping of discretized action bin indices to PaLI-X integer tokens; action string constructed by concatenating tokens for each action dimension with spaces; output constrained to action-token vocabulary for robot tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>RGB images processed via PaLI-X's ViT encoder (ViT-22B in full PaLI-X architecture description; smaller variants use smaller ViTs).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>RT-2-PaLI-X models attained large improvements in generalization: average ≈2× improvement over RT-1/MOO baselines and >3× on emergent semantic tasks vs RT-1; larger 55B model outperformed smaller 5B in generalization ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>A PaLI-X model trained from scratch on robot data performed poorly (even the 5B model trained from scratch performed poorly), and naive finetuning only on robot data underperformed co-fine-tuning; RT-1 baseline had similar in-distribution performance but substantially worse generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>Co-fine-tuning regimen: RT-2-PaLI-X-55B: learning rate 1e-3, batch size 2048, 80K gradient steps; RT-2-PaLI-X-5B: same LR and batch size, 270K steps. The robotics dataset was weighted to ~50% of the mixture. Exact number of robot episodes required is not specified in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td>Not reported as episode counts; models trained from scratch on the robot data (no VLM pretraining) required the same training procedure but performed poorly even at 5B, indicating pretraining is critical; quantitative episode counts not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td>No explicit numeric sample-efficiency factor reported; co-fine-tuning with VLM pretraining delivered markedly better generalization than finetuning from scratch or finetuning VLM only on robot data.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Availability of integer tokens in PaLI-X tokenizer enabled straightforward mapping of discretized action bins; strong image-language pretraining provides semantic and open-vocabulary understanding useful for generalization; co-fine-tuning preserves VLM capabilities while learning action mapping.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Training from scratch failed even at large scale; improvements rely on retaining VLM pretraining (co-fine-tuning) and sufficient model capacity; physical skill repertoire still limited by the diversity of robot data.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>PaLI-X-based RT-2 benefits strongly from VLM pretraining and model scaling: co-fine-tuning PaLI-X with robot actions (using integer-token action encoding) yields substantially better out-of-distribution generalization and emergent semantic behaviors than baselines or training from scratch; larger models generalize better.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1843.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1843.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RT-2-PaLM-E</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RT-2-PaLM-E (RT-2 built on PaLM-E)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An RT-2 variant that starts from PaLM-E, a decoder-only embodied multimodal LLM that projects visual inputs and other continuous modalities into language token space, then co-fine-tunes to output robot action tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>RT-2-PaLM-E (12B variant)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Uses PaLM-E-12B (decoder-only LLM with ViT visual projector) as the backbone; PaLM-E maps images into the language embedding space and is fine-tuned together with web VLM tasks and robot trajectories to emit tokenized actions (action tokens created by overwriting least-frequent tokens).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Multimodal (vision + language) pretraining and LLM-style language pretraining; VLM and language tasks including VQA and captioning mixtures from Driess et al. (2023).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>PaLM-E weights from Driess et al. (2023) pretrained on an interwoven mix of visual and language tasks; co-fine-tuning mixture weighted robotics dataset to ≈66% for RT-2-PaLM-E experiments. Pretraining mixture includes datasets that improve language/math reasoning capabilities in PaLM-E.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Real-world robotic manipulation (RT-1 dataset), emergent reasoning evaluations</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Same real-world manipulation suite (pick/place/knock/open/close) and emergent tasks assessing symbol understanding, reasoning (including math), and human recognition; PaLM-E variant showed relative strengths on math reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Language token output space of the decoder-only LLM; action tokens implemented by overwriting 256 least-frequent vocabulary tokens to represent discretized bins.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>6-DoF end-effector delta + gripper extension + terminate flag discretized into 256 bins per continuous dimension, encoded as concatenated tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Overwrite the 256 least-frequently used tokens in the PaLM-E vocabulary to serve as action-bin tokens, then generate action strings of those tokens; decoding constrained to action-token subset for robot action steps.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>RGB images projected via PaLM-E's ViT visual projector (ViT-4B in used PaLM-E-12B instantiation); multimodal inputs are concatenated into token space.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>RT-2-PaLM-E performed comparably overall and showed an edge in certain hard generalization splits and math/reasoning tasks, consistent with PaLM-E's pretraining mixture; overall RT-2 models (including PaLM-E) achieved large gains in generalization over baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>Baseline models (RT-1 etc.) underperformed in generalization and emergent reasoning compared to RT-2-PaLM-E; exact numeric baselines vary by split and task and are reported qualitatively in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>Co-fine-tuning hyperparameters reported: RT-2-PaLM-E-12B used learning rate 4e-4, batch size 512, co-fine-tuned for 1M gradient steps; robotics data comprised ~66% of training mixture. No explicit robot-episode-to-threshold counts provided.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td>Not provided as episode counts; compared to finetuning-only ablations, co-fine-tuning (keeping web data) provided better generalization, indicating pretraining reduces the need to rely solely on robot data diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td>No explicit numeric sample-efficiency factor is provided; qualitative gains in generalization and certain reasoning tasks (e.g., math) are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>PaLM-E's multimodal/LLM-style pretraining (including language and reasoning capabilities) supported transfer of math/multilingual reasoning to embodied tasks; vocabulary overwrite approach enabled action encoding within the same token space; co-fine-tuning mixture weighting preserved useful LLM capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Similar constraints as other RT-2 variants: no acquisition of new motion primitives beyond robot data; inference latency and compute for large models; overwriting tokens is a form of symbol tuning that requires careful fine-tuning to avoid interference.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>PaLM-E-based RT-2 inherits PaLM-E's language and reasoning strengths (notably math capabilities) and, when co-fine-tuned with robot data while overwriting tokens for action bins, achieves strong generalization and emergent reasoning in embodied tasks while retaining the same limitations on physical skill breadth.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1843.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1843.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RT-2 (CoT variant)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RT-2 with chain-of-thought (Plan + Action) fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A small modification of RT-2 training data that prepends a natural-language 'Plan:' step before the action tokens, encouraging the model to generate an explicit textual plan (chain-of-thought) followed by the action string.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>RT-2 (chain-of-thought / Plan-Action augmentation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>RT-2 variant (PaLM-E based in experiments) fine-tuned for a few hundred gradient steps with examples that include an intermediate natural-language 'Plan' sentence before the action tokens, enabling multi-stage reasoning and improved semantic decision-making.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Same vision-language pretraining (PaLM-E) with additional fine-tuning on plan-annotated robot trajectories (language-augmented actions).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Augmented robot trajectory examples where each example contains 'Instruction: ... Plan: <natural language plan>. Action: <action tokens>'; this augmentation was applied for a few hundred gradient steps on the PaLM-E RT-2 variant.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Real-world semantic reasoning manipulation (qualitative rollouts and emergent reasoning evaluations)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Complex semantic tasks requiring multi-step reasoning (e.g., choose an improvised hammer: pick a rock; decide which drink suits a tired person). The Plan step allows the model to produce an explicit natural-language rationale before generating the low-level action tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Natural language plan followed by action tokens in the same text token space.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Same 6-DoF discretized action token format as baseline RT-2 models.</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Same discretization and token overwrite/integer mapping as RT-2; augment training examples with 'Plan:' natural language prefix that the model must generate before action tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>RGB vision as in other RT-2 variants; the Plan step leverages the model's VLM/LLM capabilities to reason from images and instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>Qualitatively observed to produce more sophisticated command handling and multi-stage semantic inferences; no large-scale quantitative numbers reported for the chain-of-thought variant, though rollouts and examples show improved semantic reasoning in complex tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>Baseline RT-2 (without Plan augmentation) can perform semantic tasks but chain-of-thought augmentation enabled more complex multi-step reasoning qualitatively; no numeric comparison to quantify magnitude.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>Chain-of-thought fine-tuning performed for a few hundred gradient steps on top of existing RT-2 weights; paper does not report episode counts or sample-efficiency curves for this variant.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td>Not quantified numerically; qualitative evidence suggests improved reasoning with modest additional fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Providing a natural-language planning slot leverages VLM/LLM pretraining and scaffolds multi-step reasoning that can then be grounded in the same token space for actions.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Quantitative benefits and generalization of the CoT variant are not fully characterized (no large-scale numeric evaluation), and additional fine-tuning steps are required.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Augmenting robot examples with an explicit natural-language 'Plan' before actions elicits more sophisticated semantic and multi-stage reasoning in RT-2, demonstrating that the language capabilities of pretrained VLMs/LLMs can be used as an internal planning representation for embodied control.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1843.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1843.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaLI-X</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaLI-X (multilingual vision-language model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large multilingual vision-language model (VLM) used as the base VLM for RT-2-PaLI-X; PaLI-X provides ViT visual encoders and encoder-decoder text generation backbones trained on Web-scale image-text mixtures.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Pali-x: On scaling up a multilingual vision and language model</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>PaLI-X (pretrained VLM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Vision-language architecture combining ViT image encoders and an encoder-decoder language generation backbone (UL2-like); trained on large multilingual vision-language mixtures enabling VQA, captioning, and related tasks; used as initialization for RT-2-PaLI-X.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Vision-language web image-text pairs (WebLI) and VQA/captioning datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>WebLI (~10B image-text pairs filtered to ~1B high-quality examples) plus other captioning and VQA datasets as in Chen et al. (2023a); models available at different scales including PaLI-3B, PaLI-X 5B and 55B variants.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Used as base for RT-2-PaLI-X fine-tuned to robotic manipulation tasks (see RT-2 entries)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Fine-tuned with robot trajectory data to produce low-level action tokens for real-world manipulation tasks (pick/place/knock/open/close) and evaluated on generalization and emergent reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Natural language and integer tokens (PaLI-X tokenizer provides unique tokens for integers up to 1000, enabling direct mapping from discretized action bins to integer tokens).</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>6-DoF discretized end-effector deltas + gripper extension + terminate token encoded as integer tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Map discretized bin indices to PaLI-X integer tokens; action string concatenation and vocabulary-constrained decoding for action steps.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>RGB images processed by ViT encoders inside PaLI-X (ViT variants depending on model size).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>When used as RT-2-PaLI-X, provided strong semantic generalization and emergent behavior; larger PaLI-X-based RT-2 (55B) yielded better generalization than smaller variants and than baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>Training comparable architectures from scratch on robot data performed poorly even at 5B scale, indicating the pretraining was critical.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>PaLI-X based co-fine-tuning schedules: see RT-2-PaLI-X entry (80K steps for 55B, 270K for 5B); robotics mixture approx. 50%. No explicit episode counts.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td>Not reported as explicit episode counts; models trained from scratch failed to match co-fine-tuned performance.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td>Not numerically specified; pretraining is necessary to obtain good generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Integer-token-aware tokenizer allowed straightforward action token mapping; extensive image-text pretraining supplied semantic priors used in downstream embodied tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Pretraining does not replace the need for diverse robot data to learn new motions; large compute/inference costs.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>PaLI-X's tokenizer and VLM pretraining make it a practical backbone for direct action generation when combined with discretized-action token mapping and co-fine-tuning; pretraining is essential to achieve strong generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1843.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1843.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaLM-E</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaLM-E (embodied multimodal language model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A decoder-only multimodal LLM that projects visual inputs and other continuous modalities into the language token space, pretrained on multimodal and language data and used as an RT-2 backbone.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>palm-e: An embodied multimodal language model</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>PaLM-E (pretrained multimodal LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Decoder-only LLM architecture that accepts projected visual embeddings and other modalities concatenated into the language token space; capable of high-level planning/LM-style reasoning and multimodal understanding; used as the base for RT-2-PaLM-E.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Multimodal pretraining mixing vision-language tasks and language/LLM-style training (VQA, captioning, other web and multimodal tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Pretraining mixture described by Driess et al. (2023) including image-text tasks and multimodal supervision; PaLM-E-12B uses ViT-4B visual projector as the visual front-end.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Used as a backbone for RT-2-PaLM-E to generate low-level actions for real-world robot manipulation and for chain-of-thought style planning experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Fine-tuned to output discretized action tokens for the RT-1 robotic tasks; chain-of-thought fine-tuning added Plan steps to enable multi-stage semantic reasoning for embodied tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Language token space of the decoder LLM; action tokens implemented by overwriting least-frequent vocabulary tokens to represent discretized action bins.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Same 6-DoF discretized end-effector action representation as other RT-2 variants.</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Overwrite 256 least-frequent tokens in the PaLM-E vocabulary to represent action-bin indices; produce concatenated action token strings; constrain decoding to action tokens for action steps.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>RGB images projected by a ViT visual projector (ViT-4B) into the language embedding space; concatenation of modalities into token stream.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>PaLM-E based RT-2 showed competitive generalization and an advantage on math/multilingual reasoning tasks relative to PaLI-X based RT-2, consistent with PaLM-E's pretraining strengths; co-fine-tuning produced robust policies.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>Finetuning PaLM-E from scratch on robot data was not evaluated; RT-2 variants without co-fine-tuning underperformed.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>RT-2-PaLM-E-12B co-fine-tuned for 1M gradient steps with batch size 512 and robotics mixture ~66%; no explicit robot episode counts reported.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td>Not numerically quantified; qualitative improvements in emergent reasoning and generalization noted.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Strong language and reasoning priors from PaLM-E pretraining (including math capabilities) and the ability to project images into language space facilitate transfer of high-level semantic reasoning to low-level action selection when co-fine-tuned.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Action mapping requires token overwrite which could interfere with other language capabilities if not carefully co-fine-tuned; inference latency and computational cost for large models.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>PaLM-E's LLM-style multimodal pretraining provides complementary reasoning strengths (e.g., math) when transferred to embodied control via tokenized actions and co-fine-tuning; architecture supports chain-of-thought style planning that can be grounded in actions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Pali-x: On scaling up a multilingual vision and language model <em>(Rating: 2)</em></li>
                <li>palm-e: An embodied multimodal language model <em>(Rating: 2)</em></li>
                <li>Rt-1: Robotics transformer for real-world control at scale <em>(Rating: 2)</em></li>
                <li>Robotics transformer 2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1843",
    "paper_id": "paper-38939304bb760473141c2aca0305e44fbe04e6e8",
    "extraction_schema_id": "extraction-schema-33",
    "extracted_data": [
        {
            "name_short": "RT-2",
            "name_full": "RT-2 (Vision-Language-Action models)",
            "brief_description": "A family of vision-language-action models obtained by co-fine-tuning large pretrained vision-language models on Internet-scale vision-language data together with robot trajectory (behavior cloning) data, where robot actions are tokenized as text and produced by the model at inference for closed-loop control.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_agent_name": "RT-2 (family: RT-2-PaLI-X, RT-2-PaLM-E, RT-2-PaLI-3B)",
            "model_agent_description": "VLA models created by taking large pretrained vision-language models (PaLI-X and PaLM-E variants) and co-fine-tuning them on robot trajectory data while retaining VLM tasks; models output action tokens (discretized motor deltas + gripper + terminate) as text in the same token space as natural language.",
            "pretraining_data_type": "Vision-language web data (image-text pairs, captioning, VQA) and language tasks",
            "pretraining_data_details": "Based on the original VLM training mixtures: WebLI (approx. 10B image-text pairs filtered to ~1B high-quality examples), plus many captioning and VQA datasets used in Chen et al. (2023a) and Driess et al. (2023). Models used: PaLI-X (multilingual VLM) and PaLM-E (embodied multimodal LLM) pretrained on those mixtures; sizes evaluated include PaLI-X at 5B and 55B parameters, PaLM-E at 12B, and PaLI 3B for a simulation experiment.",
            "embodied_task_name": "Real-world robotic manipulation (RT-1 robot dataset) and Language-Table simulated manipulation",
            "embodied_task_description": "Real robot tasks use trajectories collected with a 7DoF mobile manipulator across 13 robots over 17 months (Brohan et al., 2022) covering skills: Pick Object, Move Object Near Object, Place Object Upright, Knock Object Over, Open/Close Drawer, Place Into Receptacle, Pick from Receptacle then place on counter; evaluation includes seen tasks and generalization tests (unseen objects, backgrounds, environments). Also evaluated in the Language-Table 2D simulated environment for discrete 2D delta actions.",
            "action_space_text": "Pretraining tasks produce natural language tokens (captions, VQA answers); action outputs are expressed as sequences of text tokens representing discretized numeric bins (i.e. actions encoded as tokenized integers or overwritten tokens).",
            "action_space_embodied": "6-DoF end-effector delta position and rotation + gripper extension + discrete termination command; continuous dimensions discretized into 256 bins per dimension (encoded as 8 integer tokens concatenated into a string).",
            "action_mapping_method": "Continuous robot action dimensions uniformly discretized into 256 bins; action bins mapped into model tokens by (a) associating integer tokens (PaLI-X tokenizers provide integer tokens) or (b) overwriting 256 least-frequent vocabulary tokens (PaLM-E) to represent action bins; actions concatenated into a single string (e.g. \"terminate Δpos_x Δpos_y Δpos_z Δrot_x Δrot_y Δrot_z gripper\") and produced autoregressively. During decoding for robot tasks, output vocabulary is constrained so only action tokens are sampled.",
            "perception_requirements": "RGB images processed by ViT-based image encoders (ViT variants used inside PaLI-X and PaLM-E); models accept one or more images as input; no requirement for calibrated depth in the primary presented setup.",
            "transfer_successful": true,
            "performance_with_pretraining": "RT-2 models substantially improved generalization: on average ≈2× improvement over the next-best baselines (RT-1 and MOO) on held-out object/background/environment generalization tasks, and ≈6× over other baselines; in the Language-Table simulated benchmark RT-2-PaLI-3B achieved 90 ± 10% success vs baselines in the 72–77% range; RT-2-PaLI-X reported &gt;3× average success on emergent semantic skill evaluations compared to RT-1.",
            "performance_without_pretraining": "Baselines without VLM-based co-fine-tuning (e.g., RT-1) had similar in-distribution (seen task) performance but markedly worse out-of-distribution generalization; exact baseline numbers vary by split but RT-2 shows roughly 2× better generalization success than RT-1/MOO and much larger gains versus other representation baselines.",
            "sample_complexity_with_pretraining": "The paper gives training step counts used for co-fine-tuning (RT-2-PaLI-X-55B: 80K gradient steps with batch size 2048; RT-2-PaLI-X-5B: 270K steps; RT-2-PaLM-E-12B: 1M steps; RT-2-PaLI-3B for Language-Table: 300K steps). The robotics dataset proportion in the co-finetuning mixture: ~50% for RT-2-PaLI-X, ~66% for RT-2-PaLM-E. The paper does not report an explicit number of robot trajectories/episodes needed to reach a given success threshold, so episode/sample counts to convergence are not specified.",
            "sample_complexity_without_pretraining": "Not specified as explicit episode/sample counts in the paper. Baseline RT-1 and other baselines were trained on the same robotic data; the paper reports relative performance but does not report per-method sample-efficiency curves or episode counts to threshold.",
            "sample_complexity_gain": "No numeric sample-efficiency multiplier is reported; gains are described qualitatively as improved generalization and emergent capabilities from VLM pretraining rather than acquisition of new motion skills. Co-fine-tuning (keeping web data during finetuning) was shown to be more effective than naive finetuning on robot data alone.",
            "transfer_success_factors": "Key contributors: (1) shared token space enabling actions to be expressed in the same output modality as language (symbol tuning), (2) co-fine-tuning on both web VLM data and robot trajectories which prevents forgetting and preserves semantic knowledge, (3) large-scale VLM pretraining (WebLI and diverse VQA/caption data) that imparts open-vocabulary and semantic reasoning, and (4) larger model capacity (bigger PaLI-X improved generalization).",
            "transfer_failure_factors": "Limits include: (1) the model does not acquire new physical motions beyond the distribution in the robot data (pretraining provides semantics but not new motor skills), (2) compute/inference cost and latency for very large models (55B runs at 1–3 Hz; 5B at ~5 Hz), (3) small number of publicly available open VLMs and fine-tuning APIs, and (4) no explicit depth or proprioceptive modalities required by pretraining, potentially leaving spatial/physical gaps.",
            "key_findings": "Large vision-language pretraining can be directly leveraged for closed-loop robotic control by representing low-level actions as text tokens and co-fine-tuning VLMs with robot trajectories; this yields major gains in semantic generalization and emergent reasoning capabilities (symbol understanding, basic relational and math reasoning, person recognition) while preserving in-distribution task competence — however, physical skill breadth remains constrained by robot data and significant compute/latency costs persist.",
            "uuid": "e1843.0",
            "source_info": {
                "paper_title": "RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "RT-2-PaLI-X",
            "name_full": "RT-2-PaLI-X (RT-2 built on PaLI-X)",
            "brief_description": "An instantiation of RT-2 that fine-tunes PaLI-X VLM weights (available at multiple sizes) to output tokenized robot actions while co-training on original PaLI-X web-scale tasks and robot trajectories.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_agent_name": "RT-2-PaLI-X (5B and 55B variants)",
            "model_agent_description": "Uses PaLI-X vision-language backbone (ViT + encoder-decoder UL2-like backbone) at multiple parameter counts (5B and 55B); action tokens encoded using PaLI-X's tokenizer where integers up to 1000 each have unique tokens, mapping discretized action bins to integer tokens.",
            "pretraining_data_type": "Multilingual vision-language web data (image-text pairs, VQA, captioning)",
            "pretraining_data_details": "Built on PaLI-X pretraining mixtures (WebLI ~10B raw image-text pairs filtered to ~1B plus other captioning/VQA datasets). Co-fine-tuning used the PaLI-X web mixture (except Episodic WebLI) plus robotics demonstrations; robotics portion weighted ≈50% in the co-fine-tuning mixture.",
            "embodied_task_name": "Real-world manipulation tasks (RT-1 dataset) and out-of-distribution generalization evaluations",
            "embodied_task_description": "Same set of robot skills as RT-2 overall: pick, place, move near, knock, open/close drawers, pick from/in receptacle; extensive real-world generalization tests on unseen objects, backgrounds, and environments separated into easy/hard splits.",
            "action_space_text": "VLM output tokens (natural language) and integer tokens representing discretized action bins (PaLI-X tokenizer supports integer tokens up to 1000).",
            "action_space_embodied": "6-DoF positional and rotational deltas + gripper extension + terminate command; continuous dims discretized into 256 bins each, represented as 8 integer tokens forming the action string.",
            "action_mapping_method": "Direct mapping of discretized action bin indices to PaLI-X integer tokens; action string constructed by concatenating tokens for each action dimension with spaces; output constrained to action-token vocabulary for robot tasks.",
            "perception_requirements": "RGB images processed via PaLI-X's ViT encoder (ViT-22B in full PaLI-X architecture description; smaller variants use smaller ViTs).",
            "transfer_successful": true,
            "performance_with_pretraining": "RT-2-PaLI-X models attained large improvements in generalization: average ≈2× improvement over RT-1/MOO baselines and &gt;3× on emergent semantic tasks vs RT-1; larger 55B model outperformed smaller 5B in generalization ablations.",
            "performance_without_pretraining": "A PaLI-X model trained from scratch on robot data performed poorly (even the 5B model trained from scratch performed poorly), and naive finetuning only on robot data underperformed co-fine-tuning; RT-1 baseline had similar in-distribution performance but substantially worse generalization.",
            "sample_complexity_with_pretraining": "Co-fine-tuning regimen: RT-2-PaLI-X-55B: learning rate 1e-3, batch size 2048, 80K gradient steps; RT-2-PaLI-X-5B: same LR and batch size, 270K steps. The robotics dataset was weighted to ~50% of the mixture. Exact number of robot episodes required is not specified in the paper.",
            "sample_complexity_without_pretraining": "Not reported as episode counts; models trained from scratch on the robot data (no VLM pretraining) required the same training procedure but performed poorly even at 5B, indicating pretraining is critical; quantitative episode counts not provided.",
            "sample_complexity_gain": "No explicit numeric sample-efficiency factor reported; co-fine-tuning with VLM pretraining delivered markedly better generalization than finetuning from scratch or finetuning VLM only on robot data.",
            "transfer_success_factors": "Availability of integer tokens in PaLI-X tokenizer enabled straightforward mapping of discretized action bins; strong image-language pretraining provides semantic and open-vocabulary understanding useful for generalization; co-fine-tuning preserves VLM capabilities while learning action mapping.",
            "transfer_failure_factors": "Training from scratch failed even at large scale; improvements rely on retaining VLM pretraining (co-fine-tuning) and sufficient model capacity; physical skill repertoire still limited by the diversity of robot data.",
            "key_findings": "PaLI-X-based RT-2 benefits strongly from VLM pretraining and model scaling: co-fine-tuning PaLI-X with robot actions (using integer-token action encoding) yields substantially better out-of-distribution generalization and emergent semantic behaviors than baselines or training from scratch; larger models generalize better.",
            "uuid": "e1843.1",
            "source_info": {
                "paper_title": "RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "RT-2-PaLM-E",
            "name_full": "RT-2-PaLM-E (RT-2 built on PaLM-E)",
            "brief_description": "An RT-2 variant that starts from PaLM-E, a decoder-only embodied multimodal LLM that projects visual inputs and other continuous modalities into language token space, then co-fine-tunes to output robot action tokens.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_agent_name": "RT-2-PaLM-E (12B variant)",
            "model_agent_description": "Uses PaLM-E-12B (decoder-only LLM with ViT visual projector) as the backbone; PaLM-E maps images into the language embedding space and is fine-tuned together with web VLM tasks and robot trajectories to emit tokenized actions (action tokens created by overwriting least-frequent tokens).",
            "pretraining_data_type": "Multimodal (vision + language) pretraining and LLM-style language pretraining; VLM and language tasks including VQA and captioning mixtures from Driess et al. (2023).",
            "pretraining_data_details": "PaLM-E weights from Driess et al. (2023) pretrained on an interwoven mix of visual and language tasks; co-fine-tuning mixture weighted robotics dataset to ≈66% for RT-2-PaLM-E experiments. Pretraining mixture includes datasets that improve language/math reasoning capabilities in PaLM-E.",
            "embodied_task_name": "Real-world robotic manipulation (RT-1 dataset), emergent reasoning evaluations",
            "embodied_task_description": "Same real-world manipulation suite (pick/place/knock/open/close) and emergent tasks assessing symbol understanding, reasoning (including math), and human recognition; PaLM-E variant showed relative strengths on math reasoning tasks.",
            "action_space_text": "Language token output space of the decoder-only LLM; action tokens implemented by overwriting 256 least-frequent vocabulary tokens to represent discretized bins.",
            "action_space_embodied": "6-DoF end-effector delta + gripper extension + terminate flag discretized into 256 bins per continuous dimension, encoded as concatenated tokens.",
            "action_mapping_method": "Overwrite the 256 least-frequently used tokens in the PaLM-E vocabulary to serve as action-bin tokens, then generate action strings of those tokens; decoding constrained to action-token subset for robot action steps.",
            "perception_requirements": "RGB images projected via PaLM-E's ViT visual projector (ViT-4B in used PaLM-E-12B instantiation); multimodal inputs are concatenated into token space.",
            "transfer_successful": true,
            "performance_with_pretraining": "RT-2-PaLM-E performed comparably overall and showed an edge in certain hard generalization splits and math/reasoning tasks, consistent with PaLM-E's pretraining mixture; overall RT-2 models (including PaLM-E) achieved large gains in generalization over baselines.",
            "performance_without_pretraining": "Baseline models (RT-1 etc.) underperformed in generalization and emergent reasoning compared to RT-2-PaLM-E; exact numeric baselines vary by split and task and are reported qualitatively in the paper.",
            "sample_complexity_with_pretraining": "Co-fine-tuning hyperparameters reported: RT-2-PaLM-E-12B used learning rate 4e-4, batch size 512, co-fine-tuned for 1M gradient steps; robotics data comprised ~66% of training mixture. No explicit robot-episode-to-threshold counts provided.",
            "sample_complexity_without_pretraining": "Not provided as episode counts; compared to finetuning-only ablations, co-fine-tuning (keeping web data) provided better generalization, indicating pretraining reduces the need to rely solely on robot data diversity.",
            "sample_complexity_gain": "No explicit numeric sample-efficiency factor is provided; qualitative gains in generalization and certain reasoning tasks (e.g., math) are reported.",
            "transfer_success_factors": "PaLM-E's multimodal/LLM-style pretraining (including language and reasoning capabilities) supported transfer of math/multilingual reasoning to embodied tasks; vocabulary overwrite approach enabled action encoding within the same token space; co-fine-tuning mixture weighting preserved useful LLM capabilities.",
            "transfer_failure_factors": "Similar constraints as other RT-2 variants: no acquisition of new motion primitives beyond robot data; inference latency and compute for large models; overwriting tokens is a form of symbol tuning that requires careful fine-tuning to avoid interference.",
            "key_findings": "PaLM-E-based RT-2 inherits PaLM-E's language and reasoning strengths (notably math capabilities) and, when co-fine-tuned with robot data while overwriting tokens for action bins, achieves strong generalization and emergent reasoning in embodied tasks while retaining the same limitations on physical skill breadth.",
            "uuid": "e1843.2",
            "source_info": {
                "paper_title": "RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "RT-2 (CoT variant)",
            "name_full": "RT-2 with chain-of-thought (Plan + Action) fine-tuning",
            "brief_description": "A small modification of RT-2 training data that prepends a natural-language 'Plan:' step before the action tokens, encouraging the model to generate an explicit textual plan (chain-of-thought) followed by the action string.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_agent_name": "RT-2 (chain-of-thought / Plan-Action augmentation)",
            "model_agent_description": "RT-2 variant (PaLM-E based in experiments) fine-tuned for a few hundred gradient steps with examples that include an intermediate natural-language 'Plan' sentence before the action tokens, enabling multi-stage reasoning and improved semantic decision-making.",
            "pretraining_data_type": "Same vision-language pretraining (PaLM-E) with additional fine-tuning on plan-annotated robot trajectories (language-augmented actions).",
            "pretraining_data_details": "Augmented robot trajectory examples where each example contains 'Instruction: ... Plan: &lt;natural language plan&gt;. Action: &lt;action tokens&gt;'; this augmentation was applied for a few hundred gradient steps on the PaLM-E RT-2 variant.",
            "embodied_task_name": "Real-world semantic reasoning manipulation (qualitative rollouts and emergent reasoning evaluations)",
            "embodied_task_description": "Complex semantic tasks requiring multi-step reasoning (e.g., choose an improvised hammer: pick a rock; decide which drink suits a tired person). The Plan step allows the model to produce an explicit natural-language rationale before generating the low-level action tokens.",
            "action_space_text": "Natural language plan followed by action tokens in the same text token space.",
            "action_space_embodied": "Same 6-DoF discretized action token format as baseline RT-2 models.",
            "action_mapping_method": "Same discretization and token overwrite/integer mapping as RT-2; augment training examples with 'Plan:' natural language prefix that the model must generate before action tokens.",
            "perception_requirements": "RGB vision as in other RT-2 variants; the Plan step leverages the model's VLM/LLM capabilities to reason from images and instructions.",
            "transfer_successful": true,
            "performance_with_pretraining": "Qualitatively observed to produce more sophisticated command handling and multi-stage semantic inferences; no large-scale quantitative numbers reported for the chain-of-thought variant, though rollouts and examples show improved semantic reasoning in complex tasks.",
            "performance_without_pretraining": "Baseline RT-2 (without Plan augmentation) can perform semantic tasks but chain-of-thought augmentation enabled more complex multi-step reasoning qualitatively; no numeric comparison to quantify magnitude.",
            "sample_complexity_with_pretraining": "Chain-of-thought fine-tuning performed for a few hundred gradient steps on top of existing RT-2 weights; paper does not report episode counts or sample-efficiency curves for this variant.",
            "sample_complexity_without_pretraining": "Not reported.",
            "sample_complexity_gain": "Not quantified numerically; qualitative evidence suggests improved reasoning with modest additional fine-tuning.",
            "transfer_success_factors": "Providing a natural-language planning slot leverages VLM/LLM pretraining and scaffolds multi-step reasoning that can then be grounded in the same token space for actions.",
            "transfer_failure_factors": "Quantitative benefits and generalization of the CoT variant are not fully characterized (no large-scale numeric evaluation), and additional fine-tuning steps are required.",
            "key_findings": "Augmenting robot examples with an explicit natural-language 'Plan' before actions elicits more sophisticated semantic and multi-stage reasoning in RT-2, demonstrating that the language capabilities of pretrained VLMs/LLMs can be used as an internal planning representation for embodied control.",
            "uuid": "e1843.3",
            "source_info": {
                "paper_title": "RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "PaLI-X",
            "name_full": "PaLI-X (multilingual vision-language model)",
            "brief_description": "A large multilingual vision-language model (VLM) used as the base VLM for RT-2-PaLI-X; PaLI-X provides ViT visual encoders and encoder-decoder text generation backbones trained on Web-scale image-text mixtures.",
            "citation_title": "Pali-x: On scaling up a multilingual vision and language model",
            "mention_or_use": "use",
            "model_agent_name": "PaLI-X (pretrained VLM)",
            "model_agent_description": "Vision-language architecture combining ViT image encoders and an encoder-decoder language generation backbone (UL2-like); trained on large multilingual vision-language mixtures enabling VQA, captioning, and related tasks; used as initialization for RT-2-PaLI-X.",
            "pretraining_data_type": "Vision-language web image-text pairs (WebLI) and VQA/captioning datasets.",
            "pretraining_data_details": "WebLI (~10B image-text pairs filtered to ~1B high-quality examples) plus other captioning and VQA datasets as in Chen et al. (2023a); models available at different scales including PaLI-3B, PaLI-X 5B and 55B variants.",
            "embodied_task_name": "Used as base for RT-2-PaLI-X fine-tuned to robotic manipulation tasks (see RT-2 entries)",
            "embodied_task_description": "Fine-tuned with robot trajectory data to produce low-level action tokens for real-world manipulation tasks (pick/place/knock/open/close) and evaluated on generalization and emergent reasoning.",
            "action_space_text": "Natural language and integer tokens (PaLI-X tokenizer provides unique tokens for integers up to 1000, enabling direct mapping from discretized action bins to integer tokens).",
            "action_space_embodied": "6-DoF discretized end-effector deltas + gripper extension + terminate token encoded as integer tokens.",
            "action_mapping_method": "Map discretized bin indices to PaLI-X integer tokens; action string concatenation and vocabulary-constrained decoding for action steps.",
            "perception_requirements": "RGB images processed by ViT encoders inside PaLI-X (ViT variants depending on model size).",
            "transfer_successful": true,
            "performance_with_pretraining": "When used as RT-2-PaLI-X, provided strong semantic generalization and emergent behavior; larger PaLI-X-based RT-2 (55B) yielded better generalization than smaller variants and than baselines.",
            "performance_without_pretraining": "Training comparable architectures from scratch on robot data performed poorly even at 5B scale, indicating the pretraining was critical.",
            "sample_complexity_with_pretraining": "PaLI-X based co-fine-tuning schedules: see RT-2-PaLI-X entry (80K steps for 55B, 270K for 5B); robotics mixture approx. 50%. No explicit episode counts.",
            "sample_complexity_without_pretraining": "Not reported as explicit episode counts; models trained from scratch failed to match co-fine-tuned performance.",
            "sample_complexity_gain": "Not numerically specified; pretraining is necessary to obtain good generalization.",
            "transfer_success_factors": "Integer-token-aware tokenizer allowed straightforward action token mapping; extensive image-text pretraining supplied semantic priors used in downstream embodied tasks.",
            "transfer_failure_factors": "Pretraining does not replace the need for diverse robot data to learn new motions; large compute/inference costs.",
            "key_findings": "PaLI-X's tokenizer and VLM pretraining make it a practical backbone for direct action generation when combined with discretized-action token mapping and co-fine-tuning; pretraining is essential to achieve strong generalization.",
            "uuid": "e1843.4",
            "source_info": {
                "paper_title": "RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "PaLM-E",
            "name_full": "PaLM-E (embodied multimodal language model)",
            "brief_description": "A decoder-only multimodal LLM that projects visual inputs and other continuous modalities into the language token space, pretrained on multimodal and language data and used as an RT-2 backbone.",
            "citation_title": "palm-e: An embodied multimodal language model",
            "mention_or_use": "use",
            "model_agent_name": "PaLM-E (pretrained multimodal LLM)",
            "model_agent_description": "Decoder-only LLM architecture that accepts projected visual embeddings and other modalities concatenated into the language token space; capable of high-level planning/LM-style reasoning and multimodal understanding; used as the base for RT-2-PaLM-E.",
            "pretraining_data_type": "Multimodal pretraining mixing vision-language tasks and language/LLM-style training (VQA, captioning, other web and multimodal tasks).",
            "pretraining_data_details": "Pretraining mixture described by Driess et al. (2023) including image-text tasks and multimodal supervision; PaLM-E-12B uses ViT-4B visual projector as the visual front-end.",
            "embodied_task_name": "Used as a backbone for RT-2-PaLM-E to generate low-level actions for real-world robot manipulation and for chain-of-thought style planning experiments.",
            "embodied_task_description": "Fine-tuned to output discretized action tokens for the RT-1 robotic tasks; chain-of-thought fine-tuning added Plan steps to enable multi-stage semantic reasoning for embodied tasks.",
            "action_space_text": "Language token space of the decoder LLM; action tokens implemented by overwriting least-frequent vocabulary tokens to represent discretized action bins.",
            "action_space_embodied": "Same 6-DoF discretized end-effector action representation as other RT-2 variants.",
            "action_mapping_method": "Overwrite 256 least-frequent tokens in the PaLM-E vocabulary to represent action-bin indices; produce concatenated action token strings; constrain decoding to action tokens for action steps.",
            "perception_requirements": "RGB images projected by a ViT visual projector (ViT-4B) into the language embedding space; concatenation of modalities into token stream.",
            "transfer_successful": true,
            "performance_with_pretraining": "PaLM-E based RT-2 showed competitive generalization and an advantage on math/multilingual reasoning tasks relative to PaLI-X based RT-2, consistent with PaLM-E's pretraining strengths; co-fine-tuning produced robust policies.",
            "performance_without_pretraining": "Finetuning PaLM-E from scratch on robot data was not evaluated; RT-2 variants without co-fine-tuning underperformed.",
            "sample_complexity_with_pretraining": "RT-2-PaLM-E-12B co-fine-tuned for 1M gradient steps with batch size 512 and robotics mixture ~66%; no explicit robot episode counts reported.",
            "sample_complexity_without_pretraining": "Not reported.",
            "sample_complexity_gain": "Not numerically quantified; qualitative improvements in emergent reasoning and generalization noted.",
            "transfer_success_factors": "Strong language and reasoning priors from PaLM-E pretraining (including math capabilities) and the ability to project images into language space facilitate transfer of high-level semantic reasoning to low-level action selection when co-fine-tuned.",
            "transfer_failure_factors": "Action mapping requires token overwrite which could interfere with other language capabilities if not carefully co-fine-tuned; inference latency and computational cost for large models.",
            "key_findings": "PaLM-E's LLM-style multimodal pretraining provides complementary reasoning strengths (e.g., math) when transferred to embodied control via tokenized actions and co-fine-tuning; architecture supports chain-of-thought style planning that can be grounded in actions.",
            "uuid": "e1843.5",
            "source_info": {
                "paper_title": "RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control",
                "publication_date_yy_mm": "2023-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Pali-x: On scaling up a multilingual vision and language model",
            "rating": 2
        },
        {
            "paper_title": "palm-e: An embodied multimodal language model",
            "rating": 2
        },
        {
            "paper_title": "Rt-1: Robotics transformer for real-world control at scale",
            "rating": 2
        },
        {
            "paper_title": "Robotics transformer 2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control",
            "rating": 1
        }
    ],
    "cost": 0.02267625,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control</h1>
<p>Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, Pete Florence, Chuyuan Fu, Montse Gonzalez Arenas, Keerthana Gopalakrishnan, Kehang Han, Karol Hausman, Alexander Herzog, Jasmine Hsu, Brian Ichter, Alex Irpan, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Lisa Lee, Tsang-Wei Edward Lee, Sergey Levine, Yao Lu, Henryk Michalewski, Igor Mordatch, Karl Pertsch, Kanishka Rao, Krista Reymann, Michael Ryoo, Grecia Salazar, Pannag Sanketi, Pierre Sermanet, Jaspiar Singh, Anikait Singh, Radu Soricut, Huong Tran, Vincent Vanhoucke, Quan Vuong, Ayzaan Wahid, Stefan Welker, Paul Wohlhart, Jialin Wu, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, and Brianna Zitkovich<br>Google DeepMind. Authors listed in alphabetical order, with contributions listed in Appendix A.</p>
<p>We study how vision-language models trained on Internet-scale data can be incorporated directly into end-to-end robotic control to boost generalization and enable emergent semantic reasoning. Our goal is to enable a single end-to-end trained model to both learn to map robot observations to actions and enjoy the benefits of large-scale pretraining on language and vision-language data from the web. To this end, we propose to co-fine-tune state-of-the-art vision-language models on both robotic trajectory data and Internet-scale vision-language tasks, such as visual question answering. In contrast to other approaches, we propose a simple, general recipe to achieve this goal: in order to fit both natural language responses and robotic actions into the same format, we express the actions as text tokens and incorporate them directly into the training set of the model in the same way as natural language tokens. We refer to such category of models as vision-language-action models (VLA) and instantiate an example of such a model, which we call RT-2. Our extensive evaluation ( 6 k evaluation trials) shows that our approach leads to performant robotic policies and enables RT-2 to obtain a range of emergent capabilities from Internet-scale training. This includes significantly improved generalization to novel objects, the ability to interpret commands not present in the robot training data (such as placing an object onto a particular number or icon), and the ability to perform rudimentary reasoning in response to user commands (such as picking up the smallest or largest object, or the one closest to another object). We further show that incorporating chain of thought reasoning allows RT-2 to perform multi-stage semantic reasoning, for example figuring out which object to pick up for use as an improvised hammer (a rock), or which type of drink is best suited for someone who is tired (an energy drink).</p>
<h2>1. Introduction</h2>
<p>High-capacity models pretrained on broad web-scale datasets provide an effective and powerful platform for a wide range of downstream tasks: large language models can enable not only fluent text generation (Anil et al., 2023; Brohan et al., 2022; OpenAI, 2023) but emergent problem-solving (Cobbe et al., 2021; Lewkowycz et al., 2022; Polu et al., 2022) and creative generation of prose (Brown et al., 2020; OpenAI, 2023) and code (Chen et al., 2021), while vision-language models enable open-vocabulary visual recognition (Kirillov et al., 2023; Minderer et al., 2022; Radford et al., 2021) and can even make complex inferences about object-agent interactions in images (Alayrac et al., 2022; Chen et al., 2023a,b; Driess et al., 2023; Hao et al., 2022; Huang et al., 2023; Wang et al., 2022). Such semantic reasoning, problem solving, and visual interpretation capabilities would be tremendously useful for generalist robots that must perform a variety of tasks in real-world environments. However,</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1 | RT-2 overview: we represent robot actions as another language, which can be cast into text tokens and trained together with Internet-scale vision-language datasets. During inference, the text tokens are de-tokenized into robot actions, enabling closed loop control. This allows us to leverage the backbone and pretraining of vision-language models in learning robotic policies, transferring some of their generalization, semantic understanding, and reasoning to robotic control. We demonstrate examples of RT-2 execution on the project website: robotics-transformer2.github.io.
it is unclear how robots should acquire such capabilities. While a brute force approach might entail collecting millions of robotic interaction trials, the most capable language and vision-language models are trained on billions of tokens and images from the web (Alayrac et al., 2022; Chen et al., 2023a,b; Huang et al., 2023) - an amount unlikely to be matched with robot data in the near future. On the other hand, directly applying such models to robotic tasks is also difficult: such models reason about semantics, labels, and textual prompts, whereas robots require grounded low-level actions, such as Cartesian end-effector commands. While a number of recent works have sought to incorporate language models (LLMs) and vision-language models (VLMs) into robotics (Ahn et al., 2022; Driess et al., 2023; Vemprala et al., 2023), such methods generally address only the "higher level" aspects of robotic planning, essentially taking the role of a state machine that interprets commands and parses them into individual primitives (such as picking and placing objects), which are then executed by separate low-level controllers that themselves do not benefit from the rich semantic knowledge of Internet-scale models during training. Therefore, in this paper we ask: can large pretrained visionlanguage models be integrated directly into low-level robotic control to boost generalization and enable emergent semantic reasoning?</p>
<p>To this end, we explore an approach that is both simple and surprisingly effective: we directly train vision-language models designed for open-vocabulary visual question answering and visual dialogue to output low-level robot actions, along with solving other Internet-scale vision-language tasks. Although such models are typically trained to produce natural language tokens, we can train them on robotic trajectories by tokenizing the actions into text tokens and creating "multimodal sentences" (Driess et al., 2023) that "respond" to robotic instructions paired with camera observations by producing corresponding actions. In this way, vision-language models can be directly trained to act as instruction following robotic policies. This simple approach is in contrast with prior alternatives for incorporating VLMs into robot policies (Shridhar et al., 2022a) or designing new vision-languageaction architectures from scratch (Reed et al., 2022): instead, pre-existing vision-language models, with already-amortized significant compute investment, are trained without any new parameters to output text-encoded actions. We refer to this category of models as vision-language-action (VLA) models. We instantiate VLA models by building on the protocol proposed for RT-1 (Brohan et al., 2022), using a similar dataset, but expanding the model to use a large vision-language backbone. Hence we refer to our model as RT-2 (Robotics Transformer 2). We provide an overview in Figure 1.</p>
<p>We observe that robotic policies derived from such vision-language models exhibit a range of remarkable capabilities, combining the physical motions learned from the robot data with the ability to interpret images and text learned from web data into a single model. Besides the expected benefit of dramatically improving generalization to novel objects and semantically varied instructions, we observe a number of emergent capabilities. While the model's physical skills are still limited to the distribution of skills seen in the robot data, the model acquires the ability to deploy those skills in new ways by interpreting images and language commands using knowledge gleaned from the web. Some example highlights are shown in Figure 2. The model is able to re-purpose pick and place skills learned from robot data to place objects near semantically indicated locations, such as specific numbers or icons, despite those cues not being present in the robot data. The model can also interpret relations between objects to determine which object to pick and where to place it, despite no such relations being provided in the robot demonstrations. Furthermore, if we augment the command with chain of thought prompting, the model is able to make even more complex semantic inferences, such as figuring out which object to pick up for use as an improvised hammer (a rock), or which type of drink is best suited for someone who is tired (an energy drink).</p>
<p>Our main contribution is RT-2, a family of models derived from fine-tuning large vision-language models trained on web-scale data to directly act as generalizable and semantically aware robotic policies. Our experiments investigate models with up to 55B parameters trained on Internet data and instruction-annotated robotic trajectories from previous work (Brohan et al., 2022). Over the course of 6 k robotic evaluations, we show that RT-2 enable significant improvements to generalization over objects, scenes, and instructions, and exhibit a breadth of emergent capabilities inherited from web-scale vision-language pretraining.</p>
<h1>2. Related Work</h1>
<p>Vision-language models. There are several categories of Vision-Language Models (VLMs) (Gan et al., 2022), with perhaps two most relevant: (1) representation-learning models, e.g. CLIP (Radford et al., 2021), which learn common embeddings for both modalities, and (2) visual language models of the form {vision, text} $\rightarrow$ {text} which learn to take vision and language as input and provide free-form text. Both categories have been used to provide pretraining for a wide variety of applied to downstream applications such as object classification (Radford et al., 2021), detection (Gu et al., 2021), and segmentation (Ghiasi et al., 2021). In this work, we focus on the latter category (Alayrac et al., 2022; Chen et al., 2023a,b; Driess et al., 2023; Hao et al., 2022; Li et al., 2023, 2019; Lu et al., 2019). These models are generally trained on many different tasks, such as image captioning, vision-question answering (VQA), and general language tasks on multiple datasets at the same time. While prior works study VLMs for a wide range of problems and settings including in robotics, our focus is on how the capabilities of VLMs can be extended to robotics closed-loop control by endowing them with the ability to predict robot actions, thus leveraging the knowledge already present in VLMs to enable new levels of generalization.</p>
<p>Generalization in robot learning. Developing robotic controllers that can broadly succeed in a variety of scenarios is a long-standing goal in robotics research (Kaelbling, 2020; Smith and Coles, 1973). A promising approach for enabling generalization in robotic manipulation is by learning from large and diverse datasets (Dasari et al., 2019; Levine et al., 2018; Pinto and Gupta, 2016). By doing so, prior methods have demonstrated how robots can generalize to novel object instances (Finn and Levine, 2017; Levine et al., 2018; Mahler et al., 2017; Pinto and Gupta, 2016; Young et al., 2021), to tasks involving novel combinations of objects and skills (Dasari and Gupta, 2021; Finn et al., 2017; James et al., 2018; Jang et al., 2021; Yu et al., 2018), to new goals or language instructions (Jang et al., 2021; Jiang et al., 2022; Liu et al., 2022; Mees et al., 2022; Nair et al., 2022a; Pong et al.,</p>
<p>2019), to tasks with novel semantic object categories (Shridhar et al., 2021; Stone et al., 2023), and to unseen environments (Cui et al., 2022; Du et al., 2023a; Hansen et al., 2020). Unlike most of these prior works, we aim to develop and study a single model that can generalize to unseen conditions along all of these axes. A key ingredient of our approach is to leverage pre-trained models that have been exposed to data that is much broader than the data seen by the robot.</p>
<p>Pre-training for robotic manipulation. Pre-training has a long history in robotic learning. Most works focus on pre-trained visual representations that can be used to initialize the encoder of the robot's camera observations, either via supervised ImageNet classification (Shah and Kumar, 2021), data augmentation (Kostrikov et al., 2020; Laskin et al., 2020a,b; Pari et al., 2021) or objectives that are tailored towards robotic control (Karamcheti et al., 2023; Ma et al., 2022; Majumdar et al., 2023b; Nair et al., 2022b; Xiao et al., 2022b). Other works have incorporated pre-trained language models, often either as an instruction encoder (Brohan et al., 2022; Hill et al., 2020; Jang et al., 2021; Jiang et al., 2022; Lynch and Sermanet, 2020; Nair et al., 2022a; Shridhar et al., 2022b) or for high-level planning (Ahn et al., 2022; Driess et al., 2023; Huang et al., 2022; Mu et al., 2023; Singh et al., 2023; Wu et al., 2023). Rather than using pre-training vision models or pre-trained language models, we specifically consider the use of pre-trained vision-language models (VLMs), which provide rich, grounded knowledge about the world. Prior works have studied the use of VLMs for robotics (Driess et al., 2023; Du et al., 2023b; Gadre et al., 2022; Karamcheti et al., 2023; Shah et al., 2023; Shridhar et al., 2021; Stone et al., 2023), and form part of the inspiration for this work. These prior approaches use VLMs for visual state representations (Karamcheti et al., 2023), for identifying objects (Gadre et al., 2022; Stone et al., 2023), for high-level planning (Driess et al., 2023), or for providing supervision or success detection (Du et al., 2023b; Ma et al., 2023; Sumers et al., 2023; Xiao et al., 2022a; Zhang et al., 2023). While CLIPort (Shridhar et al., 2021) and MOO (Stone et al., 2023) integrate pre-trained VLMs into end-to-end visuomotor manipulation policies, both incorporate significant structure into the policy that limits their applicability. Notably, our work does not rely on a restricted 2D action space and does not require a calibrated camera. Moreover, a critical distinction is that, unlike these works, we leverage VLMs that generate language, and the unified output space of our formulation enables model weights to be entirely shared across language and action tasks, without introducing action-only model layer components.</p>
<h1>3. Vision-Language-Action Models</h1>
<p>In this section, we present our model family and the design choices for enabling training VLMs to directly perform closed-loop robot control. First, we describe the general architecture of our models and how they can be derived from models that are commonly used for vision-language tasks. Then, we introduce the recipe and challenges of fine-tuning large VLMs that are pre-trained on web-scale data to directly output robot actions, becoming VLA models. Finally, we describe how to make these models practical for robot tasks, addressing challenges with model size and inference speed to enable real-time control.</p>
<h3>3.1. Pre-Trained Vision-Language Models</h3>
<p>The vision-language models (Chen et al., 2023a; Driess et al., 2023) that we build on in this work take as input one or more images and produce a sequence of tokens, which conventionally represents natural language text. Such models can perform a wide range of visual interpretation and reasoning tasks, from inferring the composition of an image to answering questions about individual objects and their relations to other objects (Alayrac et al., 2022; Chen et al., 2023a; Driess et al., 2023; Huang et al., 2023). Representing the knowledge necessary to perform such a wide range of tasks</p>
<p>requires large models and web-scale datasets. In this work, we adapt two previously proposed VLMs to act as VLA models: PaLI-X (Chen et al., 2023a) and PaLM-E (Driess et al., 2023). We will refer to vision-language-action versions of these models as RT-2-PaLI-X and RT-2-PaLM-E. We leverage instantiations of these models that range in size from billions to tens of billions of parameters. We provide a detailed description of the architecture of these two models in Appendix D.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2 | RT-2 is able to generalize to a variety of real-world situations that require reasoning, symbol understanding, and human recognition. We study these challenging scenarios in detail in Section 4.</p>
<h1>3.2. Robot-Action Fine-tuning</h1>
<p>To enable vision-language models to control a robot, they must be trained to output actions. We take a direct approach to this problem, representing actions as tokens in the model's output, which are treated in the same way as language tokens. We base our action encoding on the discretization proposed by Brohan et al. (2022) for the RT-1 model. The action space consists of 6-DoF positional and rotational displacement of the robot end-effector, as well as the level of extension of the robot gripper and a special discrete command for terminating the episode, which should be triggered by the policy to signal successful completion. The continuous dimensions (all dimensions except for the discrete termination command) are discretized into 256 bins uniformly. Thus, the robot action can be represented using ordinals of the discrete bins as 8 integer numbers. In order to use these discretized actions to finetune a vision-language into a vision-language-action model, we need to associate tokens from the model's existing tokenization with the discrete action bins. This requires</p>
<p>reserving 256 tokens to serve as action tokens. Which tokens to choose depends on the particular tokenization used by each VLM, which we discuss later in this section. In order to define a target for VLM fine-tuning we convert the action vector into a single string by simply concatenating action tokens for each dimension with a space character:</p>
<p>$$
\text { "terminate } \Delta \text { pos }<em y="y">{x} \Delta \text { pos }</em>} \Delta \text { pos <em z="z">{z} \Delta \text { rot }</em>} \Delta \text { rot <em z="z">{y} \Delta \text { rot }</em>
$$} \text { gripper_extension". </p>
<p>A possible instantiation of such a target could be: "1 128912415101 127". The two VLMs that we finetune in our experiments, PaLI-X (Chen et al., 2023a) and PaLM-E (Driess et al., 2023), use different tokenizations. For PaLI-X, integers up to 1000 each have a unique token, so we simply associate the action bins to the token representing the corresponding integer. For the PaLM-E model, which does not provide this convenient representation of numbers, we simply overwrite the 256 least frequently used tokens to represent the action vocabulary. It is worth noting that training VLMs to override existing tokens with action tokens is a form of symbol tuning (Wei et al., 2023), which has been shown to work well for VLMs in prior work.</p>
<p>Taking the action representation described above, we convert our robot data to be suitable for VLM model fine-tuning, where our inputs include robot camera image and textual task description (using standard VQA format "Q: what action should the robot take to [task instruction]? A:"), and our output is formatted as a string of numbers/least frequently used tokens representing a robot action.</p>
<p>Co-Fine-Tuning. As we will show in our experiments, a key technical detail of the training recipe that improves robot performance is co-fine-tuning robotics data with the original web data instead of naïve finetuning on robot data only. We notice that co-fine-tuning leads to more generalizable policies since the policies are exposed to both abstract visual concepts from web scale data and low level robot actions during fine-tuning, instead of just robot actions. During co-fine-tuning we balance the ratios of robot and web data in each training batch by increasing the sampling weight on the robot dataset.</p>
<p>Output Constraint. One important distinction between RT-2 and standard VLMs is that RT-2 is required to output valid action tokens for execution on the real robot. Thus, to ensure that RT-2 outputs valid action tokens during decoding, we constrain its output vocabulary via only sampling valid action tokens when the model is prompted with a robot-action task, whereas the model is still allowed to output the full range of natural language tokens on standard vision-language tasks.</p>
<h1>3.3. Real-Time Inference</h1>
<p>The size of modern VLMs can reach tens or hundreds of billions of parameters (Chen et al., 2023a; Driess et al., 2023). The largest model trained in this work uses 55B parameters. It is infeasible to directly run such models on the standard desktop-style machines or on-robot GPUs commonly used for real-time robot control. To the best of our knowledge, our model is the largest ever, by over an order of magnitude, used for direct closed-loop robotic control, and therefore requires a new set of solutions to enable efficient real-time inference. We develop a protocol that allows us to run RT-2 models on robots by deploying them in a multi-TPU cloud service and querying this service over the network. With this solution, we can achieve a suitable frequency of control and also serve multiple robots using the same cloud service. The largest model we evaluated, the 55B parameter RT-2-PaLI-X-55B model, can run at a frequency of $1-3 \mathrm{~Hz}$. The smaller version of that model, consisting of 5B parameters, can run at a frequency of around 5 Hz .</p>
<h2>4. Experiments</h2>
<p>Our experiments focus on real-world generalization and emergent capabilities of RT-2 and aim to answer the following questions:</p>
<ol>
<li>How does RT-2 perform on seen tasks and more importantly, generalize over new objects, backgrounds, and environments?</li>
<li>Can we observe and measure any emergent capabilities of RT-2?</li>
<li>How does the generalization vary with parameter count and other design decisions?</li>
<li>Can RT-2 exhibit signs of chain-of-thought reasoning similarly to vision-language models?</li>
</ol>
<p>We evaluate our approach and several baselines with about 6,000 evaluation trajectories in a variety of conditions, which we describe in the following sections. Unless specified otherwise, we use a 7DoF mobile manipulator with the action space described in Sec. 3.2. We also demonstrate examples of RT-2 execution on the project website: robotics-transformer2.github.io. We train two specific instantiations of RT-2 that leverage pre-trained VLMs: (1) RT-2-PaLI-X is built from 5B and 55B PaLI-X (Chen et al., 2023a), and (2) RT-2-PaLM-E is built from 12B PaLM-E (Driess et al., 2023).</p>
<p>For training, we leverage the original web scale data from Chen et al. (2023a) and Driess et al. (2023), which consists of visual question answering, captioning, and unstructured interwoven image and text examples. We combine it with the robot demonstration data from Brohan et al. (2022), which was collected with 13 robots over 17 months in an office kitchen environment. Each robot demonstration trajectory is annotated with a natural language instruction that describes the task performed, consisting of a verb describing the skill (e.g., "pick", "open", "place into") and one or more nouns describing the objects manipulated (e.g., "7up can", "drawer", "napkin") (see Appendix B for more details on the used datasets). For all RT-2 training runs we adopt the hyperparameters from the original PaLI-X (Chen et al., 2023a) and PaLM-E (Driess et al., 2023) papers, including learning rate schedules and regularizations. More training details can be found in Appendix E.</p>
<p>Baselines. We compare our method to multiple state-of-the-art baselines that challenge different aspects of our method. All of the baselines use the exact same robotic data. To compare against a state-of-the-art policy, we use RT-1 (Brohan et al., 2022), a 35M parameter transformer-based model. To compare against state-of-the-art pretrained representations, we use VC-1 (Majumdar et al., 2023a) and R3M (Nair et al., 2022b), with policies implemented by training an RT-1 backbone to take their representations as input. To compare against other architectures for using VLMs, we use MOO (Stone et al., 2023), which uses a VLM to create an additional image channel for a semantic map, which is then fed into an RT-1 backbone. More information is provided in Appendix C.</p>
<h1>4.1. How does RT-2 perform on seen tasks and more importantly, generalize over new objects, backgrounds, and environments?</h1>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3 | Example generalization scenarios used for evaluation in Figures 4 and 6b and Tables 4 and 6.
To evaluate in-distribution performance as well as generalization capabilities, we compare the RT-2-PaLI-X and RT-2-PaLM-E models to the four baselines listed in the previous sections. For the seen tasks category, we use the same suite of seen instructions as in RT-1 (Brohan et al., 2022), which include over 200 tasks in this evaluation: 36 for picking objects, 35 for knocking objects, 35 for placing things upright, 48 for moving objects, 18 for opening and closing various drawers, and 36 for picking out of and placing objects into drawers. Note, however, that these "in-distribution" evaluations still vary the placement of objects and factors such as time of day and robot position, requiring the skills to generalize to realistic variability in the environment.</p>
<p>Figure 3 shows example generalization evaluations, which are split into unseen categories (objects, backgrounds and environments), and are additionally split into easy and hard cases. For unseen objects, hard cases include harder-to-grasp and more unique objects (such as toys). For unseen backgrounds, hard cases include more varied backgrounds and novel objects. Lastly, for unseen environments, hard cases correspond to a more visually distinct office desk environment with monitors and accessories, while the easier environment is a kitchen sink. These evaluations consists of over 280 tasks that focus primarily on pick and placing skills in many diverse scenarios. The list of instructions for unseen categories is specified in Appendix F.2.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure $4 \mid$ Overall performance of two instantiations of RT-2 and baselines across seen training tasks as well as unseen evaluations measuring generalization to novel objects, novel backgrounds, and novel environments. Appendix Table 4 details the full results.</p>
<p>The evaluation results are shown in Figure 4 and Appendix Table 4. The performance on seen tasks is similar between the RT-2 models and RT-1, with other baselines attaining a lower success rate. The difference between the RT-2 models and the baseline is most pronounced in the various generalization experiments, suggesting that the strength of vision-language-action models lies in transferring more generalizable visual and semantic concepts from their Internet-scale pretraining data. Here, on average, both instantiations of RT-2 perform similarly, resulting in $\sim 2 \mathrm{x}$ improvement over the next two baselines, RT-1 and MOO, and $\sim 6 \mathrm{x}$ better than the other baselines. The PaLM-E version of RT-2 seems to perform better than the RT-2-PaLI-X in harder versions of generalization scenarios while under-performing on easier ones, resulting in a similar average performance.</p>
<p>Open Source Language Table Benchmark. To provide an additional point of comparison using open-source baselines and environments, we leverage the open-source Language-Table simulation environment from Lynch et al. (2022). We co-fine-tune a smaller PaLI 3B model on several prediction tasks, including in-domain VQA tasks, for the Language-Table dataset, and evaluate the resulting policy in simulation. For the action prediction task, we discretize and encode actions as text in the format " $X$ Y", where $X$ and $Y$ range between ${-10,-9, \ldots,+9,+10}$, and represent delta 2D cartesian setpoints of the end effector. Due to its reduced size, the resulting model can run inference at a similar rate $(5 \mathrm{~Hz})$ as the other baselines. The results of this experiment are presented in Table 1. We observe a significant performance boost when using our model compared to the baselines, indicating that the VLM-based pre-training together with the expressiveness of the large PaLI model can be beneficial in other scenarios, in this case, simulation with a different robot. We also show qualitative real-world out-of-distribution behaviors behaviors in Figure 5, demonstrating novel pushing tasks and targeting objects not before seen in this environment. More details about the Language Table experiments can be found in Appendix B and D.</p>
<h1>4.2. Can we observe and measure any emergent capabilities of RT-2?</h1>
<p>In addition to evaluating the generalization capabilities of vision-language-action models, we also aim to evaluate the degree to which such models can enable new capabilities beyond those demonstrated</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5 | Real-world out-of-distribution behaviors in the Language Table environment. Identical RT-2-PaLI-3B model checkpoint is used as in Tab. 1.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Language-Table</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">BC-Zero (Jang et al., 2021)</td>
<td style="text-align: center;">$72 \pm 3$</td>
</tr>
<tr>
<td style="text-align: center;">RT-1 (Brohan et al., 2022)</td>
<td style="text-align: center;">$74 \pm 13$</td>
</tr>
<tr>
<td style="text-align: center;">LAVA (Lynch et al., 2022)</td>
<td style="text-align: center;">$77 \pm 4$</td>
</tr>
<tr>
<td style="text-align: center;">RT-2-PaLI-3B (ours)</td>
<td style="text-align: center;">$\mathbf{9 0} \pm \mathbf{1 0}$</td>
</tr>
</tbody>
</table>
<p>Table 1 | Performance on the simulated Language-Table tasks (Lynch and Sermanet, 2020).
in the robot data by transferring knowledge from the web. We refer to such capabilities as emergent, in the sense that they emerge by transferring Internet-scale pretraining. We do not expect such transfer to enable new robotic motions, but we do expect semantic and visual concepts, including relations and nouns, to transfer effectively, even in cases where those concepts were not seen in the robot data.</p>
<p>Qualitative Evaluations. First, we experiment with our RT-2-PaLI-X model to determine various emergent capabilities transferred from vision-language concepts. We demonstrate some examples of such interactions in Figure 2. We find through our explorations that RT-2 inherits novel capabilities in terms of semantic understanding and basic reasoning in the context of the scene. For example accomplishing the task "put strawberry into the correct bowl" requires a nuanced understanding of not only what a strawberry and bowl are, but also reasoning in the context the scene to know the strawberry should go with the like fruits. For the task "pick up the bag about to fall off the table," RT-2 demonstrates physical understanding to disambiguate between two bags and recognize the precariously placed object. All the interactions tested in these scenarios have never been seen in the robot data, which points to the transfer of semantic knowledge from vision-language data.</p>
<p>Quantitative Evaluations. To quantify these emergent capabilities, we take the top two baselines from the previous evaluations, RT-1 and VC-1, and compare them against our two models: RT-2-PaLI-X and RT-2-PaLM-E. To reduce the variance of these experiment, we evaluate all of the methods using the A/B testing framework (Fisher, 1936), where all four models are evaluated one after another in the exact same conditions.</p>
<p>We' split the emergent capabilities of RT-2 into three categories covering axes of reasoning and semantic understanding (with examples of each shown in Appendix Figure 8). The first we term symbol understanding, which explicitly tests whether the RT-2 policy transfers semantic knowledge from vision-language pretraining that was not present in any of the robot data. Example instructions in this category are "move apple to 3 " or "push coke can on top of heart". The second category we term reasoning, which demonstrates the ability to apply various aspects of reasoning of the underlying VLM to control tasks. These tasks require visual reasoning ("move the apple to cup with same color"), math ("move X near the sum of two plus one"), and multilingual understanding ("mueve la manzana al vaso verde"). We refer to the last category as human recognition tasks, which include tasks such as "move the coke can to the person with glasses", to demonstrate human-centric understanding and recognition. The full list of instructions used for this evaluation is specified in Appendix F.2.</p>
<p>We present the results of this experiment in Figure 6a with all the numerical results in Appendix H.2. We observe that our VLA models significantly outperform the baselines across all categories, with our best RT-2-PaLI-X model achieving more than 3x average success rate over the next best baseline (RT-1). We also note that while the larger PaLI-X-based model results in better symbol understanding, reasoning and person recognition performance on average, the smaller PaLM-E-based model has an edge on tasks that involve math reasoning. We attribute this interesting result to the different pre-training mixture used in PaLM-E, which results in a model that is more capable at math calculation than the mostly visually pre-trained PaLI-X.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" />
(a) Performance comparison on various emergent skill evalu- (b) Ablations of RT-2-PaLI-X showcasing the impact of paramations (Figure 8) between RT-2 and two baselines. eter count and training strategy on generalization.</p>
<p>Figure 6 | Quantitative performance of RT-2 across (6a) emergent skills and (6b) size and training ablations. Appendix Tables 5 and 6 detail the full numerical results.</p>
<h1>4.3. How does the generalization vary with parameter count and other design decisions?</h1>
<p>For this comparison, we use RT-2-PaLI-X model because of its flexibility in terms of the model size (due to the nature of PaLM-E, RT-2-PaLM-E is restricted to only certain sizes of PaLM and ViT models). In particular, we compare two different model sizes, 5B and 55B, as well as three different training routines: training a model from scratch, without using any weights from the VLM pre-training; fine-tuning a pre-trained model using robot action data only; and co-fine-tuning (co-training with fine-tuning), the primary method used in this work where we use both the original VLM training data as well as robotic data for VLM fine-tuning. Since we are mostly interested in the generalization aspects of these models, we remove the seen tasks evaluation from this set of experiments.</p>
<p>The results of the ablations are presented in Figure 6b and Appendix Table 6. First, we observe that training a very large model from scratch results in a very poor performance even for the 5B model. Given this result, we decide to skip the evaluation of an even bigger 55B PaLI-X model when trained from scratch. Second, we notice that co-fine-tuning a model (regardless of its size) results in a better generalization performance than simply fine-tuning it with robotic data. We attribute this to the fact that keeping the original data around the fine-tuning part of training, allows the model to not forget its previous concepts learned during the VLM training. Lastly, somewhat unsurprisingly, we notice that the increased size of the model results in a better generalization performance.</p>
<h3>4.4. Can RT-2 exhibit signs of chain-of-thought reasoning similarly to vision-language models?</h3>
<p>Inspired by the chain-of-thought prompting method in LLMs (Wei et al., 2022), we fine-tune a variant of RT-2 with PaLM-E for just a few hundred gradient steps to increase its capability of utilizing language and actions jointly with the hope that it will elicit a more sophisticated reasoning behavior. We augment the data to include an additional "Plan" step, which describes the purpose of the action that the robot is about to take in natural language first, which is then followed by the actual action tokens, e.g. "Instruction: I'm hungry. Plan: pick rxbar chocolate. Action: 1128124136121158111 255." This data augmentation scheme acts as a bridge between VQA datasets (visual reasoning) and manipulation datasets (generating actions).</p>
<p>We qualitatively observe that RT-2 with chain-of-thought reasoning is able to answer more sophisticated commands due to the fact that it is given a place to plan its actions in natural language first. This is a promising direction that provides some initial evidence that using LLMs or VLMs as planners (Ahn et al., 2022; Driess et al., 2023) can be combined with low-level policies in a single VLA model. Rollouts of RT-2 with chain-of-thought reasoning are shown in Figure 7 and in Appendix I.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7 | Rollouts of RT-2 with chain-of-thought reasoning, where RT-2 generates both a plan and an action.</p>
<h1>5. Limitations</h1>
<p>Even though RT-2 exhibits promising generalization properties, there are multiple limitations of this approach. First, although we show that including web-scale pretraining via VLMs boosts generalization over semantic and visual concepts, the robot does not acquire any ability to perform new motions by virtue of including this additional experience. The model's physical skills are still limited to the distribution of skills seen in the robot data (see Appendix G), but it learns to deploy those skills in new ways. We believe this is a result of the dataset not being varied enough along the axes of skills. An exciting direction for future work is to study how new skills could be acquired through new data collection paradigms such as videos of humans.</p>
<p>Second, although we showed we could run large VLA models in real time, the computation cost of these models is high, and as these methods are applied to settings that demand high-frequency control, real-time inference may become a major bottleneck. An exciting direction for future research is to explore quantization and distillation techniques that might enable such models to run at higher rates or on lower-cost hardware. This is also connected to another current limitation in that there are only a small number of generally available VLM models that can be used to create RT-2. We hope that more open-sourced models will become available (e.g. https://llava-vl. github.io/) and the proprietary ones will open up their fine-tuning APIs, which is a sufficient requirement to build VLA models.</p>
<h2>6. Conclusions</h2>
<p>In this paper, we described how vision-language-action (VLA) models could be trained by combining vision-language model (VLM) pretraining with robotic data. We then presented two instantiations of VLAs based on PaLM-E and PaLI-X, which we call RT-2-PaLM-E and RT-2-PaLI-X. These models are co-fine-tuned with robotic trajectory data to output robot actions, which are represented as text tokens. We showed that our approach results in very performant robotic policies and, more importantly, leads to a significantly better generalization performance and emergent capabilities inherited from</p>
<p>web-scale vision-language pretraining. We believe that this simple and general approach shows a promise of robotics directly benefiting from better vision-language models, which puts the field of robot learning in a strategic position to further improve with advancements in other fields.</p>
<h1>Acknowledgments</h1>
<p>We would like to acknowledge Fred Alcober, Jodi Lynn Andres, Carolina Parada, Joseph Dabis, Rochelle Dela Cruz, Jessica Gomez, Gavin Gonzalez, John Guilyard, Tomas Jackson, Jie Tan, Scott Lehrer, Dee M, Utsav Malla, Sarah Nguyen, Jane Park, Emily Perez, Elio Prado, Jornell Quiambao, Clayton Tan, Jodexty Therlonge, Eleanor Tomlinson, Wenxuan Zhou, and the greater Google DeepMind team for their feedback and contributions.</p>
<h1>References</h1>
<p>M. Ahn, A. Brohan, N. Brown, Y. Chebotar, O. Cortes, B. David, C. Finn, K. Gopalakrishnan, K. Hausman, A. Herzog, et al. Do as I can, not as I say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691, 2022.
J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican, M. Reynolds, et al. Flamingo: a visual language model for few-shot learning. arXiv preprint arXiv:2204.14198, 2022.
R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos, S. Shakeri, E. Taropa, P. Bailey, Z. Chen, et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023.
A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, J. Dabis, C. Finn, K. Gopalakrishnan, K. Hausman, A. Herzog, J. Hsu, et al. Rt-1: Robotics transformer for real-world control at scale. arXiv preprint arXiv:2212.06817, 2022.
T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.
D. Cer, Y. Yang, S. Kong, N. Hua, N. Limtiaco, R. S. John, N. Constant, M. Guajardo-Cespedes, S. Yuan, C. Tar, Y. Sung, B. Strope, and R. Kurzweil. Universal sentence encoder. CoRR, abs/1803.11175, 2018. URL http://arxiv.org/abs/1803.11175.
M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.
X. Chen, J. Djolonga, P. Padlewski, B. Mustafa, S. Changpinyo, J. Wu, C. R. Ruiz, S. Goodman, X. Wang, Y. Tay, S. Shakeri, M. Dehghani, D. Salz, M. Lucic, M. Tschannen, A. Nagrani, H. Hu, M. Joshi, B. Pang, C. Montgomery, P. Pietrzyk, M. Ritter, A. Piergiovanni, M. Minderer, F. Pavetic, A. Waters, G. Li, I. Alabdulmohsin, L. Beyer, J. Amelot, K. Lee, A. P. Steiner, Y. Li, D. Keysers, A. Arnab, Y. Xu, K. Rong, A. Kolesnikov, M. Seyedhosseini, A. Angelova, X. Zhai, N. Houlsby, and R. Soricut. Pali-x: On scaling up a multilingual vision and language model, 2023a.
X. Chen, X. Wang, S. Changpinyo, A. Piergiovanni, P. Padlewski, D. Salz, S. Goodman, A. Grycner, B. Mustafa, L. Beyer, A. Kolesnikov, J. Puigcerver, N. Ding, K. Rong, H. Akbari, G. Mishra, L. Xue, A. Thapliyal, J. Bradbury, W. Kuo, M. Seyedhosseini, C. Jia, B. K. Ayan, C. Riquelme, A. Steiner, A. Angelova, X. Zhai, N. Houlsby, and R. Soricut. Pali: A jointly-scaled multilingual language-image model, 2023b.
K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.
Z. J. Cui, Y. Wang, N. Muhammad, L. Pinto, et al. From play to policy: Conditional behavior generation from uncurated robot data. arXiv preprint arXiv:2210.10047, 2022.
S. Dasari and A. Gupta. Transformers for one-shot visual imitation. In Conference on Robot Learning, pages 2071-2084. PMLR, 2021.
S. Dasari, F. Ebert, S. Tian, S. Nair, B. Bucher, K. Schmeckpeper, S. Singh, S. Levine, and C. Finn. Robonet: Large-scale multi-robot learning. In Conference on Robot Learning, 2019.</p>
<p>M. Dehghani, J. Djolonga, B. Mustafa, P. Padlewski, J. Heek, J. Gilmer, A. Steiner, M. Caron, R. Geirhos, I. Alabdulmohsin, R. Jenatton, L. Beyer, M. Tschannen, A. Arnab, X. Wang, C. Riquelme, M. Minderer, J. Puigcerver, U. Evci, M. Kumar, S. van Steenkiste, G. F. Elsayed, A. Mahendran, F. Yu, A. Oliver, F. Huot, J. Bastings, M. P. Collier, A. Gritsenko, V. Birodkar, C. Vasconcelos, Y. Tay, T. Mensink, A. Kolesnikov, F. Pavetić, D. Tran, T. Kipf, M. Lučić, X. Zhai, D. Keysers, J. Harmsen, and N. Houlsby. Scaling vision transformers to 22 billion parameters, 2023.
D. Driess, F. Xia, M. S. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter, A. Wahid, J. Tompson, Q. Vuong, T. Yu, et al. Palm-e: An embodied multimodal language model. arXiv preprint arXiv:2303.03378, 2023.
M. Du, S. Nair, D. Sadigh, and C. Finn. Behavior retrieval: Few-shot imitation learning by querying unlabeled datasets. arXiv preprint arXiv:2304.08742, 2023a.
Y. Du, K. Konyushkova, M. Denil, A. Raju, J. Landon, F. Hill, N. de Freitas, and S. Cabi. Vision-language models as success detectors. arXiv preprint arXiv:2303.07280, 2023b.
C. Finn and S. Levine. Deep visual foresight for planning robot motion. In 2017 IEEE International Conference on Robotics and Automation (ICRA), pages 2786-2793. IEEE, 2017.
C. Finn, T. Yu, T. Zhang, P. Abbeel, and S. Levine. One-shot visual imitation learning via meta-learning. In Conference on robot learning, pages 357-368. PMLR, 2017.
R. A. Fisher. Design of experiments. British Medical Journal, 1(3923):554, 1936.
S. Y. Gadre, M. Wortsman, G. Ilharco, L. Schmidt, and S. Song. Clip on wheels: Zero-shot object navigation as object localization and exploration. arXiv preprint arXiv:2203.10421, 2022.
Z. Gan, L. Li, C. Li, L. Wang, Z. Liu, J. Gao, et al. Vision-language pre-training: Basics, recent advances, and future trends. Foundations and Trends ${ }^{\circledR}$ in Computer Graphics and Vision, 14(3-4):163-352, 2022.
G. Ghiasi, X. Gu, Y. Cui, and T.-Y. Lin. Open-vocabulary image segmentation. arXiv preprint arXiv:2112.12143, 2021.
K. Grauman, A. Westbury, E. Byrne, Z. Chavis, A. Furnari, R. Girdhar, J. Hamburger, H. Jiang, M. Liu, X. Liu, M. Martin, T. Nagarajan, I. Radosavovic, S. K. Ramakrishnan, F. Ryan, J. Sharma, M. Wray, M. Xu, E. Z. Xu, C. Zhao, S. Bansal, D. Batra, V. Cartillier, S. Crane, T. Do, M. Doulaty, A. Erapalli, C. Feichtenhofer, A. Fragomeni, Q. Fu, A. Gebreselasie, C. Gonzalez, J. Hillis, X. Huang, Y. Huang, W. Jia, W. Khoo, J. Kolar, S. Kottur, A. Kumar, F. Landini, C. Li, Y. Li, Z. Li, K. Mangalam, R. Modhugu, J. Munro, T. Murrell, T. Nishiyasu, W. Price, P. R. Puentes, M. Ramazanova, L. Sari, K. Somasundaram, A. Southerland, Y. Sugano, R. Tao, M. Vo, Y. Wang, X. Wu, T. Yagi, Z. Zhao, Y. Zhu, P. Arbelaez, D. Crandall, D. Damen, G. M. Farinella, C. Fuegen, B. Ghanem, V. K. Ithapu, C. V. Jawahar, H. Joo, K. Kitani, H. Li, R. Newcombe, A. Oliva, H. S. Park, J. M. Rehg, Y. Sato, J. Shi, M. Z. Shou, A. Torralba, L. Torresani, M. Yan, and J. Malik. Ego4d: Around the world in 3,000 hours of egocentric video, 2022.
X. Gu, T.-Y. Lin, W. Kuo, and Y. Cui. Open-vocabulary object detection via vision and language knowledge distillation. arXiv preprint arXiv:2104.13921, 2021.
N. Hansen, R. Jangir, Y. Sun, G. Alenyà, P. Abbeel, A. A. Efros, L. Pinto, and X. Wang. Self-supervised policy adaptation during deployment. arXiv preprint arXiv:2007.04309, 2020.
Y. Hao, H. Song, L. Dong, S. Huang, Z. Chi, W. Wang, S. Ma, and F. Wei. Language models are general-purpose interfaces. arXiv preprint arXiv:2206.06336, 2022.</p>
<p>F. Hill, S. Mokra, N. Wong, and T. Harley. Human instruction-following with deep reinforcement learning via transfer-learning from text. arXiv preprint arXiv:2005.09382, 2020.
S. Huang, L. Dong, W. Wang, Y. Hao, S. Singhal, S. Ma, T. Lv, L. Cui, O. K. Mohammed, Q. Liu, et al. Language is not all you need: Aligning perception with language models. arXiv preprint arXiv:2302.14045, 2023.
W. Huang, P. Abbeel, D. Pathak, and I. Mordatch. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. In International Conference on Machine Learning, pages 9118-9147. PMLR, 2022.
S. James, M. Bloesch, and A. J. Davison. Task-embedded control networks for few-shot imitation learning. In Conference on robot learning, pages 783-795. PMLR, 2018.
E. Jang, A. Irpan, M. Khansari, D. Kappler, F. Ebert, C. Lynch, S. Levine, and C. Finn. Bc-z: Zeroshot task generalization with robotic imitation learning. In Conference on Robot Learning, pages 991-1002. PMLR, 2021.
Y. Jiang, A. Gupta, Z. Zhang, G. Wang, Y. Dou, Y. Chen, L. Fei-Fei, A. Anandkumar, Y. Zhu, and L. Fan. Vima: General robot manipulation with multimodal prompts. arXiv preprint arXiv:2210.03094, 2022.
L. P. Kaelbling. The foundation of efficient robot learning. Science, 369(6506):915-916, 2020.
S. Karamcheti, S. Nair, A. S. Chen, T. Kollar, C. Finn, D. Sadigh, and P. Liang. Language-driven representation learning for robotics. arXiv preprint arXiv:2302.12766, 2023.
A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Lo, et al. Segment anything. arXiv preprint arXiv:2304.02643, 2023.
I. Kostrikov, D. Yarats, and R. Fergus. Image augmentation is all you need: Regularizing deep reinforcement learning from pixels. arXiv preprint arXiv:2004.13649, 2020.
M. Laskin, K. Lee, A. Stooke, L. Pinto, P. Abbeel, and A. Srinivas. Reinforcement learning with augmented data. Advances in neural information processing systems, 33:19884-19895, 2020a.
M. Laskin, A. Srinivas, and P. Abbeel. Curl: Contrastive unsupervised representations for reinforcement learning. In International Conference on Machine Learning, pages 5639-5650. PMLR, 2020b.
S. Levine, P. Pastor, A. Krizhevsky, J. Ibarz, and D. Quillen. Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection. The International journal of robotics research, 37(4-5):421-436, 2018.
A. Lewkowycz, A. Andreassen, D. Dohan, E. Dyer, H. Michalewski, V. Ramasesh, A. Slone, C. Anil, I. Schlag, T. Gutman-Solo, et al. Solving quantitative reasoning problems with language models. arXiv preprint arXiv:2206.14858, 2022.
J. Li, D. Li, S. Savarese, and S. Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023.
L. H. Li, M. Yatskar, D. Yin, C.-J. Hsieh, and K.-W. Chang. Visualbert: A simple and performant baseline for vision and language. arXiv preprint arXiv:1908.03557, 2019.
H. Liu, L. Lee, K. Lee, and P. Abbeel. Instruction-following agents with jointly pre-trained visionlanguage models. arXiv preprint arXiv:2210.13431, 2022.</p>
<p>J. Lu, D. Batra, D. Parikh, and S. Lee. Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. Advances in neural information processing systems, 32, 2019.
C. Lynch and P. Sermanet. Language conditioned imitation learning over unstructured data. arXiv preprint arXiv:2005.07648, 2020.
C. Lynch, A. Wahid, J. Tompson, T. Ding, J. Betker, R. Baruch, T. Armstrong, and P. Florence. Interactive language: Talking to robots in real time. arXiv preprint arXiv:2210.06407, 2022.
Y. J. Ma, S. Sodhani, D. Jayaraman, O. Bastani, V. Kumar, and A. Zhang. Vip: Towards universal visual reward and representation via value-implicit pre-training. arXiv preprint arXiv:2210.00030, 2022.
Y. J. Ma, W. Liang, V. Som, V. Kumar, A. Zhang, O. Bastani, and D. Jayaraman. Liv: Language-image representations and rewards for robotic control. arXiv preprint arXiv:2306.00958, 2023.
J. Mahler, J. Liang, S. Niyaz, M. Laskey, R. Doan, X. Liu, J. A. Ojea, and K. Goldberg. Dex-net 2.0: Deep learning to plan robust grasps with synthetic point clouds and analytic grasp metrics. arXiv preprint arXiv:1703.09312, 2017.
A. Majumdar, K. Yadav, S. Arnaud, Y. J. Ma, C. Chen, S. Silwal, A. Jain, V.-P. Berges, P. Abbeel, J. Malik, et al. Where are we in the search for an artificial visual cortex for embodied intelligence? arXiv preprint arXiv:2303.18240, 2023a.
A. Majumdar, K. Yadav, S. Arnaud, Y. J. Ma, C. Chen, S. Silwal, A. Jain, V.-P. Berges, P. Abbeel, J. Malik, et al. Where are we in the search for an artificial visual cortex for embodied intelligence? arXiv preprint arXiv:2303.18240, 2023b.
O. Mees, L. Hermann, and W. Burgard. What matters in language conditioned robotic imitation learning over unstructured data. IEEE Robotics and Automation Letters, 7(4):11205-11212, 2022.
M. Minderer, A. Gritsenko, A. Stone, M. Neumann, D. Weissenborn, A. Dosovitskiy, A. Mahendran, A. Arnab, M. Dehghani, Z. Shen, et al. Simple open-vocabulary object detection with vision transformers. arXiv preprint arXiv:2205.06230, 2022.
Y. Mu, Q. Zhang, M. Hu, W. Wang, M. Ding, J. Jin, B. Wang, J. Dai, Y. Qiao, and P. Luo. Embodiedgpt: Vision-language pre-training via embodied chain of thought. arXiv preprint arXiv:2305.15021, 2023.
S. Nair, E. Mitchell, K. Chen, S. Savarese, C. Finn, et al. Learning language-conditioned robot behavior from offline data and crowd-sourced annotation. In Conference on Robot Learning, pages 1303-1315. PMLR, 2022a.
S. Nair, A. Rajeswaran, V. Kumar, C. Finn, and A. Gupta. R3m: A universal visual representation for robot manipulation. arXiv preprint arXiv:2203.12601, 2022b.</p>
<p>OpenAI. Gpt-4 technical report, 2023.
J. Pari, N. M. Shafiullah, S. P. Arunachalam, and L. Pinto. The surprising effectiveness of representation learning for visual imitation. arXiv preprint arXiv:2112.01511, 2021.
L. Pinto and A. Gupta. Supersizing self-supervision: Learning to grasp from 50k tries and 700 robot hours. In 2016 IEEE international conference on robotics and automation (ICRA), pages 3406-3413. IEEE, 2016.
S. Polu, J. M. Han, K. Zheng, M. Baksys, I. Babuschkin, and I. Sutskever. Formal mathematics statement curriculum learning. arXiv preprint arXiv:2202.01344, 2022.</p>
<p>V. H. Pong, M. Dalal, S. Lin, A. Nair, S. Bahl, and S. Levine. Skew-fit: State-covering self-supervised reinforcement learning. arXiv preprint arXiv:1903.03698, 2019.
A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pages 8748-8763. PMLR, 2021.
S. Reed, K. Zolna, E. Parisotto, S. G. Colmenarejo, A. Novikov, G. Barth-Maron, M. Gimenez, Y. Sulsky, J. Kay, J. T. Springenberg, et al. A generalist agent. arXiv preprint arXiv:2205.06175, 2022.
M. Ryoo, A. Piergiovanni, A. Arnab, M. Dehghani, and A. Angelova. Tokenlearner: Adaptive space-time tokenization for videos. Advances in Neural Information Processing Systems, 34:12786-12797, 2021.
D. Shah, B. Osiński, b. ichter, and S. Levine. Lm-nav: Robotic navigation with large pre-trained models of language, vision, and action. In K. Liu, D. Kulic, and J. Ichnowski, editors, Proceedings of The 6th Conference on Robot Learning, volume 205 of Proceedings of Machine Learning Research, pages 492504. PMLR, 14-18 Dec 2023. URL https://proceedings.mlr.press/v205/shah23b.html.
R. Shah and V. Kumar. Rrl: Resnet as representation for reinforcement learning. arXiv preprint arXiv:2107.03380, 2021.
M. Shridhar, L. Manuelli, and D. Fox. Cliport: What and where pathways for robotic manipulation. In Proceedings of the 5th Conference on Robot Learning (CoRL), 2021.
M. Shridhar, L. Manuelli, and D. Fox. Cliport: What and where pathways for robotic manipulation. In Conference on Robot Learning, pages 894-906. PMLR, 2022a.
M. Shridhar, L. Manuelli, and D. Fox. Perceiver-actor: A multi-task transformer for robotic manipulation. arXiv preprint arXiv:2209.05451, 2022b.
I. Singh, V. Blukis, A. Mousavian, A. Goyal, D. Xu, J. Tremblay, D. Fox, J. Thomason, and A. Garg. Progprompt: Generating situated robot task plans using large language models. In ICRA, 2023.
M. H. Smith and L. S. Coles. Design of a low cost, general purpose robot. In IJCAI, pages 324-336, 1973.
A. Stone, T. Xiao, Y. Lu, K. Gopalakrishnan, K.-H. Lee, Q. Vuong, P. Wohlhart, B. Zitkovich, F. Xia, C. Finn, et al. Open-world object manipulation using pre-trained vision-language models. arXiv preprint arXiv:2303.00905, 2023.
T. Sumers, K. Marino, A. Ahuja, R. Fergus, and I. Dasgupta. Distilling internet-scale vision-language models into embodied agents. arXiv preprint arXiv:2301.12507, 2023.
Y. Tay, M. Dehghani, V. Q. Tran, X. Garcia, J. Wei, X. Wang, H. W. Chung, S. Shakeri, D. Bahri, T. Schuster, H. S. Zheng, D. Zhou, N. Houlsby, and D. Metzler. Ul2: Unifying language learning paradigms, 2023.
S. Vemprala, R. Bonatti, A. Bucker, and A. Kapoor. Chatgpt for robotics: Design principles and model abilities. Microsoft Auton. Syst. Robot. Res, 2:20, 2023.
J. Wang, Z. Yang, X. Hu, L. Li, K. Lin, Z. Gan, Z. Liu, C. Liu, and L. Wang. Git: A generative image-to-text transformer for vision and language. arXiv preprint arXiv:2205.14100, 2022.
J. Wei, X. Wang, D. Schuurmans, M. Bosma, E. Chi, Q. Le, and D. Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022.</p>
<p>J. Wei, L. Hou, A. Lampinen, X. Chen, D. Huang, Y. Tay, X. Chen, Y. Lu, D. Zhou, T. Ma, and Q. V. Le. Symbol tuning improves in-context learning in language models, 2023.
J. Wu, R. Antonova, A. Kan, M. Lepert, A. Zeng, S. Song, J. Bohg, S. Rusinkiewicz, and T. Funkhouser. Tidybot: Personalized robot assistance with large language models. arXiv preprint arXiv:2305.05658, 2023.
T. Xiao, H. Chan, P. Sermanet, A. Wahid, A. Brohan, K. Hausman, S. Levine, and J. Tompson. Robotic skill acquisition via instruction augmentation with vision-language models. arXiv preprint arXiv:2211.11736, 2022a.
T. Xiao, I. Radosavovic, T. Darrell, and J. Malik. Masked visual pre-training for motor control. arXiv preprint arXiv:2203.06173, 2022b.
S. Young, D. Gandhi, S. Tulsiani, A. Gupta, P. Abbeel, and L. Pinto. Visual imitation made easy. In Conference on Robot Learning, pages 1992-2005. PMLR, 2021.
K.-T. Yu, M. Bauza, N. Fazeli, and A. Rodriguez. More than a million ways to be pushed. a high-fidelity experimental dataset of planar pushing. In 2016 IEEE/RSJ international conference on intelligent robots and systems (IROS), pages 30-37. IEEE, 2016.
T. Yu, C. Finn, A. Xie, S. Dasari, T. Zhang, P. Abbeel, and S. Levine. One-shot imitation from observing humans via domain-adaptive meta-learning. arXiv preprint arXiv:1802.01557, 2018.
X. Zhai, A. Kolesnikov, N. Houlsby, and L. Beyer. Scaling vision transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12104-12113, 2022.
X. Zhang, Y. Ding, S. Amiri, H. Yang, A. Kaminski, C. Esselink, and S. Zhang. Grounding classical task planners via vision-language models. arXiv preprint arXiv:2304.08587, 2023.</p>
<h1>A. Contributions</h1>
<ul>
<li>Training and Evaluations (designing and executing procedures for training models, evaluating models in simulation and the real world, running ablations for algorithm design choices): Yevgen Chebotar, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Pete Florence, Chuyuan Fu, Montse Gonzalez Arenas, Keerthana Gopalakrishnan, Kehang Han, Alexander Herzog, Brian Ichter, Alex Irpan, Isabel Leal, Lisa Lee, Yao Lu, Henryk Michalewski, Igor Mordatch, Karl Pertsch, Michael Ryoo, Anikait Singh, Quan Vuong, Ayzaan Wahid, Paul Wohlhart, Fei Xia, Ted Xiao, and Tianhe Yu.</li>
<li>Network Architecture (designing and implementing model network modules, working on tokenization of actions, enabling inference of the model networks during experiments): Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Danny Driess, Pete Florence, Keerthana Gopalakrishnan, Kehang Han, Karol Hausman, Brian Ichter, Alex Irpan, Isabel Leal, Lisa Lee, Henryk Michalewski, Igor Mordatch, Kanishka Rao, Michael Ryoo, Anikait Singh, Quan Vuong, Ayzaan Wahid, Jialin Wu, Fei Xia, Ted Xiao, and Tianhe Yu.</li>
<li>Data Collection (collecting data on real robots, running real robot evaluations, executing operations required for running real robots): Noah Brown, Justice Carbajal, Tianli Ding, Krista Reymann, Grecia Salazar, Pierre Sermanet, Jaspiar Singh, Huong Tran, Stefan Welker, and Sichun Xu.</li>
<li>Leadership (leading the project efforts, managing the project staff, advising on project directions): Yevgen Chebotar, Chelsea Finn, Karol Hausman, Brian Ichter, Sergey Levine, Yao Lu, Igor Mordatch, Kanishka Rao, Pannag Sanketi, Radu Soricut, Vincent Vanhoucke, and Tianhe Yu.</li>
<li>Paper (working on the paper manuscript, designing paper visualizations and figures): Yevgen Chebotar, Danny Driess, Chelsea Finn, Pete Florence, Karol Hausman, Brian Ichter, Lisa Lee, Sergey Levine, Igor Mordatch, Karl Pertsch, Quan Vuong, Fei Xia, Ted Xiao, and Tianhe Yu.</li>
<li>Infrastructure (working on infrastructure and code base backbone needed for training models, running experiments, storing and accessing data): Anthony Brohan, Yevgen Chebotar, Danny Driess, Kehang Han, Jasmine Hsu, Brian Ichter, Alex Irpan, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Lisa Lee, Tsang-Wei Edward Lee, Yao Lu, Igor Mordatch, Quan Vuong, Ayzaan Wahid, Fei Xia, Ted Xiao, Peng Xu, and Tianhe Yu.</li>
</ul>
<h2>B. Datasets</h2>
<p>The vision-language datasets are based on the dataset mixtures from Chen et al. (2023b) and Driess et al. (2023). The bulk of this data consists of the WebLI dataset, which is around 10B image-text pairs across 109 languages, filtered to the top $10 \%$ scoring cross-modal similarity examples to give 1B training examples. Many other captioning and vision question answering datasets are included as well, and more info on the dataset mixtures can be found in Chen et al. (2023b) for RT-2-PaLI-X, and Driess et al. (2023) for RT-2-PaLM-E. When co-fine-tuning RT-2-PaLI-X, we do not use the Episodic WebLI dataset described by Chen et al. (2023a).</p>
<p>The robotics dataset is based on the dataset from Brohan et al. (2022). This consists of demonstration episodes collected with a mobile manipulation robot. Each demonstration is annotated with a natural language instruction from one of seven skills: "Pick Object", "Move Object Near Object", "Place Object Upright", "Knock Object Over", "Open Drawer", "Close Drawer", "Place Object into Receptacle", and "Pick Object from Receptacle and place on the counter". Further details can be found in Brohan et al. (2022).</p>
<p>RT-2-PaLI-X weights the robotics dataset such that it makes up about $50 \%$ of the training mixture</p>
<p>for co-fine-tuning. RT-2-PaLM-E weights the robotics dataset to be about $66 \%$ of the training mixture.
For the results on Language-Table in Table 1, our model is trained on the Language-Table datasets from Lynch et al. (2022). Our model is co-fine-tuned on several prediction tasks: (1) predict the action, given two consecutive image frames and a text instruction; (2) predict the instruction, given image frames; (3) predict the robot arm position, given image frames; (4) predict the number of timesteps between given image frames; and (5) predict whether the task was successful, given image frames and the instruction.</p>
<h1>C. Baselines</h1>
<p>We compare our method to multiple state-of-the-art baselines that challenge different aspects of our method. All of the baselines use the exact same robotic data.</p>
<ul>
<li>RT-1: Robotics Transformer 1 Brohan et al. (2022) is a transformer-based model that achieved state-of-the-art performance on a similar suite of tasks when it was published. The model does not use VLM-based pre-training so it provides an important data point demonstrating whether VLM-based pre-training matters.</li>
<li>VC-1: VC-1 Majumdar et al. (2023a) is a visual foundation model that uses pre-trained visual representations specifically designed for robotics tasks. We use pre-trained representations from the VC-1 ViT-L model. Since VC-1 does not include language conditioning, we add this by separately embedding the language command via Universal Sentence Encoder Cer et al. (2018) to enable comparison to our method. In particular, we concatenate the resulting language embedding tokens to the image tokens produced by VC-1, and pass the concatenated token sequences through token learner Ryoo et al. (2021). The token sequences produced by token learner are then consumed by an RT-1 decoder-only transformer model to predict robot action tokens. We train the VC-1 baseline end-to-end and unfreeze the VC-1 weights during training, since this led to far better results than using frozen VC-1 weights.</li>
<li>R3M: R3M Nair et al. (2022b) is a similar method to VC-1 in that R3M uses pre-trained visual-language representations to improve policy training. In this case the authors use Ego4D dataset Grauman et al. (2022) of human activities to learn the representation that is used by the policy. Both VC-1 and R3M test different state-of-the-art representation learning methods as an alternative to using a VLM. To obtain a language-conditioned policy from the R3M pretrained representation, we follow the same procedure as described above for VC-1, except we use the R3M ResNet50 model to obtain the image tokens, and unfreeze it during training.</li>
<li>MOO: MOO Stone et al. (2023) is an object-centric approach, where a VLM is first used to specify the object of interest in a form of a single, colored pixel in the original image. This pixelmodified image is then trained with an end-to-end policy to accomplish a set of manipulation tasks. This baseline corresponds to a situation where a VLM is used as a separate module that enhances perception but its representations are not used for policy learning.</li>
</ul>
<h2>D. VLMs for RT-2</h2>
<p>The PaLI-X model architecture consists of a ViT-22B Dehghani et al. (2023) to process images, which can accept sequences of $n$ images, leading to $n \times k$ tokens per image, where $k$ is the number of patches per image. The image tokens passing over a projection layer is then consumed by an encoder-decoder backbone of 32B parameters and 50 layers, similar to UL2 Tay et al. (2023), which jointly processes text and images as embeddings to generate output tokens in an auto-regressive manner. The text</p>
<p>input usually consists of the type of task and any additional context (e.g., "Generate caption in 〈lang〉" for captioning tasks or "Answer in 〈lang〉: question" for VQA tasks).</p>
<p>The PaLI-3B model trained on Language-Table (Table 1) uses a smaller ViT-G/14 (Zhai et al., 2022) (2B parameters) to process images, and UL2-3B (Tay et al., 2023) for the encoder-decoder network.</p>
<p>The PaLM-E model is based on a decoder-only LLM that projects robot data such as images and text into the language token space and outputs text such as high-level plans. In the case of the used PaLM-E-12B, the visual model used to project images to the language embedding space is a ViT-4B Chen et al. (2023b). The concatenation of continuous variables to textual input allows PaLM-E to be fully multimodal, accepting a wide variety of inputs such as multiple sensor modalities, object-centric representations, scene representations and object entity referrals.</p>
<h1>E. Training Details</h1>
<p>We perform co-fine-tuning on pre-trained models from the PaLI-X (Chen et al., 2023a) 5B \&amp; 55B model, PaLI (Chen et al., 2023b) 3B model and the PaLM-E (Driess et al., 2023) 12B model. For RT-2-PaLI-X-55B, we use learning rate 1e-3 and batch size 2048 and co-fine-tune the model for 80 K gradient steps whereas for RT-2-PaLI-X-5B, we use the same learning rate and batch size and co-fine-tune the model for 270K gradient steps. For RT-2-PaLM-E-12B, we use learning rate 4e-4 and batch size 512 to co-fine-tune the model for 1 M gradient steps. Both models are trained with the next token prediction objective, which corresponds to the behavior cloning loss in robot learning. For RT-2-PaLI-3B model used for Language-Table results in Table 1, we use learning rate 1e-3 and batch size 128 to co-fine-tune the model for 300K gradient steps.</p>
<h2>F. Evaluation Details</h2>
<h2>F.1. Evaluation Scenarios</h2>
<p>For studying the emergent capabilities of RT-2 in a quantitative manner, we study various challenging semantic evaluation scenarios that aim to measure capabilities such as reasoning, symbol understanding, and human recognition. A visual overview of a subset of these scenes is provided in Figure 8, and the full list of instructions used for quantiative evalution is shown in Table 3.</p>
<h2>F.2. Evaluation Instructions</h2>
<p>Table 2 lists natural language instructions used in model evaluations for unseen objects, backgrounds, and environments. Each instruction was run between 1-5 times, depending on the number of total instructions in that evaluation set. Table 3 lists natural language instructions used to evaluate quantitative emergent evals. Each instruction was run 5 times.</p>            </div>
        </div>

    </div>
</body>
</html>
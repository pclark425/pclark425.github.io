<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5668 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5668</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5668</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-116.html">extraction-schema-116</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <p><strong>Paper ID:</strong> paper-258887717</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2305.15594v1.pdf" target="_blank">Flocks of Stochastic Parrots: Differentially Private Prompt Learning for Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) are excellent in-context learners. However, the sensitivity of data contained in prompts raises privacy concerns. Our work first shows that these concerns are valid: we instantiate a simple but highly effective membership inference attack against the data used to prompt LLMs. To address this vulnerability, one could forego prompting and resort to fine-tuning LLMs with known algorithms for private gradient descent. However, this comes at the expense of the practicality and efficiency offered by prompting. Therefore, we propose to privately learn to prompt. We first show that soft prompts can be obtained privately through gradient descent on downstream data. However, this is not the case for discrete prompts. Thus, we orchestrate a noisy vote among an ensemble of LLMs presented with different prompts, i.e., a flock of stochastic parrots. The vote privately transfers the flock's knowledge into a single public prompt. We show that LLMs prompted with our private algorithms closely match the non-private baselines. For example, using GPT3 as the base model, we achieve a downstream accuracy of 92.7% on the sst2 dataset with ($\epsilon=0.147, \delta=10^{-6}$)-differential privacy vs. 95.2% for the non-private baseline. Through our experiments, we also show that our prompt-based approach is easily deployed with existing commercial APIs.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5668.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5668.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MIA_on_prompts</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Membership Inference Attack on Prompted LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical demonstration that the presence and formulation of demonstrations in discrete prompts (e.g., one-shot examples) makes prompted LLM outputs distinguishably different for member vs non-member examples, enabling strong membership inference attacks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (Babbage) / GPT2-xl</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Membership inference on prompted demonstrations (classification datasets: dbpedia, sst2, agnews, trec)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Adversary queries an LLM prompted with a discrete prompt containing demonstration(s) and inspects the output probability assigned to the correct class for candidate examples to infer whether a candidate was included in the prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Discrete prompt with one-shot (single demonstration) or few-shot examples embedded in an instruction template (template from Zhao et al.); attacker queries the prompted model on candidate inputs and inspects next-token/class probabilities for the correct label token.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Non-member baseline: same query inputs but not present in prompt (50 randomly sampled non-member training points); random-guess baseline (AUC=0.5).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Example: on dbpedia with GPT3-Babbage one-shot prompting the average AUC-ROC of the MIA = 0.84 (over 100 trials); member output probabilities were significantly higher than non-members.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Random guessing AUC-ROC = 0.5; MIA >> random (AUC ~0.84 reported for dbpedia).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>AUC increase of ~0.34 above random on dbpedia (0.84 vs 0.5).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>format increased leakage / improved attack success</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>When a data point is included as a demonstration in the prompt, the LLM assigns notably higher probability to the correct class token for that example; this systematic difference between member and non-member probabilities makes membership distinguishable. The paper attributes this to in-context learning behavior and memorization of prompt examples.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Flocks of Stochastic Parrots: Differentially Private Prompt Learning for Large Language Models', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5668.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5668.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PromptPATE_discrete</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PromptPATE: PATE-style private discrete prompting (noisy ensemble voting → student prompt)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method showing that discrete prompt format (natural-language demonstrations) can be used with a private noisy-aggregation (PATE) over an ensemble of differently-prompted LLMs to privately transfer knowledge into a single public discrete student prompt with high utility.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (Babbage, Curie) and Claude (v1)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Text classification (sst2, agnews, dbpedia, trec and others)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Downstream classification tasks where prompts contain demonstration examples and the LLM's next-token predictions map to class labels; teacher ensemble formed by many discrete prompts drawn from private data.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Teachers: discrete natural-language prompts (one-shot or few-shot) built from private examples; for a public unlabeled input the teachers each vote (next-token/class). Aggregation: Confident GNMax (noisy argmax after consensus thresholding) to label public sequences privately. Student: a discrete prompt constructed from one or a few labeled public sequences (selected/validated on held-out labeled public data).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to zero-shot (instruction-only), non-private one-shot best-example prompt (upper bound), and private ensemble accuracy (no student).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Example: sst2 with GPT-3 (Curie) PromptPATE student achieved 92.7% accuracy at (ε = 0.147, δ = 1e-6) vs non-private baseline 95.2% (reported in text). Across datasets PromptPATE often matched or closely approached the non-private one-shot baseline while far outperforming zero-shot (e.g., dbpedia→agnews example: PromptPATE 74.6% vs zero-shot 44.2%).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Zero-shot significantly lower (example 44.2%); non-private one-shot slightly higher (example sst2 non-private 95.2% vs private 92.7%).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Private PromptPATE student often within a few percentage points of the non-private one-shot baseline (example: sst2: −2.5 percentage points from 95.2% to 92.7%); compared to zero-shot, gains of many tens of percentage points (e.g., +30.4% in the dbpedia→agnews example).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>discrete few-shot prompting (via PromptPATE) greatly improved downstream accuracy compared to zero-shot and closely matched non-private discrete prompting</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Discrete prompts are highly data-efficient: a small number (often ≲100) of privately-labeled public sequences suffice to construct a high-utility student prompt. High consensus across teacher votes (because teachers are different prompts run on the same underlying LLM) yields low per-query privacy cost, enabling many useful labels under tight privacy budgets. Noisy aggregation ensures DP while transferring the format-specific information from many private prompts into a single public prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Flocks of Stochastic Parrots: Differentially Private Prompt Learning for Large Language Models', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5668.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5668.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PromptDPSGD_soft</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PromptDPSGD: DPSGD for private soft-prompt / prefix tuning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Applying differentially-private SGD to train small continuous soft-prompt embeddings (or per-layer prefix embeddings) prepended to a frozen LLM, showing that prompt format (soft vs prefix) can match private fine-tuning on simpler tasks while being far more parameter-efficient.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RoBERTa (base) (used for PromptDPSGD experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>125M (RoBERTa-base total params), soft/prefix params << model</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GLUE classification tasks (sst2, qnli, qqp, mnli)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Standard supervised NLU classification tasks where soft prompts/prefixes are tuned on private downstream data under DP via DPSGD.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Soft prompts: trainable continuous embeddings (prompt length typically 10 tokens) prepended to input; Prefix: embeddings prepended at every layer. Training under privacy via DPSGD operating only on the soft/prefix embeddings (frozen backbone).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to DP fine-tuning of full model or LoRA-style adapters; also compared to non-private baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Soft prompts and prefix achieve accuracies close to non-private baselines: reported differences between non-private baseline and private soft prompt range from ~3% (sst2, simple task) up to ~7% (mnli, harder task). For simple tasks (sst2, qnli) soft prompt/prefix matched private fine-tuning performance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Vs full DP fine-tuning: similar for simple tasks; for harder tasks full fine-tuning may outperform (but gap modest).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Accuracy reductions vs non-private of ~3% (sst2) to ~7% (mnli) for private soft-prompt (values stated in text summary).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>soft/prefix prompting preserved most utility vs full fine-tuning while being much more parameter-efficient</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Because soft prompts / prefix tune orders of magnitude fewer parameters, DPSGD noise is injected into a much smaller parameter set which improves the privacy-utility trade-off and reduces compute/storage costs; the prompt-format (continuous embeddings) is an effective, compact way to present task context to the frozen LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>For more challenging tasks (qqp, mnli) soft-prompt/prefix were somewhat worse than full fine-tuning—performance depends on task complexity and number of tuned parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Flocks of Stochastic Parrots: Differentially Private Prompt Learning for Large Language Models', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5668.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5668.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>shot_and_model_size_effects</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of number of in-context examples (shots) and model scale on prompt performance</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical finding that increasing the number of demonstrations (shots) in prompts and using larger/more capable LLMs improves prompted downstream performance (and can reduce privacy cost for PromptPATE).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 family (Babbage, Curie)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>sst2 (sentiment classification) and agnews (topic classification)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Few-shot in-context classification where the prompt contains one or more example demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>One-shot vs few-shot (e.g., 1-shot vs 4-shot) discrete prompts; also model scale comparison: GPT3-Babbage vs GPT3-Curie.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>1-shot (single demonstration) vs 4-shot; smaller LLM (Babbage) vs larger LLM (Curie).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Example: sst2 private PromptPATE: GPT3-Curie student accuracy 92.7% (ε=0.147) vs GPT3-Babbage 87.2% (text reports); for agnews they observe improvements in the 4-shot setting (qualitative, consistent with prior non-private studies).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Larger model + same prompting led to higher accuracy (example +5.5 percentage points Curie vs Babbage on sst2 in reported experiment) and slightly smaller ε (privacy improved: ε decreased from 0.178 to 0.147 in the reported comparison).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Example effect: +5.5% absolute accuracy increase (87.2 → 92.7) for sst2 when using a larger model (Curie) in the private-prompting experiments; 4-shot gave clear improvements for agnews (no precise number reported for private setting in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>more shots / larger model → improved performance and favorable privacy-utility trade-off</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Larger models have stronger in-context learning capabilities and more expressive context windows; more demonstration examples reduce variance and provide richer task-specific context to the LLM, so prompts become more informative and performant; this also yields higher teacher consensus in PromptPATE lowering per-query privacy costs.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Flocks of Stochastic Parrots: Differentially Private Prompt Learning for Large Language Models', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5668.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5668.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>prompt_selection_calibration</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt selection, instruction/template tuning, and contextual calibration effects</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper reports that template/instruction wording and probability recalibration affect prompted performance; they used a fixed template (Zhao et al.) and applied contextual calibration to improve output probabilities and utility, and note that further tuning of templates could improve PromptPATE.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Calibrate before use: Improving few-shot performance of language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (Babbage) (recalibration applied)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Various classification tasks used with PromptPATE (sst2, agnews, dbpedia, trec)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Few-shot in-context classification where output probabilities are recalibrated and candidate prompts are selected based on validation on labeled public data.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Discrete prompt templates (instruction + demonstrations) from prior work (Zhao et al.); contextual calibration applied to teachers' and students' output probabilities; prompt selection based on validation accuracy on labeled public data (not on private data).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Untuned templates / no calibration vs calibrated outputs and selected prompts (paper used calibrated outputs; they did not tune templates).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>No single numeric delta provided for calibration effect in this paper, but authors state calibration and prompt selection improved utility and that further template tuning could further improve PromptPATE.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>formatting and calibration improved performance (qualitative)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Contextual calibration reduces systematic probability biases in few-shot outputs, improving label quality used in aggregation and selection. Prompt template and instruction wording affect example presentation and thus LLM behavior; the authors note they did not tune templates and that doing so could further improve results.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>Authors explicitly note as a limitation that they did not tune templates/instructions, implying that untuned templates may underperform a tuned prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Flocks of Stochastic Parrots: Differentially Private Prompt Learning for Large Language Models', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5668.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5668.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>data_efficiency_queries</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Data efficiency and per-query privacy cost vs number of public queries in PromptPATE</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>PromptPATE's performance saturates after a relatively small number of privately-labeled public queries (~100), giving favorable privacy-utility trade-offs because each additional public query consumes privacy but yields diminishing utility gains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (Babbage) (experiment example)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>dbpedia (private) labeled public queries from agnews (public) — PromptPATE ablation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Ablation studying student accuracy as a function of number of public queries labeled privately by the teacher ensemble.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Discrete prompts for teachers; Confident-GNMax noisy aggregation to label public inputs; student prompt derived and evaluated on private test set; vary number of public queries (labels) obtained.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Different numbers of public queries (e.g., 0 → many); privacy budget tracked (ε), and accuracy measured.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Observed that after roughly 100 queries the student test accuracy plateaus; reported that even with ε < 0.2 (after ~100 queries) accuracy saturates, yielding good privacy-utility trade-offs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Utility plateau once public queries ≈100; per-query privacy costs small due to high teacher consensus (no precise per-query ε in text beyond qualitative statement and plotted trends).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>Increasing number of labeled public queries initially improves student prompt utility but exhibits diminishing returns and plateaus (~100 queries); more queries increase privacy consumption with little additional utility after plateau.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Discrete prompts are data-efficient so only a small set of labeled public sequences suffices to produce a strong student prompt; additionally, high consensus among teachers (they are variants of same LLM) reduces noise needed (lower per-query privacy cost), which concentrates utility early and causes saturation.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Flocks of Stochastic Parrots: Differentially Private Prompt Learning for Large Language Models', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Calibrate before use: Improving few-shot performance of language models <em>(Rating: 2)</em></li>
                <li>What makes good in-context examples for gpt-3? <em>(Rating: 2)</em></li>
                <li>P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks <em>(Rating: 2)</em></li>
                <li>The power of scale for parameter-efficient prompt tuning <em>(Rating: 1)</em></li>
                <li>Language models are few-shot learners <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5668",
    "paper_id": "paper-258887717",
    "extraction_schema_id": "extraction-schema-116",
    "extracted_data": [
        {
            "name_short": "MIA_on_prompts",
            "name_full": "Membership Inference Attack on Prompted LLMs",
            "brief_description": "Empirical demonstration that the presence and formulation of demonstrations in discrete prompts (e.g., one-shot examples) makes prompted LLM outputs distinguishably different for member vs non-member examples, enabling strong membership inference attacks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3 (Babbage) / GPT2-xl",
            "model_size": null,
            "task_name": "Membership inference on prompted demonstrations (classification datasets: dbpedia, sst2, agnews, trec)",
            "task_description": "Adversary queries an LLM prompted with a discrete prompt containing demonstration(s) and inspects the output probability assigned to the correct class for candidate examples to infer whether a candidate was included in the prompt.",
            "problem_format": "Discrete prompt with one-shot (single demonstration) or few-shot examples embedded in an instruction template (template from Zhao et al.); attacker queries the prompted model on candidate inputs and inspects next-token/class probabilities for the correct label token.",
            "comparison_format": "Non-member baseline: same query inputs but not present in prompt (50 randomly sampled non-member training points); random-guess baseline (AUC=0.5).",
            "performance": "Example: on dbpedia with GPT3-Babbage one-shot prompting the average AUC-ROC of the MIA = 0.84 (over 100 trials); member output probabilities were significantly higher than non-members.",
            "performance_comparison": "Random guessing AUC-ROC = 0.5; MIA &gt;&gt; random (AUC ~0.84 reported for dbpedia).",
            "format_effect_size": "AUC increase of ~0.34 above random on dbpedia (0.84 vs 0.5).",
            "format_effect_direction": "format increased leakage / improved attack success",
            "explanation_or_hypothesis": "When a data point is included as a demonstration in the prompt, the LLM assigns notably higher probability to the correct class token for that example; this systematic difference between member and non-member probabilities makes membership distinguishable. The paper attributes this to in-context learning behavior and memorization of prompt examples.",
            "counterexample_or_null_result": null,
            "uuid": "e5668.0",
            "source_info": {
                "paper_title": "Flocks of Stochastic Parrots: Differentially Private Prompt Learning for Large Language Models",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "PromptPATE_discrete",
            "name_full": "PromptPATE: PATE-style private discrete prompting (noisy ensemble voting → student prompt)",
            "brief_description": "A method showing that discrete prompt format (natural-language demonstrations) can be used with a private noisy-aggregation (PATE) over an ensemble of differently-prompted LLMs to privately transfer knowledge into a single public discrete student prompt with high utility.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3 (Babbage, Curie) and Claude (v1)",
            "model_size": null,
            "task_name": "Text classification (sst2, agnews, dbpedia, trec and others)",
            "task_description": "Downstream classification tasks where prompts contain demonstration examples and the LLM's next-token predictions map to class labels; teacher ensemble formed by many discrete prompts drawn from private data.",
            "problem_format": "Teachers: discrete natural-language prompts (one-shot or few-shot) built from private examples; for a public unlabeled input the teachers each vote (next-token/class). Aggregation: Confident GNMax (noisy argmax after consensus thresholding) to label public sequences privately. Student: a discrete prompt constructed from one or a few labeled public sequences (selected/validated on held-out labeled public data).",
            "comparison_format": "Compared to zero-shot (instruction-only), non-private one-shot best-example prompt (upper bound), and private ensemble accuracy (no student).",
            "performance": "Example: sst2 with GPT-3 (Curie) PromptPATE student achieved 92.7% accuracy at (ε = 0.147, δ = 1e-6) vs non-private baseline 95.2% (reported in text). Across datasets PromptPATE often matched or closely approached the non-private one-shot baseline while far outperforming zero-shot (e.g., dbpedia→agnews example: PromptPATE 74.6% vs zero-shot 44.2%).",
            "performance_comparison": "Zero-shot significantly lower (example 44.2%); non-private one-shot slightly higher (example sst2 non-private 95.2% vs private 92.7%).",
            "format_effect_size": "Private PromptPATE student often within a few percentage points of the non-private one-shot baseline (example: sst2: −2.5 percentage points from 95.2% to 92.7%); compared to zero-shot, gains of many tens of percentage points (e.g., +30.4% in the dbpedia→agnews example).",
            "format_effect_direction": "discrete few-shot prompting (via PromptPATE) greatly improved downstream accuracy compared to zero-shot and closely matched non-private discrete prompting",
            "explanation_or_hypothesis": "Discrete prompts are highly data-efficient: a small number (often ≲100) of privately-labeled public sequences suffice to construct a high-utility student prompt. High consensus across teacher votes (because teachers are different prompts run on the same underlying LLM) yields low per-query privacy cost, enabling many useful labels under tight privacy budgets. Noisy aggregation ensures DP while transferring the format-specific information from many private prompts into a single public prompt.",
            "counterexample_or_null_result": null,
            "uuid": "e5668.1",
            "source_info": {
                "paper_title": "Flocks of Stochastic Parrots: Differentially Private Prompt Learning for Large Language Models",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "PromptDPSGD_soft",
            "name_full": "PromptDPSGD: DPSGD for private soft-prompt / prefix tuning",
            "brief_description": "Applying differentially-private SGD to train small continuous soft-prompt embeddings (or per-layer prefix embeddings) prepended to a frozen LLM, showing that prompt format (soft vs prefix) can match private fine-tuning on simpler tasks while being far more parameter-efficient.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "RoBERTa (base) (used for PromptDPSGD experiments)",
            "model_size": "125M (RoBERTa-base total params), soft/prefix params &lt;&lt; model",
            "task_name": "GLUE classification tasks (sst2, qnli, qqp, mnli)",
            "task_description": "Standard supervised NLU classification tasks where soft prompts/prefixes are tuned on private downstream data under DP via DPSGD.",
            "problem_format": "Soft prompts: trainable continuous embeddings (prompt length typically 10 tokens) prepended to input; Prefix: embeddings prepended at every layer. Training under privacy via DPSGD operating only on the soft/prefix embeddings (frozen backbone).",
            "comparison_format": "Compared to DP fine-tuning of full model or LoRA-style adapters; also compared to non-private baselines.",
            "performance": "Soft prompts and prefix achieve accuracies close to non-private baselines: reported differences between non-private baseline and private soft prompt range from ~3% (sst2, simple task) up to ~7% (mnli, harder task). For simple tasks (sst2, qnli) soft prompt/prefix matched private fine-tuning performance.",
            "performance_comparison": "Vs full DP fine-tuning: similar for simple tasks; for harder tasks full fine-tuning may outperform (but gap modest).",
            "format_effect_size": "Accuracy reductions vs non-private of ~3% (sst2) to ~7% (mnli) for private soft-prompt (values stated in text summary).",
            "format_effect_direction": "soft/prefix prompting preserved most utility vs full fine-tuning while being much more parameter-efficient",
            "explanation_or_hypothesis": "Because soft prompts / prefix tune orders of magnitude fewer parameters, DPSGD noise is injected into a much smaller parameter set which improves the privacy-utility trade-off and reduces compute/storage costs; the prompt-format (continuous embeddings) is an effective, compact way to present task context to the frozen LLM.",
            "counterexample_or_null_result": "For more challenging tasks (qqp, mnli) soft-prompt/prefix were somewhat worse than full fine-tuning—performance depends on task complexity and number of tuned parameters.",
            "uuid": "e5668.2",
            "source_info": {
                "paper_title": "Flocks of Stochastic Parrots: Differentially Private Prompt Learning for Large Language Models",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "shot_and_model_size_effects",
            "name_full": "Effect of number of in-context examples (shots) and model scale on prompt performance",
            "brief_description": "Empirical finding that increasing the number of demonstrations (shots) in prompts and using larger/more capable LLMs improves prompted downstream performance (and can reduce privacy cost for PromptPATE).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3 family (Babbage, Curie)",
            "model_size": null,
            "task_name": "sst2 (sentiment classification) and agnews (topic classification)",
            "task_description": "Few-shot in-context classification where the prompt contains one or more example demonstrations.",
            "problem_format": "One-shot vs few-shot (e.g., 1-shot vs 4-shot) discrete prompts; also model scale comparison: GPT3-Babbage vs GPT3-Curie.",
            "comparison_format": "1-shot (single demonstration) vs 4-shot; smaller LLM (Babbage) vs larger LLM (Curie).",
            "performance": "Example: sst2 private PromptPATE: GPT3-Curie student accuracy 92.7% (ε=0.147) vs GPT3-Babbage 87.2% (text reports); for agnews they observe improvements in the 4-shot setting (qualitative, consistent with prior non-private studies).",
            "performance_comparison": "Larger model + same prompting led to higher accuracy (example +5.5 percentage points Curie vs Babbage on sst2 in reported experiment) and slightly smaller ε (privacy improved: ε decreased from 0.178 to 0.147 in the reported comparison).",
            "format_effect_size": "Example effect: +5.5% absolute accuracy increase (87.2 → 92.7) for sst2 when using a larger model (Curie) in the private-prompting experiments; 4-shot gave clear improvements for agnews (no precise number reported for private setting in paper).",
            "format_effect_direction": "more shots / larger model → improved performance and favorable privacy-utility trade-off",
            "explanation_or_hypothesis": "Larger models have stronger in-context learning capabilities and more expressive context windows; more demonstration examples reduce variance and provide richer task-specific context to the LLM, so prompts become more informative and performant; this also yields higher teacher consensus in PromptPATE lowering per-query privacy costs.",
            "counterexample_or_null_result": null,
            "uuid": "e5668.3",
            "source_info": {
                "paper_title": "Flocks of Stochastic Parrots: Differentially Private Prompt Learning for Large Language Models",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "prompt_selection_calibration",
            "name_full": "Prompt selection, instruction/template tuning, and contextual calibration effects",
            "brief_description": "The paper reports that template/instruction wording and probability recalibration affect prompted performance; they used a fixed template (Zhao et al.) and applied contextual calibration to improve output probabilities and utility, and note that further tuning of templates could improve PromptPATE.",
            "citation_title": "Calibrate before use: Improving few-shot performance of language models",
            "mention_or_use": "use",
            "model_name": "GPT-3 (Babbage) (recalibration applied)",
            "model_size": null,
            "task_name": "Various classification tasks used with PromptPATE (sst2, agnews, dbpedia, trec)",
            "task_description": "Few-shot in-context classification where output probabilities are recalibrated and candidate prompts are selected based on validation on labeled public data.",
            "problem_format": "Discrete prompt templates (instruction + demonstrations) from prior work (Zhao et al.); contextual calibration applied to teachers' and students' output probabilities; prompt selection based on validation accuracy on labeled public data (not on private data).",
            "comparison_format": "Untuned templates / no calibration vs calibrated outputs and selected prompts (paper used calibrated outputs; they did not tune templates).",
            "performance": "No single numeric delta provided for calibration effect in this paper, but authors state calibration and prompt selection improved utility and that further template tuning could further improve PromptPATE.",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "formatting and calibration improved performance (qualitative)",
            "explanation_or_hypothesis": "Contextual calibration reduces systematic probability biases in few-shot outputs, improving label quality used in aggregation and selection. Prompt template and instruction wording affect example presentation and thus LLM behavior; the authors note they did not tune templates and that doing so could further improve results.",
            "counterexample_or_null_result": "Authors explicitly note as a limitation that they did not tune templates/instructions, implying that untuned templates may underperform a tuned prompt.",
            "uuid": "e5668.4",
            "source_info": {
                "paper_title": "Flocks of Stochastic Parrots: Differentially Private Prompt Learning for Large Language Models",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "data_efficiency_queries",
            "name_full": "Data efficiency and per-query privacy cost vs number of public queries in PromptPATE",
            "brief_description": "PromptPATE's performance saturates after a relatively small number of privately-labeled public queries (~100), giving favorable privacy-utility trade-offs because each additional public query consumes privacy but yields diminishing utility gains.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3 (Babbage) (experiment example)",
            "model_size": null,
            "task_name": "dbpedia (private) labeled public queries from agnews (public) — PromptPATE ablation",
            "task_description": "Ablation studying student accuracy as a function of number of public queries labeled privately by the teacher ensemble.",
            "problem_format": "Discrete prompts for teachers; Confident-GNMax noisy aggregation to label public inputs; student prompt derived and evaluated on private test set; vary number of public queries (labels) obtained.",
            "comparison_format": "Different numbers of public queries (e.g., 0 → many); privacy budget tracked (ε), and accuracy measured.",
            "performance": "Observed that after roughly 100 queries the student test accuracy plateaus; reported that even with ε &lt; 0.2 (after ~100 queries) accuracy saturates, yielding good privacy-utility trade-offs.",
            "performance_comparison": null,
            "format_effect_size": "Utility plateau once public queries ≈100; per-query privacy costs small due to high teacher consensus (no precise per-query ε in text beyond qualitative statement and plotted trends).",
            "format_effect_direction": "Increasing number of labeled public queries initially improves student prompt utility but exhibits diminishing returns and plateaus (~100 queries); more queries increase privacy consumption with little additional utility after plateau.",
            "explanation_or_hypothesis": "Discrete prompts are data-efficient so only a small set of labeled public sequences suffices to produce a strong student prompt; additionally, high consensus among teachers (they are variants of same LLM) reduces noise needed (lower per-query privacy cost), which concentrates utility early and causes saturation.",
            "counterexample_or_null_result": null,
            "uuid": "e5668.5",
            "source_info": {
                "paper_title": "Flocks of Stochastic Parrots: Differentially Private Prompt Learning for Large Language Models",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Calibrate before use: Improving few-shot performance of language models",
            "rating": 2,
            "sanitized_title": "calibrate_before_use_improving_fewshot_performance_of_language_models"
        },
        {
            "paper_title": "What makes good in-context examples for gpt-3?",
            "rating": 2,
            "sanitized_title": "what_makes_good_incontext_examples_for_gpt3"
        },
        {
            "paper_title": "P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks",
            "rating": 2,
            "sanitized_title": "ptuning_v2_prompt_tuning_can_be_comparable_to_finetuning_universally_across_scales_and_tasks"
        },
        {
            "paper_title": "The power of scale for parameter-efficient prompt tuning",
            "rating": 1,
            "sanitized_title": "the_power_of_scale_for_parameterefficient_prompt_tuning"
        },
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 2,
            "sanitized_title": "language_models_are_fewshot_learners"
        }
    ],
    "cost": 0.01723075,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Flocks of Stochastic Parrots: Differentially Private Prompt Learning for Large Language Models
24 May 2023</p>
<p>Haonan Duan haonand@cs.toronto.edu 
University of Toronto and Vector Institute</p>
<p>Adam Dziedzic 
University of Toronto and Vector Institute</p>
<p>Nicolas Papernot 
University of Toronto and Vector Institute</p>
<p>Franziska Boenisch 
University of Toronto and Vector Institute</p>
<p>Flocks of Stochastic Parrots: Differentially Private Prompt Learning for Large Language Models
24 May 2023C027640BDF02670D842C0C4BDFEC3613arXiv:2305.15594v1[cs.LG]
Large language models (LLMs) are excellent in-context learners.However, the sensitivity of data contained in prompts raises privacy concerns.Our work first shows that these concerns are valid: we instantiate a simple but highly effective membership inference attack against the data used to prompt LLMs.To address this vulnerability, one could forego prompting and resort to fine-tuning LLMs with known algorithms for private gradient descent.However, this comes at the expense of the practicality and efficiency offered by prompting.Therefore, we propose to privately learn to prompt.We first show that soft prompts can be obtained privately through gradient descent on downstream data.However, this is not the case for discrete prompts.Thus, we orchestrate a noisy vote among an ensemble of LLMs presented with different prompts, i.e., a flock of stochastic parrots.The vote privately transfers the flock's knowledge into a single public prompt.We show that LLMs prompted with our private algorithms closely match the non-private baselines.For example, using GPT3 as the base model, we achieve a downstream accuracy of 92.7% on the sst2 dataset with (ε = 0.147, δ = 10 −6 )-differential privacy vs. 95.2% for the non-private baseline.Through our experiments, we also show that our prompt-based approach is easily deployed with existing commercial APIs.</p>
<p>Introduction</p>
<p>Large language models (LLMs) exhibit strong capabilities for in-context learning [6,40].By prepending the adequate prompt to an LLM's input, the model can perform a myriad of natural language downstream tasks without any modifications to its parameters [41].While the data used to train an LLM is usually assumed to be public, downstream data used in the prompt is often more sensitive.This can elicit confidentiality issues, for instance, if prompts contain information that represents valuable intellectual property [34].At the same time, it also raises privacy concerns when the data involves personal information about individuals.</p>
<p>In this paper, our first contribution is to show that these concerns are valid.We are the first to instantiate a highly effective membership inference attack (MIA) [7,45] against prompts.Our attack is able to determine if a given data point was used within the prompt of the LLM.The only existing solution to mitigate this privacy risk would be to forego prompting and instead fine-tune the LLM with a privacy-preserving training algorithm [26,54].Yet, fine-tuning lacks the efficiency and practicality of prompting.Indeed, fine-tuning requires significantly more data [42], computational resources [26], and storage space [25].Additionally, fine-tuning requires access to the LLM parameters.However, many of the state-of-the-art LLMs are proprietary models deployed behind an API which only allows its users to query the LLMs [3,6,10,17,35].</p>
<p>Figure 1: Our methods for private prompt learning.Left: PromptDPSGD obtains the input gradients from the LLM, and performs DPSGD to update the soft prompt embedding while keeping the LLM frozen.Right: PromptPATE creates a noisy ensemble of private discrete prompts, and then transfers knowledge by selecting a student prompt that can be publicly released.PromptPATE only needs black-box access of the LLM and, thus, can be easily deployed with commercial APIs.</p>
<p>To leverage the benefits of prompting while at the same time protecting the data contained in prompts, we propose the first algorithms for prompt learning with privacy.Our algorithms offer rigorous guarantees expressed using differential privacy [15].Perhaps closest to existing work on fine-tuning, we propose to leverage the canonical DPSGD algorithm [1] to learn soft prompts with differential privacy guarantees.Our PromptDPSGD algorithm performs a private gradient descent on the soft prompt embeddings prepended to the LLM's private input.Since these embeddings have very few parameters in comparison to LLMs, our PromptDPSGD is efficient and yields competitive privacy utility trade-offs at a fraction of the training complexity of private fine-tuning.</p>
<p>However, learning soft prompts with DPSGD may not always be possible because it requires computing gradients with respect to the prompt input.As mentioned previously, current APIs [3,6,10,17,35] usually do not provide these gradients.We thus turn to discrete prompts which consist of natural language tokens.Discrete prompts address the aforementioned limitations while being more data-efficient.Our insight is to observe that LLMs with discrete prompts naturally lend themselves to another canonical approach of differentially private learning known as the private aggregation of teacher ensembles (PATE) [37].We introduce PromptPATE, which creates an ensemble of LLMs with different discrete prompts from the private dataset which we refer to as a flock of stochastic parrots [5].Since interacting with the flock directly can leak private information about the prompts, as we demonstrate with our MIA, PromptPATE additionally performs a knowledge transfer.Therefore, each model in the flock generates a next token prediction for a short input sequence of some public data.By performing a noisy majority vote over all models' token output, we generate a single output that, due to the noise addition, implements differential privacy guarantees while incorporating knowledge from the flock.The public input together with the noisy aggregated output form a new single example for the discrete student prompt that can be prepended to the LLM in lieu of the individual prompts which contain private information.In addition to providing rigorous privacy guarantees, our PromptPATE is highly efficient, since, instead of having to query every model from the flock at inference time, it suffices to query the LLM prepended with the student prompt once.</p>
<p>We perform extensive experiments against multiple popular LLMs, such as GPT3 [6] and Claude [3], that are deployed behind commercial black-box APIs.Our results highlight that PromptPATE provides high downstream performance that matches the one of non-private prompting even at very strong privacy guarantees.On the sst2 dataset with GPT3, for instance, we reach an accuracy of 92.7% with privacy costs as little as (ε = 0.147, δ = 10 −6 )-differential privacy, even when the public data used during PromptPATE's knowledge transfer stem from a different distribution than sst2.Our results closely matches the non-private baseline accuracy (95.2%).Thus, we conclude that prompt learning for LLMs is not only more efficient and practical than fine-tuning but can also achieve high utility even with strong and practical privacy protection in place.</p>
<p>In summary, we make the following contributions:</p>
<p>• We instantiate the first MIA on prompted LLMs and show that we can effectively infer membership of the prompted data points with high success.</p>
<p>• We propose a lightweight alternative to DP fine-tuning, namely PromptDPSGD, which optimizes orders of magnitude fewer parameters while keeping the original LLM frozen.</p>
<p>• We propose PromptPATE, the first method for DP learning with LLMs that requires only black-box access to the model-making it easily deployable for commercial LLM APIs.</p>
<p>• Our experiments on multiple state-of-the-art commercial APIs [6,3] highlight that our methods achieve both high utility and strong privacy protections in various setups.</p>
<p>Background and Related Work</p>
<p>Prompts for LLMs.The success of LLMs, such as BERT, Claude, OPT, or different versions of GPT and their exceptional in-context learning capacities gave rise to prompt-based learning [14,6,39,40,35,56].Prompts serve as demonstrations of the downstream task, which the model can then generalize from.There are two paradigms for LLM prompting, namely discrete and soft prompts.</p>
<p>Discrete prompts [6,16,18,27,44] are natural-language instructions that contain examples from the downstream task in a well-crafted template.Tuning discrete prompts is often done by prompting the model with different combination of examples, assessing their performance on the downstream task, and choosing the combination that yields the highest performance as the final prompt.</p>
<p>In contrast to discrete prompts, soft prompts [24,27] prepend trainable continuous embeddings to the inputs of LLMs.These embeddings are initialized either at random or with embedding vectors that correspond to tokens from the dictionary.During tuning, the embeddings are updated through gradient descent to minimize the loss of the prompted model on the private downstream task.To increase performance further, trainable embeddings can be prepended not only to the input but also to every LLM layer, a technique known as prefix [25,28,29].</p>
<p>Both soft prompts and prefix train end-to-end without any human involvement through backpropagation over the LLM.On the other hand, discrete prompts have to be designed manually through careful prompt engineering.Yet, prompt engineering only needs inference passes over the LLM which makes discrete prompt more computationally lightweight.Our work provides privacy protection for all of these paradigms: discrete prompts, as well as for soft prompts and prefix.</p>
<p>Privacy Leakage in LLMs.LLMs have been shown to memorize data both from their original large training corpora [8,20,23,32,48,55] and from smaller private datasets used to fine-tune them for downstream tasks [33].The only prior work around privacy leakage in prompt-based learning utilizes prompts to extract knowledge from trained LLMs [13,22,38].In contrast, we study the privacy of the prompting data itself.To do so, we investigate the canonical privacy attack known as membership inference attacks (MIA) [7,45].Its use as a practical means to demonstrate leakage of private information in ML was recently popularized by a line of work on quantifying memorization [9,43,47].While prior work utilizes MIAs to assess whether a given data point was used to train an LLM, we instantiate a MIA to assess whether a given data point was used within the prompt prepended to the inputs of a trained LLM.</p>
<p>Defending Against Privacy Leakage in LLMs.Prior work either focuses on training [2,19] or fine-tuning [26,54] LLMs with privacy guarantees.These approaches rely on the mathematical framework of differential privacy (DP) [15] and in particular the DPSGD algorithm for private stochastic gradient descent [1].Here, DPSGD is applied to guarantee that one outputs approximately the same model parameters whether or not any given data point was used to train or fine-tune the model.To achieve this, DPSGD clips the per-example gradients that are computed during training and adds well-calibrated noise to each model update.These two operations typically increase the computational complexity of training and decrease the utility of the resulting model [1,4,49].To counteract these effects, state-of-the-art methods for full DP-fine tuning in LLMs require extensive hyperparameter tuning and vast computational resources [26].Alternative approaches refrain from updating the large number of model parameters and instead introduce additional layers into the model architecture and only fine-tune these layers with DPSGD [54].To the best of our knowledge, no prior work attempted to provide DP guarantees for prompt data in LLMs.left: We present the prediction probabilities at the correct class for members (the one-shot example) and non-members (50 randomly sampled private points).The output probability for members is significantly higher than for non-member data points.right: We present the AUC-ROC curves of our MIA against the 100 prompts (gray lines) and the blue line as an average over all attacks.Given that each prompt has only one member, the resulting TPRs can only be 0% or 100% which leads to the step-shape of the gray curves.The result indicates that our attack is significantly more successful than random guessing (the red dashed line).</p>
<p>Setup and Notation.We denote by P the soft or discrete prompt that is prepended to any input sequence x i when querying the language model L. For brevity, we denote L([P, x i ]) by L P (x i ). 3he output y i of L P (x i ) is an M -dimensional probability vector, with M being the size of the model's vocabulary.Each component of y i corresponds to the probability that the L P assigns to the respective token for being the next token in the sequence x i .The semantic meaning of the next token varies depending on the given downstream task.For instance, for classification, the index with the highest probability indicates the token of the class that L P assigns to x i .</p>
<p>3 Private Information about Prompt Data Leaks from Prompted LLMs</p>
<p>By instantiating a MIA against prompted LLMs, we want to highlight that the private data used within a prompt (which we refer to as prompt data from hereon) can be subject to a substantial privacy risk.We showcase this risk at the example of LLMs that are prompted with discrete prompts P containing tuples of demonstrations from classification downstream tasks as prompt data p = {(p x , p y )}.For example, in a prompt with one demonstration (one-shot learning), the prompt data p may be specified as p = {("The movie was great.","positive")}.Our prompts are provided in a consistent template where one or multiple demonstrations are combined with instructions as P = [Instruction, (text sequence p x , class-label token p y ), . . .].</p>
<p>For our MIA, we consider an adversary who aims at inferring whether a given private demonstration (p x , p y ) was used within the prompt data p.The adversary holds n candidate demonstrations of text sequences and corresponding labels l i and queries the text sequences
(x 1 , • • • , x n ) to L P with black-box access. The prompted model L P then returns the output probability vectors (y 1 , • • • , y n ).
Following prior work [21,53], we analyze the model's output probability at token y i,li that corresponds to the correct target class label of every x i .The intuition to distinguish between members and non-members is that the output probabilities at the correct class l i will be significantly higher for demonstrations that were used within the prompt, i.e., members with (p x , p y ) = (x i , l i ).We show that even with this simple MIA, we can reliably determine membership for the prompt data.</p>
<p>Experimental Setup.We prompt GPT3-Babbage [6] with multiple one-shot examples to solve four standard downstream text classification tasks, namely dbpedia [57], sst2 [46], agnews [57] and trec [50].The template of our prompts follows [58].To evaluate our MIAs, we consider the single data point used within the prompt as a members and 50 other randomly selected data points from the respective task's training dataset as non-members.This skewed distribution between members and non-members (1 vs 50) corresponds to a realistic scenario where only a small proportion of the candidate data targeted by the adversary are members [21].To quantify the success of our attack, we report the AUC-ROC curves of 100 random trials.</p>
<p>Results.Before evaluating the success of the MIA, we analyze the probability output from GPT3 for the correct target class between member and non-member data points.Figure 2a shows for the dbpedia dataset that the prediction probabilities for non-members are significantly lower than for members.Figure 2b shows that this leads to a high MIA risk in terms of an average AUC score of 0.84 for the prompt data.Similar results for other datasets and models are presented in Appendix D.</p>
<p>These results highlight that private information can leak from prompt data easily and thus motivate the urgent need for defenses which we develop in the rest of this paper.</p>
<p>Methods for Privacy Preserving Prompts</p>
<p>As of now, if we want to protect the private downstream data, we have to forego prompting altogether because, to the best of our knowledge, no algorithms for private prompt learning exist.The only alternative to privately adapt the LLM would be to perform DP fine-tuning [26,54].However, this approach is only feasible when we have direct access to the LLM to update its parameters with DPSGD [26] or to even change the model architecture to insert additional parameters-fine-tuned with DPSGD [54].This is prohibitively expensive and mostly impossible with the commercial API, thus we propose the first algorithms that enable differentially private prompt learning.</p>
<p>We consider two main paradigms of prompting: soft prompts and discrete prompts.To learn private soft prompts, we introduce PromptDPSGD.PromptDPSGD is a parameter-efficient alternative to DP fine-tuning that does not need modifying the parameters or architectures of the LLM.However, many popular APIs [3,6,10,17,35] do not support soft prompts yet as it requires gradients with respect to the input.Therefore, we propose PromptPATE for discrete prompts.PromptPATE requires only black-box access to an LLM without any knowledge of the LLM's architecture or mode of operation.Instead, the algorithm only needs the next-token prediction of the LLM.This, to our knowledge represents the first solution for privately adapting LLMs in restricted API setups.</p>
<p>PromptDPSGD: DPSGD for Private Soft Prompt Learning</p>
<p>In general, all discrete input tokens to LLMs are internally transformed into continuous input embeddings that the LLM then operates on.Soft prompts are just additional continuous input embeddings that can be prepended to the original input embeddings before passing them through the LLM.To train (or tune) soft prompts, we require training data from a potentially private downstream task.After prepending the continuous soft prompt embeddings to input examples from the training data, we can calculate the gradients for the loss of the prompted LLM with respect to these soft prompt embeddings.The gradients provide information about how the soft prompt should be updated in order to minimize the loss on the training data.</p>
<p>If we can obtain the gradients for soft prompts, we can learn these prompts with privacy guarantees by applying the canonical DPSGD algorithm [1].The same applies to prefix, therefore, when we talk about soft prompts in the following, we implicitly also include prefix.We call this approach PromptDPSGD.The algorithm yields soft prompts with DP guarantees that can be deployed with the LLM to solve the respective downstream task.The privacy analysis of PromptDPSGD follows the one of the standard DPSGD.Note, however, that while conceptually similar to fine-tuning the LLM's parameters with DPSGD [54,26], PromptDPSGD differs in a crucial aspect.In DP-SGD fine-tuning, we require the gradients with respect to all or a subset of the model parameters and update these parameters to minimize the loss.In contrast, in PromptDPSGD, we use the gradients with respect to the soft prompt embeddings and only alter these.We highlight this difference in our PromptDPSGD-algorithm that we present in Appendix C.</p>
<p>While this difference seems subtle, it has far-reaching consequences.First, there are orders of magnitude fewer parameters that need to be updated which increases training efficiency.Second, and most importantly, it allows us to keep operating on the original LLM.We discuss the resulting advantages, such as storage efficiency, and the ability to process multiple different tasks simultaneously at the end of this section (in 4.3).These advantages make PromptDPSGD conceptually superior to private fine-tuning.At the same time, as we show in our evaluation, despite the small number of trainable parameters, PromptDPSGD, for simpler tasks, matches the performance of private fine-tuning.Yet, current APIs [3,6,10,17,35] do not support soft prompting, prefix, or private fine-tuning and only provide black-box access through discrete prompts.For these setups, we propose PromptPATE.</p>
<p>PromptPATE: PATE for Privacy Preserving Discrete Prompts</p>
<p>PATE [36,37] enables learning classifiers with DP guarantees.It first trains an ensemble of teacher models on disjoint subsets of the private data.Second, through a noisy labeling process, the ensemble privately transfers its knowledge to an unlabeled public dataset.Finally, a separate student model is trained on this labeled public dataset for release.The noisy knowledge transfer in the second step relies on the Confident GNMAX algorithm [37] that we detail in Appendix C. It consists of three main parts: for any input data point from the public unlabeled dataset, each teacher votes for the most likely class.Then, the consensus over the teachers' votes is determined and queries with low consensus are rejected to avoid revealing too much information about the private decision boundary.Finally, the returned class label for any non-rejected data point is determined as a noisy argmax over all teachers' vote counts-where the added noise is sampled from a Gaussian distribution to implement the DP guarantees.For each rejected or labeled data point from the public dataset, privacy costs are accumulated and the ensemble stops labeling once a target privacy budget is reached.</p>
<p>Our PromptPATE follows the general flow of standard PATE: training the teacher models, private knowledge transfer, and training a student model.However, due to the significant differences between in-context learning for LLMs and supervised learning in the original PATE and how these different paradigms leverage private and public data, we had to redesign each of these building blocks.This allows to leverage both the data-efficiency of prompts and the rigorous privacy protection from PATE.</p>
<p>In the following, we present the building blocks in our PromptPATE.</p>
<p>Teacher Models (Flock of Stochastic Parrots).Instead of training teacher models on disjoint partitions of the private data, we use the private data to create disjoint prompts for the LLM.More specifically, we use examples, for instance {("The movie was great.","positive"), ...}, from the private training data to create prompts that can then be deployed with the LLM as teachers.</p>
<p>Private Knowledge Transfer.During the private knowledge transfer, the teachers label public data sequences, such as ("I did enjoy it.",_).Each teacher votes with the most likely class labels for the private downstream task.In Appendix D, we show that PromptPATE can also operate directly on pure next token predictions from Claude [3] without access to per-token probabilities-enabling full black-box private prompts.By performing the private voting process according to standard PATE with the Confident GNMAX algorithm, we turn our per-teacher predictions into a final class label token that will be appended to the sequence, e.g., ("I did enjoy it", "positive").The privacy accounting and analysis of our PromptPATE exactly follows the one of standard PATE [37].</p>
<p>Student.The most naive way to obtain a student model following standard PATE would be to label many public sequences and train a language classifier using supervised learning on this data.However, due to the relatively high number of data needed for supervised learning, and the fact that each query to the private teachers consumes privacy, this process would incur high privacy costs.We propose a better approach building on the data-efficiency of prompting [42] by using labeled public sequences to create new discrete student prompts.The selected prompt can then be deployed with the LLM as the PromptPATE student model.</p>
<p>In theory, labeling one public sequence by the ensemble would be sufficient to create such a prompt.This approach yields negligible privacy costs, but the resulting prompt might not have good utility due to the high variance in the performance of prompts [58].Therefore, we generate multiple prompts based on different labeled public sequences and perform prompt tuning to select the best student prompt.Care must be taken during selection: utility cannot be evaluated on the private data anymore given that the prompt will be publicly deployed and selecting based on the private data would incur additional privacy costs.We solve this tension by using parts of the newly-labeled public data as validation data to assess utility of the student prompts.By selecting the prompt with the highest validation accuracy, we deploy the student prompt that most resembles the private teachers.</p>
<p>Advantages of (Private) Prompting over (Private) Fine-Tuning</p>
<p>Our private prompt learning enables us to leverage the general advantages of prompting over finetuning while preserving privacy.Private prompting requires significantly less storage than private Dataset M Soft-Prompt (Our) Prefix (Our) Full-Tuning [26] LoRA-Tuning [54] We run the experiment on RoBERTa [30].The first row M: the type of the private Method, the second row P: the number of Parameters tuned for the method, and the third row G: DP Guarantee.We also present results for ε = 3 in Appendix D.
P &lt;10K &lt;100K 125M 1.2M G ε = 8 ε = ∞ ε = 8 ε = ∞ ε = 8 ε = ∞ ε = 8 ε = ∞ sst2 92
fine-tuning.While fine-tuning requires storing a separate copy of the LLM model for each downstream task [24], prompts operate only on the input level of LLMs without adapting model parameters, such that only a small task-specific prompt needs to be stored for each downstream task.For example, each copy of the fine-tuned RoBERTa base model requires 125M parameters (∼500MB).This becomes prohibitively expensive, especially as the number of parameters for state-of-the-art LLMs rapidly increases.In contrast, soft-prompts and prefix, as the one generated by PromptDPSGD (using implementation from [29]) with the standard prompt length of 10 tokens require less than 10K parameters (40KB) for the soft-prompt and 100K parameters (400KB) for the prefix.A discrete prompt, such as the one generated in PromptPATE, requires less than 1 KB of prepended text.Prompts also enable processing many examples from different tasks in a single batch [25], called mixed-task inference.This allows more efficient use of LLMs since we do not have to wait for a sufficient number of requests for a single task before processing them.This is not possible with any form of fine-tuning, where the fine-tuned model can serve solely a single task.</p>
<p>Experimental Evaluation</p>
<p>We evaluate both PromptDPSGD and PromptPATE and show that they match the performance of non-private prompting while providing strong privacy guarantees.</p>
<p>PromptDPSGD</p>
<p>Experimental Setup.To train soft-prompts and prefix, we follow the experimental setup from prior work on DP fine-tuning.Specifically, we use differentially-private optimization engines for transformers, such as models from the BERT family for the language understanding tasks.The experimental results for classification were performed on the RoBERTa models [30], using the standard NLP datasets, namely sst2, qnli, qqp, and mnli, from the GLUE benchmark [51].Our implementation for soft-prompt and prefix is based on P-Tuning v2 [29].To tune the (hyper)parameters for PromptDPSGD, we adjust the length of the soft-prompt or prefix in the private setting (with the default value of 10, which commonly yields good performance).For the privacy parameters, we set the δ = 1/N , where N is the number of data points in a given dataset, The clipping threshold of per-example gradients is set to 0.1 in most cases.We use a batch size of 1024.The detailed selection of (hyper-)parameters is presented in Appendix E.</p>
<p>Results.We compare our PromptDPSGD against state-of-the-art approaches for private finetuning on multiple private downstream datasets.Our results are shown in Table 1.We highlight that both soft prompts and prefix provide competitive privacy utility trade-offs.For example, the difference in accuracy values between the non-private baseline and the private soft prompt ranges from 3% (for the simplest sst2 dataset) and up to 7% (for the most difficult mnli dataset).This mirrors results for other private methods, such as the private fine-tuning of LoRA [54].We also observe that, similarly, for simple tasks, such as sst2 or qnli, the performance of soft prompt or prefix matches the one of fine-tuning.For the more difficult tasks, namely qqp and mnli, the performance of prefix and soft prompts is also relatively close to fine-tuning.The results obtained for these methods are highly influenced by the number of optimized parameters.For example, for the SST2 Table 2: Performance of PromptPATE.We compare PromptPATE with three baselines: zero-shot (Lower Bound), the ensemble's accuracy (Ens.Acc), and the non-private baseline (Upper Bound) on four classification benchmarks.We study two settings, (IID Transfer) when the public dataset is from the same and (OOD Transfer) different distribution than the private data.We find that PromptPATE achieves strong privacy protection (ε &lt; 0.3 at δ = 10 −6 ) and utility close to the non-private and significantly higher than the zero-shot.Unless otherwise specified, the experiments are performed on GPT3-Babbage with one-shot prompts.Additionally, we also run experiments on GPT3-Curie for sst2 (C) and 4-shot prompts for agnews (4).</p>
<p>task and the RoBERTa-Base model, the prefix requires 19970 additional parameters while soft prompt adds solely 2306 parameters.On the other hand, the number of privately tuned parameters is a few orders of magnitude bigger for fine-tuning and equal to the size of the trained model, namely 125M for the method proposed in [26], while the fine-tuning approach from [54] optimizes around 1.2M parameters.Our results reflect a general trend, where prompts are suited for small downstream tasks while fine-tuning with its bigger number of parameters can also cater to more complex tasks with larger training data sets.</p>
<p>PromptPATE</p>
<p>Experimental Setup.Teachers: Unless otherwise specified, we rely on GPT3-Babbage as the base LLM and select one-shot examples randomly without replacement from the private downstream task as prompt data.Our prompt template follows Zhao et al. [58].For each setting, we deploy 200 teacher prompts.Private knowledge transfer: We use the implementation of PATE's Confident GNMAX algorithm and the privacy accounting from [12] and report our algorithm's hyperparameters in Appendix E. Student: For each private downstream task, we experiment with two setups (1) selecting public input sequences from the same (IID) and ( 2) from a different distribution (OOD) as the private data.We introduce three new datasets for the OOD setup: imdb [31], arisetv [11] and qqp [52].The details of preprocessing these datasets can be found in Appendix E. In both the IID and OOD setup, we limit the size of the public dataset to 500 input sequences from the respective datasets.After the ensemble finishes labelling, we select the best labeled public sequence as prompt data based on the validation accuracy on the labeled public set.We repeat the process three times and report average and standard deviation of the test accuracy for the selected student prompt on the private test set.To improve utility, both teachers' and students' output probabilities from GPT3 are recalibrated using contexual calibration [58].</p>
<p>Results.We compare PromptPATE against three baselines: the lower bound baseline represented by a zero-shot prediction (ε = 0), i.e., when the LLM is only prompted with an instruction, the private ensemble accuracy (ε = ∞), and the upper bound as a non-private one-shot prediction (ε = ∞) using the best example from the private data as prompt data.(To save costs, we select from 200 candidates.)Table 2 shows that, over all setups, PromptPATE achieves similar utility to the non-private baseline and significantly improves over zero-shot predictions-even at very strong privacy protection (ε &lt; 0.3, δ = 10 −6 ).Our results also highlight that the distribution of the public data does not need to be very close to the distribution of the private data to yield high-utility student prompts.For example, they can be collected from different domains (dbpedia holds extracts from wikipedia while its public data agnews contains news articles) and for different tasks (trec aims to classify the topic of a given answer while qqp serves to measure the similarity of two questions).Still, with dbpedia being the private downstream data and agnews as public, we achieve an accuracy of 74.6%, which is significantly higher than the zero-shot baseline with 44.2%.We also provide further insights into the privacy-utility trade-offs that can be achieved with Prompt-PATE in Figure 3b.Our results highlight that with more public sequences queried to the ensemble, the privacy consumption increases while, after roughly 100 queries, with even ε &lt; 0.2, the student model's test accuracy saturates.This yields very favorable privacy-utility trade-offs which we attribute mainly to the data efficiency of discrete prompts: Even from within as little as 100 labeled examples, a high-performing student prompt can be derived.Additionally, we observe that the per-query privacy costs of PromptPATE are relatively low, further benefiting the privacy-utility trade-off.The small privacy costs result from the high consensus between the teacher predictions4 , see Figure 3a-that might result from all teachers relying on the same underlying LLM, just with different prompts.</p>
<p>Scalability.Finally, we also study how PromptPATE scales with larger LLMs and more examples in the prompt.We experiment with a more performant LLM (GPT3-Currie) for sst2.Due to the higher per-query costs, we are not able to repeat this experiment for all datasets.Our results show that the performance of our private prompt increases together with the performance of the public prompt (92.7% accuracy on Curie vs. 87.2% on Babbage) while the privacy budget ϵ decreases (from 0.178 to 0.147).To investigate flexibility in terms of numbers of private examples provided as prompt data, we also experiment for agnews with 4-shot teachers.Similar to the non-private study [58] that reports improvements for agnews in the 4-shot setting over 1-shot, we observe that this improvement also translates to the private prompt.Our results indicate that with increasingly more powerful LLMs and larger context windows, private prompting will increase further in terms of privacy-utility trade-offs.</p>
<p>Conclusions and Outlook</p>
<p>By instantiating the first simple yet effective membership inference attack against prompted LLMs, we show that they leak private information about their prompt data.We propose private prompt learning as a holistic and broadly applicable new approach to mitigate this risk.We first introduce PromptDPSGD that enables to train soft-prompts with privacy guarantees.In contrast to finetuning, soft prompts optimize significantly fewer parameters and do not require any update of LLM parameters or changes to its architecture.As the first solution to private downstream learning with LLMs in black-box access scenarios, we propose PromptPATE.PromptPATE builds on the highly data-efficient discrete prompts and implements privacy through a noisy knowledge transfer.Through our evaluation against two popular LLMs deployed behind commercial black-box APIs (GPT3 and Claude) [6,3], we highlight that this method yields downstream performance that matches the one of non-private prompting at very strong privacy guarantees.As LLMs rapidly improve and increase in size, prompts are achieving consistently higher performance while fine-tuning becomes more challenging at this scale.This suggests that privacy protections for prompts will become even more important, especially as context sizes expand.</p>
<p>A Broader Impacts</p>
<p>The growing importance of in-context learning as a paradigm for leveraging LLMs on private downstream tasks has significant implications for privacy.We present the first approaches for obtaining prompts with privacy guarantees, thereby enabling the use of this learning paradigm on sensitive data.This advancement has the potential to increase trust and acceptance of LLM-based systems for private applications.Our approach PromptPATE is the first viable technique for private downstream adaptation of black-box LLMs, which enables integrations into the state-of-the-art commercial LLM APIs.We acknowledge that-as with any application that relies on DP-care must be taken when choosing the privacy parameters ε and δ since setting these incorrectly can lead to a false sense of privacy.Therefore, our work orientates at the privacy parameters that have been shown to provide reasonable protection in prior work.Thereby, we also ensure consistency and comparability in evaluations between the different appraoches.</p>
<p>B Limitations</p>
<p>Tuning Instructions and Templates.For our discrete prompts, we did not tune the instructions or templates but instead relied on a template from prior work [58].The effectiveness and performance of our PromptPATE could potentially be further improved by tuning the instructions and templates.</p>
<p>Privacy Risk of Pretrained LLM: We build on pretrained LLMs to learn and deploy our private prompts.Our methods solely target the protection of the private data used for these prompts.However, it is also important to acknowledge the inherent privacy risks for data used to pretrain the LLM.We leave the pretrainig of LLMs with privacy guarantees to an orthogonal line of work.</p>
<p>Limited Monetary Budget for our Experiments.Due to cost limitations, we were unable to experiment with the latest and best available model, GPT4.Our experiments with GPT3-Curie in comparison to less powerful GPT3-Babbage however indicate the clear trend the our private prompts improve in performance as the non-private baseline improves due to better models.Furthermore, again due to the cost limitation, we were not able to incorporate a larger number of teachers in our experiments for PromptPATE.Therefore, the best non-private teacher baseline that we report might not be the best achievable if one had more teachers to choose from.We chose from 200 and note that with more (and potentially better teachers), not only the baseline but also the teacher ensemble's performance would get better.</p>
<p>Hyperparameter Tuning.To save computation costs, we did not exhaustively tune all hyperparameters in our experiments.While our approach still achieves high utility and good privacy-utility trade-offs, we acknowledge that with more hyperparameter tuning the performance together with the understanding of optimal configurations for private prompt learning could increase.</p>
<p>Assumption of a Trusted LLM API Provider.In our work, the API provider gets to interact with the private data, for example, through the teachers' prompts in PromptPATE.Therefore, we have to assume trust in the API provider.The privacy guarantees through our private prompt learning protect the privacy of the prompt data against users that interact with the prompted LLM.In practice, companies that are concerned about the privacy of their data with respect to the API provider could make contracts with the API providers on the use of their data or buy access plans that guarantee that data queried to the API is treated privately.We leave implementing cryptographic approaches that could relief the assumption on trusting the API provider entirely, for example, by enabling the LLM to run inference on encrypted private data to future work.</p>
<p>C Additional Insights into our Methods</p>
<p>C.1 PromptDPSGD</p>
<p>We present the full PromptDPSGD algorithm in Algorithm 1.</p>
<p>Algorithm 1: PromptDPSGD.In contrast to the standard DPSGD algorithm that updates model parameters during private training or fine-tuning, our PromptDPSGD privately updates the soft prompt parameters.We highlight these changes with respect to standard DPSGD training or fine-tuning in blue.</p>
<p>C.2 PromptPATE</p>
<p>Extended Background on PATE.We include the standard Confident-GNMax Aggregator Algorithm from [37] below.</p>
<p>Algorithm 2: Confident-GNMax Aggregator by [37] Require: input x, threshold T , noise parameters σ 1 and σ 2
1: if max j { i∈[E] n i,j (x)} + N (0, σ 2 1 ) ≥ T then 2: Output arg max j { i∈[E] n i,j (x) + N (0, σ 2 2 )} 3: else 4:
Output ⊥ 5: end if</p>
<p>C.3 Privacy Analysis</p>
<p>PromptDPSGD.Our PromptDPSGD can be seen as a repeated sampled Gaussian mechanism [1], with sampling performed over the entirety of the private prompt dataset.The difference to standard DPSGD for training or fine-tuning is that we do not update the model parameters, but the trainable embeddings for the soft prompts.This is conceptually different from standard DPSGD in terms of which parameters are updated.The privacy guarantees of the training mechanism still follow Abadi et al. [1], but with respect to the soft prompt embeddings: whether or not a particular data point will be included in the private training set used for tuning the prompt, the resulting soft prompt embeddings after training will be roughly the same.Especially by applying the clipping operation at every step, each mechanism's sensitivity is bounded by c. Privacy is then implemented as the trainable soft prompt embeddings are updated while adding noise noise drawn from N (0, c 2 σ 2 I).Theorem 1 (Privacy of PromptDPSGD).Let T be the total number of repetitions (training iterations) of our PromptDPSGD and the sampling rate be denoted by q.Then, there exist two constants c 1 and c 2 , such that for any ε &lt; c 1 q 2 T our PromptDPSGD guarantees (ε, δ)-DP, if for any δ &gt; 0, we choose the noise according to σ ≥ c 2 qc √ T log 1/δ ε .</p>
<p>Proof.The proof follows the one by Abadi et al. [1], using their moments accountant that models the privacy loss as a random variable dependent on the stochastic noise added.</p>
<p>PromptPATE.</p>
<p>Our PromptPATE relies entirely on the Confident GNMAX algorithm from Papernot et al. [37].We preserve the assumption underlying the algorithm and the respective privacy analysis that the sensitivity during the voting mechanism equals one.This is done in PromptPATE by assigning disjoint data points from the private prompt downstream dataset to all teachers.As a consequence, the privacy analysis of our PromptPATE entirely follows Papernot et al. [37].</p>
<p>Both our PromptDPSGD and PromptPATE experience the post-processing properties of DP, i.e., once trained, the privacy guarantee (ε, δ) sets an upper bound on privacy leakage for the prompt data, independent on the number and type of queries that will be posed to the final prompted LLM.</p>
<p>D Additional Results</p>
<p>D.1 Membership Inference Attacks</p>
<p>We present the full results of MIA against GPT3 with one-shot prompts on 4 datasets in 4.  We present the prediction probabilities at the correct class for members (the one-shot example) and non-members (50 randomly sampled private points).The output probability for members is significantly higher than for non-member data points.bottom: We present the AUC-ROC curves of our MIA against the 100 prompts (gray lines) and the blue line as an average over all attacks.Given that each prompt has only one member, the resulting TPRs can only be 0% or 100% which leads to the step-shape of the gray curves.The result indicates that our attack is significantly more successful than random guessing (the red dashed line).</p>
<p>In addition, we also perform similar experiments on GPT2-xl with four-shot examples, with results presented in Figure 5.We replace dbpedia with cb because the input in dbpedia is usually longer than the context length of GPT2.</p>
<p>D.2 PromptPATE on Claude</p>
<p>We present the experiment results of PromptPATE on Claude [3].Different from GPT3 that outputs logits over the whole vocabulary, Claude only gives us access to the next most likely token.</p>
<p>Experimental Setup.Teachers: We rely on Claude-v1 as the base LLM.We use 2-shot prompts for sst2 and agnews, 4-shot for trec and 1-shot for dbpedia.We set the maximum generated tokens to 1 and temperatures to 0. We also create an "other" category in case the moel's output does not fall under any specified categories.For each setting, we deploy 400 teacher prompts.Private knowledge transfer: We use the implementation of PATE's Confident GNMAX algorithm and the privacy accounting from [12] and report our algorithm's hyperparameters in Appendix E. Student: We limit the size of the public dataset to 200 input sequences from the respective datasets.The number of shots for students corresponds with the teachers.</p>
<p>D.3 More results for PromptDPSGD</p>
<p>We present the additional results for PromptDPSGD with ε = 3 on the classification tasks in Table 5.We present the prediction probabilities at the correct class for members (the one-shot example) and non-members (50 randomly sampled private points).The output probability for members is significantly higher than for non-member data points.bottom: We present the AUC-ROC curves of our MIA against the 100 prompts (gray lines) and the blue line as an average over all attacks.Given that each prompt has only one member, the resulting TPRs can only be 0%, 25%, 50%, 75% or 100% which leads to the step-shape of the gray curves.The result indicates that our attack is significantly more successful than random guessing (the red dashed line).We compare PromptPATE with three baselines: zero-shot (Lower Bound), the ensemble's accuracy (Ens.Acc), and the non-private baseline (Upper Bound) on four classification benchmarks.We find that PromptPATE achieves strong privacy protection (ε &lt; 0.1 at δ = 10 −6 ) and utility close to the non-private and significantly higher than the zero-shot.</p>
<p>Lower</p>
<p>E Additional Setup E.1 PromptDPSGD</p>
<p>We train PromptDPSGD on NVIDIA A100 GPUs.We execute (hyper-)parameter search that takes into account learning rate (LR), max grad norm (GRAD), number of epochs (Epochs), the token length of prefix and prompt.In general, we find that the prompt and prefix token length of 10 is close to the optimal value in most cases.For the private (hyper-)parameters, in most cases we tune for ε = 8 and use similar (or even the same) parameters for other ε values.We set the max grad norm to 0.1 in most cases and then adjust the number of epochs (the more the better, for example, 100), and the learning rate [54]5 .The batch size is set by default to 1024.</p>
<p>We show the specific parameters chosen for PromptDPSGD in Table 6.</p>
<p>Dataset</p>
<p>M Soft-Prompt (Our) Prefix (Our) Full-Tuning [26] LoRA-Tuning [54]  1.
P &lt;10K &lt;100K 125M 1.2M G ε = 3 ε = ∞ ε = 3 ε = ∞ ε = 3 ε = ∞ ε = 3 ε = ∞ SST2 90</p>
<p>E.2 PromptPATE</p>
<p>E.2.1 Hyperparameters for Confident-GNMax</p>
<p>We present our hyperparameters for Confident-GNMax in Table 7.</p>
<p>E.2.2 Dataset Preprocessing</p>
<p>sst2, trec, agnews, dbpedia and cb are taken from the repo of [58].All other public datasets are downloaded from huggingface.To reduce the cost of quering APIs, we randomly sample 300 points from the test set to report the test accuracy.For imdb, we random select one sentence from each entry and also remove the <br/> tag.For qqp, we only take the column of "question 1" in the public set.</p>
<p>Figure 2 :
2
Figure 2: MIA Risk.We study GPT3 prompted with 100 different one-shot examples (dbpedia).left:We present the prediction probabilities at the correct class for members (the one-shot example) and non-members (50 randomly sampled private points).The output probability for members is significantly higher than for non-member data points.right: We present the AUC-ROC curves of our MIA against the 100 prompts (gray lines) and the blue line as an average over all attacks.Given that each prompt has only one member, the resulting TPRs can only be 0% or 100% which leads to the step-shape of the gray curves.The result indicates that our attack is significantly more successful than random guessing (the red dashed line).</p>
<p>Figure 3 :
3
Figure 3: Additional Insights of PromptPATE.We perform ablation studies on GPT3-Babbage and use dbpedia as private and agnews as public data.Left: Teacher consensus as the fraction of teachers who vote for the correct class over 500 public input sequences.PromptPATE achieves overall high consensus.Right: Student accuracy as a function of the public query set's size.Already with as few as 100 queries, we observe a plateau in accuracy which highlights PromptPATE's data efficiency.</p>
<p>Figure 4 :
4
Figure 4: MIA Risk over Multiple Datasets on GPT3.We study GPT3-babbage prompted with 100 different one-shot examples on four datasets.top:We present the prediction probabilities at the correct class for members (the one-shot example) and non-members (50 randomly sampled private points).The output probability for members is significantly higher than for non-member data points.bottom: We present the AUC-ROC curves of our MIA against the 100 prompts (gray lines) and the blue line as an average over all attacks.Given that each prompt has only one member, the resulting TPRs can only be 0% or 100% which leads to the step-shape of the gray curves.The result indicates that our attack is significantly more successful than random guessing (the red dashed line).</p>
<p>Figure 5 :
5
Figure 5: MIA Risk over Multiple Datasets on GPT2-xl (4 shot).We study GPT2-xl prompted with 100 different four-shot examples on four datasets.top:We present the prediction probabilities at the correct class for members (the one-shot example) and non-members (50 randomly sampled private points).The output probability for members is significantly higher than for non-member data points.bottom: We present the AUC-ROC curves of our MIA against the 100 prompts (gray lines) and the blue line as an average over all attacks.Given that each prompt has only one member, the resulting TPRs can only be 0%, 25%, 50%, 75% or 100% which leads to the step-shape of the gray curves.The result indicates that our attack is significantly more successful than random guessing (the red dashed line).</p>
<p>Table 1 :
1
Performance of PromptDPSGD.We report the accuracy values (%) for each dataset.All ε values are reported as standard DP guarantees.
.3195.6491.97 96.33 85.8996.4092.9796.60qnli84.1189.4887.17 94.84 84.8194.7088.5994.70qqp81.5286.5682.58 91.42 86.1592.2086.2692.20mnli75.1582.4980.57 90.34 83.3090.2082.9290.20</p>
<p>Private downstream data D = {(xi, yi) | i ∈ [N ]}, prompt sequence length s, embedding dimensionality e, trained LLM L with frozen parameters, loss function ℓ(Lp, x) for prompted LLM, Params: learning rate ηt, noise scale σ, sampling rate q, max gradient norm c, training iterations T .1: Initialize P0 ∈ R s×e at random 2: for t ∈ [T ] do 3: Sample mini-batch Bt according to sampling rate q from D {Poisson sampling} 4: For each i ∈ |Bt|, compute gt(xi) ← ∇P t ℓ(LP , xi) {Compute per sample gradient w.r.t.pt} Output pT and compute the overall privacy cost (ε, δ).
5:ḡt(xi) ← gt(xi)/ max 1, ∥g t (x i )∥ 2 c{Clip gradient}6:gt ← 1 |B t |i ḡt(xi) + N 0, σ 2 c 2 I {Add noise}7:Pt+1 ← Pt −ηt gt {Update soft prompt}8: end for9:
Require:</p>
<p>Table 3 :
3
Performance of PromptPATE on Claude.
Ens.UpperOur PromptPATEBoundAcc.BoundPrivateε = 0 ε = ∞ ε = ∞PublicεTest accsst292.796.098.0sst20.048 95.7 ± 1.4agnews72.479.182.7agnews 0.056 74.6 ± 1.5trec69.079.982.2trec0.068 79.3 ± 1.2dbpedia88.092.493.5dbpedia 0.042 90.9 ± 0.6</p>
<p>Table 4 :
4
Private classification with soft prompts and prefix for ε = {3, ∞} and the RoBERTa BASE model.We use the same setup and notation as in Table1.
.4895.6490.37 96.33 91.8696.4092.6096.60QNLI83.6289.4886.05 94.84 87.4294.7086.9794.70QQP80.2986.5680.89 91.42 85.5692.2085.1292.20MNLI73.9782.4980.10 90.34 82.9990.2082.0890.20DatasetM Soft-Prompt (Our) Prefix (Our) Full-Tuning [26]P&lt;10K&lt;100K125MSST290.7193.5890.94QNLI87.6289.4589.42QQP82.2983.5087.49MNLI76.0586.4086.28</p>
<p>Table 5 :
5
Private classification with soft prompts and prefix for ε = 8 and the RoBERTa LARGE model.We use the same setup and notation as in Table</p>
<p>In the prefix method, LP denotes prepending trainable parameters to every layer's input, and not only to the model input xi.
As motivated in Section 4.2, high consensus reveals less information about the private decision boundary, and hence incurs smaller privacy costs.
We would like to thank the authors of[54] for their help, especially for the very useful and practical pieces of advice on how to tune the parameters for differential privacy from Huseyin A. Inan.
AcknowledgmentsWe would like to acknowledge our sponsors, who support our research with financial and in-kind contributions: Amazon, Apple, CIFAR through the Canada CIFAR AI Chair, DARPA through the GARD project, Intel, Meta, NSERC through a Discovery Grant, the Ontario Early Researcher Award, and the Sloan Foundation.Resources used in preparing this research were provided, in part, by the Province of Ontario, the Government of Canada through CIFAR, and companies sponsoring the Vector Institute.We also thank members of the CleverHans Lab for their feedback.
Deep learning with differential privacy. Martin Abadi, Andy Chu, Ian Goodfellow, Brendan Mcmahan, Ilya Mironov, Kunal Talwar, Li Zhang, Proceedings of the 2016 ACM SIGSAC conference on computer and communications security. the 2016 ACM SIGSAC conference on computer and communications security2016</p>
<p>. Rohan Anil, Badih Ghazi, Vineet Gupta, Ravi Kumar, Pasin Manurangsi, arXiv:2108.016242021Large-scale differentially private bert. arXiv preprint</p>
<p>Introducing claude. Antropic Website. Antropic, 2023</p>
<p>Private empirical risk minimization: Efficient algorithms and tight error bounds. Raef Bassily, Adam Smith, Abhradeep Thakurta, 2014 IEEE 55th annual symposium on foundations of computer science. IEEE2014</p>
<p>On the dangers of stochastic parrots: Can language models be too big?. Emily M Bender, Timnit Gebru, Angelina Mcmillan-Major, Shmargaret Shmitchell, Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT '21. the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT '21New York, NY, USAAssociation for Computing Machinery2021</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Membership inference attacks from first principles. Nicholas Carlini, Steve Chien, Milad Nasr, Shuang Song, Andreas Terzis, Florian Tramer, 2022 IEEE Symposium on Security and Privacy (SP). IEEE2022</p>
<p>Quantifying memorization across neural language models. Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, Chiyuan Zhang, arXiv:2202.076462022arXiv preprint</p>
<p>Extracting training data from large language models. Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, 30th USENIX Security Symposium (USENIX Security 21). 2021</p>
<p>Lamda: Towards safe, grounded, and high-quality dialog models for everything. Heng-Tze Cheng, Romal Thoppilan, 2022. 2023-05-09Google Blog Post. </p>
<p>. Samuel Okite, 2022news-data. Huggingface</p>
<p>Capc learning: Confidential and private collaborative learning. Natalie Christopher A Choquette-Choo, Adam Dullerud, Yunxiang Dziedzic, Somesh Zhang, Nicolas Jha, Xiao Papernot, Wang, International Conference on Learning Representations. 2021</p>
<p>Commonsense knowledge mining from pretrained models. Joe Davison, Joshua Feldman, Alexander M Rush, Proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing (EMNLP-IJCNLP). the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing (EMNLP-IJCNLP)2019</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, arXiv:1810.048052018arXiv preprint</p>
<p>Differential privacy. Cynthia Dwork, Automata, Languages and Programming: 33rd International Colloquium, ICALP 2006. Venice, ItalySpringerJuly 10-14, 2006. 2006Proceedings, Part II 33</p>
<p>Making pre-trained language models better few-shot learners. Tianyu Gao, Adam Fisch, Danqi Chen, arXiv:2012.157232020arXiv preprint</p>
<p>Towards safe, grounded, and high-quality dialog models for everything. Website. Google, Lamda, 2023. 2023</p>
<p>Efficient (soft) q-learning for text generation with limited good data. Han Guo, Bowen Tan, Zhengzhong Liu, Eric Xing, Zhiting Hu, Findings of the Association for Computational Linguistics: EMNLP 2022. 2022</p>
<p>Learning and evaluating a differentially private pre-trained language model. Shlomo Hoory, Amir Feder, Avichai Tendler, Sofia Erell, Alon Peled-Cohen, Itay Laish, Hootan Nakhost, Uri Stemmer, Ayelet Benjamini, Avinatan Hassidim, Findings of the Association for Computational Linguistics: EMNLP 2021. 2021</p>
<p>Preventing verbatim memorization in language models gives a false sense of privacy. Daphne Ippolito, Florian Tramèr, Milad Nasr, Chiyuan Zhang, Matthew Jagielski, Katherine Lee, Christopher A Choquette-Choo, Nicholas Carlini, arXiv:2210.175462022arXiv preprint</p>
<p>Revisiting membership inference under realistic assumptions. Bargav Jayaraman, Lingxiao Wang, Katherine Knipmeyer, Quanquan Gu, David Evans, Proceedings on Privacy Enhancing Technologies. on Privacy Enhancing Technologies20212021</p>
<p>How can we know what language models know? Transactions of the. Zhengbao Jiang, Frank F Xu, Jun Araki, Graham Neubig, 2020Association for Computational Linguistics8</p>
<p>How bpe affects memorization in transformers. Eugene Kharitonov, Marco Baroni, Dieuwke Hupkes, arXiv:2110.027822021arXiv preprint</p>
<p>The power of scale for parameter-efficient prompt tuning. Brian Lester, Rami Al-Rfou, Noah Constant, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingNovember 2021</p>
<p>Prefix-tuning: Optimizing continuous prompts for generation. Lisa Xiang, Percy Li, Liang, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing20211</p>
<p>Large language models can be strong differentially private learners. Xuechen Li, Florian Tramer, Percy Liang, Tatsunori Hashimoto, International Conference on Learning Representations. 2022</p>
<p>Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, Weizhu Chen, arXiv:2101.06804What makes good in-context examples for gpt-3?. 2021arXiv preprint</p>
<p>Ptuning: Prompt tuning can be comparable to fine-tuning across scales and tasks. Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengxiao Du, Zhilin Yang, Jie Tang, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational LinguisticsMay 20222Short Papers)</p>
<p>P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks. Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Lam Tam, Zhengxiao Du, Zhilin Yang, Jie Tang, arXiv:2110.076022021arXiv preprint</p>
<p>Ro{bert}a: A robustly optimized {bert} pretraining approach. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, 2020</p>
<p>Learning word vectors for sentiment analysis. Andrew Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, Christopher Potts, Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies. the 49th annual meeting of the association for computational linguistics: Human language technologies2011</p>
<p>How much do language models copy from their training data? evaluating linguistic novelty in text generation using raven. Thomas Mccoy, Paul Smolensky, Tal Linzen, Jianfeng Gao, Asli Celikyilmaz, arXiv:2111.095092021arXiv preprint</p>
<p>Fatemehsadat Mireshghallah, Archit Uniyal, Tianhao Wang, David Evans, Taylor Berg-Kirkpatrick, arXiv:2205.12506Memorization in nlp fine-tuning methods. 2022arXiv preprint</p>
<p>Samsung fab data leak: How chatgpt exposed sensitive information. electropages. Robin Mitchell, 2023</p>
<p>. OpenAI. Gpt-4 technical report. 2023</p>
<p>Semisupervised knowledge transfer for deep learning from private training data. Nicolas Papernot, Martín Abadi, Úlfar Erlingsson, Ian Goodfellow, Kunal Talwar, International Conference on Learning Representations. 2017</p>
<p>Scalable private learning with pate. Nicolas Papernot, Shuang Song, Ilya Mironov, Kunal Ananth Raghunathan, Ulfar Talwar, Erlingsson, International Conference on Learning Representations. 2022</p>
<p>Fabio Petroni, Tim Rocktäschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H Miller, Sebastian Riedel, arXiv:1909.01066Language models as knowledge bases?. 2019arXiv preprint</p>
<p>Improving language understanding by generative pre-training. Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, 2018OpenAI</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI blog. 1892019</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, The Journal of Machine Learning Research. 2112020</p>
<p>How many data points is a prompt worth?. Le Teven, Alexander M Scao, Rush, arXiv:2103.084932021arXiv preprint</p>
<p>Membership inference attacks against NLP classification models. Virat Shejwalkar, Amir Huseyin A Inan, Robert Houmansadr, Sim, NeurIPS 2021 Workshop Privacy in Machine Learning. 2021</p>
<p>Autoprompt: Eliciting knowledge from language models with automatically generated prompts. Taylor Shin, Yasaman Razeghi, Robert L Logan, I V , Eric Wallace, Sameer Singh, arXiv:2010.159802020arXiv preprint</p>
<p>Membership inference attacks against machine learning models. Reza Shokri, Marco Stronati, Congzheng Song, Vitaly Shmatikov, 2017 IEEE symposium on security and privacy (SP). IEEE2017</p>
<p>Recursive deep models for semantic compositionality over a sentiment treebank. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, Christopher Potts, Proceedings of the 2013 conference on empirical methods in natural language processing. the 2013 conference on empirical methods in natural language processing2013</p>
<p>Auditing data provenance in text-generation models. Congzheng Song, Vitaly Shmatikov, Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining2019</p>
<p>Memorization without overfitting: Analyzing the training dynamics of large language models. Kushal Tirumala, Aram H Markosyan, Luke Zettlemoyer, Armen Aghajanyan, arXiv:2205.107702022arXiv preprint</p>
<p>Differentially private learning needs better features (or much more data. Florian Tramer, Dan Boneh, International Conference on Learning Representations. 2021</p>
<p>Building a question answering test collection. M Ellen, Dawn M Voorhees, Tice, Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval. the 23rd annual international ACM SIGIR conference on Research and development in information retrieval2000</p>
<p>GLUE: A multi-task benchmark and analysis platform for natural language understanding. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel R Bowman, International Conference on Learning Representations. 2019</p>
<p>Bilateral multi-perspective matching for natural language sentences. Zhiguo Wang, Wael Hamza, Radu Florian, Proceedings of the 26th International Joint Conference on Artificial Intelligence. the 26th International Joint Conference on Artificial Intelligence2017</p>
<p>Privacy risk in machine learning: Analyzing the connection to overfitting. Samuel Yeom, Irene Giacomelli, Matt Fredrikson, Somesh Jha, 2018 IEEE 31st computer security foundations symposium (CSF). IEEE2018</p>
<p>Differentially private fine-tuning of language models. Da Yu, Saurabh Naik, Arturs Backurs, Sivakanth Gopi, Gautam Huseyin A Inan, Janardhan Kamath, Yin Tat Kulkarni, Andre Lee, Lukas Manoel, Sergey Wutschitz, Huishuai Yekhanin, Zhang, International Conference on Learning Representations. 2022</p>
<p>Chiyuan Zhang, Daphne Ippolito, Katherine Lee, Matthew Jagielski, Florian Tramèr, Nicholas Carlini, arXiv:2112.12938Counterfactual memorization in neural language models. 2021arXiv preprint</p>
<p>Opt: Open pre-trained transformer language models. Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, arXiv:2205.010682022arXiv preprint</p>
<p>Character-level convolutional networks for text classification. Xiang Zhang, Junbo Zhao, Yann Lecun, Advances in neural information processing systems. 282015</p>
<p>Calibrate before use: Improving few-shot performance of language models. Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, Sameer Singh, International Conference on Machine Learning. PMLR2021</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3593 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3593</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3593</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-79.html">extraction-schema-79</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate or design novel chemicals for specific applications, including details of the model, the application, the generation method, evaluation metrics, results, and any reported limitations or challenges.</div>
                <p><strong>Paper ID:</strong> paper-2b3554a8fea6f123fc04bd3e120f2293f227e1b2</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/2b3554a8fea6f123fc04bd3e120f2293f227e1b2" target="_blank">InstructMol: Multi-Modal Integration for Building a Versatile and Reliable Molecular Assistant in Drug Discovery</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> InstructMol, a multi-modal LLM, effectively aligns molecular structures with natural language via an instruction-tuning approach, utilizing a two-stage training strategy that adeptly combines limited domain-specific data with molecular and textual information.</p>
                <p><strong>Paper Abstract:</strong> The rapid evolution of artificial intelligence in drug discovery encounters challenges with generalization and extensive training, yet Large Language Models (LLMs) offer promise in reshaping interactions with complex molecular data. Our novel contribution, InstructMol, a multi-modal LLM, effectively aligns molecular structures with natural language via an instruction-tuning approach, utilizing a two-stage training strategy that adeptly combines limited domain-specific data with molecular and textual information. InstructMol showcases substantial performance improvements in drug discovery-related molecular tasks, surpassing leading LLMs and significantly reducing the gap with specialized models, thereby establishing a robust foundation for a versatile and dependable drug discovery assistant.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3593.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3593.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate or design novel chemicals for specific applications, including details of the model, the application, the generation method, evaluation metrics, results, and any reported limitations or challenges.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InstructMol</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>InstructMol (this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multimodal molecular assistant built by instruction-tuning a 7B LLM (Vicuna-v1.3-7B) with a pre-aligned graph encoder (MoleculeSTM GIN) via a two-stage training (alignment pretraining on molecule-text pairs then task-specific instruction finetuning using LoRA); it generates text tokens (including SELFIES) conditioned on molecular graph and sequence inputs for tasks such as reaction product prediction, reagent prediction, retrosynthesis, property prediction and molecule description generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InstructMol (multimodal LLM built on Vicuna-v1.3-7B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Base LLM: Vicuna-v1.3-7B (derived from LLaMA; base LLM size ~6.9B parameters for whole system). Molecular encoder: GIN (5-layer) initialized from MoleculeSTM (pretrained via graph-text contrastive learning; encoder ~1.8M params). A lightweight linear alignment projector maps node-level graph embeddings into the LLM token embedding space. Training: two stages — (1) alignment pretraining on ~264K–330K PubChem molecule-text pairs with frozen LLM and graph encoder to train the projector, (2) task-specific instruction tuning updating projector + LLM via LoRA (rank 64, α=16) on datasets (QM9, MoleculeNet, ChEBI-20, USPTO variants). Inputs: concatenated graph tokens and optional SELFIES sequence tokens; outputs: autoregressive text (including SELFIES) produced by the LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Autoregressive generation of textual molecular representations (SELFIES) conditioned on concatenated molecular graph token embeddings + optional SELFIES; instruction-following prompting; two-stage alignment projector + LoRA finetuning on downstream instruction datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Drug discovery / chemical reaction prediction and design tasks: forward reaction product generation, reagent prediction, retrosynthesis (precursor suggestion), molecular property prediction, and molecule description generation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Validity (RDKit), Exact Match, BLEU, Levenshtein distance, fingerprint Tanimoto similarity metrics (RDKit fingerprint Tanimoto Similarity, MACCS FTS, Morgan FTS), BLEU/ROUGE/METEOR for description generation, MAE for QM regression (HOMO/LUMO/gap), ROC-AUC for classification tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>In multimodal generation tasks InstructMol substantially outperformed generalist LLM baselines and improved over prior single-modality instruction-tuned LLMs: example highlights from reported tables (InstructMol-GS = graph+sequence input): Reagent prediction — Exact: 0.129, BLEU: 0.610, Levenshtein: 19.664, RDKit FTS: 0.444, Validity: 1.000. Forward reaction prediction — Exact: 0.536, BLEU: 0.967, Levenshtein: 10.851, RDKit FTS: 0.776, Validity: 1.000. Retrosynthesis — Exact: 0.407, BLEU: 0.941, Levenshtein: 13.967, RDKit FTS: 0.753, Validity: 1.000. QM9 regression (MAE, hartree): InstructMol-GS Avg MAE ≈ 0.005. Molecule description generation (ChEBI-20): InstructMol-GS BLEU-2 = 0.475, BLEU-4 = 0.371, ROUGE-1 = 0.566, METEOR = 0.509. These results show high validity (RDKit-based) of generated molecules and strong similarity/fidelity metrics compared to other generalist LLM approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Outperforms generalist LLM baselines (Vicuna/LLaMA few-shot, Alpaca/Baize, ChatGLM) and prior single-modal instruction-tuned models (e.g., Mol-Instruction) on reaction generation, description generation, and property tasks; still behind or comparable to some specialist domain models on select benchmarks (specialist models like Retroformer or MolT5 variants outperform InstructMol on some retrosynthesis/description metrics). Incorporating both graph and sequence (GS) performs better than graph-only (G) in generation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Reported limitations include dependence on dataset scale and quality (scarcity of high-quality annotation limits generalization), remaining gap to specialist models on certain tasks, challenges with long-tail / imbalanced distributions (noted worse performance on the HIV dataset), potential modality-alignment shortcomings (need for further refinement of multimodal alignment), hallucination risk in free-text generations, and the lack of domain-specific large LLMs tailored for chemistry; the paper emphasizes dataset scarcity and the need for more rigorous evaluation to detect hallucinations. SELFIES reduces syntactic invalidity but does not eliminate all domain relevance or synthesizability concerns.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'InstructMol: Multi-Modal Integration for Building a Versatile and Reliable Molecular Assistant in Drug Discovery', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3593.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3593.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate or design novel chemicals for specific applications, including details of the model, the application, the generation method, evaluation metrics, results, and any reported limitations or challenges.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mol-Instruction</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mol-instructions (Fang et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large-scale biomolecular instruction dataset and associated instruction-tuned LLM baselines (reported in Fang et al.) used for molecular tasks; used as a baseline in this paper for many downstream tasks (property prediction, reaction prediction, molecule description).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mol-instructions: A large-scale biomolecular instruction dataset for large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mol-Instruction (instruction-tuned LLM baseline, Llama2-based in reported experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Reported in Fang et al.; described here as an instruction-tuned LLM approach using Llama2 as base for single-modality (text/sequence) instruction tuning on molecular tasks. The present paper uses reported Mol-Instruction numbers as a baseline (no reimplementation reported here).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Single-modality instruction-tuning of a base LLM (Llama2) to autoregressively produce textual molecular outputs (e.g., SMILES/SELFIES/products) given textual or sequence inputs; no multimodal graph tokens used in the original baseline as reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Drug discovery / reaction prediction, reagent prediction, retrosynthesis, property prediction, and molecule caption/description generation (used as baseline comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Same metrics presented in the paper: Validity (RDKit), BLEU, Exact Match, Levenshtein distance, fingerprint Tanimoto similarities (RDKit, MACCS, Morgan), ROC-AUC (for classification) and MAE (for regression) where applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Reported baseline results (from tables reproduced in this paper): Reagent prediction — Exact: 0.044, BLEU: 0.224, Levenshtein: 23.167, RDKit FTS: 0.237, Validity: 1.000. Forward reaction — Exact: 0.045, BLEU: 0.654, Levenshtein: 27.262, RDKit FTS: 0.313, Validity: 1.000. Retrosynthesis — Exact: 0.009, BLEU: 0.705, Levenshtein: 31.227, RDKit FTS: 0.283, Validity: 1.000. Molecule description generation and property results also reported but generally lower than InstructMol in multimodal settings.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Mol-Instruction is shown in the paper as a competitive single-modality instruction-tuned baseline (Llama2 base) but is outperformed by InstructMol when multimodal graph information is added; InstructMol reports better performance on reaction/product generation and description when using the graph+sequence multimodal input.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>As a single-modality LLM baseline it lacks structural graph input and therefore struggles relative to multimodal models to capture structural cues needed for chemical reaction/product generation; its reported metrics are lower than InstructMol on multi-modal tasks. The paper also highlights general LLM limits such as difficulty with long-tail knowledge and potential generation of invalid or less chemically-plausible outputs for generalist LLMs when not properly aligned with molecular representations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'InstructMol: Multi-Modal Integration for Building a Versatile and Reliable Molecular Assistant in Drug Discovery', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3593.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3593.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate or design novel chemicals for specific applications, including details of the model, the application, the generation method, evaluation metrics, results, and any reported limitations or challenges.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MolReGPT (ChatGPT/GPT-4 retrieval methods)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Empowering molecule discovery for molecule-caption translation with large language models: A ChatGPT perspective (Li et al., 2023b)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval-augmented approach that uses GPT family models (GPT-3.5-turbo, GPT-4) with few-shot examples and retrieval to perform molecule-caption translation and molecule description generation; reported as a retrieval-based baseline in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Empowering molecule discovery for molecule-caption translation with large language models: A chatgpt perspective</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo and GPT-4 used in a retrieval-augmented MolReGPT pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large closed-source LLMs (GPT-3.5-turbo and GPT-4-0314) used with retrieval augmentation and few-shot prompting (10-shot MolReGPT reported) for molecule description generation tasks; the present paper reports their published results as retrieval-based baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Retrieval-augmented generation with few-shot prompting (MolReGPT): the LLM generates textual molecule descriptions (and in related work can be used for generating SMILES/SELFIES with guidance); not a multimodal embedding of graphs here but retrieval supplies grounded text context.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Molecule description generation (translation between molecule representations and natural language), aiding molecule-caption translation in drug discovery contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>BLEU, ROUGE, METEOR metrics for molecule description generation used in comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Reported performance in this paper's comparisons: GPT-3.5-turbo (10-shot MolReGPT) — BLEU-2: 0.565, BLEU-4: 0.482, ROUGE-1: 0.623, METEOR: 0.585. GPT-4-0314 (10-shot MolReGPT) — BLEU-2: 0.607, BLEU-4: 0.525, ROUGE-1: 0.634, METEOR: 0.610. These retrieval-based LLM approaches show strong descriptive generation capability and in some metrics compete with or exceed single-modal LLM baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Retrieval-augmented GPT-3.5/GPT-4 (MolReGPT) are stronger than zero-shot generalist LLMs on description generation and are competitive with specialist MolT5-style models on some metrics; the paper notes retrieval methods (GPT-4/GPT-3.5 with retrieval) demonstrate strong capabilities though InstructMol aims to integrate structural graph information directly for multimodal generation and task breadth.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Dependence on retrieval resources and prompt engineering; potential hallucination if retrieved context is poor; retrieval-based LLMs are not shown here to incorporate explicit graph encodings (structural molecular signals) and may not directly output chemically-valid molecular structures for reaction-design tasks unless specially prompted or post-processed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'InstructMol: Multi-Modal Integration for Building a Versatile and Reliable Molecular Assistant in Drug Discovery', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3593.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3593.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate or design novel chemicals for specific applications, including details of the model, the application, the generation method, evaluation metrics, results, and any reported limitations or challenges.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Generalist LLM baselines (Vicuna / LLaMA / Alpaca / Baize / ChatGLM / Galactica)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Various generalist large language models used as baselines (Vicuna, LLaMA, Alpaca, Baize, ChatGLM, Galactica)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Multiple general-purpose LLMs (open-source and research chat models) used as baselines or few-shot comparators in molecular generation/prediction tasks; many struggled to generate valid or chemically-accurate molecules when used without multimodal alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Vicuna-v1.3-7B / LLaMA-2 / Alpaca / Baize / ChatGLM / Galactica (as reported baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>General-purpose LLMs of varying sizes (e.g., LLaMA-derived Vicuna 7B/13B, Llama-2, Alpaca variants, Baize, ChatGLM, Galactica); these are either few-shot prompted or LoRA-finetuned in prior work / baseline experiments reported in tables. Specific training details vary by model and are referenced to their original sources.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Primarily few-shot in-context learning or simple instruction tuning (some LoRA experiments reported) producing SMILES/SELFIES or natural-language outputs; generally single-modal (text) prompting without explicit graph embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Attempted applications include reaction product generation, reagent prediction, retrosynthesis, property classification/regression, and molecule description generation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Validity (RDKit), BLEU, Exact Match, Levenshtein distance, fingerprint similarities, ROC-AUC, MAE depending on task; many generalist LLMs report near-zero Exact/validity in reaction tasks when used without molecular alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>The paper reproduces/quotes poor performance of many generalist LLMs on molecule-generation tasks: e.g., multiple generalist LLMs (Alpaca, Baize, ChatGLM, vanilla LLaMA) report Exact = 0.000 and low validity in reaction tasks; Vicuna few-shot variants had non-zero but weak performance on some property tasks. These models often fail to generate valid or high-similarity molecules for reaction prediction without multimodal alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Generalist LLMs perform substantially worse than InstructMol and specialist molecular models on generation/generation-similarity metrics; InstructMol's multimodal alignment markedly improves validity and similarity metrics relative to these baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Generalist LLMs typically lack explicit structural molecular inputs and pre-aligned molecular representations, yielding low validity or meaningless outputs for molecule generation; they struggle with long-tail/imbalanced datasets and require retrieval or specialized finetuning to be useful for chemical generation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'InstructMol: Multi-Modal Integration for Building a Versatile and Reliable Molecular Assistant in Drug Discovery', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Mol-instructions: A large-scale biomolecular instruction dataset for large language models <em>(Rating: 2)</em></li>
                <li>Empowering molecule discovery for molecule-caption translation with large language models: A chatgpt perspective <em>(Rating: 2)</em></li>
                <li>Molecular Transformer: A model for uncertainty-calibrated chemical reaction prediction <em>(Rating: 2)</em></li>
                <li>Unifying molecular and textual representations via multi-task language modelling <em>(Rating: 1)</em></li>
                <li>BioMedGPT: Open multimodal generative pretrained transformer for biomedicine <em>(Rating: 2)</em></li>
                <li>MolCA: Molecular graph-language modeling with cross-modal projector and uni-modal adapter <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3593",
    "paper_id": "paper-2b3554a8fea6f123fc04bd3e120f2293f227e1b2",
    "extraction_schema_id": "extraction-schema-79",
    "extracted_data": [
        {
            "name_short": "InstructMol",
            "name_full": "InstructMol (this paper)",
            "brief_description": "A multimodal molecular assistant built by instruction-tuning a 7B LLM (Vicuna-v1.3-7B) with a pre-aligned graph encoder (MoleculeSTM GIN) via a two-stage training (alignment pretraining on molecule-text pairs then task-specific instruction finetuning using LoRA); it generates text tokens (including SELFIES) conditioned on molecular graph and sequence inputs for tasks such as reaction product prediction, reagent prediction, retrosynthesis, property prediction and molecule description generation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "InstructMol (multimodal LLM built on Vicuna-v1.3-7B)",
            "model_description": "Base LLM: Vicuna-v1.3-7B (derived from LLaMA; base LLM size ~6.9B parameters for whole system). Molecular encoder: GIN (5-layer) initialized from MoleculeSTM (pretrained via graph-text contrastive learning; encoder ~1.8M params). A lightweight linear alignment projector maps node-level graph embeddings into the LLM token embedding space. Training: two stages — (1) alignment pretraining on ~264K–330K PubChem molecule-text pairs with frozen LLM and graph encoder to train the projector, (2) task-specific instruction tuning updating projector + LLM via LoRA (rank 64, α=16) on datasets (QM9, MoleculeNet, ChEBI-20, USPTO variants). Inputs: concatenated graph tokens and optional SELFIES sequence tokens; outputs: autoregressive text (including SELFIES) produced by the LLM.",
            "generation_method": "Autoregressive generation of textual molecular representations (SELFIES) conditioned on concatenated molecular graph token embeddings + optional SELFIES; instruction-following prompting; two-stage alignment projector + LoRA finetuning on downstream instruction datasets.",
            "application_domain": "Drug discovery / chemical reaction prediction and design tasks: forward reaction product generation, reagent prediction, retrosynthesis (precursor suggestion), molecular property prediction, and molecule description generation.",
            "evaluation_metrics": "Validity (RDKit), Exact Match, BLEU, Levenshtein distance, fingerprint Tanimoto similarity metrics (RDKit fingerprint Tanimoto Similarity, MACCS FTS, Morgan FTS), BLEU/ROUGE/METEOR for description generation, MAE for QM regression (HOMO/LUMO/gap), ROC-AUC for classification tasks.",
            "results_summary": "In multimodal generation tasks InstructMol substantially outperformed generalist LLM baselines and improved over prior single-modality instruction-tuned LLMs: example highlights from reported tables (InstructMol-GS = graph+sequence input): Reagent prediction — Exact: 0.129, BLEU: 0.610, Levenshtein: 19.664, RDKit FTS: 0.444, Validity: 1.000. Forward reaction prediction — Exact: 0.536, BLEU: 0.967, Levenshtein: 10.851, RDKit FTS: 0.776, Validity: 1.000. Retrosynthesis — Exact: 0.407, BLEU: 0.941, Levenshtein: 13.967, RDKit FTS: 0.753, Validity: 1.000. QM9 regression (MAE, hartree): InstructMol-GS Avg MAE ≈ 0.005. Molecule description generation (ChEBI-20): InstructMol-GS BLEU-2 = 0.475, BLEU-4 = 0.371, ROUGE-1 = 0.566, METEOR = 0.509. These results show high validity (RDKit-based) of generated molecules and strong similarity/fidelity metrics compared to other generalist LLM approaches.",
            "comparison_to_baselines": "Outperforms generalist LLM baselines (Vicuna/LLaMA few-shot, Alpaca/Baize, ChatGLM) and prior single-modal instruction-tuned models (e.g., Mol-Instruction) on reaction generation, description generation, and property tasks; still behind or comparable to some specialist domain models on select benchmarks (specialist models like Retroformer or MolT5 variants outperform InstructMol on some retrosynthesis/description metrics). Incorporating both graph and sequence (GS) performs better than graph-only (G) in generation tasks.",
            "limitations_challenges": "Reported limitations include dependence on dataset scale and quality (scarcity of high-quality annotation limits generalization), remaining gap to specialist models on certain tasks, challenges with long-tail / imbalanced distributions (noted worse performance on the HIV dataset), potential modality-alignment shortcomings (need for further refinement of multimodal alignment), hallucination risk in free-text generations, and the lack of domain-specific large LLMs tailored for chemistry; the paper emphasizes dataset scarcity and the need for more rigorous evaluation to detect hallucinations. SELFIES reduces syntactic invalidity but does not eliminate all domain relevance or synthesizability concerns.",
            "uuid": "e3593.0",
            "source_info": {
                "paper_title": "InstructMol: Multi-Modal Integration for Building a Versatile and Reliable Molecular Assistant in Drug Discovery",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Mol-Instruction",
            "name_full": "Mol-instructions (Fang et al., 2023)",
            "brief_description": "A large-scale biomolecular instruction dataset and associated instruction-tuned LLM baselines (reported in Fang et al.) used for molecular tasks; used as a baseline in this paper for many downstream tasks (property prediction, reaction prediction, molecule description).",
            "citation_title": "Mol-instructions: A large-scale biomolecular instruction dataset for large language models",
            "mention_or_use": "mention",
            "model_name": "Mol-Instruction (instruction-tuned LLM baseline, Llama2-based in reported experiments)",
            "model_description": "Reported in Fang et al.; described here as an instruction-tuned LLM approach using Llama2 as base for single-modality (text/sequence) instruction tuning on molecular tasks. The present paper uses reported Mol-Instruction numbers as a baseline (no reimplementation reported here).",
            "generation_method": "Single-modality instruction-tuning of a base LLM (Llama2) to autoregressively produce textual molecular outputs (e.g., SMILES/SELFIES/products) given textual or sequence inputs; no multimodal graph tokens used in the original baseline as reported here.",
            "application_domain": "Drug discovery / reaction prediction, reagent prediction, retrosynthesis, property prediction, and molecule caption/description generation (used as baseline comparisons).",
            "evaluation_metrics": "Same metrics presented in the paper: Validity (RDKit), BLEU, Exact Match, Levenshtein distance, fingerprint Tanimoto similarities (RDKit, MACCS, Morgan), ROC-AUC (for classification) and MAE (for regression) where applicable.",
            "results_summary": "Reported baseline results (from tables reproduced in this paper): Reagent prediction — Exact: 0.044, BLEU: 0.224, Levenshtein: 23.167, RDKit FTS: 0.237, Validity: 1.000. Forward reaction — Exact: 0.045, BLEU: 0.654, Levenshtein: 27.262, RDKit FTS: 0.313, Validity: 1.000. Retrosynthesis — Exact: 0.009, BLEU: 0.705, Levenshtein: 31.227, RDKit FTS: 0.283, Validity: 1.000. Molecule description generation and property results also reported but generally lower than InstructMol in multimodal settings.",
            "comparison_to_baselines": "Mol-Instruction is shown in the paper as a competitive single-modality instruction-tuned baseline (Llama2 base) but is outperformed by InstructMol when multimodal graph information is added; InstructMol reports better performance on reaction/product generation and description when using the graph+sequence multimodal input.",
            "limitations_challenges": "As a single-modality LLM baseline it lacks structural graph input and therefore struggles relative to multimodal models to capture structural cues needed for chemical reaction/product generation; its reported metrics are lower than InstructMol on multi-modal tasks. The paper also highlights general LLM limits such as difficulty with long-tail knowledge and potential generation of invalid or less chemically-plausible outputs for generalist LLMs when not properly aligned with molecular representations.",
            "uuid": "e3593.1",
            "source_info": {
                "paper_title": "InstructMol: Multi-Modal Integration for Building a Versatile and Reliable Molecular Assistant in Drug Discovery",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "MolReGPT (ChatGPT/GPT-4 retrieval methods)",
            "name_full": "Empowering molecule discovery for molecule-caption translation with large language models: A ChatGPT perspective (Li et al., 2023b)",
            "brief_description": "A retrieval-augmented approach that uses GPT family models (GPT-3.5-turbo, GPT-4) with few-shot examples and retrieval to perform molecule-caption translation and molecule description generation; reported as a retrieval-based baseline in this paper.",
            "citation_title": "Empowering molecule discovery for molecule-caption translation with large language models: A chatgpt perspective",
            "mention_or_use": "mention",
            "model_name": "GPT-3.5-turbo and GPT-4 used in a retrieval-augmented MolReGPT pipeline",
            "model_description": "Large closed-source LLMs (GPT-3.5-turbo and GPT-4-0314) used with retrieval augmentation and few-shot prompting (10-shot MolReGPT reported) for molecule description generation tasks; the present paper reports their published results as retrieval-based baselines.",
            "generation_method": "Retrieval-augmented generation with few-shot prompting (MolReGPT): the LLM generates textual molecule descriptions (and in related work can be used for generating SMILES/SELFIES with guidance); not a multimodal embedding of graphs here but retrieval supplies grounded text context.",
            "application_domain": "Molecule description generation (translation between molecule representations and natural language), aiding molecule-caption translation in drug discovery contexts.",
            "evaluation_metrics": "BLEU, ROUGE, METEOR metrics for molecule description generation used in comparisons.",
            "results_summary": "Reported performance in this paper's comparisons: GPT-3.5-turbo (10-shot MolReGPT) — BLEU-2: 0.565, BLEU-4: 0.482, ROUGE-1: 0.623, METEOR: 0.585. GPT-4-0314 (10-shot MolReGPT) — BLEU-2: 0.607, BLEU-4: 0.525, ROUGE-1: 0.634, METEOR: 0.610. These retrieval-based LLM approaches show strong descriptive generation capability and in some metrics compete with or exceed single-modal LLM baselines.",
            "comparison_to_baselines": "Retrieval-augmented GPT-3.5/GPT-4 (MolReGPT) are stronger than zero-shot generalist LLMs on description generation and are competitive with specialist MolT5-style models on some metrics; the paper notes retrieval methods (GPT-4/GPT-3.5 with retrieval) demonstrate strong capabilities though InstructMol aims to integrate structural graph information directly for multimodal generation and task breadth.",
            "limitations_challenges": "Dependence on retrieval resources and prompt engineering; potential hallucination if retrieved context is poor; retrieval-based LLMs are not shown here to incorporate explicit graph encodings (structural molecular signals) and may not directly output chemically-valid molecular structures for reaction-design tasks unless specially prompted or post-processed.",
            "uuid": "e3593.2",
            "source_info": {
                "paper_title": "InstructMol: Multi-Modal Integration for Building a Versatile and Reliable Molecular Assistant in Drug Discovery",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Generalist LLM baselines (Vicuna / LLaMA / Alpaca / Baize / ChatGLM / Galactica)",
            "name_full": "Various generalist large language models used as baselines (Vicuna, LLaMA, Alpaca, Baize, ChatGLM, Galactica)",
            "brief_description": "Multiple general-purpose LLMs (open-source and research chat models) used as baselines or few-shot comparators in molecular generation/prediction tasks; many struggled to generate valid or chemically-accurate molecules when used without multimodal alignment.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Vicuna-v1.3-7B / LLaMA-2 / Alpaca / Baize / ChatGLM / Galactica (as reported baselines)",
            "model_description": "General-purpose LLMs of varying sizes (e.g., LLaMA-derived Vicuna 7B/13B, Llama-2, Alpaca variants, Baize, ChatGLM, Galactica); these are either few-shot prompted or LoRA-finetuned in prior work / baseline experiments reported in tables. Specific training details vary by model and are referenced to their original sources.",
            "generation_method": "Primarily few-shot in-context learning or simple instruction tuning (some LoRA experiments reported) producing SMILES/SELFIES or natural-language outputs; generally single-modal (text) prompting without explicit graph embeddings.",
            "application_domain": "Attempted applications include reaction product generation, reagent prediction, retrosynthesis, property classification/regression, and molecule description generation.",
            "evaluation_metrics": "Validity (RDKit), BLEU, Exact Match, Levenshtein distance, fingerprint similarities, ROC-AUC, MAE depending on task; many generalist LLMs report near-zero Exact/validity in reaction tasks when used without molecular alignment.",
            "results_summary": "The paper reproduces/quotes poor performance of many generalist LLMs on molecule-generation tasks: e.g., multiple generalist LLMs (Alpaca, Baize, ChatGLM, vanilla LLaMA) report Exact = 0.000 and low validity in reaction tasks; Vicuna few-shot variants had non-zero but weak performance on some property tasks. These models often fail to generate valid or high-similarity molecules for reaction prediction without multimodal alignment.",
            "comparison_to_baselines": "Generalist LLMs perform substantially worse than InstructMol and specialist molecular models on generation/generation-similarity metrics; InstructMol's multimodal alignment markedly improves validity and similarity metrics relative to these baselines.",
            "limitations_challenges": "Generalist LLMs typically lack explicit structural molecular inputs and pre-aligned molecular representations, yielding low validity or meaningless outputs for molecule generation; they struggle with long-tail/imbalanced datasets and require retrieval or specialized finetuning to be useful for chemical generation tasks.",
            "uuid": "e3593.3",
            "source_info": {
                "paper_title": "InstructMol: Multi-Modal Integration for Building a Versatile and Reliable Molecular Assistant in Drug Discovery",
                "publication_date_yy_mm": "2023-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Mol-instructions: A large-scale biomolecular instruction dataset for large language models",
            "rating": 2
        },
        {
            "paper_title": "Empowering molecule discovery for molecule-caption translation with large language models: A chatgpt perspective",
            "rating": 2
        },
        {
            "paper_title": "Molecular Transformer: A model for uncertainty-calibrated chemical reaction prediction",
            "rating": 2
        },
        {
            "paper_title": "Unifying molecular and textual representations via multi-task language modelling",
            "rating": 1
        },
        {
            "paper_title": "BioMedGPT: Open multimodal generative pretrained transformer for biomedicine",
            "rating": 2
        },
        {
            "paper_title": "MolCA: Molecular graph-language modeling with cross-modal projector and uni-modal adapter",
            "rating": 1
        }
    ],
    "cost": 0.018993999999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>InstructMol: Multi-Modal Integration for Building a Versatile and Reliable Molecular Assistant in Drug Discovery</h1>
<p>He Cao ${ }^{1,2 *}$, Zijing Liu ${ }^{1}$, Xingyu Lu ${ }^{1,3}$, Yuan Yao ${ }^{2}$, Yu Li ${ }^{1 \dagger}$<br>${ }^{1}$ International Digital Economy Academy (IDEA)<br>${ }^{2}$ Hong Kong University of Science and Technology<br>${ }^{3}$ Tsinghua Shenzhen International Graduate School, Tsinghua University<br>hcaoaf@connect.ust.hk luxy22@mails.tsinghua.edu.cn<br>yuany@ust.hk {liuzijing, liyu}@idea.edu.cn</p>
<h4>Abstract</h4>
<p>The rapid evolution of artificial intelligence in drug discovery encounters challenges with generalization and extensive training, yet Large Language Models (LLMs) offer promise in reshaping interactions with complex molecular data. Our novel contribution, InstructMol ${ }^{\dagger}$, a multi-modal LLM, effectively aligns molecular structures with natural language via an instruction-tuning approach, utilizing a twostage training strategy that adeptly combines limited domain-specific data with molecular and textual information. InstructMol showcases substantial performance improvements in drug discovery-related molecular tasks, surpassing leading LLMs and significantly reducing the gap with specialists, thereby establishing a robust foundation for a versatile and dependable drug discovery assistant.</p>
<h2>1 Introduction</h2>
<p>The drug discovery process, from target identification to clinical trials, requires substantial investments in time and expertise for optimized exploration of chemical spaces (Coley, 2020). Artificial intelligence-driven drug discovery (AIDD) facilitates a data-driven modeling approach (Kim et al., 2021; Rifaioglu et al., 2018; Askr et al., 2022; Feng et al., 2024) and helps to understand the complex molecular space, reducing iterative testing and minimizing failure rates. Previous approaches involved employing task-specific models trained on labeled data, which had restricted adaptability and required laborious training for individual tasks. The advent of Large Language Models (LLMs (Devlin et al., 2019; Raffel et al., 2019; Brown et al., 2020)) like</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>ChatGPT (OpenAI, 2023a), trained through selfsupervised learning on a large amount of unlabeled text data, has shown strong generalization capabilities across various tasks. Additionally, these models can attain professional-level proficiency in specific domains through proper fine-tuning. Hence, developing a ChatGPT-like molecular assistant AI can revolutionize human interactions with complex molecule structures. A unified model can address various needs, such as understanding molecule structures, answering drug-related queries, aiding synthesis planning, facilitating drug repurposing, etc., as shown in Figure 1.</p>
<p>Numerous studies have explored multimodal LLMs for visual understanding (Liu et al., 2023b; Ye et al., 2023; Zhu et al., 2023). However, when it comes to the domain of molecular research, there are several challenges that need to be addressed, including:</p>
<ul>
<li>Crafting a molecule representation integrates with LLMs alongside textual modalities;</li>
<li>Requiring extensive datasets encompasses molecule structures, inherent properties, reactions, and annotations related to biological activities;</li>
<li>Developing an effective training paradigm that guides LLMs in utilizing molecular representations and adapting to various tasks.
Several prior studies (Liang et al., 2023; Luo et al., 2023c; Fang et al., 2023) have fine-tuned generalist LLMs to develop foundational models within the molecular domain. Despite their enhancement to the original generalist LLM, these preceding works have unveiled several issues:</li>
<li>Insufficient alignment between modalities.</li>
<li>The consideration of an optimal molecular structure encoder remains unexplored.</li>
</ul>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Empowering LLMs with molecular modalities to unlock the drug discovery domain and serve as assistants in molecular research.</p>
<ul>
<li>A rudimentary design of the training pipeline neglects the update of LLMs' knowledge.</li>
</ul>
<p>These issues lead to a significant disparity in the performance of current AI assistants across various practical tasks compared to traditional specialist models.</p>
<p>To address these problems, we introduce <strong>InstructMol</strong> (Figure 2), a multi-modality instruction-tuning-based LLM. This model aligns molecular graphs and chemical sequential modalities with humans' natural language. Using a calibrated collection of molecule-related instruction datasets and a two-stage training scheme, <strong>InstructMol</strong> effectively leverages the pre-trained LLM and molecule graph encoder for molecule-text alignment. In the first alignment pretraining stage, we employ molecule-description pairs to train a lightweight and adaptable interface, which is designed to project the molecular node-level representation into the textual space that the LLM can understand. Subsequently, we finetune with multiple task-specific instructions. During this process, we freeze the molecule graph encoder and train low-rank adapters (LoRA (Hu et al., 2021)) on the LLM to adapt our model to various scenarios. This efficient approach enables the seamless integration of molecular and textual information, promoting the development of versatile and robust cognitive abilities in the molecular domain.</p>
<p>To illustrate the capabilities of our model, we perform experiments that span three facets of drug discovery-related tasks, including compound property prediction, molecule description generation, and analysis of chemical reactions involving compounds. These tasks serve as robust benchmarks to assess the model's ability to deliver useful and accurate knowledge feedback in practical drug discovery scenarios. The results in all experiments consistently indicate that our model significantly improves the performance of LLMs in tasks related to the understanding and design of molecular compounds. Consequently, this advance effectively reduces the disparity with specialized models. Our main contributions can be summarized as follows:</p>
<ul>
<li>We introduce <strong>InstructMol</strong>, a molecular-related multi-modality LLM, representing a pioneering effort in bridging the gap between molecular and textual information.</li>
<li>In the context of a scarcity of high-quality annotated data in the drug discovery domain, our approach strives to efficiently extract molecular representations (targets on <strong>Issue2</strong>). Employing a two-stage instruction tuning paradigm enhances the LLM's understanding of molecular structural and sequential knowledge (targets on <strong>Issue1</strong> and <strong>Issue3</strong>).</li>
<li>InstructMol enables swift fine-tuning, generating lightweight checkpoints (used as plugins) for cross-modality tasks. It provides the flexibility to load or combine functionalities through plugins, retaining the open dialogue and reasoning capabilities of a general LLM.</li>
<li>We evaluate our model through multiple practical assessments, demonstrating its substantial improvement compared to state-of-the-art LLMs. Our work lays the foundation for creating a versatile and reliable molecular research assistant in the drug discovery domain.</li>
</ul>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Overview of InstructMol model architecture design and two-stage training paradigm. The example molecule in the figure is Terephthalaldehyde (Sonmez et al., 2012) (CID 12173).
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Comparison of biomolecule-domain moleculetext dataset scale with existing general domain vision-language datasets.</p>
<h2>2 Related Work</h2>
<h3>2.1 Multimodal Instruction Tuning</h3>
<p>There have been notable advancements in LLMs (OpenAI, 2023a; Touvron et al., 2023a,b; Chiang et al., 2023; Zeng et al., 2022a; Anil et al., 2023) achieved through scaling up model and data size. Consequently, LLMs have shown remarkable performances in zero/few-shot NLP tasks (OpenAI, 2023a; Wei et al., 2021; Ouyang et al., 2022). A key technique in LLMs is instruction tuning, where pre-trained LLMs are fine-tuned on instruction-formatted datasets (Wei et al., 2021), allowing them to generalize to new tasks. Recently, with the emergence of large foundation models in various domains, several efforts have been made to transition from unimodal LLMs to multimodal LLMs (MLLMs) (OpenAI, 2023b; Liu et al., 2023b; Zhu et al., 2023; Ye et al., 2023; Bai et al., 2023). The primary research on multimodal instruction tuning (M-IT) includes the following (Yin et al., 2023): Constructing effective M-IT datasets (adapting existing benchmarks datasets (Zhu et al., 2023; Liu et al., 2023b; Dai et al., 2023) or using self-instruction (Liu et al., 2023b; Wang et al., 2023; Li et al., 2023a; Zhang et al., 2023)), Bridging diverse modalities (project-based (Liu et al., 2023b; Li et al., 2023a; Pi et al., 2023) and querybased (Wang et al., 2023; Zhu et al., 2023; Ye et al., 2023)) and Employing reliable evaluation methods (GPT-scoring (Liu et al., 2023b; Li et al., 2023a; Chen et al., 2023; Luo et al., 2023a), manual scoring (Ye et al., 2023; Yang et al., 2023), or closed-set measurement (Liu et al., 2023b; Li et al., 2023a; Zhu et al., 2023; Luo et al., 2023a; Zhu et al., 2023;</p>
<p>Dai et al., 2023; Chen et al., 2023)). Most current MLLM research focuses on integrating vision and language while combining other modalities(e.g., graphs (Tang et al., 2023; Liu et al., 2023c)) with natural language remains nascent.</p>
<h3>2.2 Molecule Foundation Models</h3>
<p>The foundation models, trained on vast unlabeled data, serve as a paradigm for adaptable AI systems across diverse applications. In the single modality domain, researchers are exploring the molecule representations from diverse sources, such as 1D sequences (e.g., SMILES (Chithrananda et al., 2020; Irwin et al., 2021; Wang et al., 2019)), 2D molecular graphs (Wang et al., 2021; Hu et al., 2019; You et al., 2020), 3D geometric conformations (Stärk et al., 2021; Liu et al., 2021; Stärk et al., 2021), or textual information from biomedical literature (Gu et al., 2020; Lee et al., 2019; Beltagy et al., 2019). In the realm of multimodal analysis, research initiatives employ diverse approaches. These include encoder-decoder models to establish intermodal bridges (Edwards et al., 2022; Christofidellis et al., 2023; Lu and Zhang, 2022a), joint generative modeling of SMILES and textual data (Zeng et al., 2022b), and the adoption of contrastive learning for integrating molecular knowledge across varying modalities (Su et al., 2022; Luo et al., 2023b; Liu et al., 2022, 2023d).</p>
<h3>2.3 Molecule-related LLMs</h3>
<p>Given the rapid progress in LLMs, some researchers are considering developing ChatGPT-like AI systems for drug discovery. Their goal is to offer guidance for optimizing lead compounds, accu-</p>
<p>rately predicting drug interactions, and improving the comprehension of structure-activity relationships <em>Liang et al. (2023)</em>. Several initiatives have already commenced to create instruction datasets within the biomolecular domain <em>Fang et al. (2023); Lu et al. (2024)</em>. They aim to utilize instruction tuning techniques to enable LLMs, initially trained on general domain data, to acquire knowledge about biomolecular science <em>Wu et al. (2023); Luo et al. (2023c)</em>. Additionally, other researchers are investigating methods to align structural data with textual information, bridging the gap between biological data and natural language <em>Luo et al. (2023c); Liang et al. (2023); Cao et al. (2024b)</em>.</p>
<p>Remark. Our work involves molecule foundation models and multimodal language models (LLMs). It uses an efficient molecule graph encoder to capture structural information and integrates it with sequential data into a generalist LLM. InstructMol enables the LLM to understand molecule representations and generalize to various molecular tasks.</p>
<h2>3 Method</h2>
<h3>3.1 Multimodal Instruction Tuning</h3>
<p>Instruction tuning refers to finetuning pretrained LLMs on instruction datasets, enabling generalization to specific tasks by adhering to new instructions. Multimodal instruction tuning integrates modalities like images and graphs into an LLM, expanding the model’s capability to accommodate multiple modalities.</p>
<p>A multimodal instruction tuning sample comprises an instruction $I$ (e.g., "Describe the compound in detail") and an input-output pair. In the context of our study, the input is one or more modalities derived from a molecule (e.g., molecule graph and sequence), collectively denoted as $M$. The output $R$ represents the textual response to the instruction conditioned on the input. The model aims to predict an answer given the instruction and multimodal input: $\hat{R}=f(I, M ; \theta)$, where $\theta$ are the parameters of MLLM. The training objective is typically the same auto-regressive objective as the LLM pre-training stage, which can be expressed as: $\mathcal{L}(\theta)=-\sum_{i=1}^{L} \log p\left(R_{i} \mid I, M, R_{&lt;i} ; \theta\right)$, where $L$ is the target $R$ 's token length.</p>
<h3>3.2 Construction of Molecular Instruction</h3>
<p>Data Collection. In the field of biomolecular research, there is a noticeable scarcity of molecular datasets with comprehensive text annotations when compared to the vision-language domain, as depicted in Figure 3. While it is possible to construct instruction datasets in general domains by adapting benchmarks or using self-instruction, the application of these methods in the biomolecular domain presents challenges. This difficulty arises from two main factors: 1) biomolecular domain annotation demands expert knowledge and entails substantial complexity; 2) the knowledge within this domain spans a broad range of subjects, including structural biology, computational chemistry, and chemical synthesis processes.</p>
<p>In our efforts, we have gathered recent openaccess text-molecule pairs datasets and also independently constructed a portion of instruction data suitable for property prediction. Table 5 illustrates the composition of the data utilized during the twostage training process.</p>
<p>Molecule Input. We utilize both the structure and sequence information of a molecule. We encode the structural information of a molecule as a graph, denoted by $\mathcal{G}=(\mathcal{V}, \mathcal{E}, \mathcal{A}, \mathcal{X})$, where $\mathcal{V}$ is the set of atoms (nodes) and $|\mathcal{V}|=N$ is the total number of atoms. The set of edges $\mathcal{E}$ includes all chemical bonds, and $\mathcal{A} \in \mathbb{R}^{N \times N}$ is the adjacency matrix. Additionally, $\mathcal{X} \in \mathbb{R}^{N \times F}$ encompasses attributes associated with each node, where $F$ is the feature dimension. With a Graph Encoder $f_{g}$, we extract a graph representation $\mathbf{Z}_{G} \in \mathbb{R}^{N \times d}$ at the node level, effectively describing the inherent structure of the molecule. Simultaneously, we consider encoding the sequential information of the molecule, denoted as $S$, as a supplementary source of structural information. To enhance the robustness of sequential molecular descriptors and mitigate syntactic and semantic invalidity present in SMILES <em>Weininger (1988)</em>, we employ SELFIES <em>Krenn et al. (2019)</em> as $S$, which is designed for mapping each token to a distinct structure or reference.</p>
<p>Input Formulation. We formulate a molecule-text pair ( $\mathbf{X}<em c="c">{M} \&amp; \mathbf{X}</em>}$ ) to the corresponding instructionfollowing version like Human: $\mathbf{X<em M="M">{I}&lt;\mathrm{mol}&gt;\mathbf{X}</em>}$ <STOP> Assistant: $\mathbf{X<em M="M">{A}&lt;$ STOP&gt;. The $\mathbf{X}</em>}$ represents the molecule, including the molecule graph $\mathbf{X<em S="S">{G}$ and optionally the SELFIES $\mathbf{X}</em>} . \mathbf{X<em A="A">{I}$ denotes for the instruction and $\mathbf{X}</em>$ is the answer. For a given answer sequence of length $L$, our optimization objective is to maximize the probability of</p>
<p>generating the target answers $X_{A}$ by maximizing:</p>
<p>$p\left(\mathbf{X}<em M="M">{A} \mid \mathbf{X}</em>}, \mathbf{X<em i="1">{I}\right)=\prod</em>}^{L} p_{\theta}\left(x_{i} \mid \mathbf{X<em S="S">{G} | \mid \mathbf{X}</em>}, \mathbf{X<em A_i="A,&lt;i">{I}, \mathbf{X}</em>\right)$.</p>
<p>To diversify $\mathbf{X}<em G="G">{I}$, we craft clear task descriptions and use GPT-3.5-turbo to generate varied questions, enhancing instructions' robustness. Note that we simply concatenate $\mathbf{X}</em>$ along the length-dimension. More complex fusion methods require additional loss designs for supervision (Liu et al., 2023d; Luo et al., 2023b), but here we prioritize simplicity.}$ and $\mathbf{X}_{S</p>
<h3>3.3 Architecture</h3>
<p>Molecular Encoder. The molecular graph encoder, $f_{g}$, needs to efficiently extract node representations while preserving the molecular graph's connectivity information. It is crucial that $f_{g}$ inherently establishes a pre-alignment in the representation space with the text space to facilitate $\mathbf{Z}<em g="g">{G}$ in the following alignment stage. Taking inspiration from common practices in the Vision Large Language Models (VLLM) domain (Bai et al., 2023; Liu et al., 2023b; Ye et al., 2023), where models like ViT initialized from CLIP (Radford et al., 2021) serve as vision encoders, we optimize for MoleculeSTM's graph encoder as $f</em>$ (Liu et al., 2022), instead of GraphMVP used by prior methodologies (Liang et al., 2023; Luo et al., 2023c). The MoleculeSTM graph encoder model is obtained through molecular-textual contrastive training, mitigating the requirement for an extensive amount of paired data during training to align different modalities.
Light-weight Alignment Projector. To map graph features into the word embedding space, we utilize a trainable projection matrix $\mathbf{W}$ to transform $\mathbf{Z}<em G="G">{G}$ into $\mathbf{X}</em>$ has undergone partial alignment with the text through contrastive training, we believe a straightforward linear projection will meet the subsequent alignment needs. For approaches like gated cross-attention (Alayrac et al., 2022), Q-former (et.al., 2023), or position-aware visionlanguage adapters (Bai et al., 2023), they require a large number of pairs for pretraining alignment, which is typically unavailable in the biomolecular domain. We therefore do not explore these more complex alignment methods.
Large Language Model. InstructMol incorporates a pre-trained LLM as its foundational component.}$, ensuring that it has the same dimension as the word embedding space. Since the selected $f_{g</p>
<p>We optimize for Vicuna-7B (Chiang et al., 2023) as the initialized weights, which is derived from LLaMA (Touvron et al., 2023a) through supervised instruction finetuning.</p>
<h3>3.4 Two-Stage of Instruction Tuning</h3>
<p>As illustrated in Figure 2, the training process of InstructMol consists of two stages: alignment pretraining and instruction fine-tuning training.
Alignment Pretraining. In the first stage, we aim to align the modality of molecules with text, ensuring that the LLMs can perceive both the structural and sequential information of molecules and integrate molecular knowledge into their internal capabilities.</p>
<p>We primarily employ a dataset consisting of molecule-text pairs sourced from PubChem (Kim et al., 2022). Each molecule structure is associated with a textual description elucidating chemical and physical properties or high-level bioactivity information. The construction of the PubChemDataset predominantly follows the MoleculeSTM (Liu et al., 2022) pipeline. We meticulously remove molecules with invalid descriptions and syntactic errors in their molecular descriptors. To ensure fairness, we also eliminate compounds that might appear in the downstream molecule-caption test set. This results in a dataset of 330 K molecule-text pairs. Subsequently, we adopt a self-instructionlike approach to generate a diverse set of task descriptions as instructions.</p>
<p>During training, to prevent overfitting and leverage pre-trained knowledge, we freeze both the graph encoder and LLM, focusing solely on finetuning the alignment projector. After a few epochs of training, our aim is that the projector has successfully learned to map graph representations to graph tokens, aligning effectively with text tokens.
Task-specific Instruction Tuning. In the second stage, we target three distinct downstream scenarios. We advocate for task-specific instruction tuning to address the particular constraints inherent in various drug-discovery-related tasks. For compound property prediction, we utilize the quantum mechanics properties instruction dataset from Fang et al. (2023) for regression prediction and the MoleculeNet dataset (Wu et al., 2017) for property classification. For chemical reaction analysis, we incorporate forward reaction prediction, retrosynthesis analysis, and reagent prediction tasks, all derived from Fang et al. (2023). To assess the</p>
<p>model's proficiency in translating between natural language and molecular expression, we integrate ChEBI-20 (Edwards et al., 2021) for the molecule description generation task. For each task, corresponding instruction templates are designed.</p>
<p>During training, we utilize the checkpoint of the alignment projector that was trained in the first stage as initialization. We only keep the molecular encoder $f_{g}$ frozen and continue to update the pre-trained weights of the projector and the LLM. To adapt the LLM effectively for diverse tasks, we employ low-rank adaptation (i.e., LoRA (Hu et al., 2021)), opting against full-tuning to mitigate potential forgetting issues. In practical applications, we have the flexibility to substitute different adaptors based on specific scenario requirements or combine multiple adaptors to integrate knowledge, thereby showcasing the model's modularization capabilities. Moreover, LoRA allows the LLM to retain the inherent capacity for common-sense reasoning in dialogue (as shown in Table 13).</p>
<h2>4 Experiments</h2>
<p>We use a graph neural network as the molecule graph encoder $\left(f_{g}\right)$ which is initialized with the MoleculeSTM graph encoder, pre-trained through molecular graph-text contrastive learning. We employ Vicuna-v-1.3-7B (Chiang et al., 2023) as the base LLM. More specifically, InstructMol+GS denotes we inject both molecular graph tokens and sequence tokens into the input, while InstructMol+G means only incorporates graph tokens. For Instruct-S, which utilizes only a 1D molecular sequence as input, it corresponds to the fine-tuning of the base large language model, Vicuna-7B, directly on downstream tasks. In the following sections, the results of Vicuna-v1.3-7B will consistently be used to represent the performance of Instruct-S. Implementation details about model settings and training hyper-parameters can be referred to Appendix B.</p>
<h3>4.1 Property Prediction Task</h3>
<p>Experiment Setup. Property prediction intends to forecast a molecule's intrinsic physical and chemical properties from its structural or sequential characteristics. In the context of the regression task, we undertake experiments on the Property Prediction dataset from Fang et al. (2023), where the objective is to predict the quantum mechanic's properties of a given molecule, specifically including HOMO, LUMO, and the HOMO-LUMO gap (Ramakrish-
nan et al., 2014b). For the classification task, we incorporate three binary classification datasets of molecular biological activity, namely BACE, BBBP, and HIV. In classification, all dataset samples are converted into an instruction format and we use the recommended splits from (Ramsundar et al., 2019). Each item comprises an instruction explaining the property for prediction and the representation of the molecule. Subsequently, models are tasked with generating a single prediction ("yes" or "no"). Scaffold splits are used for the classification task, and the experiments are conducted with three random seeds, yielding low variances in the reported mean values.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">METHOD</th>
<th style="text-align: center;">HOMO $\downarrow$</th>
<th style="text-align: center;">LUMO $\downarrow$</th>
<th style="text-align: center;">$\Delta \epsilon \downarrow$</th>
<th style="text-align: center;">AVG $\downarrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">LLM Based Generalist Models</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Alpaca ${ }^{\dagger}$ (Tao1 et al., 2023)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">322.109</td>
</tr>
<tr>
<td style="text-align: center;">Baize ${ }^{\dagger}$ (Xu et al., 2023)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">261.343</td>
</tr>
<tr>
<td style="text-align: center;">Galactica ${ }^{\dagger}$ (Taylor et al., 2022)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.568</td>
</tr>
<tr>
<td style="text-align: center;">LLama-2-7B (5-shot ICL)</td>
<td style="text-align: center;">0.7367</td>
<td style="text-align: center;">0.8641</td>
<td style="text-align: center;">0.5152</td>
<td style="text-align: center;">0.7510</td>
</tr>
<tr>
<td style="text-align: center;">Vicuna-17B (5-shot ICL)</td>
<td style="text-align: center;">0.7135</td>
<td style="text-align: center;">3.6807</td>
<td style="text-align: center;">1.5407</td>
<td style="text-align: center;">1.9783</td>
</tr>
<tr>
<td style="text-align: center;">Mol Instruction</td>
<td style="text-align: center;">0.0210</td>
<td style="text-align: center;">0.0210</td>
<td style="text-align: center;">0.0203</td>
<td style="text-align: center;">0.0210</td>
</tr>
<tr>
<td style="text-align: center;">InstructMol-G</td>
<td style="text-align: center;">0.0060</td>
<td style="text-align: center;">0.0070</td>
<td style="text-align: center;">0.0082</td>
<td style="text-align: center;">0.0070</td>
</tr>
<tr>
<td style="text-align: center;">InstructMol-GS</td>
<td style="text-align: center;">0.0048</td>
<td style="text-align: center;">0.0050</td>
<td style="text-align: center;">0.0061</td>
<td style="text-align: center;">0.0050</td>
</tr>
</tbody>
</table>
<p>Table 1: Results (MAE in hartree unit) for QM9 property regression tasks. $\dagger$ denotes few-shot in-context learning (ICL) results from Fang et al. (2023). $\Delta \epsilon$ denotes HOMO-LUMO energy gap.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">METHOD</th>
<th style="text-align: center;">BACE $\uparrow$</th>
<th style="text-align: center;">BBBP $\uparrow$</th>
<th style="text-align: center;">HIV $\uparrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"># MOLECULES</td>
<td style="text-align: center;">1513</td>
<td style="text-align: center;">2039</td>
<td style="text-align: center;">41127</td>
</tr>
<tr>
<td style="text-align: left;">Specialist Models</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">ChemBERTa v2 (Walsh et al., 2022)</td>
<td style="text-align: center;">73.5</td>
<td style="text-align: center;">69.8</td>
<td style="text-align: center;">79.3</td>
</tr>
<tr>
<td style="text-align: left;">DMP(TF+GNN) (Jinhua et al., 2023)</td>
<td style="text-align: center;">89.4</td>
<td style="text-align: center;">77.8</td>
<td style="text-align: center;">81.4</td>
</tr>
<tr>
<td style="text-align: left;">KV-PLM (Zeng et al., 2022b)</td>
<td style="text-align: center;">78.5</td>
<td style="text-align: center;">70.5</td>
<td style="text-align: center;">71.8</td>
</tr>
<tr>
<td style="text-align: left;">GraphCL (You et al., 2020)</td>
<td style="text-align: center;">75.3</td>
<td style="text-align: center;">69.7</td>
<td style="text-align: center;">78.5</td>
</tr>
<tr>
<td style="text-align: left;">GraphMVP-C (Liu et al., 2021)</td>
<td style="text-align: center;">81.2</td>
<td style="text-align: center;">72.4</td>
<td style="text-align: center;">77.0</td>
</tr>
<tr>
<td style="text-align: left;">MoMu (Su et al., 2022)</td>
<td style="text-align: center;">76.7</td>
<td style="text-align: center;">70.5</td>
<td style="text-align: center;">75.9</td>
</tr>
<tr>
<td style="text-align: left;">MolPM (Luo et al., 2023b)</td>
<td style="text-align: center;">83.9</td>
<td style="text-align: center;">72.9</td>
<td style="text-align: center;">78.8</td>
</tr>
<tr>
<td style="text-align: left;">Uni-Mol (Zhou et al., 2023)</td>
<td style="text-align: center;">85.7</td>
<td style="text-align: center;">72.9</td>
<td style="text-align: center;">80.8</td>
</tr>
<tr>
<td style="text-align: left;">LLM Based Generalist Models</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Galactica-6.7B</td>
<td style="text-align: center;">58.4</td>
<td style="text-align: center;">53.5</td>
<td style="text-align: center;">72.2</td>
</tr>
<tr>
<td style="text-align: left;">Vicuna-v1.3-13b-16k (4-shot)</td>
<td style="text-align: center;">49.2</td>
<td style="text-align: center;">52.7</td>
<td style="text-align: center;">50.5</td>
</tr>
<tr>
<td style="text-align: left;">Vicuna-v1.3-7B*</td>
<td style="text-align: center;">68.3</td>
<td style="text-align: center;">60.1</td>
<td style="text-align: center;">58.1</td>
</tr>
<tr>
<td style="text-align: left;">LLama-2-7B-chat*</td>
<td style="text-align: center;">74.8</td>
<td style="text-align: center;">65.6</td>
<td style="text-align: center;">62.3</td>
</tr>
<tr>
<td style="text-align: left;">MolCA(1D) (Liu et al., 2023f)</td>
<td style="text-align: center;">79.3</td>
<td style="text-align: center;">70.8</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">MolCA(1D + 2D) (Liu et al., 2023f)</td>
<td style="text-align: center;">79.8</td>
<td style="text-align: center;">70.0</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Instruct-G</td>
<td style="text-align: center;">84.3 ( $\pm 0.6$ )</td>
<td style="text-align: center;">68.6 ( $\pm 0.3$ )</td>
<td style="text-align: center;">74.0 ( $\pm 0.1$ )</td>
</tr>
<tr>
<td style="text-align: left;">Instruct-GS</td>
<td style="text-align: center;">82.1 ( $\pm 0.1$ )</td>
<td style="text-align: center;">72.4 ( $\pm 0.3$ )</td>
<td style="text-align: center;">68.9 ( $\pm 0.3$ )</td>
</tr>
</tbody>
</table>
<p>Table 2: ROC-AUC of molecular property prediction tasks (classification) on MoleculeNet (Wu et al., 2017) benchmarks. ${ }^{*}$ : use LoRA tuning. We indicate the best performance among domain specialist models by underlining the results, while the best performance among LLM-based generalist models is highlighted in bold.</p>
<p>Results. Our models are compared against baselines on the test set for regression, measured by Mean Absolute Error (MAE) in Table 1. Compared to previous single-modal instruction-tuned LLM-based methods (Fang et al., 2023), InstructMol demonstrates a further improvement in the regression task. ROC-AUC scores for classifica-</p>
<table>
<thead>
<tr>
<th style="text-align: left;">MODEL</th>
<th style="text-align: center;">BLEU-2 $\uparrow$</th>
<th style="text-align: center;">BLEU-4 $\uparrow$</th>
<th style="text-align: center;">ROUGE-1 $\uparrow$</th>
<th style="text-align: center;">ROUGE-2 $\uparrow$</th>
<th style="text-align: center;">ROUGE-L $\uparrow$</th>
<th style="text-align: center;">METEOR $\uparrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Specialist Models</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">MolT5-base (Edwards et al., 2022)</td>
<td style="text-align: center;">0.540</td>
<td style="text-align: center;">0.457</td>
<td style="text-align: center;">0.634</td>
<td style="text-align: center;">0.485</td>
<td style="text-align: center;">0.568</td>
<td style="text-align: center;">0.569</td>
</tr>
<tr>
<td style="text-align: left;">MoMu (MolT5-base) (Su et al., 2022)</td>
<td style="text-align: center;">0.549</td>
<td style="text-align: center;">0.462</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.576</td>
</tr>
<tr>
<td style="text-align: left;">MolFM (MolT5-base) (Luo et al., 2023b)</td>
<td style="text-align: center;">0.585</td>
<td style="text-align: center;">0.498</td>
<td style="text-align: center;">0.653</td>
<td style="text-align: center;">0.508</td>
<td style="text-align: center;">0.594</td>
<td style="text-align: center;">0.607</td>
</tr>
<tr>
<td style="text-align: left;">MolXPT (Liu et al., 2023e)</td>
<td style="text-align: center;">0.594</td>
<td style="text-align: center;">0.505</td>
<td style="text-align: center;">0.660</td>
<td style="text-align: center;">0.511</td>
<td style="text-align: center;">0.597</td>
<td style="text-align: center;">0.626</td>
</tr>
<tr>
<td style="text-align: left;">GIT-Mol-graph (Liu et al., 2023d)</td>
<td style="text-align: center;">0.290</td>
<td style="text-align: center;">0.210</td>
<td style="text-align: center;">0.540</td>
<td style="text-align: center;">0.445</td>
<td style="text-align: center;">0.512</td>
<td style="text-align: center;">0.491</td>
</tr>
<tr>
<td style="text-align: left;">GIT-Mol-SMILES (Liu et al., 2023d)</td>
<td style="text-align: center;">0.264</td>
<td style="text-align: center;">0.176</td>
<td style="text-align: center;">0.477</td>
<td style="text-align: center;">0.374</td>
<td style="text-align: center;">0.451</td>
<td style="text-align: center;">0.430</td>
</tr>
<tr>
<td style="text-align: left;">GIT-Mol-(graph+SMILES) (Liu et al., 2023d)</td>
<td style="text-align: center;">0.352</td>
<td style="text-align: center;">0.263</td>
<td style="text-align: center;">0.575</td>
<td style="text-align: center;">0.485</td>
<td style="text-align: center;">0.560</td>
<td style="text-align: center;">0.430</td>
</tr>
<tr>
<td style="text-align: left;">MolCA, Galac ${ }_{1-\mathrm{M}}$ (Liu et al., 2023f)</td>
<td style="text-align: center;">0.620</td>
<td style="text-align: center;">0.531</td>
<td style="text-align: center;">0.681</td>
<td style="text-align: center;">0.537</td>
<td style="text-align: center;">0.618</td>
<td style="text-align: center;">0.651</td>
</tr>
<tr>
<td style="text-align: left;">Text+Chem T5-augm-base (Christofidellis et al., 2023)</td>
<td style="text-align: center;">$\underline{0.625}$</td>
<td style="text-align: center;">$\underline{0.542}$</td>
<td style="text-align: center;">$\underline{0.682}$</td>
<td style="text-align: center;">$\underline{0.543}$</td>
<td style="text-align: center;">$\underline{0.622}$</td>
<td style="text-align: center;">0.648</td>
</tr>
<tr>
<td style="text-align: left;">Retrieval Based LLMs</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">GPT-3.5-turbo (10-shot MolReGPT) (Li et al., 2023b)</td>
<td style="text-align: center;">0.565</td>
<td style="text-align: center;">0.482</td>
<td style="text-align: center;">0.623</td>
<td style="text-align: center;">0.450</td>
<td style="text-align: center;">0.543</td>
<td style="text-align: center;">0.585</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4-0314 (10-shot MolReGPT) (Li et al., 2023b)</td>
<td style="text-align: center;">0.607</td>
<td style="text-align: center;">0.525</td>
<td style="text-align: center;">0.634</td>
<td style="text-align: center;">0.476</td>
<td style="text-align: center;">0.562</td>
<td style="text-align: center;">0.610</td>
</tr>
<tr>
<td style="text-align: left;">LLM Based Generalist Models</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">GPT-3.5-turbo (zero-shot) (Li et al., 2023b)</td>
<td style="text-align: center;">0.103</td>
<td style="text-align: center;">0.050</td>
<td style="text-align: center;">0.261</td>
<td style="text-align: center;">0.088</td>
<td style="text-align: center;">0.204</td>
<td style="text-align: center;">0.161</td>
</tr>
<tr>
<td style="text-align: left;">BioMedGPT-10B (Luo et al., 2023c)</td>
<td style="text-align: center;">0.234</td>
<td style="text-align: center;">0.141</td>
<td style="text-align: center;">0.386</td>
<td style="text-align: center;">0.206</td>
<td style="text-align: center;">0.332</td>
<td style="text-align: center;">0.308</td>
</tr>
<tr>
<td style="text-align: left;">Mol-Instruction (Fang et al., 2023)</td>
<td style="text-align: center;">0.249</td>
<td style="text-align: center;">0.171</td>
<td style="text-align: center;">0.331</td>
<td style="text-align: center;">0.203</td>
<td style="text-align: center;">0.289</td>
<td style="text-align: center;">0.271</td>
</tr>
<tr>
<td style="text-align: left;">InstructMol-G</td>
<td style="text-align: center;">0.466</td>
<td style="text-align: center;">0.365</td>
<td style="text-align: center;">0.547</td>
<td style="text-align: center;">0.365</td>
<td style="text-align: center;">0.479</td>
<td style="text-align: center;">0.491</td>
</tr>
<tr>
<td style="text-align: left;">InstructMol-GS</td>
<td style="text-align: center;">$\mathbf{0 . 4 7 5}$</td>
<td style="text-align: center;">$\mathbf{0 . 3 7 1}$</td>
<td style="text-align: center;">$\mathbf{0 . 5 6 6}$</td>
<td style="text-align: center;">$\mathbf{0 . 3 9 4}$</td>
<td style="text-align: center;">$\mathbf{0 . 5 0 2}$</td>
<td style="text-align: center;">$\mathbf{0 . 5 0 9}$</td>
</tr>
</tbody>
</table>
<p>Table 3: Results of molecular description generation task on the test split of ChEBI-20.
tion outcomes are presented in Table 2. In comparison to LLM-based generalist models, both the Galactica (Taylor et al., 2022) series models trained on an extensive scientific literature dataset and the single-modality LLM fine-tuned with task-specific instructions (Fang et al., 2023), InstructMol demonstrates consistent improvements in accuracy across the three task datasets. However, our predictive results still exhibit some disparity compared to expert models (Zhou et al., 2023; Liu et al., 2021) specifically trained on a vast molecule structure dataset. Further, InstructMol performs worse than GIN on the imbalanced HIV dataset with a long-tail distribution. Previous research (Kandpal et al., 2023) highlights LLMs’ challenges in learning long-tail knowledge. To tackle this, strategies like resampling or class reweighting can be employed.</p>
<h3>4.2 Molecule Description Generation Task</h3>
<p>Experiment Setup. Molecule description generation encapsulates a comprehensive molecule depiction, covering its structure, properties, biological activity, and applications based on molecular descriptors. This task is more complex than classification or regression, providing a robust measure of the model's understanding of molecules. We convert the training subset of the ChEBI-20 dataset (Edwards et al., 2021) into an instructional format and subsequently perform fine-tuning based on these instructions. Our assessment uses evaluation metrics aligned with (Edwards et al., 2022).
Baselines. Three kinds of models are used as baselines, including: 1) MolT5-like expert models (Edwards et al., 2022; Liu et al., 2023e) and the models employing MolT5 as a decoder (Su et al., 2022;</p>
<p>Luo et al., 2023b; Liu et al., 2023d; Christofidellis et al., 2023), 2) models based on retrieval methods that utilize ChatGPT/GPT-4 as a foundational component (Li et al., 2023b), 3) other models derived through instruction-tuning with LLMs to achieve generalist unimodal (Fang et al., 2023) and multimodalities (Luo et al., 2023c) capabilities.
Results. Table 3 presents the overall results for molecule description generation. Our model outperforms other generalist LLM-based models in generating precise, contextually relevant molecule descriptions. We observe that incorporating both molecule structural information and sequential information in the input yields higher-quality results ( $\sim 2 \%$ improvement) than providing structural information alone. While expert models demonstrate better efficacy in comparison, it is noteworthy that they are constrained by their training schemes and lack the versatile capabilities inherent in our approach. Retrieval methods, supported by ChatGPT/GPT-4, demonstrate strong capabilities. Our future efforts will focus on integrating these methods to improve the accuracy and credibility of generated content.</p>
<h3>4.3 Chemical Reaction-related task</h3>
<p>Experiment Setup. Traditionally, identifying chemical reactions relied on intuition and expertise. Integrating deep learning for predicting reactions can accelerate research and improve drug discovery. The general format of a chemical reaction is "reactant $\rightarrow$ reagent $\rightarrow$ product". Here we mainly focus on three tasks: 1) Forward Reaction Prediction: predict the probable product(s) given specific reactants and reagents; 2) Reagent Predic-</p>
<p>tion: ascertain the suitable catalysts, solvents, or ancillary substances required for a specific chemical reaction given reactant(s) and product(s); 3) Retrosynthesis: anticipate deducing potential precursor molecule(s) from given product(s).</p>
<p>We utilize the dataset sourced from Fang et al. (2023), training it on the pre-defined training split and evaluating its performance on the test set. The performance is assessed by metrics like Fingerprint Tanimoto Similarity (FTS), BLEU, Exact Match, and Levenshtein distance to measure the similarity between ground truth and prediction. We also measure the validity of predicted molecules using RDKit.
Results. Table 4 reports the outcomes of tasks related to chemical reactions. It is evident that InstructMol outperforms the baselines significantly. The results obtained by generalist LLMs are derived from Fang et al. (2023), and they exhibit a pronounced inability to comprehend any chemical reaction prediction task, struggling to generate valid molecule(s) as answers. Mol-Instruction (Fang et al., 2023), employing Llama2 (Touvron et al., 2023b) as the base LLM, is jointly trained on multiple molecule-oriented instruction datasets. In addition, we supplement this by adopting the same training settings but exclusively training on chemical reaction-related datasets. Through comparison, InstructMol, as a multi-modality LLM, demonstrates a superior understanding of the task compared to single-modality models, confirming its effectiveness as a chemical reaction assistant.</p>
<h3>4.4 Ablation Studies</h3>
<p>In this subsection, we conduct an ablation study to investigate the architecture and training scheme design of our proposed framework. We explore variations from several perspectives and validate them on the task of molecule description generation. The ablation results are presented in Appendix Table 10 as follows: 1) Employing an MLP connector instead of a linear projector. Drawing inspiration from the observations made in (Liu et al., 2023a), we attempt to change the alignment projector to a two-layer MLP, demonstrating an enhancement in the model's multimodal capabilities. 2) Scaling up the LLM to 13B. The results indicate that scaling up the LLM only yields minor improvements. Thus, it substantiates the assertion that, for specific domains characterized by dataset scarcity, employing a 7B size model is sufficiently efficient for modeling. 3) Replacing the
graph encoder $f_{g}$ with a single-modality module (i.e., GraphMVP (Liu et al., 2021) with the same parameter size and architecture as we used). The results affirm our perspective: utilizing an encoder pre-aligned with text enhances the effectiveness of modality alignment. 4) Skipping alignment stage-1. We included a comparison where stage-1 was skipped entirely. The results demonstrate that separating projector training (stage-1) from downstream fine-tuning (stage-2) yields better performance. 5) Freezing the LLM in the second stage. Adopting a strategy akin to BioMedGPT10B (Luo et al., 2023c) and DrugChat (Liang et al., 2023), we choose not to update LLM weights in the second stage. The training outcomes reveal challenges in convergence and an inability to complete normal inference, thus demonstrating the necessity for the instruct-tuning stage to adapt LLM knowledge to the specific task.</p>
<h2>5 Discussion and Conclusion</h2>
<p>Conclusion. We propose InstructMol, a novel multi-modality foundational model that connects molecular modalities with human natural language. By integrating structural and sequential information of molecules into LLMs through a dualstage alignment pre-training and instruction tuning paradigm, we enhance the general LLM's capacity to comprehend and interpret molecular information, specifically in drug discovery tasks. Extensive experimental evaluation confirms the effectiveness of our model architecture and training approach, demonstrating its potential for practical applications in the field of drug discovery.
Future Work. Integrating multiple modalities with LLMs significantly enhances molecular research within this domain and is a valuable direction to explore. However, several challenges exist. The scale and quality of relevant datasets are as good as those in the vision and language community. The lack of well-defined task objectives poses a challenge. A more scientifically robust evaluation is needed to address issues such as hallucinations in generation outputs.</p>
<h2>6 Limitations</h2>
<p>In our investigation, several limitations have emerged. Firstly, the scale and quality of the dataset pose significant constraints; the scarcity of high-quality annotated domain data may hinder the model's ability to generalize across the diverse</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">Exact $\dagger$</th>
<th style="text-align: left;">BLEU $\dagger$</th>
<th style="text-align: left;">Levenshtein $\downarrow$</th>
<th style="text-align: left;">RDK FTS $\dagger$</th>
<th style="text-align: left;">MACCS FTS $\dagger$</th>
<th style="text-align: left;">Morgan FTS $\dagger$</th>
<th style="text-align: left;">Validity $\dagger$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Reagent Prediction</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Alpaca ${ }^{1}$ (Taori et al., 2023)</td>
<td style="text-align: left;">0.000</td>
<td style="text-align: left;">0.026</td>
<td style="text-align: left;">29.037</td>
<td style="text-align: left;">0.029</td>
<td style="text-align: left;">0.016</td>
<td style="text-align: left;">0.001</td>
<td style="text-align: left;">0.186</td>
</tr>
<tr>
<td style="text-align: left;">Baize ${ }^{1}$ (Xia et al., 2023)</td>
<td style="text-align: left;">0.000</td>
<td style="text-align: left;">0.051</td>
<td style="text-align: left;">30.628</td>
<td style="text-align: left;">0.022</td>
<td style="text-align: left;">0.018</td>
<td style="text-align: left;">0.004</td>
<td style="text-align: left;">0.099</td>
</tr>
<tr>
<td style="text-align: left;">ChatGLM ${ }^{1}$ (Zeng et al., 2022a)</td>
<td style="text-align: left;">0.000</td>
<td style="text-align: left;">0.019</td>
<td style="text-align: left;">29.169</td>
<td style="text-align: left;">0.017</td>
<td style="text-align: left;">0.006</td>
<td style="text-align: left;">0.002</td>
<td style="text-align: left;">0.074</td>
</tr>
<tr>
<td style="text-align: left;">LLama ${ }^{1}$ (Touvron et al., 2023a)</td>
<td style="text-align: left;">0.000</td>
<td style="text-align: left;">0.003</td>
<td style="text-align: left;">28.040</td>
<td style="text-align: left;">0.037</td>
<td style="text-align: left;">0.001</td>
<td style="text-align: left;">0.001</td>
<td style="text-align: left;">0.001</td>
</tr>
<tr>
<td style="text-align: left;">Vienna ${ }^{1}$ (Chiang et al., 2023)</td>
<td style="text-align: left;">0.000</td>
<td style="text-align: left;">0.010</td>
<td style="text-align: left;">27.948</td>
<td style="text-align: left;">0.038</td>
<td style="text-align: left;">0.002</td>
<td style="text-align: left;">0.001</td>
<td style="text-align: left;">0.007</td>
</tr>
<tr>
<td style="text-align: left;">Mol-Instruction (Fang et al., 2023)</td>
<td style="text-align: left;">0.044</td>
<td style="text-align: left;">0.224</td>
<td style="text-align: left;">23.167</td>
<td style="text-align: left;">0.237</td>
<td style="text-align: left;">0.364</td>
<td style="text-align: left;">0.213</td>
<td style="text-align: left;">1.000</td>
</tr>
<tr>
<td style="text-align: left;">LLama-7b ${ }^{1}$ (Touvron et al., 2023a)(LoRA)</td>
<td style="text-align: left;">0.000</td>
<td style="text-align: left;">0.283</td>
<td style="text-align: left;">53.510</td>
<td style="text-align: left;">0.136</td>
<td style="text-align: left;">0.294</td>
<td style="text-align: left;">0.106</td>
<td style="text-align: left;">1.000</td>
</tr>
<tr>
<td style="text-align: left;">InstructMol-G</td>
<td style="text-align: left;">0.070</td>
<td style="text-align: left;">$\mathbf{0 . 8 9 0}$</td>
<td style="text-align: left;">24.732</td>
<td style="text-align: left;">$\mathbf{0 . 4 6 9}$</td>
<td style="text-align: left;">$\mathbf{0 . 6 9 1}$</td>
<td style="text-align: left;">$\mathbf{0 . 4 2 6}$</td>
<td style="text-align: left;">1.000</td>
</tr>
<tr>
<td style="text-align: left;">InstructMol-GS</td>
<td style="text-align: left;">$\mathbf{0 . 1 2 9}$</td>
<td style="text-align: left;">0.610</td>
<td style="text-align: left;">$\mathbf{1 9 . 6 6 4}$</td>
<td style="text-align: left;">0.444</td>
<td style="text-align: left;">0.539</td>
<td style="text-align: left;">0.400</td>
<td style="text-align: left;">1.000</td>
</tr>
<tr>
<td style="text-align: left;">Forward Reaction Prediction</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Alpaca ${ }^{1}$ (Taori et al., 2023)</td>
<td style="text-align: left;">0.000</td>
<td style="text-align: left;">0.065</td>
<td style="text-align: left;">41.989</td>
<td style="text-align: left;">0.004</td>
<td style="text-align: left;">0.024</td>
<td style="text-align: left;">0.008</td>
<td style="text-align: left;">0.138</td>
</tr>
<tr>
<td style="text-align: left;">Baize ${ }^{1}$ (Xia et al., 2023)</td>
<td style="text-align: left;">0.000</td>
<td style="text-align: left;">0.044</td>
<td style="text-align: left;">41.500</td>
<td style="text-align: left;">0.004</td>
<td style="text-align: left;">0.025</td>
<td style="text-align: left;">0.009</td>
<td style="text-align: left;">0.097</td>
</tr>
<tr>
<td style="text-align: left;">ChatGLM ${ }^{1}$ (Zeng et al., 2022a)</td>
<td style="text-align: left;">0.000</td>
<td style="text-align: left;">0.183</td>
<td style="text-align: left;">40.008</td>
<td style="text-align: left;">0.050</td>
<td style="text-align: left;">0.100</td>
<td style="text-align: left;">0.044</td>
<td style="text-align: left;">0.108</td>
</tr>
<tr>
<td style="text-align: left;">LLama ${ }^{1}$ (Touvron et al., 2023a)</td>
<td style="text-align: left;">0.000</td>
<td style="text-align: left;">0.020</td>
<td style="text-align: left;">42.002</td>
<td style="text-align: left;">0.001</td>
<td style="text-align: left;">0.002</td>
<td style="text-align: left;">0.001</td>
<td style="text-align: left;">0.039</td>
</tr>
<tr>
<td style="text-align: left;">Vienna ${ }^{1}$ (Chiang et al., 2023)</td>
<td style="text-align: left;">0.000</td>
<td style="text-align: left;">0.057</td>
<td style="text-align: left;">41.690</td>
<td style="text-align: left;">0.007</td>
<td style="text-align: left;">0.016</td>
<td style="text-align: left;">0.006</td>
<td style="text-align: left;">0.059</td>
</tr>
<tr>
<td style="text-align: left;">Mol-Instruction (Fang et al., 2023)</td>
<td style="text-align: left;">0.045</td>
<td style="text-align: left;">0.654</td>
<td style="text-align: left;">27.262</td>
<td style="text-align: left;">0.313</td>
<td style="text-align: left;">0.509</td>
<td style="text-align: left;">0.262</td>
<td style="text-align: left;">1.000</td>
</tr>
<tr>
<td style="text-align: left;">LLama-7b ${ }^{1}$ (Touvron et al., 2023a)(LoRA)</td>
<td style="text-align: left;">0.012</td>
<td style="text-align: left;">0.804</td>
<td style="text-align: left;">29.947</td>
<td style="text-align: left;">0.499</td>
<td style="text-align: left;">0.649</td>
<td style="text-align: left;">0.407</td>
<td style="text-align: left;">1.000</td>
</tr>
<tr>
<td style="text-align: left;">Text+ChemT5 (Christobdellis et al., 2023)</td>
<td style="text-align: left;">0.454</td>
<td style="text-align: left;">0.602</td>
<td style="text-align: left;">26.545</td>
<td style="text-align: left;">0.729</td>
<td style="text-align: left;">0.773</td>
<td style="text-align: left;">0.700</td>
<td style="text-align: left;">0.851</td>
</tr>
<tr>
<td style="text-align: left;">MolelcularTransformer (Schwaller et al., 2018)</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.476</td>
<td style="text-align: left;">45.979</td>
<td style="text-align: left;">0.761</td>
<td style="text-align: left;">0.673</td>
<td style="text-align: left;">0.540</td>
<td style="text-align: left;">1.000</td>
</tr>
<tr>
<td style="text-align: left;">InstructMo-G</td>
<td style="text-align: left;">0.153</td>
<td style="text-align: left;">0.906</td>
<td style="text-align: left;">20.155</td>
<td style="text-align: left;">0.519</td>
<td style="text-align: left;">0.717</td>
<td style="text-align: left;">0.457</td>
<td style="text-align: left;">1.000</td>
</tr>
<tr>
<td style="text-align: left;">InstructMol-GS</td>
<td style="text-align: left;">$\mathbf{0 . 5 3 6}$</td>
<td style="text-align: left;">$\mathbf{0 . 9 6 7}$</td>
<td style="text-align: left;">$\mathbf{1 0 . 8 5 1}$</td>
<td style="text-align: left;">$\mathbf{0 . 7 7 6}$</td>
<td style="text-align: left;">$\mathbf{0 . 8 7 8}$</td>
<td style="text-align: left;">$\mathbf{0 . 7 4 1}$</td>
<td style="text-align: left;">1.000</td>
</tr>
<tr>
<td style="text-align: left;">Retrosynthesis</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Alpaca ${ }^{1}$ (Taori et al., 2023)</td>
<td style="text-align: left;">0.000</td>
<td style="text-align: left;">0.063</td>
<td style="text-align: left;">46.915</td>
<td style="text-align: left;">0.005</td>
<td style="text-align: left;">0.023</td>
<td style="text-align: left;">0.007</td>
<td style="text-align: left;">0.160</td>
</tr>
<tr>
<td style="text-align: left;">Baize ${ }^{1}$ (Xia et al., 2023)</td>
<td style="text-align: left;">0.000</td>
<td style="text-align: left;">0.095</td>
<td style="text-align: left;">44.714</td>
<td style="text-align: left;">0.025</td>
<td style="text-align: left;">0.050</td>
<td style="text-align: left;">0.023</td>
<td style="text-align: left;">0.112</td>
</tr>
<tr>
<td style="text-align: left;">ChatGLM ${ }^{1}$ (Zeng et al., 2022a)</td>
<td style="text-align: left;">0.000</td>
<td style="text-align: left;">0.117</td>
<td style="text-align: left;">48.365</td>
<td style="text-align: left;">0.056</td>
<td style="text-align: left;">0.075</td>
<td style="text-align: left;">0.043</td>
<td style="text-align: left;">0.046</td>
</tr>
<tr>
<td style="text-align: left;">LLama ${ }^{1}$ (Touvron et al., 2023a)</td>
<td style="text-align: left;">0.000</td>
<td style="text-align: left;">0.036</td>
<td style="text-align: left;">46.844</td>
<td style="text-align: left;">0.018</td>
<td style="text-align: left;">0.029</td>
<td style="text-align: left;">0.017</td>
<td style="text-align: left;">0.010</td>
</tr>
<tr>
<td style="text-align: left;">Vienna ${ }^{1}$ (Chiang et al., 2023)</td>
<td style="text-align: left;">0.000</td>
<td style="text-align: left;">0.057</td>
<td style="text-align: left;">46.877</td>
<td style="text-align: left;">0.025</td>
<td style="text-align: left;">0.030</td>
<td style="text-align: left;">0.021</td>
<td style="text-align: left;">0.017</td>
</tr>
<tr>
<td style="text-align: left;">Mol-Instruction (Fang et al., 2023)</td>
<td style="text-align: left;">0.009</td>
<td style="text-align: left;">0.705</td>
<td style="text-align: left;">31.227</td>
<td style="text-align: left;">0.283</td>
<td style="text-align: left;">0.487</td>
<td style="text-align: left;">0.230</td>
<td style="text-align: left;">1.000</td>
</tr>
<tr>
<td style="text-align: left;">LLama-7b ${ }^{1}$ (Touvron et al., 2023a)(LoRA)</td>
<td style="text-align: left;">0.000</td>
<td style="text-align: left;">0.283</td>
<td style="text-align: left;">53.510</td>
<td style="text-align: left;">0.136</td>
<td style="text-align: left;">0.294</td>
<td style="text-align: left;">0.106</td>
<td style="text-align: left;">1.000</td>
</tr>
<tr>
<td style="text-align: left;">Text+ChemT5 (Christobdellis et al., 2023)</td>
<td style="text-align: left;">0.033</td>
<td style="text-align: left;">0.314</td>
<td style="text-align: left;">88.672</td>
<td style="text-align: left;">0.457</td>
<td style="text-align: left;">0.469</td>
<td style="text-align: left;">0.350</td>
<td style="text-align: left;">0.632</td>
</tr>
<tr>
<td style="text-align: left;">Retroformer-untyped (Yao et al., 2022)</td>
<td style="text-align: left;">$\mathbf{0 . 5 3 6}$</td>
<td style="text-align: left;">0.881</td>
<td style="text-align: left;">$\mathbf{1 0 . 2 7 7}$</td>
<td style="text-align: left;">$\mathbf{0 . 8 6 5}$</td>
<td style="text-align: left;">$\mathbf{0 . 9 0 4}$</td>
<td style="text-align: left;">$\mathbf{0 . 8 3 0}$</td>
<td style="text-align: left;">0.995</td>
</tr>
<tr>
<td style="text-align: left;">InstructMol-G</td>
<td style="text-align: left;">0.114</td>
<td style="text-align: left;">0.586</td>
<td style="text-align: left;">21.271</td>
<td style="text-align: left;">0.422</td>
<td style="text-align: left;">0.523</td>
<td style="text-align: left;">0.285</td>
<td style="text-align: left;">1.000</td>
</tr>
<tr>
<td style="text-align: left;">InstructMol-GS</td>
<td style="text-align: left;">0.407</td>
<td style="text-align: left;">$\mathbf{0 . 9 4 1}$</td>
<td style="text-align: left;">13.967</td>
<td style="text-align: left;">0.753</td>
<td style="text-align: left;">0.852</td>
<td style="text-align: left;">0.714</td>
<td style="text-align: left;">1.000</td>
</tr>
</tbody>
</table>
<p>Table 4: Results of chemical reaction tasks. $\dagger$ : few-shot ICL results from (Fang et al., 2023). $*$ : use task-specific instruction data to fine-tune. Model indicates a domain expert method.
and intricate molecular landscapes encountered in real-world applications. Secondly, the integration and evaluation of multiple modalities have also revealed areas needing improvement. Further refinement is necessary to ensure robust alignment and utilization of different molecule modalities within the model, enhancing its capacity to interpret and generate responses accurately across the molecular domain. Lastly, our base LLM originates from a general-domain model. However, the absence of specialized LLMs tailored specifically for chemistry and molecular science, like models such as LLaMA, highlights the need for larger, more versatile domain-specific LLMs to enhance performance and expand applications. Addressing these challenges is pivotal for enhancing the model's reliability and extending its utility in advancing drug discovery methodologies.</p>
<h2>7 Potential Risks</h2>
<p>The application of AI in drug discovery entails several potential risks. A primary concern is the potential misuse of AI to develop hazardous or illicit substances, which presents significant safety and
ethical challenges. Moreover, inaccuracies in AIgenerated outputs could lead to hazardous chemical reactions if not thoroughly verified, posing risks of harm or damage to equipment. Dependence on AIgenerated content heightens the risk of accidents and unsafe practices. Therefore, stringent oversight and rigorous adherence to ethical guidelines are essential to mitigate these risks and ensure the safe and responsible application of AI in drug discovery. Further insights into these issues and potential safeguard approaches can be found in recent literature (Wong et al., 2024; Cao et al., 2024a; Wang et al., 2024).</p>
<h2>8 Acknowledgements</h2>
<p>This project was supported in part by Shenzhen Hetao Shenzhen-Hong Kong Science and Technology Innovation Cooperation Zone, under Grant No. HTHZQSWS-KCCYB-2023052, National Natural Science Foundation of China / Research Grants Council Joint Research Scheme Grant N_HKUST635/20, and HKRGC Grant 16308321.</p>
<h2>References</h2>
<p>PubChem Structure Search. https: //pubchem.ncbi.nlm.nih.gov/search/ help_search.html.</p>
<p>Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. 2022. Flamingo: a visual language model for few-shot learning. ArXiv, abs/2204.14198.</p>
<p>Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Tachard Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Z. Chen, Eric Chu, J. Clark, Laurent El Shafey, Yanping Huang, Kathleen S. Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernández Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan A. Botha, James Bradbury, Siddhartha Brahma, Kevin Michael Brooks, Michele Catasta, Yongzhou Cheng, Colin Cherry, Christopher A. Choquette-Choo, Aakanksha Chowdhery, C Crépy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, M. C. D’iaz, Nan Du, Ethan Dyer, Vladimir Feinberg, Fan Feng, Vlad Fienber, Markus Freitag, Xavier García, Sebastian Gehrmann, Lucas González, Guy Gur-Ari, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, An Ren Hu, Jeffrey Hui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wen Hao Jia, Kathleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric Li, Mu-Li Li, Wei Li, Yaguang Li, Jun Yu Li, Hyeontaek Lim, Han Lin, ZhongZhong Liu, Frederick Liu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez, Vedant Misra, Maysam Moussalem, Zachary Nado, John Nham, Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek, Oleksandr Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker Riley, Alexandra Ros,</p>
<p>Aurko Roy, Brennan Saeta, Rajkumar Samuel, Renee Marie Shelby, Ambrose Slone, Daniel Smilkov, David R. So, Daniela Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu, Ke Xu, Yunhan Xu, Lin Wu Xue, Pengcheng Yin, Jiahui Yu, Qiaoling Zhang, Steven Zheng, Ce Zheng, Wei Zhou, Denny Zhou, Slav Petrov, and Yonghui Wu. 2023. Palm 2 technical report. ArXiv, abs/2305.10403.</p>
<p>Heba Askr, Enas Elgeldawi, Heba Aboul Ella, Yaseen A.M.M. Elshaier, Mamdouh M. Gomaa, and Aboul Ella Hassanien. 2022. Deep learning in drug discovery: an integrative review and future challenges. Artificial Intelligence Review, 56:5975 - 6037.</p>
<p>Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. 2023. Qwen-vl: A frontier large vision-language model with versatile abilities. ArXiv, abs/2308.12966.</p>
<p>Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In IEEvaluation@ACL.</p>
<p>Iz Beltagy, Kyle Lo, and Arman Cohan. 2019. Scibert: A pretrained language model for scientific text. In Conference on Empirical Methods in Natural Language Processing.</p>
<p>Andrés M Bran, Sam Cox, Oliver Schilter, Carlo Baldassari, Andrew D. White, and Philippe Schwaller. 2023. Chemcrow: Augmenting largelanguage models with chemistry tools.</p>
<p>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, T. J. Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. ArXiv, abs/2005.14165.</p>
<p>He Cao, Weidi Luo, Yu Wang, Zijing Liu, Bing Feng, Yuan Yao, and Yu Li. 2024a. Guide for defense (g4d): Dynamic guidance for robust and balanced defense in large language models. arXiv preprint arXiv:2410.17922.</p>
<p>He Cao, Yanjun Shao, Zhiyuan Liu, Zijing Liu, Xiangru Tang, Yuan Yao, and Yu Li. 2024b. PRESTO: Progressive pretraining enhances synthetic chemistry outcomes. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 10197-10224, Miami, Florida, USA. Association for Computational Linguistics.</p>
<p>Feilong Chen, Minglun Han, Haozhi Zhao, Qingyang Zhang, Jing Shi, Shuang Xu, and Bo Xu. 2023. X-llm: Bootstrapping advanced large language models by treating multi-modalities as foreign languages. ArXiv, abs/2305.04160.</p>
<p>Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An open-source chatbot impressing gpt4 with $90 \% *$ chatgpt quality.</p>
<p>Seyone Chithrananda, Gabriel Grand, and Bharath Ramsundar. 2020. Chemberta: Large-scale selfsupervised pretraining for molecular property prediction. ArXiv, abs/2010.09885.</p>
<p>Dimitrios Christofidellis, Giorgio Giannone, Jannis Born, Ole Winther, Teodoro Laino, and Matteo Manica. 2023. Unifying molecular and textual representations via multi-task language modelling. In International Conference on Machine Learning.</p>
<p>Connor W. Coley. 2020. Defining and exploring chemical spaces. Trends in Chemistry.</p>
<p>Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Albert Li, Pascale Fung, and Steven C. H. Hoi. 2023. Instructblip: Towards generalpurpose vision-language models with instruction tuning. ArXiv, abs/2305.06500.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language
understanding. In North American Chapter of the Association for Computational Linguistics.</p>
<p>Joseph L. Durant, Burton A. Leland, Douglas R. Henry, and James G. Nourse. 2002. Reoptimization of mdl keys for use in drug discovery. Journal of chemical information and computer sciences, 42 6:1273-80.</p>
<p>Carl N. Edwards, T. Lai, Kevin Ros, Garrett Honke, and Heng Ji. 2022. Translation between molecules and natural language. ArXiv, abs/2204.11817.</p>
<p>Carl N. Edwards, Chengxiang Zhai, and Heng Ji. 2021. Text2mol: Cross-modal molecule retrieval with natural language queries. In Conference on Empirical Methods in Natural Language Processing.</p>
<p>Li et.al. 2023. Blip-2: Bootstrapping languageimage pre-training with frozen image encoders and large language models. In ICML.</p>
<p>Yin Fang, Xiaozhuan Liang, Ningyu Zhang, Kangwei Liu, Rui Huang, Zhuo Chen, Xiaohui Fan, and Huajun Chen. 2023. Mol-instructions: A large-scale biomolecular instruction dataset for large language models. ArXiv, abs/2306.08018.</p>
<p>Bin Feng, Zequn Liu, Nanlan Huang, Zhiping Xiao, Haomiao Zhang, Srbuhi Mirzoyan, Hanwen Xu, Jiaran Hao, Yinghui Xu, Ming Zhang, et al. 2024. A bioactivity foundation model using pairwise meta-learning. Nature Machine Intelligence, 6(8):962-974.</p>
<p>Yu Gu, Robert Tinn, Hao Cheng, Michael R. Lucas, Naoto Usuyama, Xiaodong Liu, Tristan Naumann, Jianfeng Gao, and Hoifung Poon. 2020. Domain-specific language model pretraining for biomedical natural language processing. ACM Transactions on Computing for Healthcare (HEALTH), 3:1 - 23.</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Xiaodong Song, and Jacob Steinhardt. 2020. Measuring massive multitask language understanding. ArXiv, abs/2009.03300.
J. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. ArXiv, abs/2106.09685.</p>
<p>Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay S. Pande, and Jure Leskovec. 2019. Strategies for pre-training graph neural networks. arXiv: Learning.</p>
<p>Ross Irwin, Spyridon Dimitriadis, Jiazhen He, and Esben Jannik Bjerrum. 2021. Chemformer: a pre-trained transformer for computational chemistry. Machine Learning: Science and Technology, 3.</p>
<p>Zhu Jinhua, Xia Yingce, Wu Lijun, Xie Shufang, Zhou Wengang, Qin Tao, Li Houqiang, and Liu Tie-Yan. 2023. Dual-view molecular pre-training. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 3615-3627.</p>
<p>Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel. 2023. Large language models struggle to learn long-tail knowledge. In International Conference on Machine Learning, pages 15696-15707. PMLR.</p>
<p>Jin Kim, Sera Park, Dongbo Min, and Wankyu Kim. 2021. Comprehensive survey of recent drug discovery using deep learning. International Journal of Molecular Sciences, 22.</p>
<p>Sunghwan Kim, Jie Chen, Tiejun Cheng, Asta Gindulyte, Jia He, Siqian He, Qingliang Li, Benjamin A. Shoemaker, Paul A. Thiessen, Bo Yu, Leonid Y. Zaslavsky, Jian Zhang, and Evan E. Bolton. 2022. Pubchem 2023 update. Nucleic acids research.</p>
<p>Sunghwan Kim, Paul A. Thiessen, Tiejun Cheng, Jian Zhang, Asta Gindulyte, and Evan E. Bolton. 2019. Pug-view: programmatic access to chemical annotations integrated in pubchem. Journal of Cheminformatics, 11.</p>
<p>Mario Krenn, Florian Hase, AkshatKumar Nigam, Pascal Friederich, and Alán Aspuru-Guzik. 2019. Self-referencing embedded strings (selfies): A 100\% robust molecular string representation. Machine Learning: Science and Technology, 1.</p>
<p>Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang. 2019. Biobert: a pre-trained biomedical language representation model for biomedical text mining. Bioinformatics, 36:1234 $-1240$.</p>
<p>Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon, and Jianfeng Gao. 2023a. Llava-med: Training a large language-and-vision assistant for biomedicine in one day. ArXiv, abs/2306.00890.</p>
<p>Jiatong Li, Yunqing Liu, Wenqi Fan, Xiao Wei, Hui Liu, Jiliang Tang, and Qing Li. 2023b. Empowering molecule discovery for molecule-caption translation with large language models: A chatgpt perspective. ArXiv, abs/2306.06615.</p>
<p>Youwei Liang, Ruiyi Zhang, Li Zhang, and Peng Xie. 2023. Drugchat: Towards enabling chatgptlike capabilities on drug molecule graphs. ArXiv, abs/2309.03907.</p>
<p>Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Annual Meeting of the Association for Computational Linguistics.</p>
<p>Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2023a. Improved baselines with visual instruction tuning. ArXiv, abs/2310.03744.</p>
<p>Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023b. Visual instruction tuning. ArXiv, abs/2304.08485.</p>
<p>Jiawei Liu, Cheng Yang, Zhiyuan Lu, Junze Chen, Yibo Li, Mengmei Zhang, Ting Bai, Yuan Fang, Lichao Sun, Philip S. Yu, and Chuan Shi. 2023c. Towards graph foundation models: A survey and beyond. ArXiv, abs/2310.11829.</p>
<p>Peng Liu, Yiming Ren, and Zhixiang Ren. 2023d. Git-mol: A multi-modal large language model for molecular science with graph, image, and text. ArXiv, abs/2308.06911.</p>
<p>Shengchao Liu, Weili Nie, Chengpeng Wang, Jiarui Lu, Zhuoran Qiao, Ling Liu, Jian Tang, Chaowei Xiao, and Anima Anandkumar. 2022. Multi-modal molecule structure-text model for text-based retrieval and editing. ArXiv, abs/2212.10789.</p>
<p>Shengchao Liu, Hanchen Wang, Weiyang Liu, Joan Lasenby, Hongyu Guo, and Jian Tang. 2021. Pretraining molecular graph representation with 3d geometry. ArXiv, abs/2110.07728.</p>
<p>Zequn Liu, W. Zhang, Yingce Xia, Lijun Wu, Shufang Xie, Tao Qin, Ming Yang Zhang, and TieYan Liu. 2023e. Molxpt: Wrapping molecules with text for generative pre-training. ArXiv, abs/2305.10688.</p>
<p>Zhiyuan Liu, Sihang Li, Yancheng Luo, Hao Fei, Yixin Cao, Kenji Kawaguchi, Xiang Wang, and Tat-Seng Chua. 2023f. Molca: Molecular graphlanguage modeling with cross-modal projector and uni-modal adapter. In Conference on Empirical Methods in Natural Language Processing.</p>
<p>Jieyu Lu and Yingkai Zhang. 2022a. Unified deep learning model for multitask reaction predictions with explanation. Journal of chemical information and modeling.</p>
<p>Jieyu Lu and Yingkai Zhang. 2022b. Unified deep learning model for multitask reaction predictions with explanation. Journal of chemical information and modeling.</p>
<p>Xingyu Lu, He Cao, Zijing Liu, Shengyuan Bai, Leqing Chen, Yuan Yao, Hai-Tao Zheng, and Yu Li. 2024. MoleculeQA: A dataset to evaluate factual accuracy in molecular comprehension. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 3769-3789, Miami, Florida, USA. Association for Computational Linguistics.</p>
<p>Gen Luo, Yiyi Zhou, Tianhe Ren, Shen Chen, Xiaoshuai Sun, and Rongrong Ji. 2023a. Cheap and quick: Efficient vision-language instruction tuning for large language models. ArXiv, abs/2305.15023.</p>
<p>Yi Luo, Kai Yang, Massimo Hong, Xingyi Liu, and Zaiqing Nie. 2023b. Molfm: A multimodal molecular foundation model. ArXiv, abs/2307.09484.</p>
<p>Yi Luo, Jiahuan Zhang, Siqi Fan, Kai Yang, Yushuai Wu, Mu Qiao, and Zaiqing Nie. 2023c. Biomedgpt: Open multimodal generative pretrained transformer for biomedicine. ArXiv, abs/2308.09442.</p>
<p>OpenAI. 2023a. "chatgpt: A language model for conversational ai.</p>
<p>OpenAI. 2023b. Gpt-4 technical report. ArXiv, abs/2303.08774.</p>
<p>Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke E. Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Francis Christiano, Jan Leike, and Ryan J. Lowe. 2022. Training language models to follow instructions with human feedback. ArXiv, abs/2203.02155.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2001. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics - ACL '02.</p>
<p>Renjie Pi, Jiahui Gao, Shizhe Diao, Rui Pan, Hanze Dong, Jipeng Zhang, Lewei Yao, Jianhua Han, Hang Xu, and Lingpeng Kong Tong Zhang. 2023. Detgpt: Detect what you need via reasoning. ArXiv, abs/2305.14167.</p>
<p>Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 2021. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning.</p>
<p>Colin Raffel, Noam M. Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2019. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21:140:1-140:67.</p>
<p>Raghunathan Ramakrishnan, Pavlo O. Dral, Pavlo O. Dral, Matthias Rupp, and O. Anatole von Lilienfeld. 2014a. Quantum chemistry structures and properties of 134 kilo molecules. Scientific Data, 1.</p>
<p>Raghunathan Ramakrishnan, Pavlo O Dral, Matthias Rupp, and O Anatole Von Lilienfeld. 2014b. Quantum chemistry structures and properties of 134 kilo molecules. Scientific data, $1(1): 1-7$.
B. Ramsundar, P. Eastman, P. Walters, and V. Pande. 2019. Deep Learning for the Life Sciences: Applying Deep Learning to Genomics, Microscopy, Drug Discovery, and More. O'Reilly.</p>
<p>Ahmet Sureyya Rifaioglu, Heval Atas, Maria Jesus Martin, Rengul Cetin-Atalay, Volkan Atalay, and Tunca Dogan. 2018. Recent applications of deep learning and machine intelligence on in silico drug discovery: methods, tools and databases. Briefings in Bioinformatics, 20:1878 - 1912.</p>
<p>Nadine Schneider, Roger A. Sayle, and Gregory A. Landrum. 2015. Get your atoms in order - an open-source implementation of a novel and robust molecular canonicalization algorithm. Journal of chemical information and modeling, 55 10:2111-20.</p>
<p>Philippe Schwaller, Teodoro Laino, Théophile Gaudin, Peter Bolgar, Constantine Bekas, and Alpha Albert Lee. 2018. Molecular transformer: A model for uncertainty-calibrated chemical reaction prediction. ACS Central Science, 5:1572 1583.</p>
<p>Hayal Bulbul Sonmez, Figen Kuloğlu, Koksal Karadag, and Fred Wudl. 2012. Terephthalaldehyde- and isophthalaldehydebased polyspiroacetals. Polymer Journal, 44:217-223.</p>
<p>Hannes Stärk, D. Beaini, Gabriele Corso, Prudencio Tossou, Christian Dallago, Stephan Gunnemann, and Pietro Lio'. 2021. 3d infomax improves gnns for molecular property prediction. In International Conference on Machine Learning.</p>
<p>Bing Su, Dazhao Du, Zhao-Qing Yang, Yujie Zhou, Jiangmeng Li, Anyi Rao, Haoran Sun, Zhiwu Lu, and Ji rong Wen. 2022. A molecular multimodal foundation model associating molecule graphs with natural language. ArXiv, abs/2209.05481.</p>
<p>Jiabin Tang, Yuhao Yang, Wei Wei, Lei Shi, Lixin Su, Suqi Cheng, Dawei Yin, and Chao Huang. 2023. Graphgpt: Graph instruction tuning for large language models. ArXiv, abs/2310.13023.</p>
<p>Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/ stanford_alpaca.</p>
<p>Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony S. Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez,
and Robert Stojnic. 2022. Galactica: A large language model for science. ArXiv, abs/2211.09085.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023a. Llama: Open and efficient foundation language models. ArXiv, abs/2302.13971.</p>
<p>Hugo Touvron, Louis Martin, Kevin R. Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Daniel M. Bikel, Lukas Blecher, Cristian Cantón Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony S. Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel M. Kloumann, A. V. Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, R. Subramanian, Xia Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zhengxu Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023b. Llama 2: Open foundation and fine-tuned chat models. ArXiv, abs/2307.09288.</p>
<p>Ahmad Walid, Simon Elana, Chithrananda Seyone, Grand Gabriel, and Ramsundar Bharath. 2022. Chemberta-2: Towards chemical foundation models. arXiv preprint arXiv:2209.01712.</p>
<p>Sheng Wang, Yuzhi Guo, Yuhong Wang, Hongmao Sun, and Junzhou Huang. 2019. Smiles-bert: Large scale unsupervised pre-training for molecular property prediction. Proceedings of the 10th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics.</p>
<p>Wen Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Y. Qiao, and Jifeng Dai. 2023.</p>
<p>Visionllm: Large language model is also an openended decoder for vision-centric tasks. ArXiv, abs/2305.11175.</p>
<p>Yu Wang, Xiaogeng Liu, Yu Li, Muhao Chen, and Chaowei Xiao. 2024. Adashield: Safeguarding multimodal large language models from structure-based attack via adaptive shield prompting. arXiv preprint arXiv:2403.09513.</p>
<p>Yuyang Wang, Jianren Wang, Zhonglin Cao, and Amir Barati Farimani. 2021. Molecular contrastive learning of representations via graph neural networks. Nature Machine Intelligence, 4:279 - 287.</p>
<p>Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. 2021. Finetuned language models are zero-shot learners. ArXiv, abs/2109.01652.</p>
<p>Jinmao Wei, Xiao-Jie Yuan, Qinghua Hu, and Shuqin Wang. 2010. A novel measure for evaluating classifiers. Expert Syst. Appl., 37:37993809 .</p>
<p>David Weininger. 1988. Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules. J. Chem. Inf. Comput. Sci., 28:31-36.</p>
<p>Aidan Wong, He Cao, Zijing Liu, and Yu Li. 2024. Smiles-prompting: A novel approach to llm jailbreak attacks in chemical synthesis. arXiv preprint arXiv:2410.15641.</p>
<p>Chaoyi Wu, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, and Weidi Xie. 2023. Pmc-llama: Towards building open-source language models for medicine.</p>
<p>Zhenqin Wu, Bharath Ramsundar, Evan N. Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S. Pappu, Karl Leswing, and Vijay S. Pande. 2017. Moleculenet: A benchmark for molecular machine learning. arXiv: Learning.</p>
<p>Canwen Xu, Daya Guo, Nan Duan, and Julian McAuley. 2023. Baize: An open-source chat model with parameter-efficient tuning on selfchat data. ArXiv, abs/2304.01196.</p>
<p>Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. 2018. How powerful are graph neural networks? ArXiv, abs/1810.00826.</p>
<p>Rui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge, Xiu Li, and Ying Shan. 2023. Gpt4tools: Teaching large language model to use tools via self-instruction. ArXiv, abs/2305.18752.</p>
<p>Weiran Yao, Shelby Heinecke, Juan Carlos Niebles, Zhiwei Liu, Yihao Feng, Le Xue, Rithesh Murthy, Zeyuan Chen, Jianguo Zhang, Devansh Arpit, Ran Xu, Phi Thi Mi, Haiquan Wang, Caiming Xiong, and Silvio Savarese. 2022. Retroformer: Pushing the limits of interpretable end-to-end retrosynthesis transformer. In ICML.</p>
<p>Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yi Zhou, Junyan Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, Chenliang Li, Yuanhong Xu, Hehong Chen, Junfeng Tian, Qiang Qi, Ji Zhang, and Feiyan Huang. 2023. mplug-owl: Modularization empowers large language models with multimodality. ArXiv, abs/2304.14178.</p>
<p>Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. 2023. A survey on multimodal large language models. ArXiv, abs/2306.13549.</p>
<p>Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and Yang Shen. 2020. Graph contrastive learning with augmentations. ArXiv, abs/2010.13902.</p>
<p>Rong Yu, Bian Yatao, Xu Tingyang, Xie Weiyang, Wei Ying, Huang Wenbing, and Huang Junzhou. 2020. Self-supervised graph transformer on large-scale molecular data. Advances in neural information processing systems, 33:1255912571.</p>
<p>Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, P. Zhang, Yuxiao Dong, and Jie Tang. 2022a. Glm-130b: An open bilingual pretrained model. ArXiv, abs/2210.02414.</p>
<p>Zheni Zeng, Yuan Yao, Zhiyuan Liu, and Maosong Sun. 2022b. A deep-learning system bridging molecule structure and biomedical text with comprehension comparable to human professionals. Nature Communications, 13.</p>
<p>Xiaoman Zhang, Chaoyi Wu, Ziheng Zhao, Weixiong Lin, Ya Zhang, Yanfeng Wang, and Weidi Xie. 2023. Pmc-vqa: Visual instruction tuning</p>
<p>for medical visual question answering. ArXiv, abs/2305.10415.</p>
<p>Gengmo Zhou, Zhifeng Gao, Qiankun Ding, Hang Zheng, Hongteng Xu, Zhewei Wei, Linfeng Zhang, and Guolin Ke. 2023. Uni-mol: A universal 3d molecular representation learning framework. In International Conference on Learning Representations.</p>
<p>Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. 2023. Minigpt4: Enhancing vision-language understanding with advanced large language models. ArXiv, abs/2304.10592.</p>
<h2>A Tasks Definition and Dataset Details</h2>
<p>Property Prediction. Molecular Property Prediction involves the forecasting or estimation of the biophysical and chemical properties of a molecule. In this work, our emphasis lies on three binary classification tasks sourced from the MoleculeNet benchmark (BBBP, BACE, and HIV) (Wu et al., 2017), and three regression tasks concentrating on the quantum properties of molecules from the QM9 (Ramakrishnan et al., 2014a) dataset.</p>
<p>Molecule Description Generation. Generating molecular descriptions involves compiling a detailed overview of a molecule's structure, properties, activities, and functions. This process aids chemists and biologists by swiftly providing crucial molecular insights for their research. Our data collection involves the extraction of molecular text annotations from PubChem (Kim et al., 2022). Leveraging PubChem's Power User Gateway (Kim et al., 2019), we retrieve abstracts of compound records in XML format. Subsequently, we extracted valid molecular description texts identified by unique PubChem Chemical Identifiers (CIDs), filtering out SMILES strings with syntactic errors or deviations from established chemical principles. Furthermore, we utilize the ChEBI20 dataset (Edwards et al., 2021) for downstream tasks in molecule description generation, comprising 33,010 molecule description pairs divided into $80 \%$ for training, $10 \%$ for validation and $10 \%$ for testing. To prevent data leakage, compounds in the PubChem text annotations that coincide with the ChEBI-20 test split are excluded.</p>
<p>Forward Reaction Prediction. Predicting the forward reaction involves anticipating the probable product(s) of a chemical reaction based on given reactants and reagents. For this task, we utilize the forward-reaction-prediction dataset from (Fang et al., 2023), comprising 138,768 samples sourced from the USPTO dataset (Wei et al., 2010). Each entry includes reactants and reagents separated by '.' within the instruction, with the output product.</p>
<p>Reagent Prediction. Reagent prediction identifies the substances necessary for a chemical reaction, helping to discover new types of reaction and optimal conditions. We use the reagent Prediction data from (Fang et al., 2023), sourced from the USPTO_500MT dataset (Lu and Zhang, 2022b). Each entry features a chemical reaction indicated as
"reactants $&gt;&gt;$ product," with the output indicating the reagents involved in the reaction.</p>
<p>Retrosynthesis Prediction. Retrosynthetic analysis in organic chemistry reverses engineering by tracing potential synthesis routes from the target compound backward. This strategy is vital for efficient synthesis of complex molecules and to foster innovation in pharmaceuticals and materials. For this task, we also used the dataset from (Fang et al., 2023), which is sourced from USPTO_500MT. The data organize inputs as products and outputs as reactants separated by '.' for each compound.</p>
<p>Discussion on License. As depicted in Table 6, we elaborate on the origins and legal permissions associated with each data component utilized in the development of the InstructMol. This encompasses both biomolecular data and textual descriptions. Thorough scrutiny was conducted on all data origins to confirm compatibility with our research objectives and subsequent utilization. Proper and accurate citation of these data sources is consistently maintained throughout the paper.</p>
<h2>B Implementation Details</h2>
<p>Model Settings. A graph neural network with five graph isomorphism network (GIN) (Xu et al., 2018) layers is used as the molecule graph encoder $f_{g}$. The hidden dimension is set to be 300. The GIN model is initialized using the MoleculeSTM (Liu et al., 2022) graph encoder, which is pre-trained through molecular graph-text contrastive learning. We employ Vicuna-v-1.3-7B (Chiang et al., 2023) as the base LLM, which has been trained through instruction-tuning. The total number of parameters of InstructMol is around 6.9B.
Training Details. In the first stage, we employ the training split comprising around 264 K moleculecaption pairs from PubMed. Using a batch size of 128, we conduct training for 5 epochs. We use the AdamW optimizer, with $\beta=(0.9,0.999)$ and a learning rate of $2 \mathrm{e}-3$, without weight decay. Warm-up is executed over $3 \%$ of the total training steps, followed by a cosine schedule for learning rate decay. For the second stage, we conduct training for three specific scenarios. For fair comparisons with traditional methods, training spans 20 to 50 epochs for the molecule description generation task using the ChEBI-20 training split. Property prediction and reaction tasks undergo 10 epochs using corresponding instruction datasets. In InstructMol training, we</p>
<table>
<thead>
<tr>
<th>TASKS</th>
<th># SAMPLES</th>
<th>Data SOURCE</th>
</tr>
</thead>
<tbody>
<tr>
<td>Alignment Pretrain</td>
<td>264K</td>
<td>PubMed (Kim et al., 2022)</td>
</tr>
<tr>
<td>Property Prediction(Regression)</td>
<td>362K</td>
<td>QM9 (Fang et al., 2023; Wu et al., 2017)</td>
</tr>
<tr>
<td>Property Prediction(Classification)</td>
<td>35,742</td>
<td>BACE, BBBP, HIV (Wu et al., 2017)</td>
</tr>
<tr>
<td>Molecule Description Generation</td>
<td>26,507</td>
<td>ChEBI-20 (Edwards et al., 2021)</td>
</tr>
<tr>
<td>Forward Prediction</td>
<td>125K</td>
<td>USPTO (Fang et al., 2023; Wei et al., 2010)</td>
</tr>
<tr>
<td>Retrosynthesis</td>
<td>130K</td>
<td>USPTO_500MT (Fang et al., 2023; Lu and Zhang, 2022b)</td>
</tr>
<tr>
<td>Reagent Prediction</td>
<td>125K</td>
<td>USPTO_500K (Fang et al., 2023; Lu and Zhang, 2022b)</td>
</tr>
</tbody>
</table>
<p>Table 5: Details of InstrutMol two-stage training data.</p>
<table>
<thead>
<tr>
<th>Data SOURCES</th>
<th>LICENSE URL</th>
<th>LICENSE NOTE</th>
</tr>
</thead>
<tbody>
<tr>
<td>PubChem</td>
<td>https://www.nlm.nih.gov/web_policies. html</td>
<td>Works produced by the U.S. government are not subject to copyright protection in the United States. Any such works found on National Library of Medicine (NLM) Web sites may be freely used or reproduced without permission in the U.S.</td>
</tr>
<tr>
<td>ChEBI</td>
<td>https://creativecommons.org/ licenses/by/4.0/</td>
<td>You are free to: Share - copy and redistribute the material in any medium or format. Adapt - remix, transform, and build upon the material for any purpose, even commercially.</td>
</tr>
<tr>
<td>USPTO</td>
<td>https://www.uspto.gov/ learning-and-resources/ open-data-and-mobility</td>
<td>It can be freely used, reused, and redistributed by anyone.</td>
</tr>
<tr>
<td>MoleculeNet</td>
<td>https://opensource.org/license/mit/</td>
<td>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so.</td>
</tr>
</tbody>
</table>
<p>Table 6: Data resources and licenses utilized in data collection..
maintain a consistent batch size of 128 and set the learning rate to $8 \mathrm{e}-5$. Linear layers within the LLM utilize a LoRA rank of 64 and a scaling value $\alpha$ of 16. All experiments are run with $4 \times$ RTX A6000 (48GB) GPUs.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Configuration</th>
<th style="text-align: center;">Value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Graph encoder $f_{g}$ init.</td>
<td style="text-align: center;">$\mathrm{GIN}_{\text {MoleculeSTM }}$</td>
</tr>
<tr>
<td style="text-align: left;"># params $f_{g}$</td>
<td style="text-align: center;">1.8 M</td>
</tr>
<tr>
<td style="text-align: left;">LLM init.</td>
<td style="text-align: center;">Vicuna-v-1.3-7B</td>
</tr>
<tr>
<td style="text-align: left;"># params LLM</td>
<td style="text-align: center;">6.9 B</td>
</tr>
<tr>
<td style="text-align: left;">Stage1 batch-size</td>
<td style="text-align: center;">128</td>
</tr>
<tr>
<td style="text-align: left;">Stage2 batch-size</td>
<td style="text-align: center;">128</td>
</tr>
<tr>
<td style="text-align: left;">Optimizer</td>
<td style="text-align: center;">AdamW</td>
</tr>
<tr>
<td style="text-align: left;">Warm-up ratios</td>
<td style="text-align: center;">0.03</td>
</tr>
<tr>
<td style="text-align: left;">Stage1 peak lr</td>
<td style="text-align: center;">$2 \mathrm{e}-3$</td>
</tr>
<tr>
<td style="text-align: left;">Stage2 peak lr</td>
<td style="text-align: center;">$8 \mathrm{e}-5$</td>
</tr>
<tr>
<td style="text-align: left;">Learning rate schedule</td>
<td style="text-align: center;">cosine decay</td>
</tr>
<tr>
<td style="text-align: left;">Weight decay</td>
<td style="text-align: center;">0.</td>
</tr>
<tr>
<td style="text-align: left;">Stage1 train epochs</td>
<td style="text-align: center;">5</td>
</tr>
<tr>
<td style="text-align: left;">Stage2 train epochs</td>
<td style="text-align: center;">$20-50$</td>
</tr>
<tr>
<td style="text-align: left;">Numerical precision</td>
<td style="text-align: center;">bfloat16</td>
</tr>
<tr>
<td style="text-align: left;">Activation checkpointing</td>
<td style="text-align: center;">True</td>
</tr>
</tbody>
</table>
<p>Table 7: Training hyperparameters of InstructMol.</p>
<h2>C Evaluate Metrics</h2>
<p>Molecule Description Generation Metric. Following (Edwards et al., 2022), NLP metrics such as BLEU (Papineni et al., 2001), ROUGE (Lin, 2004) and METEOR (Banerjee and Lavie, 2005) are used to assess the proximity of generated descriptions
to the truth of the ground. Specifically, these metrics are tested on the ChEBI-20 test dataset. In our experiments, we observed that after 50 epochs of finetuning on the training split, the metrics tend to converge, differing from previous approaches that often involved fine-tuning for over 100 epochs (Edwards et al., 2022; Su et al., 2022; Luo et al., 2023b).</p>
<p>Molecule Generation Metric. In chemical reaction tasks, we view it as akin to a text-based molecule generation task. Initially, we employ RDKit to validate the chemical validity of the generated results, ensuring their "validity". Subsequently, we gauge the sequential proximity between the generated sequence and the ground truth using NLP metrics such as BLEU, Exact Match scores, and Levenshtein distance. Additionally, we present performance based on molecule-specific metrics that assess molecular similarity, encompassing RDKit, MACCS (Durant et al., 2002), and Morgan (Schneider et al., 2015) fingerprints similarity.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">TASK</th>
<th style="text-align: center;">INSTRUCTION</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Alignment Pretrain</td>
<td style="text-align: center;">Instruction: Provide a brief overview of this molecule.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Output: The molecule is a non-proteinogenic alpha-amino acid that is ...</td>
</tr>
<tr>
<td style="text-align: center;">Property Prediction <br> (Regression)</td>
<td style="text-align: center;">Instruction: Could you give me the LUMO energy value of this molecule?</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Output: 0.0576</td>
</tr>
<tr>
<td style="text-align: center;">Property Prediction <br> (Classification)</td>
<td style="text-align: center;">Instruction: Evaluate whether the given molecule is able to enter the blood-brain barrier.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Output: Yes</td>
</tr>
<tr>
<td style="text-align: center;">Molecule Description Generation</td>
<td style="text-align: center;">Instruction: Could you give me a brief overview of this molecule?</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Output:The molecule is a fatty acid ester obtained by ...</td>
</tr>
<tr>
<td style="text-align: center;">Forward Prediction</td>
<td style="text-align: center;">Instruction: Based on the given reactants and reagents, suggest a possible product.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Output: SELFIES of product</td>
</tr>
<tr>
<td style="text-align: center;">Retrosynthesis</td>
<td style="text-align: center;">Instruction: Please suggest potential reactants used in the synthesis of the provided product.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Output: <REACTANT A>.<REACTANT B>...</REAGENT A>.<REAGENT B>...</td>
</tr>
<tr>
<td style="text-align: center;">Reagent Prediction</td>
<td style="text-align: center;">Instruction: Can you provide potential reagents for the following chemical reaction?</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Output: SELFIES of reagent</td>
</tr>
</tbody>
</table>
<p>Table 8: Examples of instruction samples for each task. $|$ means concatenate along the token dimension.
messages = [ "role":"system", "content": f"""You're acting as a molecule property prediction assistant. You'll be given SMILES of molecules and you need to make binary classification with a return result only in "True" or "False".</p>
<h1>The background of the dataset and task is shown below:</h1>
<p>The Blood-brain barrier penetration (BBBP) dataset comes from a recent study on the modeling and prediction of barrier permeability. As a membrane separating circulating blood and brain extracellular fluid, the blood-brain barrier blocks most drugs, hormones, and neurotransmitters. Thus penetration of the barrier forms a long-standing issue in the development of drugs targeting the central nervous system.</p>
<p>We provide several examples for this binary classification task:
###
Instruction: Predict whether the given compound has barrier permeability. Return True or False.
SMILES: $\mathrm{CCC}(=\mathrm{O}) \mathrm{C}(\mathrm{CC}(\mathrm{C}) \mathrm{N}(\mathrm{C}) \mathrm{C})(\mathrm{clccccc1}) \mathrm{c} 2 \mathrm{ccccc} 2$
Output: True
###
###
Instruction: Predict whether the provided compound exhibits barrier permeability. Return True or False.
SMILES: clcc2c(cc(CC3=CNCl=NC3=O)NCCSCc3oc(cc3)CN(C)C)cc2)cc1
Output: False
###
...
Given the following instructions and SMILES, return your prediction result:
Instruction: Predict whether the provided compound exhibits barrier permeability. Return True or False.
SMILES: TARGET SMILES
"" }
]</p>
<p>Table 9: An illustration of the few-shot in-context-learning prompt construction process for Llama (Touvron et al., 2023a,b) / Vicuna (Chiang et al., 2023) models in property prediction tasks.</p>
<h1>D More Results</h1>
<h2>D. 1 Ablation study results</h2>
<table>
<thead>
<tr>
<th style="text-align: left;">METHODS</th>
<th style="text-align: center;">BLEU-2 $\uparrow$</th>
<th style="text-align: center;">BLEU-4 $\uparrow$</th>
<th style="text-align: center;">ROUGE-1 $\uparrow$</th>
<th style="text-align: center;">ROUGE-2 $\uparrow$</th>
<th style="text-align: center;">ROUGE-L $\uparrow$</th>
<th style="text-align: center;">METEOR $\uparrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">InstructMol-G</td>
<td style="text-align: center;">0.4620</td>
<td style="text-align: center;">0.3560</td>
<td style="text-align: center;">0.5439</td>
<td style="text-align: center;">0.3644</td>
<td style="text-align: center;">0.4765</td>
<td style="text-align: center;">0.4832</td>
</tr>
<tr>
<td style="text-align: left;">+MLP XL connector</td>
<td style="text-align: center;">$\mathbf{0 . 4 6 6 5}(+0.97 \%)$</td>
<td style="text-align: center;">$\mathbf{0 . 3 6 1 3}(+1.49 \%)$</td>
<td style="text-align: center;">$\mathbf{0 . 5 4 9 7}(+1.07 \%)$</td>
<td style="text-align: center;">$\mathbf{0 . 3 6 9 9}(+1.51 \%)$</td>
<td style="text-align: center;">$\mathbf{0 . 4 8 0 5}(+0.84 \%)$</td>
<td style="text-align: center;">$\mathbf{0 . 4 9 1 7}(+1.76 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">+Scale up LLM</td>
<td style="text-align: center;">$0.4615(-0.11 \%)$</td>
<td style="text-align: center;">$0.3566(+0.17 \%)$</td>
<td style="text-align: center;">$0.5449(+0.18 \%)$</td>
<td style="text-align: center;">$0.3660(+0.44 \%)$</td>
<td style="text-align: center;">$0.4776(+0.23 \%)$</td>
<td style="text-align: center;">$0.4868(+0.75 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">Replace $f_{g}$ with GraphMVP</td>
<td style="text-align: center;">$0.4452(-3.64 \%)$</td>
<td style="text-align: center;">$0.3377(-5.14 \%)$</td>
<td style="text-align: center;">$0.5318(-0.11 \%)$</td>
<td style="text-align: center;">$0.3484(-2.22 \%)$</td>
<td style="text-align: center;">$0.4638(-2.67 \%)$</td>
<td style="text-align: center;">$0.4691(-2.92 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">Skip Stage-1</td>
<td style="text-align: center;">$0.4631(+0.23 \%)$</td>
<td style="text-align: center;">$0.3569(+0.25 \%)$</td>
<td style="text-align: center;">$0.5419(-0.37 \%)$</td>
<td style="text-align: center;">$0.3610(-0.93 \%)$</td>
<td style="text-align: center;">$0.4720(-0.94 \%)$</td>
<td style="text-align: center;">$0.4391(-9.12 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">Freeze LLM in the second stage</td>
<td style="text-align: center;">$\sim 0$</td>
<td style="text-align: center;">$\sim 0$</td>
<td style="text-align: center;">$\sim 0$</td>
<td style="text-align: center;">$\sim 0$</td>
<td style="text-align: center;">$\sim 0$</td>
<td style="text-align: center;">$\sim 0$</td>
</tr>
</tbody>
</table>
<p>Table 10: Ablation of the model architecture and training scheme design. We chose to conduct experiments on the Molecule Description Generation task. $f_{g}$ represents the molecule graph encoder.</p>
<h2>D. 2 More Results of Molecule Description Generation</h2>
<table>
<thead>
<tr>
<th style="text-align: center;">Mol-Instruction</th>
<th style="text-align: center;">InstructMol</th>
<th style="text-align: center;">Ground Truth</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">The molecule is the potassium salt of acetic acid. It has a role as a buffer. It is a potassium salt and an acetate salt. It contains an acetate.</td>
<td style="text-align: center;">The molecule is a potassium salt that is the monokis salt of carbonic acid. It has a role as a buffer. It is a carbonate salt and a potassium salt. It contains a carbonate.</td>
<td style="text-align: center;">The molecule is the potassium salt of formic acid. It has a role as a buffer. It is a potassium salt and a one-carbon compound. It derives from a formic acid.</td>
</tr>
<tr>
<td style="text-align: center;">CID: 2735122</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">The molecule is an organic molecular entity.</td>
<td style="text-align: center;">The molecule is a hydrochloride salt resulting from the reaction of equimolar amounts of 1-nitrobenzene and hydrogen chloride. It has a role as a mutagen. It contains a 1-nitrobenzene(2+).</td>
<td style="text-align: center;">The molecule is a hydrochloride resulting from the reaction of equimolar amounts of phenylhydrazine and hydrogen chloride. It contains a phenylhydrazine.</td>
</tr>
<tr>
<td style="text-align: center;">The molecule is a triterpenoid.</td>
<td style="text-align: center;">The molecule is a tirucallane triterpenoid that is [15alpha,14beta,17alpha,20S,25E)-lanosta-7,23diene substituted by an oxo group at position 3 and a methoxy group at position 25. It has been isolated from the stem and stem barbs of Cornus watteri. It has a role as a plant metabolite. It is an ether, a tirucallane triterpenoid and a secondary alcohol.</td>
<td style="text-align: center;">The molecule is a tirucallane triterpenoid that is [15alpha,14beta,17alpha,20S,25E)-lanosta-7,23diene substituted by a beta-hydroxy group at position 3 and a methoxy group at position 25. It has been isolated from the stem and stem barbs of Cornus watteri. It has a role as a plant metabolite. It is a tirucallane triterpenoid, an ether and a secondary alcohol.</td>
</tr>
<tr>
<td style="text-align: center;">The molecule is an organic thiophosphate.</td>
<td style="text-align: center;">The molecule is an organic thiophosphate that is 0,0-dimethyl hydrogen phosphorothioate in which the hydrogen of the hydroxy group has been replaced by a 3,5,6-trichloropyridin-2-yl group. It has a role as an agrochemical, an EC 3.1.1.7 (acetylcholinesterase) inhibitor and an environmental contaminant. It is an organic thiophosphate, a chloropyridine and an organochlorine pesticide.</td>
<td style="text-align: center;">The molecule is an organic thiophosphate that is 0,0diethyl hydrogen phosphorothioate in which the hydrogen of the hydroxy group has been replaced by a 3,5,6-trichloropyridin-2-yl group. It has a role as an EC 3.1.1.7 (acetylcholinesterase) inhibitor, an agrochemical, an EC 3.1.1.8 (cholinesterase) inhibitor, an environmental contaminant, a xenobiotic, an acaricide and an insecticide. It is an organic thiophosphate and a chloropyridine.</td>
</tr>
<tr>
<td style="text-align: center;">The molecule is a natural product found in Homo sapiens with data available.</td>
<td style="text-align: center;">The molecule is a 17beta-hydroxy steroid that is testosterone in which the 17beta hydrogen is replaced by a methyl group. It has a role as an androgen. It is a 3-oxoDelta(4) steroid and a 17beta-hydroxy steroid. It derives from a testosterone.</td>
<td style="text-align: center;">The molecule is a 17beta-hydroxy steroid that is testosterone that contains an additional double bond between positions 6 and 7. It is a 3-oxo-Delta(4) steroid, a 17beta-hydroxy steroid and an enone. It derives from a testosterone.</td>
</tr>
<tr>
<td style="text-align: center;">The molecule is a metabolite found in or produced by Escherichia coli (strain K12, MG1655).</td>
<td style="text-align: center;">The molecule is a myo-inositol monophosphate. It has a role as an algal metabolite and a mouse metabolite. It derives from a myo-inositol. It is a conjugate acid of a 10-myo-inositol 4phosphate(2-).</td>
<td style="text-align: center;">The molecule is an inositol having myoconfiguration substituted at position 1 by a phosphate group. It has a role as a human metabolite, an Escherichia coli metabolite and a mouse metabolite. It derives from a myo-inositol. It is a conjugate acid of a 10-myo-inositol 1-phosphate(2-).</td>
</tr>
<tr>
<td style="text-align: center;">The molecule is a monocarboxylic acid anion resulting from the removal of a proton from the carboxy group of (R)imazamox. It is a conjugate base of a (R)imazamox. It is an enantiomer of a (S)imazamox(1-).</td>
<td style="text-align: center;">The molecule is a monocarboxylic acid anion resulting from the removal of a proton from the carboxy group of (S)methyl 2-[4-chloro-2-methylphenoxy]acetate. It is a conjugate base of a (S)-methyl 2-[4-chloro-2-methylphenoxy]acetate. It is an enantiomer of a (R)-methyl 2-[4-chloro-2-methylphenoxy]acetate(1-).</td>
<td style="text-align: center;">The molecule is a monocarboxylic acid anion that is the conjugate base of (S)-2-[4-chloro-2-methylphenoxy]propanoic acid, obtained by deprotonation of the carboxy group. It is a conjugate base of a (S) mecoprop. It is an enantiomer of a (R)-2-(4-chloro-2-methylphenoxy)propanoate.</td>
</tr>
</tbody>
</table>
<p>Figure 4: More examples of molecule description generation task on ChEBI-20 (Edwards et al., 2021) test set. We include Mol-Instruction (Fang et al., 2023) as the baseline. CID (CID): PubChem Compound Identification, a non-zero integer PubChem accession identifier for a unique chemical structure.</p>
<h1>D. 3 More Results of Forward Reaction Prediction</h1>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: More examples of forward reaction prediction task. We include Mol-Instruction (Fang et al., 2023) and Multitask-Text-and-Chemistry-T5 (Christofidellis et al., 2023) as baselines.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*Work done during an internship at IDEA.
${ }^{\dagger}$ Corresponding authors: Yu Li (liyu@idea.edu.cn)
${ }^{\dagger}$ https://github.com/IDEA-XL/InstructMol&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-527 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-527</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-527</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-15.html">extraction-schema-15</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <p><strong>Paper ID:</strong> paper-63c74d15940af1af9b386b5762e4445e54c73719</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/63c74d15940af1af9b386b5762e4445e54c73719" target="_blank">VinVL: Revisiting Visual Representations in Vision-Language Models</a></p>
                <p><strong>Paper Venue:</strong> Computer Vision and Pattern Recognition</p>
                <p><strong>Paper TL;DR:</strong> This paper develops an improved object detection model to provide object-centric representations of images and feeds the visual features generated into a Transformer-based VL fusion model OSCAR, and utilizes an improved approach OSCar+ to pre-train the VL model and fine-tune it on a wide range of downstream VL tasks.</p>
                <p><strong>Paper Abstract:</strong> This paper presents a detailed study of improving visual representations for vision language (VL) tasks and develops an improved object detection model to provide object-centric representations of images. Compared to the most widely used bottom-up and top-down model [2], the new model is bigger, better-designed for VL tasks, and pre-trained on much larger training corpora that combine multiple public annotated object detection datasets. Therefore, it can generate representations of a richer collection of visual objects and concepts. While previous VL research focuses mainly on improving the vision-language fusion model and leaves the object detection model improvement untouched, we show that visual features matter significantly in VL models. In our experiments we feed the visual features generated by the new object detection model into a Transformer-based VL fusion model OSCAR [20], and utilize an improved approach OSCAR+ to pre-train the VL model and fine-tune it on a wide range of downstream VL tasks. Our results show that the new visual features significantly improve the performance across all VL tasks, creating new state-of-the-art results on seven public benchmarks. Code, models and pre-extracted features are released at https://github.com/pzzhang/VinVL.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e527.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e527.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VinVL (X152-C4)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>VinVL: ResNeXt-152 C4 object-attribute detection model pre-trained on 4 OD datasets</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large object-detection model (ResNeXt-152 C4) pre-trained on a merged corpus of COCO, OpenImages, Objects365 and Visual Genome to produce rich object-centric region features (2048-d) and position encodings (6-d), plus an attribute head (524 classes).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>X152-C4 (VinVL)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>ResNeXt-152 C4 backbone object detector pre-trained on a merged dataset (4Sets) with class-aware sampling and fine-tuned on Visual Genome with an attribute branch; outputs per-region visual embeddings (2048-D) and a 6-D position encoding (box coords + h/w). Uses class-agnostic NMS and dilation-free conv head for efficient region feature extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Vision-Language downstream tasks (VQA, GQA, Image Captioning, Retrieval, NLVR2)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Standard vision-language evaluation tasks where the model's pre-extracted region features and object tags are used as inputs to a transformer-based fusion model to answer questions, generate captions, or match text-image pairs. Not an embodied navigation/manipulation benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>visual question answering / image captioning / retrieval (vision-language tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial+object-relational (object locations via bounding boxes and position encodings; object attributes and categories; relations implied by co-occurrence and attributes)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pre-training on large-scale object detection datasets (COCO, OpenImages, Objects365, Visual Genome) and fine-tuning on Visual Genome for attributes</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>pre-extraction of visual region features used as inputs to downstream VL models (no end-to-end sensory stream during VL fine-tuning), i.e., fixed region-feature pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Dense region feature vectors (2048-D), discrete object-tag tokens (textual labels), and numeric position encodings (R=6: box coords + height & width); attributes as additional label outputs</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Downstream VL metrics depending on task (e.g., VQA accuracy, CIDEr for captioning, retrieval R@k).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Replacing classical OD features with VinVL yielded large gains: VQA test-dev (OSCAR+ w/ VinVL) = 75.95 (vs 73.16 baseline) and vqa-dev improvements (e.g., OSCAR_B baseline 72.38 -> OSCAR+_B w/ VinVL 74.90); image captioning CIDEr and NoCaps gains reported (e.g., large CIDEr improvements up to +5–6 pts in some settings).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Improved object recognition and attribute detection; richer and more diverse semantic concepts (1848 object classes, 524 attributes) yielding better grounding of language tokens to image regions; improved performance on compositional reasoning benchmarks (GQA) and VQA where object attributes and coarse spatial layout matter.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Not designed/tested for embodied planning/navigation/manipulation; Visual Genome detection mAP is low due to annotation sparsity and missing labels; region proposals can have imperfect localization; no explicit continuous 3D spatial map or affordance grounding for manipulation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Baselines: Anderson et al. (R101-C4 / 'bottom-up' features) and X152-FPN trained on OpenImages; VinVL outperforms Anderson et al. features across tasks (e.g., VQA improvements of ~2.7–3.5 absolute), with the paper attributing ~95% of total VL performance gains to improved vision features.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Ablations show (1) vision pre-training (merged 4Sets) and larger model size drive most gains (model+data are additive), (2) attribute training (524 classes) significantly improves VL tasks, (3) C4 architecture outperforms FPN for VL due to ImageNet pre-trained convolutional head and inductive bias, (4) class-agnostic NMS and conv changes speed up extraction without accuracy loss.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Spatial and object-relational knowledge in VL pipelines is encoded as region-feature vectors coupled with compact position encodings and discrete object/attribute tags; richness/diversity of object/attribute vocabularies and pre-training data scale dominate downstream VL performance improvements; these representations materially improve tasks that rely on object-centric spatial/relational reasoning (VQA, GQA), but the paper does not extend these representations to embodied navigation or manipulation benchmarks or to operation without direct sensory input.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'VinVL: Revisiting Visual Representations in Vision-Language Models', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e527.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e527.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OSCAR+</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OSCAR+: Object-Semantics Aligned Pre-training (improved)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Transformer-based vision-language fusion model that uses object tags as anchors and combines masked-token loss with a novel 3-way contrastive loss for joint image-text pre-training at scale.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OSCAR+ (Transformer with BERT init)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>BERT-Base (L=12,H=768) and BERT-Large (L=24,H=1024) variants</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A unified Transformer encoder initialized from BERT (base/large) that takes as input concatenated language tokens and pre-extracted region features (linear-projected to match embedding size) plus object-tag tokens; trained with a Masked Token Loss on text and a 3-way contrastive loss to jointly optimize VQA and text-image retrieval objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Vision-language pre-training and downstream VL tasks (VQA, GQA, captioning, retrieval, NLVR2)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Pre-training on large image-text-tag corpora to learn joint representations that can be fine-tuned on downstream tasks requiring grounding of language in visual regions and object tags; tasks require answering questions, generating captions, and matching text-image pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step multimodal reasoning / visual question answering / caption generation / retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial+object-relational (via region features and object-tag tokens), and to some extent procedural in answering question sequences (compositional reasoning), but procedural knowledge is not a central focus</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pre-training on 5.65M image dataset with images, captions, and image-tags (from COCO, CC, SBU, Flicker30k, OpenImages, VQA, GQA, VG-QA), and region features from VinVL</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>pre-training (masked-token and 3-way contrastive), then fine-tuning on downstream tasks; uses object tags as anchors to elicit alignment between language and regions</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Implicit knowledge encoded in transformer weights; explicit inputs are object tag tokens (textual), region feature vectors (2048-D projected), and position encodings (6-D) that together form multimodal contextualized representations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Task-specific metrics: VQA accuracy, GQA accuracy, CIDEr/METEOR/BLEU/SPICE for captioning, R@k for retrieval, accuracy for NLVR2.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>OSCAR+ with VinVL (Base) achieved VQA test-dev 75.95 and vqa-dev ~74.90 (substantial improvement over prior OSCAR with Anderson et al. features). On GQA test-dev it achieved 65.05 (vs prior 61.58), and state-of-the-art single-model results on COCO captioning and NoCaps.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Strong grounding of object names and attributes to region features via object-tag anchors; improved cross-modal retrieval and QA performance indicating robust use of object-relational and coarse spatial cues (via position encodings) for reasoning about scenes.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Model relies on pre-extracted region features (not end-to-end sensory stream), lacks an explicit geometric or metric spatial map for navigation/manipulation, and the pre-training contrastive objectives only simulate retrieval and VQA tasks — not embodied planning. No experiments on language-only (no-sensory) operation for embodied control are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to OSCAR (original) and other VLP models (UNITER, LXMERT, ViLBERT), OSCAR+ with VinVL features outperforms them across multiple VL tasks (e.g., VQA, GQA, captioning); the paper quantifies that improved vision features contribute ~95% of the downstream gains.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Ablations show the 3-way contrastive loss improves transfer to both VQA and retrieval (versus binary loss), scaling pre-training corpus (adding OpenImages tagging/self-training) improves VQA, and that replacing visual features with VinVL drives most gains; removing attribute supervision or reducing vocab hurts performance.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using discrete object tags as anchors plus dense region embeddings and position encodings allows a transformer language model to learn object-centric spatial and relational abstractions sufficient for improved VQA, retrieval and captioning; however, OSCAR+ operates on fixed visual features and is not demonstrated for embodied planning/navigation/manipulation or for operating without direct sensory inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'VinVL: Revisiting Visual Representations in Vision-Language Models', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e527.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e527.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Region features + position encoding</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Region feature vectors (2048-D) with R=6 position encoding (VinVL outputs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Per-region continuous feature vectors derived from the detector's last-linear-layer input (2048-D) paired with a compact 6-D position encoding representing bounding box coordinates and height/width for each detected region.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Region feature representation (used by OSCAR+ and other VL models)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Each detected region r is represented as a tuple (v_hat, z) where v_hat is a 2048-dimensional visual embedding and z is a 6-dimensional position encoding (box coordinates + height & width); region features are linearly projected to the transformer's embedding size and concatenated with object-tag tokens as inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Vision-language tasks (VQA, GQA, captioning, retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Used as the visual input modality for joint multimodal Transformer models to ground textual queries and generate language conditioned on image regions and their spatial metadata.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>visual reasoning / question answering / caption generation</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial+object-relational (explicit 2D bounding-box spatial encodings; object identity and attribute information encoded in region vectors and tags)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>learned by OD pre-training on multi-dataset corpora and fine-tuned on Visual Genome attribute labels; positional encoding derived from bounding box coordinates</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>fed as fixed inputs to a Transformer during pre-training and fine-tuning (no additional sensory processing during VL fine-tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Numeric continuous embeddings (implicit in weights) for appearance and discrete numeric tuple for 2D spatial metadata; these are combined and contextualized by self-attention layers to support relational reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Used indirectly; downstream metrics reported for models consuming these features (e.g., VQA accuracy, CIDEr).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Switching to VinVL region features (vs older R101-C4 features) raised VQA vqa-dev from ~72.46 to ~74.90 (OSCAR+_B), demonstrating that richer region features and position encodings materially improve VL reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Enables better grounding of object mentions and attributes, helps in tasks requiring identification of objects and their coarse 2D spatial relations (relative positions via box coordinates), improves retrieval and QA accuracy where object-level grounding matters.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Provides only 2D bounding-box spatial cues (no explicit 3D geometry, metric distances, or continuous egocentric spatial maps), so fine-grained spatial navigation/orientation reasoning for embodied tasks is not supported; reliance on pre-extracted regions can miss affordance-related properties.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Vs grid features or COCO GT boxes: richer OD proposals with larger vocabularies outperformed COCO GT boxes on VQA, showing that semantic richness can matter more than perfect localization for VL tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Using COCO GT boxes (limited vocabulary) performs worse on VQA than proposals from VG-trained models; adding attribute labels and larger vocabularies improves VQA; FPN vs C4 head choice influences region feature quality.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Spatial knowledge in current VL pipelines is represented as compact numeric box encodings combined with high-dimensional region embeddings; these representations suffice to improve many language-grounded scene reasoning tasks but do not equate to the kinds of egocentric continuous spatial maps needed for embodied navigation or manipulation without additional modules.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'VinVL: Revisiting Visual Representations in Vision-Language Models', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e527.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e527.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Object tags as anchors (q)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Object tag tokens used as semantic anchors in OSCAR+</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Discrete textual labels (object tags) produced by the object detector are used as anchor tokens concatenated with region features and language tokens to improve image-text alignment during pre-training and fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Object-tag anchors (VinVL tags fed to OSCAR+)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Object names and attribute labels detected by the OD model are supplied as textual tokens q; these tokens serve as anchors in the Transformer input sequence to improve alignment between language and specific image regions.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Vision-language pre-training and downstream tasks (VQA, captioning, retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tags help the model ground words to image regions by providing discrete semantic anchors that complement continuous region features, aiding tasks that require object-level reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>visual grounding / question answering / captioning</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>object-relational (object identity, attributes, and implied relationships via co-occurrence and attention patterns); supports some spatial reasoning when combined with region position encodings</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>object detector outputs (VinVL) trained on merged OD datasets and VG attribute labels</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>provided as explicit tokens in the transformer's input sequence (used in masked-token loss and contrastive objectives)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Discrete natural-language tokens representing object names/attributes, which are embedded and contextualized by Transformer self-attention together with region vectors and textual queries</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Improvement measured via downstream metrics (VQA accuracy, retrieval R@k, captioning CIDEr).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Including object tags as anchors (as in OSCAR/OSCAR+) leads to stronger VL alignment and improved downstream results; paper reports that models using tags yield better performance than those that only use region features (contributed materially in OSCAR prior work and here).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Improves grounding of language to specific objects and attributes, enhances retrieval and QA performance especially when object identity/attribute is crucial.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Tags are limited by detector vocabulary and can be noisy or missing; they do not encode continuous spatial metrics or action affordances needed for manipulation; not evaluated in absence of visual input.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>OSCAR without object-tag anchors vs OSCAR/OSCAR+ with tags: tags act as anchors and improve alignment; replacing Anderson et al. features with VinVL tags/features gives large gains.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>The paper controlled for tag usage by keeping tags fixed when studying vision features; experiments show that improving vision features yields larger gains even when tags are held constant, but tags remain an important component.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Explicit semantic tokens (object tags) serve as powerful alignment anchors for transformer-based VL models, enabling better utilization of object-relational knowledge; however, this mechanism presumes availability of visual detection and is not demonstrated for purely language-driven embodied planning without sensory input.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'VinVL: Revisiting Visual Representations in Vision-Language Models', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Bottom-up and top-down attention for image captioning and visual question answering <em>(Rating: 2)</em></li>
                <li>Oscar: Object-semantics aligned pre-training for vision-language tasks <em>(Rating: 2)</em></li>
                <li>Visual genome: Connecting language and vision using crowdsourced dense image annotations <em>(Rating: 1)</em></li>
                <li>VIVO: Surpassing human performance in novel object captioning with visual vocabulary pre-training <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-527",
    "paper_id": "paper-63c74d15940af1af9b386b5762e4445e54c73719",
    "extraction_schema_id": "extraction-schema-15",
    "extracted_data": [
        {
            "name_short": "VinVL (X152-C4)",
            "name_full": "VinVL: ResNeXt-152 C4 object-attribute detection model pre-trained on 4 OD datasets",
            "brief_description": "A large object-detection model (ResNeXt-152 C4) pre-trained on a merged corpus of COCO, OpenImages, Objects365 and Visual Genome to produce rich object-centric region features (2048-d) and position encodings (6-d), plus an attribute head (524 classes).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "X152-C4 (VinVL)",
            "model_size": null,
            "model_description": "ResNeXt-152 C4 backbone object detector pre-trained on a merged dataset (4Sets) with class-aware sampling and fine-tuned on Visual Genome with an attribute branch; outputs per-region visual embeddings (2048-D) and a 6-D position encoding (box coords + h/w). Uses class-agnostic NMS and dilation-free conv head for efficient region feature extraction.",
            "task_name": "Vision-Language downstream tasks (VQA, GQA, Image Captioning, Retrieval, NLVR2)",
            "task_description": "Standard vision-language evaluation tasks where the model's pre-extracted region features and object tags are used as inputs to a transformer-based fusion model to answer questions, generate captions, or match text-image pairs. Not an embodied navigation/manipulation benchmark.",
            "task_type": "visual question answering / image captioning / retrieval (vision-language tasks)",
            "knowledge_type": "spatial+object-relational (object locations via bounding boxes and position encodings; object attributes and categories; relations implied by co-occurrence and attributes)",
            "knowledge_source": "pre-training on large-scale object detection datasets (COCO, OpenImages, Objects365, Visual Genome) and fine-tuning on Visual Genome for attributes",
            "has_direct_sensory_input": true,
            "elicitation_method": "pre-extraction of visual region features used as inputs to downstream VL models (no end-to-end sensory stream during VL fine-tuning), i.e., fixed region-feature pipeline",
            "knowledge_representation": "Dense region feature vectors (2048-D), discrete object-tag tokens (textual labels), and numeric position encodings (R=6: box coords + height & width); attributes as additional label outputs",
            "performance_metric": "Downstream VL metrics depending on task (e.g., VQA accuracy, CIDEr for captioning, retrieval R@k).",
            "performance_result": "Replacing classical OD features with VinVL yielded large gains: VQA test-dev (OSCAR+ w/ VinVL) = 75.95 (vs 73.16 baseline) and vqa-dev improvements (e.g., OSCAR_B baseline 72.38 -&gt; OSCAR+_B w/ VinVL 74.90); image captioning CIDEr and NoCaps gains reported (e.g., large CIDEr improvements up to +5–6 pts in some settings).",
            "success_patterns": "Improved object recognition and attribute detection; richer and more diverse semantic concepts (1848 object classes, 524 attributes) yielding better grounding of language tokens to image regions; improved performance on compositional reasoning benchmarks (GQA) and VQA where object attributes and coarse spatial layout matter.",
            "failure_patterns": "Not designed/tested for embodied planning/navigation/manipulation; Visual Genome detection mAP is low due to annotation sparsity and missing labels; region proposals can have imperfect localization; no explicit continuous 3D spatial map or affordance grounding for manipulation tasks.",
            "baseline_comparison": "Baselines: Anderson et al. (R101-C4 / 'bottom-up' features) and X152-FPN trained on OpenImages; VinVL outperforms Anderson et al. features across tasks (e.g., VQA improvements of ~2.7–3.5 absolute), with the paper attributing ~95% of total VL performance gains to improved vision features.",
            "ablation_results": "Ablations show (1) vision pre-training (merged 4Sets) and larger model size drive most gains (model+data are additive), (2) attribute training (524 classes) significantly improves VL tasks, (3) C4 architecture outperforms FPN for VL due to ImageNet pre-trained convolutional head and inductive bias, (4) class-agnostic NMS and conv changes speed up extraction without accuracy loss.",
            "key_findings": "Spatial and object-relational knowledge in VL pipelines is encoded as region-feature vectors coupled with compact position encodings and discrete object/attribute tags; richness/diversity of object/attribute vocabularies and pre-training data scale dominate downstream VL performance improvements; these representations materially improve tasks that rely on object-centric spatial/relational reasoning (VQA, GQA), but the paper does not extend these representations to embodied navigation or manipulation benchmarks or to operation without direct sensory input.",
            "uuid": "e527.0",
            "source_info": {
                "paper_title": "VinVL: Revisiting Visual Representations in Vision-Language Models",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "OSCAR+",
            "name_full": "OSCAR+: Object-Semantics Aligned Pre-training (improved)",
            "brief_description": "A Transformer-based vision-language fusion model that uses object tags as anchors and combines masked-token loss with a novel 3-way contrastive loss for joint image-text pre-training at scale.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "OSCAR+ (Transformer with BERT init)",
            "model_size": "BERT-Base (L=12,H=768) and BERT-Large (L=24,H=1024) variants",
            "model_description": "A unified Transformer encoder initialized from BERT (base/large) that takes as input concatenated language tokens and pre-extracted region features (linear-projected to match embedding size) plus object-tag tokens; trained with a Masked Token Loss on text and a 3-way contrastive loss to jointly optimize VQA and text-image retrieval objectives.",
            "task_name": "Vision-language pre-training and downstream VL tasks (VQA, GQA, captioning, retrieval, NLVR2)",
            "task_description": "Pre-training on large image-text-tag corpora to learn joint representations that can be fine-tuned on downstream tasks requiring grounding of language in visual regions and object tags; tasks require answering questions, generating captions, and matching text-image pairs.",
            "task_type": "multi-step multimodal reasoning / visual question answering / caption generation / retrieval",
            "knowledge_type": "spatial+object-relational (via region features and object-tag tokens), and to some extent procedural in answering question sequences (compositional reasoning), but procedural knowledge is not a central focus",
            "knowledge_source": "pre-training on 5.65M image dataset with images, captions, and image-tags (from COCO, CC, SBU, Flicker30k, OpenImages, VQA, GQA, VG-QA), and region features from VinVL",
            "has_direct_sensory_input": true,
            "elicitation_method": "pre-training (masked-token and 3-way contrastive), then fine-tuning on downstream tasks; uses object tags as anchors to elicit alignment between language and regions",
            "knowledge_representation": "Implicit knowledge encoded in transformer weights; explicit inputs are object tag tokens (textual), region feature vectors (2048-D projected), and position encodings (6-D) that together form multimodal contextualized representations.",
            "performance_metric": "Task-specific metrics: VQA accuracy, GQA accuracy, CIDEr/METEOR/BLEU/SPICE for captioning, R@k for retrieval, accuracy for NLVR2.",
            "performance_result": "OSCAR+ with VinVL (Base) achieved VQA test-dev 75.95 and vqa-dev ~74.90 (substantial improvement over prior OSCAR with Anderson et al. features). On GQA test-dev it achieved 65.05 (vs prior 61.58), and state-of-the-art single-model results on COCO captioning and NoCaps.",
            "success_patterns": "Strong grounding of object names and attributes to region features via object-tag anchors; improved cross-modal retrieval and QA performance indicating robust use of object-relational and coarse spatial cues (via position encodings) for reasoning about scenes.",
            "failure_patterns": "Model relies on pre-extracted region features (not end-to-end sensory stream), lacks an explicit geometric or metric spatial map for navigation/manipulation, and the pre-training contrastive objectives only simulate retrieval and VQA tasks — not embodied planning. No experiments on language-only (no-sensory) operation for embodied control are reported.",
            "baseline_comparison": "Compared to OSCAR (original) and other VLP models (UNITER, LXMERT, ViLBERT), OSCAR+ with VinVL features outperforms them across multiple VL tasks (e.g., VQA, GQA, captioning); the paper quantifies that improved vision features contribute ~95% of the downstream gains.",
            "ablation_results": "Ablations show the 3-way contrastive loss improves transfer to both VQA and retrieval (versus binary loss), scaling pre-training corpus (adding OpenImages tagging/self-training) improves VQA, and that replacing visual features with VinVL drives most gains; removing attribute supervision or reducing vocab hurts performance.",
            "key_findings": "Using discrete object tags as anchors plus dense region embeddings and position encodings allows a transformer language model to learn object-centric spatial and relational abstractions sufficient for improved VQA, retrieval and captioning; however, OSCAR+ operates on fixed visual features and is not demonstrated for embodied planning/navigation/manipulation or for operating without direct sensory inputs.",
            "uuid": "e527.1",
            "source_info": {
                "paper_title": "VinVL: Revisiting Visual Representations in Vision-Language Models",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "Region features + position encoding",
            "name_full": "Region feature vectors (2048-D) with R=6 position encoding (VinVL outputs)",
            "brief_description": "Per-region continuous feature vectors derived from the detector's last-linear-layer input (2048-D) paired with a compact 6-D position encoding representing bounding box coordinates and height/width for each detected region.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Region feature representation (used by OSCAR+ and other VL models)",
            "model_size": null,
            "model_description": "Each detected region r is represented as a tuple (v_hat, z) where v_hat is a 2048-dimensional visual embedding and z is a 6-dimensional position encoding (box coordinates + height & width); region features are linearly projected to the transformer's embedding size and concatenated with object-tag tokens as inputs.",
            "task_name": "Vision-language tasks (VQA, GQA, captioning, retrieval)",
            "task_description": "Used as the visual input modality for joint multimodal Transformer models to ground textual queries and generate language conditioned on image regions and their spatial metadata.",
            "task_type": "visual reasoning / question answering / caption generation",
            "knowledge_type": "spatial+object-relational (explicit 2D bounding-box spatial encodings; object identity and attribute information encoded in region vectors and tags)",
            "knowledge_source": "learned by OD pre-training on multi-dataset corpora and fine-tuned on Visual Genome attribute labels; positional encoding derived from bounding box coordinates",
            "has_direct_sensory_input": true,
            "elicitation_method": "fed as fixed inputs to a Transformer during pre-training and fine-tuning (no additional sensory processing during VL fine-tuning)",
            "knowledge_representation": "Numeric continuous embeddings (implicit in weights) for appearance and discrete numeric tuple for 2D spatial metadata; these are combined and contextualized by self-attention layers to support relational reasoning.",
            "performance_metric": "Used indirectly; downstream metrics reported for models consuming these features (e.g., VQA accuracy, CIDEr).",
            "performance_result": "Switching to VinVL region features (vs older R101-C4 features) raised VQA vqa-dev from ~72.46 to ~74.90 (OSCAR+_B), demonstrating that richer region features and position encodings materially improve VL reasoning.",
            "success_patterns": "Enables better grounding of object mentions and attributes, helps in tasks requiring identification of objects and their coarse 2D spatial relations (relative positions via box coordinates), improves retrieval and QA accuracy where object-level grounding matters.",
            "failure_patterns": "Provides only 2D bounding-box spatial cues (no explicit 3D geometry, metric distances, or continuous egocentric spatial maps), so fine-grained spatial navigation/orientation reasoning for embodied tasks is not supported; reliance on pre-extracted regions can miss affordance-related properties.",
            "baseline_comparison": "Vs grid features or COCO GT boxes: richer OD proposals with larger vocabularies outperformed COCO GT boxes on VQA, showing that semantic richness can matter more than perfect localization for VL tasks.",
            "ablation_results": "Using COCO GT boxes (limited vocabulary) performs worse on VQA than proposals from VG-trained models; adding attribute labels and larger vocabularies improves VQA; FPN vs C4 head choice influences region feature quality.",
            "key_findings": "Spatial knowledge in current VL pipelines is represented as compact numeric box encodings combined with high-dimensional region embeddings; these representations suffice to improve many language-grounded scene reasoning tasks but do not equate to the kinds of egocentric continuous spatial maps needed for embodied navigation or manipulation without additional modules.",
            "uuid": "e527.2",
            "source_info": {
                "paper_title": "VinVL: Revisiting Visual Representations in Vision-Language Models",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "Object tags as anchors (q)",
            "name_full": "Object tag tokens used as semantic anchors in OSCAR+",
            "brief_description": "Discrete textual labels (object tags) produced by the object detector are used as anchor tokens concatenated with region features and language tokens to improve image-text alignment during pre-training and fine-tuning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Object-tag anchors (VinVL tags fed to OSCAR+)",
            "model_size": null,
            "model_description": "Object names and attribute labels detected by the OD model are supplied as textual tokens q; these tokens serve as anchors in the Transformer input sequence to improve alignment between language and specific image regions.",
            "task_name": "Vision-language pre-training and downstream tasks (VQA, captioning, retrieval)",
            "task_description": "Tags help the model ground words to image regions by providing discrete semantic anchors that complement continuous region features, aiding tasks that require object-level reasoning.",
            "task_type": "visual grounding / question answering / captioning",
            "knowledge_type": "object-relational (object identity, attributes, and implied relationships via co-occurrence and attention patterns); supports some spatial reasoning when combined with region position encodings",
            "knowledge_source": "object detector outputs (VinVL) trained on merged OD datasets and VG attribute labels",
            "has_direct_sensory_input": true,
            "elicitation_method": "provided as explicit tokens in the transformer's input sequence (used in masked-token loss and contrastive objectives)",
            "knowledge_representation": "Discrete natural-language tokens representing object names/attributes, which are embedded and contextualized by Transformer self-attention together with region vectors and textual queries",
            "performance_metric": "Improvement measured via downstream metrics (VQA accuracy, retrieval R@k, captioning CIDEr).",
            "performance_result": "Including object tags as anchors (as in OSCAR/OSCAR+) leads to stronger VL alignment and improved downstream results; paper reports that models using tags yield better performance than those that only use region features (contributed materially in OSCAR prior work and here).",
            "success_patterns": "Improves grounding of language to specific objects and attributes, enhances retrieval and QA performance especially when object identity/attribute is crucial.",
            "failure_patterns": "Tags are limited by detector vocabulary and can be noisy or missing; they do not encode continuous spatial metrics or action affordances needed for manipulation; not evaluated in absence of visual input.",
            "baseline_comparison": "OSCAR without object-tag anchors vs OSCAR/OSCAR+ with tags: tags act as anchors and improve alignment; replacing Anderson et al. features with VinVL tags/features gives large gains.",
            "ablation_results": "The paper controlled for tag usage by keeping tags fixed when studying vision features; experiments show that improving vision features yields larger gains even when tags are held constant, but tags remain an important component.",
            "key_findings": "Explicit semantic tokens (object tags) serve as powerful alignment anchors for transformer-based VL models, enabling better utilization of object-relational knowledge; however, this mechanism presumes availability of visual detection and is not demonstrated for purely language-driven embodied planning without sensory input.",
            "uuid": "e527.3",
            "source_info": {
                "paper_title": "VinVL: Revisiting Visual Representations in Vision-Language Models",
                "publication_date_yy_mm": "2021-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Bottom-up and top-down attention for image captioning and visual question answering",
            "rating": 2
        },
        {
            "paper_title": "Oscar: Object-semantics aligned pre-training for vision-language tasks",
            "rating": 2
        },
        {
            "paper_title": "Visual genome: Connecting language and vision using crowdsourced dense image annotations",
            "rating": 1
        },
        {
            "paper_title": "VIVO: Surpassing human performance in novel object captioning with visual vocabulary pre-training",
            "rating": 1
        }
    ],
    "cost": 0.016676499999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>VinVL: Revisiting Visual Representations in Vision-Language Models</h1>
<p>Pengchuan Zhang ${ }^{\odot \dagger}$ Xiujun $\mathrm{Li}^{\odot \boldsymbol{\Delta} \dagger}$ Xiaowei $\mathrm{Hu}^{\odot} \quad$ Jianwei Yang ${ }^{\odot} \quad$ Lei Zhang ${ }^{\odot}$<br>Lijuan Wang ${ }^{\odot} \quad$ Yejin Choi ${ }^{\boldsymbol{\Delta}} \quad$ Jianfeng Gao ${ }^{\odot}$</p>
<p>March 11, 2021</p>
<h4>Abstract</h4>
<p>This paper presents a detailed study of improving visual representations for vision language (VL) tasks and develops an improved object detection model to provide object-centric representations of images. Compared to the most widely used bottom-up and top-down model [2], the new model is bigger, better-designed for VL tasks, and pre-trained on much larger training corpora that combine multiple public annotated object detection datasets. Therefore, it can generate representations of a richer collection of visual objects and concepts. While previous VL research focuses mainly on improving the vision-language fusion model and leaves the object detection model improvement untouched, we show that visual features matter significantly in VL models. In our experiments we feed the visual features generated by the new object detection model into a Transformer-based VL fusion model OSCAR [21], and utilize an improved approach OSCAR+ to pre-train the VL model and fine-tune it on a wide range of downstream VL tasks. Our results show that the new visual features significantly improve the performance across all VL tasks, creating new state-of-the-art results on seven public benchmarks. Code, models and pre-extracted features are released at https://github.com/pzzhang/VinVL.</p>
<h2>1 Introduction</h2>
<p>Vision language pre-training (VLP) has proved effective for a wide range of vision-language (VL) tasks [26, 36, 4, 34, 20, 19, 45, 21]. VLP typically consists of two stages: (1) an object detection model is pre-trained to encode an image and the visual objects in the image to feature vectors, and (2) a crossmodal fusion model is pre-trained to blend text and visual features. While existing VLP research focuses mainly on improving the cross-modal fusion model, this paper focuses on improving the object-centric visual representations and presents a comprehensive empirical study to demonstrate that visual features matter in VL models.</p>
<p>Among the aforementioned work, a widely-used object detection (OD) model [2] is trained on the Visual Genome dataset [16]. The OD model provides an object-centric representation of images, and has been used in many VL models as a black box. In this work, we pre-train a large-scale object-attribute detection model based on the ResNeXt-152 C4 architecture (short as X152-C4). Compared to the OD model of [2], the new model is better-designed for VL tasks, and is bigger and trained on much larger amounts of data, combining multiple public object detection datasets, including COCO [25], OpenImages (OI) [17], Objects365 [31]</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th>Visual feature</th>
<th>VQA</th>
<th></th>
<th>GQA</th>
<th></th>
<th>Image Captioning</th>
<th></th>
<th></th>
<th></th>
<th>NoCaps</th>
<th></th>
<th>Image Retrieval</th>
<th></th>
<th></th>
<th>Text Retrieval</th>
<th></th>
<th></th>
<th>NLVR2</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>test-dev</td>
<td>test-sid</td>
<td>test-dev</td>
<td>test-sid</td>
<td>B@4</td>
<td>M</td>
<td>C</td>
<td>S</td>
<td>C</td>
<td>S</td>
<td>R@1</td>
<td>R@5</td>
<td>R@10</td>
<td>R@1</td>
<td>R@5</td>
<td>R@10</td>
<td>dev</td>
<td>test-P</td>
</tr>
<tr>
<td>Anderson et al. [2]</td>
<td>73.16</td>
<td>73.44</td>
<td>61.58</td>
<td>61.62</td>
<td>40.5</td>
<td>29.7</td>
<td>137.6</td>
<td>22.8</td>
<td>86.58</td>
<td>12.38</td>
<td>54.0</td>
<td>80.8</td>
<td>88.5</td>
<td>70.0</td>
<td>91.1</td>
<td>95.5</td>
<td>78.07</td>
<td>78.36</td>
<td></td>
</tr>
<tr>
<td>Ours</td>
<td>75.95</td>
<td>76.12</td>
<td>65.05</td>
<td>64.65</td>
<td>40.9</td>
<td>30.9</td>
<td>140.6</td>
<td>25.1</td>
<td>92.46</td>
<td>13.07</td>
<td>58.1</td>
<td>83.2</td>
<td>90.1</td>
<td>74.6</td>
<td>92.6</td>
<td>96.3</td>
<td>82.05</td>
<td>83.08</td>
<td></td>
</tr>
<tr>
<td>$\Delta$</td>
<td>2.79 $\dagger$</td>
<td>2.68 $\dagger$</td>
<td>3.47 $\dagger$</td>
<td>3.03 $\dagger$</td>
<td>0.4 $\dagger$</td>
<td>1.2 $\dagger$</td>
<td>3.0 $\dagger$</td>
<td>2.3 $\dagger$</td>
<td>5.9 $\dagger$</td>
<td>0.7 $\dagger$</td>
<td>4.1 $\dagger$</td>
<td>2.4 $\dagger$</td>
<td>1.6 $\dagger$</td>
<td>4.6 $\dagger$</td>
<td>1.5 $\dagger$</td>
<td>0.8 $\dagger$</td>
<td>3.98 $\dagger$</td>
<td>4.71 $\dagger$</td>
<td></td>
</tr>
</tbody>
</table>
<p>Table 1: Uniform improvements on seven VL tasks by replacing visual features from Anderson et al. [2] with ours. The NoCaps baseline is from VIVO [9], and our results are obtained by directly replacing the visual features. The baselines for rest tasks are from OSCAR [21], and our results are obtained by replacing the visual features and performing OSCAR+ pre-training. All models are BERT-Base size. As analyzed in Section 5.2, the new visual features contributes 95% of the improvement.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Predictions from an X152-FPN model trained on OpenImages (Left) and our X152-C4 model trained on four public object detection datasets (Right). Our model contains much richer semantics, such as richer visual concepts and attribute information, and the detected bounding boxes cover nearly all semantically meaningful regions. Compared with those from the common object classes in typical OD models (Left), the rich and diverse region features from our model (Right) are crucial for vision-language tasks. For concepts detected by both models, e.g., "boy", attributes from our model offer richer information, e.g., "young barefoot shirtless standing surfing smiling little playing looking blond boy". There are object concepts that are detected by our model but not by the OpenImages model, including fin, wave, foot, shadow, sky, hair, mountain, water, (bare, tan, light, beige) back, (blue, colorful, floral, multi colored, patterned) trunk, sand, beach, ocean, (yellow, gold) bracelet, logo, hill, head, (black, wet) swim trunks, black, wet swim trunks. Compared to the R101-C4 model of [2], our model produces more accurate object-attribute detection results and better visual features for VL applications; see Appendix A for the full pictures and predictions from [2].
and Visual Genome (VG) [16]. As a result, our OD model achieves much better results on a wide range of VL tasks, as shown in Table 1. Compared to other typical OD models, such as X152-FPN trained on OpenImages, our new model can encode a more diverse collection of visual objects and concepts (e.g., producing visual representations for 1848 object categories and 524 attribute categories), as illustrated by an example in Figure 1.</p>
<p>To validate the effectiveness of the new OD model, we pre-train a Transformer-based cross-modal fusion model OSCAR+ [21] on a public dataset consisting of 8.85 million text-image pairs, where the visual representations of these images are produced by the new OD model and are fixed during OSCAR+ pre-training.</p>
<p>We then fine-tune the pre-trained OSCAR+ for a wide range of downstream tasks, including VL understanding tasks such as VQA [8], GQA [13], NLVR2 [35], and COCO text-image retrieval [25], and VL generation tasks such as COCO image captioning [25] and NoCaps [1]. Our results show that the object-centric representations produced by the new OD model significantly improve the performance across all the VL tasks, often by a large margin over strong baselines using the classical OD model [2], creating new state of the arts on all these tasks, including GQA on which none of the published pre-trained models has surpassed the deliberately designed neural state machine (NSM) [12]. We will release the new OD model to the research community.</p>
<p>The main contributions of this work can be summarized as follows: (i) We present a comprehensive empirical study to demonstrate that visual features matter in VL models. (ii) We have developed a new object detection model that can produce better visual features of images than the classical OD model [2] and substantially uplifts the state-of-the-art results on all major VL tasks across multiple public benchmarks. (iii) We provide a detailed ablation study of our pre-trained object detection model to investigate the relative contribution to the performance improvement due to different design choices regarding diversity of object categories, visual attribute training, training data scale, model size, and model architecture.</p>
<h1>2 Improving Vision (V) in Vision Language (VL)</h1>
<p>Deep learning-based VL models typically consist of two modules: an image understanding module Vision and a cross-modal understanding module VL:</p>
<p>$$
(\boldsymbol{q}, \boldsymbol{v})=\operatorname{Vision}(\operatorname{Img}), \quad y=\operatorname{VL}(\boldsymbol{w}, \boldsymbol{q}, \boldsymbol{v})
$$</p>
<p>where Img and $\boldsymbol{w}$ are the inputs of the vision and language modalities, respectively. The output of the Vision module consists of $\boldsymbol{q}$ and $\boldsymbol{v} . \boldsymbol{q}$ is the semantic representation of the image, such as tags or detected objects, and $\boldsymbol{v}$ the distributional representation of the image in a high-dimensional latent space represented using e.g., the box or region ${ }^{1}$ features produced by a VG-pre-trained Faster-RCNN model [2]. Most VL models use only the visual features $\boldsymbol{v}$, while the recently proposed OSCAR [21] model shows that $\boldsymbol{q}$ can serve as anchors for learning better vision-language joint representations and and thus can improve the performance on various VL tasks. $\boldsymbol{w}$ and $y$ of the VL module of Equation (1) vary among different VL tasks. In VQA, $\boldsymbol{w}$ is a question and $y$ is an answer to be predicted. In text-image retrieval, $\boldsymbol{w}$ is a sentence and $y$ is the matching score of a sentence-image pair. In image captioning, $\boldsymbol{w}$ is not given and $y$ is a caption to be generated.</p>
<p>Inspired by the great success of pre-trained language models to various natural language processing tasks, vision-language pre-training (VLP) has achieved remarkable success in improving the performance of the cross-modal understanding module VL by (1) unifying vision and language modeling VL with Transformer and (2) pre-training the unified VL with large-scale text-image corpora. However, most recent works on VLP treat the image understanding module Vision as a black box and leave the visual feature improvement untouched since the development of the classical OD model [2] three years ago, despite that there has been much research progress on improving object detection by 1) developing much more diverse, richer, and larger training datasets (e.g. OpenImages and Objects 365), 2) gaining new insights in object detection algorithms such as feature pyramid network [23], one-stage dense prediction [24], and anchor-free detectors [37], and 3) leveraging more powerful GPUs for training bigger models.</p>
<p>In this work, we focus on improving Vision for better visual representations. We developed a new OD model by enriching the visual object and attribute categories, enlarging the model size and training on a</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>much larger OD dasetset, and thus advanced the state of the arts on a wide range of VL tasks. We detail how the new OD model is developed in the rest of this section and then describe the use of OSCAR+ for VL pre-training in Section 3.</p>
<h1>2.1 Object Detection Pre-training</h1>
<p>To improve the OD model for VL tasks, we utilize four public object detection datasets. As most datasets do not have attribute annotations, we adopt a pre-training and fine-tuning strategy to build our OD model. We first pre-train an OD model on a large-scale corpus consisting of four public datasets, and then fine-tune the model with an additional attribute branch on Visual Genome, making it capable of detecting both objects and attributes.</p>
<p>Data. Table 2 summarizes the statistics of the four public datasets used in our object detection pre-training, including COCO, OpenImagesV5 (OI), Objects365V1, and Visual Genome (VG). These datasets have complementary characters, and are extremely unbalanced in terms of data size, object vocabulary, and the number of annotations in each class. For example, the VG dataset has a rich and diverse set of annotations for both objects and their attributes with an open vocabulary. But its annotations are noisy and suffer from the missing-annotation problem. The COCO dataset, on the other hand, is very well annotated. But the coverage of visual objects and attributes is much lower than that in VG although we use both its 80 object classes and 91 stuff classes to include as diverse visual concepts as possible. We take the following steps to build a unified corpus by combining the four datasets.</p>
<ol>
<li>First of all, to enhance visual concepts of tail classes, we perform class-aware sampling for OpenImages and Objects365 to get at least 2000 instances per class, resulting in 2.2 M and 0.8 M images, respectively.</li>
<li>To balance the contribution of each dataset, we merge the four datasets with 8 copies of COCO $(8 \times 0.11 \mathrm{M}), 8$ copies of VG $(8 \times 0.1 \mathrm{M}), 2$ copies of class-aware sampled Objects $365(2 \times 0.8 \mathrm{M})$ and one copy of the class-aware sampled OpenImages $(2.2 \mathrm{M})$.</li>
<li>To unify their object vocabularies, we use the VG vocabulary and its object aliases as the base vocabulary, merge a class from the other three datasets into a VG class if their class names or aliases match, and add a new class if no match is found.</li>
<li>Finally, we keep all VG classes that contain at least 30 instances, resulting in 1594 VG classes and 254 classes from the other three datasets that cannot be mapped to the VG vocabulary, resulting in a merged object detection dataset that contains 1848 classes.</li>
</ol>
<table>
<thead>
<tr>
<th style="text-align: center;">Source</th>
<th style="text-align: center;">VG</th>
<th style="text-align: center;">COCO w/ stuff</th>
<th style="text-align: center;">Objects365</th>
<th style="text-align: center;">OpenImagesV5</th>
<th style="text-align: center;">Total</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Image</td>
<td style="text-align: center;">97 k</td>
<td style="text-align: center;">111 k</td>
<td style="text-align: center;">609 k</td>
<td style="text-align: center;">1.67 M</td>
<td style="text-align: center;">2.49 M</td>
</tr>
<tr>
<td style="text-align: center;">classes</td>
<td style="text-align: center;">1594</td>
<td style="text-align: center;">171</td>
<td style="text-align: center;">365</td>
<td style="text-align: center;">500</td>
<td style="text-align: center;">1848</td>
</tr>
<tr>
<td style="text-align: center;">Sampling</td>
<td style="text-align: center;">$\times 8$</td>
<td style="text-align: center;">$\times 8$</td>
<td style="text-align: center;">CA-2k, $\times 2$</td>
<td style="text-align: center;">CA-2k</td>
<td style="text-align: center;">5.43 M</td>
</tr>
</tbody>
</table>
<p>Table 2: Statistics of the Vision pre-training datasets. In sampling, $\times k$ means $k$ copies in one epoch and "CA-2k" means class-aware sampling with at least 2000 instances per class.</p>
<p>Model Architecture (FPN vs C4). Although [23] shows that the FPN model outperforms the C4 model for object detection, recent studies [14] demonstrate that FPN does not provide more effective region features for VL tasks than C4, which is also confirmed by our experimental results ${ }^{2}$. We thus conduct a set of carefully designed experiments, as to be detailed in Appendix E, and find two main reasons for this. The first is that all layers in the C4 model used for region feature extraction are pre-trained using the ImageNet dataset while the multi-layer-perceptron (MLP) head of the FPN model are not. It turns out that the VG dataset is still too small to train a good enough visual features for VL tasks and using ImageNet-pre-trained weights is beneficial. The second is due to the different network architectures (CNN vs. MLP). The convolutional head used in C4 has a better inductive bias for encoding visual information than the MLP head of FPN. Therefore, in this study we use C4 architecture for VLP.</p>
<p>Model Pre-Training. Following the common practice in object detection training, we freeze the first convolution layer, the first residual block, and all the batch-norm layers. We also use several data augmentation methods, including horizontal flipping and multi-scale training. To train a detection model with the X152C4 architecture, we initialize the model backbone from an ImageNet-5K checkpoint [40] and train for 1.8M iterations with a batch size of 16 images.</p>
<h1>2.2 Injecting attribute information into the model</h1>
<p>Following [2], we add an attribute branch to the pre-trained OD model, and then fine-tune the OD model on VG to inject attribute information (524 classes). Since the object representations are pre-trained in the object detection pre-training stage, we can focus the VG fine-tuning on learning attributes by picking a much larger attribute loss weight 1.25 , compared to 0.5 used in [2, 14]. Thus, our fine-tuned model significantly outperforms previous models [2, 14] in detecting objects and attributes on VG.</p>
<h3>2.3 Efficient region feature extractor for VL tasks</h3>
<p>With a richer set of visual objects and attributes, the classical class-aware non-maximal suppression (NMS) post-processing takes a significantly larger amount of time to remove overlapped bounding boxes, making the feature extraction process extremely slow. To improve the efficiency, we replace the class-aware NMS with the class-agnostic NMS that only conducts the NMS operation once ${ }^{3}$. We also replace the timeconsuming conv layers with dilation=2 used in [2] with conv layers without dilation. These two replacements make the region feature extraction process much faster than that in [2] without any accuracy drop on VL downstream tasks. We report the end-to-end inference time of VL models with different vision models on a Titan-X GPU and a CPU with a single thread in Table 21 in Appendix F.</p>
<p>In summary, the pre-trained OD model serves as the image understanding module, as in Equation (1), to produce vision presentations $(\boldsymbol{q}, \boldsymbol{v})$ for downstream VL tasks. Here, $\boldsymbol{q}$ is the set of detected object names (in text) and $\boldsymbol{v}$ is the set of region features. Each region feature is denoted as $(\hat{v}, z)$, where $\hat{v}$ is a $P$-dimensional representation from the input of the last linear classification layer of the detection head ( i.e., $P=2048$ ) and $z$ is a $R$-dimensional position encoding of the region (i.e., $R=6$ ) ${ }^{4}$.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>3 OSCAR+ Pre-training</h1>
<p>The success of VLP lies in the use of a unifying model architecture for a wide range of VL tasks and the large-scale pre-training of the unified model using objectives that correlate with the performance metrics of these downstream VL tasks. In this study we pre-train an improved version of OSCAR [21], known as OSCAR+ models, to learn the joint image-text representations using image tags as anchors for image-text alignment.</p>
<h3>3.1 Pre-training corpus</h3>
<p>We build our pre-training corpus based on three types of existing vision and VL datasets: (1) image captioning datasets with human-annotated captions as $\boldsymbol{w}$ and machine-generated ${ }^{5}$ image tags as $\boldsymbol{q}$, including COCO [25], Conceptual Captions (CC) [32], SBU captions [28] and flicker30k [42]; (2) visual QA datasets with questions as $\boldsymbol{w}$ and human-annotated answers as $\boldsymbol{q}$, including GQA [13], VQA [8] and VG-QAs; (3) image tagging datasets with machine-generated ${ }^{6}$ captions as $\boldsymbol{w}$ and human-annotated tags as $\boldsymbol{q}$, including a subset of OpenImages ( 1.67 M images). In total, the corpus contains 5.65 million unique images, 8.85 million text-tag-image triples. The detailed statistics are presented in Table 17 in the Appendix. The size of the pre-training corpus could have been significantly increased by combining large-scale image tagging datasets, such as the full set of OpenImages ( 9 M images) and YFCC ( 92 M images). We leave it to future work to leverage much larger corpora for model pre-training.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Loss</th>
<th style="text-align: center;">$\left(\boldsymbol{w}, \boldsymbol{q} / \boldsymbol{q}^{\prime}, \boldsymbol{v}\right)$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">$\left(\boldsymbol{w} / \boldsymbol{w}^{\prime}, \boldsymbol{q}, \boldsymbol{v}\right)$</th>
<th style="text-align: center;">3-way contrastive</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$\boldsymbol{w}^{\prime} / \boldsymbol{q}^{\prime}$</td>
<td style="text-align: center;">All $\boldsymbol{q}$ 's (OSCAR)</td>
<td style="text-align: center;">$\boldsymbol{q}$ 's from QA</td>
<td style="text-align: center;">All $\boldsymbol{w}$ 's</td>
<td style="text-align: center;">All (OSCAR+)</td>
<td style="text-align: center;">$\boldsymbol{q}$ 's from QA</td>
</tr>
<tr>
<td style="text-align: center;">VQA (vqa-dev)</td>
<td style="text-align: center;">$\mathbf{6 9 . 8} \pm 0.08$</td>
<td style="text-align: center;">$\mathbf{7 0 . 1} \pm 0.08$</td>
<td style="text-align: center;">$69.5 \pm 0.05$</td>
<td style="text-align: center;">$\mathbf{6 9 . 8} \pm 0.06$</td>
<td style="text-align: center;">$\mathbf{6 9 . 7} \pm 0.06$</td>
</tr>
<tr>
<td style="text-align: center;">COCO-IR</td>
<td style="text-align: center;">$73.9 \pm 0.2$</td>
<td style="text-align: center;">$\mathbf{7 5 . 0} \pm 0.2$</td>
<td style="text-align: center;">$\mathbf{7 5 . 0} \pm 0.7$</td>
<td style="text-align: center;">$\mathbf{7 8 . 3} \pm 0.3$</td>
<td style="text-align: center;">$\mathbf{7 7 . 7} \pm 0.7$</td>
</tr>
</tbody>
</table>
<p>Table 3: Effects of different pre-training contrastive losses on downstream tasks (R50-C4 as Vision module and 4-layer Transformer as VL module in (1) ). COCO-IR metric is Image-to-Text retrieval R@1 at COCO 1 K test set. Blue indicates the best result for a task and Black indicates the runner-up.</p>
<h3>3.2 Pre-training Objectives</h3>
<p>There are two terms in the OSCAR+ pre-training loss as in Equation (2).</p>
<p>$$
\mathcal{L}<em _mathrm_MTL="\mathrm{MTL">{\text {Pre-training }}=\mathcal{L}</em>
$$}}+\mathcal{L}_{\mathrm{CL} 3</p>
<p>$\mathcal{L}<em _mathrm_CL="\mathrm{CL">{\text {MTL }}$ is the Masked Token Loss defined on the text modality ( $\boldsymbol{w}$ and $\boldsymbol{q}$ ), following closely [21]. (See Appendix B. 2 for details.) $\mathcal{L}</em>$ : the {caption, image-tags, image-features $}$ triplets of the image captioning and image tagging data, and the {question, answer, image-features} triplets of the VQA data.} 3}$ is a novel 3-way Contrastive Loss. Different from the binary contrastive loss used in OSCAR [21], the proposed 3-way Contrastive Loss to effectively optimize the training objectives used for VQA [41] and text-image matching [6]. As shown in Equation 3, $\mathcal{L}_{\mathrm{CL} 3}$ takes into account two types of training samples $\boldsymbol{x</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>To compute contrastive losses, negative examples need to be constructed. We construct two types of negative (unmatched) triplets for the two types of training samples, respectively. One is the polluted "captions" $\left(\boldsymbol{w}^{\prime}, \boldsymbol{q}, \boldsymbol{v}\right)$ and the other the polluted "answers" $\left(\boldsymbol{w}, \boldsymbol{q}^{\prime}, \boldsymbol{v}\right)$. To classify whether a caption-tags-image triplet contains a polluted caption is a text-image matching task. To classify whether a question-answerimage triplet contains a polluted answer is an answer selection task for VQA. Since the encoding of [CLS] can be viewed as a representation of the triplet $(\boldsymbol{w}, \boldsymbol{q}, \boldsymbol{v})$, we apply a fully-connected (FC) layer on top of it as a 3-way classifier $f($.$) to predict whether the triplet is matched (c=0)$, contains a polluted $\boldsymbol{w}(c=1)$, or contains a polluted $\boldsymbol{q}(c=2)$. The 3-way contrastive loss is defined as</p>
<p>$$
\mathcal{L}<em _boldsymbol_w="(\boldsymbol{w">{\mathrm{CL} 3}=-\mathbb{E}</em>))
$$}, \boldsymbol{q}, \boldsymbol{v} ; c) \sim \tilde{\mathcal{D}}} \log p(c \mid f(\boldsymbol{w}, \boldsymbol{q}, \boldsymbol{v</p>
<p>where the dataset $(\boldsymbol{w}, \boldsymbol{q}, \boldsymbol{v} ; c) \in \tilde{\mathcal{D}}$ contains $50 \%$ matched triples, $25 \% \boldsymbol{w}$-polluted triples, and $25 \% \boldsymbol{q}$ polluted triples. For efficient implementation, the polluted $\boldsymbol{w}^{\prime}$ is uniformly sampled from all $\boldsymbol{w}$ 's (captions and questions) and $\boldsymbol{q}^{\prime}$ is uniformly sampled from all $\boldsymbol{q}$ 's (tags and answers) in the corpus. As demonstrated in Table 3, when only the answer-polluted triplets are used, i.e., $\left(\boldsymbol{w}, \boldsymbol{q}^{\prime}, \boldsymbol{v}\right)$ with $\boldsymbol{q}^{\prime}$ sampled from $\boldsymbol{q}$ 's from QA corpus, the contrastive loss simulates closely the objective for the VQA task but not the text-image retrieval task. As a result, the pre-trained model can be effectively adapted to VQA, but not so to text-image retrieval. By contrast, the proposed 3-way contrastive loss transfers well to both tasks.</p>
<h1>3.3 Pre-trained models</h1>
<p>We pre-train two model variants, denoted as OSCAR $+<em _mathrm_L="\mathrm{L">{\mathrm{B}}$ and OSCAR $+</em>}}$, which are initialized with parameters $\boldsymbol{\theta<em _BERT="{BERT" _text="\text">{\text {BERT }}$ of BERT base $(L=12, H=768, A=12)$ and large $(L=24, H=1024, A=16)$, respectively, where $L$ is the number of layers, $H$ the hidden size, and $A$ the number of self-attention heads. To ensure that the image region features have the same input embedding size as BERT, we transform the position-augmented region features using a linear projection via matrix $\mathbf{W}$. The trainable parameters are $\boldsymbol{\theta}=\left{\boldsymbol{\theta}</em>\right}$. OSCAR $+}}, \mathbf{W<em _mathrm_L="\mathrm{L">{\mathrm{B}}$ is trained for at least 1 M steps, with learning rate $1 e^{-4}$ and batch size 1024 . OSCAR $+</em>$ are 35 and 50 , respectively.}}$ is trained for at least 1 M steps, with learning rate $3 e^{-5}$ and batch size 1024. The sequence length of language tokens $[\boldsymbol{w}, \boldsymbol{q}]$ and region features $\boldsymbol{v</p>
<h2>4 Adapting to VL Tasks</h2>
<p>We adapt the pre-trained models to seven downstream VL tasks, including five understanding tasks and two generation tasks. Each task poses different challenges for adaptation. This section briefly introduces the tasks and our fine-tuning strategy. We refer the readers to Appendix C for details.</p>
<p>VQA \&amp; GQA These two are the most widely used understanding task for evaluating VL models in the research community. The tasks require the model to answer natural language questions based on an image. In this study, we perform experiments on the widely-used VQA v2.0 dataset [8] and GQA dataset [13], Following the setting of [2], for each question, the model picks an answer from a shared answer set (i.e., 3,129 candidates for VQA, 1, 852 candidates for GQA). When adapting a VLP model to the VQA task, we</p>
<p>construct the input by concatenating a given question, object tags and object region features, and then feed the [CLS] output from OSCAR+ to a task-specific linear classifier with a softmax layer for answer prediction.</p>
<p>Image Captioning \&amp; NoCaps The captioning task is to generate a natural language caption for an image. This is the most widely used VL generation task in the research community - the Image Captioning Leaderboard ${ }^{8}$ hosts more than 260 models as of December 10, 2020. To enable caption generation, we fine-tune OSCAR+ using the seq2seq objective. Each training sample is converted to a triplet consisting of a caption, a set of image region features, and a set of object tags. We randomly mask out $15 \%$ of the caption tokens, and use the encoding of the remaining context (the triplet) to predict the masked tokens. Similar to VLP [21, 45], the self-attention mask is constrained such that a caption token can only attend to the tokens before its position to simulate a uni-directional generation process. All caption tokens have full attentions to image regions and object tags but not the other way around. During inference, we first encode the image regions, object tags, and a special token [CLS] as input. Then the model starts to generate a caption by feeding in a [MASK] token and sampling a token from a vocabulary based on the token probability output. Next, the [MASK] token in the previous input sequence is replaced with the sampled token and a new [MASK] is appended for the next word prediction. The generation process terminates when the model outputs the [STOP] token or the generated sentence exceeds a pre-defined max length. We perform image captioning experiments on the COCO image captioning dataset [25]. Novel Object Captioning at Scale [1] extends the image captioning task to test a model's capability of describing novel objects from the Open Images dataset [17] which are unseen in the training corpus. Following the restriction guideline of NoCaps, we use the predicted Visual Genome and Open Images labels to form the input tag sequences, and directly train OSCAR+ on COCO without the initialization from pre-training. VIVO [9] proposed a VLP technique by only using image tagging data, and achieved SOTA results on NoCaps by fine-tuning on COCO captions. We reproduced VIVO with only one change, i.e., replacing its original vision model with our new vision model, and improved the VIVO performance significantly (short as VinVL+VIVO), as reported in Table 9.</p>
<p>Image(-to-Text) Retrieval \&amp; Text(-to-Image) Retrieval Both tasks require the model to calculate a similarity score between an image and a sentence. Thus, the task is widely used to directly measure the quality of the cross-modal VL representation. Following [21], we formulate the task as a binary classification problem, where given a matched image-text pair, we randomly select a different image or a different sentence to form an unmatched pair. The representation of [CLS] is used as the input to a classifier to predict a score indicating how likely the given pair is matched. In testing, the predicted score is used to rank a given image-text pairs of a query. Following [19], we report the top- $K$ retrieval results on both the 1 K and 5 K COCO test sets.</p>
<p>NLVR2 The dataset is developed for joint reasoning about natural language and images [35]. The task is to determine whether a text description is true about a pair of images. For fine-tuning, we first construct two input sequences, each containing the concatenation of the given text description and one of the images, and then two [CLS] outputs from OSCAR+ are concatenated to form the input to a binary classifier for prediction.</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>5 Experiments \&amp; Analysis</h1>
<h3>5.1 Main Results</h3>
<p>To account for model parameter efficiency, we group the SoTA models in three categories: (i) $\mathrm{SoTA}<em B="B">{S}$ indicates the best performance achieved by small models prior to the Transformer-based VLP models. (ii) $\mathrm{SoTA}</em>$ indicates the best performance yielded by VLP models that have a similar size to BERT large.}$ indicates the best performance produced by VLP models of a similar size to BERT base. (iii) $\mathrm{SoTA}_{L</p>
<p>Table 4 gives an overview of the results of OSCAR+ with VinVL(short for VinVL) on seven VL tasks, compared to previous SoTAs ${ }^{9}$. VinVLoutperforms previous SoTA models on all tasks ${ }^{10}$, often by a significantly large margin. The result demonstrates the effectiveness of the region features produced by the new OD model.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">VQA test-dev</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">GQA <br> test-dev</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Image Captioning</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">NoCaps</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Image Retrieval</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Text Retrieval</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">NLVR2</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">B@4</td>
<td style="text-align: center;">M</td>
<td style="text-align: center;">C</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">C</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">R@1</td>
<td style="text-align: center;">R@5</td>
<td style="text-align: center;">R@10</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">R@1</td>
<td style="text-align: center;">R@5</td>
<td style="text-align: center;">R@10</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">dev</td>
<td style="text-align: center;">test-P</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$\mathrm{SoTA}_{S}$</td>
<td style="text-align: center;">70.55</td>
<td style="text-align: center;">70.92</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">63.17</td>
<td style="text-align: center;">38.9</td>
<td style="text-align: center;">29.2</td>
<td style="text-align: center;">129.8</td>
<td style="text-align: center;">22.4</td>
<td style="text-align: center;">61.5</td>
<td style="text-align: center;">9.2</td>
<td style="text-align: center;">39.2</td>
<td style="text-align: center;">68.0</td>
<td style="text-align: center;">81.3</td>
<td style="text-align: center;">56.6</td>
<td style="text-align: center;">84.5</td>
<td style="text-align: center;">92.0</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">54.10</td>
<td style="text-align: center;">54.80</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$\mathrm{SoTA}_{B}$</td>
<td style="text-align: center;">73.59</td>
<td style="text-align: center;">73.67</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">61.58</td>
<td style="text-align: center;">61.62</td>
<td style="text-align: center;">40.5</td>
<td style="text-align: center;">29.7</td>
<td style="text-align: center;">137.6</td>
<td style="text-align: center;">22.8</td>
<td style="text-align: center;">86.58</td>
<td style="text-align: center;">12.38</td>
<td style="text-align: center;">54.0</td>
<td style="text-align: center;">80.8</td>
<td style="text-align: center;">88.5</td>
<td style="text-align: center;">70.0</td>
<td style="text-align: center;">91.1</td>
<td style="text-align: center;">95.5</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">78.39</td>
<td style="text-align: center;">79.30</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$\mathrm{SoTA}_{L}$</td>
<td style="text-align: center;">74.75</td>
<td style="text-align: center;">74.93</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">41.7</td>
<td style="text-align: center;">30.6</td>
<td style="text-align: center;">140.0</td>
<td style="text-align: center;">24.5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">57.5</td>
<td style="text-align: center;">82.8</td>
<td style="text-align: center;">89.8</td>
<td style="text-align: center;">73.5</td>
<td style="text-align: center;">92.3</td>
<td style="text-align: center;">96.0</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">79.76</td>
<td style="text-align: center;">81.47</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">VinVL ${ }_{6}$</td>
<td style="text-align: center;">75.95</td>
<td style="text-align: center;">76.12</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">65.05</td>
<td style="text-align: center;">64.65</td>
<td style="text-align: center;">40.9</td>
<td style="text-align: center;">30.9</td>
<td style="text-align: center;">140.6</td>
<td style="text-align: center;">25.1</td>
<td style="text-align: center;">92.46</td>
<td style="text-align: center;">13.07</td>
<td style="text-align: center;">58.1</td>
<td style="text-align: center;">83.2</td>
<td style="text-align: center;">90.1</td>
<td style="text-align: center;">74.6</td>
<td style="text-align: center;">92.6</td>
<td style="text-align: center;">96.3</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">82.05</td>
<td style="text-align: center;">83.08</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">VinVL ${ }_{L}$</td>
<td style="text-align: center;">76.52</td>
<td style="text-align: center;">76.60</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">41.0</td>
<td style="text-align: center;">31.1</td>
<td style="text-align: center;">140.9</td>
<td style="text-align: center;">25.2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">58.8</td>
<td style="text-align: center;">83.5</td>
<td style="text-align: center;">90.3</td>
<td style="text-align: center;">75.4</td>
<td style="text-align: center;">92.9</td>
<td style="text-align: center;">96.2</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">82.67</td>
<td style="text-align: center;">83.98</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$\Delta$</td>
<td style="text-align: center;">1.77 $\uparrow$</td>
<td style="text-align: center;">1.67 $\uparrow$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">3.47 $\uparrow$</td>
<td style="text-align: center;">1.48 $\uparrow$</td>
<td style="text-align: center;">0.7 $\downarrow$</td>
<td style="text-align: center;">0.5 $\uparrow$</td>
<td style="text-align: center;">0.9 $\uparrow$</td>
<td style="text-align: center;">0.7 $\uparrow$</td>
<td style="text-align: center;">5.9 $\uparrow$</td>
<td style="text-align: center;">0.7 $\uparrow$</td>
<td style="text-align: center;">1.3 $\uparrow$</td>
<td style="text-align: center;">0.7 $\uparrow$</td>
<td style="text-align: center;">0.5 $\uparrow$</td>
<td style="text-align: center;">1.9 $\uparrow$</td>
<td style="text-align: center;">0.6 $\uparrow$</td>
<td style="text-align: center;">0.3 $\uparrow$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">2.91 $\uparrow$</td>
<td style="text-align: center;">2.51 $\uparrow$</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 4: An overall comparison with SoTAs on seven tasks. $\Delta$ indicates the improvement over SoTA. SoTA with subscript S, B, L indicates performance achieved by small models, and models with the model size similar to BERT base and large, respectively. SoTAs: VQA is from ERNIE-VIL [43], GQA is from NSM [12], NoCaps is from VIVO [9], NLVR2 is from VILLA [7], the rest tasks are from OSCAR [21].</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">ViLBERT <br> Base</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">Large</td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">Large</td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">Large</td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">Large</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Ensemble ${ }^{a}$</td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">Large</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Test-dev</td>
<td style="text-align: center;">70.63</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">70.50</td>
<td style="text-align: center;">70.80</td>
<td style="text-align: center;">72.42</td>
<td style="text-align: center;">73.15</td>
<td style="text-align: center;">72.27</td>
<td style="text-align: center;">73.24</td>
<td style="text-align: center;">73.16</td>
<td style="text-align: center;">73.61</td>
<td style="text-align: center;">73.59</td>
<td style="text-align: center;">73.69</td>
<td style="text-align: center;">72.62</td>
<td style="text-align: center;">74.75</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">75.95</td>
<td style="text-align: center;">76.52</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Test-std</td>
<td style="text-align: center;">70.92</td>
<td style="text-align: center;">70.83</td>
<td style="text-align: center;">71.00</td>
<td style="text-align: center;">72.54</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">72.46</td>
<td style="text-align: center;">73.40</td>
<td style="text-align: center;">73.44</td>
<td style="text-align: center;">73.82</td>
<td style="text-align: center;">73.67</td>
<td style="text-align: center;">74.87</td>
<td style="text-align: center;">72.85</td>
<td style="text-align: center;">74.93</td>
<td style="text-align: center;">76.10</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">76.12</td>
<td style="text-align: center;">76.60</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 5: Evaluation results on VQA. * denotes the No. 1 ensemble model of InterBERT Large on the VQA leaderboard.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">LXMERT</th>
<th style="text-align: center;">MMN [3]</th>
<th style="text-align: center;">12-in-1</th>
<th style="text-align: center;">OSCAR ${ }_{\text {B }}$</th>
<th style="text-align: center;">NSM [12]</th>
<th style="text-align: center;">OSCAR+ ${ }_{\text {B }}$ w/ VinVL</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Test-dev</td>
<td style="text-align: center;">60.00</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">61.58</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">65.05</td>
</tr>
<tr>
<td style="text-align: center;">Test-std</td>
<td style="text-align: center;">60.33</td>
<td style="text-align: center;">60.83</td>
<td style="text-align: center;">60.65</td>
<td style="text-align: center;">61.62</td>
<td style="text-align: center;">63.17</td>
<td style="text-align: center;">64.65</td>
</tr>
</tbody>
</table>
<p>Table 6: Evaluation results on GQA.
In Tables 5 to 11, we report the detailed results for each downstream task, respectively. (i) The VQA results are shown in Table 5, where our single OSCAR $+_{\mathrm{B}}$ model outperforms the best ensemble model (InterBERT large [22]) on the VQA leaderboard as of Dec. 12, $2020^{11}$. (ii) The GQA results are shown in Table 6, where OSCAR+w/VinVLis the first VLP model that outperforms the neural state machine (NSM) [12] which contains some sophisticated reasoning components deliberately designed for the task. (iii) The Image Captioning results on the public "Karpathy" 5k test split are shown in Table 7. Table 8 shows on a concise version of the COCO image captioning online leaderboard ${ }^{12}$. The online testing setting</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">cross-entropy optimization</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">CIDEr optimization</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">B@4</td>
<td style="text-align: center;">M</td>
<td style="text-align: center;">C</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">B@4</td>
<td style="text-align: center;">M</td>
<td style="text-align: center;">C</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">BUTD [2]</td>
<td style="text-align: center;">36.2</td>
<td style="text-align: center;">27.0</td>
<td style="text-align: center;">113.5</td>
<td style="text-align: center;">20.3</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">36.3</td>
<td style="text-align: center;">27.7</td>
<td style="text-align: center;">120.1</td>
<td style="text-align: center;">21.4</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">VLP [45]</td>
<td style="text-align: center;">36.5</td>
<td style="text-align: center;">28.4</td>
<td style="text-align: center;">117.7</td>
<td style="text-align: center;">21.3</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">39.5</td>
<td style="text-align: center;">29.3</td>
<td style="text-align: center;">129.3</td>
<td style="text-align: center;">23.2</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">AoANet [10]</td>
<td style="text-align: center;">37.2</td>
<td style="text-align: center;">28.4</td>
<td style="text-align: center;">119.8</td>
<td style="text-align: center;">21.3</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">38.9</td>
<td style="text-align: center;">29.2</td>
<td style="text-align: center;">129.8</td>
<td style="text-align: center;">22.4</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">OSCAR ${ }_{B}$ [21]</td>
<td style="text-align: center;">36.5</td>
<td style="text-align: center;">30.3</td>
<td style="text-align: center;">123.7</td>
<td style="text-align: center;">23.1</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">40.5</td>
<td style="text-align: center;">29.7</td>
<td style="text-align: center;">137.6</td>
<td style="text-align: center;">22.8</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">OSCAR ${ }_{L}$ [21]</td>
<td style="text-align: center;">37.4</td>
<td style="text-align: center;">30.7</td>
<td style="text-align: center;">127.8</td>
<td style="text-align: center;">23.5</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">41.7</td>
<td style="text-align: center;">30.6</td>
<td style="text-align: center;">140.0</td>
<td style="text-align: center;">24.5</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">OSCAR+ ${ }_{\mathrm{B}} \mathrm{w} /$ VinVL</td>
<td style="text-align: center;">38.2</td>
<td style="text-align: center;">30.3</td>
<td style="text-align: center;">129.3</td>
<td style="text-align: center;">23.6</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">40.9</td>
<td style="text-align: center;">30.9</td>
<td style="text-align: center;">140.4</td>
<td style="text-align: center;">25.1</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">OSCAR+ ${ }_{\mathrm{L}} \mathrm{w} /$ VinVL</td>
<td style="text-align: center;">38.5</td>
<td style="text-align: center;">30.4</td>
<td style="text-align: center;">130.8</td>
<td style="text-align: center;">23.4</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">41.0</td>
<td style="text-align: center;">31.1</td>
<td style="text-align: center;">140.9</td>
<td style="text-align: center;">25.2</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 7: Image captioning evaluation results (single model) on COCO "Karpathy" test split. (Note: B@4: BLEU@4, M: METEOR, C: CIDEr, S: SPICE.)</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">BLEU@1</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">BLEU@2</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">BLEU@3</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">BLEU@4</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">METEOR</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">ROUGE-L</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">CIDEr-D</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">c5</td>
<td style="text-align: center;">c40</td>
<td style="text-align: center;">c5</td>
<td style="text-align: center;">c40</td>
<td style="text-align: center;">c5</td>
<td style="text-align: center;">c40</td>
<td style="text-align: center;">c5</td>
<td style="text-align: center;">c40</td>
<td style="text-align: center;">c5</td>
<td style="text-align: center;">c40</td>
<td style="text-align: center;">c5</td>
<td style="text-align: center;">c40</td>
<td style="text-align: center;">c5</td>
<td style="text-align: center;">c40</td>
</tr>
<tr>
<td style="text-align: center;">BUTD [2]</td>
<td style="text-align: center;">80.2</td>
<td style="text-align: center;">95.2</td>
<td style="text-align: center;">64.1</td>
<td style="text-align: center;">88.8</td>
<td style="text-align: center;">49.1</td>
<td style="text-align: center;">79.4</td>
<td style="text-align: center;">36.9</td>
<td style="text-align: center;">68.5</td>
<td style="text-align: center;">27.6</td>
<td style="text-align: center;">36.7</td>
<td style="text-align: center;">57.1</td>
<td style="text-align: center;">72.4</td>
<td style="text-align: center;">117.9</td>
<td style="text-align: center;">120.5</td>
</tr>
<tr>
<td style="text-align: center;">AoANet [10]</td>
<td style="text-align: center;">81.0</td>
<td style="text-align: center;">95.0</td>
<td style="text-align: center;">65.8</td>
<td style="text-align: center;">89.6</td>
<td style="text-align: center;">51.4</td>
<td style="text-align: center;">81.3</td>
<td style="text-align: center;">39.4</td>
<td style="text-align: center;">71.2</td>
<td style="text-align: center;">29.1</td>
<td style="text-align: center;">38.5</td>
<td style="text-align: center;">58.9</td>
<td style="text-align: center;">74.5</td>
<td style="text-align: center;">126.9</td>
<td style="text-align: center;">129.6</td>
</tr>
<tr>
<td style="text-align: center;">X-Transformer [29]</td>
<td style="text-align: center;">81.9</td>
<td style="text-align: center;">95.7</td>
<td style="text-align: center;">66.9</td>
<td style="text-align: center;">90.5</td>
<td style="text-align: center;">52.4</td>
<td style="text-align: center;">82.5</td>
<td style="text-align: center;">40.3</td>
<td style="text-align: center;">72.4</td>
<td style="text-align: center;">29.6</td>
<td style="text-align: center;">39.2</td>
<td style="text-align: center;">59.5</td>
<td style="text-align: center;">75.0</td>
<td style="text-align: center;">131.1</td>
<td style="text-align: center;">133.5</td>
</tr>
<tr>
<td style="text-align: center;">OSCAR+ w/ VinVL</td>
<td style="text-align: center;">81.9</td>
<td style="text-align: center;">96.9</td>
<td style="text-align: center;">66.9</td>
<td style="text-align: center;">92.4</td>
<td style="text-align: center;">52.6</td>
<td style="text-align: center;">84.7</td>
<td style="text-align: center;">40.4</td>
<td style="text-align: center;">74.9</td>
<td style="text-align: center;">30.6</td>
<td style="text-align: center;">40.8</td>
<td style="text-align: center;">60.4</td>
<td style="text-align: center;">76.8</td>
<td style="text-align: center;">134.7</td>
<td style="text-align: center;">138.7</td>
</tr>
</tbody>
</table>
<p>Table 8: Leaderboard of the state-of-the-art image captioning models on the COCO online testing.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">in-domain</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">near-domain</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">out-of-domain</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">overall</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">in-domain</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">near-domain</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">out-of-domain</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">overall</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CIDEr</td>
<td style="text-align: center;">SPICE</td>
<td style="text-align: center;">CIDEr</td>
<td style="text-align: center;">SPICE</td>
<td style="text-align: center;">CIDEr</td>
<td style="text-align: center;">SPICE</td>
<td style="text-align: center;">CIDEr</td>
<td style="text-align: center;">SPICE</td>
<td style="text-align: center;">CIDEr</td>
<td style="text-align: center;">SPICE</td>
<td style="text-align: center;">CIDEr</td>
<td style="text-align: center;">SPICE</td>
<td style="text-align: center;">CIDEr</td>
<td style="text-align: center;">SPICE</td>
<td style="text-align: center;">CIDEr</td>
<td style="text-align: center;">SPICE</td>
</tr>
<tr>
<td style="text-align: center;">Validation Set</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Test Set</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">UpDown ${ }^{+}$</td>
<td style="text-align: center;">79.3</td>
<td style="text-align: center;">12.4</td>
<td style="text-align: center;">73.8</td>
<td style="text-align: center;">11.4</td>
<td style="text-align: center;">71.7</td>
<td style="text-align: center;">9.9</td>
<td style="text-align: center;">74.3</td>
<td style="text-align: center;">11.2</td>
<td style="text-align: center;">76.0</td>
<td style="text-align: center;">11.8</td>
<td style="text-align: center;">74.2</td>
<td style="text-align: center;">11.5</td>
<td style="text-align: center;">66.7</td>
<td style="text-align: center;">9.7</td>
<td style="text-align: center;">73.1</td>
<td style="text-align: center;">11.2</td>
</tr>
<tr>
<td style="text-align: center;">OSCAR ${ }_{B}{ }^{+}$</td>
<td style="text-align: center;">83.4</td>
<td style="text-align: center;">12.0</td>
<td style="text-align: center;">81.6</td>
<td style="text-align: center;">12.0</td>
<td style="text-align: center;">77.6</td>
<td style="text-align: center;">10.6</td>
<td style="text-align: center;">81.1</td>
<td style="text-align: center;">11.7</td>
<td style="text-align: center;">81.3</td>
<td style="text-align: center;">11.9</td>
<td style="text-align: center;">79.6</td>
<td style="text-align: center;">11.9</td>
<td style="text-align: center;">73.6</td>
<td style="text-align: center;">10.6</td>
<td style="text-align: center;">78.8</td>
<td style="text-align: center;">11.7</td>
</tr>
<tr>
<td style="text-align: center;">OSCAR ${ }_{L}{ }^{+}$</td>
<td style="text-align: center;">85.4</td>
<td style="text-align: center;">11.9</td>
<td style="text-align: center;">84.0</td>
<td style="text-align: center;">11.7</td>
<td style="text-align: center;">80.3</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">83.4</td>
<td style="text-align: center;">11.4</td>
<td style="text-align: center;">84.8</td>
<td style="text-align: center;">12.1</td>
<td style="text-align: center;">82.1</td>
<td style="text-align: center;">11.5</td>
<td style="text-align: center;">73.8</td>
<td style="text-align: center;">9.7</td>
<td style="text-align: center;">80.9</td>
<td style="text-align: center;">11.3</td>
</tr>
<tr>
<td style="text-align: center;">Human [1]</td>
<td style="text-align: center;">84.4</td>
<td style="text-align: center;">14.3</td>
<td style="text-align: center;">85.0</td>
<td style="text-align: center;">14.3</td>
<td style="text-align: center;">95.7</td>
<td style="text-align: center;">14.0</td>
<td style="text-align: center;">87.1</td>
<td style="text-align: center;">14.2</td>
<td style="text-align: center;">80.6</td>
<td style="text-align: center;">15.0</td>
<td style="text-align: center;">84.6</td>
<td style="text-align: center;">14.7</td>
<td style="text-align: center;">91.6</td>
<td style="text-align: center;">14.2</td>
<td style="text-align: center;">85.3</td>
<td style="text-align: center;">14.6</td>
</tr>
<tr>
<td style="text-align: center;">VIVO* [9]</td>
<td style="text-align: center;">92.2</td>
<td style="text-align: center;">12.9</td>
<td style="text-align: center;">87.8</td>
<td style="text-align: center;">12.6</td>
<td style="text-align: center;">87.5</td>
<td style="text-align: center;">11.5</td>
<td style="text-align: center;">88.3</td>
<td style="text-align: center;">12.4</td>
<td style="text-align: center;">89.0</td>
<td style="text-align: center;">12.9</td>
<td style="text-align: center;">87.8</td>
<td style="text-align: center;">12.6</td>
<td style="text-align: center;">80.1</td>
<td style="text-align: center;">11.1</td>
<td style="text-align: center;">86.6</td>
<td style="text-align: center;">12.4</td>
</tr>
<tr>
<td style="text-align: center;">VinVL*</td>
<td style="text-align: center;">96.8</td>
<td style="text-align: center;">13.5</td>
<td style="text-align: center;">90.7</td>
<td style="text-align: center;">13.1</td>
<td style="text-align: center;">87.4</td>
<td style="text-align: center;">11.6</td>
<td style="text-align: center;">90.9</td>
<td style="text-align: center;">12.8</td>
<td style="text-align: center;">93.8</td>
<td style="text-align: center;">13.3</td>
<td style="text-align: center;">89.0</td>
<td style="text-align: center;">12.8</td>
<td style="text-align: center;">66.1</td>
<td style="text-align: center;">10.9</td>
<td style="text-align: center;">85.5</td>
<td style="text-align: center;">12.5</td>
</tr>
<tr>
<td style="text-align: center;">VinVL+VIVO</td>
<td style="text-align: center;">103.7</td>
<td style="text-align: center;">13.7</td>
<td style="text-align: center;">95.6</td>
<td style="text-align: center;">13.4</td>
<td style="text-align: center;">83.8</td>
<td style="text-align: center;">11.9</td>
<td style="text-align: center;">94.3</td>
<td style="text-align: center;">13.1</td>
<td style="text-align: center;">98.0</td>
<td style="text-align: center;">13.6</td>
<td style="text-align: center;">95.2</td>
<td style="text-align: center;">13.4</td>
<td style="text-align: center;">78.0</td>
<td style="text-align: center;">11.5</td>
<td style="text-align: center;">92.5</td>
<td style="text-align: center;">13.1</td>
</tr>
</tbody>
</table>
<p>Table 9: NoCaps evaluation results. All the models are trained on COCO without additional image-caption pairs following the restriction of NoCaps. (UpDown ${ }^{+}$is UpDown+ELMo+CBS, the models with * is $+\mathrm{SCST}+\mathrm{CBS}$, VinVL+VIVO is with SCST only.)</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method $\downarrow$</th>
<th style="text-align: center;">BERT</th>
<th style="text-align: center;">1K Test Set</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">5K Test Set</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Text Retrieval</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Image Retrieval</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Text Retrieval</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Image Retrieval</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">R@1</td>
<td style="text-align: center;">R@5</td>
<td style="text-align: center;">R@10</td>
<td style="text-align: center;">R@1</td>
<td style="text-align: center;">R@5</td>
<td style="text-align: center;">R@10</td>
<td style="text-align: center;">R@1</td>
<td style="text-align: center;">R@5</td>
<td style="text-align: center;">R@10</td>
<td style="text-align: center;">R@1</td>
</tr>
<tr>
<td style="text-align: center;">Unicoder-VL [19]</td>
<td style="text-align: center;">B</td>
<td style="text-align: center;">84.3</td>
<td style="text-align: center;">97.3</td>
<td style="text-align: center;">99.3</td>
<td style="text-align: center;">69.7</td>
<td style="text-align: center;">93.5</td>
<td style="text-align: center;">97.2</td>
<td style="text-align: center;">62.3</td>
<td style="text-align: center;">87.1</td>
<td style="text-align: center;">92.8</td>
<td style="text-align: center;">46.7</td>
</tr>
<tr>
<td style="text-align: center;">UNITER [4]</td>
<td style="text-align: center;">B</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">63.3</td>
<td style="text-align: center;">87.0</td>
<td style="text-align: center;">93.1</td>
<td style="text-align: center;">48.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">L</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">66.6</td>
<td style="text-align: center;">89.4</td>
<td style="text-align: center;">94.3</td>
<td style="text-align: center;">51.7</td>
</tr>
<tr>
<td style="text-align: center;">OSCAR</td>
<td style="text-align: center;">B</td>
<td style="text-align: center;">88.4</td>
<td style="text-align: center;">99.1</td>
<td style="text-align: center;">99.8</td>
<td style="text-align: center;">75.7</td>
<td style="text-align: center;">95.2</td>
<td style="text-align: center;">98.3</td>
<td style="text-align: center;">70.0</td>
<td style="text-align: center;">91.1</td>
<td style="text-align: center;">95.5</td>
<td style="text-align: center;">54.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">L</td>
<td style="text-align: center;">89.8</td>
<td style="text-align: center;">98.8</td>
<td style="text-align: center;">99.7</td>
<td style="text-align: center;">78.2</td>
<td style="text-align: center;">95.8</td>
<td style="text-align: center;">98.3</td>
<td style="text-align: center;">73.5</td>
<td style="text-align: center;">92.2</td>
<td style="text-align: center;">96.0</td>
<td style="text-align: center;">57.5</td>
</tr>
<tr>
<td style="text-align: center;">OSCAR+ w/ VinVL</td>
<td style="text-align: center;">B</td>
<td style="text-align: center;">89.8</td>
<td style="text-align: center;">98.8</td>
<td style="text-align: center;">99.7</td>
<td style="text-align: center;">78.2</td>
<td style="text-align: center;">95.6</td>
<td style="text-align: center;">98.0</td>
<td style="text-align: center;">74.6</td>
<td style="text-align: center;">92.6</td>
<td style="text-align: center;">96.3</td>
<td style="text-align: center;">58.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">L</td>
<td style="text-align: center;">90.8</td>
<td style="text-align: center;">99.0</td>
<td style="text-align: center;">99.8</td>
<td style="text-align: center;">78.8</td>
<td style="text-align: center;">96.1</td>
<td style="text-align: center;">98.5</td>
<td style="text-align: center;">75.4</td>
<td style="text-align: center;">92.9</td>
<td style="text-align: center;">96.2</td>
<td style="text-align: center;">58.8</td>
</tr>
</tbody>
</table>
<p>Table 10: Text and Image retrieval evaluation on the COCO 1 K and 5 K test sets. (B for Base, L for Large)</p>
<p>| Method | MAC | VisualBERT <br> base | LXMERT <br> base | 12-in-1 <br> base | UNITER <br> base |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  | </p>
<table>
<thead>
<tr>
<th style="text-align: center;">vision</th>
<th style="text-align: center;">no VLP</th>
<th style="text-align: center;">$\underset{[21]}{\operatorname{OSCAR}_{B}}$</th>
<th style="text-align: center;">$\begin{gathered} \operatorname{OSCAR}+_{\mathrm{B}} \ \text { (ours) } \end{gathered}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">R101-C4 [2]</td>
<td style="text-align: center;">$68.52 \pm 0.11$</td>
<td style="text-align: center;">72.38</td>
<td style="text-align: center;">$72.46 \pm 0.05$</td>
</tr>
<tr>
<td style="text-align: center;">VinVL (ours)</td>
<td style="text-align: center;">$71.34 \pm 0.17$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$74.90 \pm 0.05$</td>
</tr>
</tbody>
</table>
<p>Table 12: Effects of vision (V) and vision-language (VL) pre-training on VQA.
reports the results on 40 K images, with 5 reference captions (c5) and 40 reference captions (c40) per image. At the time of submitting this paper, our single model achieves No. 1 on the entire leaderboard, outperforming all 263 models, including many ensemble (and anonymous) models. (iv) The Novel Object Captioning (NoCaps) results are shown in Table 9. Without any VLP, i.e. by directly training a BERT-based captioning model on COCO, the model with our new visual features (denoted as VinVL) already surpasses the human performance in CIDEr ${ }^{13}$. By adding VIVO [9] pre-training, our VinVL improves the original VIVO result by 6 CIDEr points and creates a new SoTA. (v) Overall, on all these tasks (VQA in Table 5, Image Captioning in Table 7, NoCaps in Table 9, Image-Text Retrieval in Table 10, NLVR2 in Table 11), we show that $\operatorname{OSCAR}+<em L="L">{B}$ can match or outperform previous SoTA large models, and OSCAR $+</em>$ substantially uplifts the SoTA.</p>
<h1>5.2 Ablation Analysis</h1>
<p>We select the VQA task for the ablation study because its evaluation metric is well-defined and the task has been used as a testbed for all VLP models. To assist our analysis, we create a local validation set, vqa-dev, out of the standard validation set to select the best model during training for evaluation. vqa-dev contains randomly sampled 2 K images and their corresponding questions, amounting to 10.4 K image-QA pairs in total. Except for Table 4 and 5, all our VQA results are reported on this vqa-dev set. Unless otherwise specified, the reported STD is half of the difference of two runs of the VQA training with different random seeds.</p>
<p>In VQA, the VL model $y=\mathbf{V L}(\boldsymbol{w}, \boldsymbol{q}, \boldsymbol{v})$ has $\boldsymbol{w}$ as the question and $y$ as the answer. We focus on studying the effect of visual features $\boldsymbol{v}$ produced by different Vision models Vision $(\operatorname{Img})$ to better understand their relative contribution in the VQA performance. To eliminate the impact of using different tags $\boldsymbol{q}$, we use the same tags in the VQA models of OSCAR [21]. All the ablation experiments are conducted using models of the BERT-base size.</p>
<p>How much do the V and VL matter to the SoTA? Table 12 shows the VQA results with different vision models, i.e., R101-C4 model from [2] and our X152-C4 model pre-trained with 4 datasets (VinVL), and with different VLP methods, i.e., no VLP, OSCAR [21] and our OSCAR+. Taking the OSCAR ${ }<em B="B">{B}$ model with R101C4 features as the baseline, the OSCAR $+</em>$ model with our X152-C4 features improves the absolute accuracy from 72.38 to 74.90 , in which the OSCAR+ pre-training contributes $5 \%$ of the gain (i.e., $72.38 \rightarrow 72.46$ ) and the vision pre-training (improved visual features) $95 \%$ (i.e., $72.46 \rightarrow 74.90$ ). This demonstrates that vision representations matter significantly in VLP and downstream tasks.</p>
<p>Taking the "no VLP" model with R101-C4 features as the baseline, Table 12 shows that the gains of $\operatorname{VinVL}(71.34-68.52=2.82)$ and $\operatorname{VLP}(72.46-68.52=3.94)$ are additive $(74.90-68.52 \approx 2.82+3.94)$. This is intuitive because vision pre-training and VLP improve the Vision model Vision $(\operatorname{Img})$ and VL</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">$\frac{\text { model }}{\text { data }}$</th>
<th style="text-align: center;">R50-FPN</th>
<th style="text-align: center;">R50-C4</th>
<th style="text-align: center;">R101-C4 [2]</th>
<th style="text-align: center;">X152-C4</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">VG</td>
<td style="text-align: center;">$67.35 \pm 0.26$</td>
<td style="text-align: center;">$67.86 \pm 0.31$</td>
<td style="text-align: center;">$68.52 \pm 0.11$</td>
<td style="text-align: center;">$69.10 \pm 0.06$</td>
</tr>
<tr>
<td style="text-align: center;">4Sets $\rightarrow$ VG</td>
<td style="text-align: center;">$68.3 \pm 0.11$</td>
<td style="text-align: center;">$68.39 \pm 0.16$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$71.34 \pm 0.17$</td>
</tr>
</tbody>
</table>
<p>Table 13: Ablation of model size and data size on training vision models.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model <br> Pre-training dataset</th>
<th style="text-align: center;">R50-FPN</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">R50-C4</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">X152-C4</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ImageNet</td>
<td style="text-align: center;">4Sets</td>
<td style="text-align: center;">ImageNet</td>
<td style="text-align: center;">4Sets</td>
<td style="text-align: center;">ImageNet5k</td>
<td style="text-align: center;">4Sets</td>
</tr>
<tr>
<td style="text-align: center;">COCO $m A P$</td>
<td style="text-align: center;">40.2 [40]</td>
<td style="text-align: center;">$44.78^{*}$</td>
<td style="text-align: center;">38.4 [40]</td>
<td style="text-align: center;">42.4</td>
<td style="text-align: center;">42.17</td>
<td style="text-align: center;">50.51</td>
</tr>
<tr>
<td style="text-align: center;">VG obj $m A P^{50}$</td>
<td style="text-align: center;">9.6</td>
<td style="text-align: center;">11.3</td>
<td style="text-align: center;">9.6</td>
<td style="text-align: center;">12.1</td>
<td style="text-align: center;">11.2</td>
<td style="text-align: center;">13.8</td>
</tr>
<tr>
<td style="text-align: center;">attr $m A P$ with gt boxes</td>
<td style="text-align: center;">5.4</td>
<td style="text-align: center;">5.5</td>
<td style="text-align: center;">6.3</td>
<td style="text-align: center;">6.1</td>
<td style="text-align: center;">6.6</td>
<td style="text-align: center;">7.1</td>
</tr>
</tbody>
</table>
<p><sup id="fnref8:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 14: Effect of vision pre-training on object detection tasks.
model $\mathbf{V L}(\boldsymbol{w}, \boldsymbol{q}, \boldsymbol{v})$ separately. This also indicates that our pre-trained vision model can be utilized in any VL models by directly replacing their vision models, such as R101-C4 [2], with ours.</p>
<p>How much do data and model sizes matter to the new vision model? The improvement of VQA from R101-C4 [2] to VinVL (ours) in Table 12 is a compound effect of increasing model size (from R101-C4 to X152-C4) and data size (from VG to our merged four OD datasets). Table 13 shows the ablation of the two factors without VLP. Although VG's large object and attribute vocabulary allows to learn rich semantic concepts, VG does not contain large amounts of annotations for effective training of deep models. Vision models trained using the merged four OD datasets perform much better than VG-only-trained models, and the improvement is larger with the increase of the model size. ${ }^{14}$</p>
<p>How much does OD model architecture matter? The choice of model architecture affects the VQA performance. Table 13 shows that R50-FPN under-performs R50-C5 when they are trained only on VG; but the performance gap diminishes when both are trained on the merged dataset (4Sets). A detailed comparison between FPN and C4 architectures is presented in Appendix E.</p>
<p>How much does OD pre-training matter for object detection tasks? Table 14 presents the object detection results on COCO and the object-attribute detection results on VG (1594 object classes, 524 attribute classes). The results show that OD pre-training benefits the object detection tasks. Note that the mAP on VG is much lower than that on typical OD datasets (such as COCO) due to two reasons: (1) VG contains a large number of object classes with limited and extremely unbalanced annotations, (2) there are many missing annotations in the VG evaluation data. ${ }^{15}$ Although the mAP numbers are low, the detection result using X152-C4 is reasonably good; see Appendix A for more visualizations. We also see that FPN models</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset name</th>
<th style="text-align: center;">ImageNet</th>
<th style="text-align: center;">VG-obj</th>
<th style="text-align: center;">VG w/o attr</th>
<th style="text-align: center;">VG [2]</th>
<th style="text-align: center;">VG</th>
<th style="text-align: center;">4Sets $\rightarrow$ VG</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">#obj \&amp; #attr</td>
<td style="text-align: center;">$1000 \&amp; 0$</td>
<td style="text-align: center;">$317 \&amp; 0$</td>
<td style="text-align: center;">$1594 \&amp; 0$</td>
<td style="text-align: center;">$1600 \&amp; 400$</td>
<td style="text-align: center;">$1594 \&amp; 524$</td>
<td style="text-align: center;">$1848 \&amp; 524$</td>
</tr>
<tr>
<td style="text-align: center;">R50-C4 + BERT $B$</td>
<td style="text-align: center;">$66.13 \pm 0.04$</td>
<td style="text-align: center;">$64.25 \pm 0.16$</td>
<td style="text-align: center;">$66.51 \pm 0.11$</td>
<td style="text-align: center;">$67.63 \pm 0.25$</td>
<td style="text-align: center;">$67.86 \pm 0.31$</td>
<td style="text-align: center;">$68.39 \pm 0.16$</td>
</tr>
</tbody>
</table>
<p>Table 15: Effect of object-attribute vocabulary. We use all grid features (maximal 273) for the ImageNet classification model (first column), and maximal 50 region features for OD models (other columns).
perform consistently worse in attribute detection than C4 models, neither do FPN models show any advantage in object detection on VG. This contributes to the inferior performance of FPN, compared to C4, on downstream VL tasks, as discussed in Section 2.1.</p>
<p>How much does the diversity of visual concepts, i.e., object and attribute vocabularies, matter? We directly train vision models on different datasets, including (1) standard ImageNet with 1 K classes (ImageNet), (2) Visual Genome with 317 object classes (VG-obj) that are shared with COCO 80 classes and OpenImagesV5 500 classes, (3) VG with all 1594 object classes (VG w/o attr), (4) VG with 1594 object classes and 524 attribute classes (VG), and (5) the merged OD dataset (4Sets) for pre-training and VG for fine-tuning. For all the OD models (the last four columns in Table 15), we initialize the OD training with an ImageNet-pre-trained classification model, and use maximal 50 region features per image as input to the VL fusion module. For the ImageNet pre-trained classification model (the second column in Table 15), we use all the grid features (maximal 273) for each image ${ }^{16}$. The results show that</p>
<ul>
<li>In general, vocabularies with richer objects lead to better VQA results: VG-obj $&lt;$ ImageNet $&lt;$ VG w/o attr. The VG-obj vocabulary contains 79 of 80 COCO classes (only missing potted plant) and 313 of 500 OpenImagesV5 classes, and is a good approximation of common object classes of typical OD tasks. However, our results show that this vocabulary is not rich enough for VL tasks because it misses many important visual concepts (e.g., sky, water, mountain, etc.) which are crucial for VL tasks, as also illustrated by the comparison of detected regions in Figure 1. ${ }^{17}$.</li>
<li>Attribute information is crucial to VL tasks: models trained with attributes (VG and 4Sets $\rightarrow$ VG) are significantly better than those without attributes.</li>
<li>Even for the small vision model R50-C4, vision pre-training improves visual features for VQA, i.e., 4 Sets $\rightarrow$ VG is the best performer.</li>
</ul>
<p>In Table 16, we use different kinds of region proposals to extract image features. COCO groundtruth object regions (GT-Obj, 80 classes) and object-stuff regions (GT-Obj\&amp;Stuff, 171 classes) are perfect in terms of localization, but their vocabulary sizes are limited. Regions proposed by VG-trained models ([2] and VinVL) are imperfect in localization but using a larger vocabulary. For the VQA task, COCO GT boxes are much worse than the proposals generated by VG-trained models. The result demonstrates the difference between the typical OD tasks and the OD tasks in VL: OD in VL requires much richer visual semantics to align with the rich semantics in the language modality. This further echoes our claim that an image understanding module trained using richer vocabularies performs better for VL tasks.</p>
<p><sup id="fnref9:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">region</th>
<th style="text-align: center;">GT-Obj</th>
<th style="text-align: center;">GT-Obj\&amp;Stuff</th>
<th style="text-align: center;">Anderson <br> et al. [2]</th>
<th style="text-align: center;">VinVL (ours)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">model</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Anderson</td>
<td style="text-align: center;">$63.81 \pm 0.94$</td>
<td style="text-align: center;">$66.68 \pm 0.16$</td>
<td style="text-align: center;">$68.52 \pm 0.11$</td>
<td style="text-align: center;">$69.05 \pm 0.06$</td>
</tr>
<tr>
<td style="text-align: center;">et al. [2]</td>
<td style="text-align: center;">$65.60 \pm 0.21$</td>
<td style="text-align: center;">$68.13 \pm 0.26$</td>
<td style="text-align: center;">$70.25 \pm 0.05$</td>
<td style="text-align: center;">$71.34 \pm 0.17$</td>
</tr>
</tbody>
</table>
<p>Table 16: Effect of different region proposals on VQA.</p>
<h1>6 Conclusion</h1>
<p>In this paper we have presented a new recipe to pre-train an OD model for VL tasks. Compared to the most widely used bottom-up and top-down model [2], the new model is bigger, better-designed for VL tasks, and pre-trained on much larger text-image corpora, and thus can generate visual features for a richer collection of visual objects and concepts that are crucial for VL tasks. We validate the new model via a comprehensive empirical study where we feed the visual features to a VL fusion model which is pre-trained on a large-scale paired text-image corpus and then fine-tuned on seven VL tasks. Our results show that the new OD model can substantially uplift the SoTA results on all seven VL tasks across multiple public benchmarks. Our ablation study shows that the improvement is mainly attributed to our design choices regarding diversity of object categories, visual attribute training, training data scale, model size, and model architecture.</p>
<h2>Acknowledgement</h2>
<p>We thank Xi Yin for her contributions to this project while she was in Microsoft. We thank Xiyang Dai for his conjecture that C 4 arch is better than FPN because C 4 arch makes better use of ImageNet initialization weights.</p>
<h2>References</h2>
<p>[1] Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen, Rishabh Jain, Mark Johnson, Dhruv Batra, Devi Parikh, Stefan Lee, and Peter Anderson. nocaps: novel object captioning at scale. In ICCV, 2019. 3, 8, 10, 23
[2] Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei Zhang. Bottom-up and top-down attention for image captioning and visual question answering. In CVPR, 2018. 1, 2, 3, $5,7,10,11,12,13,14,17,18,20,21,22,23,26,28,30$
[3] Wenhu Chen, Zhe Gan, Linjie Li, Yu Cheng, William Wang, and Jingjing Liu. Meta module network for compositional visual reasoning. arXiv preprint arXiv:1910.03230, 2019. 9, 22
[4] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. Uniter: Learning universal image-text representations. arXiv preprint arXiv:1909.11740, 2019. 1, 10, 24
[5] Fartash Faghri, David J Fleet, Jamie Ryan Kiros, and Sanja Fidler. Vse++: Improved visual-semantic embeddings. arXiv preprint arXiv:1707.05612, 2(7):8, 2017. 24
[6] Hao Fang, Saurabh Gupta, Forrest Iandola, Rupesh K Srivastava, Li Deng, Piotr Dollár, Jianfeng Gao, Xiaodong He, Margaret Mitchell, John C Platt, et al. From captions to visual concepts and back. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1473-1482, 2015. 6
[7] Zhe Gan, Yen-Chun Chen, Linjie Li, Chen Zhu, Yu Cheng, and Jingjing Liu. Large-scale adversarial training for vision-and-language representation learning. arXiv preprint arXiv:2006.06195, 2020. 9
[8] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the V in VQA matter: Elevating the role of image understanding in visual question answering. In CVPR, 2017. 3, 6, 7, 22</p>
<p>[9] Xiaowei Hu, Xi Yin, Kevin Lin, Lijuan Wang, Lei Zhang, Jianfeng Gao, and Zicheng Liu. Vivo: Surpassing human performance in novel object captioning with visual vocabulary pre-training. arXiv preprint arXiv:2009.13682, 2020. 2, 8, 9, 10, 11
[10] Lun Huang, Wenmin Wang, Jie Chen, and Xiao-Yong Wei. Attention on attention for image captioning. In ICCV, 2019. 10, 23
[11] Zhicheng Huang, Zhaoyang Zeng, Bei Liu, Dongmei Fu, and Jianlong Fu. Pixel-bert: Aligning image pixels with text by deep multi-modal transformers. arXiv preprint arXiv:2004.00849, 2020. 13
[12] Drew Hudson and Christopher D Manning. Learning by abstraction: The neural state machine. In NeurIPS, 2019. 3, 9
[13] Drew A Hudson and Christopher D Manning. GQA: A new dataset for real-world visual reasoning and compositional question answering. arXiv preprint arXiv:1902.09506, 2019. 3, 6, 7, 22
[14] Huaizu Jiang, Ishan Misra, Marcus Rohrbach, Erik Learned-Miller, and Xinlei Chen. In defense of grid features for visual question answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10267-10276, 2020. 5, 26
[15] Andrej Karpathy and Li Fei-Fei. Deep visual-semantic alignments for generating image descriptions. In CVPR, 2015. 23, 24
[16] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. International Journal of Computer Vision, 123(1):32-73, 2017. 1, 2
[17] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Tom Duerig, et al. The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale. arXiv preprint arXiv:1811.00982, 2018. 1, 8, 23
[18] Kuang-Huei Lee, Xi Chen, Gang Hua, Houdong Hu, and Xiaodong He. Stacked cross attention for image-text matching. In ECCV, 2018. 24
[19] Gen Li, Nan Duan, Yuejian Fang, Daxin Jiang, and Ming Zhou. Unicoder-VL: A universal encoder for vision and language by cross-modal pre-training. arXiv preprint arXiv:1908.06066, 2019. 1, 8, 10, 23, 24
[20] Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. Visualbert: A simple and performant baseline for vision and language. arXiv preprint arXiv:1908.03557, 2019. 1
[21] Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, Yejin Choi, and Jianfeng Gao. Oscar: Object-semantics aligned pre-training for vision-language tasks. In European Conference on Computer Vision, pages 121-137. Springer, 2020. 1, 2, 3, 6, 8, 9, 10, 11, 19, 20, 22, 23, 27
[22] Junyang Lin, An Yang, Yichang Zhang, Jie Liu, Jingren Zhou, and Hongxia Yang. Interbert: Vision-andlanguage interaction for multi-modal pretraining. arXiv preprint arXiv:2003.13198, 2020. 9
[23] Tsung-Yi Lin, Piotr Dollár, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages $2117-2125,2017.3,5$
[24] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár. Focal loss for dense object detection. In Proceedings of the IEEE international conference on computer vision, pages 2980-2988, 2017. 3
[25] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. Microsoft COCO: Common objects in context. In ECCV, 2014. 1, 3, 6, 8, 22, 23
[26] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. VilBERT: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. In NeurIPS, 2019. 1
[27] Jiasen Lu, Vedanuj Goswami, Marcus Rohrbach, Devi Parikh, and Stefan Lee. 12-in-1: Multi-Task vision and language representation learning. arXiv preprint arXiv:1912.02315, 2019. 24
[28] Vicente Ordonez, Girish Kulkarni, and Tamara L Berg. Im2text: Describing images using 1 million captioned photographs. In NeurIPS, 2011. 6
[29] Yingwei Pan, Ting Yao, Yehao Li, and Tao Mei. X-linear attention networks for image captioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10971-10980, 2020. 10</p>
<p>[30] Steven J Rennie, Etienne Marcheret, Youssef Mroueh, Jerret Ross, and Vaibhava Goel. Self-critical sequence training for image captioning. In CVPR, 2017. 23
[31] Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, and Jian Sun. Objects365: A large-scale, high-quality dataset for object detection. In Proceedings of the IEEE international conference on computer vision, pages 8430-8439, 2019. 1, 12
[32] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Annual Meeting of the Association for Computational Linguistics, 2018. 6, 23
[33] Botian Shi, Lei Ji, Pan Lu, Zhendong Niu, and Nan Duan. Knowledge aware semantic concept expansion for image-text matching. In IJCAI, 2019. 24
[34] Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai. VL-BERT: Pre-training of generic visual-linguistic representations. arXiv preprint arXiv:1908.08530, 2019. 1
[35] Alane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang, Huajun Bai, and Yoav Artzi. A corpus for reasoning about natural language grounded in photographs. arXiv preprint arXiv:1811.00491, 2018. 3, 8, 24
[36] Hao Tan and Mohit Bansal. LXMERT: Learning cross-modality encoder representations from transformers. EMNLP, 2019. 1
[37] Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. Fcos: A simple and strong anchor-free object detector. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020. 3
[38] Yaxiong Wang, Hao Yang, Xueming Qian, Lin Ma, Jing Lu, Biao Li, and Xin Fan. Position focused attention network for image-text matching. arXiv preprint arXiv:1907.09748, 2019. 24
[39] Zihao Wang, Xihui Liu, Hongsheng Li, Lu Sheng, Junjie Yan, Xiaogang Wang, and Jing Shao. CAMP: CrossModal adaptive message passing for text-image retrieval. In ICCV, 2019. 24
[40] Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen Lo, and Ross Girshick. Detectron2. https:// github.com/facebookresearch/detectron2, 2019. 5, 12, 29
[41] Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, and Alex Smola. Stacked attention networks for image question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 21-29, 2016. 6
[42] Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics, 2:67-78, 2014. 6
[43] Fei Yu, Jiji Tang, Weichong Yin, Yu Sun, Hao Tian, Hua Wu, and Haifeng Wang. Ernie-vil: Knowledge enhanced vision-language representations through scene graph. arXiv preprint arXiv:2006.16934, 2020. 9
[44] Zhedong Zheng, Liang Zheng, Michael Garrett, Yi Yang, and Yi-Dong Shen. Dual-path convolutional image-text embedding with instance loss. arXiv preprint arXiv:1711.05535, 2017. 24
[45] Luowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu, Jason J Corso, and Jianfeng Gao. Unified visionlanguage pre-training for image captioning and VQA. AAAI, 2020. 1, 8, 10, 22, 23</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Predictions from X152-FPN trained on OpenImages. Test image: COCO_test2015_000000028839</p>
<h1>A Qualitative study of three pre-trained vision models</h1>
<p>We apply three (pre-trained) object detection models on the image in Figure 1 and list their detection results for a more detailed comparison.</p>
<p>Detections from X152-FPN trained on Open Images V5. See Figure 2:
Surfboard; Surfboard; Surfboard; Surfboard; Man; Human leg; Human leg; Swimwear; Swimwear; Shorts; Shorts; Boy; Human arm.</p>
<p>Detections from R101-C4 trained on VG by Anderson et al. [2]. There are obviously wrong detections, marked in red. See Figure 3 (top):
black shorts; young, shirtless, standing, barefoot, surfing, little, playing boy; shirtless, standing, barefoot, walking, wet, surfing, young man; tan, bare, shirtless back; blue, clear, cloudy, hazy, light blue sky; young, shirtless, standing, surfing, barefoot, little boy; brown, short, wet, blond hair; brown, short, wet, blond hair; small, crashing wave; white, wet surfboard; white, crashing, big, rolling wave; wet, tan surfboard; green, blue fin; blue, calm, choppy, wavy, ocean, splashing, foamy, water, rough, sandy, wet ocean; wet, calm, sandy, splashing, wavy water; white, wet surfboard; bare, wet foot; blue, colorful, multi colored, floral shorts; calm, choppy, water, rough,</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Predictions from R101-C4 trained on VG from [2] (top), X152-C4 pre-trained on 4 OD datasets and finetuned on VG (bottom). Test image: COCO_test2015_000000028839</p>
<p>foamy, wavy water; distant, rocky, hazy mountains; standing, shirtless, young, barefoot, wet, surfing, walking, smiling boy; calm ocean; distant, rocky mountain; white, bare, wet surfboard; wet, sandy, calm, tan beach; gray, big rock; blue, calm background; wet, brown, tan, sandy sand; wet shadow; blue, colorful, floral, multi colored swim trunks; yellow, plastic hand.</p>
<p>Detections from our pre-trained X152-C4 model pre-trained on four datasets and fine-tuned on VG. There are some repetitive detections, but no obvious wrong detections. See Figure 3 (bottom):
blue, green fin; young, barefoot, shirtless, standing, surfing, smiling, little, playing, looking, blond boy; young, barefoot, standing, shirtless, smiling, surfing, blond, playing, looking, little, walking, riding boy; shirtless, barefoot, standing, young, smiling, surfing, walking, wet, playing man; bare, wet foot; black, white surfboard; small, large, white, crashing, big, water, rolling, splashing, rough, foamy wave; bare, wet foot; dark, black, wet, cast shadow; blue, clear, hazy, cloudy, cloudless sky; black, gray, white, raised surfboard; black, wet, short short; brown, short, blond, wet, curly, wavy hair; distant, brown, large, rocky, hazy, big mountain; brown, short, dark, blond, wet hair; blue, white, calm, wavy, choppy, ocean, splashing, water, rough, clear, shallow water; bare, tan, light, beige back; black, blue, wet surfboard; small, dark, water, crashing, rolling, splashing, big wave; wet, white, sandy, tan surfboard; blue, colorful, floral, multi colored, patterned trunk; wet, brown, sandy, tan sand; white, blue, calm, foamy, choppy, splashing, wavy, ocean, rough, water, clear, shallow water; wet, brown, sandy, calm, tan, shallow, smooth, muddy, rough beach; black, white, young board; shirtless, young, standing, barefoot, smiling, surfing, looking, walking, playing boy; blue, calm, choppy, wavy, ocean, clear, rough, splashing, water, foamy, shallow, rippled ocean; yellow, gold bracelet; white, silver, black logo; wet, bare, bent, tan, crossed, hairy, short, skinny, back, muscular, extended, outstretched leg; black, gray, white board; brown, distant, large, rocky, big hill; brown, short, blond, wet, curly head; red, black logo; bare, raised, extended, holding, open, up, bent, outstretched hand; black, wet swim trunks; bare, wet, bent, tan, crossed, skinny, short, back, muscular leg; wet, brown, muddy, sandy, tan, shallow reflection.</p>
<h1>B OSCAR+ pre-training</h1>
<h2>B. 1 Pre-training Corpus</h2>
<p>Table 17 shows the statistics of image and text of the pre-training corpora. In our ablation study, we use corpora of three different sizes: 'Small', 'Medium', 'Large'. Different from OSCAR [21], we make use of image tagging datasets OpenImages, by generating captions using OSCAR's image captioning model to form triplets of (generated caption, image tags, image features) for OSCAR+ pre-training. By self-training technique, our pre-training corpora can be scaled to a much larger amount by making use of large-scale image tagging datasets, e.g., OpenImages (9M) and YFCC (92M).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Small</th>
<th style="text-align: center;">0.22M Images, 2.5M QAs, 0.7M captions</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Medium</td>
<td style="text-align: center;">1.89M Images, 2.5M QAs, 0.7M captions, 1.67M pseudo-captions</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Large</td>
<td style="text-align: center;">5.65M Images, 2.5M QAs, 4.68M captions, 1.67M pseudo-captions</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Source</td>
<td style="text-align: center;">VQA <br> (train)</td>
<td style="text-align: center;">GQA <br> (bal-train)</td>
<td style="text-align: center;">VG-QA <br> (train)</td>
<td style="text-align: center;">COCO <br> (train)</td>
<td style="text-align: center;">Flicker30k <br> (train)</td>
<td style="text-align: center;">OpenImages <br> (od train)</td>
<td style="text-align: center;">CC <br> (train)</td>
<td style="text-align: center;">SBU <br> (all)</td>
</tr>
<tr>
<td style="text-align: center;">Image/Text</td>
<td style="text-align: center;">83k/545k</td>
<td style="text-align: center;">79k/1026k</td>
<td style="text-align: center;">87k/931k</td>
<td style="text-align: center;">112k/559k</td>
<td style="text-align: center;">29k/145k</td>
<td style="text-align: center;">1.67M/1.67M</td>
<td style="text-align: center;">3.1M/3.1M</td>
<td style="text-align: center;">875k/875k</td>
</tr>
<tr>
<td style="text-align: center;">$\boldsymbol{w}, \boldsymbol{q}, \boldsymbol{v}$</td>
<td style="text-align: center;">Question, Answer, ImageFeatures</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">(Generated) Caption, (Generated) ImageTags, ImageFeatures</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 17: Statistics of the pre-training corpus.</p>
<h1>B. 2 OSCAR+ pre-training objectives</h1>
<p>Masked Token Loss: A Loss Mimics Image Captioning. The word tokens of image captions (questions) $\boldsymbol{w}$ and word tokens of object tags (answers) $\boldsymbol{q}$ share the same linguistic semantic space, and the Masked Token Loss (MTL) is applied on tokens of both $\boldsymbol{w}$ and $\boldsymbol{q}$. We define the discrete token sequence as $\boldsymbol{h} \triangleq$ $[\boldsymbol{w}, \boldsymbol{q}]$, and apply the Masked Token Loss (MTL) for pre-training. At each iteration, we randomly mask each input token in $\boldsymbol{h}$ with probability $15 \%$, and replace the masked one $h_{i}$ with a special token [MASK]. The goal of training is to predict these masked tokens based on their surrounding tokens $\boldsymbol{h}_{\backslash i}$ and image features $\boldsymbol{v}$ by minimizing the negative log-likelihood:</p>
<p>$$
\mathcal{L}<em _boldsymbol_v="(\boldsymbol{v">{\mathrm{MTL}}=-\mathbb{E}</em>\right)
$$}, \boldsymbol{h}) \sim \mathcal{D}} \log p\left(h_{i} \mid \boldsymbol{h}_{\backslash i}, \boldsymbol{v</p>
<p>This is the same MTL as in OSCAR [21] and similar to the masked language model used by BERT. The masked word or tag needs to be recovered from its surrounding context, with additional image information to help ground the learned word embeddings in the vision context.</p>
<p>3-way Contrastive Loss: A Loss Mimics Text-Image Retrieval and Visual Question Answering Simultaneously. We present our 3-way contrastive loss in Section 3.2 in the main paper.</p>
<h2>B. 3 Ablation of the two new techniques</h2>
<p>Effect of self-training: Leveraging Image Tagging data. In Figure 4, we show the effect of self-training by making use of tagging data in OSCAR+, by fine-tuning OSCAR+ pre-training checkpoints on VQA. Compared with "OSCAR+, Small; VinVL" (green), "OSCAR+, Medium; VinVL" (yellow) adds the 1.7M OpenImages Tagging data into pre-training and its performance gets improved significantly, demonstrating the effect of self-training by making use of tagging data. As baselines, we also provide performance of OSCAR and OSCAR+ with image features from [2], which clearly demonstrates that the new image features pre-trained by VinVL matter significantly in the VL pre-training and VL downstream tasks.</p>
<p>Effect of the new 3-way contrastive loss. As illustrated in Table 3, with the new 3-way contrastive loss, the VQA performance is the same as the OSCAR pre-training, while the Text-Image Retrieval performance improves significantly compared with the OSCAR pre-training.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Effect of OSCAR+ pre-training corpus size and effect of self-training by making use of tagging data in OSCAR+. Each curve, with legend "VLP, Corpus; VisionFeature", denotes a VLP experiment where the VLP method is either OSCAR or OSCAR+, the VLP pre-training Corpus is Small/Medium/Large (defined in Table 17), and VisionFeature is either our new vision features (VinVL for short) or those from [2] ([2] for short). X-axis denotes the pre-training iterations of OSCAR+ checkpoints. Y -axix is the vqa-dev accuracy of a VQA model initialized from the corresponding pre-training checkpoint and fine-tuned with a fixed scheme. Compared with "OSCAR+, Small; VinVL" (green), "OSCAR+, Medium; VinVL" (yellow) adds the 1.7M OpenImages Tagging data into the pre-training and its performance gets improved significantly, demonstrating the effect of self-training by making use of tagging data. The "OSCAR+, Large; VinVL" (blue) further scales up the pre-training corpus by adding Google Conceptual Captions and SBU datasets with generated tags and its performance gets further improved, demonstrating the effect of OSCAR+ pre-training corpus size. As baselines, we also provide performance of OSCAR and OSCAR+ with image features from [2], which clearly demonstrates that our new image features (VinVL) matter significantly in the VL pre-training and VL downstream tasks.</p>
<p>Overall improvement from OSCAR to OSCAR+. We point out that the improvement from OSCAR to OSCAR+ with image features from [2] is minor, because (1) we only add 1.7M OpenImages' tagging data</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{16}$ Our use of grid feature follows PixelBert [11]. See Appendix F for details.
${ }^{17}$ Using the same training procedure on VG, we trained an R50-C4 model on the OpenImagesV5 dataset (500 classes). Using the region features produced by this model, the VQA performance is $63.55 \pm 0.14$. The result is slightly worse than that of VG-obj because both VG and VQA images are from the COCO dataset but OpenImages images are not.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref8:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref9:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{14}$ The R101-C4 model in Table 13 is exactly the VG-pre-pretrained model from [2]. We do not train this model on our merged OD dataset because this model architecture is old-fashioned and is slow to train.
${ }^{15}$ As a reference, the R101-C4 model from [2] on VG with 1600 objects and 400 attributes has mAP of 8.7/7.8 evaluated in our code, whereas it was reported as $10.2 / 7.8$ due to differences in OD evaluation pipeline.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
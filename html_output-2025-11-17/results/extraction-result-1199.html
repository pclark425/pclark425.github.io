<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1199 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1199</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1199</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-28.html">extraction-schema-28</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <p><strong>Paper ID:</strong> paper-259360658</p>
                <p><strong>Paper Title:</strong> <a href="https://www.aclanthology.org/2023.acl-long.373.pdf" target="_blank">Learning Symbolic Rules over Abstract Meaning Representations for Textual Reinforcement Learning</a></p>
                <p><strong>Paper Abstract:</strong> Text-based reinforcement learning agents have predominantly been neural network-based models with embeddings-based representation, learning uninterpretable policies that often do not generalize well to unseen games. On the other hand, neuro-symbolic methods, specifically those that leverage an intermediate formal representation, are gaining significant attention in language understanding tasks. This is because of their advantages ranging from inherent interpretability, the lesser requirement of training data, and being generalizable in scenarios with unseen data. Therefore, in this paper, we propose a modular, NEuro-Symbolic Textual Agent (NESTA) that combines a generic semantic parser with a rule induction system to learn abstract interpretable rules as policies. Our experiments on established text-based game benchmarks show that the proposed NESTA method outperforms deep reinforcement learning-based techniques by achieving better generalization to unseen test games and learning from fewer training interactions.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1199.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1199.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NESTA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NEuro-Symbolic Textual Agent</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A modular neuro-symbolic agent for text-based reinforcement learning that uses AMR-based semantic parsing to produce symbolic triples, an LNN/ILP rule learner to induce lifted (entity-abstract) action rules from reward, and a look-ahead pruner to reduce action branching.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>TextWorld Commonsense (TWC)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>A household 'cleanup' text-based game suite where the agent must place objects into correct containers; provides ConceptNet subgraphs as commonsense knowledge and has in-distribution and out-of-distribution splits (easy/medium: single-room; hard: two-room requiring inter-room object transport).</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Not specified as an explicit graph metric in the paper; environment described as small (single-room or two-room) partially-observable POMDPs with a large action-space branching factor (many admissible textual actions per state).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>Easy/Medium: single-room tasks; Hard: two-room tasks (exact number of rooms/nodes and edges not enumerated).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>NESTA (LNN/ILP + AMR + Pruner)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Semantic parser (AMR -> triples) produces symbolic facts; separate LNN-based ILP models learn lifted first-order rules per action predicate from trajectories and reward (policy is a weighted conjunction of template predicates); a look-ahead pruner removes non-contributing action verbs; outlier rejection via consensus subsets.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>Number of steps to reach goal; normalized score (total reward normalized by maximum reward); sample efficiency measured as training interactions required to reach a performance level.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td>Reported examples: In hard games (seed summary): learned rules (before human correction) -> in-distribution norm score 0.71 (Steps: 46.4), out-of-distribution norm score 0.85 (Steps: 37.4). From ablation tables: NESTA (hard) in-distribution: 43.44 steps, norm score 0.77; NESTA+ActionPruning: 35.84 steps, 0.85 score. Out-of-distribution: NESTA: 47.52 steps, 0.60; NESTA+AP: 31.40 steps, 0.91. Also reported that NESTA can outperform deep text agents when trained with up to 5x fewer interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Reported via normalized score (proxy): e.g., hard out-of-distribution after human rule correction reached normalized score 1.0 (Steps: 19.8); typical NESTA out-of-distribution scores reported around 0.60–0.91 depending on ablations and pruning.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Interpretable, lifted-rule policies (symbolic first-order Horn-clause-like rules learned by LNN/ILP) combined with pruning; these abstract (entity-agnostic) rules are reported as optimal for generalization across unseen entities.</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>Reported relationships: (1) Environment complexity (single-room vs two-room) increases required strategy complexity — two-room (hard) tasks require object transport across rooms and thus more complex multi-step policies. (2) Action-space branching (many admissible textual actions) harms exploration; look-ahead action pruning that reduces branching improves steps-to-goal and normalized score (greater improvement observed for NESTA than for a neural baseline). (3) Abstract lifted rules (entity-agnostic) improve generalization to out-of-distribution entities, improving success metrics relative to deep RL which overfits entity-specific patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td>Paper compares easy/medium (single-room) vs hard (two-room) games: NESTA outperforms baselines on all splits but relative gains are larger on hard (two-room) games, indicating better generalization in multi-room/navigation-required tasks. Action-pruning yields larger improvements in harder settings and for the symbolic agent than for the neural baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Policies composed of lifted symbolic rules (e.g., go(x) :- direction(x); take(x) :- be-located-at(x); put(x,y) :- carry(x) ∧ atlocation(x,y)) generalize to unseen entities and reduce required training samples; human-correctable rules can dramatically improve performance (e.g., correcting take(x,y) to require ¬atlocation(x,y) produced large gains). Pruning of non-state-changing actions (e.g., examine(x)) reduces branching and improves exploration efficiency. Overall, higher-diameter / multi-room tasks demand strategies that chain actions across states (memory or multi-step rules), which NESTA's rule structure can represent.</td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings_short</strong></td>
                            <td>Lifted-rule policies require representing multi-step relations (beneficial in multi-room tasks); pruning and rule abstraction reduce search/steps and improve generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Symbolic Rules over Abstract Meaning Representations for Textual Reinforcement Learning', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1199.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1199.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TWC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TextWorld Commonsense (TWC)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A benchmark suite of text-based 'cleanup' games for evaluating commonsense-aware agents; provides conceptnet subgraphs and two test splits (in-distribution vs out-of-distribution) to measure generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>TextWorld Commonsense (TWC)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Household/object cleanup text games where goal is to place objects into correct containers; includes commonsense graphs (ConceptNet subgraphs) and splits for testing generalization to unseen entity vocabularies/configurations.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Not expressed as explicit graph-topology metrics in the paper; tasks are small-scale (single-room or two-room) POMDPs with potential multi-step dependencies between actions (e.g., pick in room1 then move to room2 and put).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>Not enumerated; categorized by difficulty: easy/medium single-room; hard two-room. Number of objects/containers unspecified in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>various (used to evaluate agents)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Environment used to evaluate agents including NESTA, text-only GRU agents, TWCAgent (text + commonsense embeddings), KG-A2C, BiKE, and CBR.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>Number of steps to reach goal; normalized score; sample-efficiency (# training interactions).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>Paper uses TWC to show that (a) multi-room (hard) tasks require chaining actions across rooms, increasing difficulty; (b) agents that learn abstract lifted rules generalize better across entity distributions. Explicit graph-topology metrics (diameter, clustering) are not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td>Performance reported across easy/medium/hard splits: NESTA performs best across splits and shows largest relative improvements on hard (two-room) tasks, demonstrating that policy abstraction and pruning help with more topologically complex navigation-like tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>See NESTA entry: rules that abstract entities and represent multi-step relations improve generalization and reduce steps particularly in two-room scenarios; action pruning improves exploration efficiency by reducing branching factor.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Symbolic Rules over Abstract Meaning Representations for Textual Reinforcement Learning', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1199.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1199.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TextWorld</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TextWorld (general)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A general learning environment/framework for procedurally-generated text-based games used in research on text-adventure agents and exploration strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>TextWorld (general)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>A flexible text-game generation framework supporting many kinds of text-adventure tasks (various domains), commonly used in research on exploration and representation for text-based agents.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Symbolic Rules over Abstract Meaning Representations for Textual Reinforcement Learning', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Textworld: A learning environment for text-based games <em>(Rating: 2)</em></li>
                <li>Textworld commonsense <em>(Rating: 2)</em></li>
                <li>Playing text-adventure games with graph-based deep reinforcement learning <em>(Rating: 2)</em></li>
                <li>Graph constrained reinforcement learning for natural language action spaces <em>(Rating: 2)</em></li>
                <li>Learning dynamic belief graphs to generalize on text-based games <em>(Rating: 2)</em></li>
                <li>Counting to explore and generalize in text-based games <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1199",
    "paper_id": "paper-259360658",
    "extraction_schema_id": "extraction-schema-28",
    "extracted_data": [
        {
            "name_short": "NESTA",
            "name_full": "NEuro-Symbolic Textual Agent",
            "brief_description": "A modular neuro-symbolic agent for text-based reinforcement learning that uses AMR-based semantic parsing to produce symbolic triples, an LNN/ILP rule learner to induce lifted (entity-abstract) action rules from reward, and a look-ahead pruner to reduce action branching.",
            "citation_title": "here",
            "mention_or_use": "use",
            "environment_name": "TextWorld Commonsense (TWC)",
            "environment_description": "A household 'cleanup' text-based game suite where the agent must place objects into correct containers; provides ConceptNet subgraphs as commonsense knowledge and has in-distribution and out-of-distribution splits (easy/medium: single-room; hard: two-room requiring inter-room object transport).",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": null,
            "dead_ends_count": null,
            "door_constraints_present": null,
            "door_constraints_description": null,
            "graph_connectivity": "Not specified as an explicit graph metric in the paper; environment described as small (single-room or two-room) partially-observable POMDPs with a large action-space branching factor (many admissible textual actions per state).",
            "environment_size": "Easy/Medium: single-room tasks; Hard: two-room tasks (exact number of rooms/nodes and edges not enumerated).",
            "agent_name": "NESTA (LNN/ILP + AMR + Pruner)",
            "agent_description": "Semantic parser (AMR -&gt; triples) produces symbolic facts; separate LNN-based ILP models learn lifted first-order rules per action predicate from trajectories and reward (policy is a weighted conjunction of template predicates); a look-ahead pruner removes non-contributing action verbs; outlier rejection via consensus subsets.",
            "exploration_efficiency_metric": "Number of steps to reach goal; normalized score (total reward normalized by maximum reward); sample efficiency measured as training interactions required to reach a performance level.",
            "exploration_efficiency_value": "Reported examples: In hard games (seed summary): learned rules (before human correction) -&gt; in-distribution norm score 0.71 (Steps: 46.4), out-of-distribution norm score 0.85 (Steps: 37.4). From ablation tables: NESTA (hard) in-distribution: 43.44 steps, norm score 0.77; NESTA+ActionPruning: 35.84 steps, 0.85 score. Out-of-distribution: NESTA: 47.52 steps, 0.60; NESTA+AP: 31.40 steps, 0.91. Also reported that NESTA can outperform deep text agents when trained with up to 5x fewer interactions.",
            "success_rate": "Reported via normalized score (proxy): e.g., hard out-of-distribution after human rule correction reached normalized score 1.0 (Steps: 19.8); typical NESTA out-of-distribution scores reported around 0.60–0.91 depending on ablations and pruning.",
            "optimal_policy_type": "Interpretable, lifted-rule policies (symbolic first-order Horn-clause-like rules learned by LNN/ILP) combined with pruning; these abstract (entity-agnostic) rules are reported as optimal for generalization across unseen entities.",
            "topology_performance_relationship": "Reported relationships: (1) Environment complexity (single-room vs two-room) increases required strategy complexity — two-room (hard) tasks require object transport across rooms and thus more complex multi-step policies. (2) Action-space branching (many admissible textual actions) harms exploration; look-ahead action pruning that reduces branching improves steps-to-goal and normalized score (greater improvement observed for NESTA than for a neural baseline). (3) Abstract lifted rules (entity-agnostic) improve generalization to out-of-distribution entities, improving success metrics relative to deep RL which overfits entity-specific patterns.",
            "comparison_across_topologies": true,
            "topology_comparison_results": "Paper compares easy/medium (single-room) vs hard (two-room) games: NESTA outperforms baselines on all splits but relative gains are larger on hard (two-room) games, indicating better generalization in multi-room/navigation-required tasks. Action-pruning yields larger improvements in harder settings and for the symbolic agent than for the neural baseline.",
            "policy_structure_findings": "Policies composed of lifted symbolic rules (e.g., go(x) :- direction(x); take(x) :- be-located-at(x); put(x,y) :- carry(x) ∧ atlocation(x,y)) generalize to unseen entities and reduce required training samples; human-correctable rules can dramatically improve performance (e.g., correcting take(x,y) to require ¬atlocation(x,y) produced large gains). Pruning of non-state-changing actions (e.g., examine(x)) reduces branching and improves exploration efficiency. Overall, higher-diameter / multi-room tasks demand strategies that chain actions across states (memory or multi-step rules), which NESTA's rule structure can represent.",
            "policy_structure_findings_short": "Lifted-rule policies require representing multi-step relations (beneficial in multi-room tasks); pruning and rule abstraction reduce search/steps and improve generalization.",
            "uuid": "e1199.0",
            "source_info": {
                "paper_title": "Learning Symbolic Rules over Abstract Meaning Representations for Textual Reinforcement Learning",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "TWC",
            "name_full": "TextWorld Commonsense (TWC)",
            "brief_description": "A benchmark suite of text-based 'cleanup' games for evaluating commonsense-aware agents; provides conceptnet subgraphs and two test splits (in-distribution vs out-of-distribution) to measure generalization.",
            "citation_title": "",
            "mention_or_use": "use",
            "environment_name": "TextWorld Commonsense (TWC)",
            "environment_description": "Household/object cleanup text games where goal is to place objects into correct containers; includes commonsense graphs (ConceptNet subgraphs) and splits for testing generalization to unseen entity vocabularies/configurations.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": null,
            "dead_ends_count": null,
            "door_constraints_present": null,
            "door_constraints_description": null,
            "graph_connectivity": "Not expressed as explicit graph-topology metrics in the paper; tasks are small-scale (single-room or two-room) POMDPs with potential multi-step dependencies between actions (e.g., pick in room1 then move to room2 and put).",
            "environment_size": "Not enumerated; categorized by difficulty: easy/medium single-room; hard two-room. Number of objects/containers unspecified in paper.",
            "agent_name": "various (used to evaluate agents)",
            "agent_description": "Environment used to evaluate agents including NESTA, text-only GRU agents, TWCAgent (text + commonsense embeddings), KG-A2C, BiKE, and CBR.",
            "exploration_efficiency_metric": "Number of steps to reach goal; normalized score; sample-efficiency (# training interactions).",
            "exploration_efficiency_value": null,
            "success_rate": null,
            "optimal_policy_type": null,
            "topology_performance_relationship": "Paper uses TWC to show that (a) multi-room (hard) tasks require chaining actions across rooms, increasing difficulty; (b) agents that learn abstract lifted rules generalize better across entity distributions. Explicit graph-topology metrics (diameter, clustering) are not provided.",
            "comparison_across_topologies": true,
            "topology_comparison_results": "Performance reported across easy/medium/hard splits: NESTA performs best across splits and shows largest relative improvements on hard (two-room) tasks, demonstrating that policy abstraction and pruning help with more topologically complex navigation-like tasks.",
            "policy_structure_findings": "See NESTA entry: rules that abstract entities and represent multi-step relations improve generalization and reduce steps particularly in two-room scenarios; action pruning improves exploration efficiency by reducing branching factor.",
            "uuid": "e1199.1",
            "source_info": {
                "paper_title": "Learning Symbolic Rules over Abstract Meaning Representations for Textual Reinforcement Learning",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "TextWorld",
            "name_full": "TextWorld (general)",
            "brief_description": "A general learning environment/framework for procedurally-generated text-based games used in research on text-adventure agents and exploration strategies.",
            "citation_title": "",
            "mention_or_use": "mention",
            "environment_name": "TextWorld (general)",
            "environment_description": "A flexible text-game generation framework supporting many kinds of text-adventure tasks (various domains), commonly used in research on exploration and representation for text-based agents.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": null,
            "dead_ends_count": null,
            "door_constraints_present": null,
            "door_constraints_description": null,
            "graph_connectivity": null,
            "environment_size": null,
            "agent_name": null,
            "agent_description": null,
            "exploration_efficiency_metric": null,
            "exploration_efficiency_value": null,
            "success_rate": null,
            "optimal_policy_type": null,
            "topology_performance_relationship": null,
            "comparison_across_topologies": null,
            "topology_comparison_results": null,
            "policy_structure_findings": null,
            "uuid": "e1199.2",
            "source_info": {
                "paper_title": "Learning Symbolic Rules over Abstract Meaning Representations for Textual Reinforcement Learning",
                "publication_date_yy_mm": "2023-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Textworld: A learning environment for text-based games",
            "rating": 2,
            "sanitized_title": "textworld_a_learning_environment_for_textbased_games"
        },
        {
            "paper_title": "Textworld commonsense",
            "rating": 2,
            "sanitized_title": "textworld_commonsense"
        },
        {
            "paper_title": "Playing text-adventure games with graph-based deep reinforcement learning",
            "rating": 2,
            "sanitized_title": "playing_textadventure_games_with_graphbased_deep_reinforcement_learning"
        },
        {
            "paper_title": "Graph constrained reinforcement learning for natural language action spaces",
            "rating": 2,
            "sanitized_title": "graph_constrained_reinforcement_learning_for_natural_language_action_spaces"
        },
        {
            "paper_title": "Learning dynamic belief graphs to generalize on text-based games",
            "rating": 2,
            "sanitized_title": "learning_dynamic_belief_graphs_to_generalize_on_textbased_games"
        },
        {
            "paper_title": "Counting to explore and generalize in text-based games",
            "rating": 1,
            "sanitized_title": "counting_to_explore_and_generalize_in_textbased_games"
        }
    ],
    "cost": 0.0124205,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Learning Symbolic Rules over Abstract Meaning Representations for Textual Reinforcement Learning
Long PapersCopyright Long PapersJuly 9-14, 2023</p>
<p>Subhajit Chaudhury subhajit@ibm.com 
IBM Research</p>
<p>Sarathkrishna Swaminathan sarath.swaminathan@ibm.com 
IBM Research</p>
<p>Daiki Kimura 
IBM Research</p>
<p>Prithviraj Sen 
IBM Research</p>
<p>Keerthiram Murugesan keerthiram.murugesan@ibm.com 
IBM Research</p>
<p>Rosario Uceda-Sosa 
IBM Research</p>
<p>Michiaki Tatsubori 
IBM Research</p>
<p>Achille Fokoue 
IBM Research</p>
<p>Pavan Kapanipathi kapanipa@us.ibm.com 
IBM Research</p>
<p>Asim Munawar 
IBM Research</p>
<p>Alexander Gray alexander.gray@ibm.com 
IBM Research</p>
<p>Learning Symbolic Rules over Abstract Meaning Representations for Textual Reinforcement Learning</p>
<p>Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics
the 61st Annual Meeting of the Association for Computational LinguisticsLong Papers1July 9-14, 2023
Text-based reinforcement learning agents have predominantly been neural network-based models with embeddings-based representation, learning uninterpretable policies that often do not generalize well to unseen games. On the other hand, neuro-symbolic methods, specifically those that leverage an intermediate formal representation, are gaining significant attention in language understanding tasks. This is because of their advantages ranging from inherent interpretability, the lesser requirement of training data, and being generalizable in scenarios with unseen data. Therefore, in this paper, we propose a modular, NEuro-Symbolic Textual Agent (NESTA) that combines a generic semantic parser with a rule induction system to learn abstract interpretable rules as policies. Our experiments on established textbased game benchmarks show that the proposed NESTA method outperforms deep reinforcement learning-based techniques by achieving better generalization to unseen test games and learning from fewer training interactions.</p>
<p>Introduction</p>
<p>Text-based games (TBGs)  serve as popular sandbox environments for evaluating natural language-based reinforcement learning. The agent observes the state of the game in pure text and issues a textual command to interact with the environment. TBGs are partially observable where the full state of the world is hidden and action commands facilitate the agent to explore the unobserved parts of the environment. The reward signal from the environment is used to improve the agent's policy and make progress in the game.</p>
<p>Text-based games sit at the intersection of two research areas, i.e., language understanding and reinforcement learning. Existing RL agents for TBGs primarily use embeddings for observation as representations and are fed to an action scorer for predicting the next action (Narasimhan et al.,  2015a; Yuan et al., 2019;He et al., 2016), ignoring the advances in language understanding. On the other hand, there has been a recent surge in neurosymbolic techniques, particularly those that use symbolic representations, for better language understanding (Lu et al., 2021; through reasoning. In light of exploring such advances for text-based reinforcement learning, this work proposes a neuro-symbolic approach. Our approach, named NESTA (NEuro Symbolic Textual Agent) is a modular approach comprising a generic semantic parser in combination with a symbolic rule induction system as shown in Figure 1. The semantic parser translates text into the form of symbolic triples. NESTA uses Abstract Meaning Representation (Banarescu et al., 2013) as the initial parse which is then transformed into triples. This symbolic representation is used by an adaptation of the Inductive Logic Programming (ILP) sys-tem using Logical Neural Networks (Riegel et al., 2020) for learning horn clauses as action rules. NESTA, in comparison to other end-to-end learning approaches, has the following advantages: (a) modular language understanding using pre-trained large language models enabling our system to leverage the advances in semantic parsing. While such modular semantic parsing-based techniques have been around for other NLP tasks such as reading comprehension (Mitra and Baral, 2016;Galitsky, 2020), knowledge base question answering , and natural language inference (Lien and Kouylekov, 2015), this work is the first to demonstrate the application for TBGs ; (b) learning symbolic rules for model-free RL using a neuro-symbolic framework facilitates inherent interpretability and generalizability to unseen situations (Ma et al., 2021;Jiang and Luo, 2019;Dong et al., 2019). The rules learned by NESTA are abstract and not specific to entities in the training data. These abstract action rules in policies for TBGs enable reasoning over unseen entities during training.</p>
<p>Our main contributions in this work are: (1) We propose a novel and modular neuro-symbolic agent named NESTA. To the best of our knowledge, NESTA is the first to use a generic semantic parser with a rule learning system for TBGs, (2) Our empirical analysis of commonsense-aware textworld games shows that NESTA outperforms deep RL methods by a significant margin. We also show that NESTA has better sample efficiency compared to traditional text-based RL agents obtaining better test performance with up to 5× lesser training interactions, and (3) Our method produces interpretable abstract rules from the rule induction system.</p>
<p>NEuro-Symbolic Textual Agent</p>
<p>Text-based RL agents for TBGs interact with the environment using text-only action commands and obtain feedback solely as textual observations. As the agent does not have access to global state information, it is modeled as a Partially Observable Markov Decision Process (POMDP) (Kaelbling et al., 1998) represented as (S, A, T , R, Ω, O), where (S, A, T , R) represent a Markov Decision Process. Ω represents the finite set of all observations, and O represents the observation function representing the conditional distribution over observations for a given action and next state. The goal of the agent is to learn optimal action probabilities at each step such that the expected future reward is maximized.</p>
<p>We present NEuro-Symbolic Textual Agent (NESTA), a modular approach for TBGs. Figure 1 illustrates the overview of NESTA which comprises of three primary components: (a) Semantic Parser, which extracts symbolic representation of the text using AMR as the generic semantic representation, (b) Rule Learner, an ILP-based rule induction module, which learns logical rules that abstract out the entities in the games, making these rules generally applicable to test games containing unseen entities, and (c) Pruner, that reduces the amount of branching factor at each step by pruning actions that do not contribute to the expected future reward. Below, we describe these components in detail.</p>
<p>Semantic Parser: Text to symbolic triples using AMR</p>
<p>The first step in NESTA is to translate the text into symbolic representation. To this end, inspired by works that address different NLP tasks Galitsky, 2020;Mitra and Baral, 2016), we use an AMR parser as a generic semantic parser. The use of a generic semantic parse such as AMR allows the system to benefit from independent advances in AMR research. For example, the performance of AMR has improved in Smatch score ) from 70.9 (Van Noord and Bos, 2017) to 86.7 (Lee et al., 2022) on LDC2017T10 in the last few years due to advances in large language models. The AMRs are subsequently transformed into a symbolic form using a deterministic AMR-to-triples approach. Abstract Meaning Representation (AMR): AMR parsing produces rooted, directed acyclic graphs from the input sentences, where each node represents concepts from propbank frames (Kingsbury and Palmer, 2002) or entities from the text. The edges represent the arguments for the semantic frames. Fig. 2 shows the AMR graph generated from the sentence "There is a brown golf shoe and a blue moccasin on the cabinet.". The resultant AMR graph is rooted at the propbank frame be-located-at-91 with ARG1 and ARG2 edges leading to its children. The other parts of the graph are used to describe the entities for "brown golf shoe" and "blue moccasin". We use Struct-BART (Zhou et al., 2021) for parsing a text to AMR.</p>
<p>AMR-to-triples: We design an AMR-to-triples  Figure 2: Overview of our method for neuro-symbolic reasoning. Our methods first extract symbolic facts from the surface text observations. During training, NESTA learns first-order lifted rules based on the reward signals. The learned rules are then used for obtaining the action probabilities.</p>
<p>module to extract a set of symbolic facts consisting of generic domain-agnostic predicates from the AMR semantic representation. Fig. 2 shows the extraction of facts from AMR. The AMR-totriples module performs a set of graph operations to extract propbank nodes as the predicates and the children entities as the arguments. In the example, the two operands of the "and" node are converted into two symbolic facts with be-located-at predicate with two separate entities of "brown golf shoe" and "blue moccasins". We convert the symbolic facts to unary predicates. For example, we convert the be-located-at predicate with two arguments into single argument facts. These simplifications result in some loss of representational power but make the task of rule learning simpler. We also add the commonsense predicates from conceptnet subgraph (Speer et al., 2017) provided by the TWC environment (Murugesan et al., 2020).</p>
<p>Rule Learner: ILP from Rewards</p>
<p>In order to learn interpretable rules that can be debugged by humans, we use the symbolic representation obtained from the above step. Such symbolic rules are learned from reward signals by interacting with the environment. For this purpose, we use Inductive Logic Programming (ILP) in an RL setting with the objective of expected future reward maximization. We use Logical Neural Networks (LNN) as the differentiable rule learning engine. Logical Neural Networks: LNN (Riegel et al., 2020) proposes a differentiable rule learning framework that retains the benefits of both neural networks and symbolic learners. It proposes a logi-cal neuron that has the core properties of gradientbased learning similar to a standard neuron but adds logic-aware forward functions and constrained optimization making it suitable for logical operations. This can be illustrated on 2-input logical conjunction (AND) neuron with (x, y) as two logical inputs to the conjunction node. The LNN conjunction neuron generalizes the classical AND logic for realvalued logic by defining a noise threshold (α). The real-values in [α, 1] and [0, 1 − α] signify a logical high and logical low respectively. To emulate an AND neuron, LNN uses the standard truth table of the conjunction (AND) gate to obtain the following constraints,
f (x, y) ≤ 1 − α, ∀x, y ∈ [0, 1 − α] f (x, y) ≤ 1 − α, ∀x ∈ [0, 1 − α], y ∈ [α, 1] f (x, y) ≤ 1 − α, ∀x ∈ [α, 1], y ∈ [0, 1 − α] f (x, y) ≥ α, ∀x, y ∈ [α, 1]
.</p>
<p>LNN uses the forward function as the weighted
Łukasiewicz t-norm, f (x, y; β, w 1 , w 2 ) = β − w 1 (1 − x) − w 2 (1 − y),
where β, w 1 , w 2 are the bias and weights of the inputs. Given a target label, the weights and biases are tuned to learn the logical rule that best describes the data. ILP-based reward maximization: Our ILP rule learner is based on the LNN rule learning implementation in Sen et al. (2022). However, our rulelearning model makes significant modifications to adapt the previous algorithm for model-free policy optimization suitable for text-based RL. Consider the state transition at time step t as (o t , a t , r t , o t+1 ), where o t represents the textual observation, a t is the action command that yields the reward r t and takes the agent to the next state with observation o t+1 . AMR-to-triples semantic parser is used to obtain the symbolic state s t (list of symbolic facts) from o t as shown in Figure 2. At each step, the agent has to choose from a set of admissible action commands which are also converted to their symbolic form. Starting from an initial random policy π, we sample trajectories τ ∼ π and store the transitions (s t , a t , r t , s t+1 ) in a buffer B. We also store the admissible actions set adm t and the discounted future reward g t = T k=t γ k−t r k , for each step in the buffer, where γ is the discount factor.</p>
<p>From the buffer B, we find a set of template predicates P = {p | p ∈ s t , for s t ∈ B}, where p ∈ s t operation states whether facts with predicate p exist in the symbolic state s t . We also obtain a set of action predicates A = {a | a ∈ adm t , for adm t ∈ B} finding all action predicates in the admissible action set. We initialize ILP rule learner π a (θ) for each action predicate a ∈ A. Action predicates for TBGs typically coincide with the action verbs. The LNN policy is formulated as a weighted conjunction operation over the template predicates P. The likelihood of action a for abstract lifted variables x, y is given as a conjunction template over the predicate list as follows: unary action likelihood is given as L(a(x)|s t ) = k w k p k (x) and binary action likelihood is formulated as L(a(x, y)|s t ) = k w k p k (x) m w m q m (x, y). The predicates p k and q m are 1 and 2 arity predicates in P respectively and represents the LNN's logical conjunction operator. The weights w k and w m constitute the LNN parameters θ that are updated during training. At any given step, the likelihood of each action is normalized over all actions in the admissible action set to obtain the action probabilities.</p>
<p>For training the rule learning model π a (θ) for a specific action a, we only extract transitions from the buffer containing the action a and store it in a sub-buffer B a . The model is updated following the policy gradient loss, L = ∇ θ E (st,gt)∼Ba log(π a (a t = a|s t )g t , where the trajectories are sampled from B a . We assume that π a gives normalized probabilities for this loss formulation. Therefore, this training procedure yields separate rules learned for each action predicate. Figure 2 shows the learned rules for each action.</p>
<p>Generalization under Distribution Shift: Having learned the action rules for each action predi-cate using dedicated ILP models, NESTA uses the rules for obtaining the action probabilities at each step. This process consists of three steps: (a) For each action in the admissible action list, invoke the learned rule for that action predicate, (b) Assign the abstract variables with the symbolic action arguments, and (c) Match the symbolic facts using entity alignment by root noun matching (instead of an exact match). The probabilities are then obtained by the LNN conjunction node feed-forward operation based on the current weights. This procedure is also used for sampling during training. Figure 2 shows the reasoning steps for fixed weights after training is complete. Since the rules learned by NESTA abstract out the entities in the form of lifted variables, human interpretability and generalization to unseen entities is a natural advantage of our method. In addition to this, since we modularize language understanding and RL policy learning into separate modules, our LNN symbolic learner can solely focus on optimal reward performance leading to sample-efficient learning.</p>
<p>Pruner: Irrelevant Action Pruning by Look-Ahead</p>
<p>The third module in NESTA, tackles the large action space problem in TBGs by removing actions from the admissible commands that do contribute to future rewards in the games. A large number of possible actions at each step can increase the branching factor of the agent at each step during the training and testing leading to a combinatorially large search problem. We employ a look-ahead strategy to find out which actions do not contribute to future reward accumulation. For example, the action examine(x) returns the description of the entity x, but does not change the state of the game and does not contribute to future rewards. However, for the action take(x), although an immediate reward is not obtained on execution, it leads to a future reward when the object x is put in the correct container y using the put(x, y) command. Therefore, the action command of type examine(x) can be pruned but take(x) is essential and hence cannot be pruned. This can be computed by looking ahead from the current step and comparing the future reward if that particular action was removed from the trajectory. Due to AMR error propagation and undesirable credit assignment (for example, examine(x) command issued just before a rewarded action), the rule 1.00 ± 0.00 11.76 ± 1.78 0.98 ± 0.03 35.84 ± 7.88 0.85 ± 0.09 Human 2.12 ± 0.00 1.00 ± 0.00 5.33 ± 0.00 1.00 ± 0.00 15.00 ± 0.00 1.00 ± 0.00 learner can assign high action probabilities to noncontributing actions. Therefore, the pruner module is desirable to remove such action predicates (a) by evaluating the total reward in action trajectories with and without the particular action predicate a. More specifically, for each episodic trajectory, we remove the action predicates a and re-evaluate the episodic reward obtained from the environment. If the average episodic reward in both cases, with and without removal of a is the same then the action predicate is not contributing to the future reward. Therefore, it can be removed from the original action predicate set, A to obtain the pruned action set A pruned for which LNN models are learned.</p>
<p>Outlier Rejection in Policy Training</p>
<p>The training samples that NESTA collects from interacting with the environment can be noisy and this can affect learning a good policy. There exists two sources of noise: (a) AMR noise, where AMR incorrectly parses the surface text resulting in erroneous identification of entity extraction or relationships between entities, and (b) RL credit assignment noise, where discounted reward gives reward to a suboptimal action taken right before a correct action. Although symbolic reasoners have the advantages of learning from fewer data and better generalization, they are not robust to noise. We mitigate the effect of noise in LNN policy training by using a consensus-based noise rejection method. Our noise rejection method trains the LNN Policy on multiple subsets of training data and selects the model with the smallest training error as the best model. The multiple subsets of training data are prepared as follows -for each training subset, a particular predicate p from the predicate list P is given priority. We only choose state transitions that contain the predicate p ensuring that this predicate will be part of the final learned rule, thus elimi-nating the source of AMR noise for this predicate (such a subset is rejected if the number of such transitions is less than some threshold percentage). Subsequently, the resulting transitions are sorted by the discounted reward g t and we only retain the top first k% of this sorted data as training data. This encourages action transition with more immediate average reward gains to constitute the training data.</p>
<p>Experimental results</p>
<p>Our experiments are designed to answer these questions that analyze if NESTA can overcome the common drawbacks of deep RL methods: (i) Can NESTA enable better generalization in test environments? (ii) Does NESTA improve upon sample efficiency while still maintaining good reward performance, (iii) Are the rules learned by NESTA, human interpretable? For comparing the performance of various methods, we use the metrics of normalized score (total reward from the games normalized by maximum reward) and number of steps to reach the goal (lower is better). Our experiments were conducted on Ubuntu 18.04 operating system with NVidia V100 GPUs.</p>
<p>Environment</p>
<p>We use the textworld commonsense (TWC) environment (Murugesan et al., 2020) for empirical evaluation of our method. The goal here is to clean up a messy room by placing the objects in the correct containers. The game provides conceptnet sub-graphs relating the game entities which are used as commonsense graphs. TWC provides two splits of testing games: (i) in-distribution games that have the same entities as training games but unseen object-container configuration, and (ii) outof-distribution games that use new objects not seen during training. This provides a systematic framework for measuring generalization in NESTA and 1.00 ± 0.00 3.60 ± 0.00 1.00 ± 0.00 31.40 ± 6.38 0.91 ± 0.05 Human 2.24 ± 0.00 1.00 ± 0.00 4.40 ± 0.00 1.00 ± 0.00 17.67 ± 0.00 1.00 ± 0.00 other baseline agents for both within-training distribution and out-of-training distributions. Since we are focusing on generalization aspects, we do not use other textworld games  because these environments primarily focus on the agent's exploration strategies and are therefore not suitable to evaluate the agent's generalization ability.</p>
<p>Agents</p>
<p>For baseline agents, we report performance by these deep RL-based methods: (1) Text-based agent that uses a GRU network for observation representation and action scorer units, (2) TWCAgent (Text + CS) that uses combined textual and commonsense embeddings for action scoring, (3) KG-A2C  that uses extracted knowledge graphs as input, (4) BiKE (Murugesan et al., 2021) which leverages graph structures in both textual and commonsense information and (5) CBR (Atzeni et al., 2021) which is the SOTA method using case-based reasoning for improving generalization in text-based agents. We did not compare with previous neurosymbolic methods  because they use a hand-crafted game-specific predicate design scheme that was not available for TWC.</p>
<p>Generalization to Test Games</p>
<p>We evaluate the generalization ability of NESTA on TWC easy, medium and hard games. Table 1 and Table 2 shows the performance of baseline and our agents on in-distribution and out-of-distribution games, including the human performance from Murugesan et al. (2020). For the baseline models, we report scores from Atzeni et al. (2021). For NESTA, we report the mean of 5 independent runs.</p>
<p>For easy games, NESTA gets a perfect score outperforming previous games with similar steps as human performance. For medium and hard games, NESTA greatly surpasses the SOTA agent and needs a lesser number of steps for both indistribution and out-of-distribution games. For medium out-of-distribution games, NESTA outperforms humans in terms of the number of steps. This might be due to the fact that during human annotation, the subjects would take a larger number of steps for the initial few games due to trial-and-error, thus increasing the average number of steps.</p>
<p>While easy and medium games have a singleroom setting, hard games present a two-room setting where the agent might require picking up an object in room 1 and putting it in a container in room 2. This requires learning a complex strategy especially for generalizing to unseen entities. Our method NESTA scores significantly higher compared to SOTA on hard games, thus exhibiting the ability of our method to generalize in complex settings while deep RL methods fail to generalize due to overfitting the training data. Furthermore, our outlier rejection model helps improve the number of steps to reach the goal for both in-distribution and out-of-distribution games.</p>
<p>Ablation Results with Action Pruning</p>
<p>To study the effect of our action pruning module on deep RL agents, we implemented action pruning on the publicly available TWCAgent code from Murugesan et al. (2020). We follow the exact same methodology for TWCAgent that we used for the NESTA agent. Using the look-ahead method, we obtain A retain , the list of action verbs to retain at a specific episode (episode num 10 for this result). For all subsequent training steps, only action verbs a ∈ A retain were retained from the admissible list.</p>
<p>In-distribution</p>
<p>Steps Norm. Score TWCAgent 47.77 ± 1.50 0.49 ± 0.04 TWCAgent + AP 47.14 ± 0.85 0.61 ± 0.03 NESTA 43.44 ± 4.67 0.77 ± 0.08 NESTA + AP 35.84 ± 7.88 0.85 ± 0.09 Out-of-distribution</p>
<p>Steps Norm. Score TWCAgent 50.00 ± 0.00 0.21 ± 0.05 TWCAgent + AP 50.00 ± 0.00 0.37 ± 0.02 NESTA 47.52 ± 2.34 0.60 ± 0.15 NESTA + AP 31.40 ± 6.38 0.91 ± 0.05 We also follow the same strategy for the test games. Table 3 shows the results for action pruning for both TWCAgent and NESTA. Firstly, even without action pruning, NESTA outperforms the TWCAgent with action pruning. NESTA+AP shows a higher gain in performance compared to NESTA only, whereas TWCAgent did not exhibit such large improvements. We found that even without AP, TWCAgent learns to avoid sub-optimal actions. However, it suffers from overfitting and hence cannot generalize to unseen configurations and entities.</p>
<p>Human-in-the-loop Rule Debugging</p>
<p>NESTA enables the user to verify all the learned rules. It provides the facility to add new rules that might be missing or edit the rules if they are suboptimal. The ability of human-in-the-loop debugging is what sets NESTA apart from other methods that tend to provide some level of explainability. Table 4 shows the human-interpretable learned rules for a particular training on hard games. The rule for take(x, y) can be identified as sub-optimal because it implies that the agent should take any object that is present in a container y present in the current room. The human-corrected rule implies the agent should only "take" objects that are not in their assigned location according to conceptnet facts. The human-corrected rule perfectly solves the out-of-distribution hard games in close to the optimal number of steps. This demonstrates that NESTA's human-in-the-loop rule debugging feature can be readily used to achieve favorable performance gains.</p>
<p>Learned rules for hard games by NESTA go(x) : − direction(x) take(x) : − be-located-at(x) take(x, y) : − be-located-at(y) put(x, y) : − carry(x) ∧ atlocation(x, y) insert(x, y) : − carry(x) ∧ atlocation(x, y) In-distribution norm score: 0.71 (Steps: 46.4) Out-distribution norm score: 0.85 (Steps: 37.4) After rule correction by human take(x, y) : − ¬atlocation(x, y) In-distribution norm score: 0.88 (Steps: 42.4) Out-distribution norm score: 1.0 (Steps: 19.8) Table 4: Action rules learned for NESTA (seed=2) on hard games. The rules are human-interpretable making them easy to debug. We highlight that the rule learned for take(x, y) is sub-optimal and can be improved by human-in-the-loop correction of that single rule with large performance gains.</p>
<p>Sample efficient learning</p>
<p>We hypothesize that deep RL policies require a large number of training interactions because they learn both language understanding and action scoring from rewards ignoring external language pretraining. NESTA, on the other hand, decouples language understanding to AMR-based semantic representations while the LNN-ILP rule learner can focus on RL policy optimization resulting in learning from fewer samples. Figure 3 shows that the NESTA model obtains better scores for both in-distribution and out-distribution games at much fewer training interactions compared to the deep RL text agent. In fact, NESTA can outperform text agents even when it learns from 5× lesser training interactions.</p>
<p>We also computed computational time for NESTA compared to neural agents. Average computational times (out-of-distribution) required for each step for NESTA compared to neural agents. For easy games, the average computation time for neural agents was 0.12 ± 0.06 s, and that for NESTA was 0.16 ± 0.05. The corresponding numbers for medium games were 0.17 ± 0.06 and 0.22 ± 0.06 respectively. NESTA requires extra time due to parsing. However, since it has a lower overall number of steps (almost 5 times lower for easy/medium games from Table 2), time per game would be lower or comparable.</p>
<p>Related Work</p>
<p>Text-only Agents: Early work on text-based reinforcement learning agents used an LSTM- based representation learning from textual observations (Narasimhan et al., 2015b), andQ-learning (Watkins andDayan, 1992) in the action scorer of LSTM-DQN to assign probability scores to the possible actions.  used LSTM units in the action scorer of LSTM-DRQN to handle the better generalization. Chaudhury et al. (2020) further improved generalization and reduced overfitting by training a bootstrapped model, named CREST, on context-relevant observation text. Adolphs and Hofmann (2020) presented one of the winning strategies in the First-TextWorld Competition using the actor-critic algorithm (Mnih et al., 2016) for training the policy. Unlike these text-only models, NESTA uses symbolic reasoning over the lifted rules for better generalization and interpretability.</p>
<p>Graph-based Agents: Instead of relying on the neural models to capture the structure of the observed text , recent works considered the graph representation of the observed text to guide the agent for better exploration. Graph-based agents from (Ammanabrolu and Riedl, 2019;) build a knowledge graph representation of the textual state for efficient exploration and handling large action space. Adhikari et al. (2020) learns a dynamic belief graph from raw observations using adversarial learning on the First Textworld Problems (FTWP). Atzeni et al. (2021) proposed a case-based reasoning approach that improves upon existing graph-based methods by reusing the past positive experiences stored in the agent's memory. Unlike NESTA, these graphbased methods suffer from noise in the observation as the graphs are generated from the observed text.</p>
<p>Reasoning-based Agents: Both text-only and graph-based methods use only the texts observed during the game interaction. Murugesan et al. (2020) introduced Textworld commonsense (TWC), text-based cleanup games that require commonsense reasoning-based knowledge about everyday household objects Recent works tried to enrich text-only agents with commonsense reasoning for exploiting readily-available external knowledge graphs (Murugesan et al., 2021) and images generated from the observed texts using pre-trained models (Murugesan et al., 2022). These methods suffer from noisy features extracted from the external knowledge thus hindering the learning ability of the text-based RL agents. Unlike the traditional deep RL agents, ; ; Basu et al. (2021) proposed neurosymbolic agents for TBGs that show near-perfect performance. Related work from Li et al. (2021) uses the world model as a symbolic representation to capture the current state of the game. These approaches require hand-engineering of domainspecific symbolic state representation. On the other hand, NESTA presents a generic domainindependent symbolic logic representation with an automatic symbolic rule learner that handles large action spaces and noisy observation with ease.</p>
<p>In other symbolic methods, there are works (Petersen et al.; Costa et al., 2020) which employ deep learning for neuro-symbolic regression. Compared to these methods, NESTA aims to improve the generalization to unseen cases, whereas these methods train and test in the same setting. Additionally, neuro-symbolic regression methods have limited interaction with the environment in intermediate steps, and reward is obtained at the terminal state. However, for NESTA we use the symbolic representation from intermediate steps to learn action rules from partially-observable symbolic states.</p>
<p>Conclusion</p>
<p>In this paper, we present NESTA, a neuro-symbolic policy learning method that modularizes language understanding using an AMR-based semantic parsing module and RL policy optimization using an ILP rule learner. NESTA benefits from prior advances in AMR-based generic parsers for symbolic fact extraction allowing the ILP symbolic learner to solely learn interpretable action rules. NESTA outperforms SOTA models on TBGs by showing better generalization while learning from a fewer number of training interactions. We believe our model is one of the first works combining advances in neural semantic parsing and efficient symbolic planning for text-based RL. We hope this work will encourage future research in this direction.</p>
<p>Limitations</p>
<p>The neuro-symbolic rule learning presented in the paper can handle most generic text-based games.</p>
<p>Only in a few specific use cases, additional training of the AMR parser would be required. Since AMR is used for symbolic representation for text-based games, the vocabulary of the extracted triples is limited by the vocabulary of PropBank semantic roles. For applications in a very specific kind of domain where the predicates and entities do not match with this pre-defined vocabulary (for example, specific financial, legal domains, etc.), the AMR semantic parsing engine needs to be retrained first on such specific data before using it for rule learning. However, even in the cases where the testing environment requires additional rules, NESTA allows human-in-the-loop debugging to conveniently add them making it adaptable to generic environments.</p>
<p>Ethics Statement</p>
<p>Our method uses a constrained set of action samples to generate the textual actions in each step. Since this action set is generated from a controlled vocabulary of actions and entities, the produced actions cannot contain harmful content like hate speech and racial biases. Furthermore, our neurosymbolic model produces human interpretable rules for the action policy thereby making the model transparent and easier to control. Due to these reasons, the ethical risk from this work is low.</p>
<p>ACL 2023 Responsible NLP Checklist</p>
<p>A For every submission:</p>
<p>A1. Did you describe the limitations of your work?</p>
<p>Section 7 is the limitations section A2. Did you discuss any potential risks of your work?</p>
<p>Section 8</p>
<p>A3. Do the abstract and introduction summarize the paper's main claims?</p>
<p>Section 1 A4. Have you used AI writing assistants when working on this paper?</p>
<p>Grammar check assistant that corrected spellings on the paper B Did you use or create scientific artifacts?</p>
<p>Section 2.1, Section 4 B1. Did you cite the creators of artifacts you used?</p>
<p>Section 2.1, Section 4 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?</p>
<p>We will discuss the details of the license in the final code distribution B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)? Not applicable. Left blank.</p>
<p>B4. Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect / anonymize it? Not applicable. Left blank.</p>
<p>B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.? Section 2.1, Section 4 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits, etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the number of examples in train / validation / test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be. Section 4 C Did you run computational experiments? Section 4 C1. Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used? Partially in Section 4</p>
<p>The Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing assistance.</p>
<p>…Figure 1 :
1You see an open pedal bin … There is a brown golf shoe on the floor and a blue moccasin in the cabinet. You are carrying a wet tissue. take brown golf shoes(2) put wet tissue into pedal bin (3) take blue moccasins from the cabinet (4) close pedal binAdmissible actions take blue moccasins from the cabinet put wet tissue into pedal binAbstract Meaning Representation (AMR)Symbolic facts: carry(wet tissue) Our proposed NESTA model learns interpretable action rules as policy using ILP and outperforms SOTA deep RL methods on TBGs.</p>
<p>Figure 3 :
3Normalized score obtained by NESTA and deep RL text agent on TWC games. NESTA achieves a higher score compared to the deep agent even when learning from 5x lesser training interactions. The topleft part represent better performance while the bottomright part conveys worse performance.</p>
<p>Table 1 :
1Proposed NESTA model shows better performance in terms of normalized score and steps to reach the goal compared to Deep RL methods on unseen TWC in-distribution games.</p>
<p>Table 2 :
2Normalized score and number of steps to reach the final goal for various methods on unseen TWC out-of-distribution games. NESTA shows large improvements over previous Deep RL methods, especially for hard games. OR is Outlier Rejection.</p>
<p>Table 3 :
3Ablation study showing the effect of our pro-
posed symbolic action pruning (AP) on NESTA and 
TWCAgent for hard games. Proposed action pruning 
method shows better improvements on NESTA model 
when compared to improvements on TWCAgent. </p>
<p>Keerthiram Murugesan, Subhajit Chaudhury, and Kartik Talamadupula. 2022. Eye of the beholder: Improved relation generalization for text-based reinforcement learning agents. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 11094-11102. Karthik Narasimhan, Tejas Kulkarni, and Regina Barzilay. 2015a. Language understanding for text-based games using deep reinforcement learning. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1-11.
D Did you use human annotators (e.g., crowdworkers) or research with human participants?Left blank.D1. Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators, etc.? No response.D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants' demographic (e.g., country of residence)? No response.D3. Did you discuss whether and how consent was obtained from people whose data you're using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used? No response.D4. Was the data collection protocol approved (or determined exempt) by an ethics review board? No response.D5. Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data? No response.
Learning dynamic belief graphs to generalize on text-based games. Ashutosh Adhikari, Xingdi Yuan, Marc-Alexandre Côté, Mikuláš Zelinka, Marc-Antoine Rondeau, Romain Laroche, Pascal Poupart, Jian Tang, Adam Trischler, Will Hamilton, Advances in Neural Information Processing Systems. Curran Associates, Inc33Ashutosh Adhikari, Xingdi Yuan, Marc-Alexandre Côté, Mikuláš Zelinka, Marc-Antoine Rondeau, Romain Laroche, Pascal Poupart, Jian Tang, Adam Trischler, and Will Hamilton. 2020. Learning dynamic belief graphs to generalize on text-based games. In Ad- vances in Neural Information Processing Systems, volume 33, pages 3045-3057. Curran Associates, Inc.</p>
<p>Ledeepchef deep reinforcement learning agent for families of text-based games. Leonard Adolphs, Thomas Hofmann, 10.1609/aaai.v34i05.6228Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence34Leonard Adolphs and Thomas Hofmann. 2020. Ledeepchef deep reinforcement learning agent for families of text-based games. Proceedings of the AAAI Conference on Artificial Intelligence, 34(05):7342-7349.</p>
<p>Graph constrained reinforcement learning for natural language action spaces. Prithviraj Ammanabrolu, Matthew Hausknecht, International Conference on Learning Representations. Prithviraj Ammanabrolu and Matthew Hausknecht. 2020. Graph constrained reinforcement learning for natural language action spaces. In International Con- ference on Learning Representations.</p>
<p>Playing text-adventure games with graph-based deep reinforcement learning. Prithviraj Ammanabrolu, Mark Riedl, 10.18653/v1/N19-1358Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics1Prithviraj Ammanabrolu and Mark Riedl. 2019. Play- ing text-adventure games with graph-based deep re- inforcement learning. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 3557-3565, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Shehzaad Zuzar Dhuliawala, Keerthiram Murugesan, and Mrinmaya Sachan. 2021. Casebased reasoning for better generalization in textual reinforcement learning. Mattia Atzeni, International Conference on Learning Representations. Mattia Atzeni, Shehzaad Zuzar Dhuliawala, Keerthi- ram Murugesan, and Mrinmaya Sachan. 2021. Case- based reasoning for better generalization in textual reinforcement learning. In International Conference on Learning Representations.</p>
<p>Abstract meaning representation for sembanking. Laura Banarescu, Claire Bonial, Shu Cai, Madalina Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin Knight, Philipp Koehn, Martha Palmer, Nathan Schneider, Proceedings of the 7th linguistic annotation workshop and interoperability with discourse. the 7th linguistic annotation workshop and interoperability with discourseLaura Banarescu, Claire Bonial, Shu Cai, Madalina Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin Knight, Philipp Koehn, Martha Palmer, and Nathan Schneider. 2013. Abstract meaning representation for sembanking. In Proceedings of the 7th linguis- tic annotation workshop and interoperability with discourse, pages 178-186.</p>
<p>A hybrid neuro-symbolic approach for text-based games using inductive logic programming. Kinjal Basu, Keerthiram Murugesan, Mattia Atzeni, Pavan Kapanipathi, Kartik Talamadupula, Tim Klinger, Murray Campbell, Mrinmaya Sachan, Gopal Gupta, Combining Learning and Reasoning: Programming Languages, Formalisms, and Representations. Kinjal Basu, Keerthiram Murugesan, Mattia Atzeni, Pa- van Kapanipathi, Kartik Talamadupula, Tim Klinger, Murray Campbell, Mrinmaya Sachan, and Gopal Gupta. 2021. A hybrid neuro-symbolic approach for text-based games using inductive logic programming. In Combining Learning and Reasoning: Program- ming Languages, Formalisms, and Representations.</p>
<p>Smatch: an evaluation metric for semantic feature structures. Shu Cai, Kevin Knight, Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics. the 51st Annual Meeting of the Association for Computational LinguisticsShort Papers2Shu Cai and Kevin Knight. 2013. Smatch: an evaluation metric for semantic feature structures. In Proceed- ings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 748-752.</p>
<p>Asim Munawar, and Ryuki Tachibana. 2020. Bootstrapped Q-learning with context relevant observation pruning to generalize in text-based games. Subhajit Chaudhury, Daiki Kimura, Kartik Talamadupula, Michiaki Tatsubori, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Subhajit Chaudhury, Daiki Kimura, Kartik Tala- madupula, Michiaki Tatsubori, Asim Munawar, and Ryuki Tachibana. 2020. Bootstrapped Q-learning with context relevant observation pruning to gener- alize in text-based games. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 3002-3008.</p>
<p>Neuro-symbolic approaches for text-based policy learning. Subhajit Chaudhury, Prithviraj Sen, Masaki Ono, Daiki Kimura, Michiaki Tatsubori, Asim Munawar, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingSubhajit Chaudhury, Prithviraj Sen, Masaki Ono, Daiki Kimura, Michiaki Tatsubori, and Asim Munawar. 2021. Neuro-symbolic approaches for text-based policy learning. In Proceedings of the 2021 Con- ference on Empirical Methods in Natural Language Processing, pages 3073-3078.</p>
<p>Fast neural models for symbolic regression at scale. Allan Costa, Rumen Dangovski, Owen Dugan, Samuel Kim, Pawan Goyal, Marin Soljačić, Joseph Jacobson, arXiv:2007.10784arXiv preprintAllan Costa, Rumen Dangovski, Owen Dugan, Samuel Kim, Pawan Goyal, Marin Soljačić, and Joseph Ja- cobson. 2020. Fast neural models for symbolic re- gression at scale. arXiv preprint arXiv:2007.10784.</p>
<p>Ákos Marc-Alexandre Côté, Xingdi Kádár, Ben Yuan, Tavian Kybartas, Emery Barnes, James Fine, Matthew Moore, Layla El Hausknecht, Mahmoud Asri, Adada, arXiv:1806.11532Textworld: A learning environment for text-based games. arXiv preprintMarc-Alexandre Côté, Ákos Kádár, Xingdi Yuan, Ben Kybartas, Tavian Barnes, Emery Fine, James Moore, Matthew Hausknecht, Layla El Asri, Mahmoud Adada, et al. 2018. Textworld: A learning en- vironment for text-based games. arXiv preprint arXiv:1806.11532.</p>
<p>Honghua Dong, Jiayuan Mao, Tian Lin, Chong Wang, Lihong Li, Denny Zhou, arXiv:1904.11694Neural logic machines. arXiv preprintHonghua Dong, Jiayuan Mao, Tian Lin, Chong Wang, Lihong Li, and Denny Zhou. 2019. Neural logic machines. arXiv preprint arXiv:1904.11694.</p>
<p>Employing abstract meaning representation to lay the last-mile toward reading comprehension. Boris Galitsky, Artificial Intelligence for Customer Relationship Management. SpringerBoris Galitsky. 2020. Employing abstract meaning rep- resentation to lay the last-mile toward reading com- prehension. In Artificial Intelligence for Customer Relationship Management, pages 57-86. Springer.</p>
<p>Interactive fiction games: A colossal adventure. Matthew Hausknecht, Prithviraj Ammanabrolu, Marc-Alexandre Côté, Xingdi Yuan, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence34Matthew Hausknecht, Prithviraj Ammanabrolu, Marc- Alexandre Côté, and Xingdi Yuan. 2020. Interactive fiction games: A colossal adventure. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 7903-7910.</p>
<p>Deep reinforcement learning with a natural language action space. Ji He, Jianshu Chen, Xiaodong He, Jianfeng Gao, Lihong Li, Li Deng, Mari Ostendorf, 10.18653/v1/P16-1153Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. the 54th Annual Meeting of the Association for Computational LinguisticsJi He, Jianshu Chen, Xiaodong He, Jianfeng Gao, Li- hong Li, Li Deng, and Mari Ostendorf. 2016. Deep reinforcement learning with a natural language action space. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Vol- ume 1: Long Papers), pages 1621-1630.</p>
<p>Neural logic reinforcement learning. Zhengyao Jiang, Shan Luo, PMLRInternational conference on machine learning. Zhengyao Jiang and Shan Luo. 2019. Neural logic reinforcement learning. In International conference on machine learning, pages 3110-3119. PMLR.</p>
<p>Planning and acting in partially observable stochastic domains. Leslie Pack Kaelbling, Anthony R Michael L Littman, Cassandra, Artificial intelligence. 1011-2Leslie Pack Kaelbling, Michael L Littman, and An- thony R Cassandra. 1998. Planning and acting in partially observable stochastic domains. Artificial intelligence, 101(1-2):99-134.</p>
<p>Leveraging abstract meaning representation for knowledge base question answering. Pavan Kapanipathi, Ibrahim Abdelaziz, Srinivas Ravishankar, Salim Roukos, Alexander Gray, Ramón Fernandez Astudillo, Maria Chang, Cristina Cornelio, Saswati Dana, Achille Fokoue-Nkoutche, Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. Pavan Kapanipathi, Ibrahim Abdelaziz, Srinivas Ravis- hankar, Salim Roukos, Alexander Gray, Ramón Fer- nandez Astudillo, Maria Chang, Cristina Corne- lio, Saswati Dana, Achille Fokoue-Nkoutche, et al. 2021. Leveraging abstract meaning representation for knowledge base question answering. In Find- ings of the Association for Computational Linguis- tics: ACL-IJCNLP 2021, pages 3884-3894.</p>
<p>Neuro-symbolic reinforcement learning with first-order logic. Daiki Kimura, Masaki Ono, Subhajit Chaudhury, Ryosuke Kohita, Akifumi Wachi, Don Joven Agravante, Michiaki Tatsubori, Asim Munawar, Alexander Gray, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDaiki Kimura, Masaki Ono, Subhajit Chaudhury, Ryosuke Kohita, Akifumi Wachi, Don Joven Agra- vante, Michiaki Tatsubori, Asim Munawar, and Alexander Gray. 2021. Neuro-symbolic reinforce- ment learning with first-order logic. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3505-3511.</p>
<p>From treebank to propbank. R Paul, Martha Kingsbury, Palmer, LREC. CiteseerPaul R Kingsbury and Martha Palmer. 2002. From treebank to propbank. In LREC, pages 1989-1993. Citeseer.</p>
<p>Maximum Bayes Smatch ensemble distillation for AMR parsing. Young-Suk Lee, Ramón Astudillo, Thanh Hoang, Tahira Lam, Radu Naseem, Salim Florian, Roukos, 10.18653/v1/2022.naacl-main.393Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesAssociation for Computational LinguisticsYoung-Suk Lee, Ramón Astudillo, Hoang Thanh Lam, Tahira Naseem, Radu Florian, and Salim Roukos. 2022. Maximum Bayes Smatch ensemble distilla- tion for AMR parsing. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5379-5392. Associa- tion for Computational Linguistics.</p>
<p>Implicit representations of meaning in neural language models. Z Belinda, Maxwell Li, Jacob Nye, Andreas, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingLong Papers1Belinda Z Li, Maxwell Nye, and Jacob Andreas. 2021. Implicit representations of meaning in neural lan- guage models. In Proceedings of the 59th Annual Meeting of the Association for Computational Lin- guistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1813-1827.</p>
<p>Semantic parsing for textual entailment. Elisabeth Lien, Milen Kouylekov, Proceedings of the 14th International Conference on Parsing Technologies. the 14th International Conference on Parsing TechnologiesElisabeth Lien and Milen Kouylekov. 2015. Semantic parsing for textual entailment. In Proceedings of the 14th International Conference on Parsing Technolo- gies, pages 40-49.</p>
<p>Neurologic decoding:(un) supervised neural text generation with predicate logic constraints. Ximing Lu, Peter West, Rowan Zellers, Le Ronan, Chandra Bras, Yejin Bhagavatula, Choi, NAACL-HLT. Ximing Lu, Peter West, Rowan Zellers, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2021. Neuro- logic decoding:(un) supervised neural text generation with predicate logic constraints. In NAACL-HLT.</p>
<p>Zhihao Ma, Yuzheng Zhuang, Paul Weng, Hankui Hankz, Dong Zhuo, Wulong Li, Jianye Liu, Hao, arXiv:2103.082282021. Learning symbolic rules for interpretable deep reinforcement learning. arXiv preprintZhihao Ma, Yuzheng Zhuang, Paul Weng, Hankz Han- kui Zhuo, Dong Li, Wulong Liu, and Jianye Hao. 2021. Learning symbolic rules for inter- pretable deep reinforcement learning. arXiv preprint arXiv:2103.08228.</p>
<p>Language understanding for text-based games using deep reinforcement learning. Karthik Narasimhan, Tejas Kulkarni, Regina Barzilay, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language ProcessingKarthik Narasimhan, Tejas Kulkarni, and Regina Barzi- lay. 2015b. Language understanding for text-based games using deep reinforcement learning. In Pro- ceedings of the 2015 Conference on Empirical Meth- ods in Natural Language Processing, pages 1-11.</p>
<p>Deep symbolic regression: Recovering mathematical expressions from data via risk-seeking policy gradients. Mikel Landajuela Brenden K Petersen, Larma, N Terrell, Claudio Prata Mundhenk, Soo Kyung Santiago, Joanne Taery Kim, Kim, International Conference on Learning Representations. Brenden K Petersen, Mikel Landajuela Larma, Terrell N Mundhenk, Claudio Prata Santiago, Soo Kyung Kim, and Joanne Taery Kim. Deep symbolic regression: Recovering mathematical expressions from data via risk-seeking policy gradients. In International Con- ference on Learning Representations.</p>
<p>Ismail Yunus Akhalwaya. Ryan Riegel, Alexander Gray, Francois Luus, Naweed Khan, Ndivhuwo Makondo, arXiv:2006.13155Udit Sharma, et al. 2020. Logical neural networks. Haifeng Qian, Ronald Fagin, Francisco BarahonaarXiv preprintRyan Riegel, Alexander Gray, Francois Luus, Naweed Khan, Ndivhuwo Makondo, Ismail Yunus Akhal- waya, Haifeng Qian, Ronald Fagin, Francisco Bara- hona, Udit Sharma, et al. 2020. Logical neural net- works. arXiv preprint arXiv:2006.13155.</p>
<p>Neuro-symbolic inductive logic programming with logical neural networks. Prithviraj Sen, Breno Wsr De, Ryan Carvalho, Alexander Riegel, Gray, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence36Prithviraj Sen, Breno WSR de Carvalho, Ryan Riegel, and Alexander Gray. 2022. Neuro-symbolic induc- tive logic programming with logical neural networks. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 8212-8219.</p>
<p>Conceptnet 5.5: An open multilingual graph of general knowledge. Robyn Speer, Joshua Chin, Catherine Havasi, Thirty-first AAAI conference on artificial intelligence. Robyn Speer, Joshua Chin, and Catherine Havasi. 2017. Conceptnet 5.5: An open multilingual graph of gen- eral knowledge. In Thirty-first AAAI conference on artificial intelligence.</p>
<p>Neural semantic parsing by character-based translation: Experiments with abstract meaning representations. Rik Van Noord, Johan Bos, arXiv:1705.09980arXiv preprintRik Van Noord and Johan Bos. 2017. Neural semantic parsing by character-based translation: Experiments with abstract meaning representations. arXiv preprint arXiv:1705.09980.</p>
<p>Qlearning. Jch Christopher, Peter Watkins, Dayan, Machine learning. 83-4Christopher JCH Watkins and Peter Dayan. 1992. Q- learning. Machine learning, 8(3-4):279-292.</p>
<p>Counting to explore and generalize in text-based games. Xingdi Yuan, Marc-Alexandre Côté, Alessandro Sordoni, Romain Laroche, arXiv:1806.11525Remi Tachet des Combes. arXiv preprintXingdi Yuan, Marc-Alexandre Côté, Alessandro Sor- doni, Romain Laroche, Remi Tachet des Combes, Matthew Hausknecht, and Adam Trischler. 2018. Counting to explore and generalize in text-based games. arXiv preprint arXiv:1806.11525.</p>
<p>Counting to explore and generalize in text-based games. Xingdi Yuan, Marc-Alexandre Côté, Alessandro Sordoni, Romain Laroche, Remi Tachet des Combes, Matthew Hausknecht, and Adam Trischler. Xingdi Yuan, Marc-Alexandre Côté, Alessandro Sor- doni, Romain Laroche, Remi Tachet des Combes, Matthew Hausknecht, and Adam Trischler. 2019. Counting to explore and generalize in text-based games.</p>
<p>Ramón Fernandez Astudillo, and Radu Florian. Jiawei Zhou, Tahira Naseem, arXiv:2104.14674Amr parsing with action-pointer transformer. arXiv preprintJiawei Zhou, Tahira Naseem, Ramón Fernandez As- tudillo, and Radu Florian. 2021. Amr parsing with action-pointer transformer. arXiv preprint arXiv:2104.14674.</p>
<p>Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values? Section 4. C2C2. Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values? Section 4</p>
<p>error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. C3. Did you report descriptive statistics about your results. or just a single run? Section 4C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run? Section 4</p>
<p>for preprocessing, for normalization, or for evaluation), did you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE, etc.)? Not applicable. C4. If you used existing packages. Left blankC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE, etc.)? Not applicable. Left blank.</p>            </div>
        </div>

    </div>
</body>
</html>
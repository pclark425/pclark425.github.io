<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-634 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-634</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-634</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-19.html">extraction-schema-19</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <p><strong>Paper ID:</strong> paper-89744cbaa080c82785b1cb8d54710bbbca32f8ed</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/89744cbaa080c82785b1cb8d54710bbbca32f8ed" target="_blank">Data Curation Alone Can Stabilize In-context Learning</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> This paper shows that carefully curating a subset of training data greatly stabilizes ICL performance without any other changes to the ICL algorithm, and introduces two methods to choose training subsets—both score training examples individually, then select the highest-scoring ones.</p>
                <p><strong>Paper Abstract:</strong> In-context learning (ICL) enables large language models (LLMs) to perform new tasks by prompting them with a sequence of training examples. However, it is known that ICL is very sensitive to the choice of training examples: randomly sampling examples from a training set leads to high variance in performance. In this paper, we show that carefully curating a subset of training data greatly stabilizes ICL performance without any other changes to the ICL algorithm (e.g., prompt retrieval or calibration). We introduce two methods to choose training subsets—both score training examples individually, then select the highest-scoring ones. CondAcc scores a training example by its average dev-set ICL accuracy when combined with random training examples, while Datamodels learns linear regressors that estimate how the presence of each training example influences LLM outputs. Across five tasks and two LLMs, sampling from stable subsets selected by CondAcc and Datamodels improves average accuracy over sampling from the entire training set by 7.7% and 6.3%, respectively.Surprisingly, the stable subset examples are not especially diverse in content or low in perplexity, in contrast with other work suggesting that diversity and perplexity are important when prompting LLMs.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e634.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e634.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt-sampling variability</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Variability due to random sampling of in-context training examples (prompt sampling variability)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper documents large performance variance in in-context learning (ICL) when prompts are formed by randomly sampling K examples from a training pool: different sampled prompts produce widely different accuracies, showing that prompt composition is a primary source of stochasticity in LLM ICL experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPTJ-6B, OPT-13B (main); also GPT-Neo-2.7B, OPT-6.7B (additional)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>6B, 13B (also 2.7B, 6.7B)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Natural language processing / NLP (few-shot classification)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Few-shot in-context learning classification (SST-2, BoolQ, Subj, Scicite, AGNews), sampling K examples per prompt</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Random sampling of training examples to form prompts (which examples are included); prompt composition stochasticity across sampled prompts</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Average accuracy across sampled prompts, standard deviation of accuracy across sampled prompts, worst-case (minimum) accuracy across sampled prompts</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Main evaluation sampled 50 prompts per selected subset and reported mean ± std and minimum accuracy; e.g., GPTJ-6B All baseline on SST-2: 77.8% average accuracy with std 11.2 and min 50.8; CondAcc on SST-2: 86.7% ± 5.9 (min 68.2). Overall CondAcc and Datamodels improve average accuracy over All by 7.7% and 6.3%, respectively.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Comparison of mean/std/min accuracy across 50 sampled prompts; overlap counts of selected stable examples between models; Pearson correlation of example scores between model pairs</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>Curating a stable subset reduces variance and improves worst-case accuracy (example: GPTJ-6B SST-2 std reduced from 11.2 to 5.9 and min improved from 50.8 to 68.2 with CondAcc). Overlap of stable examples between LLM pairs is limited (e.g., GPTJ-6B vs OPT-13B SST-2 overlap 4 examples; Pearson corr of example scores 0.15).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>High sensitivity of ICL to which examples are sampled into prompts; order/composition dependence; label-pattern induced bias; model-to-model differences in which examples are 'stable'</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Curating a stable training subset and then randomly sampling prompts from that subset (CondAcc and DATAMODELS); balancing label presence in prompts; ensuring each sampled prompt contains at least one example per class; calibration (Calib) as baseline</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Curated subsets (CondAcc/DATAMODELS) increased average accuracy (CondAcc +7.7% avg vs All; Datamodels +6.3% avg) and reduced std-dev and raised worst-case accuracy on many tasks (see SST-2 example above).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>50 sampled prompts per selected subset for main evaluation; dev collection D_ICL contains ~40k–100k prompts depending on task (authors used ~50k prompts commonly); datamodels test set: 5000 unseen prompts; authors note at least ~10,000 sampled prompts are needed for stable estimates.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Random prompt sampling induces large stochastic variability in ICL performance; careful curation of a small stable subset substantially reduces variance and improves both mean and worst-case accuracy across models and tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Data Curation Alone Can Stabilize In-context Learning', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e634.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e634.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt-order sensitivity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sensitivity of ICL to the order (positioning) of examples within the prompt</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>ICL performance depends not only on which examples are included but also on their order in the prompt; the paper models and exploits index effects in the DATAMODELS method by learning per-index example weights.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPTJ-6B, OPT-13B (main); also GPT-Neo-2.7B, OPT-6.7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>6B, 13B (also 2.7B, 6.7B)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP (few-shot in-context learning)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Estimating how example presence at each index affects LLM logits/margins and using that to score and select stable examples</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Ordering of examples within the prompt (index effects); interactions between examples depending on their positional index</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Datamodel weights per (example, index) and aggregated count of positive weights; prediction error of datamodels (Pearson correlation and L1 distance between predicted and actual LLM margins)</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Datamodels approximated LLM outcomes well: Table 6 reports per-task average Pearson correlations (e.g., GPTJ-6B SST-2 corr 0.962, L1 distance 0.133; OPT-13B SST-2 corr 0.930, L1 0.264), indicating index and example-existence features capture much of ordering effects.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Accuracy of datamodel predictions on unseen prompts (Pearson correlation, L1 distance); downstream effect measured by improved stability (mean/std/min) when selecting examples using datamodel scores</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>Datamodel-based selection yields improved average and worst-case accuracy (e.g., GPTJ-6B average across tasks 72.4% with Datamodels vs 65.2% All), demonstrating that modeling order effects helps reproducibility of ICL performance.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Complex combinatorial space of orders (K! per prompt), requiring many sampled prompts to estimate position-specific influences; computational cost of collecting enough prompt statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Fit linear datamodels with features for (example existence, example index) and aggregate positive per-index weights to score examples; two-phase datamodel training that buckets by label-pattern to capture label-proportion effects.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Datamodels provided informative weights and produced subsets that reduce variance and improve mean/worst-case accuracy (average improvement vs All ~+6.3%), and datamodel predictions have high correlation with true LLM outcomes (up to ~0.96).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>Datamodels trained on M prompts from D_ICL (M typically tens of thousands; authors used ~40k–100k prompts per setup); evaluated on 5000 unseen prompt outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Index (order) effects are important for ICL; a simple linear datamodel using example presence and index can accurately predict LLM outcome margins and be used to select stable examples that reduce stochasticity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Data Curation Alone Can Stabilize In-context Learning', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e634.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e634.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Label-pattern / single-label bias</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bias induced by the proportion and pattern of in-context labels (label-pattern bias)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper documents that prompts with skewed label patterns (e.g., all examples with same label) bias LLM predictions and that selected stable examples can mitigate this bias even when prompts contain only one label.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPTJ-6B, OPT-13B (main)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>6B, 13B</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP (few-shot classification)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Assessing performance when prompts contain only a single label (single-label prompts) and evaluating whether curated subsets still produce informative signals</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Label proportions and label-pattern (which labels and how many of each appear in the K-shot prompt) that bias the model toward certain outputs</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Average accuracy and standard deviation for prompts with specific label patterns (e.g., [0,0,0,0] or [1,1,1,1]) across 50 sampled prompts</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Baselines All and TopPrompts performed near chance in many single-label prompt setups, while CondAcc and Datamodels single-label prompts substantially outperformed majority guessing; example: GPTJ-6B SST-2 with [0,0,0,0] All = 51.7%±1.7, CondAcc = 61.8%±4.9, Datamodels = 72.8%±4.9.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Mean/std across 50 single-label prompts; comparisons across selection methods and models</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>Selected subsets contain examples that convey task-defining signals robust to label-pattern-induced bias; single-label prompts sampled from CondAcc/Datamodels give well-above-chance accuracy, indicating improved reproducibility of task behavior under extreme label patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>LLMs can be heavily biased by label mixes in context, making many prompts non-informative; ensuring fair evaluation requires controlling label-pattern coverage in collected prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Selecting stable examples that convey task definition (CondAcc/Datamodels); during main evaluation ensure every sampled prompt contains at least one example per class to avoid trivial label bias.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Curated subsets enabled above-majority performance even under single-label prompts (examples above show improvements of ~10–20 percentage points over All).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>50 sampled single-label prompts per subset for evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Careful example selection can produce prompts that still convey task structure even if all in-context labels are identical, thereby reducing label-pattern-induced variability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Data Curation Alone Can Stabilize In-context Learning', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e634.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e634.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CondAcc</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Conditional Accuracy (CondAcc) example scoring and subset selection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A selection method introduced in this paper that scores a training example by its expected dev-set ICL accuracy conditioned on the example appearing in a randomly sampled prompt, and selects top-scoring examples per class to form a stable subset.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPTJ-6B, OPT-13B (main); also GPT-Neo-2.7B, OPT-6.7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>6B, 13B (also 2.7B, 6.7B)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP (few-shot ICL)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Score training examples by conditional dev-set accuracy over many sampled prompts (D_ICL) and select a balanced set of E examples to reduce ICL variance</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Variation caused by choosing different in-context examples; CondAcc directly targets this source by estimating per-example average effect</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Average accuracy, standard deviation, and minimum accuracy across 50 sampled prompts from the selected subset; number of gold-labeled examples selected in unlabeled setup</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>CondAcc-selected subsets reduce variance and increase mean/worst-case accuracy: averaged across tasks CondAcc outperforms All by +7.7% (average accuracy). Example: GPTJ-6B SST-2 All: 77.8%±11.2 (min 50.8) -> CondAcc: 86.7%±5.9 (min 68.2).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Same mean/std/min reported across multiple random prompt draws; performance on out-of-distribution tasks and unlabeled setups</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>CondAcc generalizes to OOD tasks and unlabeled setups (Un-CondAcc sometimes outperforms All); e.g., Un-CondAcc outperforms All by 5.7% average accuracy and Un-All by 10.2%.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Requires many sampled prompts (authors used ~10k–50k+) to obtain reliable conditional accuracy estimates; high GPU compute/memory cost for collecting D_ICL.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Estimate conditional accuracy by running ICL on dev set across a large D_ICL (many prompts) and selecting top E' examples per class with highest conditional accuracy; ensure each example is sampled multiple times in D_ICL.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Quantitative gains: average accuracy improvements vs All (CondAcc +7.7%); reduced std-dev and increased worst-case accuracy on multiple tasks and models; Un-CondAcc can outperform labeled All in some tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>D_ICL sizes ~40k–100k prompts used to estimate s_ca(i); evaluation uses 50 sampled test prompts per selected subset; authors note at least ~10,000 prompts needed.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A simple, direct data-curation method (CondAcc) that scores examples by their conditional dev-set accuracy can substantially stabilize ICL performance, improving mean and worst-case metrics across tasks and models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Data Curation Alone Can Stabilize In-context Learning', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e634.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e634.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DATAMODELS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Datamodels-based selection (linear datamodels over example-existence and index)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An adapted datamodels approach that fits linear regressors predicting LLM outcome margins from features indicating which example appears at which index; examples with many positive weights across dev datamodels are selected for a stable subset.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPTJ-6B, OPT-13B (main); also GPT-Neo-2.7B, OPT-6.7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>6B, 13B (also 2.7B, 6.7B)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP (few-shot ICL)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Train linear datamodels per-dev-example to predict LLM margin outcomes for prompts and aggregate per-example positive weights across indices and dev examples to score examples for subset selection</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Example presence and positional index interactions in prompts; label-pattern effects (handled by bucketing label patterns during two-phase datamodel training)</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Datamodel predictive accuracy: Pearson correlation and L1 distance between predicted and true LLM margins on 5000 unseen prompts; downstream mean/std/min accuracy across 50 sampled prompts from selected subset</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Datamodels predictions correlate highly with LLM outcomes (Table 6: e.g., GPTJ-6B SST-2 corr 0.962 L1 0.133; other tasks corr range ~0.89–0.96). Datamodels-based selection improved average accuracy over All by ~6.3% on average and reduced variance on many tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Pearson correlation and L1 between datamodel predictions and LLM outcomes; downstream stability metrics (mean/std/min) for ICL</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>High datamodel-to-LLM correlations indicate datamodels can reliably approximate outcome variability; selected subsets from Datamodels achieve improved mean and often reduced std-dev relative to All (e.g., average across tasks for GPTJ-6B with Datamodels = 72.4% vs All = 65.2%).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Need large D_ICL to train datamodels; must account for label-pattern variety (authors use two-phase training and bucket by label pattern); computational cost is significant.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Fit linear datamodels using features (example id, index) per dev example; aggregate positive weights; use two-phase training with label-pattern bucketing; select top E' per class.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Datamodels provided strong, measurable stability gains (average accuracy improvements ~6.3% vs All) and produced accurate approximations of LLM outcome variability (correlations up to ~0.96).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>Trained on M prompts from D_ICL (M tens of thousands); evaluated datamodels on 5000 unseen prompts; main ICL evaluations use 50 sampled prompts per selected subset.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A simple linear datamodel that encodes example existence and index can accurately predict LLM outcome variability and be used to select stable in-context examples that reduce stochasticity in ICL.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Data Curation Alone Can Stabilize In-context Learning', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e634.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e634.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Evaluation protocol & metrics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Evaluation protocol and reproducibility / variability metrics used in the experiments</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper's evaluation protocol quantifies variability by sampling multiple prompts from a (selected) subset and reporting average accuracy, standard deviation, and worst-case (minimum) accuracy; it also assesses datamodel predictive fidelity via Pearson correlation and L1 distance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPTJ-6B, OPT-13B (main); other LLMs included in ablations</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>6B, 13B (others 2.7B, 6.7B)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP (few-shot ICL)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Stability evaluation of ICL subsets and datamodel predictive accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Stochastic prompt sampling, ordering, label-patterns, model differences</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Primary: mean accuracy, standard deviation (std) across 50 sampled prompts, minimum (worst-case) accuracy across sampled prompts; Secondary: Pearson correlation and L1 distance for datamodel predictions; overlap counts and Pearson correlation between example scores across models</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Main results reported as mean_{std} and min in tables; e.g., GPTJ-6B All on SST-2 77.8_{11.2} (min 50.8); CondAcc 86.7_{5.9} (min 68.2). Datamodels correlation values in Table 6 range ~0.89–0.96 with L1 distances 0.088–0.340 depending on task/model.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>See above; reproducibility also assessed via OOD experiments and unlabeled setups (Un-CondAcc) comparing methods and baselines</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>Quantitative improvements in mean and worst-case performance when using curated subsets; Un-CondAcc sometimes outperforms All, indicating selection can improve reproducibility even without gold labels.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Need sufficiently many sampled prompts to estimate statistics reliably (authors used tens of thousands for collection and 50 for reporting); heavy compute cost for collecting D_ICL.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Use of large D_ICL to estimate statistics; evaluate with multiple sampled prompts (50) and report mean/std/min; datamodel-based prediction checks (5000 unseen prompts) to validate modeling assumptions.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Protocol allowed reliable detection of variance reduction and mean improvement from curated subsets; datamodel prediction metrics indicate modeling assumptions are valid (high corr, low L1).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>50 sampled prompts per subset for main test-set reporting; D_ICL collection sizes ~40k–100k prompts; datamodels evaluated on 5000 unseen prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Robust stability evaluation requires multiple sampled prompts and reporting of mean, standard deviation, and worst-case accuracy; datamodel predictive metrics (Pearson/L1) are effective to validate surrogate models of LLM variability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Data Curation Alone Can Stabilize In-context Learning', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>What makes good in-context examples for gpt-3? <em>(Rating: 2)</em></li>
                <li>Calibrate before use: Improving few-shot performance of language models <em>(Rating: 2)</em></li>
                <li>Datamodels: Predicting predictions from training data <em>(Rating: 2)</em></li>
                <li>Data Shapley: Equitable valuation of data for machine learning <em>(Rating: 2)</em></li>
                <li>In-context example selection with influences <em>(Rating: 2)</em></li>
                <li>Rethinking the role of demonstrations: What makes in-context learning work? <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-634",
    "paper_id": "paper-89744cbaa080c82785b1cb8d54710bbbca32f8ed",
    "extraction_schema_id": "extraction-schema-19",
    "extracted_data": [
        {
            "name_short": "Prompt-sampling variability",
            "name_full": "Variability due to random sampling of in-context training examples (prompt sampling variability)",
            "brief_description": "The paper documents large performance variance in in-context learning (ICL) when prompts are formed by randomly sampling K examples from a training pool: different sampled prompts produce widely different accuracies, showing that prompt composition is a primary source of stochasticity in LLM ICL experiments.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPTJ-6B, OPT-13B (main); also GPT-Neo-2.7B, OPT-6.7B (additional)",
            "model_size": "6B, 13B (also 2.7B, 6.7B)",
            "scientific_domain": "Natural language processing / NLP (few-shot classification)",
            "experimental_task": "Few-shot in-context learning classification (SST-2, BoolQ, Subj, Scicite, AGNews), sampling K examples per prompt",
            "variability_sources": "Random sampling of training examples to form prompts (which examples are included); prompt composition stochasticity across sampled prompts",
            "variability_measured": true,
            "variability_metrics": "Average accuracy across sampled prompts, standard deviation of accuracy across sampled prompts, worst-case (minimum) accuracy across sampled prompts",
            "variability_results": "Main evaluation sampled 50 prompts per selected subset and reported mean ± std and minimum accuracy; e.g., GPTJ-6B All baseline on SST-2: 77.8% average accuracy with std 11.2 and min 50.8; CondAcc on SST-2: 86.7% ± 5.9 (min 68.2). Overall CondAcc and Datamodels improve average accuracy over All by 7.7% and 6.3%, respectively.",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Comparison of mean/std/min accuracy across 50 sampled prompts; overlap counts of selected stable examples between models; Pearson correlation of example scores between model pairs",
            "reproducibility_results": "Curating a stable subset reduces variance and improves worst-case accuracy (example: GPTJ-6B SST-2 std reduced from 11.2 to 5.9 and min improved from 50.8 to 68.2 with CondAcc). Overlap of stable examples between LLM pairs is limited (e.g., GPTJ-6B vs OPT-13B SST-2 overlap 4 examples; Pearson corr of example scores 0.15).",
            "reproducibility_challenges": "High sensitivity of ICL to which examples are sampled into prompts; order/composition dependence; label-pattern induced bias; model-to-model differences in which examples are 'stable'",
            "mitigation_methods": "Curating a stable training subset and then randomly sampling prompts from that subset (CondAcc and DATAMODELS); balancing label presence in prompts; ensuring each sampled prompt contains at least one example per class; calibration (Calib) as baseline",
            "mitigation_effectiveness": "Curated subsets (CondAcc/DATAMODELS) increased average accuracy (CondAcc +7.7% avg vs All; Datamodels +6.3% avg) and reduced std-dev and raised worst-case accuracy on many tasks (see SST-2 example above).",
            "comparison_with_without_controls": true,
            "number_of_runs": "50 sampled prompts per selected subset for main evaluation; dev collection D_ICL contains ~40k–100k prompts depending on task (authors used ~50k prompts commonly); datamodels test set: 5000 unseen prompts; authors note at least ~10,000 sampled prompts are needed for stable estimates.",
            "key_findings": "Random prompt sampling induces large stochastic variability in ICL performance; careful curation of a small stable subset substantially reduces variance and improves both mean and worst-case accuracy across models and tasks.",
            "uuid": "e634.0",
            "source_info": {
                "paper_title": "Data Curation Alone Can Stabilize In-context Learning",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "Prompt-order sensitivity",
            "name_full": "Sensitivity of ICL to the order (positioning) of examples within the prompt",
            "brief_description": "ICL performance depends not only on which examples are included but also on their order in the prompt; the paper models and exploits index effects in the DATAMODELS method by learning per-index example weights.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPTJ-6B, OPT-13B (main); also GPT-Neo-2.7B, OPT-6.7B",
            "model_size": "6B, 13B (also 2.7B, 6.7B)",
            "scientific_domain": "NLP (few-shot in-context learning)",
            "experimental_task": "Estimating how example presence at each index affects LLM logits/margins and using that to score and select stable examples",
            "variability_sources": "Ordering of examples within the prompt (index effects); interactions between examples depending on their positional index",
            "variability_measured": true,
            "variability_metrics": "Datamodel weights per (example, index) and aggregated count of positive weights; prediction error of datamodels (Pearson correlation and L1 distance between predicted and actual LLM margins)",
            "variability_results": "Datamodels approximated LLM outcomes well: Table 6 reports per-task average Pearson correlations (e.g., GPTJ-6B SST-2 corr 0.962, L1 distance 0.133; OPT-13B SST-2 corr 0.930, L1 0.264), indicating index and example-existence features capture much of ordering effects.",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Accuracy of datamodel predictions on unseen prompts (Pearson correlation, L1 distance); downstream effect measured by improved stability (mean/std/min) when selecting examples using datamodel scores",
            "reproducibility_results": "Datamodel-based selection yields improved average and worst-case accuracy (e.g., GPTJ-6B average across tasks 72.4% with Datamodels vs 65.2% All), demonstrating that modeling order effects helps reproducibility of ICL performance.",
            "reproducibility_challenges": "Complex combinatorial space of orders (K! per prompt), requiring many sampled prompts to estimate position-specific influences; computational cost of collecting enough prompt statistics.",
            "mitigation_methods": "Fit linear datamodels with features for (example existence, example index) and aggregate positive per-index weights to score examples; two-phase datamodel training that buckets by label-pattern to capture label-proportion effects.",
            "mitigation_effectiveness": "Datamodels provided informative weights and produced subsets that reduce variance and improve mean/worst-case accuracy (average improvement vs All ~+6.3%), and datamodel predictions have high correlation with true LLM outcomes (up to ~0.96).",
            "comparison_with_without_controls": true,
            "number_of_runs": "Datamodels trained on M prompts from D_ICL (M typically tens of thousands; authors used ~40k–100k prompts per setup); evaluated on 5000 unseen prompt outcomes.",
            "key_findings": "Index (order) effects are important for ICL; a simple linear datamodel using example presence and index can accurately predict LLM outcome margins and be used to select stable examples that reduce stochasticity.",
            "uuid": "e634.1",
            "source_info": {
                "paper_title": "Data Curation Alone Can Stabilize In-context Learning",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "Label-pattern / single-label bias",
            "name_full": "Bias induced by the proportion and pattern of in-context labels (label-pattern bias)",
            "brief_description": "The paper documents that prompts with skewed label patterns (e.g., all examples with same label) bias LLM predictions and that selected stable examples can mitigate this bias even when prompts contain only one label.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPTJ-6B, OPT-13B (main)",
            "model_size": "6B, 13B",
            "scientific_domain": "NLP (few-shot classification)",
            "experimental_task": "Assessing performance when prompts contain only a single label (single-label prompts) and evaluating whether curated subsets still produce informative signals",
            "variability_sources": "Label proportions and label-pattern (which labels and how many of each appear in the K-shot prompt) that bias the model toward certain outputs",
            "variability_measured": true,
            "variability_metrics": "Average accuracy and standard deviation for prompts with specific label patterns (e.g., [0,0,0,0] or [1,1,1,1]) across 50 sampled prompts",
            "variability_results": "Baselines All and TopPrompts performed near chance in many single-label prompt setups, while CondAcc and Datamodels single-label prompts substantially outperformed majority guessing; example: GPTJ-6B SST-2 with [0,0,0,0] All = 51.7%±1.7, CondAcc = 61.8%±4.9, Datamodels = 72.8%±4.9.",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Mean/std across 50 single-label prompts; comparisons across selection methods and models",
            "reproducibility_results": "Selected subsets contain examples that convey task-defining signals robust to label-pattern-induced bias; single-label prompts sampled from CondAcc/Datamodels give well-above-chance accuracy, indicating improved reproducibility of task behavior under extreme label patterns.",
            "reproducibility_challenges": "LLMs can be heavily biased by label mixes in context, making many prompts non-informative; ensuring fair evaluation requires controlling label-pattern coverage in collected prompts.",
            "mitigation_methods": "Selecting stable examples that convey task definition (CondAcc/Datamodels); during main evaluation ensure every sampled prompt contains at least one example per class to avoid trivial label bias.",
            "mitigation_effectiveness": "Curated subsets enabled above-majority performance even under single-label prompts (examples above show improvements of ~10–20 percentage points over All).",
            "comparison_with_without_controls": true,
            "number_of_runs": "50 sampled single-label prompts per subset for evaluation",
            "key_findings": "Careful example selection can produce prompts that still convey task structure even if all in-context labels are identical, thereby reducing label-pattern-induced variability.",
            "uuid": "e634.2",
            "source_info": {
                "paper_title": "Data Curation Alone Can Stabilize In-context Learning",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "CondAcc",
            "name_full": "Conditional Accuracy (CondAcc) example scoring and subset selection",
            "brief_description": "A selection method introduced in this paper that scores a training example by its expected dev-set ICL accuracy conditioned on the example appearing in a randomly sampled prompt, and selects top-scoring examples per class to form a stable subset.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPTJ-6B, OPT-13B (main); also GPT-Neo-2.7B, OPT-6.7B",
            "model_size": "6B, 13B (also 2.7B, 6.7B)",
            "scientific_domain": "NLP (few-shot ICL)",
            "experimental_task": "Score training examples by conditional dev-set accuracy over many sampled prompts (D_ICL) and select a balanced set of E examples to reduce ICL variance",
            "variability_sources": "Variation caused by choosing different in-context examples; CondAcc directly targets this source by estimating per-example average effect",
            "variability_measured": true,
            "variability_metrics": "Average accuracy, standard deviation, and minimum accuracy across 50 sampled prompts from the selected subset; number of gold-labeled examples selected in unlabeled setup",
            "variability_results": "CondAcc-selected subsets reduce variance and increase mean/worst-case accuracy: averaged across tasks CondAcc outperforms All by +7.7% (average accuracy). Example: GPTJ-6B SST-2 All: 77.8%±11.2 (min 50.8) -&gt; CondAcc: 86.7%±5.9 (min 68.2).",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Same mean/std/min reported across multiple random prompt draws; performance on out-of-distribution tasks and unlabeled setups",
            "reproducibility_results": "CondAcc generalizes to OOD tasks and unlabeled setups (Un-CondAcc sometimes outperforms All); e.g., Un-CondAcc outperforms All by 5.7% average accuracy and Un-All by 10.2%.",
            "reproducibility_challenges": "Requires many sampled prompts (authors used ~10k–50k+) to obtain reliable conditional accuracy estimates; high GPU compute/memory cost for collecting D_ICL.",
            "mitigation_methods": "Estimate conditional accuracy by running ICL on dev set across a large D_ICL (many prompts) and selecting top E' examples per class with highest conditional accuracy; ensure each example is sampled multiple times in D_ICL.",
            "mitigation_effectiveness": "Quantitative gains: average accuracy improvements vs All (CondAcc +7.7%); reduced std-dev and increased worst-case accuracy on multiple tasks and models; Un-CondAcc can outperform labeled All in some tasks.",
            "comparison_with_without_controls": true,
            "number_of_runs": "D_ICL sizes ~40k–100k prompts used to estimate s_ca(i); evaluation uses 50 sampled test prompts per selected subset; authors note at least ~10,000 prompts needed.",
            "key_findings": "A simple, direct data-curation method (CondAcc) that scores examples by their conditional dev-set accuracy can substantially stabilize ICL performance, improving mean and worst-case metrics across tasks and models.",
            "uuid": "e634.3",
            "source_info": {
                "paper_title": "Data Curation Alone Can Stabilize In-context Learning",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "DATAMODELS",
            "name_full": "Datamodels-based selection (linear datamodels over example-existence and index)",
            "brief_description": "An adapted datamodels approach that fits linear regressors predicting LLM outcome margins from features indicating which example appears at which index; examples with many positive weights across dev datamodels are selected for a stable subset.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPTJ-6B, OPT-13B (main); also GPT-Neo-2.7B, OPT-6.7B",
            "model_size": "6B, 13B (also 2.7B, 6.7B)",
            "scientific_domain": "NLP (few-shot ICL)",
            "experimental_task": "Train linear datamodels per-dev-example to predict LLM margin outcomes for prompts and aggregate per-example positive weights across indices and dev examples to score examples for subset selection",
            "variability_sources": "Example presence and positional index interactions in prompts; label-pattern effects (handled by bucketing label patterns during two-phase datamodel training)",
            "variability_measured": true,
            "variability_metrics": "Datamodel predictive accuracy: Pearson correlation and L1 distance between predicted and true LLM margins on 5000 unseen prompts; downstream mean/std/min accuracy across 50 sampled prompts from selected subset",
            "variability_results": "Datamodels predictions correlate highly with LLM outcomes (Table 6: e.g., GPTJ-6B SST-2 corr 0.962 L1 0.133; other tasks corr range ~0.89–0.96). Datamodels-based selection improved average accuracy over All by ~6.3% on average and reduced variance on many tasks.",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Pearson correlation and L1 between datamodel predictions and LLM outcomes; downstream stability metrics (mean/std/min) for ICL",
            "reproducibility_results": "High datamodel-to-LLM correlations indicate datamodels can reliably approximate outcome variability; selected subsets from Datamodels achieve improved mean and often reduced std-dev relative to All (e.g., average across tasks for GPTJ-6B with Datamodels = 72.4% vs All = 65.2%).",
            "reproducibility_challenges": "Need large D_ICL to train datamodels; must account for label-pattern variety (authors use two-phase training and bucket by label pattern); computational cost is significant.",
            "mitigation_methods": "Fit linear datamodels using features (example id, index) per dev example; aggregate positive weights; use two-phase training with label-pattern bucketing; select top E' per class.",
            "mitigation_effectiveness": "Datamodels provided strong, measurable stability gains (average accuracy improvements ~6.3% vs All) and produced accurate approximations of LLM outcome variability (correlations up to ~0.96).",
            "comparison_with_without_controls": true,
            "number_of_runs": "Trained on M prompts from D_ICL (M tens of thousands); evaluated datamodels on 5000 unseen prompts; main ICL evaluations use 50 sampled prompts per selected subset.",
            "key_findings": "A simple linear datamodel that encodes example existence and index can accurately predict LLM outcome variability and be used to select stable in-context examples that reduce stochasticity in ICL.",
            "uuid": "e634.4",
            "source_info": {
                "paper_title": "Data Curation Alone Can Stabilize In-context Learning",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "Evaluation protocol & metrics",
            "name_full": "Evaluation protocol and reproducibility / variability metrics used in the experiments",
            "brief_description": "The paper's evaluation protocol quantifies variability by sampling multiple prompts from a (selected) subset and reporting average accuracy, standard deviation, and worst-case (minimum) accuracy; it also assesses datamodel predictive fidelity via Pearson correlation and L1 distance.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPTJ-6B, OPT-13B (main); other LLMs included in ablations",
            "model_size": "6B, 13B (others 2.7B, 6.7B)",
            "scientific_domain": "NLP (few-shot ICL)",
            "experimental_task": "Stability evaluation of ICL subsets and datamodel predictive accuracy",
            "variability_sources": "Stochastic prompt sampling, ordering, label-patterns, model differences",
            "variability_measured": true,
            "variability_metrics": "Primary: mean accuracy, standard deviation (std) across 50 sampled prompts, minimum (worst-case) accuracy across sampled prompts; Secondary: Pearson correlation and L1 distance for datamodel predictions; overlap counts and Pearson correlation between example scores across models",
            "variability_results": "Main results reported as mean_{std} and min in tables; e.g., GPTJ-6B All on SST-2 77.8_{11.2} (min 50.8); CondAcc 86.7_{5.9} (min 68.2). Datamodels correlation values in Table 6 range ~0.89–0.96 with L1 distances 0.088–0.340 depending on task/model.",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "See above; reproducibility also assessed via OOD experiments and unlabeled setups (Un-CondAcc) comparing methods and baselines",
            "reproducibility_results": "Quantitative improvements in mean and worst-case performance when using curated subsets; Un-CondAcc sometimes outperforms All, indicating selection can improve reproducibility even without gold labels.",
            "reproducibility_challenges": "Need sufficiently many sampled prompts to estimate statistics reliably (authors used tens of thousands for collection and 50 for reporting); heavy compute cost for collecting D_ICL.",
            "mitigation_methods": "Use of large D_ICL to estimate statistics; evaluate with multiple sampled prompts (50) and report mean/std/min; datamodel-based prediction checks (5000 unseen prompts) to validate modeling assumptions.",
            "mitigation_effectiveness": "Protocol allowed reliable detection of variance reduction and mean improvement from curated subsets; datamodel prediction metrics indicate modeling assumptions are valid (high corr, low L1).",
            "comparison_with_without_controls": true,
            "number_of_runs": "50 sampled prompts per subset for main test-set reporting; D_ICL collection sizes ~40k–100k prompts; datamodels evaluated on 5000 unseen prompts.",
            "key_findings": "Robust stability evaluation requires multiple sampled prompts and reporting of mean, standard deviation, and worst-case accuracy; datamodel predictive metrics (Pearson/L1) are effective to validate surrogate models of LLM variability.",
            "uuid": "e634.5",
            "source_info": {
                "paper_title": "Data Curation Alone Can Stabilize In-context Learning",
                "publication_date_yy_mm": "2022-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "What makes good in-context examples for gpt-3?",
            "rating": 2
        },
        {
            "paper_title": "Calibrate before use: Improving few-shot performance of language models",
            "rating": 2
        },
        {
            "paper_title": "Datamodels: Predicting predictions from training data",
            "rating": 2
        },
        {
            "paper_title": "Data Shapley: Equitable valuation of data for machine learning",
            "rating": 2
        },
        {
            "paper_title": "In-context example selection with influences",
            "rating": 2
        },
        {
            "paper_title": "Rethinking the role of demonstrations: What makes in-context learning work?",
            "rating": 1
        }
    ],
    "cost": 0.01927375,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Data Curation Alone Can Stabilize In-context Learning</h1>
<p>Ting-Yun Chang and Robin Jia<br>University of Southern California<br>{tingyun, robinjia}@usc.edu</p>
<h4>Abstract</h4>
<p>In-context learning (ICL) enables large language models (LLMs) to perform new tasks by prompting them with a sequence of training examples. However, it is known that ICL is very sensitive to the choice of training examples: randomly sampling examples from a training set leads to high variance in performance. In this paper, we show that carefully curating a subset of training data greatly stabilizes ICL performance without any other changes to the ICL algorithm (e.g., prompt retrieval or calibration). We introduce two methods to choose training subsets-both score training examples individually, then select the highest-scoring ones. CondAcc scores a training example by its average dev-set ICL accuracy when combined with random training examples, while DATAMODELS learns linear regressors that estimate how the presence of each training example influences LLM outputs. Across five tasks and two LLMs, sampling from stable subsets selected by CondAcc and Datamodels improves average accuracy over sampling from the entire training set by $7.7 \%$ and $6.3 \%$, respectively. Surprisingly, the stable subset examples are not especially diverse in content or low in perplexity, in contrast with other work suggesting that diversity and perplexity are important when prompting LLMs.</p>
<h2>1 Introduction</h2>
<p>In-context learning (ICL) is a new paradigm for few-shot learning with pretrained large language models (LLMs) without any parameter updates. In ICL, an LLM can perform a new task simply by conditioning on a prompt ${ }^{1}$ consisting of a sequence of labeled training examples. First introduced by GPT-3 (Brown et al., 2020), ICL with LLMs has reached state-of-the-art few-shot performance across many tasks (Rae et al., 2021; Smith et al.,</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: 4-shot ICL performance of GPTJ on SST2. Each boxplot summarizes the results of 50 sampled prompts. Compared with baselines (blue), our methods (pink) can greatly stablilize performance, having higher average accuracy (red diamonds) and lower variance.</p>
<p>2022; Thoppilan et al., 2022; Chowdhery et al., 2022). Compared with alternatives that use finetuning (Devlin et al., 2018; Schick and Schütze, 2020; Gao et al., 2020), ICL does not require taskspecific training, which enables its use with very large language models, and it uses a unified model for all tasks, enabling easier deployment.</p>
<p>Despite its impressive few-shot performance, ICL often exhibits unintuitive behavior (Min et al., 2022). The standard ICL approach is to randomly sample a few examples from a training set to construct a prompt (Brown et al., 2020); however, prior work (Liu et al., 2021; Zhao et al., 2021; Lu et al., 2021) has found that ICL is very sensitive to the choice of training examples and their order in the prompt. ICL is also sensitive to small changes in prompt format (Chen et al., 2022).</p>
<p>In this paper, we show that carefully curating a smaller training dataset from a larger pool can make ICL much more stable. We define a training subset $\mathcal{E}$ to be stable if randomly sampling a sequence of examples as a prompt from $\mathcal{E}$ yields much higher average and worst-case accuracy than randomly sampling from the original training set. We propose two methods to identify such a sta-</p>
<p>ble subset. Our CondAcc method scores a training example by its average dev-set ICL accuracy when combined with random training examples; these scores are closely related to Data Shapley values (Ghorbani and Zou, 2019). Our DATAMODELS method fits a linear regressor that predicts the LLM's output based on which example is present at each index in the prompt; we score a training example highly if the associated weights from the linear model indicate that its presence improves accuracy. For both methods, we then select training examples with the highest scores to form the stable subset. While some prior work improves ICL accuracy by retrieving a suitable prompt for each test example (Liu et al., 2021; Rubin et al., 2021; Su et al., 2022), we show that it is possible to achieve stably good accuracy with a randomly sampled prompt for all test examples, when given the "right" training (sub)set.</p>
<p>Our subset selection methods greatly improve performance across 5 classification datasets and 4 LLMs, with main experiments on GPTJ-6B (Wang and Komatsuzaki, 2021) and OPT-13B (Zhang et al., 2022a). On average, CondAcc and DATAMODELS outperform the baseline that uses the entire training set without selection (named All) by $7.7 \%$ and $6.3 \%$, respectively, when comparing the average accuracy over multiple sampled prompts. In contrast, baselines that choose examples found in high-performing prompts (TopPrompts) or examples that lead to high one-shot ICL accuracy (ONESHOT) to form the subsets do not perform as well (see Figure 1). Our stable subset examples generalize to out-of-distribution test data, and we can even find stable subsets for binary classification tasks that only contain examples of one label; these findings suggest that the stable subset examples help LLMs understand the overall task definition.</p>
<p>Finally, we study what makes stable subset examples special by analyzing sequence length, perplexity, and diversity in both raw text and embedding spaces. We find that these examples do not have abnormally long sequence lengths or high perplexities. In contrast with prior work optimizing diversity for prompt selection (Su et al., 2022; Ye et al., 2022), we find our stable subsets no more diverse than random subsets of the training data. In summary, we show that curating training data appropriately leads to more stable and accurate ICL performance; we hope future work can develop new strategies for writing such helpful ex-
amples. Code and data are publicly available at https://github.com/terarachang/DataICL.</p>
<h2>2 Problem Setups</h2>
<p>We use the original ICL formulation proposed by GPT-3, also known as the direct method, for all our experiments. Specifically, an LLM performs in-context learning on a new task based on a taskspecific prompt $\mathcal{Z}$ formed by concatenating $K$ labeled training examples, i.e., $\mathcal{Z}=\left[z_{1}, \ldots, z_{K}\right]$, where each $z_{j}$ is a training example $(x, y)$ consisting of an input $x$ and label $y$. The LLM then makes predictions on a test input $x_{\text {test }}$ conditioned on the prompt $\mathcal{Z}$ followed by $x_{\text {test }}$, denoted by $\arg \max <em _test="{test" _text="\text">{y \in \mathcal{C}} P(y \mid \mathcal{Z}, x</em>$ is the set of possible labels.}})$, where $\mathcal{C</p>
<p>Given a training set $D_{\text {tr }}$, a dev set $D_{\text {dev }}$, a heldout test set $D_{\text {test }}$, and a predefined number of shots $K$, our goal is to select a stable training subset $\mathcal{E} \subset D_{\text {tr }},|\mathcal{E}|&gt;K$, such that randomly sampling a sequence of $K$ examples from $\mathcal{E}$ to form a prompt generally yields good performance on $D_{\text {test }}$.</p>
<p>We propose two setups, Labeled and Unlabeled. In Labeled, our goal is to study which training examples consistently lead to high ICL accuracy. We assume access to a large labeled $D_{\mathrm{tr}}^{L}=\left{\left(x_{i}, y_{i}\right)\right}<em _mathrm_b="\mathrm{b">{i=1}^{N</em>\right}}}}$ of input-label pair, and a small labeled $D_{\text {dev }}$. Unlabeled is closer to the true fewshot learning setup (Perez et al., 2021), where we only have access to $D_{\text {dev }}$ and a large unlabeled training set $\left{x_{i<em _mathrm_b="\mathrm{b">{i=1}^{N</em>\right}\right}}}}$. We pair each input $x_{i}$ with every possible label $y \in \mathcal{C}$ to create an unlabeled training set $D_{\mathrm{tr}}^{U}=\left{\left{(x_{i}, \tilde{y}) \mid \tilde{y} \in \mathcal{C<em _mathrm_b="\mathrm{b">{i=1}^{N</em>$ for evaluating our selection methods.}}}$. In both setups, our goal is to select a subset of $E$ training examples from either $D_{\mathrm{tr}}^{L}$ or $D_{\mathrm{tr}}^{U}$. Note that we may select examples with incorrect labels under Unlabeled. We only use the large labeled $D_{\text {test }</p>
<h2>3 Methods</h2>
<p>We propose the following steps to identify a stable subset of $E$ training examples:</p>
<ol>
<li>Construct $\mathcal{D}<em _text="\text" _tr="{tr">{\text {ICL }}$, a large set of $M$ prompts, each consisting of $K$ examples randomly sampled from $D</em>$. We then run ICL on the dev set $M$ times given different prompts.}</li>
<li>Estimate the value of each training example based on the results of Step 1. We propose two methods to do this: CONDA CC (\$3.1) and DATAMODELS (\$3.2).</li>
</ol>
<p><img alt="img-1.jpeg" src="img-1.jpeg" />
stable subset consists of the highest-scroing examples
Figure 2: An overview of our CONDACC method, which scores training examples individually using its average accuracy (red diamonds) when combined with other random training examples. Each boxplot summarizes the dev-set accuracies conditioned on a training example appearing in the sampled prompts.
3. Select training examples with the highest importance scores per class to make up $\mathcal{E}(\$ 3.3)$.</p>
<h3>3.1 CondAcc</h3>
<p>We hypothesize that a good training example leads to higher accuracy, on average, when occurs in a prompt. Given a prompt $\mathcal{Z}$, we denote its dev set ICL accuracy as $\operatorname{Acc}(\mathcal{Z})$. Thus, a simple way to score the $i$-th training example $\left(x_{i}, y_{i}\right) \in D_{\text {tr }}$ is to calculate the expected accuracy conditioned on this example appearing in a prompt $\mathcal{Z}$ from $\mathcal{D}_{\text {ICL }}$ :</p>
<p>$$
\boldsymbol{s}<em _mathcal_Z="\mathcal{Z">{\mathrm{ca}}(i)=\mathbb{E}</em>} \sim \mathcal{D<em i="i">{\mathrm{ICL}}}\left[\operatorname{Acc}(\mathcal{Z}) \mid\left(x</em>\right]
$$}, y_{i}\right) \in \mathcal{Z</p>
<p>We ensure that each training example occurs in $\mathcal{D}_{\text {ICL }}$ multiple times in different orders and with different examples.</p>
<p>In Appendix A.1, we show that Eq. 1 is similar to Data Shapley value (Ghorbani and Zou, 2019), where we define the valuation function of Data Shapley on subsets of $K$ training examples since we focus on $K$-shot ICL.</p>
<h3>3.2 Datamodels</h3>
<p>The CONDACC method does not consider the order of training examples, which has a great impact on ICL performance. Also, it takes a simple average over the dev set, ignoring the LLM's confidence in individual dev examples. Thus, we propose another data valuation method that leverages Datamodels (Ilyas et al., 2022) for ICL.</p>
<p>Ilyas et al. (2022) study a complex target model's behavior in terms of the training data by replacing it with a linear, easy-to-analyze, proxy model called a datamodel. Specifically, given a subset of training data $\mathcal{S}$, a datamodel predicts the outcome of a
test input $\bar{x}$ if training the target model on $\mathcal{S}$. The outcome $f(\bar{x} ; \mathcal{S})$ is defined as the margin of the correct class, i.e., the logit for the correct class minus the highest incorrect logit. To train datamodels, Ilyas et al. (2022) first create a dataset consisting of subset-outcome pairs $(\mathcal{S}, f(\bar{x} ; \mathcal{S}))$, requiring training the target model from scratch multiple times on different subsets to obtain the outcomes. In our work, the target model is a pretrained LLM, and is inference-only during ICL. Thus, our dataset collection only requires inference of the LLM multiple times with different prompts in $\mathcal{D}_{\text {ICL }}$.</p>
<p>In particular, given a prompt $\mathcal{Z}=\left[z_{1}, \ldots, z_{K}\right]$, we run ICL on a dev example $(\bar{x}, \bar{y})$ and define an LLM's outcome as $f(\bar{x} ; \mathcal{Z})=o(\bar{y} \mid \mathcal{Z}, \bar{x})-$ $\max <em _mathrm_s="\mathrm{s">{y^{\prime} \in \mathcal{C}, y^{\prime} \neq \bar{y}} o\left(y^{\prime} \mid \mathcal{Z}, \bar{x}\right)$, where $o(y \mid \mathcal{Z}, \bar{x})$ is the output logit of the LLM on label $y$ before softmax. We hypothesize that we can approximate an LLM's outcome with a linear regressor taking two simple features: the existence of a training example and its index in $\mathcal{Z}$, which we consider to be the most important factors in ICL. Our linear datamodel is parameterized by weights $w \in \mathbb{R}^{N</em>$, with mean-squared error:}} \times K}$ and bias $b \in \mathbb{R}$; we use $w(i, j) \in \mathbb{R}$ to denote the weight for the $i$-th training example appearing at index $j$. For each dev example $(\bar{x}, \bar{y}) \in D_{\text {dev }}$, we train a datamodel $g_{w, b}$ to predict the LLM's outcome of $\bar{x}$, $f(\bar{x} ; \mathcal{Z}) \in \mathbb{R</p>
<p>$$
\begin{gathered}
g_{w, b}(\bar{x} ; \mathcal{Z})=\sum_{j=1}^{K} w\left(\operatorname{id}\left(z_{j}\right), j\right)+b \
\min <em n="1">{w, b} \frac{1}{M} \sum</em>}^{M}\left(g_{w, b}\left(\bar{x} ; \mathcal{Z<em n="n">{n}\right)-f\left(\bar{x} ; \mathcal{Z}</em>
\end{gathered}
$$}\right)\right)^{2</p>
<p>where $z_{j}$ is the training example at index $j$ in the</p>
<p>prompt $\mathcal{Z}$, and $\operatorname{id}(\cdot)$ maps $z_{j}$ back to its example ID $i$ in the training set, i.e., $z_{j}=\left(x_{i}, y_{i}\right)$.</p>
<p>By definition, $f(\bar{x} ; \mathcal{Z})&gt;0$ if the LLM is correct on $\bar{x}$. Hence, a positive $w(i, j)$ indicates that having the $i$-th training example at index $j$ in the prompt encourages answering correctly. We hypothesize that a good training example should have a beneficial effect for many dev examples, regardless of its index in the prompts. Thus, we can aggregate the datamodels of all dev examples and marginalize over all possible orders to calculate the score of the $i$-th training example:</p>
<p>$$
\boldsymbol{s}<em _bar_x="(\bar{x">{d m}(i)=\sum</em>(i, j)&gt;0\right}
$$}, \bar{y}) \in D_{\text {dev }}} \sum_{j=1}^{K} \mathbb{1}\left{w_{\bar{x}</p>
<p>where $w_{\bar{x}}(i, j)$ is the weight value $w(i, j)$ of the datamodel for $\bar{x}$. Calculating the total number of positive weights empirically works better than averaging the weights of all the datamodels.</p>
<p>In Appendix A.3, we validate our hypothesis that we can linearize an LLM's outcomes with two simple features. Table 6 shows that our datamodels can accurately approximate the outcomes of unseen prompts outside of $\mathcal{D}_{\text {ICL }}$ across different tasks.</p>
<h3>3.3 Select Training Examples</h3>
<p>Now that we assign a score for each training example (Eq. 1, 4), let $C$ be the number of classes and $E^{\prime}=E / C$, we select the top- $E^{\prime}$ training examples of each class with the highest scores to form the stable subset $\mathcal{E} \subset D_{\mathrm{tr}},|\mathcal{E}|=E$.</p>
<h2>4 Experiment</h2>
<h3>4.1 Setups</h3>
<p>Tasks. We experiment on 5 classification tasks: SST-2 (Socher et al., 2013), BoolQ (Clark et al., 2019), Subj (Pang and Lee, 2004), Scicite (Cohan et al., 2019), and AGNews (Zhang et al., 2015). We set the stable training subset size $E=20$ for all the tasks. For binary classification tasks, we set $K=4$ and do not balance the classes in the prompts. Thus, the collected $\mathcal{D}_{\text {ICL }}$ for a binary task covers all $2^{4}$ label patterns, including prompts with all positive ( $[1,1,1,1])$ and all negative ( $[0,0,0,0])$ labels, allowing us to better understand the impact of label patterns on ICL. For multiclass tasks (Scicite and AGNews), we balance the classes, sampling a training example per class to form the prompts. Table 8 in the appendix summarizes our setups.</p>
<p>Data Splits. For all the tasks, we use classbalanced $D_{\text {tr }}, D_{\text {dev }}$, and $D_{\text {test }}$ sampled from the original training set, as we do not have the gold labels of the original test set. We choose $\left|D_{\text {tr }}\right|=$ 1000 to ensure a diverse range of training examples for subset selection, and $\left|D_{\text {test }}\right|=1000$ for reliable evaluation. $D_{\text {dev }}$ consists of 50 examples per class. All three sets are balanced, randomly sampled from the original training set, and do not overlap.</p>
<p>Models. We run our main experiments with two LLMs: GPTJ-6B (Wang and Komatsuzaki, 2021) and OPT-13B (Zhang et al., 2022a). More experiments on GPT-Neo-2.7B (Black et al., 2021) and OPT-6.7B can be found in Table 9 in the appendix, where our methods also work well.</p>
<h3>4.2 Evaluation and Baselines</h3>
<p>Recall that our goal is to select a training subset $\mathcal{E} \subset D_{\text {tr }}$ that is more stable than $D_{\text {tr }}$. To evaluate a selection method, we randomly sample 50 prompts from the selected subset $\mathcal{E}$, run ICL on the test set $D_{\text {test }}$, and report the average accuracy, standard deviation, and worst accuracy.</p>
<p>As shown in Zhao et al. (2021), when a prompt only contains examples of a single label, LLMs are prone to always predict that label on every test example. Thus, in our main experiments (§5.1), we ensure that every sampled prompt contains at least one example from every class. In $\S 5.2$, we separately investigate performance when the prompt contains only one label of binary tasks. We split the selected subset $\mathcal{E}$ into two subsets, $\mathcal{E}<em 1="1">{0}$ and $\mathcal{E}</em>}$, where $\mathcal{E<em 1="1">{0}$ only contains negative training examples and $\mathcal{E}</em>}$ only contains positive examples. We then sample 50 prompts from $\mathcal{E<em 1="1">{0}$ and $\mathcal{E}</em>$, respectively.</p>
<p>Besides the two proposed selection methods, CondAcc and Datamodels, we design 5 baseline methods: All, Calib, Random, OneShot, and TopPrompts. All uses the entire training set as $\mathcal{E}$. Calib uses the same prompts as All, but with calibration to prevent LLMs biased toward certain labels, where we closely follow the implementation of Zhao et al. (2021). RANDOM randomly selects a balanced training subset of $E=20$ examples. OneShot first runs ICL with $K=1$, using each training example alone as the prompt, and then scores the example by the corresponding ICL accuracy on $D_{\text {dev }}$; these scores are used in the same way as our main methods to select examples (§3.3). OneShot tests if we can extrapolate ICL performance from $K=1$ to $K&gt;1$. Top-</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">SST-2</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">BoolQ</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Subj</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Scicite</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">AGNews</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Avg. <br> Tasks</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Avg std</td>
<td style="text-align: center;">Min</td>
<td style="text-align: center;">Avg std</td>
<td style="text-align: center;">Min</td>
<td style="text-align: center;">Avg std</td>
<td style="text-align: center;">Min</td>
<td style="text-align: center;">Avg std</td>
<td style="text-align: center;">Min</td>
<td style="text-align: center;">Avg std</td>
<td style="text-align: center;">Min</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GPTJ-6B</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">All</td>
<td style="text-align: center;">$77.8_{11.2}$</td>
<td style="text-align: center;">50.8</td>
<td style="text-align: center;">$61.0_{3.8}$</td>
<td style="text-align: center;">49.7</td>
<td style="text-align: center;">$59.8_{8.3}$</td>
<td style="text-align: center;">50.1</td>
<td style="text-align: center;">$43.8_{7.2}$</td>
<td style="text-align: center;">33.6</td>
<td style="text-align: center;">$83.5_{3.8}$</td>
<td style="text-align: center;">70.4</td>
<td style="text-align: center;">65.2</td>
</tr>
<tr>
<td style="text-align: center;">+ CALIB</td>
<td style="text-align: center;">$75.5_{9.5}$</td>
<td style="text-align: center;">53.6</td>
<td style="text-align: center;">$61.2_{3.9}$</td>
<td style="text-align: center;">50.4</td>
<td style="text-align: center;">$70.4_{7.7}$</td>
<td style="text-align: center;">55.7</td>
<td style="text-align: center;">$35.4_{2.6}$</td>
<td style="text-align: center;">32.8</td>
<td style="text-align: center;">$85.2_{2.7}$</td>
<td style="text-align: center;">78.0</td>
<td style="text-align: center;">65.5</td>
</tr>
<tr>
<td style="text-align: center;">RANDOM</td>
<td style="text-align: center;">$74.6_{11.4}$</td>
<td style="text-align: center;">50.3</td>
<td style="text-align: center;">$60.0_{4.3}$</td>
<td style="text-align: center;">49.5</td>
<td style="text-align: center;">$59.9_{10.4}$</td>
<td style="text-align: center;">50.1</td>
<td style="text-align: center;">$46.4_{6.9}$</td>
<td style="text-align: center;">35.5</td>
<td style="text-align: center;">$82.5_{4.7}$</td>
<td style="text-align: center;">67.1</td>
<td style="text-align: center;">64.7</td>
</tr>
<tr>
<td style="text-align: center;">ONESHOT</td>
<td style="text-align: center;">$79.6_{10.5}$</td>
<td style="text-align: center;">52.1</td>
<td style="text-align: center;">$63.8_{2.7}$</td>
<td style="text-align: center;">56.4</td>
<td style="text-align: center;">$63.3_{10.1}$</td>
<td style="text-align: center;">50.1</td>
<td style="text-align: center;">$44.8_{5.9}$</td>
<td style="text-align: center;">33.8</td>
<td style="text-align: center;">$83.3_{3.4}$</td>
<td style="text-align: center;">71.9</td>
<td style="text-align: center;">67.0</td>
</tr>
<tr>
<td style="text-align: center;">TopPrompts-5</td>
<td style="text-align: center;">$82.8_{8.6}$</td>
<td style="text-align: center;">56.0</td>
<td style="text-align: center;">$62.3_{3.0}$</td>
<td style="text-align: center;">54.3</td>
<td style="text-align: center;">$65.5_{9.7}$</td>
<td style="text-align: center;">50.1</td>
<td style="text-align: center;">$50.4_{6.0}$</td>
<td style="text-align: center;">36.9</td>
<td style="text-align: center;">$84.4_{3.3}$</td>
<td style="text-align: center;">74.3</td>
<td style="text-align: center;">69.1</td>
</tr>
<tr>
<td style="text-align: center;">TopPrompts-10</td>
<td style="text-align: center;">$78.5_{9.3}$</td>
<td style="text-align: center;">52.4</td>
<td style="text-align: center;">$61.2_{4.0}$</td>
<td style="text-align: center;">51.1</td>
<td style="text-align: center;">$65.1_{10.7}$</td>
<td style="text-align: center;">50.1</td>
<td style="text-align: center;">$49.4_{5.5}$</td>
<td style="text-align: center;">36.2</td>
<td style="text-align: center;">$85.4_{2.4}$</td>
<td style="text-align: center;">76.3</td>
<td style="text-align: center;">67.9</td>
</tr>
<tr>
<td style="text-align: center;">CondAcc</td>
<td style="text-align: center;">86.7 ${ }_{5.9}$</td>
<td style="text-align: center;">68.2</td>
<td style="text-align: center;">$65.1_{1.6}$</td>
<td style="text-align: center;">61.1</td>
<td style="text-align: center;">70.5 $5_{10.4}$</td>
<td style="text-align: center;">50.2</td>
<td style="text-align: center;">$52.3_{4.4}$</td>
<td style="text-align: center;">42.0</td>
<td style="text-align: center;">87.3 $3_{2.6}$</td>
<td style="text-align: center;">70.5</td>
<td style="text-align: center;">72.4</td>
</tr>
<tr>
<td style="text-align: center;">DATAMODELS</td>
<td style="text-align: center;">$86.0_{7.5}$</td>
<td style="text-align: center;">60.8</td>
<td style="text-align: center;">$65.2_{6.9}$</td>
<td style="text-align: center;">63.4</td>
<td style="text-align: center;">$69.4_{10.7}$</td>
<td style="text-align: center;">50.4</td>
<td style="text-align: center;">$54.5_{3.8}$</td>
<td style="text-align: center;">43.9</td>
<td style="text-align: center;">$86.9_{1.4}$</td>
<td style="text-align: center;">82.8</td>
<td style="text-align: center;">72.4</td>
</tr>
<tr>
<td style="text-align: center;">Un-All</td>
<td style="text-align: center;">$71.0_{11.9}$</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">$60.8_{3.5}$</td>
<td style="text-align: center;">49.6</td>
<td style="text-align: center;">$60.1_{8.8}$</td>
<td style="text-align: center;">50.1</td>
<td style="text-align: center;">$42.0_{7.6}$</td>
<td style="text-align: center;">33.5</td>
<td style="text-align: center;">$75.1_{9.9}$</td>
<td style="text-align: center;">46.5</td>
<td style="text-align: center;">61.8</td>
</tr>
<tr>
<td style="text-align: center;">Un-ONESHOT</td>
<td style="text-align: center;">$81.9_{6.3}$</td>
<td style="text-align: center;">68.5</td>
<td style="text-align: center;">$62.6_{3.3}$</td>
<td style="text-align: center;">55.6</td>
<td style="text-align: center;">$61.0_{8.7}$</td>
<td style="text-align: center;">50.1</td>
<td style="text-align: center;">$43.5_{6.7}$</td>
<td style="text-align: center;">33.4</td>
<td style="text-align: center;">$78.1_{4.2}$</td>
<td style="text-align: center;">69.8</td>
<td style="text-align: center;">65.4</td>
</tr>
<tr>
<td style="text-align: center;">Un-TOPPrompts-5</td>
<td style="text-align: center;">$80.1_{10.5}$</td>
<td style="text-align: center;">56.8</td>
<td style="text-align: center;">$61.2_{3.3}$</td>
<td style="text-align: center;">51.9</td>
<td style="text-align: center;">$60.7_{10.0}$</td>
<td style="text-align: center;">50.1</td>
<td style="text-align: center;">$48.7_{6.9}$</td>
<td style="text-align: center;">33.0</td>
<td style="text-align: center;">$76.4_{9.8}$</td>
<td style="text-align: center;">53.0</td>
<td style="text-align: center;">65.4</td>
</tr>
<tr>
<td style="text-align: center;">Un-CONDAcc</td>
<td style="text-align: center;">85.3 $3_{6.8}$</td>
<td style="text-align: center;">60.5</td>
<td style="text-align: center;">$63.7_{2.2}$</td>
<td style="text-align: center;">56.0</td>
<td style="text-align: center;">$66.0_{10.6}$</td>
<td style="text-align: center;">50.1</td>
<td style="text-align: center;">$54.2_{3.4}$</td>
<td style="text-align: center;">45.9</td>
<td style="text-align: center;">87.1 $1.1$</td>
<td style="text-align: center;">84.6</td>
<td style="text-align: center;">71.3</td>
</tr>
<tr>
<td style="text-align: center;">OPT-13B</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">All</td>
<td style="text-align: center;">$68.5_{14.0}$</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">$65.2_{5.6}$</td>
<td style="text-align: center;">49.7</td>
<td style="text-align: center;">$60.9_{10.2}$</td>
<td style="text-align: center;">49.8</td>
<td style="text-align: center;">$42.8_{3.6}$</td>
<td style="text-align: center;">35.0</td>
<td style="text-align: center;">$81.6_{5.9}$</td>
<td style="text-align: center;">64.2</td>
<td style="text-align: center;">63.8</td>
</tr>
<tr>
<td style="text-align: center;">+ CALIB</td>
<td style="text-align: center;">84.7 $7_{6.8}$</td>
<td style="text-align: center;">51.7</td>
<td style="text-align: center;">$65.5_{4.9}$</td>
<td style="text-align: center;">51.8</td>
<td style="text-align: center;">$63.7_{8.9}$</td>
<td style="text-align: center;">47.9</td>
<td style="text-align: center;">$35.5_{1.8}$</td>
<td style="text-align: center;">31.2</td>
<td style="text-align: center;">$81.8_{4.1}$</td>
<td style="text-align: center;">70.7</td>
<td style="text-align: center;">66.2</td>
</tr>
<tr>
<td style="text-align: center;">RANDOM</td>
<td style="text-align: center;">$67.7_{14.1}$</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">$64.7_{6.4}$</td>
<td style="text-align: center;">49.3</td>
<td style="text-align: center;">$61.2_{9.5}$</td>
<td style="text-align: center;">49.9</td>
<td style="text-align: center;">$41.2_{4.6}$</td>
<td style="text-align: center;">33.3</td>
<td style="text-align: center;">$78.0_{7.5}$</td>
<td style="text-align: center;">61.4</td>
<td style="text-align: center;">62.6</td>
</tr>
<tr>
<td style="text-align: center;">OneShot</td>
<td style="text-align: center;">$75.6_{13.1}$</td>
<td style="text-align: center;">50.7</td>
<td style="text-align: center;">$68.3_{2.3}$</td>
<td style="text-align: center;">62.7</td>
<td style="text-align: center;">$60.5_{9.9}$</td>
<td style="text-align: center;">49.9</td>
<td style="text-align: center;">$41.9_{3.8}$</td>
<td style="text-align: center;">33.4</td>
<td style="text-align: center;">$84.2_{2.9}$</td>
<td style="text-align: center;">73.1</td>
<td style="text-align: center;">66.1</td>
</tr>
<tr>
<td style="text-align: center;">TopPrompts-5</td>
<td style="text-align: center;">$69.6_{14.7}$</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">$63.5_{6.3}$</td>
<td style="text-align: center;">51.0</td>
<td style="text-align: center;">$67.4_{12.7}$</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">$45.9_{4.3}$</td>
<td style="text-align: center;">36.0</td>
<td style="text-align: center;">$83.9_{3.1}$</td>
<td style="text-align: center;">74.0</td>
<td style="text-align: center;">66.1</td>
</tr>
<tr>
<td style="text-align: center;">TopPrompts-10</td>
<td style="text-align: center;">$72.9_{15.6}$</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">$65.5_{5.2}$</td>
<td style="text-align: center;">50.4</td>
<td style="text-align: center;">$68.5_{13.4}$</td>
<td style="text-align: center;">49.9</td>
<td style="text-align: center;">$44.6_{3.9}$</td>
<td style="text-align: center;">36.7</td>
<td style="text-align: center;">$84.4_{3.5}$</td>
<td style="text-align: center;">70.9</td>
<td style="text-align: center;">67.2</td>
</tr>
<tr>
<td style="text-align: center;">CondAcc</td>
<td style="text-align: center;">$83.6_{9.1}$</td>
<td style="text-align: center;">56.1</td>
<td style="text-align: center;">$69.4_{2.1}$</td>
<td style="text-align: center;">62.8</td>
<td style="text-align: center;">70.6 $1_{11.9}$</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">$49.4_{3.3}$</td>
<td style="text-align: center;">41.1</td>
<td style="text-align: center;">87.0 $1.0$</td>
<td style="text-align: center;">83.6</td>
<td style="text-align: center;">72.0</td>
</tr>
<tr>
<td style="text-align: center;">Datamodels</td>
<td style="text-align: center;">$81.3_{10.3}$</td>
<td style="text-align: center;">60.3</td>
<td style="text-align: center;">$69.3_{3.8}$</td>
<td style="text-align: center;">57.3</td>
<td style="text-align: center;">$63.0_{9.4}$</td>
<td style="text-align: center;">50.1</td>
<td style="text-align: center;">$46.3_{3.9}$</td>
<td style="text-align: center;">37.4</td>
<td style="text-align: center;">$85.7_{1.7}$</td>
<td style="text-align: center;">81.8</td>
<td style="text-align: center;">69.1</td>
</tr>
<tr>
<td style="text-align: center;">Un-All</td>
<td style="text-align: center;">$61.6_{13.6}$</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">$64.8_{5.3}$</td>
<td style="text-align: center;">49.3</td>
<td style="text-align: center;">$55.8_{8.9}$</td>
<td style="text-align: center;">35.6</td>
<td style="text-align: center;">$41.9_{3.6}$</td>
<td style="text-align: center;">35.7</td>
<td style="text-align: center;">$67.3_{17.2}$</td>
<td style="text-align: center;">26.4</td>
<td style="text-align: center;">58.3</td>
</tr>
<tr>
<td style="text-align: center;">Un-ONESHOT</td>
<td style="text-align: center;">$74.8_{15.6}$</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">$68.0_{2.5}$</td>
<td style="text-align: center;">59.8</td>
<td style="text-align: center;">$54.8_{6.2}$</td>
<td style="text-align: center;">47.1</td>
<td style="text-align: center;">$41.5_{4.1}$</td>
<td style="text-align: center;">33.7</td>
<td style="text-align: center;">$82.3_{4.5}$</td>
<td style="text-align: center;">64.9</td>
<td style="text-align: center;">64.3</td>
</tr>
<tr>
<td style="text-align: center;">Un-TOPPrompts-5</td>
<td style="text-align: center;">$70.5_{17.0}$</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">$66.2_{3.4}$</td>
<td style="text-align: center;">54.6</td>
<td style="text-align: center;">$63.4_{12.3}$</td>
<td style="text-align: center;">48.3</td>
<td style="text-align: center;">$45.7_{4.7}$</td>
<td style="text-align: center;">33.6</td>
<td style="text-align: center;">$81.8_{6.9}$</td>
<td style="text-align: center;">51.8</td>
<td style="text-align: center;">65.5</td>
</tr>
<tr>
<td style="text-align: center;">Un-CONDAcc</td>
<td style="text-align: center;">80.3 $12.8$</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">$69.0_{2.6}$</td>
<td style="text-align: center;">61.5</td>
<td style="text-align: center;">$63.7_{11.7}$</td>
<td style="text-align: center;">49.9</td>
<td style="text-align: center;">$48.1_{4.0}$</td>
<td style="text-align: center;">39.2</td>
<td style="text-align: center;">$84.6_{3.1}$</td>
<td style="text-align: center;">72.5</td>
<td style="text-align: center;">69.2</td>
</tr>
</tbody>
</table>
<p>Table 1: Main results with different selection methods. The last column average accuracies across all tasks. Overall, the proposed methods CondAcc and Datamodels perform the best. Our method under the unlabeled setup (Un-CondAcc) even outperforms the All baseline that uses gold labels.</p>
<p>Prompts-5 and TopPrompts-10 select the union of the examples from the top- ${5,10}$ prompts in $\mathcal{D}_{\mathrm{ICL}}$ with the highest dev set accuracy, where the subsets contain at most $K \times 5$ and $K \times 10$ examples, respectively. Finally, we apply baselines and our CondAcc method to the unlabeled setup (§2), named with the Un- prefix.</p>
<h2>5 Results</h2>
<h3>5.1 Main Results</h3>
<p>The proposed methods outperform all baselines. Table 1 shows the test set accuracy with different training subset selection methods. Among methods without calibration, our CondAcc and DatamodELS methods are the most stable, achieving substantially higher average and worst-case accuracy across all tasks, and lower variances on most tasks. Compared with CALIB, our methods perform better in 8/10 setups. Overall, CondAcc and DatamodELS outperform the no-selection baseline All by $7.7 \%$ and $6.3 \%$ on average, respectively.</p>
<p>TopPrompts is the strongest baseline. Within the baselines, All and RANDOM have similar performance. Applying calibration improves the worst accuracy on most tasks and the average accuracy on some tasks, but is not always beneficial, especially on Scicite. OneShot outperforms All and RANDOM on SST-2 and BoolQ, but performs similarly on other tasks, indicating that we cannot extrapolate ICL behavior from $K=1$ to $K=3$ or $K=4$. TopPrompts-5 and TopPrompts-10 are the strongest baselines, performing especially well on SST-2, Subj, and Scicite, showing that the training examples that compose the highestaccuracy prompts are more stable than others.</p>
<p>Our method works without training set labels. Randomly sampling prompts from the unlabeled training set (Un-ALL) underperforms sampling from the original labeled training set (ALL), especially on SST-2 and AGNews. This shows that gold labels do matter in ICL in general, in contrast with the findings of Min et al. (2022). However, apply-</p>
<p>ing our selection method to the unlabeled training set (Un-CondAcc) surprisingly outperforms not only Un-All but All (which uses correctly labeled examples), although some selected training examples actually have the wrong labels, implying that having gold-labeled prompts is not necessary for ICL. Other baselines, Un-ONESHOT and Un-TopPrompts, outperform Un-All but substantially underperform our method. Overall, UnCONDACC outperforms baselines Un-All and All by $10.2 \%$ and $5.7 \%$ on average, respectively.</p>
<p>Does Un-CondAcc benfit from gold labels? We study the number of the stable training examples selected by Un-CondAcc that indeed have gold labels. In most of the tasks, the numbers are much higher than the expected numbers by majority guess, where BoolQ is the exception with half of the selected examples having wrong labels. We thus study if we can achieve even better results by correcting those selected examples that have wrong labels with their gold labels; the other correct examples in the subset remain unchanged. After the label correction on BoolQ, the average and worst accuracy drops by $1.9 \%$ and $4.5 \%$ respectively on GPTJ, $0.4 \%$ and $5.7 \%$ respectively on OPT. These results again suggest that on the one hand, ICL benefits from gold-labeled examples in most cases; on the other hand, some training examples with wrong labels can surprisingly achieve better performance. The full results are in Table 10 in the appendix.</p>
<p>Finally, we study the alternative that uses more shots in A.8. Table 12 shows that using 4 curated examples (CONDAcc) can outperform $K=24,16$ randomly sampled ones in SST-2 and AGNews.</p>
<h3>5.2 Single-Label Prompts</h3>
<p>We now evaluate whether it is possible to achieve good accuracy while only using training examples of a single class in a prompt (See $\S 4.2$ for more details). Table 2 compares the results of different methods. First, the baselines All and TopPROMPTS perform near chance in most cases, as the LLMs are biased by the prompts to predict the same label on every test example. In contrast, single-label prompts sampled from the subsets of CondAcc and Datamodels substantially outperform majority guessing across all setups. We conclude that the selected training examples are beneficial because they help LLMs understand the overall definition of the task. Thus, even when used in single-label prompts, they can still give LLMs</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">SST-2</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">BoolQ</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">$[0,0,0,0]$</td>
<td style="text-align: center;">$[1,1,1,1]$</td>
<td style="text-align: center;">$[0,0,0,0]$</td>
<td style="text-align: center;">$[1,1,1,1]$</td>
</tr>
<tr>
<td style="text-align: left;">GPTJ-6B</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">All</td>
<td style="text-align: center;">$51.7_{1.7}$</td>
<td style="text-align: center;">$52.6_{3.1}$</td>
<td style="text-align: center;">$52.8_{1.6}$</td>
<td style="text-align: center;">$50.7_{1.0}$</td>
</tr>
<tr>
<td style="text-align: left;">TopPrompts</td>
<td style="text-align: center;">$52.1_{2.1}$</td>
<td style="text-align: center;">$56.0_{3.7}$</td>
<td style="text-align: center;">$54.2_{1.9}$</td>
<td style="text-align: center;">$51.8_{1.6}$</td>
</tr>
<tr>
<td style="text-align: left;">CondAcc</td>
<td style="text-align: center;">$61.8_{4.9}$</td>
<td style="text-align: center;">$60.3_{2.8}$</td>
<td style="text-align: center;">$58.3_{2.1}$</td>
<td style="text-align: center;">$55.5_{1.7}$</td>
</tr>
<tr>
<td style="text-align: left;">Datamodels</td>
<td style="text-align: center;">$\mathbf{7 2 . 8}_{4.9}$</td>
<td style="text-align: center;">$\mathbf{6 8 . 4}_{4.9}$</td>
<td style="text-align: center;">$\mathbf{6 1 . 7}_{1.6}$</td>
<td style="text-align: center;">$\mathbf{5 6 . 9}_{2.2}$</td>
</tr>
<tr>
<td style="text-align: left;">OPT-13B</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">All</td>
<td style="text-align: center;">$54.0_{4.1}$</td>
<td style="text-align: center;">$73.3_{7.4}$</td>
<td style="text-align: center;">$52.4_{2.6}$</td>
<td style="text-align: center;">$51.2_{1.7}$</td>
</tr>
<tr>
<td style="text-align: left;">TopPrompts</td>
<td style="text-align: center;">$53.0_{3.0}$</td>
<td style="text-align: center;">$76.5_{6.9}$</td>
<td style="text-align: center;">$53.0_{3.2}$</td>
<td style="text-align: center;">$51.4_{2.1}$</td>
</tr>
<tr>
<td style="text-align: left;">CondAcc</td>
<td style="text-align: center;">$\mathbf{6 6 . 3}_{3.7}$</td>
<td style="text-align: center;">$81.0_{4.6}$</td>
<td style="text-align: center;">$65.3_{2.9}$</td>
<td style="text-align: center;">$53.7_{1.9}$</td>
</tr>
<tr>
<td style="text-align: left;">Datamodels</td>
<td style="text-align: center;">$63.9_{4.2}$</td>
<td style="text-align: center;">$\mathbf{8 4 . 5}_{2.6}$</td>
<td style="text-align: center;">$\mathbf{6 9 . 2}_{2.0}$</td>
<td style="text-align: center;">$\mathbf{6 0 . 1}_{2.2}$</td>
</tr>
</tbody>
</table>
<p>Table 2: Results of single-labeled prompts with different selection methods. Each prompt consists of 4 training examples with the same labels.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">OOD Tasks</th>
<th style="text-align: center;">IMDb</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">BoolQ Cst.</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Avg std</td>
<td style="text-align: center;">Min</td>
<td style="text-align: center;">Avg std</td>
<td style="text-align: center;">Min</td>
</tr>
<tr>
<td style="text-align: center;">GPTJ-6B</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">All</td>
<td style="text-align: center;">$86.5_{5.7}$</td>
<td style="text-align: center;">63.6</td>
<td style="text-align: center;">$56.6_{3.0}$</td>
<td style="text-align: center;">50.1</td>
</tr>
<tr>
<td style="text-align: center;">TopPrompts</td>
<td style="text-align: center;">$87.2_{5.2}$</td>
<td style="text-align: center;">63.0</td>
<td style="text-align: center;">$56.7_{2.6}$</td>
<td style="text-align: center;">49.9</td>
</tr>
<tr>
<td style="text-align: center;">CondAcc</td>
<td style="text-align: center;">$90.5_{1.8}$</td>
<td style="text-align: center;">84.8</td>
<td style="text-align: center;">$58.9_{1.7}$</td>
<td style="text-align: center;">54.6</td>
</tr>
<tr>
<td style="text-align: center;">Datamodels</td>
<td style="text-align: center;">$\mathbf{9 1 . 6}_{1.5}$</td>
<td style="text-align: center;">84.0</td>
<td style="text-align: center;">$57.6_{1.9}$</td>
<td style="text-align: center;">54.0</td>
</tr>
<tr>
<td style="text-align: center;">OPT-13B</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">All</td>
<td style="text-align: center;">$79.2_{12.1}$</td>
<td style="text-align: center;">50.1</td>
<td style="text-align: center;">$59.8_{2.9}$</td>
<td style="text-align: center;">51.6</td>
</tr>
<tr>
<td style="text-align: center;">TopPrompts</td>
<td style="text-align: center;">$80.5_{14.0}$</td>
<td style="text-align: center;">50.8</td>
<td style="text-align: center;">$60.3_{3.5}$</td>
<td style="text-align: center;">51.0</td>
</tr>
<tr>
<td style="text-align: center;">CondAcc</td>
<td style="text-align: center;">$83.5_{10.8}$</td>
<td style="text-align: center;">54.6</td>
<td style="text-align: center;">$60.1_{2.1}$</td>
<td style="text-align: center;">56.7</td>
</tr>
<tr>
<td style="text-align: center;">Datamodels</td>
<td style="text-align: center;">$\mathbf{8 4 . 1}_{9.3}$</td>
<td style="text-align: center;">58.9</td>
<td style="text-align: center;">$\mathbf{6 0 . 6}_{3.3}$</td>
<td style="text-align: center;">54.3</td>
</tr>
</tbody>
</table>
<p>Table 3: Accuracy of IMDb and BoolQ Contrast Set, where the prompts consist of the selected SST-2 and BoolQ training examples, respectively.
useful signal to perform the desired task.</p>
<h3>5.3 Out-of-Distribution Tasks</h3>
<p>We further evaluate on out-of-distribution (OOD) tasks, where there is a distribution shift between prompts and test data. Specifically, we apply our selection methods on a source task, sampling 50 prompts from the selected subsets of the source task as done in the main experiments, and then evaluate on test data of a target task. We use SST-2 and BoolQ as the source tasks, and IMDb (Maas et al., 2011) and BoolQ Contrast Set (Gardner et al., 2020) as our target tasks, respectively. Table 3 shows that our CondAcc and DatamodELS methods still outperform baselines on OOD tasks, especially on IMDb, implying that instead of simply overfitting the source tasks, the selected stable examples are indeed task-level examples that can generalize to OOD test data.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Accuracy versus sequence length (left) and accuracy versus perplexity (right). Each dot corresponds to a training example. Examples in good subsets are not outliers with abnormally long lengths or high perplexities.</p>
<h2>6 Analysis</h2>
<p>We further analyze what makes the selected training examples special along different dimensions: sequence length, perplexity, and diversity. We compare good with bad training examples, where we use our CondAcc method to identify a bad (resp., good) subset by selecting $E^{\prime}$ training examples with the lowest (resp., highest) importance scores in each class (§3.3). Please refer to Table 11 in the appendix for the full results of the bad subset.</p>
<h3>6.1 Sequence Length and Perplexity</h3>
<p>In Figure 3, we plot the accuracy against either sequence length or perplexity, where each dot corresponds to a training example. Here, the accuracy (Y-axis) is the importance score $s_{\text {est }}$ assigned to each training example by CondAcc in Eq. 1, i.e., the average dev-set accuracy when that example is combined with random other training examples in a random order.</p>
<p>Sequence Length. The first two subfigures show that while the bad examples (red dots) span across different sequence lengths, the good examples (blue dots) do not cover the tail distribution of long sequences. We observe little correlation between accuracy and sequence length across different tasks and LLMs (see more in Figure 6), except for a slightly negative correlation when the sequence length is very long, suggesting that abnormally long training examples can hurt ICL performance.</p>
<p>Perplexity. We calculate the perplexity of the inputs of training examples with respect to the same LLMs we run ICL on. Figure 3 shows that good examples are not outliers that have high perplexity. This could suggest future work filter out examples that have extraordinarily high perplexity in the training set before running ICL, and could be combined with active learning for ICL (Zhang et al., 2022b; Su et al., 2022), as we only need the unlabeled inputs to calculate perplexity. However, we observe no correlation between accuracy and perplexity across all the tasks and LLMs (Figure 7), implying that using perplexity alone is not enough for identifying good training examples. Our findings are inconsistent with concurrent work (Gonen et al., 2022), which shows that lower prompt perplexity strongly correlates with better performance, probably because Gonen et al. (2022) focus on perplexities under different instructions, while we focus on the differences between training inputs.</p>
<h3>6.2 Diversity</h3>
<p>DIV-I and DIV-F. We measure the diversity of a training subset with DIV-I (Yuan et al., 2020) and DIV-F (Zhdanov, 2019) metrics, following prior work in active learning. DIV-I measures diversity in raw text, while DIV-F measures diversity in a feature space. For DIV-F, we use SentBERT (Reimers and Gurevych, 2019) to encode the inputs of training examples into sentence embeddings, following Su et al. (2022). We compare the selected good subset and bad subset with 5000 randomly sampled subsets $\subset D_{\text {tr }}$, each containing $E^{\prime}$ training examples per class. Figure 4 shows that good subsets (blue dots) sometimes have low diversity scores in both metrics, especially on BoolQ and AGNews. Overall, they do not seem to be more diverse than randomly sampled subsets. Our findings are different from Su et al. (2022), which finds that diverse training subsets are better for prompt retrieval. We hypothesize that diversity matters more when retrieving similar training examples for each test input, but is less important when using a single fixed prompt. On the other hand, the bad subsets have much higher DIV-I scores than random subsets across different tasks, because they often include examples with long sequence lengths (see Figure 3), covering more distinct unigrams. However, in the SentBERT feature space, the bad subsets are often less diverse than random subsets.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Different ways to visualize the diversity of examples. (a) and (b) compare the diversity of the <em>good</em> subset, <em>bad</em> subset, and randomly sampled subsets (boxplot). For both DIV-I and DIV-F, a higher number means a subset is more diverse. Overall, <em>good</em> subsets are no more diverse than random subsets. (c) visualizes the stable training examples selected by CondAcc and Datamodels methods in Datamodels embeddings space, where each dot is a training example in AGNews. Both methods choose tightly cluttered examples instead of diverse ones.</p>
<p><strong>Datamodels Embeddings.</strong> In §3.2, we learn a datamodel for each dev example in $D_{\text{dev}}$. Here, we concatenate the weights assigned on a training example learned by all the datamodels, creating an embedding $\in \mathbb{R}^{|D_{\text{dev}}| \times K}$ for each training example. We then project the embeddings of the entire training set to a two-dimensional space with UMAP (McInnes et al., 2018) for visualization. Figure 4 and Figure 8 in the appendix show that both CondAcc and Datamodels choose tightly clustered sets of examples in the embedding space, instead of diverse ones scattering over the training set. Moreover, the two methods actually select similar examples in the embedding space, although having very different scoring methods.</p>
<h3>6.3 Do LLMs find the same stable examples?</h3>
<p>We further study if the identified stable subset examples are transferable across different LLMs. Given two LLMs, we calculate the Pearson correlation coefficient between their example scores assigned by CondAcc (§3.1) and the actual number of overlapped examples in the stable subsets of the two LLMs. Table 4 shows the mixed results: some pairs have moderate correlation, especially when both LLMs are from the OPT family; however, we find little correlation between many pairs and there are only a few overlapped examples between the stable subsets identified by different LLMs.</p>
<p>Interestingly, when using the four stable SST2 examples shared by GPTJ-6B and OPT-13B as the prompt, evaluating all 4! permutations, we achieve very high test accuracy: 88.6 ± 3.7 on GPTJ, 89.8 ± 3.0 on OPT-6.7B, and 87.3 ± 5.2 on OPT-13B. This may indicate that there exist successful factors of training examples shared among LLMs. We include the four stable examples in</p>
<table>
<thead>
<tr>
<th></th>
<th>Corr</th>
<th>Overlap</th>
</tr>
</thead>
<tbody>
<tr>
<td><em>GPTJ-6B vs OPT-13B</em></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>SST-2</strong></td>
<td>0.15</td>
<td>4</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>AGNews</strong></td>
<td>0.46</td>
<td>1</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>BoolQ</strong></td>
<td>0.41</td>
<td>2</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>Subj</strong></td>
<td>-0.03</td>
<td>2</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>Scicite</strong></td>
<td>0.02</td>
<td>2</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><em>GPTJ-6B vs OPT-6.7B</em></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>SST-2</strong></td>
<td>0.27</td>
<td>3</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>AGNews</strong></td>
<td>0.08</td>
<td>1</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><em>OPT-6.7B vs OPT-13B</em></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>SST-2</strong></td>
<td>0.76</td>
<td>3</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>AGNews</strong></td>
<td>0.42</td>
<td>1</td>
</tr>
</tbody>
</table>
<p>Table 4: Pearson correlation between the example scores (s_{cm}) of two LLMs and the number of overlapped examples in their stable subsets.</p>
<table>
<thead>
<tr>
<th>Examples</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Review: <em>k-19 : the widowmaker is a great yarn</em>.</td>
<td></td>
</tr>
<tr>
<td>Sentiment: positive</td>
<td></td>
</tr>
<tr>
<td>Review: <em>spiffy animated feature</em></td>
<td></td>
</tr>
<tr>
<td>Sentiment: positive</td>
<td></td>
</tr>
<tr>
<td>Review: <em>a plot cobbled together from largely flat and uncreative moments</em></td>
<td></td>
</tr>
<tr>
<td>Sentiment: negative</td>
<td></td>
</tr>
<tr>
<td>Review: <em>has the thrown-together feel of a summer-camp talent show : hastily written, underrehearsed, arbitrarily plotted and</em></td>
<td></td>
</tr>
<tr>
<td>Sentiment: negative</td>
<td></td>
</tr>
</tbody>
</table>
<p>Table 5: The four stable subset examples shared by GPTJ-6B and OPT-13B in SST-2 dataset, where the last example is also selected by OPT-6.7B. All three models achieve high average accuracy and low variance across the 4! permutations of these four examples.</p>
<p>Table 5 and hope future work can discover what distinguishes them from other examples.</p>
<h2>7 Discussion and Related Work</h2>
<h3>7.1 Prompt Retrieval</h3>
<p>The performance of ICL greatly depends on the choice of the prompt. A common way to do prompt selection is to retrieve the top- $K$ similar training examples for each test input (Liu et al., 2021; Rubin et al., 2021; Su et al., 2022), where the similarity is captured by sentence embeddings (Reimers and Gurevych, 2019; Robertson et al., 2009). Such instance-dependent prompt retrieval is critical for semantic parsing tasks (Rubin et al., 2021), as LLMs need to see relevant logical forms in the context to generate the appropriate predicates for the test example. In this paper, we focus on classification tasks and identify task-level training examples that work for all test examples, avoiding the retrieval process.</p>
<h3>7.2 The influence of in-context labels</h3>
<p>The proportions of labels appearing in context can greatly bias LLMs' predictions (Zhao et al., 2021). However, with careful prompt selection, Zhang et al. (2022b) finds that LLMs can perform well without observing the entire label space of a classification task in the prompt. In $\S 5.2$, we identify a subset, instead of just one prompt, of single-label examples that perform well. On the other hand, the correctness of in-context labels may not matter as much, as Min et al. (2022) find that randomly flipping them barely hurts performance. Kim et al. (2022) re-examine the importance of gold labels, showing it varies largely across tasks and experimental settings; our unlabeled experiments also show varying importance of gold labels across different tasks. Our method also identifies some training examples with wrong labels that can yield surprisingly good performance.</p>
<h3>7.3 Data Valuation</h3>
<p>Given a learning algorithm trained on a training set to produce a predictor, data valuation quantifies the value of each training example to the predictor performance. Prior work includes influence functions (Koh and Liang, 2017), Data Shapley (Ghorbani and Zou, 2019), DVRL (Yoon et al., 2020), TracIn (Pruthi et al., 2020), and Datamodels (Ilyas et al., 2022). Our setup also aims to attribute the performance of a predictor (in our case, an LLM) to each training example (in our case, in-context examples). Our CondAcc method closely resembles Data Shapley, and we adapt Ilyas et al. (2022)
as our DATAMODELS method. However, the main difference is that we are doing $K$-shot ICL, where training examples are used as prompts, and there are no parameter updates to LLMs.</p>
<p>In concurrent and independent work, Nguyen and Wong (2023) also propose methods based on Data Shapley and Datamodels to study the influence of training examples. The main differences are: (1) we adapt Datamodels to consider the positions of training examples, while Nguyen and Wong (2023) follow the original implementation and study example ordering with influence scores. (2) They propose a Perplexity baseline that selects examples according to individual perplexity, while we use perplexity in analysis to study the correlation between example perplexity and their average performance. (3) Nguyen and Wong (2023) explore a larger number of shots in ICL $(K \in[10,52])$, while we assume a few-shot setting and fix $K \in{3,4}$. Our findings on good training examples are consistent with each other: both papers find little correlation between performance and potential factors such as example length and perplexity. In general, our work demonstrates the importance of data curation on in-context examples, even in the unlabeled and OOD scenarios, while Nguyen and Wong (2023) focus more on developing influence-based example selection frameworks. Taken together, two papers present a comprehensive view of data valuation for in-context learning.</p>
<h2>8 Conclusion</h2>
<p>We propose two methods to identify stable training subsets for in-context learning, achieving substantially higher average and worst-case accuracy across different setups. Our CondAcc method is intuitive and easy-to-implement, while our DATAMODELS method provides informative weights that enable further analyses. The success of our methods implies that when provided with the "right" training set (in our case, a subset of 20 examples to randomly sample prompts from), ICL could be far less sensitive to the choice of training examples and their orders in a prompt. Our analyses on stable subsets find that they do not contain outliers with especially long sequence lengths or high perplexities, and are also no more diverse than random subsets of the training data. We hope our work is a step towards developing guidelines for finding or writing better training examples.</p>
<h2>Limitations</h2>
<p>The main limitation of our work is the high memory and computation cost. As both our methods estimate the importance of training examples based on the prompt-performance statistics, we first need to run in-context learning on the dev set multiple times with different prompts in $\mathcal{D}<em _ICL="{ICL" _text="\text">{\text {ICL }}$. Although ICL does not require any parameter updates, LLMs still require a large amount of memory footprint during inference, especially when the model size is large and the average sequence length is long. For each setup, our $\mathcal{D}</em>$. We also release our collected data of every setup in https://github.com/terarachang/DataICL to support future studies on ICL.}}$ contains around 50,000 prompts (see Table 8) and 50 dev examples per class, so the most expensive setup (running OPT13B on BoolQ) costs more than 500 GPU hours on an RTXA6000 GPU. Our preliminary study shows that the proposed methods need the statistics of at least 10,000 randomly sampled prompts to perform well. Future work may use search algorithms instead of random sampling during data collection to reduce the number of prompts in $\mathcal{D}_{\text {ICL }</p>
<p>In this paper, we only study classification tasks, for the sake of easy evaluation. Future work may study the influence of in-context examples in generation tasks under different evaluation metrics. Due to hardware constraints, we do not study LLMs of sizes over 13B, and we fix the number of shots and prompt templates for simplicity. In independent work, Nguyen and Wong (2023) complement these limitations of our paper, showing that similar approaches work well on larger models and a diverse number of shots for in-context example selection. Still, the influence of in-context examples for gigantic LLMs larger than 100B parameters has not been studied. Due to emergent abilities of LLMs (Wei et al., 2022), it is unclear whether our methods and findings would still apply when prompting these gigantic LLMs.</p>
<h2>Acknowledgements</h2>
<p>We thank Ameya Godbole, Johnny Wei, Wang Zhu, Albert Xu, Deqing Fu, Qinyuan Ye, the members of USC NLP, and our anonymous reviewers for valuable feedback on the paper. We thank Sameer Singh and Matt Gardner for helpful discussions. We thank Jesse Thomason for his support. This work was funded in part by gifts from Open Philanthropy and Google.</p>
<h2>References</h2>
<p>Akari Asai, Mohammadreza Salehi, Matthew E. Peters, and Hannaneh Hajishirzi. 2022. Attentional mixtures of soft prompt tuning for parameter-efficient multitask knowledge sharing. ArXiv, abs/2205.11961.</p>
<p>Stephen H. Bach, Victor Sanh, Zheng-Xin Yong, Albert Webson, Colin Raffel, Nihal V. Nayak, Abheesht Sharma, Taewoon Kim, M Saiful Bari, Thibault Fevry, Zaid Alyafeai, Manan Dey, Andrea Santilli, Zhiqing Sun, Srulik Ben-David, Canwen Xu, Gunjan Chhablani, Han Wang, Jason Alan Fries, Maged S. Al-shaibani, Shanya Sharma, Urmish Thakker, Khalid Almubarak, Xiangru Tang, Xiangru Tang, Mike Tian-Jian Jiang, and Alexander M. Rush. 2022. Promptsource: An integrated development environment and repository for natural language prompts.</p>
<p>Sid Black, Gao Leo, Phil Wang, Connor Leahy, and Stella Biderman. 2021. GPT-Neo: Large Scale Autoregressive Language Modeling with MeshTensorflow.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901.</p>
<p>Yanda Chen, Chen Zhao, Zhou Yu, Kathleen McKeown, and He He. 2022. On the relation between sensitivity and accuracy in in-context learning. arXiv preprint arXiv:2209.07661.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311.</p>
<p>Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416.</p>
<p>Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. 2019. Boolq: Exploring the surprising difficulty of natural yes/no questions. arXiv preprint arXiv:1905.10044.</p>
<p>Arman Cohan, Waleed Ammar, Madeleine van Zuylen, and Field Cady. 2019. Structural scaffolds for citation intent classification in scientific publications. In North American Chapter of the Association for Computational Linguistics.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference</p>
<p>of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.</p>
<p>Tianyu Gao, Adam Fisch, and Danqi Chen. 2020. Making pre-trained language models better few-shot learners. arXiv preprint arXiv:2012.15723.</p>
<p>Matt Gardner, Yoav Artzi, Victoria Basmova, Jonathan Berant, Ben Bogin, Sihao Chen, Pradeep Dasigi, Dheeru Dua, Yanai Elazar, Ananth Gottumukkala, et al. 2020. Evaluating models' local decision boundaries via contrast sets. arXiv preprint arXiv:2004.02709.</p>
<p>Amirata Ghorbani and James Zou. 2019. Data shapley: Equitable valuation of data for machine learning. In International Conference on Machine Learning, pages 2242-2251. PMLR.</p>
<p>Hila Gonen, Srini Iyer, Terra Blevins, Noah A Smith, and Luke Zettlemoyer. 2022. Demystifying prompts in language models via perplexity estimation. arXiv preprint arXiv:2212.04037.</p>
<p>Andrew Ilyas, Sung Min Park, Logan Engstrom, Guillaume Leclerc, and Aleksander Madry. 2022. Datamodels: Predicting predictions from training data. arXiv preprint arXiv:2202.00622.</p>
<p>Srinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru, Todor Mihaylov, Dániel Simig, Ping Yu, Kurt Shuster, Tianlu Wang, Qing Liu, Punit Singh Koura, et al. 2022. Opt-iml: Scaling language model instruction meta learning through the lens of generalization. arXiv preprint arXiv:2212.12017.</p>
<p>Junyeob Kim, Hyuhng Joon Kim, Hyunsoo Cho, Hwiyeol Jo, Sang-Woo Lee, Sang goo Lee, Kang Min Yoo, and Taeuk Kim. 2022. Ground-truth labels matter: A deeper look into input-label demonstrations. ArXiv, abs/2205.12685.</p>
<p>Pang Wei Koh and Percy Liang. 2017. Understanding black-box predictions via influence functions. In International conference on machine learning, pages 1885-1894. PMLR.</p>
<p>Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691.</p>
<p>Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2021. What makes good in-context examples for gpt-3? arXiv preprint arXiv:2101.06804.</p>
<p>Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2021. Fantastically ordered prompts and where to find them: Overcoming fewshot prompt order sensitivity. In Annual Meeting of the Association for Computational Linguistics.</p>
<p>Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. 2011. Learning word vectors for sentiment analysis.</p>
<p>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 142-150, Portland, Oregon, USA. Association for Computational Linguistics.</p>
<p>Leland McInnes, John Healy, and James Melville. 2018. Umap: Uniform manifold approximation and projection for dimension reduction. arXiv preprint arXiv:1802.03426.</p>
<p>Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022. Rethinking the role of demonstrations: What makes in-context learning work? arXiv preprint arXiv:2202.12837.</p>
<p>Tai Nguyen and Eric Wong. 2023. In-context example selection with influences. arXiv preprint arXiv:2302.11042.</p>
<p>Bo Pang and Lillian Lee. 2004. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In Annual Meeting of the Association for Computational Linguistics.</p>
<p>Ethan Perez, Douwe Kiela, and Kyunghyun Cho. 2021. True few-shot learning with language models. Advances in Neural Information Processing Systems, 34:11054-11070.</p>
<p>Garima Pruthi, Frederick Liu, Satyen Kale, and Mukund Sundararajan. 2020. Estimating training data influence by tracing gradient descent. Advances in Neural Information Processing Systems, 33:19920-19930.</p>
<p>Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. 2021. Scaling language models: Methods, analysis \&amp; insights from training gopher. arXiv preprint arXiv:2112.11446.</p>
<p>Nils Reimers and Iryna Gurevych. 2019. Sentence-bert: Sentence embeddings using siamese bert-networks. ArXiv, abs/1908.10084.</p>
<p>Stephen Robertson, Hugo Zaragoza, et al. 2009. The probabilistic relevance framework: Bm25 and beyond. Foundations and Trends ${ }^{\circledR}$ in Information Retrieval, 3(4):333-389.</p>
<p>Ohad Rubin, Jonathan Herzig, and Jonathan Berant. 2021. Learning to retrieve prompts for in-context learning. arXiv preprint arXiv:2112.08633.</p>
<p>Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. 2021. Multitask prompted training enables zero-shot task generalization. arXiv preprint arXiv:2110.08207.</p>
<p>Timo Schick and Hinrich Schütze. 2020. Exploiting cloze questions for few shot text classification and natural language inference. arXiv preprint arXiv:2001.07676.</p>
<p>Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, et al. 2022. Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model. arXiv preprint arXiv:2201.11990.</p>
<p>Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural language processing, pages $1631-1642$.</p>
<p>Hongjin Su, Jungo Kasai, Chen Henry Wu, Weijia Shi, Tianlu Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf, Luke Zettlemoyer, Noah A Smith, et al. 2022. Selective annotation makes language models better fewshot learners. arXiv preprint arXiv:2209.01975.</p>
<p>Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. 2022. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239.</p>
<p>Ben Wang and Aran Komatsuzaki. 2021. GPT-J6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/ mesh-transformer-jax.</p>
<p>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. 2022. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682.</p>
<p>Xi Ye, Srini Iyer, Asli Celikyilmaz, Ves Stoyanov, Greg Durrett, and Ramakanth Pasunuru. 2022. Complementary explanations for effective in-context learning. ArXiv, abs/2211.13892.</p>
<p>Jinsung Yoon, Sercan Arik, and Tomas Pfister. 2020. Data valuation using reinforcement learning. In International Conference on Machine Learning, pages 10842-10851. PMLR.</p>
<p>Michelle Yuan, Hsuan-Tien Lin, and Jordan BoydGraber. 2020. Cold-start active learning through self-supervised language modeling. arXiv preprint arXiv:2010.09535.</p>
<p>Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022a. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068.</p>
<p>Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015. Character-level convolutional networks for text classification. Advances in neural information processing systems, 28.</p>
<p>Yiming Zhang, Shi Feng, and Chenhao Tan. 2022b. Active example selection for in-context learning. ArXiv, abs/2211.04486.</p>
<p>Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate before use: Improving few-shot performance of language models. In International Conference on Machine Learning, pages 12697-12706. PMLR.</p>
<p>Fedor Zhdanov. 2019. Diverse mini-batch active learning. arXiv preprint arXiv:1901.05954.</p>
<h2>A Appendix</h2>
<h2>A. 1 Connection with Data Shapley</h2>
<p>Data Shapley has a valuation function $V(\mathcal{S})$ for any subset of training examples. We define $V(\mathcal{S})$ as the expected dev set ICL accuracy across all permutations of $\mathcal{S}$ if $|\mathcal{S}|=K$, and $V(\mathcal{S})=0$ otherwise since we focus on $K$-shot learning. Then, the Data Shapley value $s_{\text {shap }}(i)$ for the $i$-th training example $e_{i}=\left(x_{i}, y_{i}\right)$ is:</p>
<p>$$
\begin{aligned}
&amp; \mathbb{E}<em 1="1">{\left{z</em>\right}\right)\right] \
&amp; \quad-\mathbb{E}}, \ldots, z_{K-1}\right} \sim\binom{D_{\mathrm{B}} \backslash e_{i}}{K-1}}\left[V\left(\left{z_{1}, \ldots, z_{K-1}, e_{i<em 1="1">{\left{z</em>\right}\right)\right]
\end{aligned}
$$}, \ldots, z_{K}\right} \sim\binom{D_{\mathrm{B}} \backslash e_{i}}{K}}\left[V\left(\left{z_{1}, \ldots, z_{K</p>
<p>We claim that $s_{\text {shap }}(i)$ is a monotonically increasing (in fact, affine) function of $s_{c a}(i)$, thus establishing a very tight connection between CONDACC and Data Shapley.</p>
<p>First, note that the first term is exactly equal to $s_{c a}(i)$, the expected conditional accuracy when $e_{i}$ occurs in a prompt, since $V$ returns the expected accuracy over all orders of $\left{z_{1}, \ldots, z_{K-1}, e_{i}\right}$.</p>
<p>Similarly, the second term is the expected conditional accuracy when $e_{i}$ does not occur in a prompt. Denote this quantity as $t(i)$, and let $A$ denote the overall expected accuracy when randomly sampling a prompt. Since the probability of choosing a given example to be in a prompt is exactly $\frac{K}{N_{\mathrm{tr}}}$, we have</p>
<p>$$
\begin{aligned}
A &amp; =\frac{K}{N_{\mathrm{tr}}} s_{c a}(i)+\frac{N_{\mathrm{tr}}-K}{N_{\mathrm{tr}}} t(i) \
t(i) &amp; =\frac{N_{\mathrm{tr}}}{N_{\mathrm{tr}}-K}\left(A-\frac{K}{N_{\mathrm{tr}}} s_{c a}(i)\right) \
&amp; =\frac{N_{\mathrm{tr}}}{N_{\mathrm{tr}}-K} A-\frac{K}{N_{\mathrm{tr}}-K} s_{c a}(i)
\end{aligned}
$$</p>
<p>Now we can rewrite $s_{\text {shap }}(i)$ as follows:</p>
<p>$$
\begin{aligned}
&amp; s_{\text {shap }}(i)=s_{c a}(i)-t(i) \
&amp; =s_{c a}(i)-\left(\frac{N_{\mathrm{tr}}}{N_{\mathrm{tr}}-K} A-\frac{K}{N_{\mathrm{tr}}-K} s_{c a}(i)\right) \
&amp; =\frac{N_{\mathrm{tr}}}{N_{\mathrm{tr}}-K} A+\frac{N_{\mathrm{tr}}}{N_{\mathrm{tr}}-K} s_{c a}(i)
\end{aligned}
$$</p>
<p>Since $N_{\mathrm{tr}}, K$, and $A$ are all constants that do not depend on $i$, and $N_{\mathrm{tr}}, K&gt;0$, this establishes that $s_{\text {shap }}(i)$ is a monotonically increasing affine function of $s_{c a}(i)$.</p>
<h2>A. 2 Relation to Prompt Tuning</h2>
<p>At test time, our setup is similar to Prompt Tuning (Lester et al., 2021), where a fixed prompt for</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">GPTJ-6B</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">OPT-13B</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">$L_{1}$</td>
<td style="text-align: center;">Corr</td>
<td style="text-align: center;">$L_{1}$</td>
<td style="text-align: center;">Corr</td>
</tr>
<tr>
<td style="text-align: left;">SST2</td>
<td style="text-align: center;">0.133</td>
<td style="text-align: center;">0.962</td>
<td style="text-align: center;">0.264</td>
<td style="text-align: center;">0.930</td>
</tr>
<tr>
<td style="text-align: left;">Boolq</td>
<td style="text-align: center;">0.147</td>
<td style="text-align: center;">0.941</td>
<td style="text-align: center;">0.167</td>
<td style="text-align: center;">0.937</td>
</tr>
<tr>
<td style="text-align: left;">Subj</td>
<td style="text-align: center;">0.269</td>
<td style="text-align: center;">0.946</td>
<td style="text-align: center;">0.260</td>
<td style="text-align: center;">0.949</td>
</tr>
<tr>
<td style="text-align: left;">Scicite</td>
<td style="text-align: center;">0.088</td>
<td style="text-align: center;">0.937</td>
<td style="text-align: center;">0.151</td>
<td style="text-align: center;">0.938</td>
</tr>
<tr>
<td style="text-align: left;">AGNews</td>
<td style="text-align: center;">0.296</td>
<td style="text-align: center;">0.891</td>
<td style="text-align: center;">0.340</td>
<td style="text-align: center;">0.840</td>
</tr>
</tbody>
</table>
<p>Table 6: Test results of datamodels. Our datamodels can accurately predict LLMs' outcomes on unseen prompts, having high correlation and low $L_{1}$ distance to the ground-truth outcomes.
a task is prepended to the test inputs. In our case, a prompt is a sequence of training examples randomly drawn from the selected subset (fixed for all test examples). In Prompt Tuning, it is a set of continuous embeddings, called a soft prompt, learned through backpropagation.</p>
<p>At training time, however, Prompt Tuning needs to backpropagate through the LLM and thus is more memory-expensive and tends to suffer from training instability (Asai et al., 2022). In comparison, our method finds good prompts without accessing the LLM's parameters, which is a realistic setup as many LLMs only provide API access.</p>
<h2>A. 3 Evaluating Datamodels</h2>
<p>Recall that given a prompt $\mathcal{Z}$ and a dev example $(\bar{x}, \bar{y})$, a datamodel learns to approximate an LLM's outcome of $\bar{x}$ (§3.2). To evaluate how accurate our datamodels can approximate an LLM, we create a test set for datamodels $\mathcal{D}<em _mathrm_DM="\mathrm{DM">{\mathrm{DM}}$, consisting of 5000 pairs of newly sampled $\mathcal{Z}$ and the LLM's groundtruth outcomes of every dev example, where our sampling assures that every $\mathcal{Z}$ is made up of an unseen combination of training examples. We evaluate the learned datamodels on $\mathcal{D}</em>$ distance over all datamodels in Table 6. We also randomly sample 10,000 outcomes across all datamodels to visualize the ground truths against predictions in Figure 5.}}$, calculating the correlation and $L_{1}$ distance between the predicted outcomes and the ground-truth outcomes. More specifically, each datamodel yields 5000 outcomes, which we calculate the Pearson correlation coefficient with the ground-truth outcomes of the associated dev example. We report the average correlation and $L_{1</p>
<p>Overall, our datamodels can accurately approximate LLMs' outcomes across different tasks and</p>
<table>
<thead>
<tr>
<th>Task</th>
<th>Example</th>
<th>Label Mapping</th>
</tr>
</thead>
<tbody>
<tr>
<td>SST-2</td>
<td>Review: contains no wit , only labored gags <br> Sentiment: negative</td>
<td>negative/positive</td>
</tr>
<tr>
<td>BoolQ</td>
<td>Exercise: read the text and answer the question by yes or no. <br> Good Samaritan laws offer legal protection to people who give reasonable assistance... <br> Question: do good samaritan laws protect those who help at an accident? yes</td>
<td>no/yes</td>
</tr>
<tr>
<td>Subj</td>
<td>Input: the tucks have a secret , they 're immortal . <br> Type: objective</td>
<td>objective/subjective</td>
</tr>
<tr>
<td>Scicite</td>
<td>Is the following citation from a scientific paper describing a method, a result, or background? <br> However, how frataxin interacts with the Fe-S cluster biosynthesis components... <br> Answer: background</td>
<td>method/result/ background</td>
</tr>
<tr>
<td>MNLI</td>
<td>"yeah well you're a student right" Based on the previous passage, is it true that "Well you're a mechanics student right"? Yes, no, or maybe? maybe</td>
<td>yes/maybe/no</td>
</tr>
<tr>
<td>AGNews</td>
<td>Article: Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers... <br> Answer: Business</td>
<td>World/Sports/ <br> Business/Technology</td>
</tr>
</tbody>
</table>
<p>Table 7: Our templates and label mappings in different tasks. For simplicity, all the label words we use consist of a single token, so we can easily get the probability of each label.</p>
<p>LLMs. As our datamodels only consider two simple features, the existence of a training example and its index in $\mathcal{Z}$, accurate test predictions may imply that these two features have a dominating effect on ICL.</p>
<h2>A. 4 Training Details of Datamodels</h2>
<p>As the pattern of in-context labels (e.g., [0,0,0,0], $[0,0,0,1],[1,0,0,1])$ has a great impact on LLMs' predictions, for each label pattern, we train a set of $\left|D_{\text {dev }}\right|$ datamodels. Specifically, we apply twophase training: in the first phase, we train on all data with shared weights. In the second phase, we first bucket prompts in $\mathcal{D}<em _dev="{dev" _text="\text">{\text {ICL }}$ by their label patterns. Initializing from the weights learned in the first phase, we then separately fine-tune a set of datamodels for each label pattern. For example, for a binary task with 4 -shot learning, there are $2^{4}=16$ label patterns; thus, we have 16 sets of datamodels after the second-phase training, namely, $16 \times\left|D</em>$. When creating datamodel embeddings (§6), we use the unified weights learned by the first phase.}}\right|$ datamodels in total. We find that having two-phase training leads to more accurate predictions of LLMs' outcomes in appendix A.3. Thus, when assigning scores for training examples, we aggregate all sets of datamodels to obtain $\boldsymbol{s}_{d m</p>
<h2>A. 5 Implementation Details</h2>
<p>We use PyTorch and Huggingface transformers to implement in-context learning on GPT-Neo2.7B (Black et al., 2021), GPTJ-6B (Wang and Komatsuzaki, 2021), OPT-6.7B (Zhang et al., 2022a),
and OPT-13B. We run all our evaluations on a single RTXA6000 GPU (48GB). Most of our experiments can also be run on an RTX3090 GPU (24GB), except that OPT-13B model requires a GPU with larger memory.</p>
<p>Our data collection on $\mathcal{D}_{\text {ICL }}$ costs hundreds of GPU hours on an RTXA6000. Once we finish the collection, our DATAMODELS method only takes about 5 seconds to train a datamodel on an i7-10700 CPU, and our CondAcc method does not involve any training, but simply calculates accuracy.</p>
<p>Table 7 shows our task templates and label mappings, where we closely follow Bach et al. (2022); Lu et al. (2021). Table 8 summarizes our experimental setups on different tasks.</p>
<h2>A. 6 More Experiments</h2>
<p>Table 9 shows experiments on more LLMs and the MNLI task, where we evaluate on test data using 50 sampled prompts, as done in the main experiments (§4.2). Since ICL performs poorly on MNLI (majority: $33.3 \%$ ) in both prior work (Su et al., 2022) and our results in Table 9, we do not experiment more on this task. Overall, our methods CondAcc and DATAMODELS substantially outperform other 4 baselines on all setups.</p>
<h2>A. 7 Why not Instruction-Finetuned LLMs?</h2>
<p>The massive multitask learning in instructiontuning leads to leakage in the datasets we evaluate on. For example, T0 (Sanh et al., 2021), FLANT5 (Chung et al., 2022), and OPT-IML (Iyer et al., 2022) are all trained on several tasks in our paper.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Task</th>
<th style="text-align: center;">$N_{\text {class }}$</th>
<th style="text-align: center;">Bal.</th>
<th style="text-align: center;">$K$</th>
<th style="text-align: center;">$\left|D_{\mathrm{ff}}^{L}\right|$</th>
<th style="text-align: center;">$\left|D_{\mathrm{ff}}^{U}\right|$</th>
<th style="text-align: center;">Permut $^{L}$</th>
<th style="text-align: center;">Permut $^{U}$</th>
<th style="text-align: center;">$\left|\mathcal{D}_{\mathrm{ICL}}{ }^{L}\right|$</th>
<th style="text-align: center;">$\left|\mathcal{D}_{\mathrm{ICL}}{ }^{U}\right|$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">SST-2</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">2000</td>
<td style="text-align: center;">$\binom{1000}{4} \times 4!$</td>
<td style="text-align: center;">$\binom{2000}{4} \times 4!$</td>
<td style="text-align: center;">100,000</td>
<td style="text-align: center;">50,000</td>
</tr>
<tr>
<td style="text-align: left;">BoolQ</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">2000</td>
<td style="text-align: center;">$\binom{1000}{4} \times 4!$</td>
<td style="text-align: center;">$\binom{2000}{4} \times 4!$</td>
<td style="text-align: center;">100,000</td>
<td style="text-align: center;">50,000</td>
</tr>
<tr>
<td style="text-align: left;">Subj</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">2000</td>
<td style="text-align: center;">$\binom{1000}{4} \times 4!$</td>
<td style="text-align: center;">$\binom{2000}{4} \times 4!$</td>
<td style="text-align: center;">100,000</td>
<td style="text-align: center;">50,000</td>
</tr>
<tr>
<td style="text-align: left;">Scicite</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">Y</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">999</td>
<td style="text-align: center;">2997</td>
<td style="text-align: center;">$(333)^{3} \times 3!$</td>
<td style="text-align: center;">$(999)^{3} \times 3!$</td>
<td style="text-align: center;">40,000</td>
<td style="text-align: center;">50,000</td>
</tr>
<tr>
<td style="text-align: left;">AGNews</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">Y</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">4000</td>
<td style="text-align: center;">$(250)^{4} \times 4!$</td>
<td style="text-align: center;">$(1000)^{4} \times 3!$</td>
<td style="text-align: center;">40,000</td>
<td style="text-align: center;">50,000</td>
</tr>
</tbody>
</table>
<p>Table 8: Setups on different tasks. (1) We balance the classes in the prompts (Bal.) for multiclass tasks. (2) To create the unlabeled training set $D_{\mathrm{ff}}^{U}$, we pair each input with every possible label; therefore, $D_{\mathrm{ff}}^{U}$ is $N_{\text {class }}$ times larger than the gold-labeled training set $D_{\mathrm{ff}}^{L}$. (3) Permut ${ }^{L}$ and Permut ${ }^{U}$ denote the total number of possible permutations of training examples for $K$-shot ICL in the labeled and unlabeled setups, respectively. (4) $\left|\mathcal{D}<em _mathrm_ICL="\mathrm{ICL">{\mathrm{ICL}}{ }^{L}\right|$ and $\left|\mathcal{D}</em>\right|$ denote the number of the prompts we collect in the labeled and unlabeled setups, respectively.}}{ }^{U</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">SST-2</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">AGNews</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Avg std</td>
<td style="text-align: center;">Min</td>
<td style="text-align: center;">Avg std</td>
<td style="text-align: center;">Min</td>
</tr>
<tr>
<td style="text-align: center;">GPT-Noe-2.7B</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">All</td>
<td style="text-align: center;">$64.5_{13.0}$</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">$74.8_{5.8}$</td>
<td style="text-align: center;">61.8</td>
</tr>
<tr>
<td style="text-align: center;">RANDOM</td>
<td style="text-align: center;">$65.2_{12.8}$</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">$74.3_{6.8}$</td>
<td style="text-align: center;">56.2</td>
</tr>
<tr>
<td style="text-align: center;">ONESHOT</td>
<td style="text-align: center;">$66.1_{12.8}$</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">$78.3_{4.4}$</td>
<td style="text-align: center;">64.9</td>
</tr>
<tr>
<td style="text-align: center;">TopPrompts-5</td>
<td style="text-align: center;">$64.1_{12.7}$</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">$79.9_{3.4}$</td>
<td style="text-align: center;">71.1</td>
</tr>
<tr>
<td style="text-align: center;">CondAcc</td>
<td style="text-align: center;">76.5</td>
<td style="text-align: center;">52.4</td>
<td style="text-align: center;">$82.3_{2.2}$</td>
<td style="text-align: center;">77.4</td>
</tr>
<tr>
<td style="text-align: center;">DATAMODELS</td>
<td style="text-align: center;">$72.6_{14.2}$</td>
<td style="text-align: center;">50.4</td>
<td style="text-align: center;">83.5</td>
<td style="text-align: center;">80.0</td>
</tr>
<tr>
<td style="text-align: center;">OPT-6.7B</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">All</td>
<td style="text-align: center;">$76.8_{11.8}$</td>
<td style="text-align: center;">52.4</td>
<td style="text-align: center;">$67.9_{15.8}$</td>
<td style="text-align: center;">26.0</td>
</tr>
<tr>
<td style="text-align: center;">RANDOM</td>
<td style="text-align: center;">$72.6_{12.7}$</td>
<td style="text-align: center;">50.2</td>
<td style="text-align: center;">$66.1_{14.7}$</td>
<td style="text-align: center;">27.6</td>
</tr>
<tr>
<td style="text-align: center;">ONESHOT</td>
<td style="text-align: center;">$84.7_{6.6}$</td>
<td style="text-align: center;">65.7</td>
<td style="text-align: center;">$76.3_{7.8}$</td>
<td style="text-align: center;">56.7</td>
</tr>
<tr>
<td style="text-align: center;">TopPrompts-5</td>
<td style="text-align: center;">$78.8_{10.9}$</td>
<td style="text-align: center;">50.4</td>
<td style="text-align: center;">$78.2_{9.7}$</td>
<td style="text-align: center;">30.8</td>
</tr>
<tr>
<td style="text-align: center;">CondAcc</td>
<td style="text-align: center;">88.2</td>
<td style="text-align: center;">59.4</td>
<td style="text-align: center;">$83.2_{4.3}$</td>
<td style="text-align: center;">67.2</td>
</tr>
<tr>
<td style="text-align: center;">DATAMODELS</td>
<td style="text-align: center;">$87.4_{5.3}$</td>
<td style="text-align: center;">71.7</td>
<td style="text-align: center;">84.2</td>
<td style="text-align: center;">74.0</td>
</tr>
</tbody>
</table>
<p>Table 9: More results of GPT-Neo-2.7B and OPT-6.7B (left) and GPTJ-6B on MNLI task (right). Overall, the proposed subset selection methods CondAcc and DATAMODELS significantly outperform other baselines.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">GPTJ-6B</th>
<th style="text-align: center;">OPT-13B</th>
<th style="text-align: center;">Majority</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">SST-2</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">19</td>
<td style="text-align: center;">10</td>
</tr>
<tr>
<td style="text-align: left;">BoolQ</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">10</td>
</tr>
<tr>
<td style="text-align: left;">Subj</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">10</td>
</tr>
<tr>
<td style="text-align: left;">Scicite</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">6.6</td>
</tr>
<tr>
<td style="text-align: left;">AGNews</td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">5</td>
</tr>
</tbody>
</table>
<p>Table 10: The number of gold-labeled training examples identified by Un-CondAcc in the unlabeled setup. The subset size $E=20$ for all tasks.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">GPTJ-6B</th>
<th style="text-align: center;">OPT-13B</th>
<th style="text-align: center;">Majority</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">SST-2</td>
<td style="text-align: center;">$66.0_{12.3}$</td>
<td style="text-align: center;">$55.3_{10.6}$</td>
<td style="text-align: center;">50.0</td>
</tr>
<tr>
<td style="text-align: left;">BoolQ</td>
<td style="text-align: center;">$50.5_{3.4}$</td>
<td style="text-align: center;">$55.6_{5.8}$</td>
<td style="text-align: center;">50.0</td>
</tr>
<tr>
<td style="text-align: left;">Subj</td>
<td style="text-align: center;">$51.8_{4.2}$</td>
<td style="text-align: center;">$50.6_{1.8}$</td>
<td style="text-align: center;">50.0</td>
</tr>
<tr>
<td style="text-align: left;">Scicite</td>
<td style="text-align: center;">$33.9_{1.2}$</td>
<td style="text-align: center;">$36.2_{2.6}$</td>
<td style="text-align: center;">33.3</td>
</tr>
<tr>
<td style="text-align: left;">AGNews</td>
<td style="text-align: center;">$60.8_{9.4}$</td>
<td style="text-align: center;">$54.6_{10.1}$</td>
<td style="text-align: center;">25.0</td>
</tr>
</tbody>
</table>
<p>Table 11: Results of the bad training subsets, which consist of examples of the lowest scores assigned by CondAccmethod.</p>
<h1>A. 8 Larger Number of Shots</h1>
<p>We further compare our methods with an alternative that takes as many labeled, balanced, incontext examples as the context window can fit, named MAXSHOT. Similar to the All baseline, MaxShot samples 50 prompts from the entire training set, each containing $K$ training examples. The differences are that MAXSHOT uses a much larger $K$ and balances the classes in the prompt for binary tasks as well. Here, our LLM is GPTJ-6B, which has a context window of 2048 tokens.</p>
<p>Table 12 shows the number of shots $K$ for each task and compares the average test set accuracy of MaxShot with the ones of All and CondAcc in Table 1. The $\Delta_{\text {All }}$ column shows that using $K \in[8,24]$ examples in the prompt substantially improves over only 3 or 4 examples in most tasks, except for AGNews. However, MAXSHOT only outperforms CondAcc on two out of five tasks ( $\Delta_{\text {Our }}$, blue). This shows the advantages of curated training examples over randomly sampled ones.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: right;">$K$</th>
<th style="text-align: center;">Avg std</th>
<th style="text-align: center;">$\Delta_{\text {All }}$</th>
<th style="text-align: center;">$\Delta_{\text {Our }}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">SST-2</td>
<td style="text-align: right;">24</td>
<td style="text-align: center;">$85.8_{5.5}$</td>
<td style="text-align: center;">8.0</td>
<td style="text-align: center;">-0.9</td>
</tr>
<tr>
<td style="text-align: left;">Subj</td>
<td style="text-align: right;">24</td>
<td style="text-align: center;">$77.1_{8.2}$</td>
<td style="text-align: center;">17.2</td>
<td style="text-align: center;">6.6</td>
</tr>
<tr>
<td style="text-align: left;">BoolQ</td>
<td style="text-align: right;">8</td>
<td style="text-align: center;">$63.6_{1.8}$</td>
<td style="text-align: center;">2.7</td>
<td style="text-align: center;">-1.5</td>
</tr>
<tr>
<td style="text-align: left;">Scicite</td>
<td style="text-align: right;">12</td>
<td style="text-align: center;">$57.1_{6.2}$</td>
<td style="text-align: center;">13.2</td>
<td style="text-align: center;">4.8</td>
</tr>
<tr>
<td style="text-align: left;">AGNews</td>
<td style="text-align: right;">16</td>
<td style="text-align: center;">$82.3_{4.8}$</td>
<td style="text-align: center;">-1.2</td>
<td style="text-align: center;">-5.0</td>
</tr>
</tbody>
</table>
<p>Table 12: Test results of MAXSHOT on GPTJ, where $\Delta_{\text {All }}$ and $\Delta_{\text {Our }}$ show its improvements of average accuracy over All and CondAcc, respectively (both only use $K=3,4$ training examples).</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: The ground-truth outcomes of an LLM versus predicted outcomes of datamodels on the test set of datamodels, which contains a set of newly sampled prompts with unseen combinations of training examples. The high correlations show that our datamodels can make accurate predictions.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: The accuracy versus sequence length across different tasks. Each dot corresponds to a training example. Note that we select the top $E^{\prime}$ examples per class. As a class may have much lower average accuracy than others, the good (resp., bad) examples may not be the examples with the globally highest (resp., lowest) accuracy.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: The accuracy versus perplexity across different tasks. Each dot corresponds to a training example. We do not observe any correlation between perplexity and accuracy. Note that we select the top $E^{t}$ examples per class. As a class may have much lower average accuracy than others, the good (resp., bad) examples may not be the examples with the globally highest (resp., lowest) accuracy.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Visualizing the good training examples selected by CondAcc and Datamodels in datamodels embedding space across different tasks with GPTJ. Each dot is a training example, where datamodels spontaneously learn to encode class information in the embeddings.</p>
<h1>A A1. Did you describe the limitations of your work?</h1>
<h2>Limitations section</h2>
<p>A2. Did you discuss any potential risks of your work?
We use public available language models and commonly used datasets
A3. Do the abstract and introduction summarize the paper's main claims?
Abstract and Sec1
A4. Have you used AI writing assistants when working on this paper?
Left blank.</p>
<h2>B $\square$ Did you use or create scientific artifacts?</h2>
<p>Sec 3,4
B1. Did you cite the creators of artifacts you used?
Sec 4
B2. Did you discuss the license or terms for use and / or distribution of any artifacts?
We use public available artifacts
B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)?
Sec 4 and Limitations
B4. Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect / anonymize it?
Not applicable. Left blank.
B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.?
Not applicable. Left blank.
B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits, etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the number of examples in train / validation / test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be.
Sec4, Table6</p>
<h2>C $\square$ Did you run computational experiments?</h2>
<p>Sec5
C1. Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used?</p>
<h2>Appendix A. 5</h2>
<p>The Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing assistance.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ Prior work has different definitions of prompt; in this paper, we fix the task templates and follow Rubin et al. (2021) to denote prompt as a sequence of training examples for ICL.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
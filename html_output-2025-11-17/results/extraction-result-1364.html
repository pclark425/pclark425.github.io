<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1364 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1364</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1364</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-28.html">extraction-schema-28</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <p><strong>Paper ID:</strong> paper-6ff07b09cd44431603aeb1aab515a9f1355a63ed</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/6ff07b09cd44431603aeb1aab515a9f1355a63ed" target="_blank">L3MVN: Leveraging Large Language Models for Visual Target Navigation</a></p>
                <p><strong>Paper Venue:</strong> IEEE/RJS International Conference on Intelligent RObots and Systems</p>
                <p><strong>Paper TL;DR:</strong> This work proposes a new framework for visual target navigation that leverages Large Language Models (LLM) to impart common sense for object searching and introduces two paradigms: zero-shot and feed-forward approaches that use language to find the relevant frontier from the semantic map as a long-term goal and explore the environment efficiently.</p>
                <p><strong>Paper Abstract:</strong> Visual target navigation in unknown environments is a crucial problem in robotics. Despite extensive investigation of classical and learning-based approaches in the past, robots lack common-sense knowledge about household objects and layouts. Prior state-of-the-art approaches to this task rely on learning the priors during the training and typically require significant expensive resources and time for learning. To address this, we propose a new framework for visual target navigation that leverages Large Language Models (LLM) to impart common sense for object searching. Specifically, we introduce two paradigms: (i) zero-shot and (ii) feed-forward approaches that use language to find the relevant frontier from the semantic map as a long-term goal and explore the environment efficiently. Our analyse demonstrates the notable zero-shot generalization and transfer capabilities from the use of language. Experiments on Gibson and Habitat-Matterport 3D (HM3D) demonstrate that the proposed framework significantly outperforms existing map-based methods in terms of success rate and generalization. Ablation analyse also indicates that the common-sense knowledge from the language model leads to more efficient semantic exploration. Finally, we provide a real robot experiment to verify the applicability of our framework in real-world scenarios. The supplementary video and code can be accessed via the following link: https://sites.google.com/view/l3mvn.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1364.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1364.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HM3D</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Habitat-Matterport 3D Dataset (HM3D)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large-scale collection of photorealistic 3D reconstructions of indoor environments (≈1000 scenes) used for embodied AI experiments; used here as an evaluation environment for visual target navigation in the Habitat simulator.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Habitat-Matterport 3D Dataset (HM3D): 1000 Large-scale 3D Environments for Embodied AI</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>HM3D (Habitat-Matterport 3D)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Photorealistic indoor household/office scenes intended for embodied navigation and object-search tasks; used within the Habitat simulator for object-goal (visual target) navigation experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Not explicitly characterized as a graph in the paper; environments are represented as top-down semantic maps with frontier cells clustered by connected neighborhoods. Semantic information is sparse in many frontier regions.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>Dataset: ~1000 scenes total; in this paper the standard HM3D split used: 75 training scenes and 20 validation scenes. Per-scene node/graph counts not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>L3MVN (zero-shot and feed-forward variants) with FMM local planner</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Module-based navigation: global policy selects long-term goals (frontiers) by scoring frontier-area semantic descriptions via a large language model (RoBERTa-large, either zero-shot MLM scoring or fine-tuned feed-forward embeddings + 3-layer head); frontier candidate scoring also integrates a cost-utility term; local planner uses Fast Marching Method (FMM) to reach chosen frontier and short-range local goals.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>Success Rate (SR), Success weighted by Path Length (SPL), Distance to Goal (DTG); additionally frontier scoring uses a cost-utility score S^{CU} combining utility and cost.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td>For HM3D validation reported in Table I: L3MVN (Zero-Shot) SR=0.504, SPL=0.231, DTG=4.427; L3MVN (Feed-forward) SR=0.542, SPL=0.255, DTG=3.934. (Other baselines reported in table for comparison.)</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>L3MVN (Zero-Shot) 50.4%; L3MVN (Feed-forward) 54.2% on HM3D validation (reported values).</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Map-based global planner that uses semantic priors from language models for frontier selection combined with a classical local planner (FMM); feed-forward (fine-tuned embedding→head) LLM approach performs best.</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>No quantitative graph-topology metrics (e.g., diameter, clustering coefficient) were reported. The paper reports qualitative relationships: (1) sparsity of semantic information in frontier areas degrades relevance scoring and exploration efficiency; (2) integrating a cost-utility frontier score with LLM relevance improves exploration; (3) replacing cost-utility exploration with a naive nearest-frontier policy worsens success and SPL, indicating that frontier selection strategy (which implicitly depends on frontier distribution/topology) affects navigation performance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>A global policy that reasons about frontier semantic content using LLM priors (feed-forward fine-tuned model) plus a cost-utility exploration module yields higher success and better SPL than nearest-frontier or no-LLM variants; local FMM planner allows modular separation so obstacle avoidance need not be learned end-to-end.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'L3MVN: Leveraging Large Language Models for Visual Target Navigation', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1364.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1364.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gibson</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gibson Env: Real-World Perception for Embodied Agents</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A photorealistic 3D indoor environment dataset used for embodied navigation research; used here (Gibson tiny split) to evaluate visual target navigation performance of L3MVN.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gibson Env: Real-World Perception for Embodied Agents</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Gibson (tiny split scenes)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>High-resolution reconstructions of real indoor environments for embodied navigation (household/indoor domains); used with Habitat simulator for object-goal navigation experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Not explicitly reported; represented in experiments via top-down semantic maps and frontier clusters (connected neighborhoods). No explicit graph-theoretic statistics provided.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>Gibson tiny split used: 25 training scenes and 5 validation scenes. No per-scene node/edge counts provided.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>L3MVN (zero-shot and feed-forward variants) with FMM local planner</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Same L3MVN framework as HM3D: frontier extraction from explored/obstacle maps, semantic object extraction in search windows around frontiers, LLM-based scoring (zero-shot MLM scoring or fine-tuned embedding+head), integration with cost-utility frontier score, and FMM-based local planning.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>Success Rate (SR), SPL, DTG; cost-utility frontier score S^{CU} used internally for frontier ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td>For Gibson validation reported in Table I: L3MVN (Zero-Shot) SR=0.761, SPL=0.377, DTG=1.101; L3MVN (Feed-forward) SR=0.769, SPL=0.388, DTG=1.008.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>L3MVN (Zero-Shot) 76.1%; L3MVN (Feed-forward) 76.9% on Gibson validation (reported values).</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Map-based global policy that selects semantically-relevant frontiers using LLM priors combined with cost-utility; local planning via classical FMM.</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>No explicit measurements connecting graph topology metrics to performance. Paper highlights that map-based methods allow better long-term dependency capture and that semantic priors (via LLM) improve frontier selection and thus exploration efficiency; replacing learned/LLM-informed frontier selection with naive strategies (random, nearest-frontier) reduces performance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Feed-forward (fine-tuned) LLM-based frontier selection outperforms zero-shot LLM scoring; integration of semantic LLM scores with cost-utility frontier scoring is important; local planner separation (FMM) reduces need to learn obstacle avoidance and improves transfer to real world.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'L3MVN: Leveraging Large Language Models for Visual Target Navigation', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1364.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1364.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>L3MVN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>L3MVN: Leveraging Large Language Models for Visual Target Navigation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A modular visual target navigation framework that uses large language models to score semantic descriptions of frontier regions to select long-term goals, combines those language priors with cost-utility frontier scoring, and uses a classical local planner (FMM) to execute navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Habitat simulator (evaluated on Gibson and HM3D datasets) and real-world Jackal robot experiments</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Embodied visual target navigation in indoor household/office domains simulated in Habitat using photorealistic reconstructions (Gibson, HM3D); also deployed on a real Jackal robot with RGB-D and lidar sensors.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Environments are encoded as top-down semantic and obstacle maps; frontier cells are identified via contour and dilation differences and clustered by connected neighborhoods. The paper does not supply classic graph-theoretic connectivity statistics (average degree, diameter, clustering coefficient).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>Experiments run on Gibson (25 train / 5 val in tiny split) and HM3D (75 train / 20 val used here); real-world trials on a single robot environment. No per-scene node/edge counts provided.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>L3MVN (zero-shot and feed-forward LLM global policies) + FMM local policy</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Global policy: extract frontiers from semantic/explored/obstacle maps, summarize objects in a search window around each frontier into textual queries, score queries via RoBERTa-large (either zero-shot MLM sentence probability or embed+fine-tuned 3-layer head), optionally combine with cost-utility frontier score S^{CU}; Local policy: Fast Marching Method (FMM) path planner selecting nearby subgoals until frontier reached.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>Success Rate (SR), SPL (Success weighted by Path Length), Distance to Goal (DTG); internal frontier metric S^{CU} (utility minus λ*cost) used to bias exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td>Reported main results (Table I): Gibson: L3MVN Zero-shot SR=0.761, SPL=0.377, DTG=1.101; L3MVN Feed-forward SR=0.769, SPL=0.388, DTG=1.008. HM3D: L3MVN Zero-shot SR=0.504, SPL=0.231, DTG=4.427; L3MVN Feed-forward SR=0.542, SPL=0.255, DTG=3.934.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Best reported success rates: Gibson ~76.9% (feed-forward), HM3D ~54.2% (feed-forward).</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>A modular, planning-based policy: LLM-informed global frontier selection (semantics + cost-utility) + classical local planner (FMM) is empirically superior to random, frontier-only, or nearest-frontier baselines in these indoor environments.</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>The paper reports that topology-like phenomena (distribution and informativeness of frontiers, sparsity of semantic objects near frontiers, frontier clustering) materially impact performance: (i) sparser semantic information in frontier regions reduces LLM relevance and hurts exploration efficiency; (ii) combining cost-utility with LLM relevance mitigates uninformative frontiers; (iii) nearest-frontier heuristics perform worse than cost-utility/LLM-informed selection, indicating that naive topological proximity alone is insufficient. No explicit numeric relationships to graph-diameter, clustering coefficient, or dead-end counts are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>A global policy that reasons semantically about frontier contents (via LLM) plus an explicit exploration module (cost-utility) improves success and path efficiency; feed-forward fine-tuning of LLM embeddings produces better frontier-relevance predictions than zero-shot MLM scoring; modular separation where a classical local planner handles path execution reduces required learning and aids real-world transfer. Ablation study shows removing LLM or cost-utility exploration reduces SR and SPL; adding GT segmentation further improves performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'L3MVN: Leveraging Large Language Models for Visual Target Navigation', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Object goal navigation using goal-oriented semantic exploration <em>(Rating: 2)</em></li>
                <li>PONI: Potential Functions for ObjectGoal Navigation with Interaction-free Learning <em>(Rating: 2)</em></li>
                <li>Neural topological SLAM for visual navigation <em>(Rating: 2)</em></li>
                <li>Frontier-based approach for autonomous exploration <em>(Rating: 1)</em></li>
                <li>Leveraging Large Language Models for Robot 3D Scene Understanding <em>(Rating: 2)</em></li>
                <li>CLIP on Wheels: Zero-Shot Object Navigation as Object Localization and Exploration <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1364",
    "paper_id": "paper-6ff07b09cd44431603aeb1aab515a9f1355a63ed",
    "extraction_schema_id": "extraction-schema-28",
    "extracted_data": [
        {
            "name_short": "HM3D",
            "name_full": "Habitat-Matterport 3D Dataset (HM3D)",
            "brief_description": "A large-scale collection of photorealistic 3D reconstructions of indoor environments (≈1000 scenes) used for embodied AI experiments; used here as an evaluation environment for visual target navigation in the Habitat simulator.",
            "citation_title": "Habitat-Matterport 3D Dataset (HM3D): 1000 Large-scale 3D Environments for Embodied AI",
            "mention_or_use": "use",
            "environment_name": "HM3D (Habitat-Matterport 3D)",
            "environment_description": "Photorealistic indoor household/office scenes intended for embodied navigation and object-search tasks; used within the Habitat simulator for object-goal (visual target) navigation experiments.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": null,
            "dead_ends_count": null,
            "door_constraints_present": false,
            "door_constraints_description": null,
            "graph_connectivity": "Not explicitly characterized as a graph in the paper; environments are represented as top-down semantic maps with frontier cells clustered by connected neighborhoods. Semantic information is sparse in many frontier regions.",
            "environment_size": "Dataset: ~1000 scenes total; in this paper the standard HM3D split used: 75 training scenes and 20 validation scenes. Per-scene node/graph counts not provided.",
            "agent_name": "L3MVN (zero-shot and feed-forward variants) with FMM local planner",
            "agent_description": "Module-based navigation: global policy selects long-term goals (frontiers) by scoring frontier-area semantic descriptions via a large language model (RoBERTa-large, either zero-shot MLM scoring or fine-tuned feed-forward embeddings + 3-layer head); frontier candidate scoring also integrates a cost-utility term; local planner uses Fast Marching Method (FMM) to reach chosen frontier and short-range local goals.",
            "exploration_efficiency_metric": "Success Rate (SR), Success weighted by Path Length (SPL), Distance to Goal (DTG); additionally frontier scoring uses a cost-utility score S^{CU} combining utility and cost.",
            "exploration_efficiency_value": "For HM3D validation reported in Table I: L3MVN (Zero-Shot) SR=0.504, SPL=0.231, DTG=4.427; L3MVN (Feed-forward) SR=0.542, SPL=0.255, DTG=3.934. (Other baselines reported in table for comparison.)",
            "success_rate": "L3MVN (Zero-Shot) 50.4%; L3MVN (Feed-forward) 54.2% on HM3D validation (reported values).",
            "optimal_policy_type": "Map-based global planner that uses semantic priors from language models for frontier selection combined with a classical local planner (FMM); feed-forward (fine-tuned embedding→head) LLM approach performs best.",
            "topology_performance_relationship": "No quantitative graph-topology metrics (e.g., diameter, clustering coefficient) were reported. The paper reports qualitative relationships: (1) sparsity of semantic information in frontier areas degrades relevance scoring and exploration efficiency; (2) integrating a cost-utility frontier score with LLM relevance improves exploration; (3) replacing cost-utility exploration with a naive nearest-frontier policy worsens success and SPL, indicating that frontier selection strategy (which implicitly depends on frontier distribution/topology) affects navigation performance.",
            "comparison_across_topologies": false,
            "topology_comparison_results": null,
            "policy_structure_findings": "A global policy that reasons about frontier semantic content using LLM priors (feed-forward fine-tuned model) plus a cost-utility exploration module yields higher success and better SPL than nearest-frontier or no-LLM variants; local FMM planner allows modular separation so obstacle avoidance need not be learned end-to-end.",
            "uuid": "e1364.0",
            "source_info": {
                "paper_title": "L3MVN: Leveraging Large Language Models for Visual Target Navigation",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "Gibson",
            "name_full": "Gibson Env: Real-World Perception for Embodied Agents",
            "brief_description": "A photorealistic 3D indoor environment dataset used for embodied navigation research; used here (Gibson tiny split) to evaluate visual target navigation performance of L3MVN.",
            "citation_title": "Gibson Env: Real-World Perception for Embodied Agents",
            "mention_or_use": "use",
            "environment_name": "Gibson (tiny split scenes)",
            "environment_description": "High-resolution reconstructions of real indoor environments for embodied navigation (household/indoor domains); used with Habitat simulator for object-goal navigation experiments.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": null,
            "dead_ends_count": null,
            "door_constraints_present": false,
            "door_constraints_description": null,
            "graph_connectivity": "Not explicitly reported; represented in experiments via top-down semantic maps and frontier clusters (connected neighborhoods). No explicit graph-theoretic statistics provided.",
            "environment_size": "Gibson tiny split used: 25 training scenes and 5 validation scenes. No per-scene node/edge counts provided.",
            "agent_name": "L3MVN (zero-shot and feed-forward variants) with FMM local planner",
            "agent_description": "Same L3MVN framework as HM3D: frontier extraction from explored/obstacle maps, semantic object extraction in search windows around frontiers, LLM-based scoring (zero-shot MLM scoring or fine-tuned embedding+head), integration with cost-utility frontier score, and FMM-based local planning.",
            "exploration_efficiency_metric": "Success Rate (SR), SPL, DTG; cost-utility frontier score S^{CU} used internally for frontier ranking.",
            "exploration_efficiency_value": "For Gibson validation reported in Table I: L3MVN (Zero-Shot) SR=0.761, SPL=0.377, DTG=1.101; L3MVN (Feed-forward) SR=0.769, SPL=0.388, DTG=1.008.",
            "success_rate": "L3MVN (Zero-Shot) 76.1%; L3MVN (Feed-forward) 76.9% on Gibson validation (reported values).",
            "optimal_policy_type": "Map-based global policy that selects semantically-relevant frontiers using LLM priors combined with cost-utility; local planning via classical FMM.",
            "topology_performance_relationship": "No explicit measurements connecting graph topology metrics to performance. Paper highlights that map-based methods allow better long-term dependency capture and that semantic priors (via LLM) improve frontier selection and thus exploration efficiency; replacing learned/LLM-informed frontier selection with naive strategies (random, nearest-frontier) reduces performance.",
            "comparison_across_topologies": false,
            "topology_comparison_results": null,
            "policy_structure_findings": "Feed-forward (fine-tuned) LLM-based frontier selection outperforms zero-shot LLM scoring; integration of semantic LLM scores with cost-utility frontier scoring is important; local planner separation (FMM) reduces need to learn obstacle avoidance and improves transfer to real world.",
            "uuid": "e1364.1",
            "source_info": {
                "paper_title": "L3MVN: Leveraging Large Language Models for Visual Target Navigation",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "L3MVN",
            "name_full": "L3MVN: Leveraging Large Language Models for Visual Target Navigation",
            "brief_description": "A modular visual target navigation framework that uses large language models to score semantic descriptions of frontier regions to select long-term goals, combines those language priors with cost-utility frontier scoring, and uses a classical local planner (FMM) to execute navigation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "environment_name": "Habitat simulator (evaluated on Gibson and HM3D datasets) and real-world Jackal robot experiments",
            "environment_description": "Embodied visual target navigation in indoor household/office domains simulated in Habitat using photorealistic reconstructions (Gibson, HM3D); also deployed on a real Jackal robot with RGB-D and lidar sensors.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": null,
            "dead_ends_count": null,
            "door_constraints_present": false,
            "door_constraints_description": null,
            "graph_connectivity": "Environments are encoded as top-down semantic and obstacle maps; frontier cells are identified via contour and dilation differences and clustered by connected neighborhoods. The paper does not supply classic graph-theoretic connectivity statistics (average degree, diameter, clustering coefficient).",
            "environment_size": "Experiments run on Gibson (25 train / 5 val in tiny split) and HM3D (75 train / 20 val used here); real-world trials on a single robot environment. No per-scene node/edge counts provided.",
            "agent_name": "L3MVN (zero-shot and feed-forward LLM global policies) + FMM local policy",
            "agent_description": "Global policy: extract frontiers from semantic/explored/obstacle maps, summarize objects in a search window around each frontier into textual queries, score queries via RoBERTa-large (either zero-shot MLM sentence probability or embed+fine-tuned 3-layer head), optionally combine with cost-utility frontier score S^{CU}; Local policy: Fast Marching Method (FMM) path planner selecting nearby subgoals until frontier reached.",
            "exploration_efficiency_metric": "Success Rate (SR), SPL (Success weighted by Path Length), Distance to Goal (DTG); internal frontier metric S^{CU} (utility minus λ*cost) used to bias exploration.",
            "exploration_efficiency_value": "Reported main results (Table I): Gibson: L3MVN Zero-shot SR=0.761, SPL=0.377, DTG=1.101; L3MVN Feed-forward SR=0.769, SPL=0.388, DTG=1.008. HM3D: L3MVN Zero-shot SR=0.504, SPL=0.231, DTG=4.427; L3MVN Feed-forward SR=0.542, SPL=0.255, DTG=3.934.",
            "success_rate": "Best reported success rates: Gibson ~76.9% (feed-forward), HM3D ~54.2% (feed-forward).",
            "optimal_policy_type": "A modular, planning-based policy: LLM-informed global frontier selection (semantics + cost-utility) + classical local planner (FMM) is empirically superior to random, frontier-only, or nearest-frontier baselines in these indoor environments.",
            "topology_performance_relationship": "The paper reports that topology-like phenomena (distribution and informativeness of frontiers, sparsity of semantic objects near frontiers, frontier clustering) materially impact performance: (i) sparser semantic information in frontier regions reduces LLM relevance and hurts exploration efficiency; (ii) combining cost-utility with LLM relevance mitigates uninformative frontiers; (iii) nearest-frontier heuristics perform worse than cost-utility/LLM-informed selection, indicating that naive topological proximity alone is insufficient. No explicit numeric relationships to graph-diameter, clustering coefficient, or dead-end counts are provided.",
            "comparison_across_topologies": false,
            "topology_comparison_results": null,
            "policy_structure_findings": "A global policy that reasons semantically about frontier contents (via LLM) plus an explicit exploration module (cost-utility) improves success and path efficiency; feed-forward fine-tuning of LLM embeddings produces better frontier-relevance predictions than zero-shot MLM scoring; modular separation where a classical local planner handles path execution reduces required learning and aids real-world transfer. Ablation study shows removing LLM or cost-utility exploration reduces SR and SPL; adding GT segmentation further improves performance.",
            "uuid": "e1364.2",
            "source_info": {
                "paper_title": "L3MVN: Leveraging Large Language Models for Visual Target Navigation",
                "publication_date_yy_mm": "2023-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Object goal navigation using goal-oriented semantic exploration",
            "rating": 2
        },
        {
            "paper_title": "PONI: Potential Functions for ObjectGoal Navigation with Interaction-free Learning",
            "rating": 2
        },
        {
            "paper_title": "Neural topological SLAM for visual navigation",
            "rating": 2
        },
        {
            "paper_title": "Frontier-based approach for autonomous exploration",
            "rating": 1
        },
        {
            "paper_title": "Leveraging Large Language Models for Robot 3D Scene Understanding",
            "rating": 2
        },
        {
            "paper_title": "CLIP on Wheels: Zero-Shot Object Navigation as Object Localization and Exploration",
            "rating": 2
        }
    ],
    "cost": 0.01277775,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>L3MVN: Leveraging Large Language Models for Visual Target Navigation</h1>
<p>Bangguo Yu, Hamidreza Kasaei, Ming Cao</p>
<h6>Abstract</h6>
<p>Visual target navigation in unknown environments is a crucial problem in robotics. Despite extensive investigation of classical and learning-based approaches in the past, robots lack common-sense knowledge about household objects and layouts. Prior state-of-the-art approaches to this task rely on learning the priors during the training and typically require significant expensive resources and time for learning. To address this, we propose a new framework for visual target navigation that leverages Large Language Models (LLM) to impart common sense for object searching. Specifically, we introduce two paradigms: (i) zero-shot and (ii) feed-forward approaches that use language to find the relevant frontier from the semantic map as a long-term goal and explore the environment efficiently. Our analysis demonstrates the notable zero-shot generalization and transfer capabilities from the use of language. Experiments on Gibson and Habitat-Matterport 3D (HM3D) demonstrate that the proposed framework significantly outperforms existing map-based methods in terms of success rate and generalization. Ablation analysis also indicates that the common-sense knowledge from the language model leads to more efficient semantic exploration. Finally, we provide a real robot experiment to verify the applicability of our framework in real-world scenarios. The supplementary video and code can be accessed via the following link: https://sites.google.com/view/l3mvn.</p>
<h2>I. INTRODUCTION</h2>
<p>Human performance in complex and dynamic environments is partly attributed to their innate knowledge of the physical world, which is also crucial for intelligent agents in embodied tasks. Visual target navigation, a task that involves autonomous exploration in unknown environments, requires robots to possess semantic reasoning abilities. In this task, the robot is provided with the name of an object, such as "toilet," and it must efficiently explore a 3D environment to locate any instance of the object.</p>
<p>The last few years have witnessed the development of large-scale photorealistic 3D scene datasets [1] [2], fast simulators for embodied navigation [3] [4] [2], reinforcement learning [5] [6], and approaches for perception [7] [8], cumulatively leading to huge progress for this task. Traditional methods for robot navigation depend on the geometric maps building for path planning, which struggle to generalize to new instructions due to the lack of scene priors. With the development of machine learning, learning-based methods directly optimize for navigation policies grounded in end-to-end or module methods, which gained close attention and made great progress on this task. Most of the learning-based methods formulate this task as a reinforcement learning (RL)</p>
<p>This work of Yu is supported in part by the China Scholarship Council. All authors are with the Faculty of Science and Engineering, University of Groningen, 9747 AG Groningen, the Netherlands. {b.yu, hamidreza.kasaei, m.cao}@rug.nl</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1: Visual target navigation example. The robot explores the environment and uses language models to find more relevant frontier (shown in green, with the highest score) based on the observation and the target.</p>
<p>problem and develop many useful presentations of the scene feature. For the end-to-end models, the first framework [3] uses deep reinforcement learning to find an object as the appointed category in an unknown environment based on current observable images. Later, the framework is expanded by many researchers to improve the navigation performance [9] [10] [11] [12]. Meanwhile, module-based methods [13] [14] [15] attempt to use the explicit spatial map as scene memory and learns the semantic priors through deep reinforcement learning to determine the next step.</p>
<p>Visual target navigation methods that rely on learning typically demand substantial computational resources to acquire and employ scene priors. This motivates us to explore alternative approaches for obtaining scene priors that do not necessitate extensive learning processes. Recent related works have explored interaction-free learning [15] and imitation learning [16] to deal with this problem and significantly decrease the training cost. Another promising approach is to leverage large pre-trained models to transfer knowledge priors, such as zero-shot learning based on contrastive language image pretraining (CLIP) [17]. While this approach shows</p>
<p>potential, there is still significant room for improvement, as there remains a large gap between different tasks.</p>
<p>This study focuses on efficient navigation and searching for an object category in an environment based on visual observations. Our proposed approach employs large language models to develop a policy for exploration and search, with language serving as a general tool for inferring relevance from observed objects. Specially, we leverage language to describe the contents of the frontiers in the map and employ language models to either perform zero-shot inference of which frontier best fits the description or embed the description as input for target-specific classifiers. An illustrative example of visual target navigation for finding TV is shown in Fig 1. After scanning the environment, the robot need to select the next frontier to explore. Based on the objects around each frontier and our target object (tv), the large language model is adopted to find the most relevant frontier. Our model is evaluated on the Habitat simulation platform [4] and compared to previous map-based navigation approaches. We conduct experiments on the photorealistic 3D environments of Gibson [2] and HM3D [1]. In sharp contrast to many other map-based methods, our framework uses the language models as knowledge priors for exploration direction selection, leading to reduced learning costs and improved the generalization of the scene priors. Ablation experiments demonstrate the effectiveness of the language model. Futhermore, we also showcase the application of our method in real-world settings.</p>
<p>Our contributions are summarized as follows:</p>
<ul>
<li>We propose a framework that can build the environment map and select the long-term goal based on the frontiers with the inference of large language models to achieve efficient exploration and searching.</li>
<li>We analysis two paradigms (i) zero-shot and (ii) feedforward approaches for semantic reasoning about the objects using language models.</li>
<li>We evaluate the performance of our model in the realworld setting using a robot platform and discuss the gap between simulation and reality for visual target navigation tasks.</li>
</ul>
<h2>II. RELATED WORK</h2>
<h2>A. Visual Navigation</h2>
<p>Visual navigation is a key task for intelligent robots, inspired by human behavior. Classical approaches to visual navigation rely on building the environment map based on visual observation. These approaches plan a path using the map, which the robot follows to complete the navigation task. However, the accuracy of the environment map greatly affects navigation performance and classical methods struggle with unexplored or changing scenes, which require frontierbased exploration [18]. Recent works have proposed more visual navigation tasks, such as PointNav [19], ImageNav [3], ObjectNav [13], and visual-language navigation [20]. Here, we focus on the ObjectNav task in unknown environments using a novel framework.</p>
<h2>B. Navigational Policy Learning</h2>
<p>In the visual target navigation task, [3] proposed an end-toend framework that used a pre-trained ResNet to encode the input observation and target image, which were then fused into an Asynchronous Advantage Actor-Critic (A3C) [5] model. To further enhance navigation performance, several methods have been proposed, such as knowledge graphs [9] [10] or object relation graphs [11], large-scale training [19], human demonstrations [16], data augmentation [21], and auxiliary tasks [12]. These methods aim to improve the efficiency of the end-to-end navigational policy and enhance its generalization to novel scenes.</p>
<p>Compare to end-to-end methods that rely on past images [3] [9] [11] or RNN [10] [11] [19] to remember the scene features, map-based method has a strong ability to capture long-term dependencies using the map representation. These methods focus on the global policy of selecting waypoints on the environment map, and use classical path planners as the local policy to achieve navigation tasks. The global policy is learned from the different feature representations, such as topological graph [22], geometry [14] and semantic [13] map, or potential functions [15]. Recently, supervised learning has been used to learn potential functions and predict frontiers as long-term goals, resulting in improved performance and reduced computational costs for training, as demonstrated in [15].</p>
<p>While deep reinforcement end-to-end and module-based techniques have achieved success in the visual navigation task, they still require learning scene priors from scene datasets. In contrast, our framework obtains scene priors from language models, enabling more general performance and reducing the learning process compared to prior work.</p>
<h2>C. Learning from Pretrained Models</h2>
<p>Recent advances in large pretrained vision and language models have shown their effectiveness in various domains, such as semantic segmentation [23], scene understanding [24], and robot navigation [25] [20]. Researchers have explored the use of these models for visual target navigation, including using CLIP [17] as a visual enconder in RL [26], object location [25], and learning from other tasks [27] [28]. However, [25] only uses CLIP to locate the target object, and [26] [27] [28] still require learning navigational policies using reinforcement learning. Our approach proposes a novel strategy for the semantic exploration module that leverages strong prior knowledge in language models, enabling zeroshot learning and fine-tuning-based learning for the visual target navigation task.</p>
<p>There are a number of recent works that relate to our approach, such as [15] and [24]. [15] uses the potential functions with interaction-free learning to explores frontiers. [24] achieves more general and zero-shot room classification using the priors of large language models. Our method specifically addresses the problem of learning a frontierbased navigation policy from large language models for visual target navigation. To the best of our knowledge,</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 2: The architecture of the target navigation framework. The framework takes RGB-D images as input to generate a semantic map and frontiers, and selects a long-term goal based on the maps and object category using the inference of the language model. Once the long-term goal is reached, a local policy guides the final action for the robot.</p>
<p>natural language processing tools have not been previously used for this task.</p>
<h2>III. THE PROPOSED METHOD</h2>
<p>In this section, we describe the definition of the target navigation task and two paradigms with large language models.</p>
<h3>A. Task Definition</h3>
<p>The visual target navigation task involves the agent navigating an environment to find an object belonging to an appointed category. The category set is described by $C = {c_0, \ldots, c_m}$, and the scene can be represented by $S = {s_0, \ldots, s_m}$. Each episode begins with the agent being initialized at a random position $p_i$ in the scene $s_i$, and receives the target object category $c_i$. Thus, an episode can be denoted as $T_i = {s_i, c_i, p_i}$. At each time step $t$, the agent observes the environment and takes an action $a_t$. The observation includes RGB-D images, the agent's location and orientation, and the target object category. The action space, denoted by $\mathcal{A}$, includes six actions: move_forward, turn_left, turn_right, look_up, look_down, and stop. The move_forward action agent moves 25 cm, while the turn_left, turn_right, look_up, or look_down actions rotate the agent 30 degrees. The stop action is used when the agent is close to the target object. If the agent takes the stop action when the distance to the target is less than 0.1m, the episode is considered successful. The maximum number of time steps in an episode is 500.</p>
<h3>B. Overview</h3>
<p>Our framework is illustrated in Fig 2. Firstly, the agent obtains the observation of the environment to build the semantic map. The frontier map is extracted from the explored map and obstacle map. Secondly, a long-term goal is selected from all the frontiers based on the relevance score between our target object and the observations. Large language models are used to infer the semantic relevance. After getting the long-term goal, the local policy plans a path and takes the action to explore the environment and search for the target object.</p>
<h3>C. Map Representation</h3>
<p>1) <strong>Semantic Map:</strong> We construct the semantic map using RGB-D images and the agent's position, similar to [13]. It's represented as a $K \times M \times M$ tensor with $M \times M$ as map size and $K = C_n + 2$ channels. The map is initialized with zeros at each episode's start, and the agent is placed at the center. Point clouds are generated from visual input using a geometric method, which is projected onto a top-down 2D map. The map contains obstacles and explored channels from depth images, and semantic channels from semantic segmentation. The semantic mask is aligned with the point clouds, and each channel is projected onto its corresponding position on the semantic map.</p>
<p>2) <strong>Frontier Map:</strong> We obtain the frontier map from the explored map and obstacle map, following the method in [15]. First, we extract the explored edge by identifying the maximum contours from the explored map. Then, by dilating the edge of the obstacle map, we generate the frontier map as the difference between the explored and obstacle maps. Next, we use connected neighborhoods to identify and cluster frontier cells in chains. We remove clusters that are too small. To score the frontier cells from the frontier map, we use the cost-utility approach proposed in [29]. The subset of candidate destinations $F$ is composed of the cells in the center of the remaining cluster chains. So for each frontier cell $f \in F$, we can get the score $S^{CU}(a)$</p>
<p>$$S^{CU}(a) = U(a) - \lambda_{CU}C(a)$$</p>
<p>where $U(a)$ is a utility function, $C(a)$ is a cost function and the constant $\lambda_{CU}$ adjusts the relative importance between both factors.</p>
<h3>D. Global Policy</h3>
<p>After getting the semantic map and frontiers, we select a search window around each frontier and capture all the</p>
<p>semantic objects as the frontier information, which are shown as blue circles in Figure 2. We present two paradigms to use the knowledge of the large language models and apply them for selecting of the frontiers. Both two paradigms summarize a frontier’s contents in a query sentence, then process the sentence in the following ways:</p>
<ul>
<li>Zero-shot: A pre-trained language model scores which object category is best described by the query.</li>
<li>Feed-forward: The query string is embedded by a pre-trained language model. Then, a fine-tuned neural network outputs a distribution over object categories given that embedding.
<img alt="img-2.jpeg" src="img-2.jpeg" /></li>
</ul>
<p>Fig. 3: Example of zero-shot approach.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig. 4: Example of the fine-tuning-based feed-forward approach.</p>
<p>1) Preprocessing for Language Model: We want to use a sentence to describe the semantic information around the frontier, and then use the language model to score the description. To assess the relevance between a frontier and a target object, masked language models (MLMs) is used to score a string $W$ based on semantic and grammatical sensibility as [24], such as the score of the sentence "This frontier area contains sink, bathtubs, and toilet." is higher than the sentence "This frontier area contains sink, bathtubs, and tv." Based on the scores of strings containing commonsense facts, a proxy measure can be obtained for how likely it is for the fact to be true. Additionally, we utilize the highdimensional embeddings of the language model, denoted as $\operatorname{Emb}(W)$, to represent the meaning and grammatical structure of the text. By fine-tuning the language model, we aim to learn a mapping from the embeddings to a prediction score for each target object, thus providing a quantitative measure of the relevance between the frontier and the object.</p>
<p>To account for the complexity of the environment, we calculate the entropy of each object category, as proposed in [24], in order to mitigate the influence of uninformative and ubiquitous objects (e.g., doors and windows). Specifically, objects that appear less frequently across the target object categories are deemed more informative, as their presence
tends to imply certain target categories. This is reflected in their non-uniform conditional distributions $p\left(t_{j} \mid o_{i}\right)$, where $o_{i} \in L_{O}$ represents the object category, $t_{j} \in L_{T}$ represents the target category, and $L_{T}$ and $L_{O}$ are the sets of target object and object categories, respectively. In our task, $L_{T} \subseteq$ $L_{O}$ implies that some target objects may also be informative object categories. We compute these conditional probabilities using ground-truth co-occurrences from publicly available semantic annotated scene datasets [1] containing of 1,000 high-resolution 3D scans of indoor spaces. Specifically, we count the number of times each object category appears around each target category, and normalize the counts over targets to obtain $p\left(t_{j} \mid o_{i}\right)$. Once we have access to $p\left(t_{j} \mid o_{i}\right)$, we can calculate the entropy using the following formula:</p>
<p>$$
H_{O_{i}}=-\sum_{t_{j} \in L_{T}} p\left(t_{j} \mid o_{i}\right) \log p\left(t_{j} \mid o_{i}\right)
$$</p>
<p>Entropy is maximized when the considered distribution is uniform and minimized when it is one-hot, meaning more semantically-informative objects have lower corresponding entropy. We create a top entropy list of the 15 lowest-scoring objects and restrict the composition of the input string $W$ to only include objects from this list, which differs from [24]. We use the resulting string to infer relevance between the frontier and target objects.
2) Zero-shot Approach: For the zero-shot approach, we construct $\left|L_{T}\right|$ query strings for each frontier $f_{i}$, one per target category:</p>
<p>$$
\begin{aligned}
&amp; W_{t_{j}}^{f_{i}}=\text { "A frontier area containing } \
&amp; o_{1}, o_{2}, \cdots, o_{k}, t_{j} . " \forall t_{j} \in L_{T}
\end{aligned}
$$</p>
<p>where $o_{1 \cdots k}$ are the objects detected from the frontier area $f_{i}$, and $t_{j}$ is the target object. All these queries are scored via language model, with the final estimated frontier $f$ being whichever one yields the highest query sentence probability for the target $t_{j}$. The score for each frontier is:</p>
<p>$$
S_{f_{i}}^{L L M}=\log p\left(W_{t_{j}}^{f_{i}}\right)
$$</p>
<p>3) Feed-forward Approach: For the feed-forward approach, we create a single query string of the form for each frontier $f_{i}$ :</p>
<p>$$
W^{f_{i}}=\text { "This frontier contains } o_{1}, \cdots, \text { and } o_{k} \text {." }
$$</p>
<p>This string is then fed into a language model to produce a summary embedding vector. Finally, the embedding is fed into a fine-tuned neural network head, which produces a $\left|L_{T}\right|$-dimensional vector of prediction logits corresponding to the target categories, with the inferred frontier being the one corresponding to the maximum value of the target object's output. The score for each frontier is:</p>
<p>$$
S_{f_{i}}^{L L M}=\left[f_{\theta}\left(\operatorname{Emb}\left(W^{f_{i}}\right)\right)\right]<em j="j">{t</em>
$$}</p>
<p>where $f_{\theta}: \operatorname{Emb}\left(W^{f_{i}}\right) \rightarrow \mathbb{R}^{L_{T}}$ is a neural network that takes in query embeddings and outputs prediction logits. We use three-layer network for this mapping.</p>
<p>TABLE I: Results of Comparative Study.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Gibson</th>
<th></th>
<th></th>
<th>HM3D</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Success</td>
<td>SPL</td>
<td>DTG</td>
<td>Success</td>
<td>SPL</td>
<td>DTG</td>
</tr>
<tr>
<td>Random Walking</td>
<td>0.030</td>
<td>0.030</td>
<td>2.580</td>
<td>0.000</td>
<td>0.000</td>
<td>7.600</td>
</tr>
<tr>
<td>Frontier Based Method [18]</td>
<td>0.417</td>
<td>0.214</td>
<td>2.634</td>
<td>0.237</td>
<td>0.123</td>
<td>5.414</td>
</tr>
<tr>
<td>Random Sample on Map</td>
<td>0.544</td>
<td>0.288</td>
<td>1.918</td>
<td>0.300</td>
<td>0.143</td>
<td>4.761</td>
</tr>
<tr>
<td>SemExp [13]</td>
<td>0.652</td>
<td>0.336</td>
<td>1.520</td>
<td>0.379</td>
<td>0.188</td>
<td>2.943</td>
</tr>
<tr>
<td>PONI [15]</td>
<td>0.736</td>
<td>0.410</td>
<td>1.250</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>L3MVN (Zero-Shot)</td>
<td>0.761</td>
<td>0.377</td>
<td>1.101</td>
<td>0.504</td>
<td>0.231</td>
<td>4.427</td>
</tr>
<tr>
<td>L3MVN (Feed-forward)</td>
<td>0.769</td>
<td>0.388</td>
<td>1.008</td>
<td>0.542</td>
<td>0.255</td>
<td>3.934</td>
</tr>
</tbody>
</table>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Fig. 5: The visual target navigation experiment process in the Habitat platform for finding a bed. The gray channel represents the barrier, the blue spot and the circle denote the long-term goal selected by our policy, the red thick line represents the trajectory of the robot, the red thin line denotes the frontiers, and other colors represent the semantic objects.</p>
<p>4) Exploration: The sparsity of semantic information in indoor environments can result in frontier areas that lack informative objects or contain unrelated objects (e.g., a sofa in a toilet). To address this, we integrate the frontier map score $S^{CU}$ with the language model score $S^{LLM}$. We set a score bound $B$ for the frontier and normalize $S^{CU}$ to fit within this range. We then select the frontier using the following strategy:</p>
<p>$$
f=\arg \max_{L}\left{\begin{array}{cl}
S_{f_{i}}^{LLM}, &amp; \text { if } \max \left[S^{LLM}\right]&gt;\sup (B) \
\left(S_{f_{i}}^{LLM}, S_{f_{i}}^{CU}\right), &amp; \text { if } \max \left[S^{LLM}\right] \in B \
S_{f_{i}}^{CU}, &amp; \text { if } \max \left[S^{LLM}\right]&lt;\inf (B)
\end{array}\right.
$$</p>
<h3>E. Local Policy</h3>
<p>In order to navigate from the agent's current location to a long-term goal, we employ the Fast Marching Method (FMM) [30]. Subsequently, the agent selects a local goal within a restricted range of its current position and executes the final action $a_{t} \in \mathcal{A}$ to reach it. At each step, the local map and local goal are updated based on new observations. This approach, which employs module policies, enhances training efficiency and obviates the necessity of learning obstacle avoidance from scratch, as is required in the end-to-end approach.</p>
<h2>IV. EXPERIMENTS</h2>
<p>In this section, we evaluate the performance of our method by comparing it with other map-based baselines in a simulated environment. Additionally, we apply our method in a real-world robot platform to validate its practicality for navigational tasks.</p>
<h3>A. Simulation Experiment</h3>
<p>1) Dataset: Our experiments are conducted on two high-resolution photorealistic 3D reconstructions of real-world environments: HM3D [1] and Gibson [2]. The Gibson dataset comprises 25 training and 5 validation scenes from the Gibson tiny split, which come with associated semantic annotations. The HM3D dataset is in the habitat format, and we use the standard splits of 75 training and 20 validation scenes. There are 6 object goal categories defined [13]: chair, couch, potted plant, bed, toilet, and TV.</p>
<p>2) Experiment Details: We conducted our evaluation on the 3D indoor simulator Habitat platform [4], using an observation space consisting of $480 \times 640$ RGBD images, a base odometry sensor, and a goal object represented as an integer. We utilized the finetuned RedNet model [8] to predict all categories as in [12]. Our implementation was based on publicly available code from [13] and [24], using the PyTorch framework. We set the score range for the frontier to be between 0 and 1, with a bound of $B=[0.15,0.3]$.</p>
<ul>
<li>For the zero-shot approach, we utilized the pre-trained language model RoBERTa-large [31] to evaluate the query strings that were generated based on the semantic observation around each frontier and the target object.</li>
<li>For the feed-forward approach, we finetune RoBERTa-large as our sentence embedder and train head networks on HM3D datasets. We extract all the objects in each room of the HM3D scenes and check if the room contains the target objects. If so, we split the target object as the label and other objects as the sample to create the dataset. Finally, we train the finetuned models with RoBERTa embeddings on the entire dataset and evaluate the query strings for each frontier.</li>
</ul>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Fig. 6: The process of visual target navigation experiment in the real world to find a sofa.</p>
<p>3) <strong>Evaluation Metrics:</strong> We follow [32] to evaluate our method using Success Rate, Success weighted by Path Length (SPL), and Distance to Goal (DTG). SR is defined as $$\frac{1}{N} \sum_{i=1}^{N} S_i$$, and SPL is defined as $$\frac{1}{N} \sum_{i=1}^{N} S_i \frac{l_i}{\max(l_i, p_i)}$$, where N is the number of episodes, S<sup>i</sup> is 1 if the episode is successful, else is 0, l<sup>i</sup> is the shortest trajectory length between the start position and one of the success position, p<sup>i</sup> is the trajectory length of the current episode i. The DTG is the distance between the agent and the target goal when the episode ends.</p>
<p>4) <strong>Baselines:</strong> In order to evaluate the navigation performance of our model, we considered several baselines.</p>
<ul>
<li><strong>Randomly Walking:</strong> At each step, the agent selects an action uniformly at random from the action space A.</li>
<li><strong>Frontier-based Policy [18]:</strong> This baseline method employs a classical robotics pipeline for mapping and a frontier-based exploration algorithm.</li>
<li><strong>Randomly Sampling on Map:</strong> We randomly sample the long-term goal from the map, and use the local planner method to select the final action.</li>
<li><strong>SemExp [13]:</strong> We follow [13] as the baseline to explore and search for the target using semantic map.</li>
<li><strong>PONI [15]:</strong> Potential function [13] is the newest map-based work and can be set as baseline of interaction-free learning method. We can only get the results on Gibson datasets from the published work.</li>
</ul>
<p>5) <strong>Result and Discussion:</strong> The quantitative results of the comparison study are reported in TABLE I. As indicated by the results, random walking without any specialized navigation policy fails in almost all episodes. However, when we utilize the map-based framework to randomly sample the long-term goal, the performance is even superior to that of the classical frontier-based method [18]. This indicates the significant advantage of the map-based method in allowing the robot to quickly and roughly explore the environment. Furthermore, the significant improvement achieved by SemExp [13] highlights the importance of semantic information in efficient exploration. PONI [15] further enhances the performance and reduces computational costs, demonstrating the capacity for learning semantic priors in a distinct way from other reinforcement learning approaches. Our framework consistently outperforms all baselines across both datasets, achieving notable improvement over the SemExp [13] and PONI [15] baselines. The comparison between the feed-forward and zero-shot approaches indicates that feed-forward method learns more accurate relevance in large indoor scenes. The process of finding a bed is illustrated in Fig 5.</p>
<p><strong>TABLE II: Results of Ablation Study in HM3D.</strong></p>
<table>
<thead>
<tr>
<th>L3MVN ablation</th>
<th></th>
<th></th>
<th></th>
<th>HM3D results</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Near</td>
<td>Exp</td>
<td>LLM</td>
<td>GT Seg</td>
<td>Success†</td>
<td>SPL†</td>
<td>DTG↓</td>
</tr>
<tr>
<td>✓</td>
<td></td>
<td>✓</td>
<td></td>
<td>0.490</td>
<td>0.231</td>
<td>4.335</td>
</tr>
<tr>
<td></td>
<td>✓</td>
<td></td>
<td></td>
<td>0.518</td>
<td>0.244</td>
<td>4.084</td>
</tr>
<tr>
<td></td>
<td>✓</td>
<td>✓</td>
<td></td>
<td>0.542</td>
<td>0.255</td>
<td>3.934</td>
</tr>
<tr>
<td></td>
<td>✓</td>
<td>✓</td>
<td>✓</td>
<td>0.664</td>
<td>0.364</td>
<td>3.195</td>
</tr>
</tbody>
</table>
<p>6) <strong>Ablation study:</strong> To assess the relative importance of the various modules within our framework, we have performed the ablations using the HM3D dataset: the cost-utility exploration module (Exp), the feed-forward approach for language module (LLM), the nearest frontier module (Near) that only used for LLM ablation, and the ground-truth semantic segmentation (GT Seg). The results in TABLE II show that our complete model achieves the best performance (row 3, TABLE II), and the LLM and exploration policy are crucial for achieving good performance. Removing the LLM (row 2, TABLE II) decreases both success rate and SPL but still higher than the zero-shot approach, and replacing cost-utility exploration with the nearest frontier policy (row 1, TABLE II) leads to a further drop in performance, which shows the importance of exploration for this task. Augmenting our complete model with GT Seg improves performance on all cases, as semantic segmentation impacts the semantic mapping, which is also the main reason for the failures.</p>
<h4><em>B. Real World Experiment</em></h4>
<p>We utilized ROS and the Jackal Robot hardware platform with Realsense D455 camera and Ouster lidar to implement and test our policy in the real world. To minimize the gap between the simulation and the real-world scenarios, we attempted to keep the sensor data consistent with the simulation environment. Specifically, we set the same height and range of the depth image for the robot's RGB-D camera as in the simulation environment, and used the lidar to construct a real-time geometric map to improve the robot's localization accuracy. The RGB-D images, location, and object category were then fed into our model, which produced an action output. As shown in Fig 6, the long-term goal selected by our model (represented by a blue spot) guided the robot to efficiently explore the environment and locate the goal.</p>
<p>The RGB-D images and location are the primary differences between the simulation and the real world, as the depth image is affected by factors such as illumination and hardware quality. Furthermore, the lidar and odometry measurements in the real world are not as accurate as in the</p>
<p>simulation, leading to noisy points around objects and walls. But the module-based framework has the advantage in realworld transfer since it takes the scene map as input rather than direct images with noise. This reduces the impact of noise, allowing us to deploy the model on the robot platform with minimal fine-tuning.</p>
<h2>V. CONCLUSIONS</h2>
<p>We presented L3MVN, a novel module framework that leverages large language models to facilitate visual target navigation by examining two paradigms that infer the semantic relevance from the observed frontiers. By implementing experiments in Gibson and HM3D datasets, we demonstrate that our approach significantly improves the success rate and efficiency while avoiding large-scale learning processes. Ablation studies demonstrated that our language models and cost-utility exploration lead to more efficient navigation. We also validate the effectiveness in a real-world experiment, highlighting the practical applicability of our method. Our findings suggest that large language models hold immense potential in aiding robots in such tasks by providing useful knowledge. Future research should consider the design of the interaction between the robot and LLM.</p>
<h2>REFERENCES</h2>
<p>[1] S. K. Ramakrishnan, A. Gokaslan, E. Wijmans, O. Maksymets, A. Clegg, J. Turner, E. Undersander, W. Galuba, A. Westbury, A. X. Chang, M. Savva, Y. Zhao, and D. Batra, "Habitat-Matterport 3D Dataset (HM3D): 1000 Large-scale 3D Environments for Embodied AI," arXiv, sep 2021.
[2] F. Xia, A. R. Zamir, Z. He, A. Sax, J. Malik, and S. Savarese, "Gibson Env: Real-World Perception for Embodied Agents," in 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9068-9079, IEEE, jun 2018.
[3] Y. Zhu, R. Mottaghi, E. Kolve, J. J. Lim, A. Gupta, L. Fei-Fei, and A. Farhadi, "Target-driven visual navigation in indoor scenes using deep reinforcement learning," in Proceedings - IEEE International Conference on Robotics and Automation, pp. 3357-3364, IEEE, may 2017.
[4] M. Savva, A. Kadian, O. Maksymets, Y. Zhao, E. Wijmans, B. Jain, J. Straub, J. Liu, V. Koltun, J. Malik, D. Parikh, and D. Batra, "Habitat: A Platform for Embodied AI Research," in 2019 IEEE/CVF International Conference on Computer Vision (ICCV), vol. 2019Octob, pp. 9338-9346, IEEE, oct 2019.
[5] V. Mnih, A. Puigdomènech Badia, M. Mirza, T. Harley, T. P. Lillicrap, D. Silver, and K. Kavukcuoglu, "Asynchronous Methods for Deep Reinforcement Learning Volodymyr," International Conference on Machine Learning, vol. 48, 2013.
[6] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, "Proximal Policy Optimization Algorithms," ArXiv, 2017.
[7] K. He, G. Gkioxari, P. Dollar, and R. Girshick, "Mask R-CNN," in 2017 IEEE International Conference on Computer Vision (ICCV), vol. 2017-Octob, pp. 2980-2988, IEEE, oct 2017.
[8] J. Jiang, L. Zheng, F. Luo, and Z. Zhang, "RedNet: Residual EncoderDecoder Network for indoor RGB-D Semantic Segmentation," arXiv, jun 2018.
[9] W. Yang, X. Wang, A. Farhadi, A. Gupta, and R. Mottaghi, "Visual semantic navigation using scene priors," in 7th International Conference on Learning Representations, ICLR 2019, pp. 1-14, 2019.
[10] Y. Lyu, Y. Shi, and X. Zhang, "Improving Target-driven Visual Navigation with Attention on 3D Spatial Relationships," Neural Processing Letters, vol. 54, no. 5, pp. 3979-3998, 2022.
[11] R. Druon, Y. Yoshiyasu, A. Kanezaki, and A. Watt, "Visual object search by learning spatial context," IEEE Robotics and Automation Letters, vol. 5, no. 2, pp. 1279-1286, 2020.
[12] J. Ye, D. Batra, A. Das, and E. Wijmans, "Auxiliary Tasks and Exploration Enable ObjectGoal Navigation," Proceedings of the IEEE International Conference on Computer Vision, pp. 16097-16106, 2021.
[13] D. S. Chaplot, D. Gandhi, A. Gupta, and R. Salakhutdinov, "Object goal navigation using goal-oriented semantic exploration," Advances in Neural Information Processing Systems, vol. 2020-December, no. NeurIPS, pp. 1-12, 2020.
[14] D. S. Chaplot, D. Gandhi, S. Gupta, A. Gupta, and R. Salakhutdinov, "Learning to Explore using Active Neural SLAM," in International Conference on Learning Representations (ICLR), apr 2020.
[15] S. K. Ramakrishnan, D. S. Chaplot, Z. Al-Halah, J. Malik, and K. Grauman, "PONI: Potential Functions for ObjectGoal Navigation with Interaction-free Learning," Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, vol. 2022-June, pp. 18868-18878, 2022.
[16] R. Ramrakhya, E. Undersander, D. Batra, and A. Das, "Habitat-Web: Learning Embodied Object-Search Strategies from Human Demonstrations at Scale," Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, vol. 2022-June, pp. 5163-5173, apr 2022.
[17] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever, "Learning Transferable Visual Models From Natural Language Supervision," Proceedings of the 38th International Conference on Machine Learning, vol. 139, pp. 8748-8763, feb 2021.
[18] B. Yamauchi, "Frontier-based approach for autonomous exploration," Proceedings of IEEE International Symposium on Computational Intelligence in Robotics and Automation, CIRA, pp. 146-151, 1997.
[19] E. Wijmans, A. Kadian, A. Morcos, S. Lee, I. Essa, D. Parikh, M. Savva, and D. Batra, "DD-PPO: Learning Near-Perfect PointGoal Navigators from 2.5 Billion Frames," arXiv, nov 2019.
[20] C. Huang, O. Mees, A. Zeng, and W. Burgard, "Visual Language Maps for Robot Navigation," arXiv, oct 2022.
[21] O. Maksymets, V. Cartillier, A. Gokaslan, E. Wijmans, W. Galuba, S. Lee, and D. Batra, "THDA: Treasure Hunt Data Augmentation for Semantic Navigation," Proceedings of the IEEE International Conference on Computer Vision, pp. 15354-15363, 2021.
[22] D. S. Chaplot, R. Salakhutdinov, A. Gupta, and S. Gupta, "Neural topological SLAM for visual navigation," in Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 12872-12881, 2020.
[23] B. Li, K. Q. Weinberger, S. Belongie, V. Koltun, and R. Ranftl, "Language-driven Semantic Segmentation," arXiv, pp. 1-13, jan 2022.
[24] W. Chen, S. Hu, R. Talak, and L. Carlone, "Leveraging Large Language Models for Robot 3D Scene Understanding," arXiv, sep 2022.
[25] S. Y. Gadre, M. Wortsman, G. Ilharco, L. Schmidt, and S. Song, "CLIP on Wheels: Zero-Shot Object Navigation as Object Localization and Exploration," arXiv, pp. 1-22, mar 2022.
[26] A. Khandelwal, L. Weihs, R. Mottaghi, and A. Kembhavi, "Simple but Effective: CLIP Embeddings for Embodied AI," in 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 14809-14818, IEEE, jun 2022.
[27] Z. Al-Halah, S. K. Ramakrishnan, and K. Grauman, "Zero Experience Required: Plug \&amp; Play Modular Transfer Learning for Semantic Visual Navigation," Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, vol. 2022-June, pp. 17010-17020, 2022.
[28] S. Y. Min, Y.-H. H. Tsai, W. Ding, A. Farhadi, R. Salakhutdinov, Y. Bisk, and J. Zhang, "Object Goal Navigation with End-to-End SelfSupervision," arXiv, dec 2022.
[29] M. Juliá, A. Gil, and O. Reinoso, "A comparison of path planning strategies for autonomous exploration and mapping of unknown environments," Autonomous Robots, vol. 33, no. 4, pp. 427-444, 2012.
[30] J. A. Sethian, "A fast marching level set method for monotonically advancing fronts," Proceedings of the National Academy of Sciences of the United States of America, vol. 93, no. 4, pp. 1591-1595, 1996.
[31] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov, "RoBERTa: A Robustly Optimized BERT Pretraining Approach," arXiv, jul 2019.
[32] P. Anderson, A. Chang, D. S. Chaplot, A. Dosovitskiy, S. Gupta, V. Koltun, J. Kosecka, J. Malik, R. Mottaghi, M. Savva, and A. R. Zamir, "On Evaluation of Embodied Navigation Agents," arXiv, jul 2018.</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2091 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2091</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2091</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-53.html">extraction-schema-53</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <p><strong>Paper ID:</strong> paper-276872985</p>
                <p><strong>Paper Title:</strong> Cutting-edge AI tools revolutionizing scientific research in life sciences</p>
                <p><strong>Paper Abstract:</strong> Artificial intelligence (AI) is becoming a transformative force in the life sciences, pushing the boundaries of possibility. Imagine AI automating time-consuming tasks, uncovering hidden patterns in vast datasets, designing proteins in minutes instead of years, and even predicting disease outbreaks before they occur. This review explores the latest AI tools revolutionizing scientific fields, including research and data analysis, healthcare, and tools supporting scientific writing. Beyond data processing, AI is reshaping how scientists draft and share their findings, enhancing processes ranging from literature reviews to citation management. However, with great power comes great responsibility. Are we prepared for this leap? This review delves into the forefront of AI in the life sciences, where innovation meets responsibility.</p>
                <p><strong>Cost:</strong> 0.027</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2091.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2091.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AlphaFold 2/3</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AlphaFold (versions 2 and 3)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>DeepMind's deep learning system for predicting protein 3D structure from amino-acid sequence; AlphaFold 2 achieved accuracy comparable to experimental imaging and AlphaFold 3 extends predictions to biomolecular interactions and dramatically reduced runtime.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AlphaFold</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>deep neural network / protein structure prediction model</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>structural biology / proteomics / drug discovery</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>predicted protein 3D structures and protein–biomolecule interaction models</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>transformational for structure prediction; generally in-distribution extrapolation from large structural databases but extended to novel interaction predictions (moderately novel)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>pattern extraction and learned spatial/sequence relationships from large databases of protein structures and sequences to predict folding and interactions</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>comparison to experimental structural determination (X-ray crystallography, cryo-EM) and benchmarking versus community datasets; large-scale deployment (server) and community uptake as indirect validation</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Reported as achieving performance comparable to imaging methods; produced >200 million predictions (AlphaFold 2) and enabled ~6 million mapped structures accessed by ~1.8 million researchers; AlphaFold 3 provides results in minutes (no quantitative runtime distribution provided).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Described as 'comparable to imaging methods' in accuracy; no numerical accuracy/precision/recall values provided in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Not quantified; review notes AlphaFold 3 extends to interaction prediction but also that stereochemistry and some chemistry aspects still require human assistance, implying validation degrades for more novel/complex interaction predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Paper highlights that generation (fast, large-scale predictions) has outpaced full experimental validation capacity; AlphaFold produced millions of models while experimental confirmation remains slower, indicating an asymmetry between generation speed/scale and experimental validation throughput.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not described in detail in this review; no model-provided calibrated uncertainties reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not numerically reported; qualitative note that AlphaFold 3 improves interaction predictions but still faces stereochemistry challenges and requires human oversight.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Proxy comparisons to established imaging results and community benchmark performance are used rather than exhaustive experimental verification.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Implied to be required for complex cases and stereochemistry; no numeric frequency given.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>empirical (experimental structural biology) — domain is empirical and requires laboratory validation; this increases the generation-validation gap when outputs are novel.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Providing server access for community use and enabling large-scale prioritization for experimental follow-up; no concrete mitigation metrics reported.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>AlphaFold produced hundreds of millions of predictions and maps far faster than experimental methods can validate; AlphaFold 3 expands predictive capability while stereochemical issues and need for human assistance are noted, supporting a generation > validation gap.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>Claims of performance 'comparable to imaging methods' suggest that, for many proteins, generated models match experimental outcomes, indicating that for much in-distribution prediction validation is adequate.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2091.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2091.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AlphaMissense</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AlphaMissense</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A deep learning model that evaluates missense variants across the proteome using sequence and predicted structural context (built on AlphaFold2 outputs) to assess pathogenicity of single amino-acid changes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Accurate proteome-wide missense variant effect prediction with AlphaMissense.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AlphaMissense</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>deep neural network leveraging structural context</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>genomic variant interpretation / clinical genetics</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>pathogenicity scores / predicted effects of missense variants on protein function</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>moderately novel — applies structural-context-informed prediction to many variants not previously characterized</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>learned mapping from sequence + predicted structural contexts to pathogenicity likelihood (pattern extrapolation and learned representation)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Comparison with known pathogenic and benign variants and use in diagnostic interpretation pipelines; described as improving diagnostic accuracy and identifying disease-causing genes (no numeric metrics in review).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Described qualitatively as pivotal for elucidating variant effects and improving diagnostic accuracy; no numeric sensitivity/specificity provided in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Not reported numerically; model is presented as helpful for previously undetected pathogenic variants implying reasonable performance even on novel variants, but no quantitative degradation reported.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Review implies generated variant effect predictions assist diagnostics but need empirical or clinical corroboration; no detailed comparison metrics provided.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not described in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Uses agreement with previously labeled pathogenic/benign variants (proxy for ground truth) rather than prospective experimental verification in most descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Used as an aid in diagnostics; human clinical validation implied necessary; no numeric frequency provided.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>empirical / clinical genomics — requires experimental and clinical validation.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Integration with clinical workflows and structural models to prioritize variants for follow-up; specifics not provided in review.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Model produces proteome-wide predictions but clinical/experimental validation capacity lags, implying potential overreliance without full validation.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>Positive statements about improved diagnostic accuracy suggest useful correspondence with clinical outcomes for many cases.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2091.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2091.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PEACOCK</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PEACOCK</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A machine-learning tool that scores cell-type-specific enhancer–gene regulatory links genome-wide (~17 million enhancer–gene pairs) to prioritize likely regulatory relationships for disease research.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>PEA-COCK: a machine learning approach to assess the validity of cell type-specific enhancer-gene regulatory relationships.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>PEACOCK</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>machine learning model (unspecified architecture)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>functional genomics / regulatory genomics</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>quantitative scores for enhancer–gene pairs indicating likelihood of regulatory linkage</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>moderately novel — enables genome-scale scoring of enhancer–gene links that are otherwise difficult to prioritize experimentally</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>supervised ML trained on a selected set of experimental datasets and cell lines, producing scores across genome pairs</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Trained and validated using curated experimental data published in literature; produced PEREGRINE database for community access; serves as prioritization for downstream statistical and experimental analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Capable of scoring ~17 million enhancer–gene pairs genome-wide; scalability and cell-type specificity emphasized (no numeric accuracy/precision values provided in review).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Trained/validated on selected experimental datasets; review does not provide quantitative validation metrics (AUC, precision, recall) in text.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Not reported; model scores are intended as prioritization proxies rather than definitive validation — likely less reliable for highly novel enhancer–gene mechanisms without matching training examples.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Paper notes PEACOCK can generate many prioritized links but relies on published experimental data for training/validation, indicating generation outpaces experimental confirmation at genome scale.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not described in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Relies on training/validation against curated experimental datasets (proxy for ground truth) and uses quantitative scores as a proxy for likelihood.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Intended to guide experimental follow-up; frequency depends on research priorities — not quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>empirical (functional genomics) — experimental validation required.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Publishing PEREGRINE database to enable community validation and integration into statistical GWAS workflows; no measurement of effectiveness provided.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Model scores millions of pairs while targeted experimental validation remains limited, supporting a generation >> validation gap.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>Model trained/validated on curated experimental data, indicating reasonable baseline validity for in-distribution cases.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2091.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2091.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 / GPT-4o and LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer models (GPT-4, GPT-4o) and other large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Multimodal large language models (LLMs) used to generate scientific text, design biological constructs, create experimental protocols, and in some cases design antibodies/nanobodies and code for lab robots.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GPT-4 / GPT-4o (and comparable LLMs like GPT-3.5, Gemini Pro, Llama2 70b)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>large multimodal transformer (large language model)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>multidisciplinary: molecular design, experimental protocol generation, hypothesis articulation, lab automation</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>textual scientific hypotheses, experimental plans, code for robots, designed antibody/nanobody sequences and protocols</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>ranges from in-distribution summarization to moderately/highly novel in designing sequences or experimental plans when combined with search/agents</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>autoregressive learned sequence modeling / multimodal conditioning with retrieval and agent orchestration (in some systems) to propose designs and protocols</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Controlled trials and red‑teaming (company internal); experimental validation in independent labs for some outputs (e.g., antibody/nanobody designs), comparisons with ground truth where available, and human expert review.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Reported accomplishments include producing detailed Rosetta guidance that led to antibody binding to SARS‑CoV‑2 (Microsoft eval), and LLM agents designing effective SARS‑CoV‑2 nanobodies with minimal human intervention (Virtual Lab); no general success rates provided in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Validation reported in select studies via experimental testing of designed molecules (e.g., nanobodies) and controlled comparisons; review does not provide aggregate metrics such as precision/recall for LLM outputs across tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Review highlights hallucinations and confabulations (see separate entry) and notes that LLMs can fabricate plausible but incorrect information, implying decreased validation reliability for novel/out-of-distribution outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Paper emphasizes generation capabilities (fast design, code, plans) sometimes exceeded robust validation — companies use red‑teaming and controlled trials but community-level experimental validation lags, indicating a gap.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not consistently implemented; review mentions hallucination indices and leaderboards as diagnostic tools rather than intrinsic calibrated uncertainty outputs from LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Noted as problematic — hallucination rates and reference errors demonstrate poor calibration of factual confidence in many LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>LLMs can produce plausible novel outputs but are prone to hallucination; specific OOD metrics not provided in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Often rely on plausibility, coherence, and proxy human-review metrics; internal automated assessments and red-team outcomes used as proxies for safety/performance.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Recommended broadly; frequency increases with novelty of output (explicitly recommended for experimental or biological outputs).</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>varies by use-case from empirical (biology) to semi-formal (protocols); lower formalization in experimental biology increases generation-validation gap.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Company-level red‑teaming, automated assessments, controlled trials comparing task performance with and without AI, and government oversight requiring notification for models trained on biological sequences above compute thresholds.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Multiple examples where LLMs generated experimental protocols or molecule designs faster than the community can validate; review cites hallucination rates and examples of fabricated references (30–90% error rates reported in one study), supporting a generation > validation gap, especially for novel outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>Specific experimental validations (e.g., nanobody designs verified experimentally) show LLM-generated designs can be validated successfully in select cases, indicating generation can sometimes match validation.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2091.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2091.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Virtual Lab (LLM agents)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>The Virtual Lab: LLM agent ensemble for designing and validating SARS‑CoV‑2 nanobodies</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system of large language model agents (powered by GPT-4o) that designed SARS‑CoV‑2 nanobodies with minimal human intervention and included experimental validation of designed molecules.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The Virtual Lab: AI agents design new SARS-CoV-2 nanobodies with experimental validation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Virtual Lab (LLM agent system)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>multi-agent LLM orchestration system</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>molecular design / protein engineering</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>designed nanobody sequences and experimental protocols</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>moderately to highly novel (designing new binding molecules), since agents produced sequences not present in training data</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>LLM agents iteratively propose designs, search literature/data, plan experiments and produce protocols; agent orchestration to explore design space</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Experimental testing in wet lab for designed nanobodies (reported experimentally validated designs)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Reported to have designed 'effective' nanobodies with minimal human intervention; no population-level success rate provided in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Experimental validation was performed for designed nanobodies (details not quantified in this review).</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Not quantified; experimental validation demonstrates success for some novel designs but review warns of unpredictabilities and need for governance.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Explicit example where automated generation produced experimentally testable designs; however, the review frames this as raising biosafety and oversight concerns because generation can produce actionable biological designs faster than oversight/validation pipelines scale.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not described.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not numerically reported; success on designing nanobodies suggests capability on OOD molecular design in targeted settings.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Experimental activity assays used as direct validation; possibly proxy plausibility checks by experts prior to experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Experimental validation required for designs; frequency depends on throughput and prioritization — not quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>empirical (protein engineering) — wet-lab validation necessary.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Authors and reviewers call for governance, controlled trials and mitigation strategies; no empirical mitigation results reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>LLM agents produced experimentally validated designs but the review emphasizes risks and unpredictability, supporting concern that generation can outpace safe validation and oversight.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>Successful experimental validation of some designs shows generation can be meaningfully validated when resources permit.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2091.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2091.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Coscientist</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Coscientist (GPT-4 powered autonomous chemical research system)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system powered by GPT-4 that can design, plan and execute complex experiments including chemical syntheses by searching documents, writing code, and operating robotic lab devices.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Autonomous chemical research with large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Coscientist</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-driven autonomous experiment planning and execution system</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>chemistry / automated synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>experimental plans, synthesis protocols, control code for robotic devices, novel synthesized compounds</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>moderately novel — automates planning/execution but synthesis targets may be within known chemical spaces</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>LLM-based planning informed by literature search and code generation to operate lab robotics</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Execution of planned experiments in the lab (robotic execution) and empirical observation of reaction/product outcomes; article describes end-to-end experiment runs.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Described as capable of end-to-end execution including code writing and robot operating; no quantitative success/failure rates provided in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Validation via empirical experiment outcomes; no aggregate metrics or FP/FN rates provided.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Not quantified; review notes unpredictabilities and biosafety concerns when automation reduces human oversight.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>System demonstrates that generation + execution can be automated, but review highlights oversight and safety evaluation lagging — generation/execution may outpace governance and validation pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not described.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Empirical experimental outcomes (direct validation) used rather than proxies in described examples.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Human oversight and governance recommended; frequency not quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>empirical (laboratory chemistry) — requires wet-lab validation and safety oversight.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Calls for swift government action, comprehensive testing, and mitigation strategies; internal company red-teaming and controlled trials mentioned elsewhere in review but not specific to Coscientist.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Autonomous systems can design and physically execute experiments, increasing the speed at which novel biological/chemical interventions can be produced relative to traditional oversight/validation capacity.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>Empirical experiment execution demonstrates that validation can be integrated into autonomous pipelines when properly resourced.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2091.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2091.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mia (Kheiron)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mia — AI tool developed by Imperial College London and Kheiron Medical Technologies</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An AI system for mammography that in an NHS evaluation detected 13% more early-stage breast cancers and identified cases missed by human readers in a >10,000-woman dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Mia</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>medical imaging deep learning model</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>radiology / breast cancer screening</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>predicted cancer presence on mammograms (binary detections and cases to recall)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>incremental to moderately novel within medical imaging — improves sensitivity/early detection over some human readers</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>CNN or deep-learning-based image classification trained on labeled mammography datasets (architecture not specified in review)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Retrospective evaluation on NHS dataset of >10,000 women; comparison to human radiologist readings and ground truth cancer diagnoses.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Reported to detect 13% more early-stage breast cancers and to have identified 11 cases initially missed by human doctors in the NHS evaluation; led to more recalls.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Validated retrospectively on >10,000 mammograms with comparison to human readers and diagnostic follow-up; quantitative sensitivity/specificity not provided beyond the 13% improvement figure.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Not explicitly quantified in review; mentions 'led to more women being recalled' which implies increased false positives may accompany increased sensitivity but no rate provided.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Not applicable — model evaluated on screening data; review does not report how performance changes with out-of-distribution cases.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Generation (detection) was validated retrospectively and compared to humans — in this case validation kept pace because labeled clinical data existed; no major generation-validation gap for this application is described.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Comparison to human diagnosis and subsequent confirmed diagnoses used as ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Used as assistive tool in screening with human oversight; frequency implied continuous (every case) but not numerically specified.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>empirical clinical domain with established ground truth (diagnoses) enabling robust validation.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Retrospective evaluation on large real-world datasets and NHS testing provide external validation; no further mitigation strategies described in review.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Not strongly supported here — this clinical task had available labeled data enabling validation alongside generation.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>Successful NHS-scale validation indicates generation and validation were aligned for this specific clinical detection task.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2091.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2091.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TBINet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TBINet (deep-learning model for pulmonary tuberculosis infectivity)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A deep-learning model using CT images to identify pulmonary tuberculosis infectivity, validated by high AUC values and interpretability via Grad-CAM.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>TBINet</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>deep convolutional neural network for CT image analysis</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>infectious disease imaging / diagnostic radiology</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>probability of pulmonary tuberculosis infectivity / diagnostic classification</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>incremental within medical imaging diagnostics</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>CNN-based image classification trained on labeled CT datasets</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Validation on independent screening studies with reported AUC metrics and Grad-CAM visualization for interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Described as validated by 'high AUC values' (no numeric AUC in review) and provided interpretable maps via Grad-CAM.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>High AUC values reported in the original study (not numerically specified in this review).</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Not described.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Generation performance reported with conventional statistical metrics (AUC) indicating alignment between model output and validation for this imaging task.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not described.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>AUC and Grad-CAM used as validation/interpretability proxies.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Intended to assist clinicians; human oversight implied but not quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>empirical (medical imaging) with clinical ground truth enabling validation.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Use of independent validation cohorts and interpretability maps (Grad-CAM) to build trust; numerical effectiveness not provided in review.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Not strongly indicated; model validation used standard metrics with independent cohorts.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>High AUC validation suggests generation matched validation capability for this task.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2091.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e2091.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Biomed-Parse</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Biomed-Parse</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A foundation AI model for joint segmentation, detection and recognition across nine biomedical imaging modalities, trained on a dataset of >6 million image/annotation/text triples and reported to outperform models like MedSAM and SAM on irregular objects.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Biomed-Parse</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>multimodal vision-language deep learning foundation model</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>biomedical image analysis / medical imaging</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>segmentation masks, object detection and recognition labels from biomedical images (multiple modalities)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>moderately novel — unified multi-modality model addressing segmentation/detection/recognition jointly</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>vision-language pretraining on large paired image/segmentation/text datasets to enable conditional segmentation from textual prompts and integrated tasks</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Benchmarked against existing tools (MedSAM, SAM) and demonstrated superior performance on irregularly-shaped object segmentation; trained/validated on a large assembled dataset of >6 million triples.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Reported to outperform existing tools in handling irregularly shaped objects and to unify tasks; no numeric accuracy/IoU/etc. provided in review.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Qualitative statements of outperforming other models; no numeric performance metrics included in the review text.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Model comparisons to established models indicate validation benchmarks are used; specifics not provided, but review indicates a performance advantage in targeted tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not described.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Benchmarks against prior models and comparison on curated test sets used as proxies for generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Suggested for clinical deployment and interpretation; not quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>empirical (image analysis) but benefits from ground-truth segmentation annotations for validation.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Large-scale dataset creation and benchmarking against existing models; no quantitative mitigation outcomes reported.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Not directly indicated; model performance claims are benchmark-based rather than indicating generation outstrips validation.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>Benchmark comparisons imply validation kept pace for the model development lifecycle.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2091.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e2091.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hallucination / Reference Error Findings</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM hallucination and reference error reports (Chelli et al. and others)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical findings and tracking tools that document LLM hallucination rates and reference fabrication: some chatbots confabulate facts in up to ~30% of cases and various chatbots made reference errors between 30% and 90% in a 2024 study.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Hallucination Rates and Reference Accuracy of ChatGPT and Bard for Systematic Reviews: Comparative Analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>General LLM chatbots (ChatGPT, Bard, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>large language models / generative models</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP applied to scientific text / systematic review support</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>generated text, references, summaries and systematic-review related outputs</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>in-distribution text generation but can fabricate OOD facts (hallucinations)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>autoregressive language modeling trained on large text corpora</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>External fact-checking against source documents and reference corpora; community-run leaderboards and indices (Hallucination Vulnerability Index, Hallucinations Leaderboard) to quantify errors</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Varies widely; review cites study showing reference errors between 30% and 90% across different chatbots, and confabulation up to ~30% in other measures.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Validation often requires human fact-checking; automated detection tools exist but with variable effectiveness (no unified accuracy reported in review).</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Reported reference fabrication rates in one study ranged from 30% to 90% (these represent produced references that were incorrect); exact FP definition varies by study.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Hallucination frequency increases with tasks requiring factual grounding beyond model's memorized/training distribution, implying worse performance on novel/out-of-distribution factual claims.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Clear asymmetry: LLMs can rapidly generate plausible references/claims while validation (human or automated fact-checking) is slower and error-prone; review highlights this gap.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not reliably present in many LLMs; external platforms/metrics track hallucinations but intrinsic uncertainty outputs are not consistently calibrated.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Described as poor based on observed fabrication/hallucination rates.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Poorer — higher hallucination and fabricated reference rates for OOD claims; precise metrics not standardized.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Plausibility/coherence and automated detectors (various AI detectors) are used but have limited reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>High for factual claims and references; review suggests systematic human fact-checking is necessary.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>textual / bibliographic (semi-formal) but factual grounding requires empirical/source verification.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Hallucination Vulnerability Index, leaderboards, automated detection tools and recommended human fact-checking; effectiveness variable and not fully quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Empirical studies cited showing 30–90% reference errors and up to ~30% confabulation support the claim that generation exceeds reliable validation.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>None presented in review; some validation tooling exists but with limited success.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2091.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e2091.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prov-GigaPath</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prov-GigaPath (whole-slide pathology foundation model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-access whole-slide pathology foundation model pretrained on over one billion 256×256 pathology tiles and pathology reports, intended for scalable digital pathology tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Prov-GigaPath</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>vision-language foundation model / large convolutional / transformer-based architecture for pathology</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>digital pathology</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>slide-level representations, segmentation, classification and other pathology downstream task outputs</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>moderately novel — large-scale pretraining on real-world pathology tiles enabling transfer to many downstream pathology tasks</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>self-supervised / supervised pretraining on large image-text (images + pathology reports) corpora to learn slide representations</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Benchmarked across digital pathology tasks and promoted as excelling in these tasks; review states 'excelling' but provides no numeric task metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Pretrained on >1 billion tiles from >170k slides; reported to perform strongly across tasks though no quantitative metrics provided in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Qualitatively described as excelling; numerical validation metrics not included in review text.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Not described.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Review indicates foundation-model pretraining followed by downstream task evaluation; no explicit gap analysis provided but scale suggests generation capability is large and validation depends on downstream labels.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not described.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Downstream task benchmarks and comparisons to prior models used as proxies.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Human pathologist oversight implied for clinical deployment; frequency not quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>empirical clinical/diagnostic domain — requires clinical validation.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Open-weight model release to encourage community evaluation and validation; no quantitative outcomes reported.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Large-scale pretrained models can be applied quickly across many tasks while collection of high-quality clinical labels for validation remains a bottleneck.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>Qualitative claims of strong downstream performance suggest validation is feasible for many tasks with appropriate datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2091.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e2091.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CellSAM / CellFinder</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CellSAM (built on CellFinder prompting SAM) / CellFinder</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Tools for automated cell detection and segmentation across modalities: CellFinder trained for automated 3D cell detection and CellSAM repurposes prompting of SAM to segment cells across mammalian, yeast and bacterial images.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>CellSAM / CellFinder</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>object-detection and segmentation models (deep learning)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>cell imaging / spatial omics / microscopy</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>cell detection coordinates and segmentation masks (2D/3D)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>incremental to moderately novel in scale and cross-modality generalization</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>training object detectors on annotated microscopy datasets (CellFinder) and using prompt-based segmentation of a general segment-anything model (CellSAM) to adapt to cell images</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Trained and tested on large-scale images (e.g., whole-brain images for CellFinder) and across modalities for CellSAM; references the Cell Tracking Challenge and benchmarking datasets for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Reported to perform automated 3D cell detection and multi-modality segmentation; no numeric precision/recall or segmentation IoU numbers given in review.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Evaluation via community benchmarks (Cell Tracking Challenge) and curated test sets; review references improved accuracy but provides no numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Not quantified; community challenges provide standardized metrics but results not enumerated here.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Benchmarking infrastructure (Cell Tracking Challenge) exists to align generation and validation; review implies ongoing improvements but does not claim full parity.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not described.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Standardized challenge metrics and annotated video/image repositories are used as proxies for real-world performance.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Human curation/inspection implied for critical analyses and downstream biological interpretation; not quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>empirical imaging domain; validation depends on annotated datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Public benchmarks and repositories (CTC) to measure progress and encourage reproducible validation.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Mention that automated methods have improved but the need for standardized evaluation and human oversight continues, indicating that generation improvements require rigorous validation.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>Existence of community benchmarks and improved results implies validation infrastructure is keeping pace in this subdomain.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Accurate proteome-wide missense variant effect prediction with AlphaMissense. <em>(Rating: 2)</em></li>
                <li>Autonomous chemical research with large language models. <em>(Rating: 2)</em></li>
                <li>The Virtual Lab: AI agents design new SARS-CoV-2 nanobodies with experimental validation. <em>(Rating: 2)</em></li>
                <li>PEA-COCK: a machine learning approach to assess the validity of cell type-specific enhancer-gene regulatory relationships. <em>(Rating: 2)</em></li>
                <li>Hallucination Rates and Reference Accuracy of ChatGPT and Bard for Systematic Reviews: Comparative Analysis. <em>(Rating: 2)</em></li>
                <li>A foundation model for joint segmentation, detection and recognition of biomedical objects across nine modalities. <em>(Rating: 2)</em></li>
                <li>A free, AI-powered research tool for scientific literature. (Semantic Scholar / related tool descriptions) <em>(Rating: 1)</em></li>
                <li>Prov-GigaPath: Whole-Slide Foundation Model for Digital Pathology. <em>(Rating: 2)</em></li>
                <li>Neural network fast-classifies biological images through features selecting to power automated microscopy. <em>(Rating: 1)</em></li>
                <li>The cell tracking challenge: 10 years of objective benchmarking. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2091",
    "paper_id": "paper-276872985",
    "extraction_schema_id": "extraction-schema-53",
    "extracted_data": [
        {
            "name_short": "AlphaFold 2/3",
            "name_full": "AlphaFold (versions 2 and 3)",
            "brief_description": "DeepMind's deep learning system for predicting protein 3D structure from amino-acid sequence; AlphaFold 2 achieved accuracy comparable to experimental imaging and AlphaFold 3 extends predictions to biomolecular interactions and dramatically reduced runtime.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "AlphaFold",
            "system_type": "deep neural network / protein structure prediction model",
            "scientific_domain": "structural biology / proteomics / drug discovery",
            "output_type": "predicted protein 3D structures and protein–biomolecule interaction models",
            "novelty_level": "transformational for structure prediction; generally in-distribution extrapolation from large structural databases but extended to novel interaction predictions (moderately novel)",
            "generation_method": "pattern extraction and learned spatial/sequence relationships from large databases of protein structures and sequences to predict folding and interactions",
            "validation_method": "comparison to experimental structural determination (X-ray crystallography, cryo-EM) and benchmarking versus community datasets; large-scale deployment (server) and community uptake as indirect validation",
            "generation_performance": "Reported as achieving performance comparable to imaging methods; produced &gt;200 million predictions (AlphaFold 2) and enabled ~6 million mapped structures accessed by ~1.8 million researchers; AlphaFold 3 provides results in minutes (no quantitative runtime distribution provided).",
            "validation_performance": "Described as 'comparable to imaging methods' in accuracy; no numerical accuracy/precision/recall values provided in this review.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "performance_vs_novelty": "Not quantified; review notes AlphaFold 3 extends to interaction prediction but also that stereochemistry and some chemistry aspects still require human assistance, implying validation degrades for more novel/complex interaction predictions.",
            "generation_validation_comparison": "Paper highlights that generation (fast, large-scale predictions) has outpaced full experimental validation capacity; AlphaFold produced millions of models while experimental confirmation remains slower, indicating an asymmetry between generation speed/scale and experimental validation throughput.",
            "uncertainty_quantification": "Not described in detail in this review; no model-provided calibrated uncertainties reported here.",
            "calibration_quality": "Not reported.",
            "out_of_distribution_performance": "Not numerically reported; qualitative note that AlphaFold 3 improves interaction predictions but still faces stereochemistry challenges and requires human oversight.",
            "validation_proxy_metrics": "Proxy comparisons to established imaging results and community benchmark performance are used rather than exhaustive experimental verification.",
            "human_validation_required": true,
            "human_validation_frequency": "Implied to be required for complex cases and stereochemistry; no numeric frequency given.",
            "formal_verification_used": null,
            "domain_formalization_level": "empirical (experimental structural biology) — domain is empirical and requires laboratory validation; this increases the generation-validation gap when outputs are novel.",
            "gap_mitigation_strategies": "Providing server access for community use and enabling large-scale prioritization for experimental follow-up; no concrete mitigation metrics reported.",
            "evidence_supporting_gap": "AlphaFold produced hundreds of millions of predictions and maps far faster than experimental methods can validate; AlphaFold 3 expands predictive capability while stereochemical issues and need for human assistance are noted, supporting a generation &gt; validation gap.",
            "evidence_contradicting_gap": "Claims of performance 'comparable to imaging methods' suggest that, for many proteins, generated models match experimental outcomes, indicating that for much in-distribution prediction validation is adequate.",
            "computational_cost_ratio": null,
            "uuid": "e2091.0"
        },
        {
            "name_short": "AlphaMissense",
            "name_full": "AlphaMissense",
            "brief_description": "A deep learning model that evaluates missense variants across the proteome using sequence and predicted structural context (built on AlphaFold2 outputs) to assess pathogenicity of single amino-acid changes.",
            "citation_title": "Accurate proteome-wide missense variant effect prediction with AlphaMissense.",
            "mention_or_use": "mention",
            "system_name": "AlphaMissense",
            "system_type": "deep neural network leveraging structural context",
            "scientific_domain": "genomic variant interpretation / clinical genetics",
            "output_type": "pathogenicity scores / predicted effects of missense variants on protein function",
            "novelty_level": "moderately novel — applies structural-context-informed prediction to many variants not previously characterized",
            "generation_method": "learned mapping from sequence + predicted structural contexts to pathogenicity likelihood (pattern extrapolation and learned representation)",
            "validation_method": "Comparison with known pathogenic and benign variants and use in diagnostic interpretation pipelines; described as improving diagnostic accuracy and identifying disease-causing genes (no numeric metrics in review).",
            "generation_performance": "Described qualitatively as pivotal for elucidating variant effects and improving diagnostic accuracy; no numeric sensitivity/specificity provided in this review.",
            "validation_performance": null,
            "false_positive_rate": null,
            "false_negative_rate": null,
            "performance_vs_novelty": "Not reported numerically; model is presented as helpful for previously undetected pathogenic variants implying reasonable performance even on novel variants, but no quantitative degradation reported.",
            "generation_validation_comparison": "Review implies generated variant effect predictions assist diagnostics but need empirical or clinical corroboration; no detailed comparison metrics provided.",
            "uncertainty_quantification": "Not described in this review.",
            "calibration_quality": "Not reported.",
            "out_of_distribution_performance": null,
            "validation_proxy_metrics": "Uses agreement with previously labeled pathogenic/benign variants (proxy for ground truth) rather than prospective experimental verification in most descriptions.",
            "human_validation_required": true,
            "human_validation_frequency": "Used as an aid in diagnostics; human clinical validation implied necessary; no numeric frequency provided.",
            "formal_verification_used": null,
            "domain_formalization_level": "empirical / clinical genomics — requires experimental and clinical validation.",
            "gap_mitigation_strategies": "Integration with clinical workflows and structural models to prioritize variants for follow-up; specifics not provided in review.",
            "evidence_supporting_gap": "Model produces proteome-wide predictions but clinical/experimental validation capacity lags, implying potential overreliance without full validation.",
            "evidence_contradicting_gap": "Positive statements about improved diagnostic accuracy suggest useful correspondence with clinical outcomes for many cases.",
            "computational_cost_ratio": null,
            "uuid": "e2091.1"
        },
        {
            "name_short": "PEACOCK",
            "name_full": "PEACOCK",
            "brief_description": "A machine-learning tool that scores cell-type-specific enhancer–gene regulatory links genome-wide (~17 million enhancer–gene pairs) to prioritize likely regulatory relationships for disease research.",
            "citation_title": "PEA-COCK: a machine learning approach to assess the validity of cell type-specific enhancer-gene regulatory relationships.",
            "mention_or_use": "mention",
            "system_name": "PEACOCK",
            "system_type": "machine learning model (unspecified architecture)",
            "scientific_domain": "functional genomics / regulatory genomics",
            "output_type": "quantitative scores for enhancer–gene pairs indicating likelihood of regulatory linkage",
            "novelty_level": "moderately novel — enables genome-scale scoring of enhancer–gene links that are otherwise difficult to prioritize experimentally",
            "generation_method": "supervised ML trained on a selected set of experimental datasets and cell lines, producing scores across genome pairs",
            "validation_method": "Trained and validated using curated experimental data published in literature; produced PEREGRINE database for community access; serves as prioritization for downstream statistical and experimental analyses.",
            "generation_performance": "Capable of scoring ~17 million enhancer–gene pairs genome-wide; scalability and cell-type specificity emphasized (no numeric accuracy/precision values provided in review).",
            "validation_performance": "Trained/validated on selected experimental datasets; review does not provide quantitative validation metrics (AUC, precision, recall) in text.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "performance_vs_novelty": "Not reported; model scores are intended as prioritization proxies rather than definitive validation — likely less reliable for highly novel enhancer–gene mechanisms without matching training examples.",
            "generation_validation_comparison": "Paper notes PEACOCK can generate many prioritized links but relies on published experimental data for training/validation, indicating generation outpaces experimental confirmation at genome scale.",
            "uncertainty_quantification": "Not described in the review.",
            "calibration_quality": "Not reported.",
            "out_of_distribution_performance": null,
            "validation_proxy_metrics": "Relies on training/validation against curated experimental datasets (proxy for ground truth) and uses quantitative scores as a proxy for likelihood.",
            "human_validation_required": true,
            "human_validation_frequency": "Intended to guide experimental follow-up; frequency depends on research priorities — not quantified.",
            "formal_verification_used": null,
            "domain_formalization_level": "empirical (functional genomics) — experimental validation required.",
            "gap_mitigation_strategies": "Publishing PEREGRINE database to enable community validation and integration into statistical GWAS workflows; no measurement of effectiveness provided.",
            "evidence_supporting_gap": "Model scores millions of pairs while targeted experimental validation remains limited, supporting a generation &gt;&gt; validation gap.",
            "evidence_contradicting_gap": "Model trained/validated on curated experimental data, indicating reasonable baseline validity for in-distribution cases.",
            "computational_cost_ratio": null,
            "uuid": "e2091.2"
        },
        {
            "name_short": "GPT-4 / GPT-4o and LLMs",
            "name_full": "Generative Pre-trained Transformer models (GPT-4, GPT-4o) and other large language models",
            "brief_description": "Multimodal large language models (LLMs) used to generate scientific text, design biological constructs, create experimental protocols, and in some cases design antibodies/nanobodies and code for lab robots.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "GPT-4 / GPT-4o (and comparable LLMs like GPT-3.5, Gemini Pro, Llama2 70b)",
            "system_type": "large multimodal transformer (large language model)",
            "scientific_domain": "multidisciplinary: molecular design, experimental protocol generation, hypothesis articulation, lab automation",
            "output_type": "textual scientific hypotheses, experimental plans, code for robots, designed antibody/nanobody sequences and protocols",
            "novelty_level": "ranges from in-distribution summarization to moderately/highly novel in designing sequences or experimental plans when combined with search/agents",
            "generation_method": "autoregressive learned sequence modeling / multimodal conditioning with retrieval and agent orchestration (in some systems) to propose designs and protocols",
            "validation_method": "Controlled trials and red‑teaming (company internal); experimental validation in independent labs for some outputs (e.g., antibody/nanobody designs), comparisons with ground truth where available, and human expert review.",
            "generation_performance": "Reported accomplishments include producing detailed Rosetta guidance that led to antibody binding to SARS‑CoV‑2 (Microsoft eval), and LLM agents designing effective SARS‑CoV‑2 nanobodies with minimal human intervention (Virtual Lab); no general success rates provided in the review.",
            "validation_performance": "Validation reported in select studies via experimental testing of designed molecules (e.g., nanobodies) and controlled comparisons; review does not provide aggregate metrics such as precision/recall for LLM outputs across tasks.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "performance_vs_novelty": "Review highlights hallucinations and confabulations (see separate entry) and notes that LLMs can fabricate plausible but incorrect information, implying decreased validation reliability for novel/out-of-distribution outputs.",
            "generation_validation_comparison": "Paper emphasizes generation capabilities (fast design, code, plans) sometimes exceeded robust validation — companies use red‑teaming and controlled trials but community-level experimental validation lags, indicating a gap.",
            "uncertainty_quantification": "Not consistently implemented; review mentions hallucination indices and leaderboards as diagnostic tools rather than intrinsic calibrated uncertainty outputs from LLMs.",
            "calibration_quality": "Noted as problematic — hallucination rates and reference errors demonstrate poor calibration of factual confidence in many LLMs.",
            "out_of_distribution_performance": "LLMs can produce plausible novel outputs but are prone to hallucination; specific OOD metrics not provided in this review.",
            "validation_proxy_metrics": "Often rely on plausibility, coherence, and proxy human-review metrics; internal automated assessments and red-team outcomes used as proxies for safety/performance.",
            "human_validation_required": true,
            "human_validation_frequency": "Recommended broadly; frequency increases with novelty of output (explicitly recommended for experimental or biological outputs).",
            "formal_verification_used": false,
            "domain_formalization_level": "varies by use-case from empirical (biology) to semi-formal (protocols); lower formalization in experimental biology increases generation-validation gap.",
            "gap_mitigation_strategies": "Company-level red‑teaming, automated assessments, controlled trials comparing task performance with and without AI, and government oversight requiring notification for models trained on biological sequences above compute thresholds.",
            "evidence_supporting_gap": "Multiple examples where LLMs generated experimental protocols or molecule designs faster than the community can validate; review cites hallucination rates and examples of fabricated references (30–90% error rates reported in one study), supporting a generation &gt; validation gap, especially for novel outputs.",
            "evidence_contradicting_gap": "Specific experimental validations (e.g., nanobody designs verified experimentally) show LLM-generated designs can be validated successfully in select cases, indicating generation can sometimes match validation.",
            "computational_cost_ratio": null,
            "uuid": "e2091.3"
        },
        {
            "name_short": "Virtual Lab (LLM agents)",
            "name_full": "The Virtual Lab: LLM agent ensemble for designing and validating SARS‑CoV‑2 nanobodies",
            "brief_description": "A system of large language model agents (powered by GPT-4o) that designed SARS‑CoV‑2 nanobodies with minimal human intervention and included experimental validation of designed molecules.",
            "citation_title": "The Virtual Lab: AI agents design new SARS-CoV-2 nanobodies with experimental validation.",
            "mention_or_use": "mention",
            "system_name": "Virtual Lab (LLM agent system)",
            "system_type": "multi-agent LLM orchestration system",
            "scientific_domain": "molecular design / protein engineering",
            "output_type": "designed nanobody sequences and experimental protocols",
            "novelty_level": "moderately to highly novel (designing new binding molecules), since agents produced sequences not present in training data",
            "generation_method": "LLM agents iteratively propose designs, search literature/data, plan experiments and produce protocols; agent orchestration to explore design space",
            "validation_method": "Experimental testing in wet lab for designed nanobodies (reported experimentally validated designs)",
            "generation_performance": "Reported to have designed 'effective' nanobodies with minimal human intervention; no population-level success rate provided in the review.",
            "validation_performance": "Experimental validation was performed for designed nanobodies (details not quantified in this review).",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "performance_vs_novelty": "Not quantified; experimental validation demonstrates success for some novel designs but review warns of unpredictabilities and need for governance.",
            "generation_validation_comparison": "Explicit example where automated generation produced experimentally testable designs; however, the review frames this as raising biosafety and oversight concerns because generation can produce actionable biological designs faster than oversight/validation pipelines scale.",
            "uncertainty_quantification": "Not described.",
            "calibration_quality": "Not reported.",
            "out_of_distribution_performance": "Not numerically reported; success on designing nanobodies suggests capability on OOD molecular design in targeted settings.",
            "validation_proxy_metrics": "Experimental activity assays used as direct validation; possibly proxy plausibility checks by experts prior to experiments.",
            "human_validation_required": true,
            "human_validation_frequency": "Experimental validation required for designs; frequency depends on throughput and prioritization — not quantified.",
            "formal_verification_used": false,
            "domain_formalization_level": "empirical (protein engineering) — wet-lab validation necessary.",
            "gap_mitigation_strategies": "Authors and reviewers call for governance, controlled trials and mitigation strategies; no empirical mitigation results reported here.",
            "evidence_supporting_gap": "LLM agents produced experimentally validated designs but the review emphasizes risks and unpredictability, supporting concern that generation can outpace safe validation and oversight.",
            "evidence_contradicting_gap": "Successful experimental validation of some designs shows generation can be meaningfully validated when resources permit.",
            "computational_cost_ratio": null,
            "uuid": "e2091.4"
        },
        {
            "name_short": "Coscientist",
            "name_full": "Coscientist (GPT-4 powered autonomous chemical research system)",
            "brief_description": "A system powered by GPT-4 that can design, plan and execute complex experiments including chemical syntheses by searching documents, writing code, and operating robotic lab devices.",
            "citation_title": "Autonomous chemical research with large language models.",
            "mention_or_use": "mention",
            "system_name": "Coscientist",
            "system_type": "LLM-driven autonomous experiment planning and execution system",
            "scientific_domain": "chemistry / automated synthesis",
            "output_type": "experimental plans, synthesis protocols, control code for robotic devices, novel synthesized compounds",
            "novelty_level": "moderately novel — automates planning/execution but synthesis targets may be within known chemical spaces",
            "generation_method": "LLM-based planning informed by literature search and code generation to operate lab robotics",
            "validation_method": "Execution of planned experiments in the lab (robotic execution) and empirical observation of reaction/product outcomes; article describes end-to-end experiment runs.",
            "generation_performance": "Described as capable of end-to-end execution including code writing and robot operating; no quantitative success/failure rates provided in this review.",
            "validation_performance": "Validation via empirical experiment outcomes; no aggregate metrics or FP/FN rates provided.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "performance_vs_novelty": "Not quantified; review notes unpredictabilities and biosafety concerns when automation reduces human oversight.",
            "generation_validation_comparison": "System demonstrates that generation + execution can be automated, but review highlights oversight and safety evaluation lagging — generation/execution may outpace governance and validation pipelines.",
            "uncertainty_quantification": "Not described.",
            "calibration_quality": "Not reported.",
            "out_of_distribution_performance": null,
            "validation_proxy_metrics": "Empirical experimental outcomes (direct validation) used rather than proxies in described examples.",
            "human_validation_required": true,
            "human_validation_frequency": "Human oversight and governance recommended; frequency not quantified.",
            "formal_verification_used": false,
            "domain_formalization_level": "empirical (laboratory chemistry) — requires wet-lab validation and safety oversight.",
            "gap_mitigation_strategies": "Calls for swift government action, comprehensive testing, and mitigation strategies; internal company red-teaming and controlled trials mentioned elsewhere in review but not specific to Coscientist.",
            "evidence_supporting_gap": "Autonomous systems can design and physically execute experiments, increasing the speed at which novel biological/chemical interventions can be produced relative to traditional oversight/validation capacity.",
            "evidence_contradicting_gap": "Empirical experiment execution demonstrates that validation can be integrated into autonomous pipelines when properly resourced.",
            "computational_cost_ratio": null,
            "uuid": "e2091.5"
        },
        {
            "name_short": "Mia (Kheiron)",
            "name_full": "Mia — AI tool developed by Imperial College London and Kheiron Medical Technologies",
            "brief_description": "An AI system for mammography that in an NHS evaluation detected 13% more early-stage breast cancers and identified cases missed by human readers in a &gt;10,000-woman dataset.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Mia",
            "system_type": "medical imaging deep learning model",
            "scientific_domain": "radiology / breast cancer screening",
            "output_type": "predicted cancer presence on mammograms (binary detections and cases to recall)",
            "novelty_level": "incremental to moderately novel within medical imaging — improves sensitivity/early detection over some human readers",
            "generation_method": "CNN or deep-learning-based image classification trained on labeled mammography datasets (architecture not specified in review)",
            "validation_method": "Retrospective evaluation on NHS dataset of &gt;10,000 women; comparison to human radiologist readings and ground truth cancer diagnoses.",
            "generation_performance": "Reported to detect 13% more early-stage breast cancers and to have identified 11 cases initially missed by human doctors in the NHS evaluation; led to more recalls.",
            "validation_performance": "Validated retrospectively on &gt;10,000 mammograms with comparison to human readers and diagnostic follow-up; quantitative sensitivity/specificity not provided beyond the 13% improvement figure.",
            "false_positive_rate": "Not explicitly quantified in review; mentions 'led to more women being recalled' which implies increased false positives may accompany increased sensitivity but no rate provided.",
            "false_negative_rate": null,
            "performance_vs_novelty": "Not applicable — model evaluated on screening data; review does not report how performance changes with out-of-distribution cases.",
            "generation_validation_comparison": "Generation (detection) was validated retrospectively and compared to humans — in this case validation kept pace because labeled clinical data existed; no major generation-validation gap for this application is described.",
            "uncertainty_quantification": "Not reported.",
            "calibration_quality": "Not reported.",
            "out_of_distribution_performance": null,
            "validation_proxy_metrics": "Comparison to human diagnosis and subsequent confirmed diagnoses used as ground truth.",
            "human_validation_required": true,
            "human_validation_frequency": "Used as assistive tool in screening with human oversight; frequency implied continuous (every case) but not numerically specified.",
            "formal_verification_used": null,
            "domain_formalization_level": "empirical clinical domain with established ground truth (diagnoses) enabling robust validation.",
            "gap_mitigation_strategies": "Retrospective evaluation on large real-world datasets and NHS testing provide external validation; no further mitigation strategies described in review.",
            "evidence_supporting_gap": "Not strongly supported here — this clinical task had available labeled data enabling validation alongside generation.",
            "evidence_contradicting_gap": "Successful NHS-scale validation indicates generation and validation were aligned for this specific clinical detection task.",
            "computational_cost_ratio": null,
            "uuid": "e2091.6"
        },
        {
            "name_short": "TBINet",
            "name_full": "TBINet (deep-learning model for pulmonary tuberculosis infectivity)",
            "brief_description": "A deep-learning model using CT images to identify pulmonary tuberculosis infectivity, validated by high AUC values and interpretability via Grad-CAM.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "TBINet",
            "system_type": "deep convolutional neural network for CT image analysis",
            "scientific_domain": "infectious disease imaging / diagnostic radiology",
            "output_type": "probability of pulmonary tuberculosis infectivity / diagnostic classification",
            "novelty_level": "incremental within medical imaging diagnostics",
            "generation_method": "CNN-based image classification trained on labeled CT datasets",
            "validation_method": "Validation on independent screening studies with reported AUC metrics and Grad-CAM visualization for interpretability.",
            "generation_performance": "Described as validated by 'high AUC values' (no numeric AUC in review) and provided interpretable maps via Grad-CAM.",
            "validation_performance": "High AUC values reported in the original study (not numerically specified in this review).",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "performance_vs_novelty": "Not described.",
            "generation_validation_comparison": "Generation performance reported with conventional statistical metrics (AUC) indicating alignment between model output and validation for this imaging task.",
            "uncertainty_quantification": "Not described.",
            "calibration_quality": "Not reported.",
            "out_of_distribution_performance": null,
            "validation_proxy_metrics": "AUC and Grad-CAM used as validation/interpretability proxies.",
            "human_validation_required": true,
            "human_validation_frequency": "Intended to assist clinicians; human oversight implied but not quantified.",
            "formal_verification_used": null,
            "domain_formalization_level": "empirical (medical imaging) with clinical ground truth enabling validation.",
            "gap_mitigation_strategies": "Use of independent validation cohorts and interpretability maps (Grad-CAM) to build trust; numerical effectiveness not provided in review.",
            "evidence_supporting_gap": "Not strongly indicated; model validation used standard metrics with independent cohorts.",
            "evidence_contradicting_gap": "High AUC validation suggests generation matched validation capability for this task.",
            "computational_cost_ratio": null,
            "uuid": "e2091.7"
        },
        {
            "name_short": "Biomed-Parse",
            "name_full": "Biomed-Parse",
            "brief_description": "A foundation AI model for joint segmentation, detection and recognition across nine biomedical imaging modalities, trained on a dataset of &gt;6 million image/annotation/text triples and reported to outperform models like MedSAM and SAM on irregular objects.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Biomed-Parse",
            "system_type": "multimodal vision-language deep learning foundation model",
            "scientific_domain": "biomedical image analysis / medical imaging",
            "output_type": "segmentation masks, object detection and recognition labels from biomedical images (multiple modalities)",
            "novelty_level": "moderately novel — unified multi-modality model addressing segmentation/detection/recognition jointly",
            "generation_method": "vision-language pretraining on large paired image/segmentation/text datasets to enable conditional segmentation from textual prompts and integrated tasks",
            "validation_method": "Benchmarked against existing tools (MedSAM, SAM) and demonstrated superior performance on irregularly-shaped object segmentation; trained/validated on a large assembled dataset of &gt;6 million triples.",
            "generation_performance": "Reported to outperform existing tools in handling irregularly shaped objects and to unify tasks; no numeric accuracy/IoU/etc. provided in review.",
            "validation_performance": "Qualitative statements of outperforming other models; no numeric performance metrics included in the review text.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "performance_vs_novelty": "Not reported.",
            "generation_validation_comparison": "Model comparisons to established models indicate validation benchmarks are used; specifics not provided, but review indicates a performance advantage in targeted tasks.",
            "uncertainty_quantification": "Not described.",
            "calibration_quality": "Not reported.",
            "out_of_distribution_performance": null,
            "validation_proxy_metrics": "Benchmarks against prior models and comparison on curated test sets used as proxies for generalization.",
            "human_validation_required": true,
            "human_validation_frequency": "Suggested for clinical deployment and interpretation; not quantified.",
            "formal_verification_used": false,
            "domain_formalization_level": "empirical (image analysis) but benefits from ground-truth segmentation annotations for validation.",
            "gap_mitigation_strategies": "Large-scale dataset creation and benchmarking against existing models; no quantitative mitigation outcomes reported.",
            "evidence_supporting_gap": "Not directly indicated; model performance claims are benchmark-based rather than indicating generation outstrips validation.",
            "evidence_contradicting_gap": "Benchmark comparisons imply validation kept pace for the model development lifecycle.",
            "computational_cost_ratio": null,
            "uuid": "e2091.8"
        },
        {
            "name_short": "Hallucination / Reference Error Findings",
            "name_full": "LLM hallucination and reference error reports (Chelli et al. and others)",
            "brief_description": "Empirical findings and tracking tools that document LLM hallucination rates and reference fabrication: some chatbots confabulate facts in up to ~30% of cases and various chatbots made reference errors between 30% and 90% in a 2024 study.",
            "citation_title": "Hallucination Rates and Reference Accuracy of ChatGPT and Bard for Systematic Reviews: Comparative Analysis.",
            "mention_or_use": "mention",
            "system_name": "General LLM chatbots (ChatGPT, Bard, etc.)",
            "system_type": "large language models / generative models",
            "scientific_domain": "NLP applied to scientific text / systematic review support",
            "output_type": "generated text, references, summaries and systematic-review related outputs",
            "novelty_level": "in-distribution text generation but can fabricate OOD facts (hallucinations)",
            "generation_method": "autoregressive language modeling trained on large text corpora",
            "validation_method": "External fact-checking against source documents and reference corpora; community-run leaderboards and indices (Hallucination Vulnerability Index, Hallucinations Leaderboard) to quantify errors",
            "generation_performance": "Varies widely; review cites study showing reference errors between 30% and 90% across different chatbots, and confabulation up to ~30% in other measures.",
            "validation_performance": "Validation often requires human fact-checking; automated detection tools exist but with variable effectiveness (no unified accuracy reported in review).",
            "false_positive_rate": "Reported reference fabrication rates in one study ranged from 30% to 90% (these represent produced references that were incorrect); exact FP definition varies by study.",
            "false_negative_rate": null,
            "performance_vs_novelty": "Hallucination frequency increases with tasks requiring factual grounding beyond model's memorized/training distribution, implying worse performance on novel/out-of-distribution factual claims.",
            "generation_validation_comparison": "Clear asymmetry: LLMs can rapidly generate plausible references/claims while validation (human or automated fact-checking) is slower and error-prone; review highlights this gap.",
            "uncertainty_quantification": "Not reliably present in many LLMs; external platforms/metrics track hallucinations but intrinsic uncertainty outputs are not consistently calibrated.",
            "calibration_quality": "Described as poor based on observed fabrication/hallucination rates.",
            "out_of_distribution_performance": "Poorer — higher hallucination and fabricated reference rates for OOD claims; precise metrics not standardized.",
            "validation_proxy_metrics": "Plausibility/coherence and automated detectors (various AI detectors) are used but have limited reliability.",
            "human_validation_required": true,
            "human_validation_frequency": "High for factual claims and references; review suggests systematic human fact-checking is necessary.",
            "formal_verification_used": false,
            "domain_formalization_level": "textual / bibliographic (semi-formal) but factual grounding requires empirical/source verification.",
            "gap_mitigation_strategies": "Hallucination Vulnerability Index, leaderboards, automated detection tools and recommended human fact-checking; effectiveness variable and not fully quantified.",
            "evidence_supporting_gap": "Empirical studies cited showing 30–90% reference errors and up to ~30% confabulation support the claim that generation exceeds reliable validation.",
            "evidence_contradicting_gap": "None presented in review; some validation tooling exists but with limited success.",
            "computational_cost_ratio": null,
            "uuid": "e2091.9"
        },
        {
            "name_short": "Prov-GigaPath",
            "name_full": "Prov-GigaPath (whole-slide pathology foundation model)",
            "brief_description": "An open-access whole-slide pathology foundation model pretrained on over one billion 256×256 pathology tiles and pathology reports, intended for scalable digital pathology tasks.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Prov-GigaPath",
            "system_type": "vision-language foundation model / large convolutional / transformer-based architecture for pathology",
            "scientific_domain": "digital pathology",
            "output_type": "slide-level representations, segmentation, classification and other pathology downstream task outputs",
            "novelty_level": "moderately novel — large-scale pretraining on real-world pathology tiles enabling transfer to many downstream pathology tasks",
            "generation_method": "self-supervised / supervised pretraining on large image-text (images + pathology reports) corpora to learn slide representations",
            "validation_method": "Benchmarked across digital pathology tasks and promoted as excelling in these tasks; review states 'excelling' but provides no numeric task metrics.",
            "generation_performance": "Pretrained on &gt;1 billion tiles from &gt;170k slides; reported to perform strongly across tasks though no quantitative metrics provided in the review.",
            "validation_performance": "Qualitatively described as excelling; numerical validation metrics not included in review text.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "performance_vs_novelty": "Not described.",
            "generation_validation_comparison": "Review indicates foundation-model pretraining followed by downstream task evaluation; no explicit gap analysis provided but scale suggests generation capability is large and validation depends on downstream labels.",
            "uncertainty_quantification": "Not described.",
            "calibration_quality": "Not reported.",
            "out_of_distribution_performance": null,
            "validation_proxy_metrics": "Downstream task benchmarks and comparisons to prior models used as proxies.",
            "human_validation_required": true,
            "human_validation_frequency": "Human pathologist oversight implied for clinical deployment; frequency not quantified.",
            "formal_verification_used": false,
            "domain_formalization_level": "empirical clinical/diagnostic domain — requires clinical validation.",
            "gap_mitigation_strategies": "Open-weight model release to encourage community evaluation and validation; no quantitative outcomes reported.",
            "evidence_supporting_gap": "Large-scale pretrained models can be applied quickly across many tasks while collection of high-quality clinical labels for validation remains a bottleneck.",
            "evidence_contradicting_gap": "Qualitative claims of strong downstream performance suggest validation is feasible for many tasks with appropriate datasets.",
            "computational_cost_ratio": null,
            "uuid": "e2091.10"
        },
        {
            "name_short": "CellSAM / CellFinder",
            "name_full": "CellSAM (built on CellFinder prompting SAM) / CellFinder",
            "brief_description": "Tools for automated cell detection and segmentation across modalities: CellFinder trained for automated 3D cell detection and CellSAM repurposes prompting of SAM to segment cells across mammalian, yeast and bacterial images.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "CellSAM / CellFinder",
            "system_type": "object-detection and segmentation models (deep learning)",
            "scientific_domain": "cell imaging / spatial omics / microscopy",
            "output_type": "cell detection coordinates and segmentation masks (2D/3D)",
            "novelty_level": "incremental to moderately novel in scale and cross-modality generalization",
            "generation_method": "training object detectors on annotated microscopy datasets (CellFinder) and using prompt-based segmentation of a general segment-anything model (CellSAM) to adapt to cell images",
            "validation_method": "Trained and tested on large-scale images (e.g., whole-brain images for CellFinder) and across modalities for CellSAM; references the Cell Tracking Challenge and benchmarking datasets for evaluation.",
            "generation_performance": "Reported to perform automated 3D cell detection and multi-modality segmentation; no numeric precision/recall or segmentation IoU numbers given in review.",
            "validation_performance": "Evaluation via community benchmarks (Cell Tracking Challenge) and curated test sets; review references improved accuracy but provides no numbers.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "performance_vs_novelty": "Not quantified; community challenges provide standardized metrics but results not enumerated here.",
            "generation_validation_comparison": "Benchmarking infrastructure (Cell Tracking Challenge) exists to align generation and validation; review implies ongoing improvements but does not claim full parity.",
            "uncertainty_quantification": "Not described.",
            "calibration_quality": "Not reported.",
            "out_of_distribution_performance": null,
            "validation_proxy_metrics": "Standardized challenge metrics and annotated video/image repositories are used as proxies for real-world performance.",
            "human_validation_required": true,
            "human_validation_frequency": "Human curation/inspection implied for critical analyses and downstream biological interpretation; not quantified.",
            "formal_verification_used": false,
            "domain_formalization_level": "empirical imaging domain; validation depends on annotated datasets.",
            "gap_mitigation_strategies": "Public benchmarks and repositories (CTC) to measure progress and encourage reproducible validation.",
            "evidence_supporting_gap": "Mention that automated methods have improved but the need for standardized evaluation and human oversight continues, indicating that generation improvements require rigorous validation.",
            "evidence_contradicting_gap": "Existence of community benchmarks and improved results implies validation infrastructure is keeping pace in this subdomain.",
            "computational_cost_ratio": null,
            "uuid": "e2091.11"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Accurate proteome-wide missense variant effect prediction with AlphaMissense.",
            "rating": 2
        },
        {
            "paper_title": "Autonomous chemical research with large language models.",
            "rating": 2
        },
        {
            "paper_title": "The Virtual Lab: AI agents design new SARS-CoV-2 nanobodies with experimental validation.",
            "rating": 2
        },
        {
            "paper_title": "PEA-COCK: a machine learning approach to assess the validity of cell type-specific enhancer-gene regulatory relationships.",
            "rating": 2
        },
        {
            "paper_title": "Hallucination Rates and Reference Accuracy of ChatGPT and Bard for Systematic Reviews: Comparative Analysis.",
            "rating": 2
        },
        {
            "paper_title": "A foundation model for joint segmentation, detection and recognition of biomedical objects across nine modalities.",
            "rating": 2
        },
        {
            "paper_title": "A free, AI-powered research tool for scientific literature. (Semantic Scholar / related tool descriptions)",
            "rating": 1
        },
        {
            "paper_title": "Prov-GigaPath: Whole-Slide Foundation Model for Digital Pathology.",
            "rating": 2
        },
        {
            "paper_title": "Neural network fast-classifies biological images through features selecting to power automated microscopy.",
            "rating": 1
        },
        {
            "paper_title": "The cell tracking challenge: 10 years of objective benchmarking.",
            "rating": 2
        }
    ],
    "cost": 0.02663825,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Cutting-edge AI tools revolutionizing scientific research in life sciences</p>
<p>Katarzyna Lorenc-Kuku£a katarzyna.lorenc@translmed.com 
Translmed Publishing Group
WieluñPoland</p>
<p>Katarzyna Lorenc-Kukula 
Translmed Publishing Group
WieluńPoland</p>
<p>Cutting-edge AI tools revolutionizing scientific research in life sciences
79CC30E8E4E482D811101C6789A931CC10.5114/bta/200803Received: 06.01.2025; revised: 31.01.2025; accepted: 04.02.2025deep learningmachine learningAI-driven discoverypredictive modelingartificial intelligence2024 Nobel prizes
Artificial intelligence (AI) is becoming a transformative force in the life sciences, pushing the boundaries of possibility.Imagine AI automating time-consuming tasks, uncovering hidden patterns in vast datasets, designing proteins in minutes instead of years, and even predicting disease outbreaks before they occur.This review explores the latest AI tools revolutionizing scientific fields, including research and data analysis, healthcare, and tools supporting scientific writing.Beyond data processing, AI is reshaping how scientists draft and share their findings, enhancing processes ranging from literature reviews to citation management.However, with great power comes great responsibility.Are we prepared for this leap?This review delves into the forefront of AI in the life sciences, where innovation meets responsibility.</p>
<p>Introduction</p>
<p>Artificial intelligence (AI) is revolutionizing scientific research, particularly in the field of life sciences.AI advances science by enabling the analysis of data and addressing challenges that were previously beyond the scope of traditional research methods.Tools such as AlphaFold, which has revolutionized structural biology and contributed to a Nobel Prize-winning breakthrough, and BioBERT, a natural language processing model for biomedical text analysis, are prime examples of how AI supports scientific research.AI tools assist in analyzing large datasets, automating repetitive tasks, predicting, optimization and modeling complex processes, thereby accelerating scientific discovery.Figure 1 illustrates the application possibilities of artificial intelligence in the fields of healthcare, clinical trials, biosimulation, omics research, personalized medicine, early disease detection, vaccine and drug discovery, bioimaging, robotic laboratory equipment, and streamlining scientific paper writing.</p>
<p>Below, an overview of the latest AI tools and their potential applications in life sciences is presented, along with tools that aid scientists in writing scientific publications.</p>
<p>AI in proteomics</p>
<p>AI plays a transformative role in proteomics by revolutionizing the prediction of protein structures.Tools like AlphaFold, developed by DeepMind, predict protein structures with unprecedented accuracy based solely on amino acid sequences.This breakthrough has significantly accele rated research on protein functions and their roles in diseases, facilitating drug development and disease research.The significance of AI in this field was underscored in 2024 when the Nobel Prize in Chemistry was awarded to three pioneers for their development of AI-based methods for predicting protein structures (Callaway, 2024).Demis Hassabis and John Jumper of Google DeepMind were recognized for their AI programs that accurately predict the 3D shapes of proteins, while David Baker from the University of Washington was honored for his work in designing novel proteins.These advancements are essential for understanding biomolecular functions and for developing drugs and vaccines (Protein designer and structure solvers, 2024; Royal Swedish Academy of Sciences, 2024).</p>
<p>Previously, imaging techniques such as X-ray crystallography and cryo-electron microscopy were the primary methods for determining protein structures.However, these methods were time-consuming, expensive, and often challenging to apply to all proteins.In 2020, the introduction of AlphaFold 2 revolutionized protein structure prediction.By leveraging vast databases of protein structures and amino acid sequences, it achieved performance comparable to imaging methods and delivered over 200 million predictions (Service, 2020).</p>
<p>In 2021, DeepMind launched Isomorphic Labs to leverage its AI tools for drug design, inspiring pharmaceutical companies worldwide to adopt similar approaches to combat cancer, infectious diseases, hypertension, and obesity (Protein designer and structure solvers, 2024).Within just 3 years, AlphaFold 2 enabled 1.8 million researchers to map approximately six million different protein structures.In a groundbreaking development, DeepMind recently released AlphaFold 3, which provides results in minutes instead of years of laboratory work (Abramson et al., 2024).Unlike earlier versions, which focused on modeling amino acid strands folding into 3D protein shapes, AlphaFold 3 also predicts how folded proteins bind and interact with other molecules, including DNA, RNA, and other proteins.AlphaFold 3 was released shortly after RoseTTAFold All-Atom Figure 1.Applications of AI in scientific research (Krishna et al., 2024), a similar AI tool from the University of Washington led by David Baker.To promote widespread adoption, DeepMind introduced AlphaFold Server, a free online platform allowing users to generate AlphaFold 3 models of proteins interacting with nearly any biomolecule (AlphaFold Server, 2024).</p>
<p>Understanding interactions between proteins and other biological molecules is essential for drug design and the study of cellular processes.AlphaFold's breakthroughs in protein structure prediction are critical for understanding protein functions, with profound implications for drug development, biotechnology, and disease research.These advancements highlight the transformative role of AI in advancing proteomics and biomedical research.AlphaFold has ushered in a new era in science and medicine, revolutionizing our understanding of protein interactions and enabling groundbreaking advancements in drug development and biotechnology.</p>
<p>AI in genomic research</p>
<p>In biological research, especially in genomics, proteomics, metabolomics, and transcriptomics, vast quantities of data are generated.AI has become a cornerstone in clinical laboratory genomics, assisting in tasks such as identifying variants in DNA sequencing data, predicting the effects of DNA variants on protein structure and function, linking phenotype ontologies to genetic variants for faster diagnosis, correlating genomic data with tumor staging and treatment, utilizing natural language processing (NLP) to identify critical medical literature, and using interactive chatbots for genetic testing qualification and education (Aradhya et al., 2023).AI addresses challenges in genomic data analysis arising from highthroughput sequencing (Boule steix and Wright, 2022).It facilitates GWAS and PheWAS to identify genotype-phenotype associations, improves pharmacogenomics, supports clinical decisions, predicts risks, identifies causal SNPs, enhances EHR-based phenotyping, and designs CRISPR guide RNA (Lin and Ngiam, 2023).Traditional analysis methods often fall short, prompting the adoption of AI, which enhances both the speed and accuracy of genomic data analysis.AI tools, including computer vision (CV), machine learning (ML), neural networks, and NLP, have become indispensable for addressing challenges in genomic data analysis (Guo et al., 2023).They assist in the analysis, interpretation, modeling, and processing of large-scale genomic data.These tools are utilized in genetic and genomic research across various diseases (Libbrecht and Noble, 2015;Yan, et al., 2016;Dias and Torkamani, 2019;Xu et al., 2019;Alimadadi et al., 2020;De Marvao et al., 2020), biomarker discovery studies (Seashore-Ludlow et al., 2015;Mamatjan et al., 2017), annotating genomic sequence elements (Libbrecht and Noble, 2015), predicting gene functions, understanding regulatory networks, and identifying disease-associated variants.</p>
<p>Recent studies increasingly focus on integrating machine learning techniques to process and analyze high-dimensional genomic data.AI is transforming biomedical genomics by improving data analysis, enhancing disease prediction, and advancing personalized medicine.The integration of AI techniques, such as machine learning and deep learning, allows for efficient processing of complex genomic data, leading to significant progress in biomarker discovery and genetic engineering.AI tools also automate labor-intensive processes, improve diagnostic precision, and facilitate the interpretation of complex genomic data, ultimately revolutionizing genomic research and its clinical applications.A comprehensive review in 2023 summarized 82 high-quality AI-driven biomedical genomic studies, emphasizing AI's contributions to disease diagnosis, prediction, and treatment (Guo et al., 2023).The review highlighted various AI techniques, including ML, deep neural networks (DNN), transfer learning (TL), CV, graph representation learning (GRL), and NLP.In this manuscript, we briefly discuss each of the AI techniques mentioned above: • ML is widely used for analyzing large genomic datasets and identifying patterns and relationships within the data.ML encompasses techniques such as linear regression, logistic regression, decision trees, random forests (RF), support vector machines (SVMs), and neural networks (Guo et al., 2023).These methods are indispensable in medicine, particularly for diagnosing SARS-CoV-2 infections, neurological conditions (Deo, 2015;Rajkomar et al., 2019;Sidey-Gibbons and Sidey-Gibbons, 2019;Helmy et al., 2022), identifying genes linked to sepsis-induced ARDS, pinpointing chronic pain locations, uncovering genetic risk factors for various conditions, and diagnosing cancers such as leukemia, breast and lung cancer, and endometrial carcinoma (Guo et al., 2023).</p>
<p>• Convolutional neural networks (CNNs) are a prominent type of DNN.They have demonstrated significant effectiveness in identifying cancers by analyzing gene expression profiles.In one study, researchers analyzed 6136 samples from 11 cancer types, integrating gene expression profiles with protein-protein interaction (PPI) networks to create 2D images (Chuang et al., 2021).These images were used to develop a CNN that achieved high accuracy in distinguishing normal samples from tumors and identifying specific cancer types.CNNs are not only valuable for diagnosing and predicting cancer outcomes but also for identifying various cancer biomarkers.In the field of cancer research, DNN-based applications have been applied for detection purposes (Zhang et al., 2022).By utilizing medical images and omics data, CNN techniques were used to detect metastasis indicators, cancer cell types, and molecular subtypes using medical images and omics data, providing critical information essential for therapeutic management (Chuang et al., 2021).• Transfer learning is a sophisticated AI approach in genomics particularly useful in cross-population studies (Zhao et al., 2022).It enhances predictive performance for disease risk (Jónsson et al., 2019), predicts patient data trends, and provides new insights into clinical findings from genotypic information (Dong and Boyle, 2019).Transfer learning also improves functional variant prediction accuracy, enhances enhancer-promoter interaction predictions, imputes missing RNAsequencing data, and predicts rare diseases using gene expression datasets (Guo et al., 2023).In healthcare, transfer learning boosts diagnostic accuracy for cancer detection, mutation identification, cancer type detection from circulating tumor cells, and studying circular RNAs in nonobstructive azoospermia (Guo et al., 2023).• Another AI tool widely used in genomic analysis is recurrent neural networks (RNNs).RNNs are particularly valuable for analyzing biomedical data collected over time.While CNNs excel in image analysis, they do not account for temporal dimensions.RNNs, designed for sequential data, are ideal for analyzing time-series data such as patient health records.By incorporating temporal aspects, RNNs enable the understanding of data progression and changes over time, which is crucial for accurate predictions and informed decisions in genomics (Guo et al., 2023).For instance, RNNs significantly enhanced genomic analysis for predicting chronic diseases, as demonstrated in a Type 2 diabetes study using UK Biobank data by Srinivasu et al. (2022).• CV uses mathematical methods to derive three-dimensional shapes and appearances of objects from images.Popular architectures such as AlexNet, VGGNet, GoogLeNet, and ResNet have broad applications in biomedical genomic research, including linking imaging phenotypes to tumor genetic profiles (Bodalal et al., 2019).• GRL is used to transform complex genomic data into simplified, low-dimensional vectors, facilitating easier analysis and interpretation.In genomics, GRL is particularly valuable for converting sequencing data into graph structures based on gene associations or expression similarities (Guo et al., 2023).Applications of GRL techniques include diagnosing diseases such as multiple sclerosis using graph attention networks on single-cell RNA sequencing data, classifying cancers with graph convolutional networks that integrate gene expression and protein-protein interaction data, and identifying cancer subtypes and intracluster heterogeneity using dimensionality reduction techniques.GRL also aids in identifying new candidate disease genes, leveraging gene-disease associations for clinical investigation, performing link prediction tasks, integrating heterogeneous associations, constructing networks with various gene nodes, and predicting gene-disease associations and disease classifications (Guo et al., 2023).• In genomic research, NLP assists by converting textual data, such as Electronic Health Records (EHRs), into computable features for Genome-Wide Association Studies (GWAS) and Phenome-Wide Association Studies (PheWAS).It also extracts detailed phenotypic characteristics for diagnosing genetic disorders (Guo et al., 2023).BioBERT, a revolutionary NLP tool, has significantly advanced biomedical research (Lee et al., 2020).Pretrained on vast amounts of biomedical literature, BioBERT enables researchers to understand and interpret complex biomedical information, making it invaluable for biomedical text mining, drug discovery, disease research, and biotechnology (Lee et al., 2020).The rapid expansion of biomedical literature underscores the need for tools like BioBERT to handle the influx of information effectively.Its open-access model (Lee et al., 2020), which includes pre-trained weights and fine-tuning source code, allows researchers worldwide to leverage this technology without incurring significant costs.The volume of data on AI applications in gene and genome analysis continues to grow.Recently, Hu et al. (2024) in Nature Methods highlighted the effectiveness of large language models (LLMs) such as GPT-4, GPT-3.5, Gemini Pro, Mixtral Instruct, and Llama2 70b in gene function analysis.These models demonstrated greater specificity and broader gene coverage compared to traditional methods, illustrating their potential to automate functional genomics research.LLMs like GPT-4 can facilitate the understanding of gene functions and interactions.</p>
<p>AI has proven instrumental in identifying complex structural variants (cxSVs) in whole-genome sequencing data.Researchers at Stanford Medicine have developed ARC-SV, an AI-based method leveraging machine learning to improve the precision of detecting and reconstructing cxSVs from standard datasets.This technique not only uncovers rare genetic variations but also connects them to neural genes and regions of rapid human-specific evolution.Furthermore, it links cxSVs to differences in gene expression and chromatin accessibility across various brain regions, advancing the genetic understanding of major psychiatric disorders (Zhou et al., 2024).</p>
<p>The advent of artificial intelligence has driven transformative breakthroughs in genetic medicine.Genomic medicine-the application of genomic information in clinical treatments-holds immense potential for personalized and targeted medical interventions.The integration of AI algorithms into genomic data processing has yielded remarkable successes.These algorithms excel at decoding complex genetic patterns, predicting disease probabilities, and enhancing precision medicine.A study by Hassan et al. (2022) in Nature Medicine demonstrated that machine learning algorithms can accurately predict patient responses to cancer immunotherapy based on genetic profiles.This finding underscores AI's transformative impact on healthcare, particularly in advancing genomic medicine, providing insights into disease causation, and enabling personalized treatment plans.By leveraging advanced algorithms and large datasets, AI is poised to revolutionize healthcare, tailoring medical treatments to individual patients.</p>
<p>The pursuit of fast, affordable, and precise DNA sequencing remains a critical goal in advancing personalized medicine.Conventional bioinformatics methods often struggle to efficiently process and interpret the vast volumes of generated data.In contrast, deep neural networks (DNNs) excel at recognizing complex patterns, predicting phenotypes, and classifying genomic variants.Recent advancements in deep learning have demonstrated its effectiveness in various biomedical applications, especially in Next-Generation Sequencing (NGS).These methods employ advanced neural networks to process extensive genomic data, enhancing sequencing accuracy and efficiency.By automating data analysis and uncovering patterns within complex datasets, deep learning significantly advances our understanding of genetic variations and their health implications (Özgür and Orman, 2023).Integrating machine learning algorithms into NGS workflows has the potential to reveal hidden insights, accelerate discoveries, and drive breakthroughs in genomics.</p>
<p>AI can enhance our understanding of how single amino acid changes in proteins impact their function.The deep learning model AlphaMissense, building on the protein structure prediction tool AlphaFold2, utilizes vast biological sequence data and predicted structural contexts to assess the pathogenicity of gene variants.This capability is essential for understanding the implications of genetic variations in disease contexts (Cheng et al., 2023).AlphaMissense predictions play a pivotal role in elucidating how genetic variants affect protein function, aiding in the identification of pathogenic missense mutations and previously undetected diseasecausing genes.This advancement improves diagnostic accuracy for rare genetic disorders and fosters the development of advanced tools for predicting protein variant effects based on structural models.</p>
<p>In 2024, researchers introduced another therapeutic application of AI: designing and validating engineered cis-regulatory elements (CREs) using AI models.These CREs were tailored for targeted gene expression in specific cell types (Gosai et al., 2024).AI-engineered synthetic CREs demonstrated the potential to target gene therapies to particular cell populations.Deep neural network modeling was used to predict CRE activity across different cell types, while a high-throughput method called Massively Parallel Reporter Assays (MPRAs) enabled the rapid empirical testing of thousands of designed CREs.Results showed that synthetic CREs were more effective for targeted gene expression than natural sequences derived from the human genome, highlighting their utility in therapeutic and biotechnological applications.By leveraging AI in the design of synthetic CREs, scientists can create programmable regulatory elements with precise targeting capabilities, enhancing the efficacy and specificity of gene therapies (Gosai et al., 2024).</p>
<p>PEACOCK is another example of an AI tool used to analyze large datasets typical in genomic research, through machine learning techniques.This tool efficiently analyzed vast amounts of genomic data to identify potential regulatory links that may not be easily discernible through traditional methods.This model could predict cell type-specific enhancer-gene regulatory links and was trained using various cell lines and a selected set of experimental data validated and published in scientific literature.The efficiency and scalability of the PEACOCK were proved by its ability to score a vast number of enhancer-gene pairs across the entire genome (~17 million pairs).The ability to score enhancer-gene pairs quantitatively allows researchers to incorporate these scores into broader statistical analyses of disease-associated variants.The scores generated by the PEACOCK model have significant implications for disease research, particularly in understanding the role of enhancers in gene regulation.The quantitative scores allow researchers to prioritize enhancer-gene pairs that are most likely to be involved in disease processes.By focusing on high-scoring pairs, researchers can concentrate on the most promising enhancer-gene pairs in diseases like cancer, can gain insights into the molecular mechanisms underlying diseases and hopefully lead to new therapeutic targets.Moreover, the cell typespecific nature of the scores can help understand how gene regulation may differ across various tissues, which is especially important in diseases where certain genes may be activated or silenced.These scores can be used in the statistical research approach of genomic diseaseassociated variants identified in a GWAS (Genome-Wide Association Studies, 2024).GWAS surveys the genomes of people, looking for genomic variants that occur more frequently in those with a specific disease compared to those without the disease.Once such genomic variants are identified, they are used to search for nearby variants that contribute directly to the disease.By linking these variants to specific enhancer-gene interactions, researchers can uncover how genetic variations contribute to disease risk and progression, potentially leading to new therapeutic targets.Understanding which enhancers are active or inactive in specific cell types can help us understand how disruption in enhancer function contributes to various diseases and genetic disorders.Scores designed by the PEACOCK model can serve as a useful tool for disease research by enabling targeted investigations, enhancing understanding of disease mechanisms, and linking genetic variants to regulatory relationships.This approach can contribute to the development of new treatments for disease prevention.The PEREGRINE database (www.peregrineproj.org),a publicly available resource was developed as part of the PEACOCK study.These resources allow researchers to access curated data on enhancer-gene links, facilitating further research and validation of findings in genomic studies (Mills et al., 2023).</p>
<p>AI has proven invaluable in biomarker analysis, particularly during the COVID-19 pandemic.By analyzing omics and clinical datasets, AI and ML algorithms have enabled effective patient stratification and management.These tools have identified critical biomarkers indicating COVID-19 severity and survival, assisting clinicians in prioritizing treatments for patients.Additionally, AI-driven analyses have uncovered gene networks associated with disease severity, underscoring the importance of clinical biomarkers in predicting disease progression (Bello et al., 2023).</p>
<p>AI in metabolomics</p>
<p>Metabolomics has diverse applications, including analyzing metabolic products of the human gut microbiome, identifying biomarkers for disease diagnosis, prognosis, and monitoring, supporting cancer research by uncovering biomarkers and metabolic pathways, aiding studies on neurodegenerative diseases such as Alzheimer's and Parkinson's, investigating xenobiotic exposures, discovering new drug candidates, and studying drug metabolism, toxicology, efficacy, and potential side effects.Additionally, metabolomics integrates with other omics domains to provide a comprehensive view of biological systems (Chi et al., 2024).However, metabolomics generates vast datasets comprising hundreds to thousands of metabolites.Incorporating AI into metabolomics provides deeper insights into metabolic networks, advancing diagnosis, prognosis, and personalized treatment approaches for various diseases.ML techniques, including decision trees, deep learning (DL), neural networks (NN), random forests (RF), and support vector machines (SVM), are employed to classify, regress, or cluster complex metabolomic data (Galal et al., 2022).</p>
<p>Recently, ML techniques have been applied to metabolomics data from various diseases, offering significant insights into metabolic profiles.In cancer research, AI is used to identify metabolic signatures, develop predictive models for cancer detection, prognosis, and recurrence, and analyze metabolomic data to uncover potential biomarkers and metabolic pathways.These approaches have been applied across multiple cancer types, including ovarian, breast, endometrial, hepatocellular carcinoma, gastric cancer, lung, squamous cell carcinoma, non-Hodgkin's lymphoma, renal cell carcinoma, and osteosarcoma (Galal et al., 2022;Chen et al., 2024).In noncancer conditions, AI analyzes metabolomic data to identify biomarkers, disease signatures, and predictive models for various diseases, such as COVID-19, type 2 diabetes, nonalcoholic fatty liver disease (NAFLD), acute myocardial ischemia (AMI), chronic kidney disease (CKD), celiac disease, multiple sclerosis (MS), major depressive disorder, schizophrenia, and autism spectrum disorders.Additionally, AI applications extend to areas such as determining gestational age (Galal et al., 2022).</p>
<p>AI and cancer diagnosis and prediction</p>
<p>AI tools, including ML and deep learning (DL), are revolutionizing cancer diagnosis by enhancing accuracy, improving efficiency, and offering noninvasive techniques.By analyzing medical data such as genomic sequences, imaging, and electronic health records, AI aids in identifying early-stage cancer biomarkers, improving recovery rates, and reducing mortality.These technologies are becoming integral to oncology and preventive healthcare.Integrating AI with genomic studies helps identify cancer-related genes, supporting precision medicine tailored to patients' genetic profiles.</p>
<p>In addition to AI, technologies like big data analytics, cloud computing, and the Internet of Things (IoT) play vital roles in early cancer detection.Big data enables the analysis of large, complex datasets to uncover early cancer indicators, while cloud computing provides secure and efficient platforms for managing vast medical data.Wearable sensors collect biomarker data, offering real-time updates on potential cancer developments.</p>
<p>Advanced AI tools are improving the accuracy of cancer diagnosis through methods such as mammography and CT scans, often outperforming traditional techniques in detecting cancers like breast and lung cancer.AI has shown significant promise in enhancing breast cancer screening by identifying additional cases, improving positive predictive value, and reducing unnecessary recalls (Ng et al., 2023).For instance, the AI tool Mia, developed by Imperial College London and Kheiron Medical Technologies, was found to detect 13% more early-stage breast cancers (Transforming Cancer Diagnostics, 2024).Mia detected more cancers and led to more women being recalled, according to a release from Imperial College London.Mia was named one of the biggest seven medical breakthroughs in 2023 by ABC News (7 of the biggest medical breakthroughs in 2023, 2024).In a recent evaluation by Britain's National Health Service, Mia analyzed mammograms from over 10,000 women.Mia accurately identified patients who had cancer, including 11 cases that were initially missed by human doctors.Mia's impact underscores AI's potential to enhance diagnostic accuracy in breast cancer screening (NHS AI Test, 2024).AI platforms such as CHIEF (Wang et al., 2024), Sybil (Aro et al., 2024), IBM Watson for Oncology (Jie et al., 2021), and Tempus (2024) are further transforming cancer diagnosis and prediction by facilitating early detection, personalized treatment plans, and improved patient outcomes.These platforms significantly reduce diagnosis time while meticulously analyzing imaging and genomic data (Arefin, 2024).An AI model achieved 96% accuracy in diagnosing invasive lobular carcinoma (ILC) using genetic mutations as ground truth (Pareja et al., 2024).In prostate cancer, AI systems have matched the diagnostic capabilities of experienced physicians, enabling early and accurate detection (Shucai and Heyuan, 2024).AI is also revolutionizing colorectal cancer (CRC) diagnosis and treatment, with advancements in classification, detection, digital pathology, endoscopic data processing, high-precision medical image analysis, personalized treatment, and robot-assisted surgery.These developments have substantially improved diagnostic accuracy for CRC.Machine learning prediction models enable faster and more accurate earlystage diagnoses, increase treatment success rates, and reduce colorectal cancer mortality.Furthermore, AI optimizes the allocation and utilization of medical resources, enhancing healthcare efficiency (Sun et al., 2024).</p>
<p>AI in biomedical image analysis and digital pathology</p>
<p>Recently, a team of scientists has created Biomed-Parse, an AI model designed for medical image analysis.BiomedParse can handle nine imaging modalities, enhancing the prediction of systemic diseases by unifying segmentation, detection, and recognition tasks.The model introduces new capabilities, such as segmenting objects through textual descriptions, significantly improving accuracy and expanding applications.To develop BiomedParse, researchers created a dataset of over 6 million triples, including images, segmentation masks, and textual descriptions, using natural language labels from existing datasets.By integrating object recognition, detection, and segmentation, BiomedParse outperforms existing tools like MedSAM (Ma et al., 2024) and SAM (2024), particularly in handling irregularly shaped objects.Its ability to segment and label all objects in an image simultaneously makes it a comprehensive tool for biomedical image analysis, paving the way for efficient and accurate discoveries (Zhao et al., 2024).</p>
<p>Another groundbreaking AI tool is Prov-GigaPath, designed to address the unique computational challenges of digital pathology.Gigapixel slides, comprising tens of thousands of image tiles, require advanced models to process their immense size effectively.Prov-GigaPath is an open-access, whole-slide pathology foundation model pretrained on over one billion 256 × 256 pathology image tiles from more than 170,000 whole slides.By incorporating pathology reports into visionlanguage pretraining, Prov-GigaPath integrates realworld data and supports comprehensive slide modeling.Prov-GigaPath stands out as an open-weight model, excelling in various digital pathology tasks and paving the way for advancements in biomedical discoveries (GigaPath, 2024;Xu et al., 2024).</p>
<p>AI in automated cell tracking and microscopy</p>
<p>Microscopic image analysis is a cornerstone of biological research.AI, particularly deep learning algorithms, has revolutionized this field by enabling faster and more accurate identification of cells and tissue structures.AI is revolutionizing the field of microscopy by enhancing the detection and classification of cells through advanced algorithms.Recently, DeepTrack 2.0 was introduced, a user-friendly software that simplifies the creation, training, and validation of deep-learning models for digital microscopy.DeepTrack 2.0 supports applications such as particle tracking and cell classification, making deep-learning-enhanced video microscopy more accessible to a broader audience (Midtvedt et al., 2021).Additionally, AI algorithms enable realtime microscopy image analysis, facilitating immediate decision-making during image acquisition.Researchers utilized artificial intelligence for real-time cell detection and classification in automated microscopy, employing a high-dimensional feature extractor and machine learning, particularly random forests, to enhance execution performance while maintaining accuracy in biological image analysis (Balluet et al., 2022).AI has also automated cellular segmentation in microscopy images (Eisenstein et al., 2023), improving the accuracy of cell detection and enabling the extraction of quantifiable cellular features.These advancements are critical for understanding cellular organization in various pathologies (Durkee et al., 2021).A paper from 2020 presents a neural architecture search (NAS) method developed to optimize network designs for cell segmentation, achieving better performance compared to traditional methods (Zhu and Meijering, 2020).</p>
<p>Automated cell tracking has become an invaluable tool in biological research.Advances in optical microscopy and machine learning, especially deep neural networks, have driven the need for improved tracking algorithms.The Cell Tracking Challenge (CTC, http:// celltrackingchallenge.net)was created to promote the development and evaluation of these algorithms.Since its launch in 2013, the CTC has offered a free repository of annotated microscopy videos and standardized evaluation metrics.AI techniques, including deep learning models like U-Net, HRNet, R-CNN, and recurrent neural networks, have significantly advanced cell tracking, improving accuracy and enabling more insightful research (Maška et al., 2023).</p>
<p>AI and spatial omics</p>
<p>Spatial omics is a transformative field focused on measuring and mapping biomolecules such as RNA, DNA, and proteins directly within their native tissue environments.This innovative approach enables researchers to observe the spatial organization and interactions of cells, offering unprecedented insights into cellular functions and disease mechanisms.A critical step in spatial omics image analysis is cell segmentation, which facilitates the accurate identification and isolation of individual cells within tissue samples.By preserving spatial context, segmentation allows precise molecular analysis and a deeper understanding of cellular processes.This combination enables highly precise localization and quantification of biomolecules within tissues, allowing for a nuanced view of cellular behavior and interactions.The integration of spatial omics with advanced AI technologies is transforming the landscape of biological research, offering novel ways to explore the complexities of how cells interact and the impact these interactions have on health and disease.This synergy between spatial omics and AI not only enhances our understanding but also opens up new possibilities for diagnostics and therapeutic strategies.Scientists at the Children's Hospital of Philadelphia (CHOP) have recently announced the creation of an AI tool called Cello Type.CelloType is open-source software available in a public repository for noncommercial use (Cello Type, 2024).This model was designed to more accurately identify and classify cells in highcontent tissue images (Pang et al., 2024).Another similar AI tool is NVIDIA VISTA-2D.This model advances cell segmentation and morphology analysis by leveraging NVIDIA's deep learning technology.It is designed to handle large-scale tasks and improve the accuracy of cell detection and segmentation (NVIDIA DEVEL-OPER, 2024).Two companies, NanoString Technologies and NVIDIA have joined forces to advance spatial biology research.NanoString's CosMx Spatial Molecular Imager (SMI) platform integrates NVIDIA's GPU technology, enabling the rapid processing and analysis of large datasets.This collaboration allows for the imaging of the entire transcriptome within cells and tissues, providing deeper insights into cellular functions and disease mechanisms (NVIDIA, 2024a).</p>
<p>AI has also facilitated advancements in 3D cell detection and segmentation.Researchers trained an object detector called CellFinder for automated 3D cell detection in large-scale images, including neuronal somata in whole-brain images of mice (Tyson et al., 2021;BrainGlobe, 2024).In 2024, the CellSAM tool was introduced, excelling in segmenting images of mammalian cells, yeast, and bacteria (Israel et al., 2024) across various imaging modalities.CellSAM builds upon CellFinder by retraining it to prompt the Segment Anything Model (SAM) for segmentation.This innovative approach enables a single AI model to accurately segment images of mammalian cells in tissues and cell culture, as well as yeast and bacteria, collected across various imaging modalities (CellSAM, 2024).</p>
<p>AI and electron microscopy (EM)</p>
<p>Recent advancements in EM have been significantly enhanced by ML-based analysis.Research by Xu et al. (2021) demonstrates the integration of ML techniques in improving focused ion beam-scanning electron microscopy (FIB-SEM).These advancements include enhanced data acquisition, improved signal detection, and faster scanning, enabling the imaging of cellular samples at nanometer resolution.ML-based analysis efficiently processes and interprets complex imaging data, providing detailed visualizations of cellular structures.</p>
<p>Similarly, Heinrich et al. (2021) have made strides in data acquisition and ML-based analysis in microscopy, improving the ability to map and analyze molecular interactions within cells.These developments bring researchers closer to a comprehensive understanding of cellular physiology.Collectively, these studies highlight the use of AI and ML in overcoming traditional microscopy limitations and advancing the field of cell biology (Swedlow and Collinson, 2021).</p>
<p>AI and infectious diseases</p>
<p>AI has made significant advancements in infectious diseases, playing a crucial role in developing diagnostic and therapeutic methods, forecasting outbreaks, optimizing treatment plans, and analyzing diagnostic images.It is instrumental in discovering anti-infective drugs and vaccines and combating the rise of antimicrobial resistance.</p>
<p>AI-powered clinical decision support systems forecast disease outbreaks, assist with precise diagnoses, enhance treatment plans, and track epidemiological trends by analyzing extensive datasets.Furthermore, AI improves the analysis of diagnostic images and disease identification, enabling quicker and more accurate results.By examining large datasets, AI systems boost diagnostic precision and treatment strategies, predict disease outbreaks, and monitor epidemiological patterns (Aslan, n.d.).Advances in AI applications for infectious diseases hold promise for more effective intervention strategies and improved public health protection (Aslan, n.d.;Singh, 2024).</p>
<p>For instance, AI was used to predict COVID-19 hospitalization and mortality.While these techniques show promise, further validation is required to address related challenges (Shakibfar et al., 2023a).Another study highlighted AI's potential in creating a Disease Risk Score for predicting COVID-19 hospitalization and mortality using health registry data.This approach demonstrates AI's ability to identify high-risk individuals, though it faces issues with generalizability and external validation (Shakibfar et al., 2023b).Hien et al. (2024) utilized real-world clinical data to develop predictive models for severe COVID-19 outcomes, demonstrating the significant role of machine learning (ML) in the early identification and management of high-risk patients.Similarly, Banoei and colleagues employed ML techniques to forecast mortality among hospitalized COVID-19 patients.Their analysis of relationships between various risk factors identified critical indicators such as low oxygen levels and chronic kidney disease.This study provides a foundational understanding of these risk interactions, aiding in the prioritization of treatment approaches (Banoei et al., 2023).</p>
<p>Expanding beyond COVID-19, Gao et al. (2023) analyzed risk factors and predictive models for pulmonary tuberculosis, introducing TBINet, a deep-learning model leveraging CT images to identify pulmonary tuberculosis (PTB) infectivity.Validated by high AUC values and gradient-weighted class activation mapping (Grad-CAM) technology, this approach offers a swift and reliable diagnostic method, showcasing the utility of AI in medical imaging analysis for tuberculosis diagnosis and control.Their study demonstrated the feasibility of using CT images to rapidly and cost-effectively identify PTB infectivity through deep-learning methods.</p>
<p>AI has also been effectively employed in diagnosing Hansen's disease (HD).It excels in rapid case detection, personalized treatment planning, mental health counseling, case classification, ensuring compliance with multidrug therapies, tracking geographical treatment distribution, and identifying adverse drug reactions and antimicrobial resistance.Moreover, AI plays a pivotal role in the early detection of nerve damage, which is crucial for preventing disabilities and planning rehabilitation.These capabilities are especially valuable in regions with a shortage of trained healthcare professionals (Deps et al., 2024).</p>
<p>The integration of AI into the management of infectious diseases holds immense potential to revolutionize diagnosis, treatment, and understanding of disease mechanisms.AI has proven to be highly effective in predicting, detecting, and controlling the spread of infectious diseases, as particularly highlighted during the COVID-19 pandemic.This technology plays a critical role in preventing future health crises by predicting outbreaks, identifying high-risk areas, and aiding vaccine development.Additionally, AI's ability to track and trace infected individuals, identify potential hotspots, limit the spread of infections, and monitor patient symptoms empowers healthcare professionals to deliver more effective treatments (Siddig et al., 2023;Hsu et al., 2024).</p>
<p>AI and vaccine design</p>
<p>AI technology has significantly enhanced the vaccine development process, providing innovative solutions to accelerate and optimize vaccine design.Researchers at Baidu Research developed an AI tool called Linear Design, which designed the optimal mRNA sequence for the SARS-CoV-2 spike protein in just 11 min.This breakthrough achieved a 128-fold increase in the COVID-19 vaccine's antibody response and significantly improved vaccine stability (Dolgin, 2023;Zhang et al., 2023).</p>
<p>AI is also revolutionizing bacterial vaccine development.AI tools have identified 22 putative antigens for Helicobacter pylori, characterized T-cell epitopes for Mycobacterium, and validated the membrane protein FilF as a potential vaccine candidate for Acinetobacter baumannii.Additionally, AI is aiding vaccine development for Klebsiella pneumoniae, Pseudomonas aeruginosa, and Streptococcus pneumoniae (Gorki and Medhi, 2024).The potential of AI extends to parasitic infections, such as gastrointestinal nematodes (GINs), where AI techniques are being applied to develop effective anti-GIN vaccines.By increasing precision, accelerating design processes, and expanding our understanding of disease mechanisms, AI is transforming vaccine production.Despite these advancements, rigorous laboratory testing and regulatory approval remain essential to ensure vaccine safety and efficacy.</p>
<p>AI models support scientific research</p>
<p>Starting in July 2024, scientists at Los Alamos Natio nal Laboratory in New Mexico began evaluating the multimodal LLM model GPT-4o (2024) in real-world laboratory settings.This groundbreaking project aims to assess GPT-4o's ability to assist both expert and novice scientists with complex biological tasks through its visual and voice capabilities.Tasks include transformation (introducing foreign genetic material into a host organism), cell culture (maintaining and propagating cells in vitro), and cell separation (e.g., through centrifugation).By integrating AI into standard laboratory workflows, the project seeks to drive innovation in biosciences while identifying potential risks associated with AI-assisted research (OpenAI and Los Alamos National Laboratory, 2024; Pannu et al., 2024).The potential of AI-assisted biological research to enhance human health and well-being is clear, yet significant unpredictabilities and risks remain.Researchers have emphasized the need for swift govern ment action to establish comprehensive testing and mitigation strategies, leveraging decades of expertise to address large-scale biological research risks (Pannu et al., 2024).In 2023, Microsoft conducted evaluations of GPT-4, revealing its ability to provide detailed guidance for using the Rosetta protein design tool.This tool successfully created an antibody capable of binding to the SARS-CoV-2 spike protein.GPT-4 also demonstrated capabilities in automating biological experiments by converting experimental protocols into code for liquid-handling robots, significantly expediting laboratory workflows (Microsoft Research AI4Science, 2023).Further advancements were demonstrated by researchers at Carnegie Mellon University, who developed Coscientist, a system powered by GPT-4 that could design, plan, and execute complex experiments, including chemical syntheses.This system was capable of searching through documents, writing code, and operating robotic lab devices (Boiko et al., 2023).Recently, researchers from Stanford University and the Chan Zuckerberg Biohub introduced a Virtual Lab.This system comprises a group of large language model agents powered by GPT-4o that managed to design effective SARS-CoV-2 nanobodies (a type of antibody) with minimal human intervention (Swanson et al., 2024).</p>
<p>AI in drug repurposing and discovery</p>
<p>AI presents an exciting future for rapid drug repurposing and discovery.By analyzing vast biological and chemical datasets, AI uncovers hidden connections be-tween existing drugs, disease targets, and potential new treatments (Singh, 2024).AI's capability to exa mine large datasets of drugs and disease targets is a potent tool for discovering new therapeutic uses for existing medications (Singh, 2024).By analyzing extensive biological and chemical datasets, AI can reveal hidden links between existing drugs, disease targets, and new therapeutic uses.</p>
<p>For example, Baricitinib, initially developed for rheumatoid arthritis, has shown potential as a COVID-19 treatment due to its anti-inflammatory and antiviral properties.AI analysis of Baricitinib's interactions helped researchers predict and manage potential side effects, enabling safer clinical trials (Cantini et al., 2020;Stebbing et al., 2020;Saber-Ayad et al., 2021;Richardson et al., 2022).Similarly, Lopinavir/Ritonavir, an HIV medication, was investigated for its ability to inhibit a crucial SARS-CoV-2 enzyme (protease).However, AI analysis identified potential adverse effects on liver function, highlighting the drug's limitations.This insight emphasizes the need to develop new, targeted inhibitors specifically designed to combat SARS-CoV-2, potentially overcoming the constraints of repurposed drugs (Parvathaneni and Gupta, 2020;Singh, 2024).</p>
<p>By analyzing chemical structures and predicting their interactions with disease targets, AI can uncover potential candidates across various therapeutic fields.For instance, Alendronate, commonly used to treat osteoporosis, exemplifies the efficacy of AI in drug repurposing.AI identified its ability to inhibit a key enzyme essential for the proliferation of certain cancer cells, suggesting its potential as a cancer treatment and broadening the spectrum of candidate drugs beyond traditional considerations (Saul and Einav, 2020;Alachram et al., 2021;Usha et al., 2021;Singh, 2024).</p>
<p>AI has become an invaluable tool, leveraging extensive datasets to forecast and refine drug characteristics, resulting in safer and more efficient repurposed medications.Current efforts include using AI to optimize dosing schedules of antiretroviral drugs for HIV, enhancing patient adherence and minimizing side effects (Xing et al., 2020;Serghini et al., 2023).</p>
<p>In the pharmaceutical industry, AI addresses significant challenges by detecting biological activity in preclinical screenings, optimizing pharmacokinetic properties for better formulations, predicting early toxicity to reduce attrition rates, and proactively screening for genetic mutations in biological targets to prolong thera-peutic effectiveness (Serghini et al., 2023).Furthermore, integrating AI with patent data analysis offers a robust approach to identifying and repurposing existing drugs for new uses.This accelerates the development of costeffective, accessible treatments and enhances the healthcare system's preparedness for emerging diseases.</p>
<p>By examining expired patents, AI can identify drugs no longer under patent protection, making them available for repurposing.This approach is particularly beneficial for antiviral treatments targeting less common viruses, where developing new drugs may not be economically feasible.It accelerates drug repurposing and ensures more efficient use of resources in addressing pathogenic threats.For instance, AI analysis identified Teicoplanin, an antibiotic no longer under patent protection, as having potential antiviral properties against the Zika virus ( Dalal and Biswas, 2024;Ishaq et al., 2024).</p>
<p>The integration of computational and experimental methods is critical for discovering and developing molecules to combat deadly diseases.Computational approaches include active site prediction, homology modeling, ligand preparation, molecular dynamics simulation, molecular docking, pharmacophore modeling, target identification, and virtual screening.These AI-enabled capabilities highlight the potential of computer-aided drug design (CADD) to streamline and enhance the drug discovery process (Dalal and Biswas, 2024).</p>
<p>AI also plays a pivotal role in designing clinical trials for repurposed drugs.It can optimize dosages, identify outcome measures tailored to the drug and targeted virus, select highly relevant patient groups most likely to benefit, and determine treatment durations.This approach reduces costs, resource needs, and time to market while enabling researchers to achieve more conclusive trial results (Chopra et al., 2023).</p>
<p>The application of AI in drug development represents a significant breakthrough, enabling more efficient and effective discovery of medications, particularly for chronic diseases.By streamlining the drug discovery process, reducing costs, and accelerating the timeline for bringing new treatments to market, AI has the potential to revolutionize the pharmaceutical industry and improve healthcare outcomes.</p>
<p>AI and biosimulation</p>
<p>Biosimulation, which involves simulating biological systems and processes using mathematical models, leverages AI algorithms for pattern recognition in clinical trials and the analysis of connections between drugs, patients, demographics, and trial parameters.These models empower researchers to address questions related to optimal dosing, medication interactions, and population-level efficacy.</p>
<p>For instance, VeriSIM Life's BIOiSIM platform utilizes AI and ML to simulate the effects of chemicals on individual organs and entire bodily systems.This allows researchers to investigate optimal dosages, drug interactions, and overall effectiveness across populations while accelerating drug development and reducing reliance on extensive animal testing (BIOiSIM, 2024).</p>
<p>Other leaders in AI-driven biosimulation include Certara, which specializes in pharmacokinetic-pharmacodynamic (PK/PD) simulation and toxicokinetic (TK) modeling (Phoenix WinNonlin™ Software, 2024).Their SimcypPBPK platform is widely used to describe drug behavior in various body tissues, predict drug toxicity, and optimize dosing regimens (Simcyp™ PBPK Simulator, 2024).</p>
<p>Simulations Plus develops various models to predict drug toxicity and interactions, aiding research processes.Their GastroPlus software is an advanced simulation tool that models different types of drug absorption and pharmacokinetics in both humans and animals.It covers intravenous, oral, oral cavity, ocular, inhalation, dermal, subcutaneous, and intramuscular administration routes (Innovative science-based software, 2024).Schrodinger offers tools for molecular modeling and chemical engineering simulations, aiding in the discovery of new drugs (Opening New Worlds for Molecular Discovery, 2024).Genedata utilizes AI for data management and simulation in drug development processes, resulting in more efficient and cost-effective drug development (Digitalizing Biopharma R&amp;D, 2024).</p>
<p>AI in clinical trials</p>
<p>AI is revolutionizing clinical trials by speeding up data analysis, optimizing study designs, and streamlining patient recruitment, thereby boosting efficiency and cutting costs (Hutson, 2024).AI is employed in clinical studies to perform tasks such as data analysis, protocol preparation, and patient recruitment.It can help lower clinical trial drop-out rates, analyze videos to ensure medication adherence and answer patient questions through chatbots like ChatDoctor (Li et al., 2023).In a 2024 article published in Nature (Hutson, 2024), various AI platforms in clinical trials were discussed, such as HINT, which predicts trial success; SPOT, which analyzes trial timing; SEETrials, which extracts safety and efficacy information; CliniDigest, which summarizes clinical trial records; Trial Pathfinder, which assesses participation criteria; and Criteria2Query, which converts eligibility criteria into database queries.Additional platforms include DQueST for helping patients search for trials, Trial GPT for matching patients with trials, Unlearn for creating digital twins for control groups, PLIP for managing and organizing trial data, AutoCriteria for extracting eligibility criteria, ChatTrial for answering trial-related questions, and SDQ from Saama, which assists with data cleaning and milestone prediction.</p>
<p>Implementing AI in clinical trials faces challenges such as potential bias, difficulties in reproducing results, data privacy and security risks, over-reliance on AI, and the complexity of algorithms leading to a lack of transparency.Despite these challenges, the FDA recognizes the growing role of artificial intelligence and machine learning (AI/ML) throughout the drug development cycle and their potential to accelerate the process.</p>
<p>For instance, AI/ML methods can assist in clinical trials by selecting patients based on baseline characteristics such as demographic data, clinical information, vital signs, laboratory results, medical imaging, and genetic data to predict clinical outcomes following investigational treatments.These predictive models can identify patients with worse prognoses or those most likely to benefit from a treatment, ultimately aiding in demonstrating a drug's effectiveness.</p>
<p>On November 8, 2022, the FDA issued an Emergency Use Authorization (EUA) for anakinra (Kineret) to treat COVID-19 in hospitalized adults with pneumonia requiring supplemental oxygen (low-or high-flow) who were at risk of progressing to severe respiratory failure (SRF) and likely to have elevated levels of plasma soluble urokinase plasminogen activator receptor (suPAR).Elevated suPAR levels are indicative of increased inflammation or immune response (Winnicki et al., 2019).Anakinra is the first interleukin-1 inhibitor authorized for COVID-19 treatment.</p>
<p>Notably, the FDA developed an in silico scoring rule as an alternative method to identify suitable patients for anakinra treatment in the absence of a commercially available suPAR assay in the United States.This scoring rule utilized AI/ML to analyze clinical characteristics and laboratory tests to predict patients likely to have elevated suPAR levels, a key criterion for anakinra treatment.This marked the FDA's first use of AI/ML to identify an appropriate patient population for drug therapy (Liu et al., 2024).</p>
<p>AI in early disease diagnosis</p>
<p>Hospitals and medical research institutions are increasingly developing AI tools for disease detection.These tools study and analyze symptoms, medical histories, and diagnostic processes to identify whether a patient is at risk for or experiencing the early stages of a disease.Early intervention and therapy facilitated by detection algorithms can slow disease progression or alleviate symptoms.Contract Research Organizations (CROs) may use these algorithms to identify and enroll patients at earlier stages of disease development, particularly during the prodromal phase.IQVIA has developed a data-driven illness detection program that evaluates a patient's symptoms and characteristics, provides treatment recommendations-including clinical trials-and makes expert referrals (Leveraging Real World Data to Measure Disease Severity, 2024).Other companies leading in AI-driven disease detection include AiCure, which enhances treatment through real-time patient monitoring and data analysis (Patient Engagement, 2024); United Imaging, which offers AIbased solutions to improve diagnostic imaging accuracy (Alabama gets its first uCT ® ATLAS, 2024); and Ibex Medical Analytics, which provides AI solutions for pathology analysis (Trusted Cancer Diagnostics, 2024).</p>
<p>AI and small molecule drugs</p>
<p>AI tools can analyze extensive datasets of existing drugs and drug candidates to uncover groundbreaking new insights.Artificial intelligence systems can forecast interactions between small molecule drugs and their target proteins, predict potential side effects, identify relationships and patterns within drug datasets, and enable researchers to develop new small molecule drugs with improved pharmacokinetic profiles, minimized side effects, and enhanced efficacy (Kirkpatrick, 2022).Recent advancements underscore how AI is revolutionizing the design and discovery of small molecules, with examples including USP1 inhibitors, KAT6A in-hibitors, INS018_055, and small-molecule candidates in oncology, immunology, neuroinflammation, neurology, and cardiometabolic diseases (Dealmakers, n.d.).</p>
<p>AI voice assistants</p>
<p>AI-driven voice assistants are increasingly being employed in clinical trials to manage various routine monitoring tasks, such as reminding patients of upcoming appointments and tracking their daily activities.AI voice assistants like Alexa, Google Assistant, and Siri have the potential to transform healthcare by turning speech into a valuable health indicator, enabling the early detection and prediction of potential health conditions.Specific acoustic features in speech have been linked to psychiatric disorders such as depression, PTSD, anxiety, and eating disorders (Low et al., 2020).For example, a vocal biomarker has been associated with hospitalization and mortality rates in patients with heart failure (Maor et al., 2020), and vocal biomarkers have been shown to correlate with depression severity and treatment response (Mundt et al., 2012).By capturing and analyzing subtle voice changes, AI can generate a range of health measurements to provide a more comprehensive picture of overall health.Machine learning technology using speech samples, obtained either in clinical settings or remotely, could eventually serve as a biomarker to improve diagnosis and treatment.Current research primarily focuses on using speech's acoustic features to detect conditions like depression and schizophrenia (Low and Bentley, 2020).</p>
<p>Companies engaged in AI/ML-enabled discovery</p>
<p>Over the years, the interest in applying AI to drug R&amp;D has significantly increased, driven by expectations of faster timelines, reduced costs, and the ability to uncover hidden insights from large datasets.More than 150 AI-focused companies have raised substantial funding, particularly in small-molecule drug discovery, through venture capital financings, initial public offerings (IPOs), and high-value partnerships with large pharmaceutical companies (Table 1).Currently, the first AI-based small-molecule drug candidates have entered clinical trials (Kirkpatrick, 2022).</p>
<p>BenevolentAI, an AI-enabled drug discovery company, utilizes a knowledge graph that integrates publicly available biomedical and chemical data with pro-prietary datasets, allowing AI tools to generate target hypotheses.It focuses on identifying targets for chronic kidney disease (CKD), idiopathic pulmonary fibrosis (IPF), heart failure and systemic lupus erythematosus (Kirkpatrick, 2022;Chopra et al., 2023).Verge's AI platform using a proprietary collection of patient brain transcriptomes, identifies targets for amyotrophic lateral sclerosis (ALS) and other neurodegenerative diseases (Kirkpatrick, 2022).NVIDIA Corporation launched Clara Holoscan MGX to enable the creation and deployment of real-time AI applications.Clara offers tools for medical imaging, genomics, and AI model development (NVIDIA, 2024b).Insitro combines machine learning with high-throughput biology.It generates biological datasets from cellular disease models, integrating them with human clinical data to identify therapeutic targets.Insitro aims to discover novel targets for ALS and frontotemporal dementia (FTD) (Insitro, 2024;Making Medicines, 2024).Recursion Pharmaceuticals focuses on generating data from cellular models employing ML.By using image-based profiling of cellular disease models treated with various potential drug leads, Recursion identifies novel targets and medicines in neuroscience and oncology (Kirkpatrick, 2022).Exscientia specializes in AI-driven small-molecule drug design, oncology and immunology (Kirkpatrick, 2022).Insilico Medicine focuses on AI-driven preclinical research.Insilico initiated a phase 1 trial of the small-molecule inhibitor ISM001-055 for idiopathic pulmonary fibrosis.Using its AI platform, both the target and drug candidate were identified, reducing the time from target discovery to phase 1 trial initiation to less than 30 months.Partnership with Fosun Pharma led to nomination of ISM004-1057D as a first-in-class small-molecule inhibitor of the enzyme QPCTL regulating the CD47-SIRPα pathway (Kirkpatrick, 2022).Relay Therapeutics specializes in identifying drug candidates based on protein dynamics.The company's SHP2 inhibitor, RLY-1971 is currently in phase 1 trials for cancer treatment (Kirkpatrick, 2022).Takeda Pharmaceuticals aims to develop therapies and diagnostics for inflammatory bowel disease (IBD) using advanced bioinformatics and machine learning tools (Prometheus Biosciences, 2024).Takeda explored over 60 indications for preclinical and clinical molecules, identifying new treatment options within 18 months.A key advancement includes the clinical development of TAK-733 (REC-4881), a MEK inhibitor</p>
<p>AI and biosafety and biosecurity risks</p>
<p>AI developers anticipate that combinations of artificial intelligence techniques, including automation technologies, LLMs, and robotics, will enable experiments-such as the manipulation, design, and synthesis of DNA, drug candidates, or toxins-with minimal human involvement.These advances have the potential to transform biomedical research, but they also pose significant biosafety and biosecurity risks (Urbina et al., 2022).</p>
<p>In response to these growing risks, various governments have implemented measures to address safety concerns associated with advanced AI models.In 2023, the US government secured voluntary commitments from 15 leading AI companies to better manage AI-related risks.Later that year, US President Joe Biden issued an Executive Order to ensure the safe, secure, and trustworthy development and deployment of artificial intelligence.This order mandates that companies must inform the government before launching models primarily trained on biological sequence data and requiring more than 10^23 computing operations (Pannu et al., 2024).</p>
<p>In November 2024, representatives from ten governments participated in the first meeting of the International Network of AI Safety Institutes in San Francisco, California (2024).France is set to host the AI Action Summit in Paris in February 2025 (Elysee, 2024;Pannu et al., 2024).Countries such as Canada, Japan, Singapore, the United Kingdom, and the United States have established government institutes focused on AI safety, creating standards and tools to manage risks.Australia, the European Union (which has set up a safety unit within its AI Office), France, Kenya, and South Korea are the founding members of the International Network of AI Safety Institutes.</p>
<p>In the absence of comprehensive government policies to address urgent risks and mitigation strategies, companies like Anthropic and OpenAI have implemented in-house evaluation protocols.These protocols include automated assessments, red teaming-where humans attempt to elicit harmful capabilities-and controlled trials that compare task performance with and without AI assistance (Pannu et al., 2024).However, these evaluations often focus narrowly on the potential for AI models to aid in the development of bioweapons.</p>
<p>Current evaluations also tend to concentrate on basic laboratory tasks.For instance, OpenAI's tests with Los Alamos researchers assess capabilities that, while critical for beneficial research, could also be used to develop harmful agents, such as crop-destroying pathogens.Additionally, an underexplored concern is the interaction of multiple AI systems.While the US government has highlighted this issue, most companies test only individual models, overlooking the broader risks of combined system behavior (Pannu et al., 2024).</p>
<p>Guidelines on the use of AI tools in research publications</p>
<p>Organizations such as COPE, WAME, and the JAMA Network have established guidelines to address the growing use of AI tools, including ChatGPT and Large Language Models, in publications (Authorship and AI tools, 2024) 2024), allow the use of AI tools provided their use is disclosed.According to a study published in December 2024, 78 medical journals have issued guidance on AI use in peer review.Of these, 46 journals explicitly forbid the use of AI, while 32 permit it under the condition that confidentiality is maintained and authorship rights are respected (Li et al., 2024).</p>
<p>AI tools for streamlining scientific paper writing</p>
<p>Presented below are seven subsections highlighting various AI tools designed to assist with text-related tasks (Table 2).These tools are categorized based on their functionalities: literature review, content creation, citation management, proofreading and optimization, formatting, and originality verification.Each subsection illustrates how these tools can enhance the efficiency, accuracy, and quality of scientific work.</p>
<p>AI tools useful in citation management</p>
<p>For citation management, the AI tools such as Trinka (2024), Semantic Scholar (2024), and Scite (2024) mentioned in the "AI tools in literature review" section can be particularly useful.In addition to these, there are several other citation management tools like Mendeley (2024).This tool offers smart citation suggestions, research discovery capabilities, and personalized paper recommendations based on reading habits.It integrates with Elsevier journals for priority access to newly published papers and includes a social platform for researchers.Zotero (2024) is a popular open-source tool that integrates seamlessly with web browsers.It supports annotation, citation, collection, and organization of references, allowing users to create citations and bibliographies in various styles.Zotero also provides smart recommendations while browsing the web.EndNote (2024) is a robust tool for managing references and citations, offering features like intelligent citation matching and advanced collaboration tools for projects.</p>
<p>AI tools useful in text proofreading and optimization</p>
<p>Wordvice AI (2024) enhances grammar, spelling, punctuation, and style.It also assists in paraphrasing to avoid plagiarism, translating text, and summarizing documents.Grammarly (2024) offers advanced grammar and style suggestions, making it particularly useful for improving the writing quality of scientific papers.Hemingway Editor (2024) improves text readability and simplifies language while maintaining clarity.Quill-Bot (2024) utilizes AI to assist users in enhancing, generating, and paraphrasing content.Underleaf (2024) specializes in correcting grammar, improving style, and rewording content tailored for academic writing.</p>
<p>AI tools useful in text formatting</p>
<p>For formatting purposes, tools such as Cite This For Me (2024), Zotero (2024), and Scribbr (2024) can be particularly useful.These tools offer citation generators in various formats.Additionally, tools like SciSpace (2024) and Underleaf (2024) provide further assistance with formatting tasks.</p>
<p>AI tools useful for originality verification</p>
<p>Ensuring originality and avoiding plagiarism is critical when writing a manuscript.It is also essential to verify that AI-generated content adheres to publication guidelines.Depending on the publisher, AI-generated text might be prohibited outright or require explicit labeling by authors during submission.Publishers often employ advanced tools like iThenticate (2024) to detect both plagiarism and AI-generated content.Other useful tools for detecting plagiarism and AI-generated text include: Turnitin (2024), Copyscape (2024) WithdrarXiv (2024) is the first dataset of withdrawn papers from arXiv, released in December 2024.It includes over 14,000 papers and their retraction comments, covering the repository's history up to September 2024.This dataset is valuable for scientists, aiding in maintaining scientific integrity, improving quality control, developing automated verification systems, addressing ethical concerns, and serving as an educational resource for new researchers.</p>
<p>AI tool useful for retraction verification</p>
<p>AI's risks and ethical challenges</p>
<p>AI holds the promise of transforming scientific and medical sectors enabling tasks that previously took years to be completed.Recent demonstrations have shown that AI-designed proteins can tackle the century-old issue of developing new treatments for snakebites, which claim around 100,000 lives each year (Callaway, 2025).However, despite these advancements, AI also poses significant risks and demands careful ethical considerations.Ensuring equitable access, fair distribution of AI technologies and their benefits, and high-quality healthcare services for all -irrespective of disability status, ethnicity, gender, geographic location, socioeconomic status, or race -is paramount.Addressing concerns about algorithmic fairness and biases, data privacy, ensuring informed consent for data usage, and maintaining safety and transparency is essential.Additionally, there are legal and social implications, as well as concerns regarding data security and confidentiality, the validity of research findings, and potential incidents of research misconduct, that must be resolved (Bouhouita-Guermech et al., 2023;Resnik and Hosseini, 2024).</p>
<p>To ensure AI benefits everyone fairly, several ethical challenges must be addressed: Mitigating biases in data collection and promoting diversity in research.Safeguarding privacy and confidentiality.Obtaining informed consent for the use of AI technologies.Ensuring human oversight in AI applications.Developing transparent AI systems.Clearly defining the roles and responsibilities of AI developers, health professionals, and institutions.This comprehensive strategy integrates human values into AI advancements, thereby enhancing public health and overall well-being (Health Equity and Ethical Considerations, 2024).</p>
<p>Beyond these ethical considerations, there are also risks, such as the potential introduction of errors by AI.Generative AI, including large language models (LLMs), are prone to hallucinations (Why scientists trust AI too much, 2025), although the exact mechanisms of the problem are not clear.These errors likely stem from a combination of factors such as data compression, ambiguities or mistakes in the AI's training data, or incorrect facts or assumptions in prompts provided by users.To track this issue, the Hallucination Vulnerability Index was created, which sorts hallucinations into six categories and three degrees of severity.Additionally, the Hallucinations Leaderboard platform that tracks, ranks, and evaluates hallucinations in LLMs was launched (Jones, 2025).It was shown that some chatbots confabulate facts in up to 30% of cases, making up information that isn't in the given document.The issue of false scientific references is particularly problematic.Study from 2024 demonstrates that various chatbots made errors in references between 30% and 90% of the time (Chelli et al., 2024).Thus, it is very important to verify generated information and perform external fact-checking.</p>
<p>Furthermore, other limitations of AI technology must also be considered.One notable example is Alpha-Fold 3, which has enhanced the precision of predicting biomolecular structures, yet still faces challenges with stereochemistry and requires human assistance (Steinkellner et al., 2025).</p>
<p>AI models for antimicrobial resistance diagnosis, discovery, and treatment often rely on imbalanced datasets, leading to potential reliability issues and representing only certain patient populations or specific biological tests.AI models developed using these biased datasets may struggle to generalize effectively beyond their specific training data (Cesaro et al., 2025).</p>
<p>It is crucial to critically evaluate AI's role in research to prevent overdependence on its abilities.By addressing both ethical concerns and potential risks, while acknowledging the limitations of AI, the scientific community can leverage AI's potential while maintaining research integrity.</p>
<p>Conclusions</p>
<p>AI is transforming biomedical research by improving data analysis, modeling biological processes, and detecting diseases, leading to faster scientific discoveries.Scientists emphasize the usefulness of AI in summarizing scientific data from other researchers, accelerating administrative tasks, speeding up the process of writing scientific papers, generating new hypotheses, and conducting faster peer reviews.Despite its promising future, AI presents ethical challenges, including issues related to data quality, result interpretation, and responsible use.Addressing these challenges requires interdisciplinary collaboration and robust regulation.Ensuring the ethical use of AI -such as preventing plagiarism and maintaining content quality -is essential for safeguarding the integrity and progress of scientific research.</p>
<p>,</p>
<p>Crossref Similarity Check (2024), ZeroGPT (2024), Originality.AI (2024), SciSpace AI Detector (2024), Content at Scale AI Detector (2024), GPT-2 Output Detector (2024).</p>
<p>Table 1 .
1
AI-driven innovations in biotechnology: companies and their key products
Company (Ref.)AI application areaAI technologyPartnerships/CollaborationsBenevolentAIDrug discovery, CKDAI-driven drug discoveryPartnership with AstraZeneca(Kirkpatrick, 2022;and IPF, treatment,Chopra et al.,heart failure, and lupus2023)researchVerge GenomicsALS,AI platform for target identificationPartnership with Eli Lilly(Kirkpatrick, 2022)neurodegenerativein neurodegenerative diseasesdiseasesNVIDIAMedical imaging,Clara Holoscan MGXCorporationgenomics, and AI(2024)model developmentin healthcare and lifesciences; treatment,diagnosis, and drugdiscoveryInsitro (2024);Drug discovery,Machine learning combinedCollaboration with Bristol MyersMaking Medicinesneurodegenerativewith high-throughput biologySquibb(2024)diseases, ALS and FTDtreatmentRecursionDrug discovery,AI-guided drug discoveryCollaboration with Genentech,Pharmaceuticalsoncology, neurosciencewith image-based profilingRoche(Kirkpatrick, 2022)of cellular disease modelsExscientiaDrug discovery,AI-driven small-molecule drugCollaboration with Bristol Myers(Kirkpatrick, 2022)oncology, immunologydesignSquibb, SanofiInsilico MedicineDrug discovery,AI-driven preclinical research,Partnership with Fosun Pharma(Kirkpatrick, 2022)immuno-oncologysmall-molecule drug development;ISM004-1057D (a first-in-classsmall-molecule inhibitorof the enzyme QPCTL regulatingthe CD47-SIRP pathway)Relay TherapeuticsCancer, drug discoveryDrug discovery, RLY-1971 (SHP2Collaboration with Genentech(Kirkpatrick, 2022)inhibitor development for cancer)TakedaDrug development,Bioinformatics and machinePartnerships with PrometheusPharmaceuticalsoncology, IBDlearning for medical treatmentsBiosciences, Recursion(Merck, 2024;treatmentand drug innovation; TAK-733Prometheus(REC-4881), a MEK inhibitor forBiosciences, 2024;hereditary cancer syndromeTakeda, 2024)ReviveMed's AICancerAI platformBristol Myers Squibb(2024)immunotherapiesSensyne HealthStudyMachine learningBristol Myers Squibb(2024)of myeloproliferativeneoplasmsdevelopmentMedidataClinical trials,AI-powered solutions for drugAdopted across the pharmaceutical,(2024a, 2024b)drug developmentresearch and clinical trialsbiotech, academic, governmentsectors, hospitals and clinicalresearch organizations (CROs)SaamaClinical research, drugAI platformTechnologiesdiscovery(2024)Health (formerlyHealthcareMobile app for medicationPillsy 2024)compliance monitoring and patient-reported informationAliveCor (2024)Cardiology, healthcareWearable EKG device for cardiacPartnership with Medablerhythm detection</p>
<p>Table 2 .
2
AI tools for streamlining scientific paper writing
Purpose</p>
<p>Accurate structure prediction of biomolecular interactions with AlphaFold 3. J Abramson, J Adler, J Dunger, R Evans, T Green, A Pritzel, O Ronneberger, L Willmore, A J Ballard, J Bambrick, Nature. 6302024</p>
<p>AI-based tools and technologies for content generation. Alabama gets its first uCT® ATLAS. 11 Dec. 2024. 11 Dec. 2024</p>
<p>Text mining-based word representations for biomedical data analysis and protein-protein interaction networks in machine learning tasks. H Alachram, H Chereda, T Beißbarth, E Wingender, P Stegmaier, e0258623.10.1371/journal.pone.0258623PLoS One. 162021</p>
<p>Artificial intelligence and machine learning to fight COVID-19. A Alimadadi, S Aryal, I Manandhar, P B Munroe, B Joe, X Cheng, Physiol. Genomics. 522020</p>
<p>Medical-grade personal ECG device. Alivecor, 11 Dec. 2024</p>
<p>Applications of artificial intelligence in clinical laboratory genomics. Alphafold Server, ; Aradhya, S Facio, F M Metz, H Manders, T Colavin, A Kobayashi, Y Nykamp, K Johnson, B Nussbaum, R L , 10.1002/ajmg.c.32057Am J Med Genet C Semin Med Genet. 19311 Dec. 2024. 2023</p>
<p>IDMap: Leveraging AI and data technologies for early cancer detection. S Arefin, Valley Int. J. Digital Libr. 122024</p>
<p>validation of the sybil deep learning lung cancer risk prediction model in three independent screening studies. R P Aro, S Lam, M T Warkentin, G Liu, B Diergaarde, J M Yuan, D O Wilson, R Meza, R Myers, R J Hung, MA02.11J. Thorac. Oncol. 192024</p>
<p>Artificial intelligence in clinical applications for infectious diseases: diagnosis, treatment, and immunization. S Aslan, Exp. Appl. Med. Sci. 5</p>
<p>. A I Authorship, Tools, 11 Dec. 2024</p>
<p>Neural network fast-classifies biological images through features selecting to power automated microscopy. M Balluet, F Sizaire, Y El Habouz, T Walter, J Pont, B Giroux, O Bouchareb, M Tramier, J Pecreaux, J. Microsc. 2852022</p>
<p>Unraveling complex relationships between COVID-19 risk factors using machine learning-based models for predicting mortality of hospitalized patients and identification of high-risk groups: A large retrospective study. M M Banoei, H Rafiepoor, K Zendehdel, M S Seyyedsalehi, A Nahvijou, F Allameh, S Amanpour, Front. Med. 1011703312023</p>
<p>Integrating AI/ML models for patient stratification leveraging omics dataset and clinical biomarkers from COVID-19 patients: a promising approach to personalized medicine. B Bello, Y N Bundey, R Bhave, M Khotimchenko, S W Baran, K Chakravarty, J Varshney, Int. J. Mol. Sci. 2462502023</p>
<p>. Bioisim®, 11 Dec. 2024</p>
<p>Biology AI powered by full paper citations + live biology databases. Biologpt, 2024. 11 Dec. 2024</p>
<p>Radiogenomics: bridging imaging and genomics. Z Bodalal, S Trebeschi, T D L Nguyen-Kim, W Schats, R Beets-Tan, Abdom. Radiol. (NY). 442019</p>
<p>Autonomous chemical research with large language models. D A Boiko, R Macknight, B Kline, G Gomes, Nature. 6242023</p>
<p>Specific challenges posed by artificial intelligence in research ethics. S Bouhouita-Guermech, P Gogognon, J C Bélisle-Pipon, Front Artif Intell. 611490822023</p>
<p>Artificial intelligence in genomics. A L Boulesteix, M Wright, Hum. Genet. 1412022</p>
<p>Chemistry Nobel goes to developers of Al-phaFold AI that predicts protein structures. ; Brainglobe, E Callaway, Nature. 63411 Dec. 2024. 2024Interoperable Python-based tools for computational neuroanatomy</p>
<p>AI-designed proteins tackle century-old problem -making snake antivenoms. E Callaway, Nature. 6377762025</p>
<p>Baricitinib therapy in COVID-19: A pilot study on safety and clinical impact. F Cantini, L Niccoli, D Matarrese, E Nicastri, P Stobbione, D Goletti, J. Infect. 812020</p>
<p>. Cellotype, 11 Dec. 2024</p>
<p>. Cellsam , Accessed 11 Dec. 2024</p>
<p>Challenges and applications of artificial intelligence in infectious diseases and antimicrobial resistance. A Cesaro, S C Hoffman, P Das, C De La Fuente-Nunez, NPJ Antimicrob Resist. 322025</p>
<p>Chat with any PDF. Chatpdf, 2024. 11 Dec. 2024</p>
<p>Hallucination Rates and Reference Accuracy of ChatGPT and Bard for Systematic Reviews: Comparative Analysis. M Chelli, M Chelli, J Descamps, V Lavoué, C Trojani, M Azar, M Deckert, J L Raynier, G Clowez, P Boileau, C Ruetsch-Chelli, J. Med. Internet Res. 26e531642024</p>
<p>Metabolomic machine learning predictor for diagnosis and prognosis of gastric cancer. Y Chen, B Wang, Y Zhao, X Shao, M Wang, F Ma, L Yang, M Nie, P Jin, K Yao, Nat. Commun. 1516572024</p>
<p>Accurate proteome-wide missense variant effect prediction with AlphaMissense. J Cheng, G Novati, J Pan, C Bycroft, A Zemgulyte, T Applebaum, A Pritzel, L H Wong, M Zielinski, T Sargeant, Science. 38174922023</p>
<p>Cutting-edge AI tools revolutionizing scientific research in life sciences. Chi J Shu, J Li, M Mudappathi, R Jin, Y Lewis, F Boon, A Qin, X Liu, L Gu, H , Trends Analyt Chem. 1781178522024Artificial intelligence in metabolomics: a current review</p>
<p>Revolutionizing clinical trials: the role of AI in accelerating medical breakthroughs. H Chopra, Annu, D K Shin, K Munjal, Priyanka, K Dhama, T B Emran, Int. J. Surg. 1092023</p>
<p>Convolutional neural network for human cancer types prediction by integrating protein interaction networks and omics data. Y H Chuang, S H Huang, T M Hung, X Y Lin, J Y Lee, W S Lai, J M Yang, Sci. Rep. 11206912021</p>
<p>AI search engine for research. 2024. 11 Dec. 2024. 2024. 11 Dec. 2024Cite This For Me</p>
<p>AI Detector | The AI Content Detector | ChatGPT &amp; AI Checker. Contentdetector, Ai, 11 Dec. 2024</p>
<p>. Copyscape, Accessed 11 Dec. 2024</p>
<p>. Crossref Similarity, Check , 11 Dec. 2024</p>
<p>Computational approaches for the discovery of new drugs for inflammatory and infectious diseases. V Dalal, S Biswas, Computational Approaches in Biotechnology and Bioinformatics. 2024CRC Press</p>
<p>Artificial intelligence for cardiac imaging-genetics research. De Marvao, A Dawes, T J O'regan, D P , Front. Cardiovasc. Med. 61952020</p>
<p>Generative AI platforms drive drug discovery dealmaking. B Dealmakers, </p>
<p>DeepAI text generator. 11 Dec. 2024</p>
<p>Machine learning in medicine. R C Deo, Circulation. 1322015</p>
<p>The potential role of artificial intelligence in the clinical management of Hansen's disease (leprosy). P D Deps, R Yotsu, B C R S Furriel, B D De Oliveira, S L De Lima, R M Loureiro, Front. Med. (Lausanne). 1113385982024</p>
<p>Artificial intelligence in clinical and genomic diagnostics. R Dias, A Torkamani, Genome Med. 11702019</p>
<p>. Digitalizing Biopharma, R&amp;d , 11 Dec. 2024</p>
<p>Remarkable' AI tool designs mRNA vaccines that are more potent and stable. E Dolgin, Nature.10.1038/d41586-023-01487-y2023</p>
<p>Predicting functional variants in enhancer and promoter elements using Regulome DB. S Dong, A P Boyle, Hum. Mutat. 402019</p>
<p>Artificial intelligence and cellular segmentation in tissue microscopy images. M S Durkee, R Abraham, M R Clark, M L Giger, Am. J. Pathol. 1912021</p>
<p>AI under the microscope: the algorithms powering the search for cells. M Eisenstein, Nature. 6232023</p>
<p>Generative AI policies for journals. Elicit, Tools, Elysee. AI Action Summit. 11 Dec. 2024. 11 Dec. 2024. 11 Dec. 2024Elsevier</p>
<p>. Endnote, 11 Dec. 2024</p>
<p>Applications of machine learning in metabolomics: disease modeling and classification. A Galal, M Talal, A Moustafa, Front. Genet. 1310173402022</p>
<p>Distinguishing infectivity in patients with pulmonary tuberculosis using deep learning. Y Gao, Y Zhang, C Hu, P He, J Fu, F Lin, K Liu, X Fu, R Liu, J Sun, Front. Public Health. 1112471412023</p>
<p>Genome-Wide-Association-Studies-GWAS. GigaPath: Whole-Slide Foundation Model for Digital Pathology. 11 Dec. 2024. 11 Dec. 2024</p>
<p>Use of artificial intelligence in vaccine development against pathogens: challenges and future directions. V Gorki, B Medhi, Medknow. 562024</p>
<p>Machine-guided design of cell-type-targeting cisregulatory elements. S J Gosai, R I Castro, N Fuentes, J C Butts, K Mouri, M Alasoadura, S Kales, T T L Nguyen, R R Noche, A S Rao, Nature. 6342024</p>
<p>GPT-2 Output Detector Demo. 11 Dec. 2024</p>
<p>. Gpt-4o, 11 Dec. 2024</p>
<p>. Grammarly, Accessed 11 Dec. 2024</p>
<p>Artificial intelligence-driven biomedical genomics. K Guo, M Wu, Z Soo, Y Yang, Y Zhang, Q Zhang, H Lin, M Grosser, D Venter, G Zhang, J Lu, Knowledge-Based Syst. 2791109372023</p>
<p>Innovations in genomics and big data analytics for personalized medicine and health care: A review. M Hassan, F M Awan, A Naz, E J Deandrés-Galiana, O Alvarez, A Cernea, L Fernández-Brillet, J L Fernández-Martínez, A Kloczkowski, Int. J. Mol. Sci. 2346452022</p>
<p>Health equity and ethical considerations in using artificial intelligence in public health and medicine. Accessed 11 Dec. 2024</p>
<p>Whole-cell organelle segmentation in volume electron microscopy. L Heinrich, D Bennett, D Ackerman, W Park, J Bogovic, N Eckstein, A Petruncio, J Clements, S Pang, C S Xu, Nature. 5992021</p>
<p>Predicting Parkinson disease-related genes based on PyFeat and gradient boosted decision tree. M Helmy, E Eldaydamony, N Mekky, M Elmogy, H Soliman, Sci. Rep. 12100042022</p>
<p>Unveiling the future of COVID-19 patient care: groundbreaking prediction models for severe outcomes or mortality in hospitalized cases. N T K Hien, F J Tsai, Y H Chang, W Burton, P T Phuc, P A Nguyen, D Harnod, C S Lam, T C Lu, C I Chen, Front. Med. 1012899682024</p>
<p>Artificial intelligence in infectious diseases: pathogenesis and therapy. J C Hsu, C Y Lu, M.-H Hsu, 20241414056Front. Media SA</p>
<p>Evaluation of large language models for discovery of gene set function. M Hu, S Alkhairy, I Lee, R T Pillich, D Fong, K Smith, R Bachelder, T Ideker, D Pratt, Nat. Methods. 222024</p>
<p>How AI is being used to accelerate clinical trials. M Hutson, Nature. 6272024</p>
<p>Insitro announces five-year discovery collaboration with Bristol Myers Squibb to discover and develop novel treatments for amyotrophic lateral sclerosis and frontotemporal dementia. Announces-Five-Year-Discovery-Collaboration-with-Bristol-Myers-Squibb-to-Discover-and-Develop-Novel-Treatments-for-Amyotrophic-Lateral-Sclerosis-and-Frontotemporal-Dementia. 11 Dec. 2024. 11 Dec. 2024Innovative science-based software</p>
<p>Teicoplanin use is associated with rapid clinical improvement in COVID-19 pneumonia. M Ishaq, N A Afsar, S U Riaz, M Abbas, M S Mukarram, K Ishaq, S Ishaq, M S Ali, J. Med. Res. Rev. 22024</p>
<p>A foundation model for cell segmentation. U Israel, M Marks, R Dilip, Q Li, C Yu, E Laubscher, S Li, M Schwartz, E Pradhan, A Ates, bioRxiv: 2023.11.17.5676302024. 11 Dec. 2024</p>
<p>Instructions for authors. JAMA. 11 Dec. 2024</p>
<p>A meta-analysis of Watson for Oncology in clinical application. Z Jie, Z Zhiying, L Li, Sci. Rep. 1157922021</p>
<p>AI hallucinations can't be stopped -but these techniques can limit their damage. N Jones, Nature. 6372025</p>
<p>Brain age prediction using deep learning uncovers associated sequence variants. B A Jonsson, G Bjornsdottir, T E Thorgeirsson, L M Ellingsen, Bragi Walters, G Gudbjartsson, D F Stefansson, H Stefansson, K Ulfarsson, M O , Nat. Commun. 1054092019</p>
<p>Artificial intelligence makes a splash in small-molecule drug discovery. P Kirkpatrick, Biopharma Deal. 20222022</p>
<p>Generalized biomolecular modeling and design with RoseTTAFold All-Atom. R Krishna, J Wang, W Ahern, P Sturmfels, P Venkatesh, I Kalvet, G R Lee, F S Morey-Burrows, I Anishchenko, I R Humphreys, Science. 38425282024</p>
<p>BioBERT: a pre-trained biomedical language representation model for biomedical text mining. J Lee, W Yoon, S Kim, D Kim, S Kim, C H So, J Kang, Bioinformatics. 362020</p>
<p>Leveraging real world data to measure disease severity. 11 Dec. 2024leveraging-real-world-data-to-measure-disease-severity</p>
<p>Chatdoctor: A medical chat model fine-tuned on Llama model using medical domain knowledge. Y Li, Z Li, K Zhang, R Dan, S Jiang, Y Zhang, arXiv:2303.140702023arXiv preprint</p>
<p>Use of artificial intelligence in peer review among top 100 medical journals. Z Q Li, H L Xu, H J Cao, Z L Liu, Y T Fei, J P Liu, JAMA Netw. Open. 7e24486092024</p>
<p>Machine learning applications in genetics and genomics. M W Libbrecht, W S Noble, Nat. Rev. Genet. 162015</p>
<p>How data science and AI-based technologies impact genomics. J Lin, K Y Ngiam, Singapore Med. J. 642023</p>
<p>Using machine learning to determine a suitable patient population for anakinra for the treatment of COVID-19 under the emergency use authorization. Q Liu, R Nair, R Huang, H Zhu, A Anderson, O Belen, V Tran, R Chiu, K Higgins, J Chen, Clin. Pharmacol. Ther. 1152024</p>
<p>Automated assessment of psychiatric disorders using speech: a systematic review. D M Low, K H Bentley, S S Ghosh, Laryngoscope Investig. Otolaryngol. 52020</p>
<p>Segment anything in medical images. J Ma, Y He, F Li, L Han, C You, B Wang, Nat. Commun. 156542024</p>
<p>Making medicines differently. 11 Dec. 2024</p>
<p>Molecular signatures for tumor classification: an analysis of the cancer genome atlas data. Y Mamatjan, S Agnihotri, A Goldenberg, P Tonge, S Mansouri, G Zadeh, K Aldape, J. Mol. Diagn. 192017</p>
<p>Vocal biomarker is associated with hospitalization and mortality among heart failure patients. E Maor, D Perry, D Mevorach, N Taiblum, Y Luz, I Mazin, A Lerman, G Koren, V Shalev, J. Am. Heart Assoc. 9e0133592020</p>
<p>The cell tracking challenge: 10 years of objective benchmarking. M Maška, V Ulman, P Delgado-Rodriguez, E Gómez-De-Mariscal, T Necasová, F A Guerrero Peña, T I Ren, E M Meyerowitz, T Scherr, K Löffler, Nat. Methods. 202023</p>
<p>Real-time clinical trial analytics: medidata intelligent trials. Medidata, 2024a. 11 Dec. 2024</p>
<p>and-press/launch-therapeutics-selects-medidata-aiintelligent-trials-to-accelerate-clinical-trial-development. Medidata, 2024b. 11 Dec. 2024Launch Therapeutics selects Medidata AI Intelligent Trials to accelerate clinical trial development</p>
<p>Merck completes acquisition of Prometheus Biosciences. Mendeley, 11 Dec. 2024. 11 Dec. 2024</p>
<p>The impact of large language models on scientific discovery: a preliminary study using. arXiv:2311.073612023Microsoft Azure QuantumGPT-4. arXiv preprint</p>
<p>Cutting-edge AI tools revolutionizing scientific research in life sciences. B Midtvedt, S Helgadottir, A Argun, J Pineda, D Midtvedt, G Volpe, Appl. Phys. Rev. 82021Quantitative digital microscopy with deep learning</p>
<p>PEA-COCK: a machine learning approach to assess the validity of cell type-specific enhancer-gene regulatory relationships. C Mills, C N Marconett, J P Lewinger, H Mi, NPJ Syst. Biol. Appl. 992023</p>
<p>Vocal acoustic biomarkers of depression severity and treatment response. J C Mundt, A P Vogel, D E Feltner, W R Lenderking, Biol. Psychiatry. 722012</p>
<p>Prospective implementation of AI-assisted screen reading to improve early detection of breast cancer. A Y Ng, C J G Oberije, É Ambrózay, E Szabó, O Serfozo, E Karpati, G Fox, B Glocker, E A Morris, G Forrai, Nat. Med. 292023</p>
<p>NHS AI test spots tiny cancers missed by doctors. 11 Dec. 2024</p>
<p>NanoString Powered by NVIDIA GPU technology accelerates scientific discovery and the age of spatial biology. NVIDIA. 2024a. 11 Dec. 2024</p>
<p>Clara for Medical Devices. NVIDIA. 2024b. Accessed 11 Dec. 2024</p>
<p>Advancing cell segmentation and morphology analysis with NVIDIA AI Foundation Model VISTA-2D. Nvidia Developer, 2024. 11 Dec. 2024. 11 Dec. 2024. 11 Dec. 2024. 11 Dec. 2024los-alamos-national-laboratory-worktogetherOpening new worlds for molecular discovery</p>
<p>Application of deep learning technique in next-generation sequence experiments. S Özgür, M Orman, J. Big Data. 101602023</p>
<p>CelloType: a unified model for segmentation and classification of tissue images. M Pang, T K Roy, X Wu, K Tan, Nat. Methods. 222024</p>
<p>AI could pose pandemic-scale biosecurity risks. Here's how to make it safer. J Pannu, S Gebauer, G Mckelvey, Jr, A Cicero, T Inglesby, Nature. 6352024</p>
<p>A genomics-driven artificial intelligence-based model classifies breast invasive lobular carcinoma and discovers CDH1 inactivating mechanisms. F Pareja, H Dopeso, Y K Wang, A M Gazzo, D N Brown, M Banerjee, P Selenica, J H Bernhard, F Derakhshan, E M Da Silva, Cancer Res. 842024</p>
<p>Utilizing drug repurposing against COVID-19-efficacy, limitations, and challenges. V Parvathaneni, V Gupta, Life Sci. 2591182752020</p>
<p>First AI assistant to help people interact with PDFs. PDFgear Copilot. 1111 Dec. 2024. 2024. DecMade better with AI</p>
<p>The industry standard for pharmacokinetic/pharmacodynamic (PK/PD) ana lysis. Phoenix Winnonlin™, Software , Launches-First-Smart-Pill-Bottle-and-Mobile-App. 11 Dec. 2024. 11 Dec. 2024Pillsy launches first smart pill bottle and mobile app</p>
<p>practice#loc-artificial-intelligence-tools-and-technologies. Plos, 11 Dec. 2024Artificial intelligence tools and technologies</p>
<p>. PNAS. 11 Dec. 2024</p>
<p>inc-enters-into-multi-target-strategic-collaboration-withtakeda-to-develop-targeted-therapies-for-inflammatory-bowel-disease-300931019.html. 11 Dec. 2024. 11 Dec. 2024Prometheus Biosciences, IncProtein designer and structure solvers win chemistry Nobel</p>
<p>. Quillbot, 11 Dec. 2024</p>
<p>Machine learning in medicine. A Rajkomar, J Dean, I Kohane, N. Engl. J. Med. 3802019</p>
<p>Academic AI Detector. Scispace, 2024. 11 Dec. 2024</p>
<p>Recursion enters into global licensing agreement with Takeda to develop TAK-733 in hereditary cancer syndrome. Takeda, 11 Dec. 2024. 11 Dec. 2024Recursion-Enters-Into-Global-Licensing-Agreement-with-Takeda-to-Develop-TAK-733-in-Hereditary-Cancer-Syndrome</p>
<p>The ethics of using artificial intelligence in scientific research: new guidance needed for a new tool. D B Resnik, M Hosseini, 10.1007/s43681-024-00493-8AI Ethics. 2024</p>
<p>ReviveMed announces AI-driven metabolomics study with Bristol Myers Squibb. ReviveMed-Announces-AI-driven-Metabolomics-Study-with-Bristol-Myers-Squibb. 11 Dec. 2024</p>
<p>The AI-assisted identification and clinical efficacy of baricitinib in the treatment of COVID-19. Vaccines (Basel) 10: 951. Saama launches industry's first AI-driven data platform to accelerate clinical development. P J Richardson, B W S Robinson, D P Smith, J Stebbing, Pharmaceuticals. Ayad M., Hammoudeh S., Abu-Gharbieh E., Hamoudi R., Tarazi H., Al-Tel T.H., Hamid Q.146802022. 11 Dec. 2024. 2021Current status of baricitinib as a repurposed therapy for COVID-19</p>
<p>. Sage. Artificial Intelligence Policy. Accessed 11 Dec. 2024</p>
<p>Old drugs for a new virus: repurposed approaches for combating COVID-19. S Saul, S Einav, ACS Infect. Dis. 62020</p>
<p>science-journals-editorial-policies#imageand-text-integrity. Science Journals. Editorial Policies. 11 Dec. 2024</p>
<p>. Scispace, 11 Dec. 2024</p>
<p>. Scite, Ai, Research, 11 Dec. 2024</p>
<p>Harnessing connectivity in a largescale small-molecule sensitivity dataset. B Scribbr ; Seashore-Ludlow, M G Rees, J H Cheah, M Cokol, E V Price, M E Coletti, V Jones, N E Bodycombe, C K Soule, J Gould, Cancer Discov. 511 Dec. 2024. 2015</p>
<p>Segment Anything Model (SAM): a new AI model from Meta AI that can 'cut out' any object, in any image, with a single click. 11 Dec. 2024</p>
<p>A free, AI-powered research tool for scientific literature. Semantic Scholar, 11 Dec. 2024</p>
<p>BMS partners with Sensyne Health for rare blood disease research. Sensyne Health, 11 Dec. 2024</p>
<p>AI-driven enhancements in drug screening and optimization. A Serghini, S Portelli, D B Ascher, Computational Drug Discovery and Design. Springer2023</p>
<p>The game has changed. AI triumphs at protein folding. R F Service, Science. 3702020</p>
<p>Artificial intelligence-driven prediction of COVID-19-related hospitalization and death: a systematic review. S Shakibfar, F Nyberg, H Li, J Zhao, H M E Nordeng, G K F Sandve, M Pavlovic, M Hajiebrahimi, M Andersen, M Sessa, Front. Public Health. 1111837252023a</p>
<p>Machine learning-driven development of a disease risk score for COVID-19 hospitalization and mortality: a Swedish and Norwegian register-based study. S Shakibfar, J Zhao, H Li, H Nordeng, A Lupattelli, M Pavlovic, G K Sandve, F Nyberg, B Wettermark, M Hajiebrahimi, 2023bFront. Public Health111258840</p>
<p>Research progress of artificial intelligence in prostate cancer diagnosis application. Chin. H Shucai, Z Heyuan, J. Med. Instrum. 482024</p>
<p>The rise of AI: how artificial intelligence is revolutionizing infectious disease control. E E Siddig, H F Eltigani, A Ahmed, Ann. Biomed. Eng. 512023</p>
<p>Machine learning in medicine: a practical introduction. J A Sidey-Gibbons, C J Sidey-Gibbons, BMC Med. Res. Methodol. 192019</p>
<p>The standard for population-based physiologically based modeling and simulation. Pbpk Simcyp™, Simulator, Accessed 11 Dec. 2024</p>
<p>Artificial intelligence for drug repurposing against infectious diseases. A Singh, Artif. Intell. Chem. 21000712024</p>
<p>Artificial Intelligence (AI). Springer Nature, 11 Dec. 2024</p>
<p>Using recurrent neural networks for predicting type-2 diabetes from genomic and tabular data. P N Srinivasu, J Shafi, T B Krishna, C N Sujatha, S P Praveen, M F Ijaz, Diagnostics. 1230672022</p>
<p>. J Stebbing, V Krishnan, S De Bono, S Ottaviani, G Casalini, P J Richardson, V Monteil, V M Lauschke, A Mirazimi, S Youhanna, Y J Tan, EMBO Mol. Med. 12e126972020predicted testing in COVID-19 patients</p>
<p>AlphaFold 3 is great -but it still needs human help to get chemistry righSt. G Steinkellner, W Kroutil, K Gruber, C C Gruber, Nature. 6375482025</p>
<p>Application of artificial intelligence in the diagnosis and treatment of colorectal cancer: a bibliometric analysis. L Sun, R Zhang, Y Gu, L Huang, C Jin, Front. Oncol. 1414240442024. 2004-2023</p>
<p>The Virtual Lab: AI agents design new SARS-CoV-2 nanobodies with experimental validation. K Swanson, W Wu, N L Bulaong, J E Pak, J Zou, bioRxiv: 2024.11.11. 6230042024</p>
<p>The Royal Swedish Academy of Sciences has decided to award the Nobel Prize in Chemistry. J R Swedlow, L Collinson, Nature. 5992021. 11 Dec. 2024. 2024. 11 Dec. 2024. 11 Dec. 2024Transforming cancer diagnostics with the power of AI</p>
<p>Trusted cancer diagnostics for all. Trinka, 11 Dec. 2024. 11 Dec. 2024</p>
<p>. Turnitin, 11 Dec. 2024</p>
<p>A deep learning algorithm for 3D cell detection in whole mouse brain image datasets. A L Tyson, C V Rousseau, C J Niedworok, S Keshavarzi, C Tsitoura, L Cossell, M Strom, T W Margrie, PLoS Comput. Biol. 17e10090742021</p>
<p>tools revolutionizing scientific research in life sciences us-secretary-commerce-raimondo-and-us-secretary-stateblinken-announce. 11 Dec. 2024. 11 Dec. 2024U.S. Secretary of Commerce Raimondo and U.SSecretary of State Blinken announce inaugural convening of international network of AI safety institutes in San Francisco</p>
<p>Dual use of artificial-intelligence-powered drug discovery. F Urbina, F Lentzos, C Invernizzi, S Ekins, Nat. Mach. Intell. 42022</p>
<p>Drug repurposing approaches: existing leads for novel threats and drug targets. T Usha, S K Middha, A A Kukanur, R V Shravani, M N Anupama, N Harshitha, A Rahamath, S A Kukanuri, A K Goyal, Curr. Protein Pept. Sci. 222021</p>
<p>A pathology foundation model for cancer diagnosis and prognosis prediction. X Wang, J Zhao, E Marostica, W Yuan, J Jin, J Zhang, R Li, H Tang, K Wang, Y Li, Nature. 6342024</p>
<p>Why scientists trust AI too much -and what to do about it. 11 Dec. 2024. 20 Jan. 2025</p>
<p>Diagnostic and prognostic value of soluble urokinase-type plasminogen activator receptor (suPAR) in focal segmental glomerulosclerosis and impact of the detection method. W Winnicki, G Sunder-Plassmann, G Sengölge, A Handisurya, H Herkner, C Kornauth, B Bielesz, L Wagner, Z Kikic, S Pajenda, Sci. Rep. 9137832019. 11 Dec. 2024WithdrarXiv: A large-scale dataset for retraction study</p>
<p>. Wordvice, Ai, 11 Dec. 2024</p>
<p>Targeting anti-cancer agents to bone using bisphosphonates. L Xing, F H Ebetino, R K Boeckman, Jr, V Srinivasan, J Tao, T K Sawyer, J Li, Z Yao, B F Boyce, Bone. 1381154922020</p>
<p>An open-access volume electron microscopy atlas of whole cells and tissues. C S Xu, S Pang, G Shtengel, A Müller, A T Ritter, H K Hoffman, S Y Takemura, Z Lu, H A Pasolli, N Iyer, J Chung, Nature. 5992021</p>
<p>A whole-slide foundation model for digital pathology from real-world data. H Xu, N Usuyama, J Bagga, S Zhang, R Rao, T Naumann, C Wong, Z Gero, J González, Y Gu, Y Xu, Nature. 6302024</p>
<p>Translating cancer genomics into precision medicine with artificial intelligence: applications, challenges, and future perspectives. J Xu, P Yang, S Xue, B Sharma, M Sanchez-Martin, F Wang, K A Beaty, E Dehan, B Parikh, Hum. Genet. 1382019</p>
<p>Machine learning in brain imaging genomics. J Yan, L Du, X Yao, L Shen, Machine learning and medical imaging. Zhou SK. Elsevier2016</p>
<p>. Zerogpt, 11 Dec. 2024</p>
<p>Algorithm for optimized mRNA design improves stability and immunogenicity. H Zhang, L Zhang, A Lin, C Xu, Z Li, K Liu, B Liu, X Ma, F Zhao, H Jiang, Nature. 6212023</p>
<p>Ultrasensitive and affordable assay for early detection of primary liver cancer using plasma cell-free DNA fragmentomics. X Zhang, Z Wang, W Tang, X Wang, R Liu, H Bao, X Chen, Y Wei, S Wu, H Bao, Hepatology. 762022</p>
<p>A foundation model for joint segmentation, detection and recognition of biomedical objects across nine modalities. T Zhao, Y Gu, J Yang, N Usuyama, H H Lee, S Kiblawi, T Naumann, J Gao, A Crabtree, J Abel, Nat. Methods. 222024</p>
<p>The construction of cross-population polygenic risk scores using transfer learning. Z Zhao, L G Fritsche, J A Smith, B Mukherjee, S Lee, Am. J. Hum. Genet. 1092022. 1998-2008</p>
<p>Detection and analysis of complex structural variation in human genomes across populations and in brains of donors with psychiatric disorders. B Zhou, J G Arthur, H Guo, T Kim, Y Huang, R Pattni, T Wang, S Kundu, J X J Luo, H Lee, Cell. 1872024</p>
<p>Neural architecture search for microscopy cell segmentation. Y Zhu, E Meijering, Machine Learning in Medical Imaging: 11 th International Workshop. Lima, PeruSpringer20202020Conjunction with MICCAI 2020</p>            </div>
        </div>

    </div>
</body>
</html>
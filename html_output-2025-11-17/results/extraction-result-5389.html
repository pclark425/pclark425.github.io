<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5389 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5389</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5389</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-111.html">extraction-schema-111</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <p><strong>Paper ID:</strong> paper-2b967d82b25088566980aaaf5a7062d90b2fb14f</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/2b967d82b25088566980aaaf5a7062d90b2fb14f" target="_blank">GPT4Graph: Can Large Language Models Understand Graph Structured Data ? An Empirical Evaluation and Benchmarking</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> Through this study, the current limitations of language models in comprehending graph structures and performing associated reasoning tasks are uncovered and the necessity for further advancements and novel approaches to enhance their graph processing capabilities is emphasized.</p>
                <p><strong>Paper Abstract:</strong> Large language models~(LLM) like ChatGPT have become indispensable to artificial general intelligence~(AGI), demonstrating excellent performance in various natural language processing tasks. In the real world, graph data is ubiquitous and an essential part of AGI and prevails in domains like social network analysis, bioinformatics and recommender systems. The training corpus of large language models often includes some algorithmic components, which allows them to achieve certain effects on some graph data-related problems. However, there is still little research on their performance on a broader range of graph-structured data. In this study, we conduct an extensive investigation to assess the proficiency of LLMs in comprehending graph data, employing a diverse range of structural and semantic-related tasks. Our analysis encompasses 10 distinct tasks that evaluate the LLMs' capabilities in graph understanding. Through our study, we not only uncover the current limitations of language models in comprehending graph structures and performing associated reasoning tasks but also emphasize the necessity for further advancements and novel approaches to enhance their graph processing capabilities. Our findings contribute valuable insights towards bridging the gap between language models and graph understanding, paving the way for more effective graph mining and knowledge extraction.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5389.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5389.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GDL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph Description Language</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An umbrella term the paper defines for textual encodings of graphs (e.g., adjacency lists, edge lists, GML, GraphML) used as inputs to LLMs; intended to make graph-structured data interpretable by language models via text serialization and format metadata.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Graph Description Language (GDL)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>A general pipeline stage: convert graph-structured data into a textual description (called GDL) before feeding to an LLM. The paper treats GDL as any textual serialization such as adjacency list (per-node neighbor lists), edge list (one edge per line), GML (Graph Modelling Language; node/edge blocks and attributes in plain text), or GraphML (XML-based textual graph format). The GDL may also be augmented by format explanations or summaries generated by the LLM (self-prompting).</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>general graphs (citation networks, molecular graphs, knowledge graphs, knowledge-graph subgraphs)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Designable textual format; supports attribute inclusion; varying compactness and readability; affects faithfulness and interpretability depending on serialization (e.g., adjacency lists are node-centric, edge lists are connection-centric, GML/GraphML carry richer metadata); sensitive to input order and additional explanatory meta-text; amenable to self-augmentation (format explanation, summarization); potential information loss if not all properties serialized.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Used across the benchmark tasks: structure understanding (size detection, degree detection, edge detection, attribute retrieval, diameter, clustering coefficient) and semantic tasks (KGQA, graph query generation/Cypher, node classification, graph classification).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Accuracy reported on many tasks (see per-format numbers below); metrics across tasks are reported as ACC (percentage). Example aggregate numbers are format-dependent and reported per task (see entries for specific serializations).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>GDL is not one fixed format but a family; the paper compares concrete serializations (adjacency list, edge list, GML, GraphML) and shows substantial performance differences depending on chosen GDL variant (see specific entries).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>GDL design choices strongly affect LLM performance; no single GDL is uniformly best across tasks; ordering, verbosity, and inclusion/exclusion of attributes matter; risk of losing structural information in overly compact/textual encodings; requires careful prompt handling (format explanation, role prompting, change-order) to be effective.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GPT4Graph: Can Large Language Models Understand Graph Structured Data ? An Empirical Evaluation and Benchmarking', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5389.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5389.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AdjList</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adjacency List representation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A node-centric textual serialization listing for each node its neighbor nodes (neighbor lists), used as one GDL variant to present subgraphs to LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>GPT4Graph: Can Large Language Models Understand Graph Structured Data? An Empirical Evaluation and Benchmarking</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Adjacency list (text linearization)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Each node is serialized with its identifier followed by an explicit list of its neighboring node identifiers (e.g., 'Node A: neighbors B, C, D'). The paper used this representation as plain text inside the prompt and experimented with variants (1-shot examples, chain-of-thought, with/without format explanation, role prompting, change-order).</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>citation subgraphs (OGBN-ARXIV sampled 2-hop subgraphs ~10–20 nodes) used for structural tasks</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Node-centric, readily shows local neighborhoods; relatively compact; easy to parse by humans and LLMs; emphasizes per-node connectivity but can be less direct for edge-centric queries; susceptible to ordering effects and redundancy when included with large contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Structure understanding tasks: size detection, degree detection, edge detection, diameter, clustering coefficient, attribute retrieval (attribute retrieval not provided for adjacency-list variants in table).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported ACC (percentage) under 1-shot baseline: Size detection 35.50, Degree detection 15.21, Edge detection 65.45, Diameter 28.00, Clustering 5.42. Variants changed results (e.g., 1-shot-cot Size 44.00 improved size detection by +8.5).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Compared to edge-list, GML, GraphML: adjacency list performs worse on degree detection (15.21 vs edge-list 44.87) and size detection (35.50 vs GML 54.50); edge detection is lower than edge-list (65.45 vs 74.60). Some prompt variants (change-order, role prompting) altered results markedly, indicating strong sensitivity to input design.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Low accuracy on several structural tasks (degree, clustering) indicating possible difficulty for LLMs to aggregate node-centric lists into global structural measures; sensitive to order and prompt formatting; attribute retrieval was not supported in adjacency-list inputs as used in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GPT4Graph: Can Large Language Models Understand Graph Structured Data ? An Empirical Evaluation and Benchmarking', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5389.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5389.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EdgeList</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Edge List representation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An edge-centric textual serialization listing each edge as a separate textual line or entry (e.g., 'A -- B'), used as a GDL variant shown to be beneficial for local-structure tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>GPT4Graph: Can Large Language Models Understand Graph Structured Data? An Empirical Evaluation and Benchmarking</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Edge list (text linearization)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Graph is serialized as a list of edges in text, each line enumerating a pair (or tuple) of node identifiers representing an undirected/directed connection. Used verbatim as part of the prompt, optionally accompanied by format explanations and example queries.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>citation subgraphs (OGBN-ARXIV sampled 2-hop subgraphs ~10–20 nodes) for structure tasks; also used generally in benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Edge-centric and directly exposes pairwise connectivity; compact for sparse graphs; easier for LLMs to answer edge-existence and degree-related questions because edges are explicit; still order-sensitive and lacks explicit node attributes unless appended.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Structure understanding tasks: size detection, degree detection, edge detection, diameter, clustering coefficient.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported ACC (1-shot): Size 22.50, Degree 44.87, Edge detection 74.60, Diameter 43.00, Clustering 13.31. Variants (e.g., 1-shot-cot) changed metrics slightly; 'w/o 1-shot' sometimes improved edge-related tasks (edge ACC up to 80.95 in one variant).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Edge list outperformed adjacency-list and GML for degree detection (44.87 highest) and edge-detection (74.60 highest among 1-shot baselines). GML was best for size detection and attribute retrieval; GraphML and GML excelled at attribute retrieval while edge-list excelled at local structural queries.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Less suitable for attribute-rich queries unless attributes appended; overall no single best format—edge list is best for local connectivity but weaker for tasks requiring node/edge attributes or global summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GPT4Graph: Can Large Language Models Understand Graph Structured Data ? An Empirical Evaluation and Benchmarking', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5389.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5389.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GML</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph Modelling Language (GML) representation (text)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A block-structured plain-text graph format (GML) used as a verbose textual representation including node/edge attributes; here used as a GDL variant and often produced the best results for global/attribute tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>GPT4Graph: Can Large Language Models Understand Graph Structured Data? An Empirical Evaluation and Benchmarking</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>GML (Graph Modelling Language) textual serialization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>GML represents graphs via hierarchical plain-text blocks: node { id X label "..." ... } and edge { source A target B ... } plus arbitrary attributes. The paper provided GML text as input to LLMs, including node attributes when available (used for attribute retrieval experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>citation subgraphs, molecular graphs, and KG-derived subgraphs where attributes are present</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Rich, explicit attribute support; human- and machine-readable; preserves node/edge metadata; more verbose (less compact) than simple lists; tends to provide the LLM with more semantically useful information for attribute retrieval and size detection.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Structure tasks (size detection, degree, edge detection, attribute retrieval, diameter, clustering) and graph classification (OGBG datasets) where node/edge attributes matter.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>1-shot ACC: Size detection 54.50, Degree 20.91, Edge detection 50.45, Attribute retrieval 83.40, Diameter 37.00, Clustering 4.36. For graph classification (OGBG-MOLHIV) 1-shot-tot: GML 66.87 (Table 4). Some variants (w/o format explanation) improved degree and clustering in specific conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>GML achieved the best size-detection (54.50) and very high attribute retrieval (83.40), outperforming adjacency/edge-list on those tasks; however, it underperformed edge-list on degree and edge detection. For graph classification, GML slightly outperformed GraphML on some splits (e.g., 1-shot-tot MOLHIV: GML 66.87 vs GraphML 63.25).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Verbosity may increase prompt length and token use; may include redundant information; for purely structural local tasks (degree/edge), the richer format did not always translate to higher accuracy; sensitivity to prompt design (format explanation, ordering) remains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GPT4Graph: Can Large Language Models Understand Graph Structured Data ? An Empirical Evaluation and Benchmarking', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5389.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5389.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GraphML</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GraphML (XML-based textual graph format)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An XML-based textual graph serialization supporting node/edge attributes; used as a GDL variant in experiments and compared against GML, adjacency and edge lists.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>GPT4Graph: Can Large Language Models Understand Graph Structured Data? An Empirical Evaluation and Benchmarking</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>GraphML (XML textual serialization)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Graphs are encoded as GraphML XML blocks including <node id=...> and <edge source=... target=...> elements plus attribute tags. The paper supplied GraphML text as prompt input, including attribute fields when available.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>citation subgraphs, molecular graphs with node/edge attributes</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Highly structured and explicit about attributes; verbose and with XML syntax which may be more or less natural for LLM consumption; good for attribute retrieval and graph-classification tasks where structured metadata is helpful.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Structure understanding tasks (same set as others) and graph classification (OGBG-MOLHIV, MOLPCBA).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>1-shot ACC: Size detection 25.00, Degree 40.20, Edge detection 62.05, Attribute retrieval 83.87, Diameter 34.00, Clustering 9.74. Graph classification 1-shot-tot: MOLHIV GraphML 63.25, MOLPCBA GraphML 57.45.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>GraphML matches GML on attribute retrieval (GraphML 83.87 vs GML 83.40). On node-local metrics, GraphML outperforms adjacency-list on degree but is slightly behind edge-list on degree/edge detection; for graph classification GML slightly outperformed GraphML on one dataset (MOLHIV).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>XML verbosity increases token usage; formatting specifics (XML tags) can confuse LLMs unless accompanied by format explanation; sensitivity to order and prompt design observed in ablations (e.g., 'w/o change order' dramatically hurt some metrics).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GPT4Graph: Can Large Language Models Understand Graph Structured Data ? An Empirical Evaluation and Benchmarking', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5389.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5389.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SelfPrompt</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Prompting (Context Summarization & Format Explanation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A set of techniques where the LLM is asked to autoregressively generate auxiliary textual context (summaries, neighborhood summaries, and format explanations) that are appended to the input graph text to improve downstream reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Self-prompting (context summarization + format explanation)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>The LLM is instructed in a first pass to produce a summary of the graph (key nodes, important substructures, neighborhood summaries) and/or to explain the input format; the produced text is then concatenated with the original GDL and the user query to form an augmented prompt (possibly iteratively).</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>applies to any GDL-serialized graphs (citation subgraphs, KG snippets, molecular graphs)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Augments raw textual GDL with distilled, neighborhood-aware, and format-explicit text; increases interpretability and context relevance; can prune irrelevant information and surface salient nodes/edges; effectively expands semantic context while increasing prompt length.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Applied across structure understanding (e.g., node classification, graph classification) and semantic tasks (KGQA, GQL generation); used as augmentation in node-classification and graph-classification experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Improvements reported in multiple settings: e.g., graph classification benefits from self-format explanation and self-summarization (Table 4: removing self-format explanation dropped MOLHIV 1-shot-tot GML from 66.87 to 64.71 and removing self-summarization to 61.76); node classification 'one-shot + 1-hop neighborhood context summarization' achieved ACC 60.00 (highest in that table).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Self-augmentation (self-format explanation/self-summarization) outperformed plain GDL inputs in several semantic tasks and graph-classification; provides larger gains than some prompting variants like chain-of-thought for node classification; complements structural serializations (GML/GraphML).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Adds tokens and latency; quality depends on LLM's own ability to summarize faithfully (risk of hallucination or dropping essential edges); open question which self-prompting procedures (single pass vs iterative) are optimal and how to control hallucination.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GPT4Graph: Can Large Language Models Understand Graph Structured Data ? An Empirical Evaluation and Benchmarking', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5389.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5389.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PromptVariants</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompting strategies (1-shot, zero-shot, CoT, role prompting, change-order)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of prompt engineering interventions studied in the paper which modify how the GDL and external information are presented to the LLM and substantially affect performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>GPT4Graph: Can Large Language Models Understand Graph Structured Data? An Empirical Evaluation and Benchmarking</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Prompt engineering variants applied to textual graph inputs</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Variants include: zero-shot, one-shot (include an example), chain-of-thought prompting (1-shot-cot), role prompting (instructing the model to take a specific role), format explanation (explicitly describing the serialization format), change-order (placing external knowledge before/after the graph), and inclusion of graph (+graph). These are applied atop any textual GDL representation.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>applies across all graph types used in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Affects LLM attention to different parts of the textual encoding; can encourage stepwise reasoning (CoT) or focus on task-specific aspects (role prompting); order-sensitive (placing question before or after graph matters); examples can help or hurt depending on task.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>All benchmark tasks: structural tasks (degree, edge, size, diameter, clustering), semantic tasks (KGQA, GQL generation, node/graph classification).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Quantified in Table 1 and Table 2: e.g., 1-shot-cot on adjacency-list improved size detection from 35.50 to 44.00 (+8.50) but sometimes hurt other tasks; 'zero-shot+graph' raised KGQA Wiki from zero-shot 9.23 to 56.38; one-shot Cypher generation achieved 99.00 on MetaQA-1hop (Table 2). Positioning external knowledge before the graph improved many results.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Prompt variants interact with representation choice—e.g., change-order greatly hurt some adjacency/edge list baselines; format explanation improved performance when GDL was complex (GML/GraphML). No single prompting strategy uniformly dominates across formats and tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Prompt engineering gains are inconsistent across tasks and formats; examples can introduce noise; CoT not always helpful (e.g., sometimes degraded node-classification accuracy); choice of where to place external info (before/after graph) is task-sensitive.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GPT4Graph: Can Large Language Models Understand Graph Structured Data ? An Empirical Evaluation and Benchmarking', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5389.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e5389.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CypherGen</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Cypher / GQL Generation from graph text</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Task and method where the LLM generates graph query language (Cypher) statements from textual graph descriptions and natural language queries; evaluated by executing generated Cypher on a Neo4j instance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>GPT4Graph: Can Large Language Models Understand Graph Structured Data? An Empirical Evaluation and Benchmarking</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Graph text -> Cypher (GQL) generation</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>The graph (MetaQA converted into a Neo4j graph DB) and question are presented as text; the LLM is prompted to produce the corresponding Cypher query (text output), which is then executed to obtain answers. Variants include zero-shot and one-shot Cypher generation.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>knowledge-graph (MetaQA movie KG)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Transforms a textual graph + question into executable graph-query language; directly tests whether textual graph encodings provide sufficient structure to formulate formal queries; requires the model to map natural language compositional requirements to GQL patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Graph Query Language generation; indirectly KGQA by executing generated query against Neo4j.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported accuracies (Table 2): zero-shot Cypher Generation on MetaQA-1hop 30.00, 2hop 10.00, 3hop 13.00; one-shot Cypher Generation achieved 99.00 on MetaQA-1hop, 77.00 on 2hop, and 96.00 on 3hop (showing strong sensitivity to few-shot examples).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>One-shot Cypher generation dramatically outperformed zero-shot Cypher and many KGQA LLM variants; augmenting zero-shot with the graph text ('zero-shot+graph') improved KGQA substantially (e.g., Wiki 9.23 -> 56.38).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Zero-shot generation is weak for multi-hop queries; heavy reliance on few-shot exemplars to produce correct queries; correctness assessed by executing queries (requires downstream DB); creating example-to-query mappings is labor-intensive.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GPT4Graph: Can Large Language Models Understand Graph Structured Data ? An Empirical Evaluation and Benchmarking', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>StructGPT: A general framework for large language model to reason over structured data <em>(Rating: 2)</em></li>
                <li>Graph-toolformer: To empower llms with graph reasoning ability via prompt augmented by chatgpt <em>(Rating: 2)</em></li>
                <li>TableGPT: Few-shot table-to-text generation with table structure reconstruction and content matching <em>(Rating: 1)</em></li>
                <li>PLOG: Table-to-logic pretraining for logical table-to-text generation <em>(Rating: 1)</em></li>
                <li>GML: Graph modelling language <em>(Rating: 2)</em></li>
                <li>GraphML <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5389",
    "paper_id": "paper-2b967d82b25088566980aaaf5a7062d90b2fb14f",
    "extraction_schema_id": "extraction-schema-111",
    "extracted_data": [
        {
            "name_short": "GDL",
            "name_full": "Graph Description Language",
            "brief_description": "An umbrella term the paper defines for textual encodings of graphs (e.g., adjacency lists, edge lists, GML, GraphML) used as inputs to LLMs; intended to make graph-structured data interpretable by language models via text serialization and format metadata.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Graph Description Language (GDL)",
            "representation_description": "A general pipeline stage: convert graph-structured data into a textual description (called GDL) before feeding to an LLM. The paper treats GDL as any textual serialization such as adjacency list (per-node neighbor lists), edge list (one edge per line), GML (Graph Modelling Language; node/edge blocks and attributes in plain text), or GraphML (XML-based textual graph format). The GDL may also be augmented by format explanations or summaries generated by the LLM (self-prompting).",
            "graph_type": "general graphs (citation networks, molecular graphs, knowledge graphs, knowledge-graph subgraphs)",
            "representation_properties": "Designable textual format; supports attribute inclusion; varying compactness and readability; affects faithfulness and interpretability depending on serialization (e.g., adjacency lists are node-centric, edge lists are connection-centric, GML/GraphML carry richer metadata); sensitive to input order and additional explanatory meta-text; amenable to self-augmentation (format explanation, summarization); potential information loss if not all properties serialized.",
            "evaluation_task": "Used across the benchmark tasks: structure understanding (size detection, degree detection, edge detection, attribute retrieval, diameter, clustering coefficient) and semantic tasks (KGQA, graph query generation/Cypher, node classification, graph classification).",
            "performance_metrics": "Accuracy reported on many tasks (see per-format numbers below); metrics across tasks are reported as ACC (percentage). Example aggregate numbers are format-dependent and reported per task (see entries for specific serializations).",
            "comparison_to_other_representations": "GDL is not one fixed format but a family; the paper compares concrete serializations (adjacency list, edge list, GML, GraphML) and shows substantial performance differences depending on chosen GDL variant (see specific entries).",
            "limitations_or_challenges": "GDL design choices strongly affect LLM performance; no single GDL is uniformly best across tasks; ordering, verbosity, and inclusion/exclusion of attributes matter; risk of losing structural information in overly compact/textual encodings; requires careful prompt handling (format explanation, role prompting, change-order) to be effective.",
            "uuid": "e5389.0",
            "source_info": {
                "paper_title": "GPT4Graph: Can Large Language Models Understand Graph Structured Data ? An Empirical Evaluation and Benchmarking",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "AdjList",
            "name_full": "Adjacency List representation",
            "brief_description": "A node-centric textual serialization listing for each node its neighbor nodes (neighbor lists), used as one GDL variant to present subgraphs to LLMs.",
            "citation_title": "GPT4Graph: Can Large Language Models Understand Graph Structured Data? An Empirical Evaluation and Benchmarking",
            "mention_or_use": "use",
            "representation_name": "Adjacency list (text linearization)",
            "representation_description": "Each node is serialized with its identifier followed by an explicit list of its neighboring node identifiers (e.g., 'Node A: neighbors B, C, D'). The paper used this representation as plain text inside the prompt and experimented with variants (1-shot examples, chain-of-thought, with/without format explanation, role prompting, change-order).",
            "graph_type": "citation subgraphs (OGBN-ARXIV sampled 2-hop subgraphs ~10–20 nodes) used for structural tasks",
            "representation_properties": "Node-centric, readily shows local neighborhoods; relatively compact; easy to parse by humans and LLMs; emphasizes per-node connectivity but can be less direct for edge-centric queries; susceptible to ordering effects and redundancy when included with large contexts.",
            "evaluation_task": "Structure understanding tasks: size detection, degree detection, edge detection, diameter, clustering coefficient, attribute retrieval (attribute retrieval not provided for adjacency-list variants in table).",
            "performance_metrics": "Reported ACC (percentage) under 1-shot baseline: Size detection 35.50, Degree detection 15.21, Edge detection 65.45, Diameter 28.00, Clustering 5.42. Variants changed results (e.g., 1-shot-cot Size 44.00 improved size detection by +8.5).",
            "comparison_to_other_representations": "Compared to edge-list, GML, GraphML: adjacency list performs worse on degree detection (15.21 vs edge-list 44.87) and size detection (35.50 vs GML 54.50); edge detection is lower than edge-list (65.45 vs 74.60). Some prompt variants (change-order, role prompting) altered results markedly, indicating strong sensitivity to input design.",
            "limitations_or_challenges": "Low accuracy on several structural tasks (degree, clustering) indicating possible difficulty for LLMs to aggregate node-centric lists into global structural measures; sensitive to order and prompt formatting; attribute retrieval was not supported in adjacency-list inputs as used in experiments.",
            "uuid": "e5389.1",
            "source_info": {
                "paper_title": "GPT4Graph: Can Large Language Models Understand Graph Structured Data ? An Empirical Evaluation and Benchmarking",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "EdgeList",
            "name_full": "Edge List representation",
            "brief_description": "An edge-centric textual serialization listing each edge as a separate textual line or entry (e.g., 'A -- B'), used as a GDL variant shown to be beneficial for local-structure tasks.",
            "citation_title": "GPT4Graph: Can Large Language Models Understand Graph Structured Data? An Empirical Evaluation and Benchmarking",
            "mention_or_use": "use",
            "representation_name": "Edge list (text linearization)",
            "representation_description": "Graph is serialized as a list of edges in text, each line enumerating a pair (or tuple) of node identifiers representing an undirected/directed connection. Used verbatim as part of the prompt, optionally accompanied by format explanations and example queries.",
            "graph_type": "citation subgraphs (OGBN-ARXIV sampled 2-hop subgraphs ~10–20 nodes) for structure tasks; also used generally in benchmark.",
            "representation_properties": "Edge-centric and directly exposes pairwise connectivity; compact for sparse graphs; easier for LLMs to answer edge-existence and degree-related questions because edges are explicit; still order-sensitive and lacks explicit node attributes unless appended.",
            "evaluation_task": "Structure understanding tasks: size detection, degree detection, edge detection, diameter, clustering coefficient.",
            "performance_metrics": "Reported ACC (1-shot): Size 22.50, Degree 44.87, Edge detection 74.60, Diameter 43.00, Clustering 13.31. Variants (e.g., 1-shot-cot) changed metrics slightly; 'w/o 1-shot' sometimes improved edge-related tasks (edge ACC up to 80.95 in one variant).",
            "comparison_to_other_representations": "Edge list outperformed adjacency-list and GML for degree detection (44.87 highest) and edge-detection (74.60 highest among 1-shot baselines). GML was best for size detection and attribute retrieval; GraphML and GML excelled at attribute retrieval while edge-list excelled at local structural queries.",
            "limitations_or_challenges": "Less suitable for attribute-rich queries unless attributes appended; overall no single best format—edge list is best for local connectivity but weaker for tasks requiring node/edge attributes or global summaries.",
            "uuid": "e5389.2",
            "source_info": {
                "paper_title": "GPT4Graph: Can Large Language Models Understand Graph Structured Data ? An Empirical Evaluation and Benchmarking",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "GML",
            "name_full": "Graph Modelling Language (GML) representation (text)",
            "brief_description": "A block-structured plain-text graph format (GML) used as a verbose textual representation including node/edge attributes; here used as a GDL variant and often produced the best results for global/attribute tasks.",
            "citation_title": "GPT4Graph: Can Large Language Models Understand Graph Structured Data? An Empirical Evaluation and Benchmarking",
            "mention_or_use": "use",
            "representation_name": "GML (Graph Modelling Language) textual serialization",
            "representation_description": "GML represents graphs via hierarchical plain-text blocks: node { id X label \"...\" ... } and edge { source A target B ... } plus arbitrary attributes. The paper provided GML text as input to LLMs, including node attributes when available (used for attribute retrieval experiments).",
            "graph_type": "citation subgraphs, molecular graphs, and KG-derived subgraphs where attributes are present",
            "representation_properties": "Rich, explicit attribute support; human- and machine-readable; preserves node/edge metadata; more verbose (less compact) than simple lists; tends to provide the LLM with more semantically useful information for attribute retrieval and size detection.",
            "evaluation_task": "Structure tasks (size detection, degree, edge detection, attribute retrieval, diameter, clustering) and graph classification (OGBG datasets) where node/edge attributes matter.",
            "performance_metrics": "1-shot ACC: Size detection 54.50, Degree 20.91, Edge detection 50.45, Attribute retrieval 83.40, Diameter 37.00, Clustering 4.36. For graph classification (OGBG-MOLHIV) 1-shot-tot: GML 66.87 (Table 4). Some variants (w/o format explanation) improved degree and clustering in specific conditions.",
            "comparison_to_other_representations": "GML achieved the best size-detection (54.50) and very high attribute retrieval (83.40), outperforming adjacency/edge-list on those tasks; however, it underperformed edge-list on degree and edge detection. For graph classification, GML slightly outperformed GraphML on some splits (e.g., 1-shot-tot MOLHIV: GML 66.87 vs GraphML 63.25).",
            "limitations_or_challenges": "Verbosity may increase prompt length and token use; may include redundant information; for purely structural local tasks (degree/edge), the richer format did not always translate to higher accuracy; sensitivity to prompt design (format explanation, ordering) remains.",
            "uuid": "e5389.3",
            "source_info": {
                "paper_title": "GPT4Graph: Can Large Language Models Understand Graph Structured Data ? An Empirical Evaluation and Benchmarking",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "GraphML",
            "name_full": "GraphML (XML-based textual graph format)",
            "brief_description": "An XML-based textual graph serialization supporting node/edge attributes; used as a GDL variant in experiments and compared against GML, adjacency and edge lists.",
            "citation_title": "GPT4Graph: Can Large Language Models Understand Graph Structured Data? An Empirical Evaluation and Benchmarking",
            "mention_or_use": "use",
            "representation_name": "GraphML (XML textual serialization)",
            "representation_description": "Graphs are encoded as GraphML XML blocks including &lt;node id=...&gt; and &lt;edge source=... target=...&gt; elements plus attribute tags. The paper supplied GraphML text as prompt input, including attribute fields when available.",
            "graph_type": "citation subgraphs, molecular graphs with node/edge attributes",
            "representation_properties": "Highly structured and explicit about attributes; verbose and with XML syntax which may be more or less natural for LLM consumption; good for attribute retrieval and graph-classification tasks where structured metadata is helpful.",
            "evaluation_task": "Structure understanding tasks (same set as others) and graph classification (OGBG-MOLHIV, MOLPCBA).",
            "performance_metrics": "1-shot ACC: Size detection 25.00, Degree 40.20, Edge detection 62.05, Attribute retrieval 83.87, Diameter 34.00, Clustering 9.74. Graph classification 1-shot-tot: MOLHIV GraphML 63.25, MOLPCBA GraphML 57.45.",
            "comparison_to_other_representations": "GraphML matches GML on attribute retrieval (GraphML 83.87 vs GML 83.40). On node-local metrics, GraphML outperforms adjacency-list on degree but is slightly behind edge-list on degree/edge detection; for graph classification GML slightly outperformed GraphML on one dataset (MOLHIV).",
            "limitations_or_challenges": "XML verbosity increases token usage; formatting specifics (XML tags) can confuse LLMs unless accompanied by format explanation; sensitivity to order and prompt design observed in ablations (e.g., 'w/o change order' dramatically hurt some metrics).",
            "uuid": "e5389.4",
            "source_info": {
                "paper_title": "GPT4Graph: Can Large Language Models Understand Graph Structured Data ? An Empirical Evaluation and Benchmarking",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "SelfPrompt",
            "name_full": "Self-Prompting (Context Summarization & Format Explanation)",
            "brief_description": "A set of techniques where the LLM is asked to autoregressively generate auxiliary textual context (summaries, neighborhood summaries, and format explanations) that are appended to the input graph text to improve downstream reasoning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Self-prompting (context summarization + format explanation)",
            "representation_description": "The LLM is instructed in a first pass to produce a summary of the graph (key nodes, important substructures, neighborhood summaries) and/or to explain the input format; the produced text is then concatenated with the original GDL and the user query to form an augmented prompt (possibly iteratively).",
            "graph_type": "applies to any GDL-serialized graphs (citation subgraphs, KG snippets, molecular graphs)",
            "representation_properties": "Augments raw textual GDL with distilled, neighborhood-aware, and format-explicit text; increases interpretability and context relevance; can prune irrelevant information and surface salient nodes/edges; effectively expands semantic context while increasing prompt length.",
            "evaluation_task": "Applied across structure understanding (e.g., node classification, graph classification) and semantic tasks (KGQA, GQL generation); used as augmentation in node-classification and graph-classification experiments.",
            "performance_metrics": "Improvements reported in multiple settings: e.g., graph classification benefits from self-format explanation and self-summarization (Table 4: removing self-format explanation dropped MOLHIV 1-shot-tot GML from 66.87 to 64.71 and removing self-summarization to 61.76); node classification 'one-shot + 1-hop neighborhood context summarization' achieved ACC 60.00 (highest in that table).",
            "comparison_to_other_representations": "Self-augmentation (self-format explanation/self-summarization) outperformed plain GDL inputs in several semantic tasks and graph-classification; provides larger gains than some prompting variants like chain-of-thought for node classification; complements structural serializations (GML/GraphML).",
            "limitations_or_challenges": "Adds tokens and latency; quality depends on LLM's own ability to summarize faithfully (risk of hallucination or dropping essential edges); open question which self-prompting procedures (single pass vs iterative) are optimal and how to control hallucination.",
            "uuid": "e5389.5",
            "source_info": {
                "paper_title": "GPT4Graph: Can Large Language Models Understand Graph Structured Data ? An Empirical Evaluation and Benchmarking",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "PromptVariants",
            "name_full": "Prompting strategies (1-shot, zero-shot, CoT, role prompting, change-order)",
            "brief_description": "A family of prompt engineering interventions studied in the paper which modify how the GDL and external information are presented to the LLM and substantially affect performance.",
            "citation_title": "GPT4Graph: Can Large Language Models Understand Graph Structured Data? An Empirical Evaluation and Benchmarking",
            "mention_or_use": "use",
            "representation_name": "Prompt engineering variants applied to textual graph inputs",
            "representation_description": "Variants include: zero-shot, one-shot (include an example), chain-of-thought prompting (1-shot-cot), role prompting (instructing the model to take a specific role), format explanation (explicitly describing the serialization format), change-order (placing external knowledge before/after the graph), and inclusion of graph (+graph). These are applied atop any textual GDL representation.",
            "graph_type": "applies across all graph types used in experiments",
            "representation_properties": "Affects LLM attention to different parts of the textual encoding; can encourage stepwise reasoning (CoT) or focus on task-specific aspects (role prompting); order-sensitive (placing question before or after graph matters); examples can help or hurt depending on task.",
            "evaluation_task": "All benchmark tasks: structural tasks (degree, edge, size, diameter, clustering), semantic tasks (KGQA, GQL generation, node/graph classification).",
            "performance_metrics": "Quantified in Table 1 and Table 2: e.g., 1-shot-cot on adjacency-list improved size detection from 35.50 to 44.00 (+8.50) but sometimes hurt other tasks; 'zero-shot+graph' raised KGQA Wiki from zero-shot 9.23 to 56.38; one-shot Cypher generation achieved 99.00 on MetaQA-1hop (Table 2). Positioning external knowledge before the graph improved many results.",
            "comparison_to_other_representations": "Prompt variants interact with representation choice—e.g., change-order greatly hurt some adjacency/edge list baselines; format explanation improved performance when GDL was complex (GML/GraphML). No single prompting strategy uniformly dominates across formats and tasks.",
            "limitations_or_challenges": "Prompt engineering gains are inconsistent across tasks and formats; examples can introduce noise; CoT not always helpful (e.g., sometimes degraded node-classification accuracy); choice of where to place external info (before/after graph) is task-sensitive.",
            "uuid": "e5389.6",
            "source_info": {
                "paper_title": "GPT4Graph: Can Large Language Models Understand Graph Structured Data ? An Empirical Evaluation and Benchmarking",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "CypherGen",
            "name_full": "Cypher / GQL Generation from graph text",
            "brief_description": "Task and method where the LLM generates graph query language (Cypher) statements from textual graph descriptions and natural language queries; evaluated by executing generated Cypher on a Neo4j instance.",
            "citation_title": "GPT4Graph: Can Large Language Models Understand Graph Structured Data? An Empirical Evaluation and Benchmarking",
            "mention_or_use": "use",
            "representation_name": "Graph text -&gt; Cypher (GQL) generation",
            "representation_description": "The graph (MetaQA converted into a Neo4j graph DB) and question are presented as text; the LLM is prompted to produce the corresponding Cypher query (text output), which is then executed to obtain answers. Variants include zero-shot and one-shot Cypher generation.",
            "graph_type": "knowledge-graph (MetaQA movie KG)",
            "representation_properties": "Transforms a textual graph + question into executable graph-query language; directly tests whether textual graph encodings provide sufficient structure to formulate formal queries; requires the model to map natural language compositional requirements to GQL patterns.",
            "evaluation_task": "Graph Query Language generation; indirectly KGQA by executing generated query against Neo4j.",
            "performance_metrics": "Reported accuracies (Table 2): zero-shot Cypher Generation on MetaQA-1hop 30.00, 2hop 10.00, 3hop 13.00; one-shot Cypher Generation achieved 99.00 on MetaQA-1hop, 77.00 on 2hop, and 96.00 on 3hop (showing strong sensitivity to few-shot examples).",
            "comparison_to_other_representations": "One-shot Cypher generation dramatically outperformed zero-shot Cypher and many KGQA LLM variants; augmenting zero-shot with the graph text ('zero-shot+graph') improved KGQA substantially (e.g., Wiki 9.23 -&gt; 56.38).",
            "limitations_or_challenges": "Zero-shot generation is weak for multi-hop queries; heavy reliance on few-shot exemplars to produce correct queries; correctness assessed by executing queries (requires downstream DB); creating example-to-query mappings is labor-intensive.",
            "uuid": "e5389.7",
            "source_info": {
                "paper_title": "GPT4Graph: Can Large Language Models Understand Graph Structured Data ? An Empirical Evaluation and Benchmarking",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "StructGPT: A general framework for large language model to reason over structured data",
            "rating": 2
        },
        {
            "paper_title": "Graph-toolformer: To empower llms with graph reasoning ability via prompt augmented by chatgpt",
            "rating": 2
        },
        {
            "paper_title": "TableGPT: Few-shot table-to-text generation with table structure reconstruction and content matching",
            "rating": 1
        },
        {
            "paper_title": "PLOG: Table-to-logic pretraining for logical table-to-text generation",
            "rating": 1
        },
        {
            "paper_title": "GML: Graph modelling language",
            "rating": 2
        },
        {
            "paper_title": "GraphML",
            "rating": 2
        }
    ],
    "cost": 0.013727449999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>GPT4Graph: Can Large Language Models Understand Graph Structured Data? An Empirical Evaluation and Benchmarking</h1>
<p>Jiayan Guo ${ }^{1} ;$ Lun Du ${ }^{2} ;$ Hengyu Liu ${ }^{3}$, Mengyu Zhou ${ }^{2}$, Xinyi He ${ }^{4}$, Shi Han ${ }^{2}$<br>${ }^{1}$ School of Intelligence Science and Technology, Peking University;<br>${ }^{2}$ Microsoft; ${ }^{3}$ University of Technology Sydney; ${ }^{4} \mathrm{Xi}$ an Jiaotong University<br>guojiayan@pku.edu.cn, {lun.du, mezhou, shihan}@microsoft.com, hengyu.liu@uts.edu.au,hxyhxy@stu.xjtu.edu.cn</p>
<h4>Abstract</h4>
<p>Large language models (LLM) like ChatGPT have become indispensable to artificial general intelligence (AGI), demonstrating excellent performance in various natural language processing tasks. Graph data is ubiquitous and an essential part of AGI. The training corpus of large language models often includes some algorithmic components, which allows them to achieve certain effects on some graph data-related problems. However, there is still little research on their performance on a broader range of graphstructured data. In this paper, we conduct an empirical study to assess the proficiency of LLMs in comprehending graph data, employing a diverse range of structural and semanticrelated tasks that evaluate the LLMs' capabilities in graph understanding. Through our study, we uncover current limitations and future directions of LLMs in comprehending graph and performing associated reasoning tasks.</p>
<h2>1 Introduction</h2>
<p>Large Language Models (LLMs) have demonstrated significant capability across a diverse array of human-centric tasks. These tasks range from answering questions to performing semantic analysis and identifying named entities (Zhao et al., 2023). Despite the considerable strides that have been made, the capacity of LLMs to decipher and manage structured knowledge, especially in the form of graph-structured data, remains an area ripe for exploration. Understanding graph-structured data is vital, given its pervasive presence and integral role in a multitude of applications such as social network analysis, drug discovery, recommender systems, and spatio-temporal prediction. Understanding graph data is crucial for AGI.</p>
<p>Tasks based on graph data can be broadly classified into two categories based on their goals.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>The first category includes structure understanding tasks like identifying significant nodes, calculating centrality metrics (Okamoto et al., 2008; Zhang and Luo, 2017; Brandes, 2001; Barthelemy, 2004; Newman, 2005), and determining diameters (Chung et al., 1994). The second category encompasses semantic understanding tasks, such as knowledge graph question answering (can be abstracted as knowledge graph ) (Huang et al., 2019; Zhang et al., 2018), node classification (Bhagat et al., 2011; Rong et al., 2019) and graph classification (Errica et al., 2019), etc. These tasks have distinct requirements and challenges.</p>
<p>Previous researches have investigated the use of LLMs for structural understanding (Sui et al., 2023; Jiang et al., 2023; Gong et al., 2020; Liu et al., 2022), but the emphasis has been predominantly on tables, which rely heavily on structured tabular data. Graphs, on the other hand, introduce additional dimensions of complexity. Comprised of nodes that represent entities or concepts, and edges that express relationships between these entities, graphs necessitate a more sophisticated level of comprehension from LLMs. Understanding graph structued data with LLM remains challenges. First of all, graph data can not be directly handled by LLM, as graph data are unorganized and complex. Secondly, there is a wide range of graph-related tasks, designing efficient input format for different tasks and effective prompt techniques is essential while rarely explored.</p>
<p>In this paper, our goal is to setup a comprehensive comparison to show the ability of LLM in understanding graph structured data. To achieve this goal, we first bridge the existing gap between Large Language Models (LLMs) by proposing a novel framework that integrates LLMs and graphstructured data, intending to enhance their synergistic ability across a wide range of graph mining tasks. Based on the framework, we establish a benchmark across ten common scenarios</p>
<p>to assess language models' capability in handling graph-related tasks. In addition, we experiment with various prompting methods, including both handcrafted and self-generated prompts, to demonstrate their effectiveness in boosting performance in both zero-shot and few-shot settings. Our findings reveal that while LLMs have demonstrated some capability in handling graph-structured data, there remains a substantial need for further development to achieve a performance level comparable to specialized graph-oriented models. In summary, our contribution can be summarized by:</p>
<ul>
<li>We introduce a new framework that combines Large Language Models (LLMs) and graphstructured data. This setup uses the language understanding skills of LLMs and graph description language with promt engineering to improve how they work together in different situations.</li>
<li>We develope a wide-ranging set of tasks, across ten common scenarios, to check how well LLMs can handle tasks involving graph data. This set of taks provides a consistent way to check how good language models are at dealing with complex graph data.</li>
<li>Our empirical results show that, while LLMs are getting better at handling graph data, they still have a lot of improving to do if they are to catch up with models that are specifically designed to work with graphs.</li>
</ul>
<h2>2 Preliminary</h2>
<h3>2.1 Graph Mining Tasks</h3>
<p>Graph mining tasks refer to the process of extracting valuable and actionable insights from graphstructured data. Graphs are mathematical structures that represent relationships between entities, where nodes represent entities and edges represent the connections or interactions between them. Graph mining involves analyzing these graphs to discover patterns, relationships, communities, and other useful information. Some graph mining tasks include node classification, link prediction, and community detection. These tasks are crucial in various domains, including social network analysis (Wasserman and Faust, 1994), bioinformatics (Baxevanis et al., 2020), recommendation systems (Isinkaye et al., 2015), fraud detection (Bolton and Hand, 2002), and knowledge graphs (Ji et al., 2021).
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Graph Understanding with LLM Framework. The graph data is first converted to graph description language that can be understand by LLM. Then the prompt handler combines user query and GDL with potential multiple rounds to generate the answer.</p>
<h3>2.2 Graph Description Language</h3>
<p>A graph description language is a formal language or notation used to define or represent graphstructured data. It provides a standardized syntax and semantics for describing the elements and relationships within a graph. Graph description languages enable the creation, manipulation, and interpretation of graphs in a consistent and machinereadable manner. These languages provide a way to define graph structures, specify node and edge properties, and perform queries and operations on graphs. They are essential for working with graph data and enabling interoperability between graphbased systems and tools. For example, graphs can be represented by an edge list or an adjacency list, providing two distinct perspectives on the graph's structure. An edge list defines a graph in terms of its individual connections, whereas an adjacency list describes each node in terms of its neighboring nodes. Along with these basic representations, more sophisticated formats have been developed to convey richer, contextual information about the graph. For instance, the Graph Modelling Language (GML)(Himsolt, 1997) and Graph Markup Language (GraphML)(Brandes et al., 2013) provide extensible, language-based frameworks for graph representation.</p>
<h2>3 Graph Understanding with LLM Pipeline</h2>
<p>The overall pipline of graph understanding with LLMs is illustrated in Figure 1. Where for graph data, we first generate their graph description languages (GDL), and then use prompt handler to combine the user query and GDL to form the input to the LLMs. The LLMs performs reasoning and</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Illustration of Self-prompting. The first request is to ask LLMs to automatically generate the context of the input graph (w/ or w/o respect to the question). We may ask LLMs multiple context related questions. After generating the new context (such as context summarization and format explanation), the new context is combined with the original input and are sent to the LLMs to generate the final output.</p>
<p>generate answers to the user. During the reasoning, the LLMs may generate intermedium output that should be handled by prompt handler to form new input to the LLMs. Here we elaborate the prompt handler to show how to make LLM better understand graph data.</p>
<h3>3.1 Manual Prompting</h3>
<p>Manual prompting for graph-based problems involves utilizing familiar graph representations to prompt a large language model (LLM) for desired outputs. The novelty of this approach lies in the fact that it requires a shift from traditional text-based inputs to graphical representations. These graph formats have been discussed in Section 2.2. By employing these graph formats as input, we can provide more comprehensive and context-rich information about the graph to the LLM. Other manual prompting techniques include adding format explanation to make LLM better understand the format and adding role prompting to make LLM better understand the specific task. Besides, we can also change the input order between question and external input, and adding examples to utilize the in-context learning ability [wei2021gpt] of LLM.</p>
<p>Nonetheless, some recent developed change-of-thoughts promptings [kojima2022gpt; yao2023gpt] can also be applied to enhance the reasoning ability of LLM for there are many tasks requiring multiple step of reasoning (e.g., clustering coefficient computing).</p>
<h3>3.2 Self-Prompting</h3>
<p>Sometimes the given graph context contains less useful or redundant information for solving tasks. Thus we need LLM to perform self-prompting to obtain more context or eliminating irrelevant information from the given input. It can be challenging for LLM to generate effective prompts for graph-based tasks, as graphs have complex structures and relationships that need to be accurately captured in the prompt. However, there are several strategies that can be employed for self-prompting in graph-based tasks.</p>
<p><strong>Context Summarization</strong>: LLM can generate a summary of the given graph by extracting key features, such as important nodes, edges, or subgraphs. The generated summary can serve as a prompt for the subsequent graph-related questions or tasks. Besides, based on some important elements like nodes and edges, we can use LLM to summarize their context (neighborhood) information to form neighborhood aware text features.</p>
<p><strong>Format Explanation</strong>: Sometimes it is hard for a human to give the entire description of the input graph format. To make the LLM gain more context information of the input graph, we can make the LLM to generate format explanation by itself.</p>
<p>By leveraging these self-prompting strategies, LLM can actively engage in the understanding and manipulation of graphs, facilitating graph-based reasoning and learning.</p>
<h1>4 Graph Understanding Benchmark</h1>
<h3>4.1 Structure Understanding Tasks</h3>
<p><strong>Graph Size Detection.</strong> This task evaluates a large language model's (LLM) capability to discern the size of a provided graph. In this context, size refers to the count of nodes and edges present in the graph. The LLM is expected to accurately determine these metrics, even when user-provided designs and accompanying data, such as descriptions, statements, or queries, augment the graph. Despite the inherent</p>
<p>challenge this poses for language models, a precise count of nodes and edges is critical, as it enables the LLM to contextualize information accordingly. Degree Detection. This task investigates the LLM's aptitude for understanding a node's contextual relevance within a graph. Here, the degree of a node-an indicator of a node's importance and the sparsity of its connections-forms the crux of the task. The LLM must ascertain the number of neighbors for a given node, based on the graph text and any supplementary information. The degree of a node is foundational for various centrality measures such as degree centrality and clustering coefficient, underscoring the task's importance in understanding a node's local structure.</p>
<p>Edge Detection. Building on degree detection, this task further explores the LLM's understanding of a node's local structure. The model must identify the neighboring nodes of a given node, a skill that is vital for complex graph mining activities like calculating distances and discerning connectivity patterns. Mastery of this task signifies the LLM's comprehension of the fundamental aspects necessary for advanced graph analysis.</p>
<p>Attribute Retrieval. This task tests the LLM's capacity to retrieve pertinent details about a node, such as the node's attributes, which play a key role in defining its characteristics. For instance, the LLM might need to retrieve a specific attribute such as a paper's title or an author's gender. Success in this task highlights the LLM's ability to comprehend and retrieve essential node-related information.</p>
<p>Diameter Computing. This task challenges the LLM to calculate the diameter of a graph. The diameter, which is the longest shortest path between any two nodes, offers valuable insights into the graph's overall connectivity and reachability. A successful computation of the diameter showcases the LLM's grasp of the graph's structure and its ability to analyze the graph's overarching characteristics.</p>
<p>Clustering Coefficient Computing. In this task, the LLM needs to compute the clustering coefficient of a graph, a measure that indicates how closely nodes in a graph tend to cluster together. The task thereby provides a means to assess the LLM's understanding of local connectivity patterns
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Structure Understanding Tasks
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Semantic Understanding Tasks
and its ability to evaluate the degree of clustering within a graph. Besides, it tests the ability of reasoning of LLM for computing CC has several steps.</p>
<h3>4.2 Semantic Understanding Tasks</h3>
<p>Knowledge Graph Question Answering. This task gauges the LLM's proficiency in answering questions that pertain to a knowledge graph. Knowledge graphs organize data into a structured format, embodying entities, attributes, and relationships. Task success hinges on the LLM's ability to reason and understand the underlying graph structure to provide accurate answers, thus demonstrating its semantic understanding and capability to navigate and extract information from a KG.</p>
<p>Graph Query Language Generation. This task measures the LLM's capability to generate graph query languages that satisfy user requirements. These languages, including GQL and Cypher, allow users to extract specific information from a graph database. By generating appropriate graph</p>
<p>queries in response to user information needs, the LLM showcases its comprehension of user intent and precision in query formulation.</p>
<p>Node Classification. This task requires the LLM to classify nodes within a graph based on their attributes or structural features. The LLM is given labeled node examples and their associated classes, and it must correctly predict the class of unseen nodes by applying learned patterns from the labeled data. Success in node classification showcases the LLM's ability to generalize from examples and apply its understanding of node attributes and structure to classify new nodes accurately.</p>
<p>Graph Classification. This task extends the scope of node classification to encompass entire graphs. The LLM is given graphs, each labeled with specific categories or classes, and is expected to accurately classify unseen graphs by using patterns learned from the labeled examples. This task evaluates the LLM's ability to understand and apply the structural and attribute-based characteristics of a graph holistically, thus enabling accurate classification of new graphs.</p>
<h2>5 Data Collection</h2>
<h3>5.1 Structure Understanding Task</h3>
<p>To demonstrate the capabilities of language models in reasoning over Structure Understanding Tasks, we selected two well-known citation networks: obgn-arxiv (Hu et al., 2020) and Aminer (Tang et al., 2008). Our approach involved randomly sampling 100 initial seed nodes from each graph and applying a Depth-First Search (DFS) algorithm to sample 2-hop subgraphs centered around these nodes. Each subgraph consisted of approximately 10-20 nodes and 40 edges. To evaluate the performance of the language model, we assigned it the following tasks within these subgraphs: degree detection, attribute retrieval, clustering, size detection, and diameter estimation. For the first three tasks, the model provided results for each individual node in the subgraphs. However, for size detection and diameter estimation, we computed the results for each entire subgraph. Another task we tackled was Edge Detection. Here, we treated each edge in the graph as a positive sample and randomly selected an edge not present in the graph as a negative sample. We then asked the language model to determine whether a given edge belonged to the subgraph or not, based on the information provided by the subgraph.</p>
<h3>5.2 Semantic Understanding Task</h3>
<p>Shifting our focus to semantic understanding tasks, we conducted knowledge graph question answering using two widely-used datasets: Wiki, a temporal knowledge graph, and MetaQA, a multi-hop movie knowledge base. These datasets served as a testing ground to evaluate the performance of the language model in these domains. For node classification, we leveraged the original labels available in the ogbn-arxiv dataset. We randomly sampled 100 nodes from the test set and tasked the language model with predicting their labels based on information such as the node's title, abstract, and the text information from its k-hop neighbors. In addition, we explored graph query language generation using the MetaQA dataset. We constructed a graph database from this dataset and prompted the language model to generate corresponding graph query languages (GQL) like Cypher. The generated GQL statements were then executed using the Neo4j engine. Through these experiments, we aime to assess the language model's performance in various tasks related to structural and semantic understanding in graph structured data.</p>
<h2>6 Experiments</h2>
<h3>6.1 Experimental Setup</h3>
<h2>Downstream Task.</h2>
<p>Models. We evaluate the performance of the recent dominant LLM model, InstructGPT-3 (Ouyang et al., 2022), using versions text-davinci-001, text-davinci-002, and text-davinci003. Unless otherwise specified, we utilize text-davinci-003 in all experiments. The temperature is set to 0.3 to control the variety of the output.</p>
<h3>6.2 Results for Structure Understanding Task</h3>
<p>The results for the Structure Understanding Task are presented in Table 1, revealing several significant findings:
Input Design Has a Significant Impact on the Final Result. Our experiments demonstrate that the design of the input plays a crucial role in determining the performance of the model. By carefully considering the arrangement and organization of the input data, we can substantially influence the model's ability to understand the structural aspects of the task at hand. Fine-tuning the input design can</p>
<p>Table 1: Experiments on Graph Structural Understanding on OGBN-ARXIV. ACC indicates average accuracy over samples, while $\Delta$ indicates the difference of variants with the 1-shot setting. - denotes that the input format do not contain corresponding information.</p>
<table>
<thead>
<tr>
<th>Format</th>
<th>Input Design</th>
<th>Size Detection</th>
<th></th>
<th>Degree Detection</th>
<th></th>
<th>Edge Detection</th>
<th></th>
<th>Attribute Retrieval</th>
<th></th>
<th>Diameter</th>
<th></th>
<th>Clustering</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>ACC</td>
<td>$\Delta$</td>
<td>ACC</td>
<td>$\Delta$</td>
<td>ACC</td>
<td>$\Delta$</td>
<td>ACC</td>
<td>$\Delta$</td>
<td>ACC</td>
<td>$\Delta$</td>
<td>ACC</td>
<td>$\Delta$</td>
</tr>
<tr>
<td>Adjacency List</td>
<td>1-shot</td>
<td>35.50</td>
<td>0.00</td>
<td>15.21</td>
<td>0.00</td>
<td>65.45</td>
<td>0.00</td>
<td>-</td>
<td>-</td>
<td>28.00</td>
<td>0.00</td>
<td>5.42</td>
<td>0.00</td>
</tr>
<tr>
<td></td>
<td>1-shot-cot</td>
<td>44.00</td>
<td>+8.50</td>
<td>14.58</td>
<td>-0.63</td>
<td>65.25</td>
<td>-0.20</td>
<td>-</td>
<td>-</td>
<td>24.00</td>
<td>-4.00</td>
<td>1.85</td>
<td>-3.57</td>
</tr>
<tr>
<td></td>
<td>w/o format explanation</td>
<td>33.00</td>
<td>-0.25</td>
<td>16.34</td>
<td>+1.13</td>
<td>57.50</td>
<td>-8.25</td>
<td>-</td>
<td>-</td>
<td>18.00</td>
<td>-10.00</td>
<td>5.19</td>
<td>+3.43</td>
</tr>
<tr>
<td></td>
<td>w/o role prompting</td>
<td>36.60</td>
<td>+1.10</td>
<td>15.70</td>
<td>+0.49</td>
<td>55.00</td>
<td>-10.45</td>
<td>-</td>
<td>-</td>
<td>20.00</td>
<td>-8.00</td>
<td>4.71</td>
<td>-0.23</td>
</tr>
<tr>
<td></td>
<td>w/o change order</td>
<td>14.00</td>
<td>-21.50</td>
<td>26.28</td>
<td>+11.07</td>
<td>51.20</td>
<td>-14.25</td>
<td>-</td>
<td>-</td>
<td>30.00</td>
<td>+2.00</td>
<td>14.92</td>
<td>-9.50</td>
</tr>
<tr>
<td></td>
<td>w/o 1-shot</td>
<td>33.00</td>
<td>-2.50</td>
<td>17.18</td>
<td>+1.97</td>
<td>71.90</td>
<td>-6.45</td>
<td>-</td>
<td>-</td>
<td>22.00</td>
<td>-6.00</td>
<td>7.85</td>
<td>+2.43</td>
</tr>
<tr>
<td>Edge List</td>
<td>1-shot</td>
<td>22.50</td>
<td>0.00</td>
<td>44.87</td>
<td>0.00</td>
<td>74.60</td>
<td>0.00</td>
<td>-</td>
<td>-</td>
<td>43.00</td>
<td>0.00</td>
<td>13.31</td>
<td>0.00</td>
</tr>
<tr>
<td></td>
<td>1-shot-cot</td>
<td>27.00</td>
<td>+4.50</td>
<td>48.65</td>
<td>+3.78</td>
<td>74.70</td>
<td>+0.10</td>
<td>-</td>
<td>-</td>
<td>41.00</td>
<td>-2.00</td>
<td>11.33</td>
<td>-1.98</td>
</tr>
<tr>
<td></td>
<td>w/o format explanation</td>
<td>25.00</td>
<td>+2.50</td>
<td>47.86</td>
<td>+2.99</td>
<td>71.55</td>
<td>-3.05</td>
<td>-</td>
<td>-</td>
<td>36.00</td>
<td>-7.00</td>
<td>18.11</td>
<td>+4.80</td>
</tr>
<tr>
<td></td>
<td>w/o role prompting</td>
<td>18.00</td>
<td>-4.50</td>
<td>47.64</td>
<td>+2.57</td>
<td>71.70</td>
<td>-2.90</td>
<td>-</td>
<td>-</td>
<td>39.00</td>
<td>-4.00</td>
<td>13.63</td>
<td>+0.35</td>
</tr>
<tr>
<td></td>
<td>w/o change order</td>
<td>9.00</td>
<td>-13.50</td>
<td>20.48</td>
<td>-23.39</td>
<td>79.60</td>
<td>+5.00</td>
<td>-</td>
<td>-</td>
<td>10.00</td>
<td>-33.00</td>
<td>20.06</td>
<td>+ 7.05</td>
</tr>
<tr>
<td></td>
<td>w/o 1-shot</td>
<td>23.00</td>
<td>+0.50</td>
<td>49.34</td>
<td>+4.47</td>
<td>80.95</td>
<td>+6.35</td>
<td>-</td>
<td>-</td>
<td>34.00</td>
<td>-9.00</td>
<td>19.16</td>
<td>+5.84</td>
</tr>
<tr>
<td>GML</td>
<td>1-shot</td>
<td>54.50</td>
<td>0.00</td>
<td>20.91</td>
<td>0.00</td>
<td>50.45</td>
<td>0.00</td>
<td>83.40</td>
<td>0.00</td>
<td>37.00</td>
<td>0.00</td>
<td>4.36</td>
<td>0.00</td>
</tr>
<tr>
<td></td>
<td>1-shot-cot</td>
<td>55.50</td>
<td>+1.00</td>
<td>20.76</td>
<td>-0.15</td>
<td>50.10</td>
<td>-0.35</td>
<td>83.30</td>
<td>-0.10</td>
<td>28.00</td>
<td>-9.00</td>
<td>0.95</td>
<td>-3.41</td>
</tr>
<tr>
<td></td>
<td>w/o format explanation</td>
<td>55.00</td>
<td>-0.50</td>
<td>29.06</td>
<td>+8.15</td>
<td>50.00</td>
<td>-0.45</td>
<td>85.97</td>
<td>+2.57</td>
<td>41.00</td>
<td>+4.00</td>
<td>12.71</td>
<td>+8.35</td>
</tr>
<tr>
<td></td>
<td>w/o role prompting</td>
<td>54.50</td>
<td>-0.50</td>
<td>29.79</td>
<td>+8.88</td>
<td>50.00</td>
<td>-0.45</td>
<td>84.50</td>
<td>+0.10</td>
<td>35.00</td>
<td>-2.00</td>
<td>6.96</td>
<td>+2.60</td>
</tr>
<tr>
<td></td>
<td>w/o change order</td>
<td>51.50</td>
<td>-3.00</td>
<td>21.16</td>
<td>+0.24</td>
<td>55.65</td>
<td>+5.20</td>
<td>83.56</td>
<td>+0.16</td>
<td>39.00</td>
<td>+2.00</td>
<td>5.25</td>
<td>+0.89</td>
</tr>
<tr>
<td></td>
<td>w/o 1-shot</td>
<td>54.00</td>
<td>-0.50</td>
<td>19.85</td>
<td>-1.06</td>
<td>50.25</td>
<td>+0.20</td>
<td>83.22</td>
<td>-0.18</td>
<td>42.00</td>
<td>+5.00</td>
<td>5.39</td>
<td>+1.03</td>
</tr>
<tr>
<td>GraphML</td>
<td>1-shot</td>
<td>25.00</td>
<td>0.00</td>
<td>40.20</td>
<td>0.00</td>
<td>62.05</td>
<td>0.00</td>
<td>83.87</td>
<td>0.00</td>
<td>34.00</td>
<td>0.00</td>
<td>9.74</td>
<td>0.00</td>
</tr>
<tr>
<td></td>
<td>1-shot-cot</td>
<td>22.50</td>
<td>-2.50</td>
<td>40.02</td>
<td>-0.18</td>
<td>62.30</td>
<td>+0.25</td>
<td>83.75</td>
<td>-0.12</td>
<td>32.00</td>
<td>-2.00</td>
<td>7.29</td>
<td>-2.45</td>
</tr>
<tr>
<td></td>
<td>w/o format explanation</td>
<td>19.00</td>
<td>-6.00</td>
<td>46.90</td>
<td>+5.88</td>
<td>53.75</td>
<td>-8.40</td>
<td>85.37</td>
<td>+1.50</td>
<td>38.00</td>
<td>+4.00</td>
<td>22.75</td>
<td>+13.01</td>
</tr>
<tr>
<td></td>
<td>w/o role prompting</td>
<td>15.50</td>
<td>-9.50</td>
<td>49.89</td>
<td>+9.87</td>
<td>56.10</td>
<td>-5.95</td>
<td>87.63</td>
<td>+3.76</td>
<td>31.00</td>
<td>-3.00</td>
<td>14.52</td>
<td>+4.78</td>
</tr>
<tr>
<td></td>
<td>w/o change order</td>
<td>8.50</td>
<td>-16.50</td>
<td>30.60</td>
<td>-9.60</td>
<td>65.35</td>
<td>+3.30</td>
<td>9.76</td>
<td>-4.11</td>
<td>43.00</td>
<td>+9.00</td>
<td>8.00</td>
<td>-1.74</td>
</tr>
<tr>
<td></td>
<td>0-shot</td>
<td>24.50</td>
<td>-0.50</td>
<td>39.59</td>
<td>-0.61</td>
<td>73.95</td>
<td>+11.90</td>
<td>82.90</td>
<td>-0.97</td>
<td>30.00</td>
<td>-4.00</td>
<td>14.32</td>
<td>+4.58</td>
</tr>
</tbody>
</table>
<p>lead to improved performance and more accurate structural understanding.</p>
<p>Role Prompting Generally Improves Performance. Our findings indicate that incorporating role-prompting techniques generally enhances the model's performance in the Structure Understanding Task. By explicitly guiding the model to focus on specific roles or relationships within the graph, we enable it to extract more meaningful insights and make more accurate predictions. Role prompting serves as an effective mechanism for capturing the nuances of the graph's structure and leveraging that information for improved understanding.</p>
<p>Examples Have Impacts on Graph Understanding. Similar to previous research that suggests the utility of examples in large language models (LLMs), we discovered that examples also have some extend of positive effects in graph understanding scenarios. However, omitting specific examples and relying on zero-shot learning approaches sometimes yielded more powerful results. This phenomenon can be attributed to the rich inherent information present within the graph itself, which allows the model to grasp the complexities of the structure without the need for explicit examples. Examples, in some cases, can introduce noise, biases, or incomplete information, hindering the model's overall understanding.</p>
<p>The Position of External Knowledge Matters.</p>
<p>We investigated the impact of external knowledge, such as questions, statements, and examples, on graph understanding. Comparing the placement of external knowledge before or after the graph input, we observed that positioning external knowledge before the graph generally led to better performance. Placing external knowledge before the graph provides additional context information, enabling the model to better comprehend the specific graph it needs to handle. Conversely, positioning the graph behind external knowledge may hinder the model's ability to effectively utilize the relevant information, potentially degrading performance.</p>
<p>These findings show the importance of thoughtful input design, the potential benefits of role prompting techniques, the limited impact of examples in graph understanding, and the significance of positioning external knowledge for optimal performance. Understanding these factors can guide future research and inform the development of more effective models for structure understanding tasks.</p>
<h3>6.3 Results for Semantic Understanding Task</h3>
<p>The resuls for semantic understanding tasks are shown in Figure 2 and Figure 3. We have the following discoveries:
Resuts for KGQA and GQL generation. The results for KGQA and GQL generation is shown in Table 2. It's noticeable that current SOTA models consistently show higher performance across</p>
<p>Table 2: Performance on KGQA and GQL Generation</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Wiki</th>
<th style="text-align: center;">MetaQA-1hop</th>
<th style="text-align: center;">MetaQA-2hop</th>
<th style="text-align: center;">MetaQA-3hop</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">SOTA</td>
<td style="text-align: center;">64.70</td>
<td style="text-align: center;">97.50</td>
<td style="text-align: center;">$\mathbf{9 8 . 8 0}$</td>
<td style="text-align: center;">94.80</td>
</tr>
<tr>
<td style="text-align: center;">zero-shot</td>
<td style="text-align: center;">9.23</td>
<td style="text-align: center;">24.75</td>
<td style="text-align: center;">6.37</td>
<td style="text-align: center;">9.72</td>
</tr>
<tr>
<td style="text-align: center;">zero-shot-cot</td>
<td style="text-align: center;">8.71</td>
<td style="text-align: center;">18.41</td>
<td style="text-align: center;">12.86</td>
<td style="text-align: center;">21.89</td>
</tr>
<tr>
<td style="text-align: center;">zero-shot+graph</td>
<td style="text-align: center;">56.38</td>
<td style="text-align: center;">91.69</td>
<td style="text-align: center;">46.82</td>
<td style="text-align: center;">19.40</td>
</tr>
<tr>
<td style="text-align: center;">zero-shot-cot+graph</td>
<td style="text-align: center;">55.63</td>
<td style="text-align: center;">86.16</td>
<td style="text-align: center;">47.36</td>
<td style="text-align: center;">19.29</td>
</tr>
<tr>
<td style="text-align: center;">zero-shot+graph+change-order</td>
<td style="text-align: center;">51.35</td>
<td style="text-align: center;">95.20</td>
<td style="text-align: center;">40.48</td>
<td style="text-align: center;">20.17</td>
</tr>
<tr>
<td style="text-align: center;">zero-shot-cot+graph+change-order</td>
<td style="text-align: center;">56.33</td>
<td style="text-align: center;">95.87</td>
<td style="text-align: center;">47.71</td>
<td style="text-align: center;">23.95</td>
</tr>
<tr>
<td style="text-align: center;">zero-shot Cypher Generation</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">30.00</td>
<td style="text-align: center;">10.00</td>
<td style="text-align: center;">13.00</td>
</tr>
<tr>
<td style="text-align: center;">one-shot Cypher Generation</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\mathbf{9 9 . 0 0}$</td>
<td style="text-align: center;">77.00</td>
<td style="text-align: center;">$\mathbf{9 6 . 0 0}$</td>
</tr>
</tbody>
</table>
<p>all datasets, with scores ranging from 94.80 on MetaQA-3hop to 98.80 on MetaQA-2hop. However, LLM showed comparative performance on certain tasks with prompt strategies. Specifically, the 'zero-shot+graph' method has performed exceptionally well on the 'Wiki' dataset, achieving an accuracy of 56.38 , the highest among our proposed models. Similarly, the 'zero-shotcot+graph+change-order' model performs the best on MetaQA-1hop, scoring 95.87. When we compare zero-shot models with 'zero-shot-cot' counterparts, we observe a general trend that the inclusion of the graph ('+graph') and change order ('+change-order') enhancements improve the model performance. For the 'one-shot Cypher' method, an impressive performance of 99.00 is achieved on the MetaQA-1hop, surpassing the state-of-the-art and all other models in our study.</p>
<p>Results for Node Classification. For Node Classification on OGBN-ARXIV (Table 3), the 'oneshot + 1-hop neighborhood context summarization' model has the highest accuracy of 60.00 among all the variants. Interestingly, models augmented with 2-hop neighborhood context summarization ('2-hop') show better performance than their 1hop counterparts, showing that expanding context range is helpful in providing valuable information. Also, the model performs better than the change-ofthought (cot) model, suggesting that the cot strategy might not be as effective for this task. These results indicate potential areas for improvement, particularly for the 'zero-shot-cot' and 'change-order' strategies, which don't consistently improve performance. Nonetheless, the experiments provide valuable insights into the performance of different strategies in the node classification task.</p>
<p>Table 3: Performance of Node Classification on OGBNARXIV. self denotes only the use of the text feature of the target nodes. 1-hop denotes using the text feature of direct neighbors. 2-hop denotes using the text feature within 2-hop neighbors.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Context</th>
<th style="text-align: center;">ACC</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">zero-shot</td>
<td style="text-align: center;">self</td>
<td style="text-align: center;">48.00</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">1-hop</td>
<td style="text-align: center;">53.00</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">2-hop</td>
<td style="text-align: center;">57.00</td>
</tr>
<tr>
<td style="text-align: center;">zero-shot-cot</td>
<td style="text-align: center;">self</td>
<td style="text-align: center;">40.00</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">1-hop</td>
<td style="text-align: center;">40.00</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">2-hop</td>
<td style="text-align: center;">56.00</td>
</tr>
<tr>
<td style="text-align: center;">one-shot</td>
<td style="text-align: center;">self</td>
<td style="text-align: center;">50.00</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">1-hop</td>
<td style="text-align: center;">54.00</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">2-hop</td>
<td style="text-align: center;">60.00</td>
</tr>
<tr>
<td style="text-align: center;">one-shot-cot</td>
<td style="text-align: center;">self</td>
<td style="text-align: center;">43.00</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">1-hop</td>
<td style="text-align: center;">55.00</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">2-hop</td>
<td style="text-align: center;">59.00</td>
</tr>
</tbody>
</table>
<p>Results for Graph Classification. The results for graph classification task is shown in Table 4. From the result, we find that the self-augmentation is effective in improving the performance of GC. It shows that self-augmentation like self-format explanation and self-summarization can enrich the context of the original graph and will make the LLM more easily complete the task.</p>
<h2>7 Discussion</h2>
<p>Our findings suggest several promising directions for future work in structure understanding tasks with LLMs. First, more research is needed to understand how different input designs and role prompting techniques can further enhance performance. Second, we encourage researchers to in-</p>
<p>Table 4: Performance on Graph Classification</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>OGBG-MOLHIV</th>
<th></th>
<th>OGBG-MOLPCBA</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>GML</td>
<td>GraphML</td>
<td>GML</td>
<td>GraphML</td>
</tr>
<tr>
<td>1-shot-tot</td>
<td>66.87</td>
<td>63.25</td>
<td>57.18</td>
<td>57.45</td>
</tr>
<tr>
<td>1-shot-cot</td>
<td>67.65</td>
<td>64.71</td>
<td>59.26</td>
<td>57.32</td>
</tr>
<tr>
<td>w/o self-format explanation</td>
<td>64.71</td>
<td>64.71</td>
<td>58.73</td>
<td>56.24</td>
</tr>
<tr>
<td>w/o self-summarization</td>
<td>61.76</td>
<td>61.77</td>
<td>57.64</td>
<td>56.67</td>
</tr>
<tr>
<td>0-shot-cot</td>
<td>58.82</td>
<td>59.76</td>
<td>55.57</td>
<td>55.32</td>
</tr>
</tbody>
</table>
<p>vestigate why examples are less effective for graph understanding and to explore alternative strategies for leveraging the rich information embedded in graphs. Third, the role of external knowledge placement merits further exploration. Finally, new approaches for graph augmentation could be developed to improve performance on semantic understanding tasks.</p>
<p>In addition, our experiments have revealed the potential of LLMs in various tasks beyond pure natural language processing. We believe that more effort should be dedicated to integrating graph-based information into LLMs, exploring different types of graph structures, and applying LLMs to other areas such as graph theory, network science, and complex systems. In the future, we may also consider using LLM to control the use of external tools to better handle graph structured data <em>Schick et al. (2023); Zhang (2023)</em>.</p>
<h2>8 Related Works</h2>
<h3>8.1 Language Model for Structural Data Understanding</h3>
<p>Language models are being extended to understand and work with structural data, such as graphs, tables, and trees. One approach is using graph neural networks to encode structural information, capturing dependencies and relationships between elements <em>Qasim et al. (2019)</em>. Incorporating GNNs into language models enables them to generate contextually aware outputs that consider the structural characteristics of the data. Another approach is incorporating attention mechanisms into language models for structural data <em>Chen et al. (2022); Eisenschlos et al. (2021)</em>. Attention allows the model to focus on relevant parts, improving understanding of complex dependencies and enhancing performance in tasks like graph completion and table understanding. Language models can also benefit from combining knowledge graph embeddings with textual information, leveraging both textual and structural data to make informed predictions.</p>
<h3>8.2 Graph Machine Learning</h3>
<p>Graph machine learning develops models and algorithms for data structured as graphs, representing complex relationships in various domains. Traditional machine learning struggles with graph-structured data, but graph machine learning methods utilize the graph structure to extract meaningful features and make predictions. Graph convolutional (GCN) networks extend convolutional neural networks to operate on graph-structured data, capturing local and global structural patterns and excelling in tasks like node classification and graph-level classification <em>Kipf and Welling (2016)</em>. Graph attention networks (GAT) incorporate attention mechanisms, allowing adaptive aggregation of information from relevant nodes <em>Velickovic et al. (2017)</em>. GAT perform well in tasks like node classification and graph-level representation learning. Graph generative models generate new graphs to capture the structural characteristics and properties of the input data, benefiting tasks like molecule generation <em>Walters and Barzilay (2020)</em> and graph-based data augmentation <em>Zhao et al. (2021)</em>. Graph machine learning techniques enable effective analysis and extraction of insights from graph-structured data, advancing fields relying on understanding complex relationships and dependencies.</p>
<h2>9 Conclusion</h2>
<p>In this work, we analyze the ability of large language models to understand graph-structured data. Our findings indicate that there is still a long way for a LLM to understand graph data. Future research should focus on developing and refining methods for encoding graph-structured information into a format that a large language model can comprehend and manipulate effectively. This is a</p>
<p>complex challenge given the inherent differences between sequential text data and graph data, which is intrinsically multi-dimensional and relational.</p>
<h2>References</h2>
<p>Marc Barthelemy. 2004. Betweenness centrality in large complex networks. The European physical journal B, 38(2):163-168.</p>
<p>Andreas D Baxevanis, Gary D Bader, and David S Wishart. 2020. Bioinformatics. John Wiley \&amp; Sons.</p>
<p>Smriti Bhagat, Graham Cormode, and S Muthukrishnan. 2011. Node classification in social networks. arXiv preprint arXiv:1101.3291.</p>
<p>Richard J Bolton and David J Hand. 2002. Statistical fraud detection: A review. Statistical science, 17(3):235-255.</p>
<p>Ulrik Brandes. 2001. A faster algorithm for betweenness centrality. Journal of mathematical sociology, 25(2):163-177.</p>
<p>Ulrik Brandes, Markus Eiglsperger, Jürgen Lerner, and Christian Pich. 2013. Graph markup language (graphml).</p>
<p>Miao Chen, Xinjiang Lu, Tong Xu, Yanyan Li, Zhou Jingbo, Dejing Dou, and Hui Xiong. 2022. Towards table-to-text generation with pretrained language model: A table structure understanding and text deliberating approach. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 8199-8210, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.</p>
<p>Fan RK Chung, Vance Faber, and Thomas A Manteuffel. 1994. An upper bound on the diameter of a graph from eigenvalues associated with its laplacian. SIAM Journal on Discrete Mathematics, 7(3):443-457.</p>
<p>Julian Martin Eisenschlos, Maharshi Gor, Thomas Müller, and William W Cohen. 2021. Mate: multiview attention for table transformer efficiency. Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing".</p>
<p>Federico Errica, Marco Podda, Davide Bacciu, and Alessio Micheli. 2019. A fair comparison of graph neural networks for graph classification. arXiv preprint arXiv:1912.09893.</p>
<p>Heng Gong, Yawei Sun, Xiaocheng Feng, Bing Qin, Wei Bi, Xiaojiang Liu, and Ting Liu. 2020. Tablegpt: Few-shot table-to-text generation with table structure reconstruction and content matching. In Proceedings of the 28th International Conference on Computational Linguistics, pages 1978-1988.</p>
<p>Michael Himsolt. 1997. Gml: Graph modelling language. University of Passau.</p>
<p>Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. 2020. Open graph benchmark: Datasets for machine learning on graphs. Advances in neural information processing systems, 33:22118-22133.</p>
<p>Xiao Huang, Jingyuan Zhang, Dingcheng Li, and Ping Li. 2019. Knowledge graph embedding based question answering. In Proceedings of the twelfth ACM international conference on web search and data mining, pages 105-113.</p>
<p>Folasade Olubusola Isinkaye, Yetunde O Folajimi, and Bolande Adefowoke Ojokoh. 2015. Recommendation systems: Principles, methods and evaluation. Egyptian informatics journal, 16(3):261-273.</p>
<p>Shaoxiong Ji, Shirui Pan, Erik Cambria, Pekka Marttinen, and S Yu Philip. 2021. A survey on knowledge graphs: Representation, acquisition, and applications. IEEE transactions on neural networks and learning systems, 33(2):494-514.</p>
<p>Jinhao Jiang, Kun Zhou, Zican Dong, Keming Ye, Wayne Xin Zhao, and Ji-Rong Wen. 2023. Structgpt: A general framework for large language model to reason over structured data. arXiv preprint arXiv:2305.09645.</p>
<p>Thomas N Kipf and Max Welling. 2016. Semisupervised classification with graph convolutional networks. In International Conference on Learning Representations.</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. In Advances in Neural Information Processing Systems.</p>
<p>Ao Liu, Haoyu Dong, Naoaki Okazaki, Shi Han, and Dongmei Zhang. 2022. PLOG: Table-to-logic pretraining for logical table-to-text generation. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 55315546, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.</p>
<p>Mark EJ Newman. 2005. A measure of betweenness centrality based on random walks. Social networks, 27(1):39-54.</p>
<p>Kazuya Okamoto, Wei Chen, and Xiang-Yang Li. 2008. Ranking of closeness centrality for large-scale social networks. Lecture Notes in Computer Science, 5059:186-195.</p>
<p>Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730-27744.</p>
<p>Shah Rukh Qasim, Hassan Mahmood, and Faisal Shafait. 2019. Rethinking table recognition using</p>
<p>graph neural networks. In 2019 International Conference on Document Analysis and Recognition (ICDAR), pages 142-147. IEEE.</p>
<p>Yu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. 2019. Dropedge: Towards deep graph convolutional networks on node classification. arXiv preprint arXiv:1907.10903.</p>
<p>Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761.</p>
<p>Yuan Sui, Mengyu Zhou, Mingjie Zhou, Shi Han, and Dongmei Zhang. 2023. Evaluating and enhancing structural understanding capabilities of large language models on tables via input designs. arXiv preprint arXiv:2305.13062.</p>
<p>Jie Tang, Jing Zhang, Limin Yao, Juanzi Li, Li Zhang, and Zhong Su. 2008. Arnetminer: extraction and mining of academic social networks. In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 990998 .</p>
<p>Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, Yoshua Bengio, et al. 2017. Graph attention networks. stat, 1050(20):1048550 .</p>
<p>W Patrick Walters and Regina Barzilay. 2020. Applications of deep learning in molecule generation and molecular property prediction. Accounts of chemical research, 54(2):263-270.</p>
<p>Stanley Wasserman and Katherine Faust. 1994. Social network analysis: Methods and applications.</p>
<p>Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. 2021. Finetuned language models are zero-shot learners. In International Conference on Learning Representations.</p>
<p>Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. Tree of thoughts: Deliberate problem solving with large language models. arXiv preprint arXiv:2305.10601.</p>
<p>Jiawei Zhang. 2023. Graph-toolformer: To empower llms with graph reasoning ability via prompt augmented by chatgpt. arXiv preprint arXiv:2304.11116.</p>
<p>Junlong Zhang and Yu Luo. 2017. Degree centrality, betweenness centrality, and closeness centrality in social network. In 2017 2nd international conference on modelling, simulation and applied mathematics (MSAM2017), pages 300-303. Atlantis press.</p>
<p>Yuyu Zhang, Hanjun Dai, Zornitsa Kozareva, Alexander Smola, and Le Song. 2018. Variational reasoning for question answering with knowledge graph. In Proceedings of the AAAI conference on artificial intelligence, volume 32.</p>
<p>Tong Zhao, Yozen Liu, Leonardo Neves, Oliver Woodford, Meng Jiang, and Neil Shah. 2021. Data augmentation for graph neural networks. In Proceedings of the aaai conference on artificial intelligence, volume 35, pages 11015-11023.</p>
<p>Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023. A survey of large language models. arXiv preprint arXiv:2303.18223.</p>
<h2>A Detailed Description of Datasets</h2>
<h2>A. 1 OGBN-ARXIV</h2>
<p>The Open Graph Benchmark (OGB) is a collection of diverse, large-scale, and challenging datasets and benchmarking tasks for graph machine learning research. OGBN-ARXIV is a part of the OGB Node Property Prediction track. The dataset comprises academic papers from the arXiv website, which are represented as nodes in a citation graph. In the graph, the edges denote the citation relationships between the papers. Each paper is associated with a 128-dimensional word2vec feature vector derived from its title and abstract. The task associated with this dataset is to predict the subject area of each paper, making it a multi-class classification problem. We sample a subset of 100 nodes with multi-hop neighbors for testing.</p>
<h2>A. 2 OGBG-MOLX</h2>
<p>OGBG-MOLX is part of the Graph Property Prediction track in OGB and it comprises of two datasets: MOLHIV and MOLPCBA. MOLHIV dataset contains molecular graphs where the task is to predict whether a molecule inhibits HIV virus replication or not, making it a binary classification problem. On the other hand, MOLPCBA dataset contains molecular graphs with the task of predicting bioactivity for various protein targets, which is a multi-label classification problem. In these datasets, nodes represent atoms and edges represent bonds between atoms. Node and edge features include atom type, atom degree, bond type, and whether the bond is in a ring. We sample 100 graphs with the same number of positive and negative samples for testing.</p>
<h2>A. 3 Wiki</h2>
<p>The Wiki dataset is a well-known dataset that contains text from a collection of Wikipedia articles. The structure of this dataset varies depending on the particular task. For instance, for text classification tasks, each document (or article) can be represented as a bag-of-words vector, with each dimension representing the frequency of a specific word. The labels may include the categories that the articles belong to. In the context of graph-based tasks, a Wikipedia dataset could consist of a network of articles (as nodes) linked by hyperlinks (as edges), with the task being predicting article categories or link prediction between articles.</p>
<h2>A. 4 MetaQA</h2>
<p>MetaQA is a dataset designed for the task of multihop reasoning in question answering. It consists of movie-related knowledge graph entities, relations, and natural language questions. Each node in the knowledge graph represents a movie entity (such as a movie, actor, or director), and edges represent relationships between entities. The dataset also includes questions at three levels of complexity (1hop, 2-hop, and 3-hop), with each level requiring reasoning over an increasing number of edges in the graph to answer the questions correctly. The goal is to answer natural language questions by effectively reasoning over the underlying knowledge graph.</p>
<h2>B Input Design for Different Tasks</h2>
<p>The input design for different tasks are shown in Table 5, where we show the question designs for different tasks.</p>
<h2>C Cypher Introduction</h2>
<p>Cypher is a declarative graph query language developed by Neo4j, a popular graph database management system. It allows for expressive and efficient querying and updating of graph data. The language is designed to be intuitive and readable, drawing on the use of English prose and iconography. Cypher is built around the concept of pattern matching. It focuses on the clarity of expressing what to retrieve from a graph, not dictating how to retrieve it. This design makes Cypher powerful when working with graph data, as patterns are often more intuitive and easier to understand.</p>
<p>Table 5: Input Design for Different Tasks.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Task</th>
<th style="text-align: left;">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Size Detection</td>
<td style="text-align: left;"><graph text> What is the number <br> of nodes and edges in this graph? <br> Please answer with the number of <br> nodes: X, number of edges: X.</td>
</tr>
<tr>
<td style="text-align: left;">Degree Detection</td>
<td style="text-align: left;"><graph text> What is the degree of <br> node X?</td>
</tr>
<tr>
<td style="text-align: left;">Edge Detection</td>
<td style="text-align: left;"><graph text> Is there an edge be- <br> tween node X1 and node X2?</td>
</tr>
<tr>
<td style="text-align: left;">Attribute Retrieval</td>
<td style="text-align: left;">What is the title of node X?</td>
</tr>
<tr>
<td style="text-align: left;">Diameter</td>
<td style="text-align: left;">What is the diameter of this graph?</td>
</tr>
<tr>
<td style="text-align: left;">Clustering</td>
<td style="text-align: left;">What is the clustering coefficient <br> of node X?</td>
</tr>
<tr>
<td style="text-align: left;">KGQA</td>
<td style="text-align: left;">Knowledge: <graph text>, Ques- <br> tion: <question text></td>
</tr>
<tr>
<td style="text-align: left;">GQL Generation</td>
<td style="text-align: left;">Thus the Neo4j CQL of the ques- <br> tion is</td>
</tr>
<tr>
<td style="text-align: left;">Node Classification</td>
<td style="text-align: left;">Which arxiv CS subcategory does <br> paper <paper title> with abstract <br> <paper abstract> belongs to? use <br> the abbreviation to answer.</td>
</tr>
<tr>
<td style="text-align: left;">Graph Classification</td>
<td style="text-align: left;"><graph text> Whether the <br> molecule inhibits HIV virus <br> replication? Yes or no.</td>
</tr>
</tbody>
</table>
<h2>D Data and Code Release</h2>
<p>The GUC benchmark and codes in this paper will be open sourced at https://anonymous.4open. science/r/GPT4Graph. after an internal review. The synthesized labels in the benchmark will be released under CDLAPermissive-2.0 license. Our code will be released publicly with MIT license.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ Work done during the internship at MSRA.
${ }^{1}$ Corresponding Author.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6025 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6025</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6025</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-121.html">extraction-schema-121</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <p><strong>Paper ID:</strong> paper-261214653</p>
                <p><strong>Paper Title:</strong> <a href="https://ojs.aaai.org/index.php/AAAI/article/download/29872/31521" target="_blank">SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research</a></p>
                <p><strong>Paper Abstract:</strong> Recently, there has been growing interest in using Large Language Models (LLMs) for scientific research. Numerous benchmarks have been proposed to evaluate the ability of LLMs for scientific research. However, current benchmarks are mostly based on pre-collected objective questions. This design suffers from data leakage problem and lacks the evaluation of subjective Q/A ability. In this paper, we propose SciEval, a comprehensive and multi-disciplinary evaluation benchmark to address these issues. Based on Bloom's taxonomy, SciEval covers four dimensions to systematically evaluate scientific research ability. In particular, we design a"dynamic"subset based on scientific principles to prevent evaluation from potential data leakage. Both objective and subjective questions are included in SciEval. These characteristics make SciEval a more effective benchmark for scientific research ability evaluation of LLMs. Comprehensive experiments on most advanced LLMs show that, although GPT-4 achieves SOTA performance compared to other LLMs, there is still substantial room for improvement, especially for dynamic questions. The codes and data are publicly available on https://github.com/OpenDFM/SciEval.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6025.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6025.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SciEval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SciEval benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-level, multi-disciplinary benchmark introduced in this paper to evaluate LLM scientific research ability across four dimensions (Basic Knowledge, Knowledge Application, Scientific Calculation, Research Ability) using Static, Dynamic, and Experimental data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Multi-level evaluation framework based on Bloom's taxonomy using three dataset types: Static Data (pre-collected objective Qs), Dynamic Data (programmatically/generated from scientific principles to avoid data leakage), and Experimental Data (open-ended experiment questions); evaluated under Answer-Only, Chain-of-Thought, and 3-shot prompting settings.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Accuracy for objective multiple-choice questions; BLEU and extract-match for string outputs; Mean Squared Error (MSE) for numeric answers; manual human assessment for open-ended experimental responses; auxiliary use of few-shot exemplars (dev set) and CoT prompting to measure reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>biology, chemistry, physics (general scientific research capability)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Not focused on a single LLM-generated theory; evaluates LLM outputs on tasks that measure the ability to produce scientific knowledge, calculations, experiment design and analysis, which are proxies for generating scientific hypotheses/theories.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Benchmark of ~18k questions: GPT-4, GPT-3.5-turbo, and Claude-v1.3 surpassed 60% average accuracy on Static Data; GPT-4 achieved the best overall performance including highest average accuracy and BLEU on Dynamic Data; many models perform poorly on Scientific Calculation and physics Dynamic Data; Experimental Data shows strength in principles/design but weakness in result analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>SciEval (Static Data, Dynamic Data, Experimental Data); integrated sources include Socratic Q&A, MedQA (USMLE subset), PubMedQA, Reagent Selection (ChemLLMBench subset).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Experimental Data responses were manually evaluated by humans; no formal head-to-head claim that LLM-generated theories equal human-generated theories, but manual scoring is used for open-ended outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Risk of data leakage was addressed with dynamic generation but remains an evaluation challenge; LLMs show systematic weaknesses in numerical calculation (especially physics), molecule-specific chemistry knowledge, and analysis of experimental results; CoT/3-shot effects vary across models and domains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6025.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6025.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bloom's taxonomy mapping</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bloom's taxonomy-based evaluation dimensions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of Bloom's cognitive taxonomy to define four evaluation dimensions (Basic Knowledge, Knowledge Application, Scientific Calculation, Research Ability) that map to different cognitive levels.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Classify dataset questions and evaluation targets according to cognitive levels from Bloom's taxonomy (Remember, Understand, Apply, Analyze, Evaluate, Create) and measure model capability across corresponding dimensions.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Dimension-specific performance (e.g., BK measured by recall accuracy; KA by applied problem solving; SC by numerical correctness and calculation metrics; RA by quality of experimental design/analysis assessed manually).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>general scientific domains (biology, chemistry, physics)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Framework for measuring the degree to which LLM outputs meet cognitive tasks relevant to forming and evaluating scientific claims (from recall to creation).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Enabled structured analysis showing LLM strengths on recall/KA relative to weaknesses on SC and RA (especially quantitative calculation and experimental-result analysis).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Applied within SciEval's Static/Dynamic/Experimental partitions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Provides an organizing principle but no direct human-vs-model scores reported per cognitive level beyond aggregated metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Mapping cognitive levels to automated metrics is imperfect; higher-level RA requires subjective, manual judgement which is resource intensive.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6025.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6025.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Static Data</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Static Data (SciEval)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large set of pre-collected objective questions (multiple-choice, fill-in-the-blank, judgment) drawn from community Q&A and public datasets, used for standard, repeatable evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Objective question-answering evaluation using multiple-choice formatting; conversion of open answers to 4-choice MCQs via GPT-4 assistance with human verification; split into dev (few-shot), validation, and test subsets.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Accuracy (percentage correct) is the primary metric for Static Data.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>biology, chemistry, physics</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Static factual and applied questions that probe knowledge and applied reasoning; not dynamically generated, hence susceptible to training-data leakage.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Only GPT-4, GPT-3.5-turbo and Claude-v1.3 exceed 60% average accuracy on Static Data; other evaluated models perform substantially worse.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Constructed mainly from Socratic Q&A plus MedQA (USMLE), PubMedQA, Reagent Selection subset.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Static Data enables direct automated comparison to ground-truth answers (no human raters required for objective items).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Static items risk data leakage from models' training data; conversion to MCQ may simplify some complex open questions; does not capture open-ended research reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6025.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6025.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dynamic Data</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dynamic Data (SciEval)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A programmatically-generated subset that produces new evaluation questions according to scientific principles to mitigate training-data leakage, covering chemistry (KA) and physics (SC).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Chemistry items generated from molecular properties and basic information; physics items generated by Python scripts using physics formulas; dataset periodically regenerated/updated and a stable snapshot maintained for fair comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>For physics multiple-choice: accuracy; for chemistry: numeric answers evaluated by MSE, string answers by BLEU and extract-match; special rule sets (e.g., if prediction contains no number, MSE set to 1e10) to handle failures.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>chemistry (Knowledge Application), physics (Scientific Calculation)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Dynamically produced applied/calculation problems that require models to compute molecular properties or apply physics formulas rather than recall memorized items.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>GPT-4 obtained the best average accuracy and BLEU on Dynamic Data; Galactica-30B performed best on counting and calculation tasks; many models show near-random accuracy on physics subset; chemistry numeric MSE high for most LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Dynamic subsets within SciEval: chemistry ~2000 items (KA), physics ~890 items (SC).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Dynamic Data is automatically computed so comparison is to ground-truth programmatic values rather than human judgement.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Dynamic generation reduces but does not entirely eliminate leakage risk; physics questions remain very challenging (models near-random), and some models misuse formulas leading to incorrect results even if they 'know' principles.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6025.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6025.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Experimental Data</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Experimental Data (SciEval)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A subjective subset of 12 university-level basic science experiments to evaluate research ability (RA) including experimental principle, design, and analysis/summarization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Open-ended questions per experiment with manual human evaluation of model-generated responses for principle, process/design, and experimental result analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Human judgement on correctness, completeness, plausibility and quality of experimental design and analysis; no automated metric due to open-ended nature.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>general experimental science across biology, chemistry, physics</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Evaluates LLM capacity to perform higher-level research tasks akin to proposing/analysing experiments and interpreting results (proxy for generating/assessing scientific hypotheses).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>GPT-series and Claude-series achieved comparatively good scores on principle and design; almost all models struggle substantially with analysis of experimental results.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>12 experimental scenarios assembled from university basic science experiment courses (Experimental Data subset of SciEval).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Responses are manually assessed by humans; no formal quantitative comparison to expert human experimenters reported.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Open-ended manual evaluation is time-consuming and subjective; models often fail at result interpretation despite producing plausible designs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6025.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6025.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompting settings</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Answer-Only, Chain-of-Thought, and 3-shot prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Three evaluation protocols used to probe LLM reasoning: Answer-Only (AO), Chain-of-Thought (CoT), and few-shot (3-shot) exemplars from the dev set.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Each model tested under AO (direct answer), CoT (model encouraged to produce intermediate reasoning steps), and 3-shot (three exemplars provided) to measure robustness and reasoning gains.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Changes in primary metrics (accuracy, BLEU, MSE) across settings used to infer reasoning capability and sample-efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>applies across all SciEval domains (biology, chemistry, physics)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Not a theory but an interrogation method to elicit improved reasoning/answers from LLMs, which can affect the quality of generated scientific hypotheses/analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>CoT improved performance mainly for GPT-series; ~half models improved under 3-shot; on Dynamic Data CoT and 3-shot significantly improved chemistry performance but had limited effect on physics (many models remained near-random); notable numbers: GPT-3.5-turbo CoT physics accuracy 47.19, GPT-4 3-shot physics accuracy 51.01.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Applied across Static, Dynamic, and Experimental Data in SciEval.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>CoT attempts to mimic human stepwise reasoning but no direct human-vs-model chain-of-thought comparison provided.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>CoT capability varies across model families; some models lack reliable CoT behavior; CoT can sometimes lead to incorrect intermediate reasoning (e.g., using wrong formulas).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6025.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6025.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Evaluation metrics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Accuracy, BLEU, MSE, extract-match, manual scoring</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Set of automated and manual metrics used in SciEval to evaluate objective and subjective outputs from LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Automated metrics: accuracy for MCQs, BLEU for string outputs, MSE for numeric outputs; extract-match to check textual spans; manual scoring for experimental open-ended answers.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Metric-specific: accuracy (proportion correct); BLEU (n-gram overlap); MSE (squared deviation for numeric answers; special penalty 1e10 if no number predicted); manual qualitative scoring for RA outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>applies across SciEval domains</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Metrics evaluate fidelity to ground-truth answers (objective), surface-level similarity (BLEU), numerical precision (MSE), and human-assessed scientific reasoning (manual).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Used to show systemic weaknesses: high MSE in chemistry numeric tasks for most LLMs; low BLEU/accuracy on dynamic chemistry strings for many models; manual scoring shows poor experimental-result analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Metrics applied to SciEval Static/Dynamic/Experimental partitions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Manual scoring introduces human judgement; automated metrics provide reproducible comparisons to ground-truth.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>BLEU and extract-match imperfect proxies for scientific correctness; MSE punishes missing numeric predictions extremely; manual scoring is subjective and labor-intensive.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6025.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e6025.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 results</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 is the top-performing model on SciEval: strongest overall performance on Static Data and best average accuracy and BLEU on Dynamic Data, but still has notable weaknesses in some calculation/physics tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Evaluated under AO, CoT, and 3-shot on Static/Dynamic/Experimental subsets using accuracy, BLEU, MSE, and manual scoring for experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Accuracy for Static and physics MCQs; BLEU/MSE for chemistry dynamic items; manual scoring for experimental data.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>biology, chemistry, physics</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Model outputs evaluated as candidate scientific answers, calculations, and experimental analyses (proxy for generated theories/hypotheses and analyses).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Static Data average accuracy ~73.93% (top of leaderboard); only GPT-4, GPT-3.5-turbo, and Claude exceeded 60% on Static Data; on Dynamic Data GPT-4 achieved best average accuracy and BLEU, but showed poor physics AO and CoT performance (AO physics 25.84%, CoT 17.98% â†“) while 3-shot physics improved to 51.01%. Experimental Data: good at principles/design but weak at analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>SciEval (Static, Dynamic, Experimental).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Experimental answers were manually evaluated by humans; no explicit claim of parity with human scientists.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Despite top scores, GPT-4 sometimes applies incorrect formulas and struggles with quantitative physics problems and experimental result analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6025.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e6025.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5 results</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5-turbo (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A strong performer on Static Data and benefited from Chain-of-Thought in certain Dynamic physics tasks, but weaker than GPT-4 overall.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Same evaluation pipeline as other LLMs: AO, CoT, 3-shot across SciEval partitions with automated metrics and manual scoring for experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Accuracy, BLEU, MSE, manual assessment.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>biology, chemistry, physics</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Model outputs treated as candidate answers and analyses across SciEval tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Static Data average accuracy ~66.97%; Dynamic chemistry AO 7.65 (improved under CoT/3-shot), physics AO 21.80 but CoT physics accuracy rose to 47.19 (indicating CoT can reveal better physics reasoning for this model). Experimental Data: competitive on principles/design but weak on analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>SciEval (Static, Dynamic, Experimental).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Experimental Data manually scored by humans; no detailed human-model comparative analysis beyond scores.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Still struggles with chemistry molecule specifics and numerical precision; benefits from CoT in physics but inconsistent.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6025.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e6025.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude results</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Claude-v1.3 (Anthropic)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Claude-v1.3 is among the top group on Static Data, achieving >60% average accuracy and reasonable performance on Experimental Data principle/design, though weaker on Dynamic numeric tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Evaluated across SciEval datasets under AO and limited other settings where API allowed; metrics as above.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Accuracy for Static/physics MCQs; BLEU/MSE for chemistry; manual evaluation for experimental items.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Claude-v1.3</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>biology, chemistry, physics</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Model outputs used to answer and analyse scientific questions as a proxy for theory generation capability.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Average Static Data accuracy >60% (one of only three models to exceed 60%); on Experimental Data performed well on principle and design but poor on experimental-result analysis; Dynamic Data chemistry/physics performance modest.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>SciEval (Static, Dynamic, Experimental).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Manual scoring used on Experimental Data; no claim of human-level theory generation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Limited API availability constrained some CoT/3-shot evaluations; struggles on numerical/scientific calculation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6025.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e6025.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Galactica results</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Galactica (Meta) 30B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Galactica-30B, trained on a large scientific corpus, shows relatively strong performance on computational/counting tasks in Dynamic Data, outperforming some larger instruction-tuned models on certain calculation-heavy items.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Evaluated on Static and Dynamic Data with automated metrics (accuracy, BLEU, MSE) but limited on Experimental Data due to context-length/availability constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Accuracy for MCQs; MSE/BLEU for dynamic chemistry; comparative performance against other LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Galactica-30B</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>primarily chemistry and physics computational tasks</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Outputs reflect scientific knowledge and computational skill stemming from a large scientific pretraining corpus, used to answer generated scientific calculation problems.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Outperformed GPT/Claude series on certain dynamic computational/counting problems; however, its average Static Data performance lags behind GPT-4 and GPT-3.5.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>SciEval Dynamic Data (chemistry and physics subsets), Static Data.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>No explicit human-model comparisons beyond metric scores.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Although strong on some computational tasks, Galactica's overall performance is lower on general Static benchmarks and lacks instruction-tuning/RLHF advantages; model availability and dataset coverage limits evaluation breadth.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Measuring massive multitask language understanding. <em>(Rating: 2)</em></li>
                <li>Holistic evaluation of language models. <em>(Rating: 1)</em></li>
                <li>C-eval: A multilevel multi-discipline chinese evaluation suite for foundation models. <em>(Rating: 1)</em></li>
                <li>MultiMedQA <em>(Rating: 2)</em></li>
                <li>ChemLLMBench <em>(Rating: 2)</em></li>
                <li>SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6025",
    "paper_id": "paper-261214653",
    "extraction_schema_id": "extraction-schema-121",
    "extracted_data": [
        {
            "name_short": "SciEval",
            "name_full": "SciEval benchmark",
            "brief_description": "A multi-level, multi-disciplinary benchmark introduced in this paper to evaluate LLM scientific research ability across four dimensions (Basic Knowledge, Knowledge Application, Scientific Calculation, Research Ability) using Static, Dynamic, and Experimental data.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Multi-level evaluation framework based on Bloom's taxonomy using three dataset types: Static Data (pre-collected objective Qs), Dynamic Data (programmatically/generated from scientific principles to avoid data leakage), and Experimental Data (open-ended experiment questions); evaluated under Answer-Only, Chain-of-Thought, and 3-shot prompting settings.",
            "evaluation_criteria": "Accuracy for objective multiple-choice questions; BLEU and extract-match for string outputs; Mean Squared Error (MSE) for numeric answers; manual human assessment for open-ended experimental responses; auxiliary use of few-shot exemplars (dev set) and CoT prompting to measure reasoning.",
            "llm_model_name": null,
            "theory_domain": "biology, chemistry, physics (general scientific research capability)",
            "theory_description": "Not focused on a single LLM-generated theory; evaluates LLM outputs on tasks that measure the ability to produce scientific knowledge, calculations, experiment design and analysis, which are proxies for generating scientific hypotheses/theories.",
            "evaluation_results": "Benchmark of ~18k questions: GPT-4, GPT-3.5-turbo, and Claude-v1.3 surpassed 60% average accuracy on Static Data; GPT-4 achieved the best overall performance including highest average accuracy and BLEU on Dynamic Data; many models perform poorly on Scientific Calculation and physics Dynamic Data; Experimental Data shows strength in principles/design but weakness in result analysis.",
            "benchmarks_or_datasets": "SciEval (Static Data, Dynamic Data, Experimental Data); integrated sources include Socratic Q&A, MedQA (USMLE subset), PubMedQA, Reagent Selection (ChemLLMBench subset).",
            "comparison_to_human": "Experimental Data responses were manually evaluated by humans; no formal head-to-head claim that LLM-generated theories equal human-generated theories, but manual scoring is used for open-ended outputs.",
            "limitations_or_challenges": "Risk of data leakage was addressed with dynamic generation but remains an evaluation challenge; LLMs show systematic weaknesses in numerical calculation (especially physics), molecule-specific chemistry knowledge, and analysis of experimental results; CoT/3-shot effects vary across models and domains.",
            "uuid": "e6025.0",
            "source_info": {
                "paper_title": "SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Bloom's taxonomy mapping",
            "name_full": "Bloom's taxonomy-based evaluation dimensions",
            "brief_description": "Use of Bloom's cognitive taxonomy to define four evaluation dimensions (Basic Knowledge, Knowledge Application, Scientific Calculation, Research Ability) that map to different cognitive levels.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_method": "Classify dataset questions and evaluation targets according to cognitive levels from Bloom's taxonomy (Remember, Understand, Apply, Analyze, Evaluate, Create) and measure model capability across corresponding dimensions.",
            "evaluation_criteria": "Dimension-specific performance (e.g., BK measured by recall accuracy; KA by applied problem solving; SC by numerical correctness and calculation metrics; RA by quality of experimental design/analysis assessed manually).",
            "llm_model_name": null,
            "theory_domain": "general scientific domains (biology, chemistry, physics)",
            "theory_description": "Framework for measuring the degree to which LLM outputs meet cognitive tasks relevant to forming and evaluating scientific claims (from recall to creation).",
            "evaluation_results": "Enabled structured analysis showing LLM strengths on recall/KA relative to weaknesses on SC and RA (especially quantitative calculation and experimental-result analysis).",
            "benchmarks_or_datasets": "Applied within SciEval's Static/Dynamic/Experimental partitions.",
            "comparison_to_human": "Provides an organizing principle but no direct human-vs-model scores reported per cognitive level beyond aggregated metrics.",
            "limitations_or_challenges": "Mapping cognitive levels to automated metrics is imperfect; higher-level RA requires subjective, manual judgement which is resource intensive.",
            "uuid": "e6025.1",
            "source_info": {
                "paper_title": "SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Static Data",
            "name_full": "Static Data (SciEval)",
            "brief_description": "A large set of pre-collected objective questions (multiple-choice, fill-in-the-blank, judgment) drawn from community Q&A and public datasets, used for standard, repeatable evaluations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Objective question-answering evaluation using multiple-choice formatting; conversion of open answers to 4-choice MCQs via GPT-4 assistance with human verification; split into dev (few-shot), validation, and test subsets.",
            "evaluation_criteria": "Accuracy (percentage correct) is the primary metric for Static Data.",
            "llm_model_name": null,
            "theory_domain": "biology, chemistry, physics",
            "theory_description": "Static factual and applied questions that probe knowledge and applied reasoning; not dynamically generated, hence susceptible to training-data leakage.",
            "evaluation_results": "Only GPT-4, GPT-3.5-turbo and Claude-v1.3 exceed 60% average accuracy on Static Data; other evaluated models perform substantially worse.",
            "benchmarks_or_datasets": "Constructed mainly from Socratic Q&A plus MedQA (USMLE), PubMedQA, Reagent Selection subset.",
            "comparison_to_human": "Static Data enables direct automated comparison to ground-truth answers (no human raters required for objective items).",
            "limitations_or_challenges": "Static items risk data leakage from models' training data; conversion to MCQ may simplify some complex open questions; does not capture open-ended research reasoning.",
            "uuid": "e6025.2",
            "source_info": {
                "paper_title": "SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Dynamic Data",
            "name_full": "Dynamic Data (SciEval)",
            "brief_description": "A programmatically-generated subset that produces new evaluation questions according to scientific principles to mitigate training-data leakage, covering chemistry (KA) and physics (SC).",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Chemistry items generated from molecular properties and basic information; physics items generated by Python scripts using physics formulas; dataset periodically regenerated/updated and a stable snapshot maintained for fair comparisons.",
            "evaluation_criteria": "For physics multiple-choice: accuracy; for chemistry: numeric answers evaluated by MSE, string answers by BLEU and extract-match; special rule sets (e.g., if prediction contains no number, MSE set to 1e10) to handle failures.",
            "llm_model_name": null,
            "theory_domain": "chemistry (Knowledge Application), physics (Scientific Calculation)",
            "theory_description": "Dynamically produced applied/calculation problems that require models to compute molecular properties or apply physics formulas rather than recall memorized items.",
            "evaluation_results": "GPT-4 obtained the best average accuracy and BLEU on Dynamic Data; Galactica-30B performed best on counting and calculation tasks; many models show near-random accuracy on physics subset; chemistry numeric MSE high for most LLMs.",
            "benchmarks_or_datasets": "Dynamic subsets within SciEval: chemistry ~2000 items (KA), physics ~890 items (SC).",
            "comparison_to_human": "Dynamic Data is automatically computed so comparison is to ground-truth programmatic values rather than human judgement.",
            "limitations_or_challenges": "Dynamic generation reduces but does not entirely eliminate leakage risk; physics questions remain very challenging (models near-random), and some models misuse formulas leading to incorrect results even if they 'know' principles.",
            "uuid": "e6025.3",
            "source_info": {
                "paper_title": "SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Experimental Data",
            "name_full": "Experimental Data (SciEval)",
            "brief_description": "A subjective subset of 12 university-level basic science experiments to evaluate research ability (RA) including experimental principle, design, and analysis/summarization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Open-ended questions per experiment with manual human evaluation of model-generated responses for principle, process/design, and experimental result analysis.",
            "evaluation_criteria": "Human judgement on correctness, completeness, plausibility and quality of experimental design and analysis; no automated metric due to open-ended nature.",
            "llm_model_name": null,
            "theory_domain": "general experimental science across biology, chemistry, physics",
            "theory_description": "Evaluates LLM capacity to perform higher-level research tasks akin to proposing/analysing experiments and interpreting results (proxy for generating/assessing scientific hypotheses).",
            "evaluation_results": "GPT-series and Claude-series achieved comparatively good scores on principle and design; almost all models struggle substantially with analysis of experimental results.",
            "benchmarks_or_datasets": "12 experimental scenarios assembled from university basic science experiment courses (Experimental Data subset of SciEval).",
            "comparison_to_human": "Responses are manually assessed by humans; no formal quantitative comparison to expert human experimenters reported.",
            "limitations_or_challenges": "Open-ended manual evaluation is time-consuming and subjective; models often fail at result interpretation despite producing plausible designs.",
            "uuid": "e6025.4",
            "source_info": {
                "paper_title": "SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Prompting settings",
            "name_full": "Answer-Only, Chain-of-Thought, and 3-shot prompting",
            "brief_description": "Three evaluation protocols used to probe LLM reasoning: Answer-Only (AO), Chain-of-Thought (CoT), and few-shot (3-shot) exemplars from the dev set.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Each model tested under AO (direct answer), CoT (model encouraged to produce intermediate reasoning steps), and 3-shot (three exemplars provided) to measure robustness and reasoning gains.",
            "evaluation_criteria": "Changes in primary metrics (accuracy, BLEU, MSE) across settings used to infer reasoning capability and sample-efficiency.",
            "llm_model_name": null,
            "theory_domain": "applies across all SciEval domains (biology, chemistry, physics)",
            "theory_description": "Not a theory but an interrogation method to elicit improved reasoning/answers from LLMs, which can affect the quality of generated scientific hypotheses/analyses.",
            "evaluation_results": "CoT improved performance mainly for GPT-series; ~half models improved under 3-shot; on Dynamic Data CoT and 3-shot significantly improved chemistry performance but had limited effect on physics (many models remained near-random); notable numbers: GPT-3.5-turbo CoT physics accuracy 47.19, GPT-4 3-shot physics accuracy 51.01.",
            "benchmarks_or_datasets": "Applied across Static, Dynamic, and Experimental Data in SciEval.",
            "comparison_to_human": "CoT attempts to mimic human stepwise reasoning but no direct human-vs-model chain-of-thought comparison provided.",
            "limitations_or_challenges": "CoT capability varies across model families; some models lack reliable CoT behavior; CoT can sometimes lead to incorrect intermediate reasoning (e.g., using wrong formulas).",
            "uuid": "e6025.5",
            "source_info": {
                "paper_title": "SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Evaluation metrics",
            "name_full": "Accuracy, BLEU, MSE, extract-match, manual scoring",
            "brief_description": "Set of automated and manual metrics used in SciEval to evaluate objective and subjective outputs from LLMs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Automated metrics: accuracy for MCQs, BLEU for string outputs, MSE for numeric outputs; extract-match to check textual spans; manual scoring for experimental open-ended answers.",
            "evaluation_criteria": "Metric-specific: accuracy (proportion correct); BLEU (n-gram overlap); MSE (squared deviation for numeric answers; special penalty 1e10 if no number predicted); manual qualitative scoring for RA outputs.",
            "llm_model_name": null,
            "theory_domain": "applies across SciEval domains",
            "theory_description": "Metrics evaluate fidelity to ground-truth answers (objective), surface-level similarity (BLEU), numerical precision (MSE), and human-assessed scientific reasoning (manual).",
            "evaluation_results": "Used to show systemic weaknesses: high MSE in chemistry numeric tasks for most LLMs; low BLEU/accuracy on dynamic chemistry strings for many models; manual scoring shows poor experimental-result analysis.",
            "benchmarks_or_datasets": "Metrics applied to SciEval Static/Dynamic/Experimental partitions.",
            "comparison_to_human": "Manual scoring introduces human judgement; automated metrics provide reproducible comparisons to ground-truth.",
            "limitations_or_challenges": "BLEU and extract-match imperfect proxies for scientific correctness; MSE punishes missing numeric predictions extremely; manual scoring is subjective and labor-intensive.",
            "uuid": "e6025.6",
            "source_info": {
                "paper_title": "SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "GPT-4 results",
            "name_full": "GPT-4 (OpenAI)",
            "brief_description": "GPT-4 is the top-performing model on SciEval: strongest overall performance on Static Data and best average accuracy and BLEU on Dynamic Data, but still has notable weaknesses in some calculation/physics tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_method": "Evaluated under AO, CoT, and 3-shot on Static/Dynamic/Experimental subsets using accuracy, BLEU, MSE, and manual scoring for experiments.",
            "evaluation_criteria": "Accuracy for Static and physics MCQs; BLEU/MSE for chemistry dynamic items; manual scoring for experimental data.",
            "llm_model_name": "GPT-4",
            "theory_domain": "biology, chemistry, physics",
            "theory_description": "Model outputs evaluated as candidate scientific answers, calculations, and experimental analyses (proxy for generated theories/hypotheses and analyses).",
            "evaluation_results": "Static Data average accuracy ~73.93% (top of leaderboard); only GPT-4, GPT-3.5-turbo, and Claude exceeded 60% on Static Data; on Dynamic Data GPT-4 achieved best average accuracy and BLEU, but showed poor physics AO and CoT performance (AO physics 25.84%, CoT 17.98% â†“) while 3-shot physics improved to 51.01%. Experimental Data: good at principles/design but weak at analysis.",
            "benchmarks_or_datasets": "SciEval (Static, Dynamic, Experimental).",
            "comparison_to_human": "Experimental answers were manually evaluated by humans; no explicit claim of parity with human scientists.",
            "limitations_or_challenges": "Despite top scores, GPT-4 sometimes applies incorrect formulas and struggles with quantitative physics problems and experimental result analysis.",
            "uuid": "e6025.7",
            "source_info": {
                "paper_title": "SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "GPT-3.5 results",
            "name_full": "GPT-3.5-turbo (OpenAI)",
            "brief_description": "A strong performer on Static Data and benefited from Chain-of-Thought in certain Dynamic physics tasks, but weaker than GPT-4 overall.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_method": "Same evaluation pipeline as other LLMs: AO, CoT, 3-shot across SciEval partitions with automated metrics and manual scoring for experiments.",
            "evaluation_criteria": "Accuracy, BLEU, MSE, manual assessment.",
            "llm_model_name": "GPT-3.5-turbo",
            "theory_domain": "biology, chemistry, physics",
            "theory_description": "Model outputs treated as candidate answers and analyses across SciEval tasks.",
            "evaluation_results": "Static Data average accuracy ~66.97%; Dynamic chemistry AO 7.65 (improved under CoT/3-shot), physics AO 21.80 but CoT physics accuracy rose to 47.19 (indicating CoT can reveal better physics reasoning for this model). Experimental Data: competitive on principles/design but weak on analysis.",
            "benchmarks_or_datasets": "SciEval (Static, Dynamic, Experimental).",
            "comparison_to_human": "Experimental Data manually scored by humans; no detailed human-model comparative analysis beyond scores.",
            "limitations_or_challenges": "Still struggles with chemistry molecule specifics and numerical precision; benefits from CoT in physics but inconsistent.",
            "uuid": "e6025.8",
            "source_info": {
                "paper_title": "SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Claude results",
            "name_full": "Claude-v1.3 (Anthropic)",
            "brief_description": "Claude-v1.3 is among the top group on Static Data, achieving &gt;60% average accuracy and reasonable performance on Experimental Data principle/design, though weaker on Dynamic numeric tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_method": "Evaluated across SciEval datasets under AO and limited other settings where API allowed; metrics as above.",
            "evaluation_criteria": "Accuracy for Static/physics MCQs; BLEU/MSE for chemistry; manual evaluation for experimental items.",
            "llm_model_name": "Claude-v1.3",
            "theory_domain": "biology, chemistry, physics",
            "theory_description": "Model outputs used to answer and analyse scientific questions as a proxy for theory generation capability.",
            "evaluation_results": "Average Static Data accuracy &gt;60% (one of only three models to exceed 60%); on Experimental Data performed well on principle and design but poor on experimental-result analysis; Dynamic Data chemistry/physics performance modest.",
            "benchmarks_or_datasets": "SciEval (Static, Dynamic, Experimental).",
            "comparison_to_human": "Manual scoring used on Experimental Data; no claim of human-level theory generation.",
            "limitations_or_challenges": "Limited API availability constrained some CoT/3-shot evaluations; struggles on numerical/scientific calculation tasks.",
            "uuid": "e6025.9",
            "source_info": {
                "paper_title": "SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Galactica results",
            "name_full": "Galactica (Meta) 30B",
            "brief_description": "Galactica-30B, trained on a large scientific corpus, shows relatively strong performance on computational/counting tasks in Dynamic Data, outperforming some larger instruction-tuned models on certain calculation-heavy items.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_method": "Evaluated on Static and Dynamic Data with automated metrics (accuracy, BLEU, MSE) but limited on Experimental Data due to context-length/availability constraints.",
            "evaluation_criteria": "Accuracy for MCQs; MSE/BLEU for dynamic chemistry; comparative performance against other LLMs.",
            "llm_model_name": "Galactica-30B",
            "theory_domain": "primarily chemistry and physics computational tasks",
            "theory_description": "Outputs reflect scientific knowledge and computational skill stemming from a large scientific pretraining corpus, used to answer generated scientific calculation problems.",
            "evaluation_results": "Outperformed GPT/Claude series on certain dynamic computational/counting problems; however, its average Static Data performance lags behind GPT-4 and GPT-3.5.",
            "benchmarks_or_datasets": "SciEval Dynamic Data (chemistry and physics subsets), Static Data.",
            "comparison_to_human": "No explicit human-model comparisons beyond metric scores.",
            "limitations_or_challenges": "Although strong on some computational tasks, Galactica's overall performance is lower on general Static benchmarks and lacks instruction-tuning/RLHF advantages; model availability and dataset coverage limits evaluation breadth.",
            "uuid": "e6025.10",
            "source_info": {
                "paper_title": "SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research",
                "publication_date_yy_mm": "2023-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Measuring massive multitask language understanding.",
            "rating": 2,
            "sanitized_title": "measuring_massive_multitask_language_understanding"
        },
        {
            "paper_title": "Holistic evaluation of language models.",
            "rating": 1,
            "sanitized_title": "holistic_evaluation_of_language_models"
        },
        {
            "paper_title": "C-eval: A multilevel multi-discipline chinese evaluation suite for foundation models.",
            "rating": 1,
            "sanitized_title": "ceval_a_multilevel_multidiscipline_chinese_evaluation_suite_for_foundation_models"
        },
        {
            "paper_title": "MultiMedQA",
            "rating": 2,
            "sanitized_title": "multimedqa"
        },
        {
            "paper_title": "ChemLLMBench",
            "rating": 2,
            "sanitized_title": "chemllmbench"
        },
        {
            "paper_title": "SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models.",
            "rating": 2,
            "sanitized_title": "scibench_evaluating_collegelevel_scientific_problemsolving_abilities_of_large_language_models"
        }
    ],
    "cost": 0.01718275,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research</p>
<p>Liangtai Sun 
Department of Computer Science and Engineering MoE Key Lab of Artificial Intelligence
LANCE Lab
AI Institute
Shanghai Jiao Tong University Shanghai Jiao Tong University
ShanghaiChina</p>
<p>Yang Han csyanghan@sjtu.edu.cn 
Department of Computer Science and Engineering MoE Key Lab of Artificial Intelligence
LANCE Lab
AI Institute
Shanghai Jiao Tong University Shanghai Jiao Tong University
ShanghaiChina</p>
<p>Zihan Zhao 
Department of Computer Science and Engineering MoE Key Lab of Artificial Intelligence
LANCE Lab
AI Institute
Shanghai Jiao Tong University Shanghai Jiao Tong University
ShanghaiChina</p>
<p>Da Ma 
Department of Computer Science and Engineering MoE Key Lab of Artificial Intelligence
LANCE Lab
AI Institute
Shanghai Jiao Tong University Shanghai Jiao Tong University
ShanghaiChina</p>
<p>Zhennan Shen 
Department of Computer Science and Engineering MoE Key Lab of Artificial Intelligence
LANCE Lab
AI Institute
Shanghai Jiao Tong University Shanghai Jiao Tong University
ShanghaiChina</p>
<p>Baocai Chen 
Department of Computer Science and Engineering MoE Key Lab of Artificial Intelligence
LANCE Lab
AI Institute
Shanghai Jiao Tong University Shanghai Jiao Tong University
ShanghaiChina</p>
<p>Lu Chen chenlusz@sjtu.edu.cn 
Department of Computer Science and Engineering MoE Key Lab of Artificial Intelligence
LANCE Lab
AI Institute
Shanghai Jiao Tong University Shanghai Jiao Tong University
ShanghaiChina</p>
<p>Department of Computer Science and Engineering MoE Key Lab of Artificial Intelligence
LANCE Lab
AI Institute
Shanghai Jiao Tong University Shanghai Jiao Tong University
ShanghaiChina</p>
<p>Kai Yu kai.yu@sjtu.edu.cn 
Department of Computer Science and Engineering MoE Key Lab of Artificial Intelligence
LANCE Lab
AI Institute
Shanghai Jiao Tong University Shanghai Jiao Tong University
ShanghaiChina</p>
<p>Department of Computer Science and Engineering MoE Key Lab of Artificial Intelligence
LANCE Lab
AI Institute
Shanghai Jiao Tong University Shanghai Jiao Tong University
ShanghaiChina</p>
<p>SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research
8DE5F8747D39882235665D42A59CA88C
Recently, there has been growing interest in using Large Language Models (LLMs) for scientific research.Numerous benchmarks have been proposed to evaluate the ability of LLMs for scientific research.However, current benchmarks are mostly based on pre-collected objective questions.This design suffers from data leakage problem and lacks the evaluation of subjective Q/A ability.In this paper, we propose SciEval, a comprehensive and multi-disciplinary evaluation benchmark to address these issues.Based on Bloom's taxonomy, SciEval covers four dimensions to systematically evaluate scientific research ability.In particular, we design a "dynamic" subset based on scientific principles to prevent evaluation from potential data leakage.Both objective and subjective questions are included in SciEval.These characteristics make SciEval a more effective benchmark for scientific research ability evaluation of LLMs.Comprehensive experiments on most advanced LLMs show that, although GPT-4 achieves SOTA performance compared to other LLMs, there is still substantial room for improvement, especially for dynamic questions.The codes and data are publicly available on https://github.com/OpenDFM/SciEval.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs), such as ChatGPT (Schulman et al. 2022), have attracted widespread attention in general scenarios, including information search, code generation, and more.In the field of science, LLMs have also shown preliminary potential in improving scientific research efficiency and transforming scientific research paradigms (Blanco-Gonzalez et al. 2023;WANG and MIAO 2023).In the meanwhile, several scientific LLMs have been proposed by researchers (Taylor et al. 2022;Luo et al. 2022;Frey et al. 2022).In the general field, there are already numerous evaluation benchmarks to evaluate the language understanding, language generation and reasoning capabilities of LLMs, such as MMLU (Hendrycks et al. 2020), AGIEval (Zhong et al. 2023), and C-EVAL (Huang et al. 2023), shown in Table 1.Although these benchmarks cover data of science domain, the data sources are usually confined to educational materials, which can not adequately as-sess the research ability of LLMs and not align with real-life scientific research scenarios.In addition, some benchmarks have been proposed to evaluate the scientific capability of LLMs, such as MultiMedQA (Singhal et al. 2023), Chem-LLMBench (Guo et al. 2023), and MATH (Hendrycks et al. 2021), while these benchmarks are restricted to a specific scientific discipline, leaving a lack of a more general scientific evaluation benchmark. 1In addition, these benchmarks (1) lack evaluation systems for scientific capabilities, (2) are all based on objective questions, which are insufficient to assess scientific abilities, and (3) face the risk of data leakage.</p>
<p>In response to this gap, we present SciEval, an English benchmark designed to evaluate advanced abilities of LLMs in the scientific domain.SciEval consists of a total of about 18000 challenging scientific questions, spanning three important basic science fields: chemistry, physics and biology, each of which is further divided into multiple sub-topics.SciEval mainly has the following three characteristics:</p>
<p>â€¢ Multi-level and comprehensive evaluation of the ability of LLMs in the scientific field.Scientific ability of LLMs needs to be evaluated from multiple aspects.Leveraging cognitive domains of Bloom's taxonomy (Krathwohl 2002;Forehand 2010), which covers six levels, SciEval evaluates the scientific capabilities of large language models across four dimensions: basic knowledge, knowledge application, scientific calculation, and research ability, where each capability aligns with one or more cognitive levels.â€¢ Combination of objective and subjective questions.</p>
<p>SciEval is mainly based on objective questions, which allow for quick and standard model evaluations, involving multiple-choice, fill-in-the-blank, and judgment questions.These questions can help us understand whether the model can correctly understand and memorize scientific knowledge.However, objective questions are insufficient to assess scientific capability holistically.To better assess scientific reasoning and application ability, SciEval introduces a small number of subjective questions, involving a total of twelve basic science experiments, which is named Experimental Data.We conduct experiments to evaluate LLMs on SciEval in answer-only, chain-of-thought and few-shot settings.Results indicate that GPT-4 is the strongest model, with only GPT-4, GPT-3.5-turbo and Claude-v1.3surpassing 60% average accuracy on Static Data, signifying considerable opportunities for improvement.With the results of Dynamic Data, we find that these LLMs have little knowledge about molecules, and most models could only retain near-random accuracy in the physics subset.As for Experimental Data, some top-tier models could perform satisfactorily in experimental principle and design, while almost all models struggle to analyze the experimental results.With the analysis of experiment results, we claim that training on large-scale scientific corpus is helpful for the scientific ability of LLMs, and most LLMs perform bad on calculation problems, especially in physics domain.We hope SciEval can provide an excellent benchmark for the assessment of scientific capability of LLMs, and promote wide application in science.</p>
<p>Related Work</p>
<p>General Benchmarks for LLMs</p>
<p>To evaluate the performance of LLMs across different tasks, several benchmarks have been proposed.MMLU (Hendrycks et al. 2020) aims to develop a comprehensive test for evaluating text models in multi-task contexts.HELM (Liang et al. 2022) offers a comprehensive assessment, evaluating LLMs across various aspects, such as language understanding and common-sense reasoning.Big-Bench (Srivastava et al. 2022) introduces 204 challenging tasks covering various domains, aiming to evaluate tasks beyond the capabilities of existing language models.AGIEval (Zhong et al. 2023) serves as an evaluation framework for assessing the performance of foundation models in human-centric standardized exams.C-Eval (Huang et al. 2023) assesses the advanced knowledge and reasoning capabilities of foundation models in Chinese.</p>
<p>Specific Benchmarks for LLMs</p>
<p>Apart from general tasks, specific benchmarks are designed for certain downstream tasks.MultiMedQA (Singhal et al. 2023) focuses on medical question-answering, evaluating LLMs in terms of clinical knowledge and QA abilities.MATH (Hendrycks et al. 2021) assesses reasoning and problem-solving proficiencies of LLMs in mathematics.Sci-enceQA (Lu et al. 2022) proposes a multi-modal benchmark with a diverse set of science topics and annotations of their answers with corresponding lectures and explanations, collected from elementary and high school science curricula.SCIBENCH (Wang et al. 2023) examines the reasoning capabilities required for complex scientific problem-solving and proposes two datasets of college-level scientific problems.Compared to these benchmarks, SciEval (1) evaluates scientific capabilities from multiple aspects, having a broader coverage, (2) uses data of community Q&amp;A, which is more flexible and diverse, (3) designs a subset of dynamic data, making an effort to mitigate data leakage.</p>
<p>3 The SciEval Dataset cognitive domain is frequently used to structure curriculum learning objectives, assessments and activities, and is broken into six levels: Remember, Understand, Apply, Analyze, Evaluate and Create, as is shown in Figure 1, which are suitable for the evaluation of scientific capability.</p>
<p>Based on the cognitive domain of Bloom's taxonomy, the evaluation system of SciEval consists of four knowledge dimensions: Basic Knowledge (BK), Knowledge Application (KA), Scientific Calculation (SC), and Research Ability (RA).As is shown in Figure 1, BK primarily assesses the fundamental scientific knowledge of LLMs.KA focuses on how to apply basic knowledge to solve scientific problems, requiring models to have comprehension, application, and analysis abilities.SC is a specialized application of knowledge that further examines complex reasoning capabilities of LLMs based on their general knowledge application abilities.RA assesses evaluation capabilities at a higher cognitive level, requiring models to participate in various aspects of scientific research, including problem formulation, experimental design, data analysis, and summarization.</p>
<p>Based on the evaluation system, we design three different types of data: Static Data, Dynamic Data, and Experimental Data.The Static Data covers all these four knowledge dimensions and will remain constant throughout, while the Dynamic Data examines from the aspects of Knowledge Application and Scientific Calculation and will be regularly updated to prevent any data leakage.The Experimental Data comprises a set of questions for twelve scientific experiments and can be used to evaluate the Research Ability.</p>
<p>Data Collection</p>
<p>Static Data The collection steps of Static Data are shown in Figure 2. The primary source of Static Data is Socratic Q&amp;A 2 , a community-driven website that covers a wide range of subjects such as science and literature.Specifically, we collect data from the fields of biology, chemistry, and physics.To ensure quality, we employ rule-based methods 2 https://socratic.org to preprocess the crawled data.While gathering the questions, we found that not all of them are suitable as titles.To address this, we utilize GPT-4 with the "Task 1" prompt, as depicted in Figure 2, to process these questions.Since most of the collected questions are open-ended and challenging to evaluate, we employ GPT-4 to simplify ground-truth answers and generate three wrong answers to formulate them as multiple-choice questions.Additionally, we classify the questions into their respective knowledge domains.And during this process, we manually check the generated content of GPT-4 to ensure data quality.</p>
<p>To make the dataset more diverse and comprehensive, we further integrate data from some publicly available datasets:</p>
<p>â€¢ MedQA (Jin et al. 2021) is a free-form multiple-choice OpenQA dataset for solving medical problems, collected from professional medical board exams.We use the test set of USMLE, which is the English subset of MedQA.</p>
<p>â€¢ PubMedQA (Jin et al. 2019) is a biomedical questionanswering dataset collected from PubMed abstracts.The task of PubMedQA is to answer research questions with yes/no/maybe using the corresponding abstracts, which is fit for evaluating the literature comprehension ability.We incorporate 1000 expert-annotated data from it and frame them as judgment questions.</p>
<p>â€¢ Reagent Selection (Guo et al. 2023) involves the identification and proposal of the most fitting reagents for a specific chemical reaction or process, which is a subset of ChemLLMBench.We randomly select 40% data and formulate them as multiple-choice questions.</p>
<p>Dynamic Data</p>
<p>The current training of LLMs often uses a large amount of data, resulting in a risk of data leakage for evaluation.In order to solve this problem, we design a "dynamic" subset, which can generate data dynamically according to scientific principles.The dynamic subset covers two disciplines, chemistry and physics.For chemistry data, we use the basic information and properties of molecules</p>
<p>The Thirty-Eighth AAAI Conference on Artificial Intelligence (AAAI-24)</p>
<p>Socratic Q&amp;A</p>
<p>Crawl &amp; Filter</p>
<p>Raw Data</p>
<p>GPT-4 Filtered Data</p>
<p>Instruction: Given a question and its ground-truth answer, judge whether it is suitable to be used as the title of a multiple-choice question.Your answer should be "YES" or "NO".And please directly give the results without any explanation.</p>
<p>Task 1</p>
<p>Instruction: Given a question and a ground-truth answer, please simplify the answer as concise as possible.And I want to generate a 4-choice question using it, please generate 3 fake answers for me.Note that the length of the simplified answer and these 3 fake answers should be about the same and these 3 fake answers should be as confusing as possible.Furthermore, please help me to classify the domain of the question.There are three domains in total: Base Knowledge, Scientific Calculation, Knowledge Application.For physics data, we manually write some Python scripts according to the physics formulas.When obtaining the evaluation dataset, we will provide a regenerated version to users and we will update it regularly, while at the same time, we will maintain a stable version of the dynamic data to make a fair comparison.</p>
<p>Experimental Data To better evaluate the scientific thoughts and abilities of LLMs, SciEval introduces a subset of experimental data, involving 12 different basic scientific experiments.These experiments are collected from basic science experiment courses at university, and each experiment conducts a comprehensive investigation of the ability of LLMs in scientific research and experimentation from the perspectives of experimental principle, process, and analysis and summarization of experimental results.</p>
<p>Data Statistics</p>
<p>Summarized statistics are shown in For Static Data, we further split the data into dev, valid, and test set.For each data source, each knowledge domain, and each discipline, we randomly select 5 data to form the 3 https://pubchem.ncbi.nlm.nih.gov/dev set, which can be used for few-shot learning, and we split the remaining data with a ratio of 1:9 to construct the valid set and test set respectively.</p>
<p>Experiment</p>
<p>Given a question and four options, please select the right answer.Your answer should be "A", "B", "C" or "D".Please directly give the answer without any explanation.The red text is the response from the model, while the black text is the inputted prompt.</p>
<p>Given a question and four options, please select the right answer.Your answer should be "A", "B", "C" or "D".How many atoms are in 3.5 moles of arsenic atoms?</p>
<p>Experiment Setup</p>
<p>Prompts We evaluate LLMs in both Answer-Only (AO) and Chain-Of-Thought (CoT) (Kojima et al. 2022) settings.The prompts we used are shown in Figures 3 and 4 respectively.Furthermore, we also evaluate using 3-shot setting, where the three exemplars are selected from the dev set.Models In order to comprehensively assess the scientific capabilities of Large Language Models (LLMs), we evaluate 15 high-performing LLMs that are widely accessible.These models are selected to represent a diverse range of organizations and vary in size.The details of these models are summarized in Table 3.</p>
<p>Model</p>
<p>â€¢ GPT-3.5-turbo and GPT-4 (Schulman et al. 2022;Ope-nAI 2023) are the strongest GPT model variants from OpenAI that have undergone pretraining, instruction tuning, and reinforcement learning from human feedback (RLHF, (Ouyang et al. 2022)).â€¢ Claude 4 , developed by Anthropic, is often considered 4 https://www.anthropic.com/index/introducing-claude.comparable to GPT-3.5-turbo.We evaluate both the Claude-v1.We evaluate GPT-3.5-turbo,GPT4 and Claude on all three subsets, including Static Data, Dynamic Data, and Experimental Data.Since we can only assess ERNIE Bot and SparkDesk through web interface, we evaluate these two models on the Experimental Data.And for the rest LLMs with billions or tens of billions of parameters, since the length of the Experimental Data exceeds the length limit of these models7 , we evaluate them on Static Data and Dynamic Data, as is shown in Table 3.</p>
<p>Evaluation Metrics In the case of Static Data, all questions are objective, making accuracy the appropriate evaluation metric.For Dynamic Data, the physics questions are presented as multiple-choice questions, which can also be evaluated using accuracy.Conversely, the chemistry questions involve complex components, such as "What is the</p>
<p>Experiment Results</p>
<p>Answer-Only Setting Answer-only results of all the models on the test set are shown in Table 4 and detailed results of Static Data across different knowledge domains are provided in Appendix B. Analyzing the results of Static Data, GPT-4 demonstrates significantly superior performance compared to other models.And only GPT-4, GPT-3.5-turbo, and Claude-v1.3achieve an average accuracy exceeding 60%, which highlights the challenge posed by SciEval.</p>
<p>For the results of Dynamic Data, GPT-4 performs the best in terms of average accuracy and BLEU score.However, for counting and calculation questions, Galactica-30B yields the best results, indicating its strong aptitude in the field of science.Conversely, models with billions or tens of billions of parameters perform poorly on the chemistry subset, suggesting their limited knowledge about molecules.Regarding the performance of models on the physics subset, since all questions are 4-choices questions, the accuracy should be at least 25%.However, none of these models achieve satisfactory results in this subset.</p>
<p>As for Experimental Data, GPT-series models and Claude-series models achieve good results, while the other two models are not.The detailed scores models reached in each experiment are shown in Appendix C.However, although some models could get a great performance, during experiments, we find that these models are good at experimental principles and designing, while when it comes to analyzing the experiment results, the performances are not satisfying.</p>
<p>CoT Setting and 3-Shot setting Comparison of experiment results among Answer-Only, Chain-of-Thought and 3-Shot settings are shown in Figure 5 and Table 5. 9 And we refer detailed results to Appendix A and B.</p>
<p>The experimental results from Static Data reveal that solely the GPT-series LLMs get performance enhancement within the CoT setting due to the limited CoT capabilities of other LLMs.As for the 3-Shot setting, roughly half of the LLMs analyzed demonstrate superior performances relative to the Answer-Only setting.The performances of the remaining LLMs are closely similar to those observed within the Answer-Only setting.</p>
<p>From the experimental results of Dynamic Data, it is observed that both CoT and 3-Shot significantly enhance the performance of most Language Learning Models (LLMs) in the chemistry subset.However, the performances achieved are still not up to the mark.In the physics subset, the impact of CoT and 3-Shot on most LLMs is less pronounced, resulting in nearly random performances.Under the CoT setting, GPT-3.5-turboachieves an accuracy of 47.19, suggesting a robust understanding of physical principles.Conversely, the performance of GPT-4 is markedly poor, from which we find that despite its extensive knowledge of physical principles, it frequently employs incorrect formulas to solve problems.Nevertheless, GPT-4 attains an accuracy of 51.01 under 3-Shot setting, the highest among all models, demonstrating its ability to learn from a mere three examples.</p>
<p>Discussion</p>
<p>Training on large-scale scientific corpus is helpful.Based on experimental results (Table 4), Galactica (Taylor et al. 2022), which has been trained on an extensive scientific corpus, significantly outperforms other LLMs with a comparable number of parameters, although Galactica is trained with a much smaller amount of data.Remarkably, when tested on Dynamic Data, Galactica surpasses the GPTseries and Claude-series LLMs in computational problems.</p>
<p>Most LLMs perform bad on calculation problems, especially in physics domain.Detailed results across various knowledge domains on Static Data (refer to Appendix B) reveal that most LLMs underperform in the Scientific Calculation domain, while demonstrate relatively superior performance in other domains, which is particularly acute in the field of physics.Similar issues are also observed in Dynamic Data and Experimental Data.In the context of Dynamic Data, the mean square error, employed to evaluate calculation abilities within the chemistry subset, is exceedingly high for most LLMs, and almost all LLMs can only achieve nearly random performance within the physics subset.Regarding Experimental Data, our findings indicate that these LLMs struggle with the analysis of experimental results.</p>
<p>Conclusion</p>
<p>In this paper, we introduce SciEval, a benchmark designed to evaluate scientific capabilities of LLMs.SciEval comprises about 18,000 challenging scientific questions, covering three fundamental fields of science.SciEval assesses the scientific ability of LLMs across four dimensions.It incorporates both objective and subjective questions, and employs dynamic data generation to mitigate potential data leakage.We conduct comprehensive experiments on various advanced LLMs using SciEval and perform thorough analyses.Our experimental results reveal that most LLMs do not perform well on our benchmark, with the exception of the GPT-series and Claude-series LLMs.We hope that SciEval can serve as a robust benchmark for assessing scientific capabilities of LLMs.</p>
<p>Figure 1 :
1
Figure 1: The illustration of the evaluation system.SciEval covers three disciplines with amounts of sub-topics, and investigates four abilities, corresponding to six cognitive levels.</p>
<p>Figure 2 :
2
Figure 2: Data Collection steps of Static Data</p>
<p>Figure 3 :
3
Figure 3: An example of the prompt we used for AO setting.The red text is the response from the model, while the black text is the inputted prompt.</p>
<p>AFigure 4 :
4
Figure 4: An example of the prompt we used for CoT setting.The red text is the response from the model, while the blue text and black text are the inputted prompt.</p>
<p>Figure 5: Accuracy on Answer Only, Chain-of-Thought and 3-Shot settings of each LLMs for Static Data.</p>
<p>Table 1 :
1
Dataset comparison of SciEval and some other datasets covering science domain."BK"stands for Basic Knowledge, "KA" stands for Knowledge Application, "SC" stands for Scientific Calculation, and "RA" stands for Research Ability.
NameCategoryAbilitySourceData Type Dynamic #DataMMLUhumanities, social science, STEM, otherBK, KA, SCexam, book, course objective14079AGIEvalsocial science, STEM BK, KA, SCexamobjective8062C-EVALhumanities, social science, STEM, otherBK, KA, SCexamobjective12342MultiMedQAmedicalBK, KA, RAexam, researchobjective13115ChemLLMBench chemistryBK,KAknowledge baseobjective800MATHmathematicsSCexamobjective5000SciEvalscienceBK, KA,SC, RAcommunity QA, knowledge baseobjective + subjective15901model perfor-mance. And the objective questions other than DynamicData are referred to as Static Data.
â€¢ Dynamic data generation based on basic scientific principles.The huge amount of training data used for pre-training LLMs may cause the risk of data leakage for evaluation.In order to solve this problem, one of the main features of SciEval is the use of Dynamic Data, which can prevent potential data leakage and ensure the fairness and credibility of the evaluation results.The Dynamic Data will be updated regularly, and we will maintain a stable version to make a fair comparison of</p>
<p>Table 2
2, where we onlycount Static Data. For Dynamic Data, the chemistry part ex-amines the KA ability and contains 2000 data, while thephysics part evaluates the SC ability and involves 890 data.All these questions are in English and we show some dataexamples in Appendix D.AbilityBioChem PhyBasic Knowledge2147 2914456Knowledge Application 1379 372036Scientific Calculation3013401 1165Research Ability100000Total4830 10035 1657</p>
<p>Table 2 :
2
Statistics of Static Data</p>
<p>Table 3 :
3
Models evaluated in this paper.The "access" columns show whether we have full access to the model weights or we can only access through API or web.SD stands for Static Data, DD stands for Dynamic Data, and ED stands for Experimental Data.Marking " " means we evaluate the corresponding model on this subset.
Creator#Parameters Access SD DD EDGPT-4OpenAIundisclosedAPIGPT-3.5-turboOpenAIundisclosedAPIClaude-v1.3AnthropicundisclosedAPIClaude-instant-v1.1 AnthropicundisclosedAPIERNIE BotBaiduundisclosedWebSparkDeskiFLYTEKundisclosedWebVicunaLMSYS13BWeightsGalacticaMeta30B, 6.7BWeightsChatGLM2Tsinghua6BWeightsChatGLMTsinghua6BWeightsAlpacaStanford7BWeightsMOSSFudan16BWeightsLLaMaMeta7B, 13BWeightsModelStatic Data Biology Chemistry Physics Avg.Chemistry(DD) Acc. BLEU MSEPhysics(DD) Exp Acc. ScoreGPT-484.4969.3865.2273.93 11.05 23.78891.0925.8493.31GPT-3.5-turbo76.4264.3052.3066.97 7.6518.862008.7221.8088.27Claude-v1.372.5859.7254.9463.45 5.7521.981489.8726.1485.73Claude-instant-v1.170.4353.3652.3058.92 0.4516.078258.4621.4687.50Galactica-30B66.4850.1644.6554.960.94.14485.9922.47-Vicuna-13B58.3953.0645.1353.93 0.956.50766.6421.24-Galactica-6.7B57.8450.7730.9950.87 1.556.475519.8220.79-ChatGLM2-6B58.6244.0040.2648.440.21.863449.4424.83-ChatGLM-6B52.5445.3640.8047.23 0.752.4410303.9021.01-Alpaca-7B56.6642.4337.0146.540.22.92428419.2726.74-MOSS-16B47.7133.8731.7338.230.17.3730505.1724.27-LLaMa-13B48.5933.5619.4836.960.35.213707.017.08-LLaMa-7B36.2426.3815.0228.370.51.2611305.6514.38-ERNIE Bot--------61.12SparkDesk--------33.69</p>
<p>Table 4 :
4
Model performances of Answer-Only setting.The leaderboard is sorted by the average accuracy of Static Data.</p>
<p>Table 5 :
5
Results on Answer-Only, Chain-of-Thought and 3-Shot settings of each LLM for Dynamic Data.â†‘ means the performance is slightly better than that under Answer-Only setting, â†“ means worse, and âˆ¼ means the performance is nearly the same.
ModelAOChemistry CoT3-ShotAOPhysics CoT3-ShotGPT-411.05 11.65 â†‘ 12.42â†‘ 25.84 17.98 â†“ 51.01 â†‘GPT-3.5-turbo7.65 10.20 â†‘ 8.85 â†‘ 21.80 47.19 â†‘ 25.39 âˆ¼Galactica-6.7B1.551.75 â†‘3.05 â†‘ 20.79 23.37 âˆ¼ 21.12 âˆ¼Vicuna-13B0.951.95 â†‘1.80 â†‘ 21.24 18.65 âˆ¼ 23.37âˆ¼Galactica-30B0.902.60 â†‘3.30 â†‘ 22.47 14.72 â†“ 22.58 âˆ¼ChatGLM-6B0.750.80 â†‘1.15 â†‘ 21.01 25.39 âˆ¼ 23.37 âˆ¼LLaMa-7B0.500.10 â†“1.55 â†‘ 18.659.66 â†“27.53 â†‘LLaMa-13B0.300.25 âˆ¼ 2.11 â†‘7.085.84 âˆ¼22.70 â†‘ChatGLM2-6B 0.202.65 â†‘1.60 â†‘ 24.83 25.39 âˆ¼ 26.74 âˆ¼Alpaca-7B0.200.65 â†‘2.10 â†‘ 26.71 28.43 âˆ¼ 25.62 âˆ¼MOSS-16B0.100.85 â†‘0.65 â†‘ 24.27 25.06 âˆ¼ 26.40 âˆ¼
Due to the page limitation, we only compare some widely used benchmarks. For more information, we refer to(Chang et al.<br />
).The Thirty-Eighth AAAI Conference on Artificial Intelligence 
Scientific research requires different dimensions of knowledge, such as understanding and calculation, thence evaluation of scientific ability should be conducted at multiple levels. Bloom's taxonomy is a set of three hierarchical methods used for classification of educational learning objectives covering cognitive, affective and psychomotor domains. TheThe Thirty-Eighth AAAI Conference on Artificial Intelligence 
The Thirty-Eighth AAAI Conference on Artificial Intelligence (AAAI-24)
https://yiyan.baidu.com/
https://xinghuo.xfyun.cn/ The Thirty-Eighth AAAI Conference on Artificial Intelligence (AAAI-24)
The maximum context length of ChatGLM2 is extended to 32k, while it has limited ability to understand long texts. molecular weight of A?" and "What is the SMILES expression of B?". Hence, for questions with numerical answers, we employ
MSE 8 as the evaluation metric, while for questions with string answers, we utilize the BELU score(Papineni et al. 2002). Additionally, we also calculate the extract match scores. As for Experimental Data, each experiment consists of multiple open-ended questions. As a result, we assess the model-generated responses manually.
If the predictions do not contain any number, we will regard the MSE as 1 Ã— 10 10The Thirty-Eighth AAAI Conference on Artificial Intelligence (AAAI-24)
When evaluating on CoT and 3-Shot settings, Claude-Instant and Claude are not available for us, due to the limitation of API.
AcknowledgementsThis work is funded by the China NSFC Projects (92370206, U23B2057, 62106142 and 62120106006) and Shanghai Municipal Science and Technology Major Project (2021SHZDZX0102).
The role of ai in drug discovery: challenges, opportunities, and strategies. A Blanco-Gonzalez, A Cabezon, A Seco-Gonzalez, D Conde-Torres, P Antelo-Riveiro, A Pineiro, R Garcia-Fandino, Pharmaceuticals. 1668912023</p>
<p>Y Chang, X Wang, J Wang, Y Wu, K Zhu, H Chen, L Yang, X Yi, C Wang, Y Wang, arXiv:2307.03109A survey on evaluation of large language models. 2023arXiv preprint</p>
<p>GLM: General Language Model Pretraining with Autoregressive Blank Infilling. Z Du, Y Qian, X Liu, M Ding, J Qiu, Z Yang, J Tang, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational Linguistics20221</p>
<p>M Forehand, Blooms taxonomy. Emerging perspectives on learning, teaching, and technology. 201041</p>
<p>N Frey, R Soklaski, S Axelrod, S Samsi, R Gomez-Bombarelli, C Coley, V Gadepally, Neural scaling of deep chemical models. 2022</p>
<p>What indeed can GPT models do in chemistry?. T Guo, K Guo, Z Liang, Z Guo, N V Chawla, O Wiest, X Zhang, arXiv:2305.18365A comprehensive benchmark on eight tasks. 2023arXiv preprint</p>
<p>D Hendrycks, C Burns, S Basart, A Zou, M Mazeika, D Song, J Steinhardt, arXiv:2009.03300Measuring massive multitask language understanding. 2020arXiv preprint</p>
<p>D Hendrycks, C Burns, S Kadavath, A Arora, S Basart, E Tang, D Song, J Steinhardt, arXiv:2103.03874Measuring mathematical problem solving with the math dataset. 2021arXiv preprint</p>
<p>C-eval: A multilevel multi-discipline chinese evaluation suite for foundation models. Y Huang, Y Bai, Z Zhu, J Zhang, J Zhang, T Su, J Liu, C Lv, Y Zhang, J Lei, arXiv:2305.083222023arXiv preprint</p>
<p>What disease does this patient have? a large-scale open domain question answering dataset from medical exams. D Jin, E Pan, N Oufattole, W.-H Weng, H Fang, P Szolovits, Applied Sciences. 111464212021</p>
<p>Q Jin, B Dhingra, Z Liu, W W Cohen, X Lu, arXiv:1909.06146Pubmedqa: A dataset for biomedical research question answering. 2019arXiv preprint</p>
<p>Large language models are zero-shot reasoners. T Kojima, S S Gu, M Reid, Y Matsuo, Y Iwasawa, Advances in neural information processing systems. 202235</p>
<p>A revision of Bloom's taxonomy: An overview. Theory into practice. D R Krathwohl, 200241</p>
<p>P Liang, R Bommasani, T Lee, D Tsipras, D Soylu, M Yasunaga, Y Zhang, D Narayanan, Y Wu, A Kumar, arXiv:2211.09110Holistic evaluation of language models. 2022arXiv preprint</p>
<p>Learn to explain: Multimodal reasoning via thought chains for science question answering. P Lu, S Mishra, T Xia, L Qiu, K.-W Chang, S.-C Zhu, O Tafjord, P Clark, A Kalyan, Advances in Neural Information Processing Systems. 202235</p>
<p>BioGPT: generative pre-trained transformer for biomedical text generation and mining. R Luo, L Sun, Y Xia, T Qin, S Zhang, H Poon, T.-Y Liu, Briefings in Bioinformatics. 2364092022</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, arXiv:2303.08774Advances in Neural Information Processing Systems. 202235Technical Report</p>
<p>Bleu: a method for automatic evaluation of machine translation. K Papineni, S Roukos, T Ward, W.-J Zhu, Proceedings of the 40th annual meeting of the Association for Computational Linguistics. the 40th annual meeting of the Association for Computational Linguistics2002</p>
<p>ChatGPT: Optimizing language models for dialogue. J Schulman, B Zoph, C Kim, J Hilton, J Menick, J Weng, J F C Uribe, L Fedus, L Metz, M Pokorny, Nature. 2022. 2023Large language models encode clinical knowledge</p>
<p>A Srivastava, A Rastogi, A Rao, A A M Shoeb, A Abid, A Fisch, A R Brown, A Santoro, A Gupta, A Garriga-Alonso, arXiv:2206.04615Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. 2022arXiv preprint</p>
<p>MOSS: Training Conversational Language Models from Synthetic Data. T Sun, X Zhang, Z He, P Li, Q Cheng, H Yan, X Liu, Y Shao, Q Tang, X Zhao, K Chen, Y Zheng, Z Zhou, R Li, J Zhan, Y Zhou, L Li, X Yang, L Wu, Z Yin, X Huang, X Qiu, R Taori, I Gulrajani, T Zhang, Y Dubois, X Li, C Guestrin, P Liang, T B Hashimoto, R Taylor, M Kardas, G Cucurull, T Scialom, A Hartshorn, E Saravia, A Poulton, V Kerkez, R Stojnic, N Hambro, E Azhar, F , arXiv:2302.13971Novel Paradigm for AIdriven Scientific Research: From AI4S to Intelligent Science. G Izacard, X Martinet, M.-A Lachaux, T Lacroix, B RoziÃ¨re, Goyal, 2023. 2023. 2022. 2023. 202338arXiv preprintLlama: Open and efficient foundation language models</p>
<p>X Wang, Z Hu, P Lu, Y Zhu, J Zhang, S Subramaniam, A R Loomba, S Zhang, Y Sun, W Wang, arXiv:2307.10635SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models. 2023arXiv preprint</p>
<p>L Zheng, W.-L Chiang, Y Sheng, S Zhuang, Z Wu, Y Zhuang, Z Lin, Z Li, D Li, E Xing, arXiv:2306.05685Judging LLM-as-a-judge with MT-Bench and Chatbot Arena. 2023arXiv preprint</p>
<p>Agieval: A humancentric benchmark for evaluating foundation models. W Zhong, R Cui, Y Guo, Y Liang, S Lu, Y Wang, A Saied, W Chen, N Duan, arXiv:2304.063642023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
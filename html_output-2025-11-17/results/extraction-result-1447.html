<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1447 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1447</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1447</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-26.html">extraction-schema-26</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated systems making scientific discoveries, including how these discoveries are characterized as incremental or transformational, and how they are evaluated and validated.</div>
                <p><strong>Paper ID:</strong> paper-4c53cd4c5a1536f1cddcd541fec3e1c59822b6b8</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/4c53cd4c5a1536f1cddcd541fec3e1c59822b6b8" target="_blank">Causal deconvolution by algorithmic generative models</a></p>
                <p><strong>Paper Venue:</strong> Nature Machine Intelligence</p>
                <p><strong>Paper TL;DR:</strong> A universal, unsupervised and parameter-free model-oriented approach, based on the seminal concept and the first principles of algorithmic probability, to decompose an observation into its most likely algorithmic generative models to contribute to tackling the challenge of causation.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1447.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1447.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated systems making scientific discoveries, including how these discoveries are characterized as incremental or transformational, and how they are evaluated and validated.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Algorithmic Causal Deconvolution</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Causal deconvolution by algorithmic generative models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An unsupervised, parameter-free algorithmic framework that decomposes observations into their most likely algorithmic generative sources by estimating algorithmic probability and assembling small candidate programs (via CTM/BDM) to form mechanistic models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Algorithmic Causal Deconvolution</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A model-oriented unsupervised system based on algorithmic probability that (1) precomputes a large table of small program outputs (CTM/BDM), (2) estimates algorithmic complexity contributions of parts of data, and (3) iteratively removes elements (edges/pixels) with minimal information-loss to partition data into components most likely produced by the same generating mechanism. Implements Algorithm 1 (deconvolution) and Algorithm 2 (terminating criterion), uses information contribution I(G,e)=C(G)-C(G\e), and relies on Coding Theorem Method (CTM) and Block Decomposition Method (BDM) approximations.</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_domain</strong></td>
                            <td>Computational causal discovery / complex systems / network science / algorithmic information theory</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_description</strong></td>
                            <td>Automatically identifies candidate generative mechanisms (computer programs) that can reproduce observed data fragments, thereby 'discovering' the likely underlying algorithmic sources that generated intertwined data (strings, cellular-automaton space-time diagrams, images, and networks). The system outputs partitions/components and candidate short programs for each component that serve as mechanistic explanatory models and can be run to make predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type</strong></td>
                            <td>mostly a conceptual contribution and a novel framework</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type_justification</strong></td>
                            <td>The paper explicitly describes the contribution as 'mostly a conceptual contribution and a novel framework' and positions the method as an alternative to statistical approaches; it does not label discovered results as 'transformational' or 'incremental' but argues novelty arises from replacing purely statistical decomposition with algorithmic-probability-based mechanistic decomposition and from providing candidate generating programs for validation and prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Numerical experiments on synthetic and controlled datasets: interacting elementary cellular automata (ECA) space-time diagrams and composite graphs/networks; algorithmic-complexity estimates via CTM/BDM; information-contribution metric I(G,e)=C(G)-C(G\e); construction of 'information signatures' (sorted edge/pixel contributions) and examination of peaks; terminating-criterion using a threshold based on log(2)+ε; comparisons to Shannon entropy, lossless compression (Normalized Compression Distance), and Partial Information Decomposition (PID) benchmarks (detailed in Sup. Inf.); statistical checks (e.g., repeated experiments, significance testing of perturbation sensitivity), and computational-complexity analysis (O(M^2) worst-case, O(M) for terminating-step variant).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approaches</strong></td>
                            <td>Validation performed by (a) numerical reconstruction: recovering expected number of generating mechanisms and candidate programs on controlled examples (e.g., 2 interacting ECA rules recovered as distinct components, retrieval of N=3 in cases where a rule decomposes into regimes), (b) perturbation/sensitivity analysis (flip/delete pixel or edge and measure change in estimated algorithmic complexity), (c) sanity checks and repetition (experiments repeated 20 times for some cases), (d) theoretical grounding via the Coding Theorem linking CTM frequencies to algorithmic complexity, and (e) reproducibility support via publicly available code and online implementation (links provided). No wet-lab or external experimental validation beyond computational experiments is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment</strong></td>
                            <td>Novelty is assessed by demonstrating abilities that statistical or compression-based methods fail to provide: (1) ability to separate intertwined generating mechanisms where Shannon entropy and lossless compression fail, (2) providing candidate generative programs (not just scalar complexity values), (3) parameter-free terminating criterion using algorithmic information-theoretic reasoning (log(2)+ε), and (4) empirical demonstrations on CA and network examples. The paper argues these capabilities go beyond prior statistical decompositions like PID.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_metrics</strong></td>
                            <td>Primary quantitative measures are algorithmic information values: I(G,e) (bits), elements of the information signature, differences between consecutive information-signature maxima (compared to log(2)+ε), the number of retrieved components N, and computational complexity bounds (O(M^2) / O(M)). CTM/BDM numeric approximations are based on empirical distributions from small Turing machines (e.g., all TMs up to 5 states; block sizes and decomposition overlap settings such as 12 bits for strings and 4×4 for arrays are reported). The paper provides no global scalar 'impact score' or percentage success metric.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_discoveries</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_details</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>The paper does not provide a single numeric success rate; it reports qualitative/empirical success in tested cases (e.g., correctly finding the number of mechanisms and candidate programs in synthetic string and CA examples, deconvolving networks into expected subcomponents). Some experiments were repeated (e.g., 20 runs for network/CAs) with results matching theoretical expectations, but no aggregate accuracy rate or precision/recall metrics are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Known limitations discussed include: semi-computability and intractability of true algorithmic probability (necessitating empirical CTM/BDM approximations and precomputation limited to small Turing machines), boundary errors and approximation bounds in BDM (mitigated but present), computational cost (worst-case O(M^2)), ambiguity when different components have similar algorithmic complexity (making source attribution statistically unlikely to be unique), inability to directly observe the global interacting rule (the interaction may act as an apparent extra region), practical estimation of ε in terminating criterion, and that the method is demonstrated on discrete/computational systems rather than on domain-specific empirical scientific discovery (no experimental wet-lab discovery claim). The paper also notes cases where strict theoretical cutoffs do not occur in practice.</td>
                        </tr>
                        <tr>
                            <td><strong>has_incremental_transformational_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Causal deconvolution by algorithmic generative models', 'publication_date_yy_mm': '2019-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1447.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1447.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated systems making scientific discoveries, including how these discoveries are characterized as incremental or transformational, and how they are evaluated and validated.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CTM/BDM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Coding Theorem Method and Block Decomposition Method</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical methods to approximate algorithmic probability and algorithmic complexity: CTM estimates frequency of small Turing-machine outputs; BDM decomposes larger objects into blocks with known CTM values and aggregates complexity estimates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Coding Theorem Method (CTM) and Block Decomposition Method (BDM)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>CTM empirically approximates the Universal Distribution by running a very large set of small programs (small Turing machines) in quasi-lexicographic order to obtain frequencies of outputs; BDM decomposes an object's representation (e.g., adjacency matrix) into blocks for which CTM values are available and combines those with logarithmic multiplicity terms to estimate overall algorithmic complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_domain</strong></td>
                            <td>Algorithmic complexity estimation; supports mechanistic discovery in data (strings, images, networks)</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_description</strong></td>
                            <td>Used to discover candidate short generative programs for data fragments by ranking fragment hypotheses according to estimated algorithmic probability and assembling fragment models into larger mechanistic explanations; supplies the quantitative complexity estimates (C(G)) and subcomponent contributions used by the deconvolution algorithm.</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type</strong></td>
                            <td>empirical approximation method (not labeled incremental/transformational)</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type_justification</strong></td>
                            <td>The paper treats CTM/BDM as practical empirical approximations to theoretical algorithmic probability and uses them as foundational tools; the authors note their precomputation enables linear-time application of the deconvolution framework and allows capturing non-statistical features beyond Shannon entropy.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Evaluation via (a) theoretical Coding Theorem relationship (-log2 m(G) ≈ C(G) + c), (b) empirical testing on small Turing machines (up to 5 states) to build CTM lookup tables, (c) error-bounding analyses for BDM block decomposition (choice of block sizes, overlap), and (d) application success in deconvolution experiments (CA and networks) where CTM/BDM-derived estimates lead to expected partitions.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approaches</strong></td>
                            <td>Validation by comparing CTM/BDM outputs to theoretical expectations (coding theorem), demonstrating bounded error for decomposition choices, and showing that BDM-derived complexities produce meaningful partitions in experiments; availability of code and datasets to reproduce CTM/BDM lookups is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment</strong></td>
                            <td>Novelty lies in using CTM/BDM as lookup-based approximations to make algorithmic-probability-based inference tractable and integrating them into a causal-deconvolution pipeline to obtain candidate programs rather than only scalar complexity estimates.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_metrics</strong></td>
                            <td>Metrics include estimated algorithmic probabilities m(G) (frequencies), computed complexities C(G) (bits), block multiplicity terms log2(n_u), and empirical bounds on decomposition error; specific CTM precomputation limits (e.g., TMs up to 5 states) are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_discoveries</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_details</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>No single success rate; CTM/BDM are evaluated by their utility in downstream deconvolution tasks and by theoretical coding-theorem grounding; practical effectiveness demonstrated in the paper's synthetic experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Limitations are semi-computability of algorithmic probability (necessitating limited precomputation), constrained lookup tables (small TMs only), boundary effects in block decomposition, choice of block size/overlap trade-offs, and approximation errors that must be managed (ε in terminating criterion).</td>
                        </tr>
                        <tr>
                            <td><strong>has_incremental_transformational_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Causal deconvolution by algorithmic generative models', 'publication_date_yy_mm': '2019-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1447.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1447.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated systems making scientific discoveries, including how these discoveries are characterized as incremental or transformational, and how they are evaluated and validated.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AIXI (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AIXI (universal agent combining algorithmic probability with decision theory)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A theoretical reinforcement-learning agent that uses algorithmic probability as a prior combined with decision-theoretic planning; cited as related work and contrasted with practical approximations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AIXI (theoretical agent)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Mentioned as part of related work: a theoretical agent proposed by Hutter that combines algorithmic probability (Universal Distribution) with decision theory and Bayes, and which in practice is approximated via compression/Monte-Carlo/heuristics due to uncomputability/intractability.</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_domain</strong></td>
                            <td>Theoretical artificial general intelligence / reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_description</strong></td>
                            <td>No specific discovery by AIXI is reported in this paper; AIXI is cited as a related theoretical framework illustrating algorithmic-probability-based inductive inference for agents.</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type_justification</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>validation_approaches</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment</strong></td>
                            <td>Mentioned as prior foundational work combining algorithmic probability and decision theory; the paper notes that practical deployments of AIXI rely on approximations (compression, MDL, Monte Carlo).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_metrics</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_discoveries</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_details</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Paper notes AIXI's theoretical optimality but also its semi-computability/intractability in practice, and that practical approximations rely on weaker models (compression, MDL, Monte Carlo search).</td>
                        </tr>
                        <tr>
                            <td><strong>has_incremental_transformational_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Causal deconvolution by algorithmic generative models', 'publication_date_yy_mm': '2019-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Partial Information Decomposition <em>(Rating: 2)</em></li>
                <li>AIXI <em>(Rating: 2)</em></li>
                <li>Normalized Compression Distance <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1447",
    "paper_id": "paper-4c53cd4c5a1536f1cddcd541fec3e1c59822b6b8",
    "extraction_schema_id": "extraction-schema-26",
    "extracted_data": [
        {
            "name_short": "Algorithmic Causal Deconvolution",
            "name_full": "Causal deconvolution by algorithmic generative models",
            "brief_description": "An unsupervised, parameter-free algorithmic framework that decomposes observations into their most likely algorithmic generative sources by estimating algorithmic probability and assembling small candidate programs (via CTM/BDM) to form mechanistic models.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Algorithmic Causal Deconvolution",
            "system_description": "A model-oriented unsupervised system based on algorithmic probability that (1) precomputes a large table of small program outputs (CTM/BDM), (2) estimates algorithmic complexity contributions of parts of data, and (3) iteratively removes elements (edges/pixels) with minimal information-loss to partition data into components most likely produced by the same generating mechanism. Implements Algorithm 1 (deconvolution) and Algorithm 2 (terminating criterion), uses information contribution I(G,e)=C(G)-C(G\\e), and relies on Coding Theorem Method (CTM) and Block Decomposition Method (BDM) approximations.",
            "discovery_domain": "Computational causal discovery / complex systems / network science / algorithmic information theory",
            "discovery_description": "Automatically identifies candidate generative mechanisms (computer programs) that can reproduce observed data fragments, thereby 'discovering' the likely underlying algorithmic sources that generated intertwined data (strings, cellular-automaton space-time diagrams, images, and networks). The system outputs partitions/components and candidate short programs for each component that serve as mechanistic explanatory models and can be run to make predictions.",
            "discovery_type": "mostly a conceptual contribution and a novel framework",
            "discovery_type_justification": "The paper explicitly describes the contribution as 'mostly a conceptual contribution and a novel framework' and positions the method as an alternative to statistical approaches; it does not label discovered results as 'transformational' or 'incremental' but argues novelty arises from replacing purely statistical decomposition with algorithmic-probability-based mechanistic decomposition and from providing candidate generating programs for validation and prediction.",
            "evaluation_methods": "Numerical experiments on synthetic and controlled datasets: interacting elementary cellular automata (ECA) space-time diagrams and composite graphs/networks; algorithmic-complexity estimates via CTM/BDM; information-contribution metric I(G,e)=C(G)-C(G\\e); construction of 'information signatures' (sorted edge/pixel contributions) and examination of peaks; terminating-criterion using a threshold based on log(2)+ε; comparisons to Shannon entropy, lossless compression (Normalized Compression Distance), and Partial Information Decomposition (PID) benchmarks (detailed in Sup. Inf.); statistical checks (e.g., repeated experiments, significance testing of perturbation sensitivity), and computational-complexity analysis (O(M^2) worst-case, O(M) for terminating-step variant).",
            "validation_approaches": "Validation performed by (a) numerical reconstruction: recovering expected number of generating mechanisms and candidate programs on controlled examples (e.g., 2 interacting ECA rules recovered as distinct components, retrieval of N=3 in cases where a rule decomposes into regimes), (b) perturbation/sensitivity analysis (flip/delete pixel or edge and measure change in estimated algorithmic complexity), (c) sanity checks and repetition (experiments repeated 20 times for some cases), (d) theoretical grounding via the Coding Theorem linking CTM frequencies to algorithmic complexity, and (e) reproducibility support via publicly available code and online implementation (links provided). No wet-lab or external experimental validation beyond computational experiments is reported.",
            "novelty_assessment": "Novelty is assessed by demonstrating abilities that statistical or compression-based methods fail to provide: (1) ability to separate intertwined generating mechanisms where Shannon entropy and lossless compression fail, (2) providing candidate generative programs (not just scalar complexity values), (3) parameter-free terminating criterion using algorithmic information-theoretic reasoning (log(2)+ε), and (4) empirical demonstrations on CA and network examples. The paper argues these capabilities go beyond prior statistical decompositions like PID.",
            "impact_metrics": "Primary quantitative measures are algorithmic information values: I(G,e) (bits), elements of the information signature, differences between consecutive information-signature maxima (compared to log(2)+ε), the number of retrieved components N, and computational complexity bounds (O(M^2) / O(M)). CTM/BDM numeric approximations are based on empirical distributions from small Turing machines (e.g., all TMs up to 5 states; block sizes and decomposition overlap settings such as 12 bits for strings and 4×4 for arrays are reported). The paper provides no global scalar 'impact score' or percentage success metric.",
            "comparison_to_human_discoveries": false,
            "comparison_details": "",
            "success_rate": "The paper does not provide a single numeric success rate; it reports qualitative/empirical success in tested cases (e.g., correctly finding the number of mechanisms and candidate programs in synthetic string and CA examples, deconvolving networks into expected subcomponents). Some experiments were repeated (e.g., 20 runs for network/CAs) with results matching theoretical expectations, but no aggregate accuracy rate or precision/recall metrics are reported.",
            "challenges_limitations": "Known limitations discussed include: semi-computability and intractability of true algorithmic probability (necessitating empirical CTM/BDM approximations and precomputation limited to small Turing machines), boundary errors and approximation bounds in BDM (mitigated but present), computational cost (worst-case O(M^2)), ambiguity when different components have similar algorithmic complexity (making source attribution statistically unlikely to be unique), inability to directly observe the global interacting rule (the interaction may act as an apparent extra region), practical estimation of ε in terminating criterion, and that the method is demonstrated on discrete/computational systems rather than on domain-specific empirical scientific discovery (no experimental wet-lab discovery claim). The paper also notes cases where strict theoretical cutoffs do not occur in practice.",
            "has_incremental_transformational_comparison": false,
            "uuid": "e1447.0",
            "source_info": {
                "paper_title": "Causal deconvolution by algorithmic generative models",
                "publication_date_yy_mm": "2019-01"
            }
        },
        {
            "name_short": "CTM/BDM",
            "name_full": "Coding Theorem Method and Block Decomposition Method",
            "brief_description": "Empirical methods to approximate algorithmic probability and algorithmic complexity: CTM estimates frequency of small Turing-machine outputs; BDM decomposes larger objects into blocks with known CTM values and aggregates complexity estimates.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Coding Theorem Method (CTM) and Block Decomposition Method (BDM)",
            "system_description": "CTM empirically approximates the Universal Distribution by running a very large set of small programs (small Turing machines) in quasi-lexicographic order to obtain frequencies of outputs; BDM decomposes an object's representation (e.g., adjacency matrix) into blocks for which CTM values are available and combines those with logarithmic multiplicity terms to estimate overall algorithmic complexity.",
            "discovery_domain": "Algorithmic complexity estimation; supports mechanistic discovery in data (strings, images, networks)",
            "discovery_description": "Used to discover candidate short generative programs for data fragments by ranking fragment hypotheses according to estimated algorithmic probability and assembling fragment models into larger mechanistic explanations; supplies the quantitative complexity estimates (C(G)) and subcomponent contributions used by the deconvolution algorithm.",
            "discovery_type": "empirical approximation method (not labeled incremental/transformational)",
            "discovery_type_justification": "The paper treats CTM/BDM as practical empirical approximations to theoretical algorithmic probability and uses them as foundational tools; the authors note their precomputation enables linear-time application of the deconvolution framework and allows capturing non-statistical features beyond Shannon entropy.",
            "evaluation_methods": "Evaluation via (a) theoretical Coding Theorem relationship (-log2 m(G) ≈ C(G) + c), (b) empirical testing on small Turing machines (up to 5 states) to build CTM lookup tables, (c) error-bounding analyses for BDM block decomposition (choice of block sizes, overlap), and (d) application success in deconvolution experiments (CA and networks) where CTM/BDM-derived estimates lead to expected partitions.",
            "validation_approaches": "Validation by comparing CTM/BDM outputs to theoretical expectations (coding theorem), demonstrating bounded error for decomposition choices, and showing that BDM-derived complexities produce meaningful partitions in experiments; availability of code and datasets to reproduce CTM/BDM lookups is provided.",
            "novelty_assessment": "Novelty lies in using CTM/BDM as lookup-based approximations to make algorithmic-probability-based inference tractable and integrating them into a causal-deconvolution pipeline to obtain candidate programs rather than only scalar complexity estimates.",
            "impact_metrics": "Metrics include estimated algorithmic probabilities m(G) (frequencies), computed complexities C(G) (bits), block multiplicity terms log2(n_u), and empirical bounds on decomposition error; specific CTM precomputation limits (e.g., TMs up to 5 states) are reported.",
            "comparison_to_human_discoveries": false,
            "comparison_details": "",
            "success_rate": "No single success rate; CTM/BDM are evaluated by their utility in downstream deconvolution tasks and by theoretical coding-theorem grounding; practical effectiveness demonstrated in the paper's synthetic experiments.",
            "challenges_limitations": "Limitations are semi-computability of algorithmic probability (necessitating limited precomputation), constrained lookup tables (small TMs only), boundary effects in block decomposition, choice of block size/overlap trade-offs, and approximation errors that must be managed (ε in terminating criterion).",
            "has_incremental_transformational_comparison": false,
            "uuid": "e1447.1",
            "source_info": {
                "paper_title": "Causal deconvolution by algorithmic generative models",
                "publication_date_yy_mm": "2019-01"
            }
        },
        {
            "name_short": "AIXI (mentioned)",
            "name_full": "AIXI (universal agent combining algorithmic probability with decision theory)",
            "brief_description": "A theoretical reinforcement-learning agent that uses algorithmic probability as a prior combined with decision-theoretic planning; cited as related work and contrasted with practical approximations.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "AIXI (theoretical agent)",
            "system_description": "Mentioned as part of related work: a theoretical agent proposed by Hutter that combines algorithmic probability (Universal Distribution) with decision theory and Bayes, and which in practice is approximated via compression/Monte-Carlo/heuristics due to uncomputability/intractability.",
            "discovery_domain": "Theoretical artificial general intelligence / reinforcement learning",
            "discovery_description": "No specific discovery by AIXI is reported in this paper; AIXI is cited as a related theoretical framework illustrating algorithmic-probability-based inductive inference for agents.",
            "discovery_type": "",
            "discovery_type_justification": "",
            "evaluation_methods": "",
            "validation_approaches": "",
            "novelty_assessment": "Mentioned as prior foundational work combining algorithmic probability and decision theory; the paper notes that practical deployments of AIXI rely on approximations (compression, MDL, Monte Carlo).",
            "impact_metrics": "",
            "comparison_to_human_discoveries": false,
            "comparison_details": "",
            "success_rate": "",
            "challenges_limitations": "Paper notes AIXI's theoretical optimality but also its semi-computability/intractability in practice, and that practical approximations rely on weaker models (compression, MDL, Monte Carlo search).",
            "has_incremental_transformational_comparison": null,
            "uuid": "e1447.2",
            "source_info": {
                "paper_title": "Causal deconvolution by algorithmic generative models",
                "publication_date_yy_mm": "2019-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Partial Information Decomposition",
            "rating": 2
        },
        {
            "paper_title": "AIXI",
            "rating": 2
        },
        {
            "paper_title": "Normalized Compression Distance",
            "rating": 1
        }
    ],
    "cost": 0.01426125,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Causal deconvolution by algorithmic generative models</h1>
<table>
<thead>
<tr>
<th style="text-align: left;">Item Type</th>
<th style="text-align: left;">Article</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Authors</td>
<td style="text-align: left;">Zenil, Hector;Kiani, Narsis A.;Zea, Allan A.;Tegner, Jesper</td>
</tr>
<tr>
<td style="text-align: left;">Citation</td>
<td style="text-align: left;">Zenil H, Kiani NA, Zea AA, Tegnér J (2019) Causal deconvolution <br> by algorithmic generative models. Nature Machine Intelligence 1: <br> 58-66. Available: http://dx.doi.org/10.1038/s42256-018-0005-0.</td>
</tr>
<tr>
<td style="text-align: left;">Eprint version</td>
<td style="text-align: left;">Post-print</td>
</tr>
<tr>
<td style="text-align: left;">DOI</td>
<td style="text-align: left;">$10.1038 / \mathrm{s} 42256-018-0005-0$</td>
</tr>
<tr>
<td style="text-align: left;">Publisher</td>
<td style="text-align: left;">Springer Nature</td>
</tr>
<tr>
<td style="text-align: left;">Journal</td>
<td style="text-align: left;">Nature Machine Intelligence</td>
</tr>
<tr>
<td style="text-align: left;">Rights</td>
<td style="text-align: left;">The final publication is available at Springer via http:// <br> dx.doi.org/10.1038/s42256-018-0005-0</td>
</tr>
<tr>
<td style="text-align: left;">Download date</td>
<td style="text-align: left;">$2025-10-24$ 20:15:19</td>
</tr>
<tr>
<td style="text-align: left;">Link to Item</td>
<td style="text-align: left;">http://hdl.handle.net/10754/630919</td>
</tr>
</tbody>
</table>
<h1>Algorithmic Causal Deconvolution of Intertwined Data and Networks by Generating Mechanism*</h1>
<p>Hector Zenil ${ }^{1,2,3,5}$, Narsis A. Kiani ${ }^{1,2,3}$, Allan A. Zea ${ }^{1,4}$, Jesper Tegnér ${ }^{2,5}$<br>${ }^{1}$ Algorithmic Dynamics Lab, Centre for Molecular Medicine, Karolinska Institute, Stockholm, Sweden<br>${ }^{2}$ Unit of Computational Medicine, Department of Medicine, Karolinska Institute, Stockholm, Sweden<br>${ }^{3}$ Algorithmic Nature Group, LABORES for the Natural and Digital Sciences, Paris, France<br>${ }^{4}$ Escuela de Matemática, Facultad de Ciencias, UCV, Caracas, Venezuela<br>${ }^{5}$ Biological and Environmental Sciences and Engineering Division, Computer, Electrical and Mathematical Sciences and Engineering Division, King Abdullah University of Science and Technology (KAUST), Kingdom of Saudi Arabia {hector.zenil, narsis.kiani, jesper.tegner}@ki.se</p>
<h4>Abstract</h4>
<p>Complex data usually results from the interaction of objects produced by different generating mechanisms. Here we introduce a universal, unsupervised and parameter-free model-oriented approach, based upon the seminal concept of algorithmic probability, that decomposes an observation into its most likely algorithmic generative sources. Our approach uses a causal calculus to infer model representations. We demonstrate its ability to deconvolve interacting mechanisms regardless of whether the resultant objects are strings, space-time evolution diagrams, images or networks. While this is mostly a conceptual contribution and a novel framework, we provide numerical evidence evaluating the ability of our methods to separate data from observations produced by discrete dynamical systems such as cellular automata and complex networks. We think that these separating techniques can contribute to tackling the challenge of causation, thus complementing other statistically oriented approaches.</p>
<p>Keywords: information decomposition; inductive inference; image segmentation; algorithmic renormalisation; program synthesis; graph partitioning; causal clustering; algorithmic machine learning; feature selection.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>1 Introduction</h1>
<p>To extract representations leading to their generative mechanisms from data, especially without making arbitrary decisions based on biased assumptions, is a central challenge in most areas of scientific research, particularly given the major limitations of current machine and deep learning paradigms, which often lose sight of a model's components. Typically, models encode features of data in statistical form and in single variables, even in cases where several data sources are involved.</p>
<p>Broadly speaking, extracting candidate models and defining model-based approaches guided by data is one of the main challenges in the areas of machine learning, artificial intelligence and causal discovery. Here we introduce a framework based upon the theory of algorithmic probability, which in our formulation is capable of identifying different sources that may explain and provide different models for each of the possible sources of convoluted or intertwined data.</p>
<h3>1.1 Survey of Related Work</h3>
<p>Casual inference has been one of the most challenging problems in science. The debate about causality has not prevented the development of successful and mature mathematical and algorithmic frameworks, first in the form of logic and classical statistics, and today in the form of dynamical systems, computational mechanics, computability and algorithmic complexity. Based on mature mathematical notions that are acknowledged to fully characterise the concept of randomness, we introduced a suite of algorithms [57] with which to study the algorithmic information dynamics of evolving systems, and methods to reduce the dimensions of data [58] based on first principles. Algorithmic data dimension reduction and algorithmic deconvolution are challenges which can be viewed as opposite sides of the same coin. On the one hand, data reduction is achieved by finding elements that are considered redundant, using as a criterion their contribution to the algorithmic content of the description of the data. On the other hand, deconvolution by generative source is perhaps the ultimate goal of partition, clustering and machine learning algorithms. However, these approaches often lose sight of their goal of causal decomposition and rather seek to identify common features of data as evidence of the possible common origin of said data. For example, in signal processing, popular methods such as $k$ means [26] or $k$-medoids [25] define heuristics based on the minimisation of distance among data points according to some metric. Some other popular methods, such as support vector clustering [14] and traditional machine learning techniques [13], draw on probability distributions, regression, and correlation techniques, providing means for linear separation producing different groups. This includes deep neural networks [48] based on constructing a differentiable landscape on which elements are statistically mapped for classification purposes. Another type of separating method</p>
<p>applied to objects such as graphs relies on graph-theoretic properties (e.g. [12]) shared among networks. In this category belong ways to separate graphs by indices such as edge betweenness or by the frequency of over-representation of certain subgraphs, also called network motifs [18, 19], and by more sophisticated criteria such as shared graph spectral features [20, 22, 23, 24]. All these methods make the assumption that, by virtue of objects sharing statistical, topological or algebraic features [15], the said objects may be generated by the same means or from the same sources.</p>
<p>Classical information theory has provided ways to capture and encode statistical properties from data, and these have made an impact on many areas of science. For example, mutual information can capture various averages based on associated distributions of statistical properties that are contained in one variable about another, that is, how information can be combined and decomposed in purely statistical terms. Recently, some methods based on information decomposition have been introduced $[2,3]$ with the purpose of separating multivariate signals into their alleged generative sources. A recent proposal that has gained some traction is the so-called Partial Information Decomposition or PID [2], that falls short [3], among other reasons, because it can only tell what a variable can statistically tell about some other variable.</p>
<p>Thus we believe that there is a strong need to advance methods to decompose intertwined data coming from one or more sources that go beyond traditional statistics. Such methods would require novel approaches with more powerful indices-which also makes them more difficult to calculate. We call our method causal deconvolution by generative source and offer it as an alternative to statistical inference approaches such as PID, which are hamstrung by limitations. This is particularly important in applications to areas such as biology or neuroscience, where the ability of a system to decompose multiple information sources in a non-trivial fashion represents evolutionary and cognitive advantages independent of classical information-theoretic constraints. Quantitative measures to disentangle complex sources and new methods to help tell apart fused mechanisms from other signals or simply to tell noise from signals are thus of broad interest in areas where causal analysis is germane.</p>
<p>Notably related to the kind of deconvolution and decomposition explored in this paper are methods based on pattern recognition [29], classical information theory [2] (a survey can be found in [7]) and lossless compression[21], the most relevant methods being the ones based on information distances [16] and compression, such as the so-called Normalised Information Distance [30], and its related measure the Normalised Compression Distance [17], and other variations. All these methods can also be adapted to make use of other complexity indices or lossless compression algorithms.</p>
<p>One criticism levelled at several, if not most, algorithms and complexity measures is that in their estimation of some complexity index, the methods only assign a</p>
<p>number to data from which nothing else can be extracted and has very little value. An example would be a compressed file, because a compressed file is not only a black box which is almost impossible to decipher, but it is also not a model, rule or computer program that can receive inputs or be run for a larger number of steps to produce more data or make predictions. Along the same lines is Rissanen's [27] Minimum Description Length, inspired by Solomonoff's induction method of algorithmic probability [47], which eschews the strength of a Turing-complete language and elects to compress data using weaker models of computation.</p>
<p>In attempting to address the drawbacks of statistical inference, methods such as inductive inference [38], inductive programming, and program synthesis have been advanced. In this direction, the concept of algorithmic probability introduced by Solomonoff [47] was proven to be optimal and universal, but its semi-computability was a deterrent to its application, leading some researchers, led by Crutchfield et al. to circumvent it and instead use computationally constrained models such as computational mechanics $[10,11]$.</p>
<p>In the approach followed in this paper, which can be thought of as related to computational mechanics, we replace the methods used in computational mechanics based on, e.g., Markov processes and Bayesian inference, by a measure based on algorithmic probability and an empirical estimation of the Universal Distribution (the distribution associated with algorithmic probability), while staying within the boundaries of the field of computational mechanics itself. In a similar category of inductive inference, but mostly of a theoretical nature is AIXI as introduced by Hutter [9], combining algorithmic probability with decision theory by way of Bayes' theorem, replacing the prior with the Universal Distribution as a prior. It is similar to Levin's search [42], that is, it dovetails Turing machines (interleaves computer programs one step at a time from shortest to longest). It is designed particularly for applications to reinforcement learning. In current deployments AIXI circumvents uncomputability and intractability by relying on popular compression algorithms such as LZW, Minimum Description Length, Monte Carlo search and Markov processes, thereby effectively using weaker models of computation. In another category are methods introduced by Hernández-Orallo et al. and their computational measures of information gain and reinforcement in inference processes [32, 31], alternatives to ours. One novelty in our approach based on the concept of algorithmic information dynamics $[57,8]$ is the precomputation of a very large set of small models able to explain small pieces of data, which assembled together in sequence, can build a full model of larger data. The method's precomputation allows practical applications in linear time by implementing a look-up table [37, 46], which combined with classical information theory provides key hints on the algorithmically random versus non-random nature of data. It has proven able to deal with features that are not only statistical in nature, features captured by other methods such as Shannon entropy, pattern recognition, or lossless compression, but also with more</p>
<p>convoluted features of an algorithmic nature that weaker computational approaches would miss [44]. By convoluted, convolution and deconvolution, we mean the original meaning of these words and not necessarily to the current field of convolutional neural networks. However, our approach can help understand and even help current deep learning techniques, including areas of convolution and deconvolution. A convolutional neural network (CNN) combines a set of primitive features extracted from data for classification purposes, deconvolving would involve opening the CNN and separating features by their most likely common generative sources.</p>
<p>Our approach builds upon previous work but is also based on our own work combined with the seminal ideas on counterfactuals of Judea Pearl et al. and with their interventionist do-calculus [1]. Pearl's et al. interventionist calculus is a part of his theory of probabilistic causality, itself part of the study of Bayesian networks. Our approach is a complete bottom-up approach based on algorithmic probability $[47,36,41]$, similar in nature to AIXI and alternative to or within the boundaries of computational mechanics but conceived to be practical from the start and designed for immediate application, without compromising on the power of the computational model used for the inductive inference. The deconvolution method introduced here is based on our own algorithmic causal calculus [57] or algorithmic information dynamics and involves finding the most likely (and thus shortest) generating mechanisms (computer programs) capable of reproducing an observation (data). For some other examples of areas in which these methods have found applications and been demonstrated to outperform computable measures see Refs. [44], [56], and [45], and for a non-trivial example in which entropic measures fail (by offering divergent descriptions of the same evolving system) see Ref. [53]. Moreover, behind the number or sequence of numbers matching observation/data and complexity, we also offer access to the rules generating the data that represent the generative model of the said data, that can thus be used for validation against present and future data, allowing predictions.</p>
<h1>2 Methods and Algorithms</h1>
<p>Cellular automata offer an optimal testbed because they are discrete dynamical systems able to illustrate an algorithm's inner workings because of their visual nature. They can be interpreted as 1-dimensional objects that produce 2-dimensional images when adding runtime, producing highly integrated 2-dimensional objects whose rows are strongly causally connected and are thus ideal testing cases. This does not mean, however, that the same methods cannot be applied to other images. After CA evolutions We then move to more applications demonstrating the method's capabilities on other objects of convoluted nature, such as complex networks.</p>
<p>The main intuition behind our algorithms is as follows. We look for pieces of</p>
<p>observed data that may come from the same source or generating mechanism using as a guide the length of the set of possible computer programs that produce different pieces of the data when decomposed. The main point is that if a computer program generates the data, different regions of the data would be explained by the same algorithm and that algorithm will also have the same program length for regions coming from the same generating mechanism.</p>
<h1>2.1 Cellular Automata</h1>
<p>A cellular automaton is a computer program that applies in parallel a global rule composed of local rules on a tape of cells with symbols (e.g. binary). Thoroughly studied in [59], Elementary Cellular Automata (or ECA) are one-dimensional cellular automata that take into consideration in their local rules the cell next to the centre and the centre cell.</p>
<p>Definition 2.1. A cellular automaton (or CA) is a tuple $\langle S,(\mathbb{L},+), T, f\rangle$ with a set $S$ of states, a lattice $\mathbb{L}$ with a binary operation + , a neighbourhood template $T$, and a local rule $f$.</p>
<p>The set of states $S$ is a finite set with elements $s$ taken from a finite alphabet $\sum$ with at least 2 elements.</p>
<p>Definition 2.2. The neighbourhood template $T=\left\langle\eta_{1}, \ldots, \eta_{m}\right\rangle$ is a sequence of $\mathbb{L}$. In particular, the neighbourhood of cell $i$ is given by adding the cell $i$ to each element of the template $T: T=\left\langle i+\eta_{1}, \ldots, i+\eta_{m}\right\rangle$. Each cell $i$ of the CA is in a particular state $c[i] \in S$. A configuration of the CA is a function $c: \mathbb{L} \rightarrow S$. The set of all possible configurations of the CA is defined as $S_{\mathbb{L}}$.</p>
<p>As a discrete dynamical system, the evolution of the $C A$ occurs in discrete time steps $t=0,1,2, \ldots, n$. The transition from a configuration $c_{t}$ at time $t$ to the configuration $c_{(t+1)}$ at time $t+1$ is induced by applying the local rule $f$. The local rule is to be taken as a function $f: S^{[T]} \rightarrow S$ which maps the states of the neighbourhood cells of time step $t$ in the neighbourhood template $T$ to cell states of the configuration at time step $t+1$ :</p>
<p>$$
c_{t+1}[i]=f\left(c_{t}\left[i+\eta_{1}\right], \ldots, c_{t}\left[i+\eta_{m}\right]\right)
$$</p>
<p>The general transition from configuration to configuration is called the global map and is defined as: $F: S^{\mathbb{L}} \rightarrow S^{\mathbb{L}}$. The code in Wolfram Language is available in the Sup. Inf.</p>
<h1>2.2 Enumeration of ECA rules</h1>
<p>In the case of 1-dimensional CA, it is common to introduce the radius of the neighbourhood template which can be written as $\langle-r,-r+1, \ldots, r-1, r\rangle$ and has length $2 r+1$ cells. With a given radius $r$ the local rule is a function $f: \mathbb{Z}<em _S_="[S]">{[S]}^{[S]^{(2 r+1)}} \rightarrow \mathbb{Z}</em>=256$ rules.}$ with $\mathbb{Z}_{[S]}^{[S]^{(2 r+1)}}$ rules. Elementary Cellular Automata or ECA, have a radius $r=1$ (closest neighbours), having the neighbourhood template $\langle-1,0,1\rangle$, meaning that the neighbourhood comprises a central cell. From this it follows that the rule space for ECA contains $2^{2^{3}</p>
<p>It is common to follow a lexicographic ordering scheme in the enumeration of CA and ECA as introduced by Wolfram [59]. According to this scheme, the 256 ECA rules can be encoded by only 8 -bits once part of the rule is fixed for all of them.</p>
<h3>2.3 Randomly Interacting Cellular Automata</h3>
<p>The use of interacting programs such as cellular automata as examples illustrating our algorithms requires us to define how the interaction happens. That is, how it is decided what set of rules apply at the intersection of the interacting CA. For instance, one of the 2 sets of local rules or a 3rd set of rules effectively defines a super cellular automaton that most likely is another cellular automaton in a larger rule-space (requiring more states to define the 2 sub-cellular automata and the interaction).</p>
<p>Our interacting Cellular Automaton model, as introduced in [6], is of such a nature that only one of then survives the contact between the CA colours, or else they both disappear. This means that there are only 3 possible solutions to what happens when cells of both CA come into contact in the same neighbourhood, and that there is in reality a single controlling CA with 3 colours that governs the interaction of the other 2 whose local rules dictate that: grey survives, black survives or there only remains a white cell. However, grey does not need to be displayed because it is only auxiliary and determines what happens when cells of the different CA come in contact in the same neighbourhood.</p>
<p>In particular we have it that $c_{t+1}\left(x_{n}\right)$ should be either white or black in case none of $c_{t}\left(x_{n-1}\right), c_{t}\left(x_{n}\right)$ and $c_{t}\left(x_{n+1}\right)$ is grey. Likewise, $c_{t+1}\left(x_{n}\right)$ should be either white or grey in case none of $c_{t}\left(x_{n-1}\right), c_{t}\left(x_{n}\right)$ and $c_{t}\left(x_{n+1}\right)$ is black. For example, let $N=\left\langle c_{t}\left(x_{j-1}\right), c_{t}\left(x_{j}\right), c_{t}\left(x_{j+1}\right)\right\rangle$ and $M=\left\langle c_{t^{\prime}}\left(x_{i-1}\right), c_{t^{\prime}}\left(x_{i}\right), c_{t^{\prime}}\left(x_{i+1}\right)\right\rangle$ be 2 different mixed neighbourhoods. Then there is no correlation between the random values of $c_{t+1}\left(x_{j}\right)$ and of $c_{t^{\prime}+1}\left(x_{i}\right)$. Note that we impose this independence both in case $N=M$ (so that the difference is only reflected in either the location (that is $i \neq j$ ) or in the time $\left(t \neq t^{\prime}\right)$ ) and in case $N \neq M$. In particular the mixed neighbourhood $\langle 2,2,1\rangle$ may sometimes yield a 0 , sometimes a 1 and at yet other times a 2 .</p>
<p>In [4] and [5] these interactions are investigated in terms of evolution and complexity. In the Sup. Inf. we also provide 2 more examples of interacting CA and a comparison with other methods and measures, in particular the recently introduced Partial Information Decomposition [2] using both classical Mutual Information [2] and the Normalized Compression Distance [17]. One striking difference with these other methods, is that ours does not require any associated probability distributions or arbitrary parameter choice, including the terminating criterion explained in Subsection 2.5.1.</p>
<h1>2.4 Graph Complexity</h1>
<p>The concept of Algorithmic Probability (and of Levin's semi-measure, and Universal Distribution associated with it) has been introduced as a method for approximating algorithmic complexity based on the frequency of the patterns occurring in the adjacency matrix of a network. The measure applied to labelled graphs has been proven to be a tight upper bound of the algorithmic complexity of unlabelled graphs and therefore quite invariant to particular adjacency matrix choice [52].</p>
<p>More precisely, the algorithmic probability $[47,41,36]$ of a subgraph $H \subseteq G$ is a measure of algorithmic probability based on the frequency of a random computer program $p$ producing $H$ when run on a 2-dimensional tape universal (prefix-free ${ }^{1}$ ) Turing machine $U$ also referred to as a Turmite [28]. That is,</p>
<p>$$
m(G)=\sum_{p: U(p)=H \subseteq G} 1 / 2^{|p|}
$$</p>
<p>The probability semi-measure $m(G)$ is related to algorithmic complexity $C(G)$ in that $m(G)$ is at least the maximum term in the summation of programs $m(G) \geq$ $2^{-C(G)}$, given that the shortest program carries the greatest weight in the sum.</p>
<p>The algorithmic complexity (also known as Kolmogorov-Chaitin complexity) [40, 36] is the length of the shortest computer program that reproduces the data from its compressed form when running on a universal Turing machine.</p>
<p>The Coding Theorem $[47,41]$ establishes the connection between $m(G)$ and $C(G)$ as $\left|-\log <em 2="2">{2} m(G)-C(G)\right|&lt;c$, where $c$ is some fixed constant independent of $s$. The theorem implies that one can estimate the algorithmic complexity of a graph from the frequency of production by running random programs and applying the Coding theorem: $C(G)=-\log </em> m(G)+c$. We call this approach the Coding Theorem Method (CTM). The Coding theorem establishes that graphs produced with lower frequency by random computer programs have higher algorithmic complexity, and</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>vice versa. Applying the so-called Coding Theorem Method (CTM) and Block Decomposition Method (BDM), as introduced in [37, 46, 50, 56], based on estimations to algorithmic probability involves running a very large number of small computer programs according to a quasi-lexicographic order (from smaller to larger program size) to produce an empirical approximation to the Universal Distribution $m$. The BDM of a graph then consists in decomposing the adjacency matrix of a graph into subgraphs of sizes for which complexity values estimated by CTM are available, then reconstructing a sequence model that can explain the full $G$ by assembling in sequence the smaller models that produce all the parts of $G$ in combination with rules from classical information theory, as follows:</p>
<p>$$
C(G)=\sum_{\left(r_{u}, n_{u}\right) \in A d j(G)<em 2="2">{d \times d}} \log </em>\right)
$$}\left(n_{u}\right)+C\left(r_{u</p>
<p>where $A d j(G)<em u="u">{d \times d}$ represents the set with elements $\left(r</em>$ its multiplicity (number of occurrences). As can be seen from the formula, repeated subgraphs only contribute to the complexity value with the subgraph BDM complexity value once plus a logarithmic term as a function of the number of occurrences. This is because the information content of subgraphs is only sub-additive, as one would expect from the growth of their description lengths. Applications of $m(G)$ and $C(G)$ have been explored in $[37,46,50]$, and include applications to graph theory and complex networks [49] and in [50] where the technique was first introduced.}, n_{u}\right)$, obtained when decomposing the adjacency matrix of $G$ into all subgraphs of size $d$ contained in $G$. In each $\left(r_{u}, n_{u}\right)$ pair, $r_{u}$ is one such submatrix of the adjacency matrix and $n_{u</p>
<p>The only parameter used for the application of BDM, as suggested in [56], is to set the overlapping of the decomposition to the maximum 12 bits for strings and 4 square bits for arrays, given the current best CTM approximations [46] from an empirical distribution based on all Turing machines with up to 5 states, with no string/array overlapping in the decomposition for maximum efficiency (as it runs in linear time), and for which the error (due to boundary conditions) has been shown to be bounded [56].</p>
<p>However, the algorithm introduced here is independent of the method used to approximate algorithmic complexity, such as BDM. BDM assigns an index associated with the size of the most likely generating mechanism producing the data according to Algorithmic Probability [47]. BDM is capable of capturing features in data beyond statistical properties [56, 53], and thus represents an improvement over classical information theory. Because finding the program that reproduces a large object is computationally very expensive - even to approximate-BDM finds short candidate programs using another method $[37,46]$ that finds and reproduces fragments of the original object and then puts them together as a candidate algorithmic model of the whole object $[56,50]$. These short computer programs are effectively</p>
<p>candidate models explaining each fragment, with the long finite sequence of short models being itself a generating mechanistic model.</p>
<h1>2.5 Deconvolution Algorithms</h1>
<p>The aim of the deconvolution algorithm is to break a dataset into groups that do not share certain features (essentially causal clustering and algorithmic partition by probable generative mechanism, completely different from traditional clustering and partition in machine learning approaches). Usually these characteristics are a parameter to maximise, but ultimately the purpose is to distinguish components that are generated similarly from those that are generated differently. In informationtheoretic terms the question is therefore as follows: What are the elements (e.g. nodes or edges) that can break a network into the components that maximise their algorithmic information content, that is, those elements that preserve the information about the underlying programs generating the data?</p>
<p>Let $G$ be a graph and let $E=E(G)$ denote its set of edges. Let $G \backslash e$ denote the graph obtained after deleting an edge $e$ from $G$. The information contribution of $e$ to $G$ is given by $I(G, e):=C(G)-C(G \backslash e)$. A positive information contribution corresponds to information loss and a negative contribution to information gain. Here we wish to find the subset $F \subseteq E$ such that the removal of the edges in $F$ disconnects $G$ into $N$ components and minimises the loss of information among all subsets of edges, i.e. the subset such that $I(G, F) \leq I(G, S)$ for all $S \subseteq E$. Let us denote the number of connected components of $G$ by $k(G)$. Algorithm 1 allows us to obtain the subgraph $(V, E \backslash F)$ subject to the above conditions. The desired subset of edges is then given by $F=E(G) \backslash E(\operatorname{Deconvolve}(G, N))$.</p>
<div class="codehilite"><pre><span></span><code><span class="n">Algorithm</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="n">Causal</span><span class="w"> </span><span class="n">deconvolution</span><span class="w"> </span><span class="n">algorithm</span>
<span class="w">    </span><span class="n">function</span><span class="w"> </span><span class="n">Deconvolve</span><span class="w"> </span>\<span class="p">((</span><span class="n">G</span><span class="p">,</span><span class="w"> </span><span class="n">N</span><span class="p">),</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span>\<span class="n">leq</span><span class="w"> </span><span class="n">k</span><span class="p">(</span><span class="n">G</span><span class="p">)</span><span class="w"> </span>\<span class="n">leq</span><span class="w"> </span><span class="n">N</span><span class="w"> </span>\<span class="n">leq</span><span class="o">|</span><span class="n">V</span><span class="p">(</span><span class="n">G</span><span class="p">)</span><span class="o">|</span>\<span class="p">)</span>
<span class="w">        </span><span class="k">while</span><span class="w"> </span>\<span class="p">(</span><span class="n">k</span><span class="p">(</span><span class="n">G</span><span class="p">)</span><span class="o">&lt;</span><span class="n">N</span>\<span class="p">)</span><span class="w"> </span><span class="n">do</span>
<span class="w">            </span><span class="n">informationLoss</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">leftarrow</span><span class="w"> </span>\<span class="n">varnothing</span>\<span class="p">)</span>
<span class="w">            </span><span class="o">//</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">each</span><span class="w"> </span><span class="n">edge</span><span class="w"> </span>\<span class="p">(</span><span class="n">e</span>\<span class="p">)</span>
<span class="w">            </span><span class="k">for</span><span class="w"> </span>\<span class="p">(</span><span class="n">e</span><span class="w"> </span>\<span class="ow">in</span><span class="w"> </span><span class="n">E</span><span class="p">(</span><span class="n">G</span><span class="p">)</span>\<span class="p">)</span><span class="w"> </span><span class="n">do</span>
<span class="w">                </span><span class="k">if</span><span class="w"> </span>\<span class="p">(</span><span class="n">I</span><span class="p">(</span><span class="n">G</span><span class="p">,</span><span class="w"> </span><span class="n">e</span><span class="p">)</span><span class="o">&gt;</span><span class="mi">0</span>\<span class="p">)</span><span class="w"> </span><span class="n">then</span>
<span class="w">                    </span><span class="o">//</span><span class="w"> </span><span class="n">store</span><span class="w"> </span><span class="n">information</span><span class="w"> </span><span class="n">contribution</span><span class="w"> </span><span class="n">into</span><span class="w"> </span><span class="n">informationLoss</span>
<span class="w">                    </span><span class="n">informationLoss</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">leftarrow</span>\<span class="p">)</span><span class="w"> </span><span class="n">informationLoss</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">cup</span>\<span class="p">{</span><span class="n">I</span><span class="p">(</span><span class="n">G</span><span class="p">,</span><span class="w"> </span><span class="n">e</span><span class="p">)</span>\<span class="p">}</span>\<span class="p">)</span>
<span class="w">            </span><span class="o">//</span><span class="w"> </span><span class="n">calculate</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">minimal</span><span class="w"> </span><span class="n">information</span><span class="w"> </span><span class="n">loss</span><span class="w"> </span><span class="n">across</span><span class="w"> </span><span class="n">all</span><span class="w"> </span><span class="n">edges</span>
<span class="w">            </span>\<span class="p">(</span>\<span class="nb">min</span><span class="w"> </span><span class="n">L</span><span class="w"> </span><span class="n">o</span><span class="w"> </span><span class="n">s</span><span class="w"> </span><span class="n">s</span><span class="w"> </span>\<span class="n">leftarrow</span><span class="w"> </span>\<span class="nb">min</span>\<span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="n">informationLoss</span><span class="p">)</span>
<span class="w">            </span><span class="o">//</span><span class="w"> </span><span class="n">remove</span><span class="w"> </span><span class="n">all</span><span class="w"> </span><span class="n">candidate</span><span class="w"> </span><span class="n">edges</span><span class="w"> </span><span class="n">from</span><span class="w"> </span>\<span class="p">(</span><span class="n">G</span>\<span class="p">)</span>
<span class="w">            </span>\<span class="p">(</span><span class="n">G</span><span class="w"> </span>\<span class="n">leftarrow</span><span class="w"> </span><span class="n">G</span><span class="w"> </span>\<span class="n">backslash</span>\<span class="p">{</span><span class="n">e</span><span class="w"> </span>\<span class="ow">in</span><span class="w"> </span><span class="n">E</span><span class="p">(</span><span class="n">G</span><span class="p">):</span><span class="w"> </span><span class="n">I</span><span class="p">(</span><span class="n">G</span><span class="p">,</span><span class="w"> </span><span class="n">e</span><span class="p">)</span><span class="o">=</span>\<span class="n">operatorname</span><span class="p">{</span><span class="n">minLoss</span><span class="p">}</span>\<span class="p">}</span>\<span class="p">)</span>
<span class="w">        </span><span class="k">return</span><span class="w"> </span>\<span class="p">(</span><span class="n">G</span>\<span class="p">)</span>
</code></pre></div>

<p>The only parameter that Algorithm 1 requires is the number of components into which an object will be decomposed. However, there is a natural way to find the optimal terminating step and therefore the number of maximum possible components that minimise the sum of the lengths of the candidate generating mechanisms, making the algorithm truly parameter-free, as it is not required to have a preset number of desired components. $N$ should be chosen to be equal to the maximum number of components into which the graph should be broken. However, an alternative Algorithm 2, determines the optimal number of components and requires no $N$ parameter.</p>
<p>Before introducing the terminating criterion (c.f. next section) for the number of components, let's analyse what it might mean for 2 components $s_{1}$ and $s_{2}$ to have the same algorithmic information content $C\left(s_{1}\right)=C\left(s_{2}\right)$. Clearly that subcomponents $s_{1}$ and $s_{2}$ have the same algorithmic complexity (an integer-or a real value if using AP-based BDM-indicating the size of the approximated minimal program) does not imply that the 2 components are generated by exactly the same generating mechanism. However, because of the exponential decay of the algorithmic probability of an increasingly random object, we have it that the less random it is, the exponentially more likely it is that the underlying mechanism will be the same (see Fig. 5C). This is because there are exponentially fewer short programs than long ones. For example, in the extreme case of connected graphs, we have it that the complete graph denoted by $K_{n}$ has the smallest possible algorithmic complexity $\sim \log (n)$. If $C\left(s_{1}\right)=C\left(s_{2}\right) \sim \log (n)$ then $s_{1}$ and $s_{2}$ are, with extremely high probability, generated by the same algorithm that generates either the complete graph or the empty graph (with the same lowest algorithmic complexity, as it requires no description other than either all nodes connected or all nodes disconnected). Conversely, if $C\left(s_{1}\right)=C\left(s_{2}\right)$ but $C\left(s_{2}\right)$ and $C\left(s_{1}\right)$ depart from $\log (n)$ (and approximate algorithmic randomness), then the likelihood of being generated by the same algorithm exponentially vanishes. So the information regarding both the algorithmic complexity of the components and their relative size sheds light on the candidate generating mechanisms and is less likely to coincide 'by chance' for non-trivial cases.</p>
<h1>2.5.1 Algorithm Terminating Criterion</h1>
<p>The immediate question is where we should stop breaking down a system into its causal components. The previous section suggests a terminating criterion. Let $S$ be the object which has been produced by $N$ mostly independent generative mechanisms. We decompose $S$ into $n$ parts $s_{1}, \ldots, s_{n}$ in such a way that each $s_{i}$, $i \in{1 \ldots n}$ has an underlying generating mechanism found by running the algorithm iteratively for increasing $n$, But after each iteration we calculate the minimum of the differences in algorithmic complexity among all subcomponents. The algorithm should then stop where the number of subcomponents is exactly $N$ when the</p>
<p>sum of the lengths - the estimated algorithmic complexity-of each of the programs will diverge from the expected $\log (N)$ because the length of the individual causal mechanisms producing each new component will be breaking a component that could previously be explained by the causal mechanism at a previous iteration of the algorithm. An implementation of this idea for a graph is shown in Algorithm 2.</p>
<p>As a trivial example, let's take the string $1^{n}$, where $S^{n}$ means that the pattern $S$ is repeated $n$ times. After application of the algorithm, the terminating criterion will suggest that $1^{n}$ cannot be broken down into smaller segments, each with a different causal generating mechanism, the sum of whose total length will be shorter than the length of the generating mechanism producing $1^{n}$ itself. This is because the sum of the length of the shortest programs $\sum_{i}\left|p_{i}\right|$ running on a universal Turing machine generating segments of $1^{n}$ of length $m_{i}&lt;n$ each, such that the concatenation $\cup_{i=1} p_{i}=1^{n}$, will be strictly greater than $C\left(1^{n}\right)$, given that each $p_{i}$ halting criterion will require $i \log m_{i}$ bits more than $C\left(1^{n}\right)$.</p>
<p>In the case of Fig. 2, the terminating criterion retrieves $N=3$ components from the 2 interacting ECA (rule 60 and 110). This does not contradict the fact that we started from 2 generating mechanisms, because there are 3 clear regimes that are actually likely to be reproducible by 3 different generating mechanisms, as suggested by the deconvolution algorithm itself, and as found in [43], where it has been shown that rule 110 can be emulated by the composition of 2 simpler ECA rules (rules 51 and 118). As seen in Fig. 2, among the possible causal partitions, $N=2$ successfully deconvolves ECA rule 60 from rule 110 on the first run, with a stronger difference than the difference found between $N=3$ components when breaking rule 110 into its 2 different regimes.</p>
<p>The term $\varepsilon$ is related to the number of components $N$ from Algorithm 1, or rather substitutes for $N . \varepsilon$ is an auxiliary cutoff value that determines when to stop the algorithm without making an arbitrary choice of number of subcomponents $N . \varepsilon$ is 0 for the theoretical cutoff value $\log (2)$ that determines the effect of perturbations performed on the network. If removing an edge has an effect greater than $\log (2)$, then such an edge does not belong to the same underlying algorithm explaining the rest of $G$, given that the program size of a deterministic object generated by the same computer program does not change by more than $\log (2)$ and thus nor does its algorithmic complexity. In contrast, if the perturbation by edge removal has a loss of at most $\log (2)$ bits, then it means that it is likely to be reconstructed by the original computer program because $\log (2)$ is the growth in the description of a computer program (or a deterministic system) that accounts only for running time. In other words, if the perturbation is above $\log (2)$, it means that such an edge may have disconnected 2 or more causally independent components with different computer programs likely able to explain each different subcomponent with fewer bits than when keeping those components together using such an edge.</p>
<p>In practice, however, such a strict cutoff value does not occur, so $\varepsilon$ accounts</p>
<div class="codehilite"><pre><span></span><code><span class="n">Algorithm</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="n">Causal</span><span class="w"> </span><span class="n">deconvolution</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">terminating</span><span class="w"> </span><span class="n">criterion</span>
<span class="w">    </span><span class="n">function</span><span class="w"> </span><span class="n">DeConvOLVE</span><span class="w"> </span>\<span class="p">((</span><span class="n">G</span><span class="p">,</span><span class="w"> </span>\<span class="n">varepsilon</span><span class="p">)</span>\<span class="p">)</span>
<span class="w">        </span><span class="n">informationLoss</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">leftarrow</span><span class="w"> </span>\<span class="n">varnothing</span>\<span class="p">)</span>
<span class="w">        </span><span class="o">//</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">each</span><span class="w"> </span><span class="n">edge</span><span class="w"> </span>\<span class="p">(</span><span class="n">e</span>\<span class="p">)</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span>\<span class="p">(</span><span class="n">e</span><span class="w"> </span>\<span class="ow">in</span><span class="w"> </span><span class="n">E</span><span class="p">(</span><span class="n">G</span><span class="p">)</span>\<span class="p">)</span><span class="w"> </span><span class="n">do</span>
<span class="w">            </span><span class="k">if</span><span class="w"> </span>\<span class="p">(</span><span class="n">I</span><span class="p">(</span><span class="n">G</span><span class="p">,</span><span class="w"> </span><span class="n">e</span><span class="p">)</span><span class="o">&gt;</span><span class="mi">0</span>\<span class="p">)</span><span class="w"> </span><span class="n">then</span>
<span class="w">                </span><span class="o">//</span><span class="w"> </span><span class="n">store</span><span class="w"> </span><span class="n">information</span><span class="w"> </span><span class="n">contribution</span><span class="w"> </span><span class="n">into</span><span class="w"> </span><span class="n">informationLoss</span>
<span class="w">                </span><span class="n">informationLoss</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">leftarrow</span>\<span class="p">)</span><span class="w"> </span><span class="n">informationLoss</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">cup</span>\<span class="p">{</span><span class="n">I</span><span class="p">(</span><span class="n">G</span><span class="p">,</span><span class="w"> </span><span class="n">e</span><span class="p">)</span>\<span class="p">}</span>\<span class="p">)</span>
<span class="w">            </span><span class="k">for</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">operatorname</span><span class="p">{</span><span class="n">loss</span><span class="p">}</span><span class="w"> </span>\<span class="ow">in</span>\<span class="p">)</span><span class="w"> </span><span class="n">informationLoss</span><span class="w"> </span><span class="n">do</span>
<span class="w">            </span><span class="n">difference</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">leftarrow</span><span class="w"> </span><span class="mi">0</span>\<span class="p">)</span>
<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="o">|</span><span class="n">informationLoss</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">mid</span><span class="o">&gt;</span><span class="mi">1</span>\<span class="p">)</span><span class="w"> </span><span class="n">then</span>
<span class="w">                </span><span class="o">//</span><span class="w"> </span><span class="n">copy</span><span class="w"> </span><span class="n">maximum</span><span class="w"> </span><span class="n">loss</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">maxLoss</span>
<span class="w">                </span>\<span class="p">(</span>\<span class="n">operatorname</span><span class="p">{</span><span class="n">maxLoss</span><span class="p">}</span><span class="w"> </span>\<span class="n">leftarrow</span><span class="w"> </span>\<span class="nb">max</span>\<span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="n">informationLoss</span><span class="p">)</span>
<span class="w">                </span><span class="n">informationLoss</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">leftarrow</span>\<span class="p">)</span><span class="w"> </span><span class="n">informationLoss</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">backslash</span>\<span class="p">{</span>\<span class="n">operatorname</span><span class="p">{</span><span class="n">maxLoss</span><span class="p">}</span>\<span class="p">}</span>\<span class="p">)</span>
<span class="w">                </span><span class="o">//</span><span class="w"> </span><span class="n">calculate</span><span class="w"> </span><span class="n">difference</span><span class="w"> </span><span class="n">between</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">old</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">new</span><span class="w"> </span><span class="n">maxima</span>
<span class="w">                </span><span class="n">difference</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">leftarrow</span>\<span class="p">)</span><span class="w"> </span><span class="n">maxLoss</span><span class="w"> </span>\<span class="p">(</span><span class="o">-</span>\<span class="nb">max</span>\<span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="n">informationLoss</span><span class="p">)</span>
<span class="w">                </span><span class="o">//</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">difference</span><span class="w"> </span><span class="n">significantly</span><span class="w"> </span><span class="n">departs</span><span class="w"> </span><span class="n">from</span><span class="w"> </span>\<span class="p">(</span>\<span class="nb">log</span><span class="w"> </span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>\<span class="p">)</span>
<span class="w">                </span><span class="k">if</span><span class="w"> </span><span class="o">|</span><span class="n">difference</span><span class="w"> </span>\<span class="p">(</span><span class="o">-</span>\<span class="nb">log</span><span class="w"> </span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="w"> </span>\<span class="n">mid</span><span class="o">&gt;</span>\<span class="n">varepsilon</span>\<span class="p">)</span><span class="w"> </span><span class="n">then</span>
<span class="w">                    </span><span class="o">//</span><span class="w"> </span><span class="n">remove</span><span class="w"> </span><span class="n">all</span><span class="w"> </span><span class="n">candidate</span><span class="w"> </span><span class="n">edges</span><span class="w"> </span><span class="n">from</span><span class="w"> </span>\<span class="p">(</span><span class="n">G</span>\<span class="p">)</span>
<span class="w">                </span>\<span class="p">(</span><span class="n">G</span><span class="w"> </span>\<span class="n">leftarrow</span><span class="w"> </span><span class="n">G</span><span class="w"> </span>\<span class="n">backslash</span>\<span class="p">{</span><span class="n">e</span><span class="w"> </span>\<span class="ow">in</span><span class="w"> </span><span class="n">E</span><span class="p">(</span><span class="n">G</span><span class="p">):</span><span class="w"> </span><span class="n">I</span><span class="p">(</span><span class="n">G</span><span class="p">,</span><span class="w"> </span><span class="n">e</span><span class="p">)</span><span class="o">=</span>\<span class="nb">max</span><span class="w"> </span><span class="p">(</span>\<span class="n">text</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="n">informationLoss</span><span class="w"> </span><span class="p">})</span>\<span class="p">}</span>\<span class="p">)</span>
</code></pre></div>

<p>for a difference or error not far from $\log (2)$. Moreover, $\varepsilon$ can be estimated from the sequential information differences calculated from the absolute distances between the differences of consecutive values in the information signature (the list of information values for all edges sorted by maximum contribution) and its deviation from $\log (2)$, so no cut is made for an edge with information difference below $\log (2)+\varepsilon$ (see Fig. 4).</p>
<h1>2.5.2 Time Complexity</h1>
<p>The algorithms for network deconvolution introduced in this section run in polynomial time. Let $M$ denote the number of edges of the graph $G$. The brute force algorithm for this problem searches the edge such that its removal minimises the loss of information and deletes it, repeating this process for all edges of $G$ until $N$ subcomponents are reached, which has a worst-case time complexity of $O\left(M^{2}\right)$. Algorithm 1 is different from the brute force approach in that edges with equal minimal contributions to the loss of information are not deleted sequentially but all at once, but its time complexity is also of $O\left(M^{2}\right)$. The outer loop that verifies whether the number of desired subcomponents is reached is removed in Algorithm 2, allowing us</p>
<p>to find the optimal terminating step of the deconvolution in time $O(M)$.</p>
<h1>3 Numerical Experiments</h1>
<p>Behind our deconvolution methods is the idea that we can find a set of small computer rules or programs able to reconstruct a piece of data, Figs. 1C-D illustrate this. In the deconvolution of a string generated by 2 different mechanisms (Fig. 1AB ) and thus in 2 different regimes (random versus non-random) computer programs such as those in Figs. 1C-D help deconvolute the string. We then do the same in all other cases but extending the number of degrees of freedom of a Turing machine tape. Notice that from our methods as illustrated in Fig. 1A-B, the methods are invariant to direction, given that the algorithmic probability and complexity of a string and its reversal (and set of computable transformations) preserve the complexity and mechanistic origin of each object (up to a small constant which is the length of the computable transformation).</p>
<h3>3.1 Decomposition of Sequences and Space-time Diagrams</h3>
<p>We tested the causal deconvolution algorithm on different types of objects, in order to explore and explain its applicability, advantages and limitations. We start with the simplest version of an object which conveys information, a string, and move later to consider richer objects such as networks.</p>
<p>We will use different programs to produce different parts of a string, that is, a program $p$ to generate segment $s_{1}$ and a program $p^{\prime}$ to generate segment $s_{2}$ put next to each other. Clearly the string has been generated by 2 generating mechanisms ( $p$ and $p^{\prime}$ ). Now we use the algorithm to deconvolve the string and find the number of generating mechanisms and most likely model mechanisms (the programs themselves) inducing a form of algorithmic partition based on the likelihood of each segment being produced by different generating mechanisms.</p>
<p>Figs. 1A-E illustrate how strings that have short generating mechanisms are significantly and consistently more sensitive to perturbations. The resulting string is 010101010101010101010101010101010101010101010101010111010010101010000000 1001100111100110000011100110 with the colours corresponding to the parts suggested by the different regimes, according to their algorithmic contribution and the segment's resilience in the face of perturbations (by deletion and replacement) to the original string. Behind every real number approximating the algorithmic complexity of a string, there is the discovery of a large set of generating programs when using the Algorithmic Probability (AP)-based measure BDM producing the object.</p>
<p>We not only could find the number of mechanisms correctly (Figs. 1A and B) but also the candidate programs (which for this trivial example are exactly the original) that generate each segment (Figs. 1C-E) by seeking the shortest computer</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Proof of concept applied to a binary string composed of 2 segments with different underlying generating mechanisms (computer programs). A: Log plot of complexity estimation of a regular segment (blue) consisting of the repetition of ' 01 ' 25 times followed by a random-looking segment (red). B: Log plot reversing the order of A, yet preserving the qualitative behaviour of the different segments. C: The code of the smallest generating program (a non-terminating Turing machine) depicted visually (states are arrows in different directions), producing the string of $01^{n}$ for any $n$ ( 0 is white and 1 is orange) starting from a blank tape, as shown in the space-time diagram (E). D: the same computer program as a state diagram. F: An illustration of a very simple case of interacting programs with one dominating the other and each with different generating mechanism (ECA rules 255 v 110) each running for 60 steps using interacting rule 531441 as described and explained in detail in the Sup. Inf. G: Algorithmic information footprint: every pixel is perturbed by flipping its value and evaluating its contribution to the original object coloured accordingly: If grey, it makes the no contribution, blue represents a low contribution and red high contribution (its presence contributes to its algorithmic randomness).</p>
<p>programs in a bottom-up approach [37, 46]. Finding the shortest programs is, however, secondary, because we only care about the different explanatory power that different programs have to explain the data in full or in part, pinpointing the different causal nature of the segments and helping in the deconvolution of the original observation.</p>
<p>Figs. 1C-E depict the computer program (a non-terminating Turing machine) that is found when calculating the BDM of the $01^{n}$ string. The BDM approximation to the algorithmic complexity of any $01^{n}$ string is thus the number of small computer programs that are found capable of generating the same string or, conversely (via the algorithmic Coding theorem, see $[56,50]$ ), the length of the shortest program producing the string. For example, the string $01^{n}$ was trivially found to be generated by a large number of small computer programs (in Fig. 1C,D depicted a non-terminating Turing machine with E its output) using our algorithmic methods (as opposed to, e.g., using lossless compression, which would only obfuscate the possible generating model) with only 2 rules out of $2 \times 2$ rules for the size of Turing machine with only 2 states and 2 symbols and no more, thus of very low algorithmic complexity compared to, e.g., generating a random-looking string that would require a more complex (longer) computer program. The computer program of a truly random string will grow in proportion to the length of the random string, but for a low complexity string such as $01^{n}$, repeated any number of times $n$, the length of the computer program is of (almost) fixed size, growing only by $\log (n)$ if the computer program is required to stop after $n$ iterations. In this case $01^{n}$ is a trivial example with a strong statistical regularity whose low complexity could be captured by applying Shannon entropy alone on blocks of size 2.</p>
<p>Figs. 1F-G illustrate how the algorithm can separate regions produced by generating mechanisms of different algorithmic information content by observing their space-time dynamics, thereby contributing to the deconvolution of regions that are produced by different generating mechanisms. In this example both programs are sufficiently robust to not break down (see Sup. Inf.) when they interact with each other, with rule 110 prevailing over 255 . Yet, in the general case it is not always easy to tell these mechanisms apart. In more sophisticated examples, such as in Figs. 2D-E, we see how the algorithm can break down contiguous regions separating an object into 2 major components corresponding to the different generating computer programs that are intertwined and actively interacting with each other. The experiment was repeated 20 times with programs with differing qualitative (e.g. Wolfram class) behaviour.</p>
<p>Fig. 2F demonstrates that perturbations to regions in red have a more random effect after application and are thus by themselves less algorithmically random. When regions are of the same algorithmic complexity they are likely to be generated by similar algorithms, from algorithms that are of similar minimal length. The removal of pixels in the blue regions move the interacting system away from randomness and</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: A: The output of 2 different intertwined programs (ECA rules 60 and 110) with qualitatively complex output behaviour ( 11 to 60 steps depicted here, from a random initial condition) interacting with each other (one of which has been proven to be Turing-universal and the other has also been conjectured to be universal [59]), each producing structures of a similar type that, from an observer's perspective, are difficult to distinguish (see Subfigure C) as is artificially done in B (knowing which pixel is generated by which rule). C: What an observer of the last runtime would see in the form of a stream of bits with no clear statistical distinction. D: The algorithm pinpoints the regions of neutral, positive and negative, with the contiguous largest blue component segmenting the image into two. E: only negative vs positive causal contributions where both Shannon entropy and a popular lossless compression algorithms fail (see Sup. Inf.). F: Sanity check/validation: Statistically significant quantitative differences among the parts after application of the algorithm as illustrated in E among apparently weak qualitative differences as illustrated in Subfig. A. More cases are provided in the Sup. Inf.</p>
<p>are themselves more algorithmically random. Blue structures on the left hand side correspond to large triangles occurring in ECA rule 110 that are usually used to compute and transfer information in the form of particles. However, triangular patterns transfer information in a limited way because their light cone of influence reduces at the greatest possible speed of the automaton, and they are assigned an absolute neutral information value. Absolute neutral values are those closest to 0 . Once separated, the 2 regions have clearly different algorithmic characteristics given by their causal perturbation sensitivity, with the right hand side being more sensitive to both random and non-random perturbations. Moreover, Fig. 2F shows results compatible with the theoretical expectation and findings in [57] where a measure of reprogrammability associated with the number and magnitude of elements that can move a dynamical system towards or away from randomness was introduced and shown to be related to fundamental properties of the attractor space of the system.</p>
<p>Fig. 2C-F shows, for example, that by iterating the deconvolution algorithm not only do the 2 main components of the image correspond to the 2 generating ECA rules, but a second application of the algorithm would produce a third or more components corresponding to further resilient features generated by the rules, which can be considered rules themselves within a smaller rule (state/symbol) space. However, in the deconvolved observations the interacting rule determining how 2 or more rules may interact effectively constitutes a third global rule to which the algorithm has no direct access, or an apparent region in the observed window.</p>
<h1>3.2 Network Deconvolution</h1>
<p>Classification can usually be viewed as solving a problem which has an underlying tree structure according to some measure of interest. One way to think of optimal classification is to discover a tree structure at some level of depth, with tree leaves closer to each other when such objects have a common or similar causal mechanism and for which no feature of interest has been selected. Fig. 3 illustrates how the algorithm may partition data, in this case starting from a trivial example that breaks complete $K$-ary trees. Traditionally, partitioning is induced by an arbitrary distance measure of interest that determines the connections in a tree, with elements closer to a cluster centre connected by edges. The algorithm breaks the trees (see Fig. 3) into as many components as desired by iterating over remaining elements if required until the number of desired components is obtained or the terminating criterion is applied (c.f. Subsection 2.5.1). Figs. 3A,B provide examples illustrating how to maximise topological symmetry. The algorithm can be applied, without loss of generalisation, to any non-trivial graph, as in Figs. 3C,D or on any dataset for that matter.</p>
<p>Figs. 4 illustrate the algorithm and terminating criterion starting from an artificial graph composed of several graphs ( 2 simple and one E-R random: a small</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: A,B: Forced deconvolution of a tree by minimisation of graph algorithmic information loss thereby maximising causal resemblance of the resultant components (hence causal clustering). Depicted are the components of a $K$-ary trees of size 6 (A) and 10 (B) and their resulting graphs after one iteration of the deconvolution algorithm. C: Deconvolution of 20 cases of scale-free (S-F) networks generated by preferential attachment randomly connected to a complete graph. Negative edges break down the original graph into components corresponding to the different underlying generating mechanisms. Histograms correspond to each network according to the decomposition showing the expected degree distribution of the 2 resulting major components. D: Deconvolution of 20 cases of random graphs (E-R) connected to scale-free (S-F) networks (here depicted a typical case). E: The algorithm first separates the subcomponents with the largest algorithmic difference, followed by other subcomponents hence providing a natural hierarchy of source likelihood.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: A: Convoluted graph composed of 3 subgraphs produced by different generating mechanisms. B: Deconvolution of (A) by Algorithm 2, keeping edges connected if their removal does not produce a change in the algorithmic complexity of the original graph larger than $\log (2)+\varepsilon$. C: The information signature (red line with circle markers) illustrates the distribution of information values for each edge ( $x$-axis) in the original graph (A). Also shown is a line of the differences of consecutive values of the signature multiplied by -1 (blue line with square markers), indicating the breaking points (the peaks that mark the edges to be deleted) with, in this case, four peaks/values clearly standing out beyond the $\log (2)+\varepsilon$ line (orange rhombus) breaking the signature corresponding to each subgraph forming with high accuracy, thereby deconvolving the original graph (A) into the subgraphs (largest components) that are most likely generated by the same causal/algorithmic source. D: Signatures decomposition according to the breaking points found in (C) giving the colours to the subgraphs in (B) with results matching the theoretical expectation.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ The group of valid programs forms a prefix-free set (no element is a prefix of any other, a property necessary to keep $0&lt;m(G)&lt;1$ ). Because $m(G)&lt;1, m(s)$ is called a semi-measure, because not all programs halt and thus it never reaches 1 .&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
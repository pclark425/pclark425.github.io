<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-445 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-445</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-445</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-18.html">extraction-schema-18</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <p><strong>Paper ID:</strong> paper-214693121</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2003.12206v3.pdf" target="_blank">Improving Reproducibility in Machine Learning Research (A Report from the NeurIPS 2019 Reproducibility Program)</a></p>
                <p><strong>Paper Abstract:</strong> One of the challenges in machine learning research is to ensure that presented and published results are sound and reliable. Reproducibility, that is obtaining similar results as presented in a paper or talk, using the same code and data (when available)</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e445.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e445.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>under-specification_of_methods</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Under-specification or misspecification of model/training/procedure in papers vs code</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Papers frequently omit or ambiguously describe essential implementation details (hyperparameters, optimization, initialization, preprocessing), causing reimplementations or runs of provided code to diverge from paper claims and leading to failed or inconsistent reproductions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Improving Reproducibility in Machine Learning Research (A Report from the NeurIPS 2019 Reproducibility Program)</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ML research experimental pipeline / paper-to-code reproduction workflow</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>The end-to-end experimental workflow in ML research comprising the methods section of a paper, supplementary documentation/checklist, author-provided code, and any independent reimplementations used to verify results.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>research paper methods section and experimental protocol (including checklist responses)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>author-provided implementation or independent reimplementation code</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>incomplete specification / missing hyperparameters and training details</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Authors often provide high-level algorithm descriptions but omit concrete settings (exact hyperparameter values, initialization seeds, data preprocessing steps, optimizer schedules). These omissions make it impossible to deterministically reproduce reported results from the natural language description alone; reimplementations or even runs of supplied code may therefore produce different outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>model specification, training procedure, hyperparameters, data preprocessing</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>reproducibility studies and reimplementations reported in the literature (cited work) and community experience summarized by the NeurIPS reproducibility program</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Indirect measurement via replication success rates reported in prior studies (e.g., aggregated reproduction attempts) and qualitative mismatch reports; the paper cites literature showing that thorough hyperparameter search or different initializations change conclusions (references: Lucic et al., Melis et al., Henderson et al.). No single numeric delta provided in this report for each case.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Can change whether a proposed method outperforms baselines; prior cited work found that purported improvements disappear when hyperparameters or initializations are controlled. In the wider literature this has led to failures to reproduce claimed advantages, though this particular report does not give a single numerical performance delta for all cases.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Described as a common and recurrent issue in ML literature; multiple cited studies (Lucic et al., Melis et al., Henderson et al., Bouthillier et al.) document it, indicating it is widespread though no single percentage is given in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Ambiguous natural language descriptions, space-limited manuscripts, implicit assumptions left unstated, and lack of standardized reporting conventions for experimental details.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Encourage authors to share code and detailed experimental settings (via a code submission policy), deploy a reproducibility checklist asking for explicit details, and run community reproducibility challenges to uncover missing details.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Partial: the paper reports increased code availability and engagement (see other entries) but notes there is not yet conclusive evidence that these processes fully address under-specification; effectiveness is described qualitatively rather than with a single metric.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving Reproducibility in Machine Learning Research (A Report from the NeurIPS 2019 Reproducibility Program)', 'publication_date_yy_mm': '2020-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e445.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e445.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>claimed_code_vs_confirmed_code</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Discrepancy between authors' claims of code submission and reviewers' confirmation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Authors' self-reported provision of code at submission does not always match reviewer-confirmed availability; a substantial fraction of claimed code links were not validated by reviewers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Improving Reproducibility in Machine Learning Research (A Report from the NeurIPS 2019 Reproducibility Program)</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>NeurIPS paper submission and review system (CMT / supplementary materials flow)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Conference submission and review pipeline where authors declare code availability on a reproducibility checklist and may include code/supplementary materials; reviewers can indicate whether code was provided and consult it during review.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>checklist self-report / supplementary materials metadata (author-provided statements in submission)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>submitted supplementary code (links or attachments) intended to reproduce experiments</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>metadata misalignment / claim vs actual availability discrepancy</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Authors reported code availability on the checklist in a number of submissions, but reviewer confirmations showed that not all reported links or code artifacts were actually present or accessible to reviewers; this mismatch indicates a gap between natural language claims on submission forms and the real availability of runnable code.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>submission metadata and supplementary materials (availability and accessibility of code)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>comparison of author checklist responses against reviewer confirmations (surveyed reviewer responses during the NeurIPS review process)</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Simple proportion comparison: the paper reports that ~40% of authors reported providing code at initial submission, and reviewer confirmation indicated that 71.5% of those submissions were confirmed by reviewers (i.e., not all self-reports were validated). Additional statistics: absolute numbers reported for reviewer interactions (see impact_on_results).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Reduces the utility of claimed code availability for reviewers and reproducibility studies; reviewers who could not access code cannot use it to validate methods, and external reproducers may be blocked. The paper also notes that availability of code at submission was positively associated with reviewer score (p < 1e-08), so misreported availability can confound review outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Authors reported code for 40% of submissions at initial submission; of those, 71.5% were confirmed by reviewers (implying ~28.5% of claimed-provided code was not confirmed).</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Inconsistent submission practices, broken/privately hosted links, misunderstanding of checklist questions, or failure to attach/make accessible the code despite claiming so.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Require clearer submission mechanisms for code (enforced attachments or validated links), encourage authors to provide accessible repositories, and have reviewers explicitly check and report code availability; consider standard platforms or containers to ensure accessibility.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Partially effective: the paper reports increasing code submission rates over time (from <50% to nearly 75% by camera-ready), indicating the policy and cultural shift improved raw availability, but the confirmation mismatch persisted at the time of the study and no full validation metric is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning / conference peer review</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving Reproducibility in Machine Learning Research (A Report from the NeurIPS 2019 Reproducibility Program)', 'publication_date_yy_mm': '2020-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e445.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e445.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>checklist_answer_accuracy_and_effect</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mismatch and uncertain accuracy between checklist natural language answers and actual experiment/code fidelity</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Authors complete a reproducibility checklist about experiment descriptions, but the accuracy of those answers and their relationship to true reproducibility is uncertain; some checklist answers correlate with acceptance and reviewer perceptions but may not reflect true experimental fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Improving Reproducibility in Machine Learning Research (A Report from the NeurIPS 2019 Reproducibility Program)</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>NeurIPS reproducibility checklist integrated in the submission and camera-ready pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A set of structured yes/no/NA questions authors must answer about the presence of methodological details (math setting, algorithm description, experimental protocols, metrics, error bars) intended to improve reporting transparency.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>structured author-provided checklist responses (natural language yes/no/NA and optional comments)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>not applicable to 'code' per se — comparisons are between checklist answers and the actual presence/usefulness of code or experimental detail</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>ambiguous/overly coarse self-reporting; potential mismatch between checklist claims and actual reproducibility</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Although high fractions of authors answered 'yes' to key reporting questions (e.g., 97% for clear mathematical description, 89% for including descriptions of how experiments were run, 87% for defining metrics), the paper notes anomalies (36% said error bars not applicable) and raises the question of the checklist answers' accuracy when filled by authors. Furthermore, affirmative checklist responses are associated with higher acceptance rates, but it's unclear whether this reflects true methodological quality or confounding factors.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>reporting and documentation layer (methods section/checklist) and its alignment with actual experimental artifacts</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>statistical association analysis between checklist answers, reviewer judgments, and acceptance rates; reviewer survey about checklist usefulness; qualitative observations from reproducibility challenge reports</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Reported descriptive statistics: 97%/89%/87% affirmative rates on selected checklist items; 36% marking error bars as not applicable; 34% of reviewers stated the checklist answers were useful; association tests showing papers with affirmative answers had higher acceptance rates and code availability associated with reviewer score with p < 1e-08. The paper notes absence of ground-truth accuracy checks for checklist answers.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Potentially inflates perceived quality in review if checklist answers are taken at face value; may create false confidence in reproducibility where checklist answers are inaccurate. The paper explicitly lists as an open question: 'What is the accuracy of the ML checklist answers (for each question) when filled by authors?'</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>High affirmative response rates to checklist items (e.g., 97% for some items), and only one-third of reviewers found the checklist useful, indicating widespread use but uncertain fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Checklist relies on self-reporting, variable interpretation of questions by authors, social desirability or strategic answering, and lack of automated validation.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Use checklist as guidance but develop validation mechanisms (reviewer checks, automated metadata verification, requiring concrete artifacts), educate authors on correct interpretation, and refine checklist items to reduce ambiguity.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Partially effective: the checklist increased reviewer engagement (34% found it useful) and correlated with higher reviewer scores and acceptance, but the paper reports no definitive evidence that it improves factual reporting accuracy; effectiveness remains an open research question.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning / peer review processes</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving Reproducibility in Machine Learning Research (A Report from the NeurIPS 2019 Reproducibility Program)', 'publication_date_yy_mm': '2020-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e445.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e445.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>code_errors_and_implementation_mismatch</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bugs or mistakes in provided code and differences between paper-described algorithm variant and implementation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Provided code may contain errors or implement a variant different from the paper description; availability of code does not guarantee correctness and reimplementations are valuable to detect such mismatches.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Improving Reproducibility in Machine Learning Research (A Report from the NeurIPS 2019 Reproducibility Program)</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>author-provided experimental code and independent reimplementations</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Code artifacts submitted with papers (or released later) that are intended to reproduce experiments, and external reimplementations created by reproducibility challenge participants or other researchers.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>paper algorithm specification / supplementary README or code comments</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>submitted code, reimplementation code used in reproducibility challenge</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>implementation errors / different algorithm variant / replication of mistakes</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Even when code is available, it can contain bugs or implement a different variant than the paper's description; the paper emphasizes that replication of mistakes is possible and that reimplementation can be more valuable for checking robustness than mere code availability.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>implementation layer (library code, experiment scripts, evaluation code)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>manual reimplementation and code review conducted in reproducibility challenge and reported in literature; reviewers occasionally inspect code during review</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Qualitative detection through failed reproductions and divergence of results; the paper cites community experience and earlier reproducibility studies that reimplemented algorithms to determine robustness. No unified numeric measure provided here for bug prevalence.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Can lead to incorrect published conclusions being propagated if unchecked; availability of buggy code may help detect and fix mistakes but may also mislead naive reusers. The paper notes that reimplementation often reveals that claimed improvements do not hold under reimplementation or rigorous hyperparameter tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Common enough to be explicitly discussed as a recurrent concern; exact prevalence not quantified in this report.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Human errors in code, insufficient testing, implicit dataset or preprocessing assumptions, reliance on non-documented dependencies, and lack of code review prior to release.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Encourage code sharing, independent reimplementation (reproducibility challenges), use of containers (Docker) and standardized tooling, stronger code review practices, and inclusion of tests/examples in released code.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Partly effective: the paper documents growing participation in reproducibility challenges and increased code sharing, which helps discover errors; however, it emphasizes that simply having code does not guarantee correctness and more tooling and culture change are needed.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning / software engineering for experiments</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving Reproducibility in Machine Learning Research (A Report from the NeurIPS 2019 Reproducibility Program)', 'publication_date_yy_mm': '2020-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Deep reinforcement learning that matters <em>(Rating: 2)</em></li>
                <li>Are gans created equal? a large-scale study <em>(Rating: 2)</em></li>
                <li>On the state of the art of evaluation in neural language models <em>(Rating: 2)</em></li>
                <li>A step toward quantifying independently reproducible machine learning research <em>(Rating: 2)</em></li>
                <li>Unreproducible research is reproducible <em>(Rating: 1)</em></li>
                <li>Reproducibility and replicability in science <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-445",
    "paper_id": "paper-214693121",
    "extraction_schema_id": "extraction-schema-18",
    "extracted_data": [
        {
            "name_short": "under-specification_of_methods",
            "name_full": "Under-specification or misspecification of model/training/procedure in papers vs code",
            "brief_description": "Papers frequently omit or ambiguously describe essential implementation details (hyperparameters, optimization, initialization, preprocessing), causing reimplementations or runs of provided code to diverge from paper claims and leading to failed or inconsistent reproductions.",
            "citation_title": "Improving Reproducibility in Machine Learning Research (A Report from the NeurIPS 2019 Reproducibility Program)",
            "mention_or_use": "mention",
            "system_name": "ML research experimental pipeline / paper-to-code reproduction workflow",
            "system_description": "The end-to-end experimental workflow in ML research comprising the methods section of a paper, supplementary documentation/checklist, author-provided code, and any independent reimplementations used to verify results.",
            "nl_description_type": "research paper methods section and experimental protocol (including checklist responses)",
            "code_implementation_type": "author-provided implementation or independent reimplementation code",
            "gap_type": "incomplete specification / missing hyperparameters and training details",
            "gap_description": "Authors often provide high-level algorithm descriptions but omit concrete settings (exact hyperparameter values, initialization seeds, data preprocessing steps, optimizer schedules). These omissions make it impossible to deterministically reproduce reported results from the natural language description alone; reimplementations or even runs of supplied code may therefore produce different outcomes.",
            "gap_location": "model specification, training procedure, hyperparameters, data preprocessing",
            "detection_method": "reproducibility studies and reimplementations reported in the literature (cited work) and community experience summarized by the NeurIPS reproducibility program",
            "measurement_method": "Indirect measurement via replication success rates reported in prior studies (e.g., aggregated reproduction attempts) and qualitative mismatch reports; the paper cites literature showing that thorough hyperparameter search or different initializations change conclusions (references: Lucic et al., Melis et al., Henderson et al.). No single numeric delta provided in this report for each case.",
            "impact_on_results": "Can change whether a proposed method outperforms baselines; prior cited work found that purported improvements disappear when hyperparameters or initializations are controlled. In the wider literature this has led to failures to reproduce claimed advantages, though this particular report does not give a single numerical performance delta for all cases.",
            "frequency_or_prevalence": "Described as a common and recurrent issue in ML literature; multiple cited studies (Lucic et al., Melis et al., Henderson et al., Bouthillier et al.) document it, indicating it is widespread though no single percentage is given in this paper.",
            "root_cause": "Ambiguous natural language descriptions, space-limited manuscripts, implicit assumptions left unstated, and lack of standardized reporting conventions for experimental details.",
            "mitigation_approach": "Encourage authors to share code and detailed experimental settings (via a code submission policy), deploy a reproducibility checklist asking for explicit details, and run community reproducibility challenges to uncover missing details.",
            "mitigation_effectiveness": "Partial: the paper reports increased code availability and engagement (see other entries) but notes there is not yet conclusive evidence that these processes fully address under-specification; effectiveness is described qualitatively rather than with a single metric.",
            "domain_or_field": "machine learning",
            "reproducibility_impact": true,
            "uuid": "e445.0",
            "source_info": {
                "paper_title": "Improving Reproducibility in Machine Learning Research (A Report from the NeurIPS 2019 Reproducibility Program)",
                "publication_date_yy_mm": "2020-03"
            }
        },
        {
            "name_short": "claimed_code_vs_confirmed_code",
            "name_full": "Discrepancy between authors' claims of code submission and reviewers' confirmation",
            "brief_description": "Authors' self-reported provision of code at submission does not always match reviewer-confirmed availability; a substantial fraction of claimed code links were not validated by reviewers.",
            "citation_title": "Improving Reproducibility in Machine Learning Research (A Report from the NeurIPS 2019 Reproducibility Program)",
            "mention_or_use": "use",
            "system_name": "NeurIPS paper submission and review system (CMT / supplementary materials flow)",
            "system_description": "Conference submission and review pipeline where authors declare code availability on a reproducibility checklist and may include code/supplementary materials; reviewers can indicate whether code was provided and consult it during review.",
            "nl_description_type": "checklist self-report / supplementary materials metadata (author-provided statements in submission)",
            "code_implementation_type": "submitted supplementary code (links or attachments) intended to reproduce experiments",
            "gap_type": "metadata misalignment / claim vs actual availability discrepancy",
            "gap_description": "Authors reported code availability on the checklist in a number of submissions, but reviewer confirmations showed that not all reported links or code artifacts were actually present or accessible to reviewers; this mismatch indicates a gap between natural language claims on submission forms and the real availability of runnable code.",
            "gap_location": "submission metadata and supplementary materials (availability and accessibility of code)",
            "detection_method": "comparison of author checklist responses against reviewer confirmations (surveyed reviewer responses during the NeurIPS review process)",
            "measurement_method": "Simple proportion comparison: the paper reports that ~40% of authors reported providing code at initial submission, and reviewer confirmation indicated that 71.5% of those submissions were confirmed by reviewers (i.e., not all self-reports were validated). Additional statistics: absolute numbers reported for reviewer interactions (see impact_on_results).",
            "impact_on_results": "Reduces the utility of claimed code availability for reviewers and reproducibility studies; reviewers who could not access code cannot use it to validate methods, and external reproducers may be blocked. The paper also notes that availability of code at submission was positively associated with reviewer score (p &lt; 1e-08), so misreported availability can confound review outcomes.",
            "frequency_or_prevalence": "Authors reported code for 40% of submissions at initial submission; of those, 71.5% were confirmed by reviewers (implying ~28.5% of claimed-provided code was not confirmed).",
            "root_cause": "Inconsistent submission practices, broken/privately hosted links, misunderstanding of checklist questions, or failure to attach/make accessible the code despite claiming so.",
            "mitigation_approach": "Require clearer submission mechanisms for code (enforced attachments or validated links), encourage authors to provide accessible repositories, and have reviewers explicitly check and report code availability; consider standard platforms or containers to ensure accessibility.",
            "mitigation_effectiveness": "Partially effective: the paper reports increasing code submission rates over time (from &lt;50% to nearly 75% by camera-ready), indicating the policy and cultural shift improved raw availability, but the confirmation mismatch persisted at the time of the study and no full validation metric is provided.",
            "domain_or_field": "machine learning / conference peer review",
            "reproducibility_impact": true,
            "uuid": "e445.1",
            "source_info": {
                "paper_title": "Improving Reproducibility in Machine Learning Research (A Report from the NeurIPS 2019 Reproducibility Program)",
                "publication_date_yy_mm": "2020-03"
            }
        },
        {
            "name_short": "checklist_answer_accuracy_and_effect",
            "name_full": "Mismatch and uncertain accuracy between checklist natural language answers and actual experiment/code fidelity",
            "brief_description": "Authors complete a reproducibility checklist about experiment descriptions, but the accuracy of those answers and their relationship to true reproducibility is uncertain; some checklist answers correlate with acceptance and reviewer perceptions but may not reflect true experimental fidelity.",
            "citation_title": "Improving Reproducibility in Machine Learning Research (A Report from the NeurIPS 2019 Reproducibility Program)",
            "mention_or_use": "use",
            "system_name": "NeurIPS reproducibility checklist integrated in the submission and camera-ready pipeline",
            "system_description": "A set of structured yes/no/NA questions authors must answer about the presence of methodological details (math setting, algorithm description, experimental protocols, metrics, error bars) intended to improve reporting transparency.",
            "nl_description_type": "structured author-provided checklist responses (natural language yes/no/NA and optional comments)",
            "code_implementation_type": "not applicable to 'code' per se — comparisons are between checklist answers and the actual presence/usefulness of code or experimental detail",
            "gap_type": "ambiguous/overly coarse self-reporting; potential mismatch between checklist claims and actual reproducibility",
            "gap_description": "Although high fractions of authors answered 'yes' to key reporting questions (e.g., 97% for clear mathematical description, 89% for including descriptions of how experiments were run, 87% for defining metrics), the paper notes anomalies (36% said error bars not applicable) and raises the question of the checklist answers' accuracy when filled by authors. Furthermore, affirmative checklist responses are associated with higher acceptance rates, but it's unclear whether this reflects true methodological quality or confounding factors.",
            "gap_location": "reporting and documentation layer (methods section/checklist) and its alignment with actual experimental artifacts",
            "detection_method": "statistical association analysis between checklist answers, reviewer judgments, and acceptance rates; reviewer survey about checklist usefulness; qualitative observations from reproducibility challenge reports",
            "measurement_method": "Reported descriptive statistics: 97%/89%/87% affirmative rates on selected checklist items; 36% marking error bars as not applicable; 34% of reviewers stated the checklist answers were useful; association tests showing papers with affirmative answers had higher acceptance rates and code availability associated with reviewer score with p &lt; 1e-08. The paper notes absence of ground-truth accuracy checks for checklist answers.",
            "impact_on_results": "Potentially inflates perceived quality in review if checklist answers are taken at face value; may create false confidence in reproducibility where checklist answers are inaccurate. The paper explicitly lists as an open question: 'What is the accuracy of the ML checklist answers (for each question) when filled by authors?'",
            "frequency_or_prevalence": "High affirmative response rates to checklist items (e.g., 97% for some items), and only one-third of reviewers found the checklist useful, indicating widespread use but uncertain fidelity.",
            "root_cause": "Checklist relies on self-reporting, variable interpretation of questions by authors, social desirability or strategic answering, and lack of automated validation.",
            "mitigation_approach": "Use checklist as guidance but develop validation mechanisms (reviewer checks, automated metadata verification, requiring concrete artifacts), educate authors on correct interpretation, and refine checklist items to reduce ambiguity.",
            "mitigation_effectiveness": "Partially effective: the checklist increased reviewer engagement (34% found it useful) and correlated with higher reviewer scores and acceptance, but the paper reports no definitive evidence that it improves factual reporting accuracy; effectiveness remains an open research question.",
            "domain_or_field": "machine learning / peer review processes",
            "reproducibility_impact": true,
            "uuid": "e445.2",
            "source_info": {
                "paper_title": "Improving Reproducibility in Machine Learning Research (A Report from the NeurIPS 2019 Reproducibility Program)",
                "publication_date_yy_mm": "2020-03"
            }
        },
        {
            "name_short": "code_errors_and_implementation_mismatch",
            "name_full": "Bugs or mistakes in provided code and differences between paper-described algorithm variant and implementation",
            "brief_description": "Provided code may contain errors or implement a variant different from the paper description; availability of code does not guarantee correctness and reimplementations are valuable to detect such mismatches.",
            "citation_title": "Improving Reproducibility in Machine Learning Research (A Report from the NeurIPS 2019 Reproducibility Program)",
            "mention_or_use": "mention",
            "system_name": "author-provided experimental code and independent reimplementations",
            "system_description": "Code artifacts submitted with papers (or released later) that are intended to reproduce experiments, and external reimplementations created by reproducibility challenge participants or other researchers.",
            "nl_description_type": "paper algorithm specification / supplementary README or code comments",
            "code_implementation_type": "submitted code, reimplementation code used in reproducibility challenge",
            "gap_type": "implementation errors / different algorithm variant / replication of mistakes",
            "gap_description": "Even when code is available, it can contain bugs or implement a different variant than the paper's description; the paper emphasizes that replication of mistakes is possible and that reimplementation can be more valuable for checking robustness than mere code availability.",
            "gap_location": "implementation layer (library code, experiment scripts, evaluation code)",
            "detection_method": "manual reimplementation and code review conducted in reproducibility challenge and reported in literature; reviewers occasionally inspect code during review",
            "measurement_method": "Qualitative detection through failed reproductions and divergence of results; the paper cites community experience and earlier reproducibility studies that reimplemented algorithms to determine robustness. No unified numeric measure provided here for bug prevalence.",
            "impact_on_results": "Can lead to incorrect published conclusions being propagated if unchecked; availability of buggy code may help detect and fix mistakes but may also mislead naive reusers. The paper notes that reimplementation often reveals that claimed improvements do not hold under reimplementation or rigorous hyperparameter tuning.",
            "frequency_or_prevalence": "Common enough to be explicitly discussed as a recurrent concern; exact prevalence not quantified in this report.",
            "root_cause": "Human errors in code, insufficient testing, implicit dataset or preprocessing assumptions, reliance on non-documented dependencies, and lack of code review prior to release.",
            "mitigation_approach": "Encourage code sharing, independent reimplementation (reproducibility challenges), use of containers (Docker) and standardized tooling, stronger code review practices, and inclusion of tests/examples in released code.",
            "mitigation_effectiveness": "Partly effective: the paper documents growing participation in reproducibility challenges and increased code sharing, which helps discover errors; however, it emphasizes that simply having code does not guarantee correctness and more tooling and culture change are needed.",
            "domain_or_field": "machine learning / software engineering for experiments",
            "reproducibility_impact": true,
            "uuid": "e445.3",
            "source_info": {
                "paper_title": "Improving Reproducibility in Machine Learning Research (A Report from the NeurIPS 2019 Reproducibility Program)",
                "publication_date_yy_mm": "2020-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Deep reinforcement learning that matters",
            "rating": 2,
            "sanitized_title": "deep_reinforcement_learning_that_matters"
        },
        {
            "paper_title": "Are gans created equal? a large-scale study",
            "rating": 2,
            "sanitized_title": "are_gans_created_equal_a_largescale_study"
        },
        {
            "paper_title": "On the state of the art of evaluation in neural language models",
            "rating": 2,
            "sanitized_title": "on_the_state_of_the_art_of_evaluation_in_neural_language_models"
        },
        {
            "paper_title": "A step toward quantifying independently reproducible machine learning research",
            "rating": 2,
            "sanitized_title": "a_step_toward_quantifying_independently_reproducible_machine_learning_research"
        },
        {
            "paper_title": "Unreproducible research is reproducible",
            "rating": 1,
            "sanitized_title": "unreproducible_research_is_reproducible"
        },
        {
            "paper_title": "Reproducibility and replicability in science",
            "rating": 1,
            "sanitized_title": "reproducibility_and_replicability_in_science"
        }
    ],
    "cost": 0.01231175,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Improving Reproducibility in Machine Learning Research (A Report from the NeurIPS 2019 Reproducibility Program)
2 Apr 2020</p>
<p>Joelle Pineau jpineau@cs.mcgill.ca 
Philippe Vincent-Lamarre 
Koustuv Sinha koustuv.sinha@mail.mcgill.ca 
Vincent Larivière vincent.lariviere@umontreal.ca 
Alina Beygelzimer 
Télécom Paris 
Emily Fox ebfox@cs.washington.edu 
Hugo Larochelle hugolarochelle@google.com 
Emily Florence D'alché-Buc 
Hugo Larochelle Fox </p>
<p>School of Computer Science
McGill University (Mila) Facebook AI Research CIFAR</p>
<p>Ecole de bibliothèconomie et des sciences de l'information
Université de Montréal</p>
<p>School of Computer Science
McGill University (Mila) Facebook AI Research</p>
<p>Ecole de bibliothéconomie et des sciences de l'information
Université de Montréal</p>
<p>Institut Polytechnique
France</p>
<p>University of Washington</p>
<p>Improving Reproducibility in Machine Learning Research (A Report from the NeurIPS 2019 Reproducibility Program)
2 Apr 202048B8B52514ED41A908ED7AFFA0622DE3arXiv:2003.12206v3[cs.LG]Reproducibility, NeurIPS 2019
One of the challenges in machine learning research is to ensure that presented and published results are sound and reliable.Reproducibility, that is obtaining similar results as presented in a paper or talk, using the same code and data (when available), is a necessary step to verify the reliability of research findings.Reproducibility is also an important step to promote open and accessible research, thereby allowing the scientific community to quickly integrate new findings and convert ideas to practice.Reproducibility also promotes the use of robust experimental workflows, which potentially reduce unintentional errors.In 2019, the Neural Information Processing Systems (NeurIPS) conference, the premier international conference for research in machine learning, introduced a reproducibility program, designed to improve the standards across the community for how we conduct, communicate, and evaluate machine learning research.The program contained three components: a code submission policy, a community-wide reproducibility challenge, and the inclusion of the Machine Learning Reproducibility checklist as part of the paper submission process.In this paper, we describe each of these components, how it was deployed, as well as what we were able to learn from this initiative.</p>
<p>Introduction</p>
<p>At the very foundation of scientific inquiry is the process of specifying a hypothesis, running an experiment, analyzing the results, and drawing conclusions.Time and again, over the last several centuries, scientists have used this process to build our collective understanding of the natural world and the laws that govern it.However, for the findings to be valid and reliable, it is important that the experimental process be repeatable, and yield consistent results and conclusions.This is of course well-known, and to a large extent, the very foundation of the scientific process.Yet a 2016 survey in the journal Nature revealed that more than 70% of researchers failed in their attempt to reproduce another researcher's experiments, and over 50% failed to reproduce one of their own experiments (Baker, 2016).</p>
<p>In the area of computer science, while many of the findings from early years were derived from mathematics and theoretical analysis, in recent years, new knowledge is increasingly derived from practical experiments.Compared to other fields like biology, physics or sociology where experiments are made in the natural or social world, the reliability and reproducibility of experiments in computer science, where the experimental apparatus for the most part consists of a computer designed and built by humans, should be much easier to achieve.Yet in a surprisingly large number of instances, researchers have had difficulty reproducing the work of others (Henderson et al., 2018).</p>
<p>Focusing more narrowly on machine learning research, where most often the experiment consists of training a model to learn to make predictions from observed data, the reasons for this gap are numerous and include:</p>
<p>• Lack of access to the same training data / differences in data distribution;</p>
<p>• Misspecification or under-specification of the model or training procedure;</p>
<p>• Lack of availability of the code necessary to run the experiments, or errors in the code;</p>
<p>• Under-specification of the metrics used to report results;</p>
<p>• Improper use of statistics to analyze results, such as claiming significance without proper statistical testing or using the wrong statistic test;</p>
<p>• Selective reporting of results and ignoring the danger of adaptive overfitting;</p>
<p>• Over-claiming of the results, by drawing conclusions that go beyond the evidence presented (e.g.insufficient number of experiments, mismatch between hypothesis &amp; claim).</p>
<p>We spend significant time and energy (both of machines and humans), trying to overcome this gap.This is made worse by the bias in the field towards publishing positive results (rather than negative ones).Indeed, the evidence threshold for publishing a new positive finding is much lower than that for invalidating a previous finding.In the latter case, it may require several teams showing beyond the shadow of a doubt that a result is false for the research community to revise its opinion.Perhaps the most infamous instance of this is that of the false causal link between vaccines and autism.In short, we would argue that it is always more efficient to properly conduct the experiment and analysis in the first place.</p>
<p>Improving Reproducibility in Machine Learning Research</p>
<p>In 2019, the Neural Information Processing Systems (NeurIPS) conference, the premier international conference for research in machine learning, introduced a reproducibility program, designed to improve the standards across the community for how we conduct, communicate, and evaluate machine learning research.The program contained three components: a code submission policy, a community-wide reproducibility challenge, and the inclusion of the Machine Learning Reproducibility checklist as part of the paper submission process.</p>
<p>In this paper, we describe each of these components, how it was deployed, as well as what we were able to learn from this exercise.The goal is to better understand how such an approach is implemented, how it is perceived by the community (including authors and reviewers), and how it impacts the quality of the scientific work and the reliability of the findings presented in the conference's technical program.We hope that this work will inform and inspire renewed commitment towards better scientific methodology, not only in the machine learning research community, but in several other research fields.</p>
<p>Background</p>
<p>There are challenges regarding reproducibility that appear to be unique (or at least more pronounced) in the field of ML compared to other disciplines.The first is an insufficient exploration of the variables that might affect the conclusions of a study.In machine learning, a common goal for a model is to beat the top benchmarks scores.However, it is hard to assert if the aspect of a model claimed to have improved its performance is indeed the factor leading to the higher score.This limitation has been highlighted in a few studies reporting that new proposed methods are often not better than previous implementations when a more thorough search of hyper-parameters is performed (Lucic et al., 2018;Melis et al., 2017), or even when using different random parameter initializations ( (Bouthillier et al., 2019;Henderson et al., 2018).</p>
<p>The second challenge refers to the proper documentation and reporting of the information necessary to reproduce the reported results (Gundersen and Kjensmo, 2018).A recent report indicated that 63.5% of the results in 255 manuscripts were successfully replicated (Raff, 2019).Strikingly, this study found that when the original authors provided assistance to the reproducers, 85% of results were successfully reproduced, compared to 4% when the authors didn't respond.Although a selection bias could be at play (authors who knew their results would reproduce might have been more likely to provide assistance for the reproduction), this contrasts with large-scale replication studies in other disciplines that failed to observe similar improvement when the original authors of the study were involved (Klein et al., 2019).It therefore remains to be established if the field is having a reproduction problem similar to the other fields, or if it would be better described as a reporting problem.</p>
<p>Thirdly, as opposed to most scientific disciplines where uncertainty of the observed effects are routinely quantified, it appears like statistical analysis is seldom conducted in ML research (Forde and Paganini, 2019;Henderson et al., 2018).</p>
<p>Defining Reproducibility</p>
<p>Before going any further, it is worth defining a few terms that have been used (sometimes interchangeably) to describe reproducibility &amp; related concepts.We adopt the terminology from Figure 1, where Reproducible work consists of re-doing an experiment using the same data and same analytical tools, whereas Replicable work considers different data (presumably sampled from similar distribution or method), Robust work assumes the same data but different analysis (such as reimplementation of the code, perhaps different computer architecture), and Generalisable work leads to the same conclusions despite considering different data and different analytical tools.For the purposes of our work, we focus primarily on the notion of Reproducibility as defined here, and assume that any modification in analytical tools (e.g.re-running experiments on a different computer) was small enough as to be negligible.A recent report by the National Academies of Sciences, Engineering, and Medicine, provides more in-depth discussion of these concepts, as well as several recommendations for improving reproducibility broadly across scientific fields (National Academies of Sciences, Engineering, and Medicine, 2019).</p>
<p>The Open Science movement</p>
<p>"Open Science is transparent and accessible knowledge that is shared and developed through collaborative networks" (Vicente-Sáez and Martínez-Fuentes, 2018).In other words, Open science is a movement to conduct science in a more transparent way.This includes making code, data and scientific communications publicly available, increasing the transparency of the research process and improving the reporting quality in scientific manuscripts.The implementation of Open science practices has been identified as a core factor that could improve the reproducibility of science (Munafò et al., 2017).As such, the NeurIPS reproducibility program was designed to incorporate elements designed to encourage researchers to share the artefacts of their research (code, data), in addition to their manuscripts.</p>
<p>Improving Reproducibility in Machine Learning Research</p>
<p>Code submission policies</p>
<p>It has become increasingly common in recent years to require the sharing of data and code, along with a paper, when computer experiments were used in the analysis.It is now standard expectation in the Nature research journals for authors to provide access to code and data to readers (Nature Research, 2021).Similarly, the policy at the journal Science specifies that authors are expected to satisfy all reasonable requests for data, code or materials (Science -AAAS, 2018).Within machine learning and AI conferences, the ability to include supplementary material has now been standard for several years, and many authors have used this to provide the data and/or code used to produce the paper.More recently, ICML 2019, the second largest international conference in machine learning has also rolled-out an explicit code submission policy (ICML, 2019).</p>
<p>Reproducibility challenges</p>
<p>The 2018 ICLR reproducibility challenge paved the way for the NeurIPS 2019 edition.The goal of this first iteration was to investigate reproducibility of empirical results submitted to the 2018 International Conference on Learning Representations (ICLR, 2018).The organizers chose ICLR for this challenge because the timing was right for course-based participants: most participants were drawn from graduate machine learning courses, where the challenge served as the final course project.The choice of ICLR was motivated by the fact that papers submitted to the conference were automatically made available publicly on OpenReview, including during the review period.This means anyone in the world could access the paper prior to selection, and could interact with the authors via the message board on OpenReview.This first challenge was followed a year later by the 2019 ICLR Reproducibility Challenge (Pineau et al., 2019).</p>
<p>Several less formal activities, including hackathons, course projects, online blogs, opensource code packages, have participated in the effort to carry out re-implementation and replication of previous work and should be considered in the same spirit as the effort described here.</p>
<p>Checklists</p>
<p>The Checklist Manifesto presents a highly compelling case for the use of checklists in safetycritical systems (Gawande, 2010).It documents how pre-flight checklists were introduced at Boeing Corporation as early as 1935 following the unfortunate crash of an airplane prototype.Checklists are similarly used in surgery rooms across the world to prevent oversights.Similarly, the WHO Surgical Safety Checklist, which is employed in surgery rooms across the world, has been shown to significantly reduce morbidity and mortality (Clay-Williams and Colligan, 2015).</p>
<p>In the case of scientific manuscripts, reporting checklists are meant to provide the minimal information that must be included in a manuscript, and are not necessarily exhaustive.The use of checklists in scientific research has been explored in a few instances.Reporting guidelines in the form of checklists have been introduced for a wide range of study design in health research (The EQUATOR Network, 2021), and the Transparency and Openness Promotion (TOP) guidelines have been adopted by multiple journals across disciplines (Nosek et al., 2015).There are now more than 400 checklists registered in the EQUATOR Network.CONSORT, one of the most popular guidelines used for randomized controlled trials was found to be effective and to improve the completeness of reporting for 22 checklist items (Turner et al., 2012).The ML checklist described below was significantly influenced by Nature's Reporting Checklist for Life Sciences Articles (Checklist, 2021).Other guidelines are under development outside of the ML community, namely for the application of AI tools in clinical trials (Liu et al., 2019) and health-care (Collins and Moons, 2019).</p>
<p>Other considerations</p>
<p>Beyond reproducibility, there are several other factors that affect how scientific research is conducted, communicated and evaluated.One of the best practices used in many venues, including NeurIPS, is that of double-blind reviewing.It is worth remembering that in 2014, the then program chairs Neil Lawrence and Corinna Cortes ran an interesting experiment, by assigning 10% of submitted papers to be reviewed independently by two groups of reviewers (each lead by a different area chair).The results were surprising: overall the reviewers disagreed on 25.9% of papers, but when tasked with reaching a 22.5% acceptance rate, they disagreed on 57% of the list of accepted papers.We raise this point for two reasons.First, to emphasize that the NeurIPS community has for many years already demonstrated an openness towards trying new approaches, as well as looking introspectively on the effectiveness of its processes.Second, to emphasize that there are several steps that come into play when a paper is written, and selected for publication at a high-profile international venue, and that a reproducibility program is only one aspect to consider when designing community standards to improve the quality of scientific practices.</p>
<p>The NeurIPS 2019 code submission policy</p>
<p>The NeurIPS 2019 code submission policy, as defined for all authors (see Appendix, Figure 6), was drafted by the program chairs and officially approved by the NeurIPS board in winter 2019 (before the May 2019 paper submission deadline.)</p>
<p>The most frequent objections we heard to having a code submission policy (at all) include:</p>
<p>• Dataset confidentiality: There are cases where the dataset cannot be released for legitimate privacy reasons.This arises often when looking at applications of ML, for example in healthcare or finance.One strategy to mitigate this limitation is to provide complementary empirical results on an open-source benchmark dataset, in addition to the results on the confidential data.</p>
<p>• Proprietary software: The software used to derive the result contains intellectual property, or is built on top of proprietary libraries.This is of particular concern to some researchers working in industry.Nonetheless, as shown in Figure 2a, we see that many authors from industry were indeed able to submit code, and furthermore despite the policy, the acceptance rate for papers from authors in industry remained high (higher than authors from academia (Figure 2b)).By the camera-ready deadline, most submissions from the industry reported having submitted code (Figure 2a,b).</p>
<p>• Computation infrastructure: Even if data and code are provided, the experiments may require so much computation (time &amp; number of machines) that it is impractical for any reviewer, or in fact most researchers, to attempt reproducing the work.This is the case for work on training very large neural models, for example the AlphaGo game playing agent (Silver et al., 2016) or the BERT language model (Devlin et al., 2018).Nonetheless it is worth noting that both these systems have been reproduced within months (if not weeks) of their release.</p>
<p>• Replication of mistakes: Having a copy of the code used to produce the experimental results is not a guarantee that this code is correct, and there is significant value in reimplementing an algorithm directly from its description in a paper.This speaks more to the notion of Robustness defined above.It is indeed common that there are mistakes in code (as there may be in proofs for more theoretical papers).Nonetheless, the availability of the code (or proof) can be tremendously helpful to verify or re-implement the method.It is indeed much easier to verify a result (with the initial code or proof), then it is to produce from nothing (this is perhaps most poignantly illustrated by the longevity of the lack of proof for Fermat's last theorem (Wikipedia, 2020).)</p>
<p>It is worth noting that the NeurIPS 2019 code submission policy leaves significant time &amp; flexibility, in particular it says that it: "expects code only for accepted papers, and only by the camera-ready deadline".So code submission is not mandatory, and the code is not expected to be used during the review process to decide on the soundness of the work.</p>
<p>Reviewers were asked as a part of their assessment to report if code was provided along the manuscript at the initial submission stage.About 40% of authors reported that they had provided code at this stage which was confirmed by the reviewers (if at least one reviewer indicated that the code was provided for each submission) for 71.5% of those submissions (Figure 2d).Note that authors are still able to provide code (or a link to code) as part of their initial submission.In Table 1, we provide a summary of code submission frequency for ICML 2019, as well as NeurIPS 2018 and 2019.We observe a growing trend towards more papers adding a link to code, even with only soft encouragement and no coercive measures.</p>
<p>While the value of having code extends long beyond the review period, it is useful, in those cases where code is available during the review process, to know how it is used and perceived by the reviewers.When surveying reviewers at the end of the review period, we found:</p>
<p>Q. Was code provided (e.g. in the supplementary material)?Yes: 5298 If provided, did you look at the code?Yes: 2255 If provided, was the code useful in guiding your review?Yes: 1315</p>
<p>If not provided, did you wish code had been available?Yes: 3881</p>
<p>We were positively surprised by the number of reviewers willing to engage with this type of artefact during the review process.Furthermore, we found that the availability of code at submission (as indicated on the checklist) was positively associated with the reviewer score (p &lt; 1e − 08).</p>
<p>The NeurIPS 2019 Reproducibility Challenge</p>
<p>The main goal of this challenge is to provide independent verification of the empirical claims in accepted NeurIPS papers, and to leave a public trace of the findings from this secondary analysis.The reproducibility challenge officially started on Oct.31 2019, right after the final paper submission deadline, so that participants could have the benefit of any code submission by authors.By this time, the authors' identity was also known, allowing collaborative interaction between participants and authors.We used OpenReview (OpenReview.net, 2021) to enable communication between authors and challenge participants.</p>
<p>As shown in Table 2, a total of 173 papers were claimed for reproduction.This is a 92% increase since the last reproducibility challenge at ICLR 2019 (Pineau et al., 2019).We had participants from 73 different institutions distributed around the world (see Appendix, Figure 7), including 63 universities and 10 industrial labs.Institutions with the most participants came from 3 continents and include McGill University (Canada), KTH (Sweden), Brown University (US) and IIT Roorkee (India).In those cases (and several others), high participation rate occurred when a professor at the university used this challenge as a final course project.</p>
<p>All reports submitted to the challenge are available on OpenReview 1 for the community; in many cases with a link to the reimplementation code.The goal of making these available is to two-fold: first to give examples of reproducibility reports so that the practice becomes more widespread in the community, and second so that other researchers can benefit from the knowledge, and avoid the pitfalls that invariably come with reproducing another team's work.While many readers may be looking for a simple answer to the question Is this paper reproducible?There is rarely such a concise outcome to a reproducibility study.Most reports produced during the challenge offer a much more detailed &amp; nuanced account of their efforts, and the level of fidelity to which they could reproduce the methods, results &amp; claims of each paper.Similarly, while some readers may be looking for a "reproducibility score", we have not found that the findings of most reproducibility studies lend themselves to such a coarse summary.</p>
<p>Once submitted, all reproducibility reports underwent a review cycle (by reviewers of the NeurIPS conference), to select a small number of high-quality reports, which will be published in an upcoming edition of the journal ReScience (ReScience C, 2021).This provides a lasting archival record for this new type of research artefact.</p>
<p>The NeurIPS 2019 ML reproducibility checklist</p>
<p>The third component of the reproducibility program involved use of the Machine Learning reproducibility checklist (see Appendix, Figure 8).This checklist was first proposed in late 2018, at the NeurIPS conference, in response to findings of recurrent gaps in experimental methodology found in recent machine learning papers.An earlier version (v.1.1)was first deployed as a trial with submission of the final camera-ready version for NeurIPS 2018 papers (due in January 2019); this initial test allowed collection of feedback from authors and some minor modifications to the content of the checklist.The edited version 1.2 was then deployed during the NeurIPS 2019 review process, and authors were obliged to fill it both at the initial paper submission phase (May 2019), and at the final camera-ready phase (October 2019).This allowed us to analyze any change in answers, which presumably resulted from the review feedback (or authors' own improvements of the work).The checklist was implemented on the CMT platform; each question included a multiple choice "Yes, No, not applicable", and an (optional) open comment field.</p>
<p>Figure 3 shows the initial answers provided for each submitted paper.It is reassuring to see that 97% of submissions are said to contain Q#.A clear description of the mathematical setting, algorithm, and/or model.Since we expect all papers to contain this, the 3% no/na answers might reflect margin of error in how authors interpreted the questions.Next, we notice that 89% of submissions answered to the affirmative when asked Q#.For all figures and tables that present empirical results, indicate if you include: A description of how experiments were run.This is reasonably consistent with the fact that 9% of NeurIPS 2019 submissions indicated "Theory" as their primary subject area, and thus may not contain empirical results.</p>
<p>One set of responses that raises interesting questions is the following trio:</p>
<p>Q#.A clear definition of the specific measure or statistics used to report results.In particular, it seems surprising to have 87% of papers that see value in clearly defining the metrics and statistics used, yet 36% of papers judge that error bars are not applicable to their results.As shown in Figure 4, many checklist answers appear to be associated with a higher acceptance rate when the answer is "yes".However, it is too early to rule out potential covariates (e.g.paper's topic, reviewer expectations, etc.)At this stage, it is encouraging that answering "no" to any of the questions is not associated with a higher acceptance rate.There seems to be a higher acceptance rate associated with "NA" responses on a subset of questions related to "Figures and tables".Although it is still unclear at this stage why this effect is observed, it disappears when we only include manuscripts for which the reviewers indicated that the checklist was useful for the review.</p>
<p>Finally, it is worth considering the reviewers' point of view on the usefulness of the ML checklist to assess the soundness of the papers.When asked "Were the Reproducibility Checklist answers useful for evaluating the submission?", 34% responded Yes.</p>
<p>We also note, as shown in Figure 5, that reviewers who found the checklist useful gave higher scores.And that those who found the checklist useful or not useful were more confident in their assessment than those who had not read the checklist.Finally, papers where the checklist was assessed as useful were more likely to be accepted.</p>
<p>Discussion</p>
<p>We presented a summary of the activities &amp; findings from the NeurIPS 2019 reproducibility program.Perhaps the best way to think of this effort is as a case study showing how three different mechanisms (code submission, reproducibility challenge, reproducibility checklist) can be incorporated into a conference program in an attempt to improve the quality of scientific contributions.At this stage, we do not have concluding evidence that these processes indeed have an impact on the quality of the work or of the papers that are submitted and published.</p>
<p>However we note several encouraging indicators:</p>
<p>• The number of submissions to NeurIPS increased by nearly 40% this year, therefore we can assume the changes introduced did not result in a significant drop of interest by authors to submit their work to NeurIPS.</p>
<p>• The number of authors willingly submitting code is quickly increasing, from less than 50% a year ago, to nearly 75%.It seems a code submission policy based on volun-Improving Reproducibility in Machine Learning Research tary participation is sufficient at this time.We are not necessarily aiming for 100% compliance, as there are some cases where this may not be desirable.</p>
<p>• The number of reviewers indicating that they consulted the code, or wished to consult it is in the 1000's, indicating that this is useful in the review process.</p>
<p>• The number of participants in the reproducibility challenge continues to increase, as does the number of reproducibility reports, and reviewers of reproducibility reports.This suggests that an increasing segment of the community is willing to participate voluntarily in secondary analysis of research results.</p>
<p>• One-third of reviewers found the checklist answers useful, furthermore reviewers who found the checklist useful gave higher scores to the paper, which suggests the checklist's use is useful for both reviewers and authors.</p>
<p>The work leaves several questions open, which would require further investigation, and a careful study design to elucidate:</p>
<p>• What is the long-term value (e.g.reproducibility, robustness, generalization, impact of follow-up work) of the code submitted?</p>
<p>• What is the effect of different incentive mechanisms (e.g.cash payment, conference registration, a point/badge system) on the participation rate &amp; quality of work in the reproducibility challenge?</p>
<p>• What is the benefit of using the checklist for authors?</p>
<p>• What is the accuracy of the ML checklist answers (for each question) when filled by authors?</p>
<p>• What is the measurable effect of the checklist on the quality of the final paper, e.g. in terms of soundness of results, clarity of writing?</p>
<p>• What is the measurable effect of the checklist on the review process, in terms of reliability (e.g.inter-rater agreement) and efficiency (e.g.need for response/rebuttal, discussion time)?</p>
<p>A related direction to explore is the development of tools and platforms that enhance reproducibility.Throughout this work we have focused on processes &amp; guidelines, but stayed away from prescribing any infrastructure or software tooling to support reproducibility.Software containers, such as Docker, can encapsulate operating systems components, code and data into a single package.Standardization of such tools would help sharing of information and improve ease of reproducibility.</p>
<p>In conclusion, one aspect worth emphasizing is the fact that achieving reproducible results across a research community, whether NeurIPS or another, requires a significant cultural and organizational changes, not just a code submission policy or a checklist.The initiative described here is just one step in helping the community adopt better practices, in terms of conducting, communicating, and evaluating scientific research.The NeurIPS community is far from alone in looking at this problem.Several workshops have been held in recent years to discuss the issue as it pertains to machine learning and computer science (SIGCOMM, 2017;ICML, 2017ICML, , 2018;;ICLR, 2019).Specific calls for reproducibility papers have been issued (ECIR, 2020).An open-access peer-reviewed journal is dedicated to such papers (ReScience C, 2021), which was used to publish select reports in ICLR 2019 Reproducibility Challenge (Pineau et al., 2019).And in the process, many labs are changing their practices to improve reproducibility of their own results.</p>
<p>Improving Reproducibility in Machine Learning Research</p>
<p>Figure 1 :
1
Figure 1: Reproducible Research.Adapted from: https://github.com/WhitakerLab/ReproducibleResearch</p>
<p>Figure 2 :
2
Figure 2: (a) Link to code provided at initial submission and camera-ready, as a function of affiliation of the first and last authors.(b) Acceptance rate of submissions as a function of affiliation of the first and last authors.The red dashed line shows the acceptance rate for all submissions.(c) Diagram representing the transition of the code availability from initial submission to cameraready only for submissions with an author from the industry (first or last).All results presented here for code availability are based on the author's self-response in the checklist.(d) Percentage of submissions reporting that they provided code on the checklist subsequently confirmed by the reviewers.</p>
<p>Figure 3 :
3
Figure 3: Author responses to all checklist questions for NeurIPS 2019 submitted papers.</p>
<p>Figure 4 :
4
Figure 4: Acceptance rate per question.The numbers within each bar show the number of submissions for each answer.See Fig. 3 (and in Appendix Fig. 8) for text corresponding to each Question # (x-axis).The red dashed line shows the acceptance rate for all submissions.</p>
<p>Figure 5 :
5
Figure 5: Perceived usefulness of the ML reproducibility checklist vs the review outcomes.(a) Effect on the paper score (scale 1-10).(b) Effect on the reviewer confidence score (scale of 1 to 5, where 1 is lowest).(c) Effect on the final accept/reject decision.</p>
<p>Figure 7 :
7
Figure 7: NeurIPS 2019 Reproducibility Challenge Participants by geographical location.</p>
<p>Table 1 :
1
Improving Reproducibility in Machine Learning Research Code submission frequency for recent ML conferences.Source for number of papers accepted and acceptance rates: https://github.com/lixin4ever/Conference-Acceptance-Rate.ICML 2019 numbers reproduced from the ICML 2019 Code-at-Submit-Time Experiment.
Conference# papers% papers% papers w/code% papers w/codeCode submission policysubmittedacceptedat submissionat camera-readyNeurIPS 2018485620.8&lt;50%"Authors may submit up to100MB of supplementary ma-terial, such as proofs, deriva-tions, data, or source code."ICML 2019342422.636%67%"To foster reproducibility, wehighly encourage authors tosubmit code. Reproducibilityof results and easy availabilityof code will be taken into ac-count in the decision-makingprocess."NeurIPS 2019674321.140%74.4%"We expect (but not require)accompanying code to be sub-mitted with accepted papersthat contribute and presentexperiments with a new algo-rithm." See Appendix, Fig. 6Conference# papersAcceptance# papers#par-# reportssubmittedrateclaimedticipatingreviewedinstitutionsICLR 201898132.012331n/aICLR 2019159131.4903526NeurIPS 2019674321.11737384</p>
<p>Table 2 :
2
Participation</p>
<p>in the Reproducibility Challenge.Source for number of papers accepted and acceptance rates: https://github.com/lixin4ever/Conference-Acceptance-Rate</p>
<p>AcknowledgmentsWe thank the NeurIPS board and the NeurIPS 2019 general chair (Hanna Wallach) their unfailing support of this initiative.Without their courage and spirit of experimentation, none of this work would have been possible.We thank the many authors who submitted their work to NeurIPS 2019 and agreed to participate in this large experiment.We thank the program committee (reviewers, area chairs) of NeurIPS 2019 who not only incorporated the reproducibility checklist into their task flow, but also provided feedback about its usefulness.We thank Zhenyu (Sherry) Xue for preparing the data on NeurIPS papers &amp; reviews for the analysis presented here.We thank the OpenReview team (in particular Andrew McCallum, Pam Mandler, Melisa Bok, Michael Spector and Mohit Uniyal) who provided support to host the results of the reproducibility challenge.We thank CodeOcean (in particular Xu Fei) for providing free compute resources to reproducibility challenge participants.Thank you to Robert Stojnic for valuable comments on an early version of the manuscript.Finally, we thank the several participants of the reproducibility challenge who dedicated time and effort to verify results that were not their own, to help strengthen our understanding of machine learning, and the types of problems we can solve today.
1,500 scientists lift the lid on reproducibility. Monya Baker, 10.1038/533452aNature News. 5337604452May 2016. 19970</p>
<p>Unreproducible research is reproducible. Xavier Bouthillier, César Laurent, Pascal Vincent, Proceedings of the 36th International Conference on Machine Learning. Kamalika Chaudhuri, Ruslan Salakhutdinov, the 36th International Conference on Machine LearningLong Beach, California, USA09-15 Jun 201997of Proceedings of Machine Learning Research</p>
<p>. Nature Checklist. Nature checklist. 2021</p>
<p>Back to basics: Checklists in aviation and healthcare. Robyn Clay, - Williams, Lacey Colligan, 10.1136/bmjqs-2015-003957BMJ Quality &amp; Safety. 2044-5415247July 2015</p>
<p>Reporting of artificial intelligence prediction models. The Lancet. Gary S Collins, Karel G M Moons, 10.1016/S0140-6736(19)30037-6Machine Learning Research. 10181. April 2019393</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Bert, arXiv:1810.04805Pre-training of Deep Bidirectional Transformers for Language Understanding. October 2018</p>
<p>Call for Reproducibility papers. ecir2020.orgApril 2020ECIR</p>
<p>Atul Gawande. Checklist manifesto, the (HB). Jessica Zosa, Forde , Michela Paganini, arXiv:1904.10922The Scientific Method in the Science of Machine Learning. Penguin Books IndiaApril 2019. 2010cs, stat</p>
<p>State of the art: Reproducibility in artificial intelligence. Erik Odd, Sigbjørn Gundersen, Kjensmo, Thirty-second AAAI conference on artificial intelligence. 2018</p>
<p>Deep reinforcement learning that matters. Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, David Meger, Thirty-Second AAAI Conference on Artificial Intelligence. 2018</p>
<p>Iclr, Iclr, Reproducibility Challenge. 2018. 2018</p>
<p>Machine Learning Workshop. 2019</p>
<p>Machine Learning Workshop. 2017</p>
<p>Machine Learning Workshop. 2018</p>
<p>Call for papers. ICML. 2019</p>
<p>Many labs 4: Failure to replicate mortality salience effect with and without original author involvement. Corey L Richard A Klein, Cook, Christine Charles R Ebersole, Brian A Vitiello, Christopher R Nosek, Cody D Chartier, Samuel Christopherson, Brian Clay, Jarret Collisson, Crawford, 2019</p>
<p>Extension of the consort and spirit statements. Xiaoxuan Liu, Livia Faes, Melanie J Calvert, Alastair K Denniston, The Lancet. 394122510205. 2019</p>
<p>Are gans created equal? a large-scale study. Mario Lucic, Karol Kurach, Marcin Michalski, Sylvain Gelly, Olivier Bousquet, Advances in neural information processing systems. 2018Pineau et al.</p>
<p>On the state of the art of evaluation in neural language models. Gábor Melis, Chris Dyer, Phil Blunsom, arXiv:1707.055892017arXiv preprint</p>
<p>. Brian A Marcus R Munafò, Dorothy Vm Nosek, Katherine S Bishop, Christopher D Button, Nathalie Chambers, Du Percie, Uri Sert, Eric-Jan Simonsohn, Jennifer J Wagenmakers, John Pa Ware, Ioannidis, A manifesto for reproducible science. human behaviour. 112017</p>
<p>Reproducibility and replicability in science. 2019National Academies PressNational Academies of Sciences, Engineering, and Medicine.</p>
<p>Reporting standards and availability of data, materials, code and protocols. Nature Research. 2021</p>
<p>Promoting an open research culture. Brian A Nosek, George Alter, George C Banks, Denny Borsboom, Sara D Bowman, Steven J Breckler, Stuart Buck, Christopher D Chambers, Gilbert Chin, Garret Christensen, Science. 34862422015</p>
<p>Openreview, Neurips 2019 reproducibility challenge. 2021</p>
<p>ICLR Reproducibility Challenge. Joelle Pineau, Koustuv Sinha, Genevieve Fried, Rosemary Nan Ke, Hugo Larochelle, 10.5281/zenodo.3158244ReScience C. 5252019. May 2019</p>
<p>A step toward quantifying independently reproducible machine learning research. Edward Raff, Advances in Neural Information Processing Systems. 2019</p>
<p>The rescience journal. reproducible science is good. C Rescience, 2021replicated science is better.</p>
<p>-Aaas Science, Science journals: Editorial policies. 2018</p>
<p>10.1145/3097766SIGCOMM. Reproducibility '17: Proceedings of the reproducibility workshop. 2017</p>
<p>Mastering the game of go with deep neural networks and tree search. David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den, Julian Driessche, Ioannis Schrittwieser, Veda Antonoglou, Marc Panneershelvam, Lanctot, nature. 52975874842016</p>
<p>Enhancing the quality and transparency of health research. The, Network, 2021</p>
<p>Consolidated standards of reporting trials (consort) and the completeness of reporting of randomised controlled trials (rcts) published in medical journals. Lucy Turner, Larissa Shamseer, Laura Douglas G Altman, Jodi Weeks, Thilo Peters, Sofia Kober, Kenneth F Dias, Amy C Schulz, David Plint, Moher, Cochrane Database of Systematic Reviews. 112012</p>
<p>Open science now: A systematic literature review for an integrated definition. Rubén Vicente, -Sáez , Clara Martínez-Fuentes, Journal of business research. 882018</p>
<p>Fermat's Last Theorem. Wikipedia, ID: 944945734March 2020</p>            </div>
        </div>

    </div>
</body>
</html>
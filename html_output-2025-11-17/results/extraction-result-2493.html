<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2493 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2493</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2493</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-67.html">extraction-schema-67</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <p><strong>Paper ID:</strong> paper-257353594</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2303.01560v4.pdf" target="_blank">Active Learning and Bayesian Optimization: a Unified Perspective to Learn with a Goal</a></p>
                <p><strong>Paper Abstract:</strong> Science and Engineering applications are typically associated with expensive optimization problems to identify optimal design solutions and states of the system of interest. Bayesian optimization and active learning compute surrogate models through efficient adaptive sampling schemes to assist and accelerate this search task toward a given optimization goal. Both those methodologies are driven by specific infill/learning criteria which quantify the utility with respect to the set goal of evaluating the objective function for unknown combinations of optimization variables. While the two fields have seen an exponential growth in popularity in the past decades, their dualism and synergy have received relatively little attention to date. This paper discusses and formalizes the synergy between Bayesian optimization and active learning as symbiotic adaptive sampling methodologies driven by common principles. In particular, we demonstrate this unified perspective through the formalization of the analogy between the Bayesian infill criteria and active learning criteria as driving principles of both the goal-driven procedures. To support our original perspective, we propose a general classification of adaptive sampling techniques to highlight similarities and differences between the vast families of adaptive sampling, active learning, and Bayesian optimization. Accordingly, the synergy is demonstrated mapping the Bayesian infill criteria with the active learning criteria, and is formalized for searches informed by both a single information source and multiple levels of fidelity. In addition, we provide guidelines to apply those learning criteria investigating the performance of different Bayesian schemes for a variety of benchmark problems to highlight benefits and limitations over mathematical properties that characterize real-world applications.</p>
                <p><strong>Cost:</strong> 0.03</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2493.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2493.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bayesian Optimization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A surrogate-assisted, goal-driven adaptive sampling framework that fits a probabilistic model (typically a Gaussian Process) to expensive black-box objective functions and selects new evaluations by maximizing an acquisition function that trades off exploration and exploitation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Bayesian Optimization (single-fidelity)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Constructs a probabilistic surrogate (Gaussian Process) over the objective f(x); at each iteration computes an acquisition function U(x) (e.g., EI, PI, MES) on the posterior and selects x_new = argmax_x U(x). After evaluating the expensive oracle at x_new, the dataset is augmented and the GP posterior updated, producing a mutually-informing loop between learner (acquisition function) and surrogate.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>General-purpose expensive black-box optimization in science and engineering (design optimization, hyperparameter tuning, robotics, materials, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocate each expensive evaluation to the input x that maximizes a chosen acquisition function (expected utility) computed from the surrogate posterior; allocation decisions are made sequentially and adaptively based on updated posterior mean and variance.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Number of high-fidelity function evaluations (implicit), and in experiments a simple count of evaluations; in multifidelity extensions cost is expressed with λ(l) per-fidelity (see MFBO entries).</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Depends on acquisition: expected improvement (EI), probability of improvement (PI), mutual information / entropy reduction (ES/MES), knowledge-gradient (KG), or non-myopic expected gains.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Encoded in acquisition functions: EI mixes exploitation (low mean) and exploration (high std); PI is exploitative; entropy-based acquisitions explicitly aim to reduce uncertainty about optimum location/value; non-myopic methods optimize expected future gains.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>No explicit generic diversity mechanism in base BO beyond acquisition-driven spread (exploration terms); diversity arises indirectly through uncertainty-driven exploration in acquisition functions.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed computational budget (number/cost of function evaluations); experiments used cumulative computational cost B with B_max = 100 * D (problem dimension).</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Acquisition functions are evaluated under a limited budget by sequential selection; multifidelity extensions explicitly include cost terms to trade off cost vs information (see MFBO).</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Not explicitly defined as a separate novelty metric; goal is reduction of objective value (finding global minimum); optimization performance measured by proximity to analytic optimum.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>ε_x (domain search error) and ε_f (objective/goal error) as defined in paper: ε_x = ||x* - x_found|| / sqrt(N) and ε_f = (f(x_found) - f*) / (f_max - f*); budgets reported in units of cumulative cost B (no universal numeric efficiency percentages provided).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against multifidelity variants and variant acquisition functions (EI, PI, MES); implicit baseline of naive or random search referenced but experimental baselines were single-fidelity BO vs multifidelity BO variants.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Single-fidelity BO (EI/PI/MES) generally requires more computational budget than multifidelity counterparts; PI often more exploitative and faster in some low-dimensional problems, EI balances exploration/exploitation and performs robustly, MES can over-explore in noisy/discontinuous settings.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Qualitative: multifidelity extensions deliver 'significant reduction' in computational budget relative to single-fidelity BO in many benchmarks (no universal numeric value reported); paper reports median reductions across benchmark suites.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Yes — paper analyzes exploration vs exploitation trade-offs induced by acquisition functions and cost/accuracy trade-offs in multifidelity BO, documenting when exploitation (PI) is favorable (low-dimensional), when exploration (ES/MES) helps (high multimodality/noise), and how cost weighting shifts allocations toward cheaper fidelities.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Recommendations: (1) Pure exploitation can be effective in low-dimensional problems; (2) Exploration/representativeness is important in high-dimensional problems; (3) Balanced acquisition (mix of exploration and exploitation) yields robust performance when problem structure is unknown; (4) When resources are tight, multifidelity BO leveraging cheap low-fidelity models plus few high-fidelity evaluations is recommended.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Active Learning and Bayesian Optimization: a Unified Perspective to Learn with a Goal', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2493.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2493.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Expected Improvement</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An acquisition function that selects the next evaluation to maximize the expected decrease (improvement) of the best-so-far objective value under the surrogate posterior; balances mean and uncertainty via closed-form expression for GPs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Expected Improvement acquisition (single-fidelity)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Computes U_EI(x) = σ(x)[I(x)Φ(I(x)) + φ(I(x))] with I(x) = (f(x*) - μ(x)) / σ(x) under a GP; it is inexpensive to evaluate and gradients are simple, enabling efficient maximization of the acquisition to select x_new.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Used inside BO for expensive black-box optimization across engineering/science benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocates evaluations to x maximizing expected gain (EI), thus investing resources where either predicted mean is low (exploitation) or uncertainty is high (exploration) in a principled expected-value sense.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Number of function evaluations; EI computation itself is cheap (closed-form for GPs), so cost metric focuses on oracle cost.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Expected improvement in objective value (an expected-utility metric), combining predicted mean and predictive variance.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Intrinsic trade-off through I(x): higher σ(x) (uncertainty) raises EI (exploration), lower μ(x) (promising mean) raises EI (exploitation); the relative influence is governed by current posterior.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>No explicit diversity term; exploration component spreads samples via uncertainty but clustering can still occur.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed eval budget or cumulative cost (in experiments cumulative cost B used).</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>EI naturally prioritizes points with high expected benefit; in multifidelity form (MFEI) additional modifiers (correlation, noise, cost ratios) are applied to account for cost.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Breakthroughs equate to large expected reductions in f; EI directly targets such expected decreases.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Measured using ε_x and ε_f across benchmark problems; EI showed robust performance across many test functions and balanced exploration/exploitation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against PI, MES, and multifidelity variants (MFEI, MFPI, MFMES).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>EI often balances well and is competitive; in some low-dimensional problems PI can be faster, while MES may over-explore and be slower.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Qualitative improvement in many benchmarks when balanced with multifidelity extensions; specific numeric gains depend on problem and fidelity library.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper details how EI trades mean vs variance and how multifidelity modifiers change the allocation; MFEI multiplies EI by correlation/noise/cost factors to account for cheaper fidelities.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>EI (and MFEI) are recommended when a balance between exploration and exploitation is needed or when intermediate-fidelity levels provide useful information; MFEI's cost-weighting encourages cheaper fidelities for exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Active Learning and Bayesian Optimization: a Unified Perspective to Learn with a Goal', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2493.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2493.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Probability of Improvement</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An acquisition function that selects points maximizing the posterior probability of improving upon the current best observed objective value; tends to be more exploitative than EI.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Probability of Improvement acquisition (single-fidelity and multifidelity MFPI)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Under a GP, PI(x) = Φ(I(x)) where I(x) = (f(x*) - μ(x)) / σ(x). In multifidelity MFPI, PI at high-fidelity is modified by factors for correlation between fidelities, cost ratios, and a density term to penalize over-sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Bayesian optimization for expensive black-box functions; effective in low-dimensional and well-behaved objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocates evaluations toward points with highest posterior probability to beat the current best — strongly exploitative; multifidelity MFPI also adjusts for fidelity correlation and cost.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Count of oracle evaluations; in MFPI includes λ(l) cost per fidelity and may incorporate sample density penalties to control clustering.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Probability of reducing the minimum (probability of improvement); in MFPI this is modulated by correlation and cost terms encoding effective expected utility.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Primarily exploitation (prefers low μ(x)); it can increase with σ(x) only when μ(x) is near the current best; MFPI's density term η_3 penalizes local oversampling promoting some exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>MFPI includes η_3(x,l) = ∏_{i=1}^{n_l} (1 - R_{x,x_i^{(l)}}) which reduces acquisition in regions with high sampling density to discourage clustering.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed cost/budget (cumulative λ(l)); Bmax specified per experiment.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>MFPI includes explicit cost weighting η_2(l) = λ(L)/λ(l) and correlation η_1(x,l) to prefer cheaper fidelities when informative enough, and η_3 to avoid wasting budget on redundancy.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Improvement of best observed objective (same as PI); large probabilities imply higher chance of breakthrough.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>ε_x, ε_f measured across benchmarks; MFPI often produced the best performance (fastest convergence) on several low-dimensional benchmarks (Forrester family).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against EI, MES, and multifidelity EI/MES variants.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>MFPI achieved superior performance in several benchmarks (especially low-dimensional continuous problems) vs single-fidelity PI/EI and other multifidelity variants in the experiments, by concentrating resources on promising regions and using low-fidelity data when correlation supports it.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Described as 'significant reduction' in required computational budget on many tests; exact percentages not universally reported.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>MFPI explicitly balances exploitation (PI) with cost and sample-density penalties, analyzing effects of correlation and cost in allocating queries across fidelities.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>When low-fidelity models are sufficiently correlated and inexpensive, MFPI is effective: preferentially use cheap informative evaluations for aggressive exploitation while penalizing redundant local sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Active Learning and Bayesian Optimization: a Unified Perspective to Learn with a Goal', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2493.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2493.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MES</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Max-value Entropy Search</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An information-theoretic acquisition that selects evaluations that maximally reduce entropy about the minimum function value (or the location of the optimum), aiming at highest expected information gain about the optimum.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Max-value Entropy Search (MES) and its multifidelity variant MFMES</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>MES maximizes the expected reduction in entropy of the minimum value f*; mathematically U_MES(x) = H(p(f*)) - E_{f(x)}[H(p(f* | f(x)))], approximated via sampling. Multifidelity MFMES measures entropy reduction per unit cost (dividing by λ(l)) and uses Monte Carlo approximations over fidelities.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>BO for expensive multimodal/noisy optimization, especially when direct information about optimum uncertainty is desired.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocates evaluations to points expected to maximally reduce uncertainty about the global optimum or its value (information gain), potentially across fidelity levels in multifidelity variants with cost normalization.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Computation cost of oracle evaluations λ(l); MES itself has high computational cost for acquisition estimation (Monte Carlo approximations), and MFMES divides information gain by per-fidelity cost.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Mutual information / differential entropy reduction about f* (minimum value) or x* (minimizer location).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Explicitly exploration-oriented since it targets uncertainty reduction about the optimum; in practice it balances by valuing information that refines global optimum belief rather than immediate improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Promotes exploration across domain by design (uncertainty reduction); multifidelity MFMES also weights by fidelity cost which encourages using low-fidelity evaluations for broad exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed computational budget (evaluation cost); acquisition computation is also computationally expensive.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>MFMES divides information gain by λ(l) or otherwise penalizes expensive fidelities to prefer cheaper sources that still deliver information, and uses Monte Carlo sampling to approximate expected entropy reductions.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Information gain about the global minimum (reducing entropy) — breakthroughs are identified by large reductions in posterior entropy about f* or x*.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>ε_x and ε_f across benchmarks; MES sometimes over-explores and uses more budget, performing worse on some low-dimensional or discontinuous problems; MFMES performs better with available low-fidelity sources.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against EI, PI, MFEI, MFPI.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>MES can over-explore and be less budget-efficient on some problems; MFMES mitigates this by leveraging cheap fidelities and often attains good performance when multiple fidelities exist, especially for high-dimensional or highly multimodal problems.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Multifidelity MES (MFMES) shows notable budget efficiency gains vs single-fidelity MES in many benchmarks by using low-fidelity exploration, though exact numerical gains vary by problem.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper documents that entropy-based methods prioritize information gain and can over-explore, especially under noise; multifidelity cost-weighting mitigates this by shifting early exploration to low-cost models.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Use entropy-based acquisitions when global uncertainty about the optimum is critical; pair with multifidelity cost-normalization to limit expensive evaluations and use low-fidelity data for broad exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Active Learning and Bayesian Optimization: a Unified Perspective to Learn with a Goal', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2493.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2493.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MFBO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multifidelity Bayesian Optimization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An extension of BO that treats multiple information sources (fidelities) with different accuracies and costs, learning a joint surrogate across fidelities and selecting both location and fidelity for each query to optimize cost-effectively.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Multifidelity Bayesian Optimization (MFBO)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Builds a multifidelity surrogate (e.g., an autoregressive GP cascade f^{(l)}(x) = ρ^{(l-1)} f^{(l-1)}(x) + ζ^{(l)}(x)), computes multifidelity acquisition U(x,l) that jointly selects (x,l) to query, and updates the multifidelity posterior; acquisition formulations incorporate fidelity correlation, noise, and evaluation cost.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Computationally expensive design optimization problems with available cheaper approximate simulators or experiments (engineering design, physics simulations, hyperparameter tuning with cheap proxies).</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>At each iteration choose (x,l) = argmax_{x,l} U(x,l) where U extends single-fidelity infill by incorporating correlation with high-fidelity target, noise-adjustment, and cost ratios to trade off accuracy vs expense; low-fidelity queries are preferred for exploration if sufficiently informative per cost.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Per-fidelity computational cost λ(l); cumulative budget B = Σ λ(l)_i consumed; experiments set B_max = 100 * D.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Multifidelity variants use modified EI/PI/MES where information is measured as expected improvement at high-fidelity, probability of improvement at high-fidelity, or entropy reduction about f* (with expectations taken over multifidelity posterior); modifiers include posterior correlation corr(f^{(l)}, f^{(L)}) and noise adjustments.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Infill criterion extended to (x,l) with terms that trade exploration (uncertainty reduction often via cheaper fidelities) vs exploitation (high-fidelity evaluations near predicted minima); explicit cost weighting (α_3, η_2, division by λ(l)) shifts allocation toward cheaper sources for exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>MFPI includes a sample-density penalty η_3 that discourages repeated sampling in the same region for a given fidelity; other multifidelity acquisitions implicitly promote diversity via exploration terms and use of low-fidelity broad queries.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Cumulative computational cost budget (monetary/time proxy) and limited total number of expensive evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Acquisitions include explicit cost factors (α_3, η_2, division by λ(l)) so the per-query utility is normalized by cost; policy balances many cheap low-fidelity queries with fewer costly high-fidelity ones to remain within budget.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Same optimization metrics (ε_f, ε_x) and implicit potential for 'breakthrough' measured by large decreases in high-fidelity objective; also measured by information gain about the global optimum per unit cost.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>ε_x and ε_f as functions of cumulative cost B; reported median curves over repeated trials on benchmark functions (Forrester, Rosenbrock, Rastrigin, ALOS, spring-mass, Paciorek).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against single-fidelity BO algorithms (EI, PI, MES) and among multifidelity acquisition variants (MFEI, MFPI, MFMES, MFPES).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>MFBO variants consistently outperform single-fidelity BO in budget-limited settings; MFPI and MFEI were top performers on many low-dimensional benchmarks, MFMES excelled in high-dimensional or highly multimodal settings when full fidelity library available.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Described as substantial reductions in required high-fidelity computational budget; e.g., multifidelity algorithms identify optima with a fraction of cost required by single-fidelity counterparts (exact percentages vary by benchmark).</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Thorough: analyzes correlation between fidelities, noise, cost ratios, and dimensionality; highlights when low-fidelity exploration is valuable (high correlation, low cost) and when it is not (poor correlation, misleading intermediate fidelities).</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Key insights: prefer low-fidelity evaluations for exploration if correlation with high-fidelity is high and cost savings are significant; use high-fidelity selectively for exploitation/refinement; include density penalties to avoid redundant sampling; choose MFEI/MFPI when aggressive exploitation is desired in low dims, MFMES for broader exploration in complex high-dim problems.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Active Learning and Bayesian Optimization: a Unified Perspective to Learn with a Goal', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2493.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2493.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MFEI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multifidelity Expected Improvement</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Multifidelity extension of EI that multiplies the single-fidelity EI (evaluated at top fidelity) by modifiers accounting for fidelity correlation, measurement noise, and cost ratios to select (x,l) pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Multifidelity Expected Improvement (MFEI)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Defines U_MFEI(x,l) = U_EI(x,L) * α1(x,l)^{α2} * α2(x,l)^{α3} * α3(l) (paper's formulation uses multiplicative α1,α2,α3), where α1 is posterior correlation corr(f^{(l)}, f^{(L)}), α2 adjusts for observation noise and variance reduction, and α3(l) = λ(L)/λ(l) biases selection toward cheaper fidelities when informative.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>MFBO for engineering optimization where several fidelity levels exist.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Selects (x,l) maximizing modified expected improvement per fidelity, implicitly balancing expected gain at high-fidelity with how informative and cheap the lower-fidelity source is.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Per-fidelity cost λ(l) and cumulative budget B; α3(l) uses λ ratios to bias selection.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Expected improvement at high-fidelity (U_EI(x,L)) modulated by correlation and noise adjustments representing expected utility contributed by fidelity l.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Retains EI's balance; α1 and α2 penalize low-fidelity choices that are weakly correlated or noisy (reducing exploitation benefit), while α3 favors cheap evaluations for exploration phases.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>No explicit diversity term in core MFEI, diversity arises from exploration encouraged by low-fidelity, cost-weighted queries.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Cumulative evaluation cost budget (λ(l)).</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>α3(l) explicitly scales expected improvement by cost ratio to allocate budget-efficiently across fidelities.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>High expected improvement at high-fidelity is proxy for potential major improvements (breakthroughs); MFEI seeks cost-effective routes to achieve those improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Evaluated via ε_x and ε_f curves over budget B in benchmarks; MFEI is among top performers across many multifidelity experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against MFPI, MFMES, and single-fidelity EI/PI/MES.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>MFEI performed well, particularly when using the full fidelity spectrum, benefiting from combining informativeness and representativeness/diversity via cost-aware exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Observed faster convergence to optima with less cumulative cost compared to single-fidelity EI; specific numerical gains depend on benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper discusses how α1 (correlation) and α2 (noise) can reduce benefit from cheaper fidelities and how α3 (cost) encourages use of cheap evaluations for exploration—thus providing a principled cost-vs-information tradeoff.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>MFEI is effective when lower fidelities are reasonably correlated and inexpensive: use them for exploration to cheaply reduce uncertainty, then switch to high-fidelity for refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Active Learning and Bayesian Optimization: a Unified Perspective to Learn with a Goal', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2493.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2493.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MFPI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multifidelity Probability of Improvement</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Multifidelity variant of PI that multiplies the high-fidelity PI by factors for fidelity correlation, cost ratio, and a sample-density penalty to discourage redundant sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Multifidelity Probability of Improvement (MFPI)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Defines U_MFPI(x,l) = U_PI(x,L) * η1(x,l) * η2(l) * η3(x,l) where η1 is corr(f^{(l)}, f^{(L)}), η2 = λ(L)/λ(l) is the cost ratio favoring cheap fidelities, and η3 is a product over existing l-fidelity samples of (1 - R(x,x_i^{(l)})) to penalize high-density regions.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>MFBO for low-to-moderate dimensional engineering optimization, where aggressive exploitation is effective and multiple fidelities are available.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Targets locations with high probability of improving the best high-fidelity value while (i) favoring informative lower fidelities when correlated and cheap, and (ii) penalizing redundant sampling to spread budget efficiently.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Per-fidelity cost λ(l) and cumulative budget B; η2 uses the cost ratio explicitly.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Probability of improvement at high-fidelity, modulated by fidelity correlation and cost; η3 indirectly encodes marginal information benefit by penalizing dense regions.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Strong exploitation (PI) core; η3 introduces exploration by discouraging repeated sampling; η2 encourages cheaper fidelities for exploratory effort while focusing high-cost evaluations where most beneficial.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Explicit via η3: sample-density based penalty that lowers acquisition for regions already well-sampled in a given fidelity, promoting spatial diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Cumulative computational cost (λ(l)) with B_max as experiment-level cap.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Explicit cost-scaling (η2) biases selection toward cheaper fidelities to stretch budget; density penalty prevents wasteful repeated evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Probability of achieving a new best high-fidelity value; MFPI aggressively concentrates budget where breakthrough likelihood is high but moderated by cost and sample redundancy.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>ε_x and ε_f over budget B; MFPI was top-performing on many benchmarks (notably Forrester) in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to MFEI, MFMES, single-fidelity PI/EI/MES.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>MFPI often achieved fastest convergence to analytic optima in low-dimensional test problems by effectively directing budget to promising regions and leveraging cheap informative fidelities.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Reported as significant computational budget reduction to reach optima relative to single-fidelity methods (no single-number universal claim).</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper highlights MFPI's balance of aggressive exploitation with cost-awareness and explicit density-based diversity to avoid over-exploitation.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>MFPI is particularly effective when cheap fidelities are well correlated and the problem benefits from exploitation; density penalty is important to avoid wasteful repeated queries.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Active Learning and Bayesian Optimization: a Unified Perspective to Learn with a Goal', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2493.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2493.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MFMES</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multifidelity Max-value Entropy Search</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multifidelity adaptation of MES that measures expected entropy reduction about the global minimum per unit cost and selects fidelity-level-aware queries accordingly (Monte Carlo approximations used).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Multifidelity Max-Value Entropy Search (MFMES)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>U_MFMES(x,l) = [H(p(f^{(l)} | D)) - E_{f^{(l)}(x)} H(p(f^{(l)} | f*^{(L)}, D))] / λ(l) (paper reports a formulation dividing info gain by cost), approximated by Monte Carlo sampling across fidelities so cheap fidelities can provide low-cost global information.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>MFBO for complex, high-dimensional, or highly multimodal problems where information-theoretic exploration across fidelities is valuable.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Select (x,l) that maximizes expected reduction in entropy about the high-fidelity minimum per unit cost, thereby allocating expensive high-fidelity calls only when they provide proportionally large information.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Per-fidelity cost λ(l); acquisition calculation itself is computationally expensive (Monte Carlo) and cost-normalized in acquisition.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Differential entropy reduction (mutual information) about the high-fidelity minimum/value measured under the multifidelity posterior.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Prioritizes exploration and global uncertainty reduction; by dividing by λ(l) it prefers low-cost fidelities for broad exploration and reserves high-fidelity queries for high-value refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Exploration-oriented information criterion disperses queries to where entropy reduction is highest; cost-normalization further encourages diverse low-cost sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Cumulative cost (λ(l)) and limited total budget; acquisition computations are themselves nontrivial.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Normalize information gain by cost and use Monte Carlo approximations to evaluate expected entropy reductions efficiently across fidelities.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Information gain about f* (large entropy reductions per unit cost indicate potential for breakthrough discovery).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>ε_x and ε_f across benchmark problems; MFMES performed well when many fidelities were available and in high-dimensional/multimodal problems.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to MFPI, MFEI, and single-fidelity MES/EI/PI.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>MFMES outperforms single-fidelity MES and competes favorably with other MFBO methods especially when full fidelity library exists and complex search landscape requires heavy exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Demonstrated notable improvements in convergence under constrained budget for some challenging benchmarks, but at increased acquisition computation cost.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper discusses MFMES tendency to over-explore if not cost-normalized; cost-normalization and use of cheap fidelities are crucial to make it budget-efficient.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Use MFMES for hard multimodal/noisy/high-dim problems where global information matters; ensure adequate low-cost fidelities and Monte Carlo approximations to control computational overhead.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Active Learning and Bayesian Optimization: a Unified Perspective to Learn with a Goal', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2493.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e2493.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pool-AL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pool-based Active Learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An active learning paradigm where a learner selects examples from a large unlabelled pool to be labelled by an oracle, using a surrogate (classifier/regressor) and an utility function (informativeness/representativeness/diversity) to pick queries.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Pool-based Active Learning (single-oracle and multi-oracle extensions)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Maintains a surrogate model p_N(f|x) trained on currently labeled set D_N; a utility U(p_N(f|x)) scores unlabeled candidates and selects x_{N+1} = argmax U(·). Utility functions implement informativeness (uncertainty, entropy, query-by-committee), representativeness/diversity (clustering, k-center, optimal experimental design), or hybrids (serial, selection, parallel). Multi-oracle variants also select which annotator/fidelity to query, incorporating annotator cost and reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Supervised learning problems requiring expensive labels (e.g., experiments, human annotation); conceptual mapping used to relate BO acquisitions to active learning criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Select unlabeled instances to label that maximize expected utility: pure informativeness chooses uncertain points; representativeness chooses cluster centers; hybrids combine both to avoid wasteful labeling; multi-oracle versions also select which oracle to query to trade off cost and reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Labeling cost per oracle (monetary/time), number of labels acquired; some methods model oracle-specific costs.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Predictive uncertainty (entropy, margin, variance), model-change, expected reduction of loss, or mutual information depending on algorithm.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Not framed as exploration/exploitation in same sense as BO but trade-offs exist: informativeness ~ exploitation (query where most helpful for goal), representativeness/diversity ~ exploration (covering domain structure); hybrids explicitly balance them.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Explicit methods (clustering, k-center, optimal design, density-weighted selection) implement diversity/representativeness; parallel-form hybrids weight informativeness with diversity scores.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed labeling budget or cost-constrained labeling (monetary/time/resource); multi-oracle algorithms incorporate per-oracle cost constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Selection strategies incorporate cost-awareness (cost-accuracy tradeoff), relabeling strategies, or selection of cheaper oracles when appropriate; some methods use cost-effective criteria to maximize utility per cost.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Generally not explicit; in classification/regression context goal is model accuracy improvement (reduction in loss); conceptual mapping to BO regards breakthroughs as attaining target performance faster.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Common metrics: classification/regression error, label efficiency (accuracy vs number of labels), cost-weighted performance. Paper maps active learning criteria to BO infill criteria but did not run active-learning experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Random sampling, uncertainty-only sampling, cluster-only sampling; hybrid methods compared to pure criteria forms in literature.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Literature surveyed indicates hybrids that combine informativeness and representativeness/diversity often outperform pure strategies, particularly under limited budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>General observation: cost-aware and hybrid active learning reduces labels needed for a given performance compared to random or single-criterion sampling (no numerical values provided in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper frames active learning criteria as analogous to BO infill trade-offs and discusses multi-oracle cost-accuracy tradeoffs; recommends balanced criteria when problem structure is unknown.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Recommendation to combine informativeness with representativeness/diversity (via serial, selection, or parallel forms) and to use cost-aware oracle selection when multiple annotation sources exist.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Active Learning and Bayesian Optimization: a Unified Perspective to Learn with a Goal', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2493.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e2493.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multi-oracle AL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-oracle / Annotator-aware Active Learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Active learning extensions that select both which data points to query and which oracle/annotator or fidelity level to use, trading off label accuracy, annotator reliability, and cost.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Multi-oracle Active Learning (relabeling, repeating-labeling, probabilistic, transfer-knowledge, cost-effective)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Frameworks include relabeling (query multiple oracles and majority-vote), repeating-labeling (aggregate repeated noisy labels), probabilistic models estimating per-oracle reliability, transfer learning approaches to infer oracle accuracy from auxiliary domains, and cost-aware selection optimizing accuracy per cost. Decision selects (x,oracle) at each iteration to maximize expected net utility.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Scenarios with multiple imperfect information sources: crowdsourcing, multi-fidelity experiments, simulations with different approximations.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Decide which (x, oracle) to query by optimizing expected information gain weighted by oracle reliability and cost; some methods repeatedly query oracles to reduce noise while others estimate oracle accuracies to avoid redundant querying.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Oracle-specific cost (monetary/time/computation) and number of oracle queries; aggregated into expected cost for acquisition decision.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Expected model improvement considering oracle reliability (probabilistic label models), reduction in expected loss or uncertainty, and sometimes information-theoretic measures.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Balancing accurate but costly oracles (exploitation of high-quality info) vs cheap noisy oracles (exploration and coverage); hybrid criteria of informativeness and representativeness/diversity extended to include oracle selection.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Representativeness/diversity still used to spread queries; cost-effective algorithms favor less expensive oracles to permit diverse sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Monetary/computational budget across oracle queries; constraints considered explicitly in cost-aware methods.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Algorithms optimize utility per cost or use constrained optimization to select queries under budget; probabilistic models estimate marginal value of oracle information to decide whether to spend budget on higher-quality sources.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>High-value discoveries are those where high-cost/high-reliability oracles produce substantially better model updates; measured via improvement in model accuracy or task-specific utility per cost.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Label-efficiency, accuracy vs cost curves in relevant literature (not directly experimentally evaluated in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Relabeling/repeating strategies, random oracle selection, or single-best-oracle baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Surveyed literature indicates cost-aware and probabilistic multi-oracle strategies outperform naive relabeling/repetition by better allocating queries to informative/or cost-effective oracles.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Reported qualitatively: better use of budget through selective oracle querying and probabilistic estimation of oracle accuracies; no single-number claims in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper discusses the extension of the three learning criteria (informativeness, representativeness, diversity) to include oracle accuracy/cost tradeoffs and how balancing these drives efficient multi-oracle allocations.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Advice: prefer cheaper oracles for representativeness/diversity and reserve expensive, reliable oracles for high-informativeness queries; estimate oracle reliability probabilistically to guide selection.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Active Learning and Bayesian Optimization: a Unified Perspective to Learn with a Goal', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2493.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e2493.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AIS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adaptive Importance Sampling</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An adaptive probing method that iteratively updates proposal distributions to sample from complex target distributions efficiently using importance weights, improving sampling quality without building a global surrogate.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Adaptive Importance Sampling (AIS)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Initialize proposals q_n(x|θ_{n,1}), draw samples, compute importance weights w_n = π(x)/q(x), form self-normalized estimators, and adapt proposal parameters θ based on weighted samples; iterate to concentrate sampling where target π is large.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Statistical inference, rare-event estimation, Bayesian computation, and adaptive probing scenarios where building a surrogate is not used.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocate sampling density via adaptation of proposal distributions based on importance weights from prior draws; concentrates future computational budget in regions suggested by weighted evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Number of samples drawn and computational cost per sample; no budget-per-fidelity formalism in AIS as described in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Implicit via importance weights reflecting representativeness to target π(x); objective is to reduce estimator variance rather than explicit mutual information.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Adaptation of proposals progressively exploits high-weight regions while proposal diversity ensures exploration; no formal exploration/exploitation acquisition function.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Multiple proposals (mixture) can maintain diversity; adaptation can include defensive mixtures to avoid premature collapse.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Number of Monte Carlo samples / computational sampling budget.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Adapt proposals to maximize estimator efficiency given sample budget; not framed as explicit cost-vs-information optimization with external cost units.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Not applicable in optimization sense; goal is estimator accuracy/variance reduction.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Estimator variance, effective sample size, accuracy of integral estimates (I(f)).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Standard importance sampling, defensive mixtures, non-adaptive proposals, MCMC.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Adaptive schemes improve sampling efficiency and estimator accuracy vs static proposals in many settings (literature cited).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Improved effective sample size and reduced variance per unit sampling cost versus non-adaptive samplers (no single-number claim in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper contrasts AIS with surrogate-based adaptive learning: AIS allocates resources based solely on sample weighting without constructing a global surrogate and thus lacks goal-driven learning properties.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>AIS is effective for inference tasks to concentrate sampling where target density is high but is not considered a goal-driven learning method for optimization in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Active Learning and Bayesian Optimization: a Unified Perspective to Learn with a Goal', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2493.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e2493.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ASC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adaptive Stochastic Collocation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An adaptive modeling approach that builds sparse-grid polynomial interpolants in stochastic input space by adaptively adding collocation nodes where an error indicator is high, improving surrogate approximation without the surrogate informing sampling for optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Adaptive Stochastic Collocation (ASC)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Constructs sparse-grid tensor-product interpolants using difference formulas and adaptively refines the grid by adding nodes in subspaces where error indicators (or estimators) are largest; the surrogate is for uncertainty propagation, not used as a decision-making learner for optimization queries.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Uncertainty quantification, propagation, and surrogate construction for PDE-based models and stochastic simulations.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocate collocation points to subspaces with largest estimated interpolation error, concentrating computational resources where surrogate error is greatest.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Number of collocation nodes and high-fidelity model solves required (growing exponentially with dimension if not sparse-adapted).</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Error indicators/estimators (e.g., hierarchical surplus) used to guide node additions; not formulated as expected-information or mutual information.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Refinement favors regions with high error (akin to exploitation of error hotspots) while sparse-grid design maintains some spread (representativeness); no explicit exploration/exploitation acquisition.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Sparse-grid selection distributes nodes across coordinate directions, and adaptivity focuses on important directions to control growth; not an explicit diversity-promotion for hypothesis search.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed computational budget in number of collocation solves.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Adaptive refinement picks nodes that most reduce interpolation error per added node, thereby aiming to maximize surrogate quality under node budget.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Not applicable; metric is surrogate error reduction rather than discovery of breakthroughs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Interpolation error, surrogate accuracy, error indicators per number of collocation points.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Full tensor grids, non-adaptive sparse grids, other surrogate constructions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>ASC reduces required nodes vs full tensor grids and non-adaptive approaches for many problems by targeting important directions.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Reduces computational burden compared to full tensor-product sampling, especially in moderate dimensions; efficiency depends on problem smoothness and anisotropy.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper contrasts adaptive modeling (ASC) with adaptive learning: ASC builds surrogates but does not use them to inform goal-driven sampling decisions, so it doesn't directly balance cost vs information for optimization objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>ASC is recommended when the goal is surrogate accuracy for uncertainty quantification rather than goal-driven optimizer sampling; resource allocation focuses on error reduction per collocation node.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Active Learning and Bayesian Optimization: a Unified Perspective to Learn with a Goal', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Efficient global optimization of expensive black-box functions <em>(Rating: 2)</em></li>
                <li>Max-value entropy search for efficient bayesian optimization <em>(Rating: 2)</em></li>
                <li>Multi-fidelity Bayesian optimization with continuous approximations <em>(Rating: 2)</em></li>
                <li>Multi-fidelity Bayesian optimization with max-value entropy search and its parallelization <em>(Rating: 2)</em></li>
                <li>A survey of multifidelity methods in uncertainty propagation, inference, and optimization <em>(Rating: 2)</em></li>
                <li>Adaptive importance sampling: The past, the present, and the future <em>(Rating: 1)</em></li>
                <li>Pool-based active learning in approximate linear regression <em>(Rating: 1)</em></li>
                <li>RRA: Resource aware active learning for multifidelity efficient optimization <em>(Rating: 2)</em></li>
                <li>Non-myopic multifidelity bayesian optimization <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2493",
    "paper_id": "paper-257353594",
    "extraction_schema_id": "extraction-schema-67",
    "extracted_data": [
        {
            "name_short": "BO",
            "name_full": "Bayesian Optimization",
            "brief_description": "A surrogate-assisted, goal-driven adaptive sampling framework that fits a probabilistic model (typically a Gaussian Process) to expensive black-box objective functions and selects new evaluations by maximizing an acquisition function that trades off exploration and exploitation.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Bayesian Optimization (single-fidelity)",
            "system_description": "Constructs a probabilistic surrogate (Gaussian Process) over the objective f(x); at each iteration computes an acquisition function U(x) (e.g., EI, PI, MES) on the posterior and selects x_new = argmax_x U(x). After evaluating the expensive oracle at x_new, the dataset is augmented and the GP posterior updated, producing a mutually-informing loop between learner (acquisition function) and surrogate.",
            "application_domain": "General-purpose expensive black-box optimization in science and engineering (design optimization, hyperparameter tuning, robotics, materials, etc.)",
            "resource_allocation_strategy": "Allocate each expensive evaluation to the input x that maximizes a chosen acquisition function (expected utility) computed from the surrogate posterior; allocation decisions are made sequentially and adaptively based on updated posterior mean and variance.",
            "computational_cost_metric": "Number of high-fidelity function evaluations (implicit), and in experiments a simple count of evaluations; in multifidelity extensions cost is expressed with λ(l) per-fidelity (see MFBO entries).",
            "information_gain_metric": "Depends on acquisition: expected improvement (EI), probability of improvement (PI), mutual information / entropy reduction (ES/MES), knowledge-gradient (KG), or non-myopic expected gains.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Encoded in acquisition functions: EI mixes exploitation (low mean) and exploration (high std); PI is exploitative; entropy-based acquisitions explicitly aim to reduce uncertainty about optimum location/value; non-myopic methods optimize expected future gains.",
            "diversity_mechanism": "No explicit generic diversity mechanism in base BO beyond acquisition-driven spread (exploration terms); diversity arises indirectly through uncertainty-driven exploration in acquisition functions.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Fixed computational budget (number/cost of function evaluations); experiments used cumulative computational cost B with B_max = 100 * D (problem dimension).",
            "budget_constraint_handling": "Acquisition functions are evaluated under a limited budget by sequential selection; multifidelity extensions explicitly include cost terms to trade off cost vs information (see MFBO).",
            "breakthrough_discovery_metric": "Not explicitly defined as a separate novelty metric; goal is reduction of objective value (finding global minimum); optimization performance measured by proximity to analytic optimum.",
            "performance_metrics": "ε_x (domain search error) and ε_f (objective/goal error) as defined in paper: ε_x = ||x* - x_found|| / sqrt(N) and ε_f = (f(x_found) - f*) / (f_max - f*); budgets reported in units of cumulative cost B (no universal numeric efficiency percentages provided).",
            "comparison_baseline": "Compared against multifidelity variants and variant acquisition functions (EI, PI, MES); implicit baseline of naive or random search referenced but experimental baselines were single-fidelity BO vs multifidelity BO variants.",
            "performance_vs_baseline": "Single-fidelity BO (EI/PI/MES) generally requires more computational budget than multifidelity counterparts; PI often more exploitative and faster in some low-dimensional problems, EI balances exploration/exploitation and performs robustly, MES can over-explore in noisy/discontinuous settings.",
            "efficiency_gain": "Qualitative: multifidelity extensions deliver 'significant reduction' in computational budget relative to single-fidelity BO in many benchmarks (no universal numeric value reported); paper reports median reductions across benchmark suites.",
            "tradeoff_analysis": "Yes — paper analyzes exploration vs exploitation trade-offs induced by acquisition functions and cost/accuracy trade-offs in multifidelity BO, documenting when exploitation (PI) is favorable (low-dimensional), when exploration (ES/MES) helps (high multimodality/noise), and how cost weighting shifts allocations toward cheaper fidelities.",
            "optimal_allocation_findings": "Recommendations: (1) Pure exploitation can be effective in low-dimensional problems; (2) Exploration/representativeness is important in high-dimensional problems; (3) Balanced acquisition (mix of exploration and exploitation) yields robust performance when problem structure is unknown; (4) When resources are tight, multifidelity BO leveraging cheap low-fidelity models plus few high-fidelity evaluations is recommended.",
            "uuid": "e2493.0",
            "source_info": {
                "paper_title": "Active Learning and Bayesian Optimization: a Unified Perspective to Learn with a Goal",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "EI",
            "name_full": "Expected Improvement",
            "brief_description": "An acquisition function that selects the next evaluation to maximize the expected decrease (improvement) of the best-so-far objective value under the surrogate posterior; balances mean and uncertainty via closed-form expression for GPs.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Expected Improvement acquisition (single-fidelity)",
            "system_description": "Computes U_EI(x) = σ(x)[I(x)Φ(I(x)) + φ(I(x))] with I(x) = (f(x*) - μ(x)) / σ(x) under a GP; it is inexpensive to evaluate and gradients are simple, enabling efficient maximization of the acquisition to select x_new.",
            "application_domain": "Used inside BO for expensive black-box optimization across engineering/science benchmarks.",
            "resource_allocation_strategy": "Allocates evaluations to x maximizing expected gain (EI), thus investing resources where either predicted mean is low (exploitation) or uncertainty is high (exploration) in a principled expected-value sense.",
            "computational_cost_metric": "Number of function evaluations; EI computation itself is cheap (closed-form for GPs), so cost metric focuses on oracle cost.",
            "information_gain_metric": "Expected improvement in objective value (an expected-utility metric), combining predicted mean and predictive variance.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Intrinsic trade-off through I(x): higher σ(x) (uncertainty) raises EI (exploration), lower μ(x) (promising mean) raises EI (exploitation); the relative influence is governed by current posterior.",
            "diversity_mechanism": "No explicit diversity term; exploration component spreads samples via uncertainty but clustering can still occur.",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Fixed eval budget or cumulative cost (in experiments cumulative cost B used).",
            "budget_constraint_handling": "EI naturally prioritizes points with high expected benefit; in multifidelity form (MFEI) additional modifiers (correlation, noise, cost ratios) are applied to account for cost.",
            "breakthrough_discovery_metric": "Breakthroughs equate to large expected reductions in f; EI directly targets such expected decreases.",
            "performance_metrics": "Measured using ε_x and ε_f across benchmark problems; EI showed robust performance across many test functions and balanced exploration/exploitation.",
            "comparison_baseline": "Compared against PI, MES, and multifidelity variants (MFEI, MFPI, MFMES).",
            "performance_vs_baseline": "EI often balances well and is competitive; in some low-dimensional problems PI can be faster, while MES may over-explore and be slower.",
            "efficiency_gain": "Qualitative improvement in many benchmarks when balanced with multifidelity extensions; specific numeric gains depend on problem and fidelity library.",
            "tradeoff_analysis": "Paper details how EI trades mean vs variance and how multifidelity modifiers change the allocation; MFEI multiplies EI by correlation/noise/cost factors to account for cheaper fidelities.",
            "optimal_allocation_findings": "EI (and MFEI) are recommended when a balance between exploration and exploitation is needed or when intermediate-fidelity levels provide useful information; MFEI's cost-weighting encourages cheaper fidelities for exploration.",
            "uuid": "e2493.1",
            "source_info": {
                "paper_title": "Active Learning and Bayesian Optimization: a Unified Perspective to Learn with a Goal",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "PI",
            "name_full": "Probability of Improvement",
            "brief_description": "An acquisition function that selects points maximizing the posterior probability of improving upon the current best observed objective value; tends to be more exploitative than EI.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Probability of Improvement acquisition (single-fidelity and multifidelity MFPI)",
            "system_description": "Under a GP, PI(x) = Φ(I(x)) where I(x) = (f(x*) - μ(x)) / σ(x). In multifidelity MFPI, PI at high-fidelity is modified by factors for correlation between fidelities, cost ratios, and a density term to penalize over-sampling.",
            "application_domain": "Bayesian optimization for expensive black-box functions; effective in low-dimensional and well-behaved objectives.",
            "resource_allocation_strategy": "Allocates evaluations toward points with highest posterior probability to beat the current best — strongly exploitative; multifidelity MFPI also adjusts for fidelity correlation and cost.",
            "computational_cost_metric": "Count of oracle evaluations; in MFPI includes λ(l) cost per fidelity and may incorporate sample density penalties to control clustering.",
            "information_gain_metric": "Probability of reducing the minimum (probability of improvement); in MFPI this is modulated by correlation and cost terms encoding effective expected utility.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Primarily exploitation (prefers low μ(x)); it can increase with σ(x) only when μ(x) is near the current best; MFPI's density term η_3 penalizes local oversampling promoting some exploration.",
            "diversity_mechanism": "MFPI includes η_3(x,l) = ∏_{i=1}^{n_l} (1 - R_{x,x_i^{(l)}}) which reduces acquisition in regions with high sampling density to discourage clustering.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Fixed cost/budget (cumulative λ(l)); Bmax specified per experiment.",
            "budget_constraint_handling": "MFPI includes explicit cost weighting η_2(l) = λ(L)/λ(l) and correlation η_1(x,l) to prefer cheaper fidelities when informative enough, and η_3 to avoid wasting budget on redundancy.",
            "breakthrough_discovery_metric": "Improvement of best observed objective (same as PI); large probabilities imply higher chance of breakthrough.",
            "performance_metrics": "ε_x, ε_f measured across benchmarks; MFPI often produced the best performance (fastest convergence) on several low-dimensional benchmarks (Forrester family).",
            "comparison_baseline": "Compared against EI, MES, and multifidelity EI/MES variants.",
            "performance_vs_baseline": "MFPI achieved superior performance in several benchmarks (especially low-dimensional continuous problems) vs single-fidelity PI/EI and other multifidelity variants in the experiments, by concentrating resources on promising regions and using low-fidelity data when correlation supports it.",
            "efficiency_gain": "Described as 'significant reduction' in required computational budget on many tests; exact percentages not universally reported.",
            "tradeoff_analysis": "MFPI explicitly balances exploitation (PI) with cost and sample-density penalties, analyzing effects of correlation and cost in allocating queries across fidelities.",
            "optimal_allocation_findings": "When low-fidelity models are sufficiently correlated and inexpensive, MFPI is effective: preferentially use cheap informative evaluations for aggressive exploitation while penalizing redundant local sampling.",
            "uuid": "e2493.2",
            "source_info": {
                "paper_title": "Active Learning and Bayesian Optimization: a Unified Perspective to Learn with a Goal",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "MES",
            "name_full": "Max-value Entropy Search",
            "brief_description": "An information-theoretic acquisition that selects evaluations that maximally reduce entropy about the minimum function value (or the location of the optimum), aiming at highest expected information gain about the optimum.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Max-value Entropy Search (MES) and its multifidelity variant MFMES",
            "system_description": "MES maximizes the expected reduction in entropy of the minimum value f*; mathematically U_MES(x) = H(p(f*)) - E_{f(x)}[H(p(f* | f(x)))], approximated via sampling. Multifidelity MFMES measures entropy reduction per unit cost (dividing by λ(l)) and uses Monte Carlo approximations over fidelities.",
            "application_domain": "BO for expensive multimodal/noisy optimization, especially when direct information about optimum uncertainty is desired.",
            "resource_allocation_strategy": "Allocates evaluations to points expected to maximally reduce uncertainty about the global optimum or its value (information gain), potentially across fidelity levels in multifidelity variants with cost normalization.",
            "computational_cost_metric": "Computation cost of oracle evaluations λ(l); MES itself has high computational cost for acquisition estimation (Monte Carlo approximations), and MFMES divides information gain by per-fidelity cost.",
            "information_gain_metric": "Mutual information / differential entropy reduction about f* (minimum value) or x* (minimizer location).",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Explicitly exploration-oriented since it targets uncertainty reduction about the optimum; in practice it balances by valuing information that refines global optimum belief rather than immediate improvement.",
            "diversity_mechanism": "Promotes exploration across domain by design (uncertainty reduction); multifidelity MFMES also weights by fidelity cost which encourages using low-fidelity evaluations for broad exploration.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Fixed computational budget (evaluation cost); acquisition computation is also computationally expensive.",
            "budget_constraint_handling": "MFMES divides information gain by λ(l) or otherwise penalizes expensive fidelities to prefer cheaper sources that still deliver information, and uses Monte Carlo sampling to approximate expected entropy reductions.",
            "breakthrough_discovery_metric": "Information gain about the global minimum (reducing entropy) — breakthroughs are identified by large reductions in posterior entropy about f* or x*.",
            "performance_metrics": "ε_x and ε_f across benchmarks; MES sometimes over-explores and uses more budget, performing worse on some low-dimensional or discontinuous problems; MFMES performs better with available low-fidelity sources.",
            "comparison_baseline": "Compared against EI, PI, MFEI, MFPI.",
            "performance_vs_baseline": "MES can over-explore and be less budget-efficient on some problems; MFMES mitigates this by leveraging cheap fidelities and often attains good performance when multiple fidelities exist, especially for high-dimensional or highly multimodal problems.",
            "efficiency_gain": "Multifidelity MES (MFMES) shows notable budget efficiency gains vs single-fidelity MES in many benchmarks by using low-fidelity exploration, though exact numerical gains vary by problem.",
            "tradeoff_analysis": "Paper documents that entropy-based methods prioritize information gain and can over-explore, especially under noise; multifidelity cost-weighting mitigates this by shifting early exploration to low-cost models.",
            "optimal_allocation_findings": "Use entropy-based acquisitions when global uncertainty about the optimum is critical; pair with multifidelity cost-normalization to limit expensive evaluations and use low-fidelity data for broad exploration.",
            "uuid": "e2493.3",
            "source_info": {
                "paper_title": "Active Learning and Bayesian Optimization: a Unified Perspective to Learn with a Goal",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "MFBO",
            "name_full": "Multifidelity Bayesian Optimization",
            "brief_description": "An extension of BO that treats multiple information sources (fidelities) with different accuracies and costs, learning a joint surrogate across fidelities and selecting both location and fidelity for each query to optimize cost-effectively.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Multifidelity Bayesian Optimization (MFBO)",
            "system_description": "Builds a multifidelity surrogate (e.g., an autoregressive GP cascade f^{(l)}(x) = ρ^{(l-1)} f^{(l-1)}(x) + ζ^{(l)}(x)), computes multifidelity acquisition U(x,l) that jointly selects (x,l) to query, and updates the multifidelity posterior; acquisition formulations incorporate fidelity correlation, noise, and evaluation cost.",
            "application_domain": "Computationally expensive design optimization problems with available cheaper approximate simulators or experiments (engineering design, physics simulations, hyperparameter tuning with cheap proxies).",
            "resource_allocation_strategy": "At each iteration choose (x,l) = argmax_{x,l} U(x,l) where U extends single-fidelity infill by incorporating correlation with high-fidelity target, noise-adjustment, and cost ratios to trade off accuracy vs expense; low-fidelity queries are preferred for exploration if sufficiently informative per cost.",
            "computational_cost_metric": "Per-fidelity computational cost λ(l); cumulative budget B = Σ λ(l)_i consumed; experiments set B_max = 100 * D.",
            "information_gain_metric": "Multifidelity variants use modified EI/PI/MES where information is measured as expected improvement at high-fidelity, probability of improvement at high-fidelity, or entropy reduction about f* (with expectations taken over multifidelity posterior); modifiers include posterior correlation corr(f^{(l)}, f^{(L)}) and noise adjustments.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Infill criterion extended to (x,l) with terms that trade exploration (uncertainty reduction often via cheaper fidelities) vs exploitation (high-fidelity evaluations near predicted minima); explicit cost weighting (α_3, η_2, division by λ(l)) shifts allocation toward cheaper sources for exploration.",
            "diversity_mechanism": "MFPI includes a sample-density penalty η_3 that discourages repeated sampling in the same region for a given fidelity; other multifidelity acquisitions implicitly promote diversity via exploration terms and use of low-fidelity broad queries.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Cumulative computational cost budget (monetary/time proxy) and limited total number of expensive evaluations.",
            "budget_constraint_handling": "Acquisitions include explicit cost factors (α_3, η_2, division by λ(l)) so the per-query utility is normalized by cost; policy balances many cheap low-fidelity queries with fewer costly high-fidelity ones to remain within budget.",
            "breakthrough_discovery_metric": "Same optimization metrics (ε_f, ε_x) and implicit potential for 'breakthrough' measured by large decreases in high-fidelity objective; also measured by information gain about the global optimum per unit cost.",
            "performance_metrics": "ε_x and ε_f as functions of cumulative cost B; reported median curves over repeated trials on benchmark functions (Forrester, Rosenbrock, Rastrigin, ALOS, spring-mass, Paciorek).",
            "comparison_baseline": "Compared against single-fidelity BO algorithms (EI, PI, MES) and among multifidelity acquisition variants (MFEI, MFPI, MFMES, MFPES).",
            "performance_vs_baseline": "MFBO variants consistently outperform single-fidelity BO in budget-limited settings; MFPI and MFEI were top performers on many low-dimensional benchmarks, MFMES excelled in high-dimensional or highly multimodal settings when full fidelity library available.",
            "efficiency_gain": "Described as substantial reductions in required high-fidelity computational budget; e.g., multifidelity algorithms identify optima with a fraction of cost required by single-fidelity counterparts (exact percentages vary by benchmark).",
            "tradeoff_analysis": "Thorough: analyzes correlation between fidelities, noise, cost ratios, and dimensionality; highlights when low-fidelity exploration is valuable (high correlation, low cost) and when it is not (poor correlation, misleading intermediate fidelities).",
            "optimal_allocation_findings": "Key insights: prefer low-fidelity evaluations for exploration if correlation with high-fidelity is high and cost savings are significant; use high-fidelity selectively for exploitation/refinement; include density penalties to avoid redundant sampling; choose MFEI/MFPI when aggressive exploitation is desired in low dims, MFMES for broader exploration in complex high-dim problems.",
            "uuid": "e2493.4",
            "source_info": {
                "paper_title": "Active Learning and Bayesian Optimization: a Unified Perspective to Learn with a Goal",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "MFEI",
            "name_full": "Multifidelity Expected Improvement",
            "brief_description": "Multifidelity extension of EI that multiplies the single-fidelity EI (evaluated at top fidelity) by modifiers accounting for fidelity correlation, measurement noise, and cost ratios to select (x,l) pairs.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Multifidelity Expected Improvement (MFEI)",
            "system_description": "Defines U_MFEI(x,l) = U_EI(x,L) * α1(x,l)^{α2} * α2(x,l)^{α3} * α3(l) (paper's formulation uses multiplicative α1,α2,α3), where α1 is posterior correlation corr(f^{(l)}, f^{(L)}), α2 adjusts for observation noise and variance reduction, and α3(l) = λ(L)/λ(l) biases selection toward cheaper fidelities when informative.",
            "application_domain": "MFBO for engineering optimization where several fidelity levels exist.",
            "resource_allocation_strategy": "Selects (x,l) maximizing modified expected improvement per fidelity, implicitly balancing expected gain at high-fidelity with how informative and cheap the lower-fidelity source is.",
            "computational_cost_metric": "Per-fidelity cost λ(l) and cumulative budget B; α3(l) uses λ ratios to bias selection.",
            "information_gain_metric": "Expected improvement at high-fidelity (U_EI(x,L)) modulated by correlation and noise adjustments representing expected utility contributed by fidelity l.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Retains EI's balance; α1 and α2 penalize low-fidelity choices that are weakly correlated or noisy (reducing exploitation benefit), while α3 favors cheap evaluations for exploration phases.",
            "diversity_mechanism": "No explicit diversity term in core MFEI, diversity arises from exploration encouraged by low-fidelity, cost-weighted queries.",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Cumulative evaluation cost budget (λ(l)).",
            "budget_constraint_handling": "α3(l) explicitly scales expected improvement by cost ratio to allocate budget-efficiently across fidelities.",
            "breakthrough_discovery_metric": "High expected improvement at high-fidelity is proxy for potential major improvements (breakthroughs); MFEI seeks cost-effective routes to achieve those improvements.",
            "performance_metrics": "Evaluated via ε_x and ε_f curves over budget B in benchmarks; MFEI is among top performers across many multifidelity experiments.",
            "comparison_baseline": "Compared against MFPI, MFMES, and single-fidelity EI/PI/MES.",
            "performance_vs_baseline": "MFEI performed well, particularly when using the full fidelity spectrum, benefiting from combining informativeness and representativeness/diversity via cost-aware exploration.",
            "efficiency_gain": "Observed faster convergence to optima with less cumulative cost compared to single-fidelity EI; specific numerical gains depend on benchmark.",
            "tradeoff_analysis": "Paper discusses how α1 (correlation) and α2 (noise) can reduce benefit from cheaper fidelities and how α3 (cost) encourages use of cheap evaluations for exploration—thus providing a principled cost-vs-information tradeoff.",
            "optimal_allocation_findings": "MFEI is effective when lower fidelities are reasonably correlated and inexpensive: use them for exploration to cheaply reduce uncertainty, then switch to high-fidelity for refinement.",
            "uuid": "e2493.5",
            "source_info": {
                "paper_title": "Active Learning and Bayesian Optimization: a Unified Perspective to Learn with a Goal",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "MFPI",
            "name_full": "Multifidelity Probability of Improvement",
            "brief_description": "Multifidelity variant of PI that multiplies the high-fidelity PI by factors for fidelity correlation, cost ratio, and a sample-density penalty to discourage redundant sampling.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Multifidelity Probability of Improvement (MFPI)",
            "system_description": "Defines U_MFPI(x,l) = U_PI(x,L) * η1(x,l) * η2(l) * η3(x,l) where η1 is corr(f^{(l)}, f^{(L)}), η2 = λ(L)/λ(l) is the cost ratio favoring cheap fidelities, and η3 is a product over existing l-fidelity samples of (1 - R(x,x_i^{(l)})) to penalize high-density regions.",
            "application_domain": "MFBO for low-to-moderate dimensional engineering optimization, where aggressive exploitation is effective and multiple fidelities are available.",
            "resource_allocation_strategy": "Targets locations with high probability of improving the best high-fidelity value while (i) favoring informative lower fidelities when correlated and cheap, and (ii) penalizing redundant sampling to spread budget efficiently.",
            "computational_cost_metric": "Per-fidelity cost λ(l) and cumulative budget B; η2 uses the cost ratio explicitly.",
            "information_gain_metric": "Probability of improvement at high-fidelity, modulated by fidelity correlation and cost; η3 indirectly encodes marginal information benefit by penalizing dense regions.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Strong exploitation (PI) core; η3 introduces exploration by discouraging repeated sampling; η2 encourages cheaper fidelities for exploratory effort while focusing high-cost evaluations where most beneficial.",
            "diversity_mechanism": "Explicit via η3: sample-density based penalty that lowers acquisition for regions already well-sampled in a given fidelity, promoting spatial diversity.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Cumulative computational cost (λ(l)) with B_max as experiment-level cap.",
            "budget_constraint_handling": "Explicit cost-scaling (η2) biases selection toward cheaper fidelities to stretch budget; density penalty prevents wasteful repeated evaluations.",
            "breakthrough_discovery_metric": "Probability of achieving a new best high-fidelity value; MFPI aggressively concentrates budget where breakthrough likelihood is high but moderated by cost and sample redundancy.",
            "performance_metrics": "ε_x and ε_f over budget B; MFPI was top-performing on many benchmarks (notably Forrester) in experiments.",
            "comparison_baseline": "Compared to MFEI, MFMES, single-fidelity PI/EI/MES.",
            "performance_vs_baseline": "MFPI often achieved fastest convergence to analytic optima in low-dimensional test problems by effectively directing budget to promising regions and leveraging cheap informative fidelities.",
            "efficiency_gain": "Reported as significant computational budget reduction to reach optima relative to single-fidelity methods (no single-number universal claim).",
            "tradeoff_analysis": "Paper highlights MFPI's balance of aggressive exploitation with cost-awareness and explicit density-based diversity to avoid over-exploitation.",
            "optimal_allocation_findings": "MFPI is particularly effective when cheap fidelities are well correlated and the problem benefits from exploitation; density penalty is important to avoid wasteful repeated queries.",
            "uuid": "e2493.6",
            "source_info": {
                "paper_title": "Active Learning and Bayesian Optimization: a Unified Perspective to Learn with a Goal",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "MFMES",
            "name_full": "Multifidelity Max-value Entropy Search",
            "brief_description": "A multifidelity adaptation of MES that measures expected entropy reduction about the global minimum per unit cost and selects fidelity-level-aware queries accordingly (Monte Carlo approximations used).",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Multifidelity Max-Value Entropy Search (MFMES)",
            "system_description": "U_MFMES(x,l) = [H(p(f^{(l)} | D)) - E_{f^{(l)}(x)} H(p(f^{(l)} | f*^{(L)}, D))] / λ(l) (paper reports a formulation dividing info gain by cost), approximated by Monte Carlo sampling across fidelities so cheap fidelities can provide low-cost global information.",
            "application_domain": "MFBO for complex, high-dimensional, or highly multimodal problems where information-theoretic exploration across fidelities is valuable.",
            "resource_allocation_strategy": "Select (x,l) that maximizes expected reduction in entropy about the high-fidelity minimum per unit cost, thereby allocating expensive high-fidelity calls only when they provide proportionally large information.",
            "computational_cost_metric": "Per-fidelity cost λ(l); acquisition calculation itself is computationally expensive (Monte Carlo) and cost-normalized in acquisition.",
            "information_gain_metric": "Differential entropy reduction (mutual information) about the high-fidelity minimum/value measured under the multifidelity posterior.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Prioritizes exploration and global uncertainty reduction; by dividing by λ(l) it prefers low-cost fidelities for broad exploration and reserves high-fidelity queries for high-value refinement.",
            "diversity_mechanism": "Exploration-oriented information criterion disperses queries to where entropy reduction is highest; cost-normalization further encourages diverse low-cost sampling.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Cumulative cost (λ(l)) and limited total budget; acquisition computations are themselves nontrivial.",
            "budget_constraint_handling": "Normalize information gain by cost and use Monte Carlo approximations to evaluate expected entropy reductions efficiently across fidelities.",
            "breakthrough_discovery_metric": "Information gain about f* (large entropy reductions per unit cost indicate potential for breakthrough discovery).",
            "performance_metrics": "ε_x and ε_f across benchmark problems; MFMES performed well when many fidelities were available and in high-dimensional/multimodal problems.",
            "comparison_baseline": "Compared to MFPI, MFEI, and single-fidelity MES/EI/PI.",
            "performance_vs_baseline": "MFMES outperforms single-fidelity MES and competes favorably with other MFBO methods especially when full fidelity library exists and complex search landscape requires heavy exploration.",
            "efficiency_gain": "Demonstrated notable improvements in convergence under constrained budget for some challenging benchmarks, but at increased acquisition computation cost.",
            "tradeoff_analysis": "Paper discusses MFMES tendency to over-explore if not cost-normalized; cost-normalization and use of cheap fidelities are crucial to make it budget-efficient.",
            "optimal_allocation_findings": "Use MFMES for hard multimodal/noisy/high-dim problems where global information matters; ensure adequate low-cost fidelities and Monte Carlo approximations to control computational overhead.",
            "uuid": "e2493.7",
            "source_info": {
                "paper_title": "Active Learning and Bayesian Optimization: a Unified Perspective to Learn with a Goal",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "Pool-AL",
            "name_full": "Pool-based Active Learning",
            "brief_description": "An active learning paradigm where a learner selects examples from a large unlabelled pool to be labelled by an oracle, using a surrogate (classifier/regressor) and an utility function (informativeness/representativeness/diversity) to pick queries.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Pool-based Active Learning (single-oracle and multi-oracle extensions)",
            "system_description": "Maintains a surrogate model p_N(f|x) trained on currently labeled set D_N; a utility U(p_N(f|x)) scores unlabeled candidates and selects x_{N+1} = argmax U(·). Utility functions implement informativeness (uncertainty, entropy, query-by-committee), representativeness/diversity (clustering, k-center, optimal experimental design), or hybrids (serial, selection, parallel). Multi-oracle variants also select which annotator/fidelity to query, incorporating annotator cost and reliability.",
            "application_domain": "Supervised learning problems requiring expensive labels (e.g., experiments, human annotation); conceptual mapping used to relate BO acquisitions to active learning criteria.",
            "resource_allocation_strategy": "Select unlabeled instances to label that maximize expected utility: pure informativeness chooses uncertain points; representativeness chooses cluster centers; hybrids combine both to avoid wasteful labeling; multi-oracle versions also select which oracle to query to trade off cost and reliability.",
            "computational_cost_metric": "Labeling cost per oracle (monetary/time), number of labels acquired; some methods model oracle-specific costs.",
            "information_gain_metric": "Predictive uncertainty (entropy, margin, variance), model-change, expected reduction of loss, or mutual information depending on algorithm.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Not framed as exploration/exploitation in same sense as BO but trade-offs exist: informativeness ~ exploitation (query where most helpful for goal), representativeness/diversity ~ exploration (covering domain structure); hybrids explicitly balance them.",
            "diversity_mechanism": "Explicit methods (clustering, k-center, optimal design, density-weighted selection) implement diversity/representativeness; parallel-form hybrids weight informativeness with diversity scores.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Fixed labeling budget or cost-constrained labeling (monetary/time/resource); multi-oracle algorithms incorporate per-oracle cost constraints.",
            "budget_constraint_handling": "Selection strategies incorporate cost-awareness (cost-accuracy tradeoff), relabeling strategies, or selection of cheaper oracles when appropriate; some methods use cost-effective criteria to maximize utility per cost.",
            "breakthrough_discovery_metric": "Generally not explicit; in classification/regression context goal is model accuracy improvement (reduction in loss); conceptual mapping to BO regards breakthroughs as attaining target performance faster.",
            "performance_metrics": "Common metrics: classification/regression error, label efficiency (accuracy vs number of labels), cost-weighted performance. Paper maps active learning criteria to BO infill criteria but did not run active-learning experiments.",
            "comparison_baseline": "Random sampling, uncertainty-only sampling, cluster-only sampling; hybrid methods compared to pure criteria forms in literature.",
            "performance_vs_baseline": "Literature surveyed indicates hybrids that combine informativeness and representativeness/diversity often outperform pure strategies, particularly under limited budgets.",
            "efficiency_gain": "General observation: cost-aware and hybrid active learning reduces labels needed for a given performance compared to random or single-criterion sampling (no numerical values provided in this paper).",
            "tradeoff_analysis": "Paper frames active learning criteria as analogous to BO infill trade-offs and discusses multi-oracle cost-accuracy tradeoffs; recommends balanced criteria when problem structure is unknown.",
            "optimal_allocation_findings": "Recommendation to combine informativeness with representativeness/diversity (via serial, selection, or parallel forms) and to use cost-aware oracle selection when multiple annotation sources exist.",
            "uuid": "e2493.8",
            "source_info": {
                "paper_title": "Active Learning and Bayesian Optimization: a Unified Perspective to Learn with a Goal",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "Multi-oracle AL",
            "name_full": "Multi-oracle / Annotator-aware Active Learning",
            "brief_description": "Active learning extensions that select both which data points to query and which oracle/annotator or fidelity level to use, trading off label accuracy, annotator reliability, and cost.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Multi-oracle Active Learning (relabeling, repeating-labeling, probabilistic, transfer-knowledge, cost-effective)",
            "system_description": "Frameworks include relabeling (query multiple oracles and majority-vote), repeating-labeling (aggregate repeated noisy labels), probabilistic models estimating per-oracle reliability, transfer learning approaches to infer oracle accuracy from auxiliary domains, and cost-aware selection optimizing accuracy per cost. Decision selects (x,oracle) at each iteration to maximize expected net utility.",
            "application_domain": "Scenarios with multiple imperfect information sources: crowdsourcing, multi-fidelity experiments, simulations with different approximations.",
            "resource_allocation_strategy": "Decide which (x, oracle) to query by optimizing expected information gain weighted by oracle reliability and cost; some methods repeatedly query oracles to reduce noise while others estimate oracle accuracies to avoid redundant querying.",
            "computational_cost_metric": "Oracle-specific cost (monetary/time/computation) and number of oracle queries; aggregated into expected cost for acquisition decision.",
            "information_gain_metric": "Expected model improvement considering oracle reliability (probabilistic label models), reduction in expected loss or uncertainty, and sometimes information-theoretic measures.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Balancing accurate but costly oracles (exploitation of high-quality info) vs cheap noisy oracles (exploration and coverage); hybrid criteria of informativeness and representativeness/diversity extended to include oracle selection.",
            "diversity_mechanism": "Representativeness/diversity still used to spread queries; cost-effective algorithms favor less expensive oracles to permit diverse sampling.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Monetary/computational budget across oracle queries; constraints considered explicitly in cost-aware methods.",
            "budget_constraint_handling": "Algorithms optimize utility per cost or use constrained optimization to select queries under budget; probabilistic models estimate marginal value of oracle information to decide whether to spend budget on higher-quality sources.",
            "breakthrough_discovery_metric": "High-value discoveries are those where high-cost/high-reliability oracles produce substantially better model updates; measured via improvement in model accuracy or task-specific utility per cost.",
            "performance_metrics": "Label-efficiency, accuracy vs cost curves in relevant literature (not directly experimentally evaluated in this paper).",
            "comparison_baseline": "Relabeling/repeating strategies, random oracle selection, or single-best-oracle baselines.",
            "performance_vs_baseline": "Surveyed literature indicates cost-aware and probabilistic multi-oracle strategies outperform naive relabeling/repetition by better allocating queries to informative/or cost-effective oracles.",
            "efficiency_gain": "Reported qualitatively: better use of budget through selective oracle querying and probabilistic estimation of oracle accuracies; no single-number claims in this paper.",
            "tradeoff_analysis": "Paper discusses the extension of the three learning criteria (informativeness, representativeness, diversity) to include oracle accuracy/cost tradeoffs and how balancing these drives efficient multi-oracle allocations.",
            "optimal_allocation_findings": "Advice: prefer cheaper oracles for representativeness/diversity and reserve expensive, reliable oracles for high-informativeness queries; estimate oracle reliability probabilistically to guide selection.",
            "uuid": "e2493.9",
            "source_info": {
                "paper_title": "Active Learning and Bayesian Optimization: a Unified Perspective to Learn with a Goal",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "AIS",
            "name_full": "Adaptive Importance Sampling",
            "brief_description": "An adaptive probing method that iteratively updates proposal distributions to sample from complex target distributions efficiently using importance weights, improving sampling quality without building a global surrogate.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Adaptive Importance Sampling (AIS)",
            "system_description": "Initialize proposals q_n(x|θ_{n,1}), draw samples, compute importance weights w_n = π(x)/q(x), form self-normalized estimators, and adapt proposal parameters θ based on weighted samples; iterate to concentrate sampling where target π is large.",
            "application_domain": "Statistical inference, rare-event estimation, Bayesian computation, and adaptive probing scenarios where building a surrogate is not used.",
            "resource_allocation_strategy": "Allocate sampling density via adaptation of proposal distributions based on importance weights from prior draws; concentrates future computational budget in regions suggested by weighted evidence.",
            "computational_cost_metric": "Number of samples drawn and computational cost per sample; no budget-per-fidelity formalism in AIS as described in paper.",
            "information_gain_metric": "Implicit via importance weights reflecting representativeness to target π(x); objective is to reduce estimator variance rather than explicit mutual information.",
            "uses_information_gain": false,
            "exploration_exploitation_mechanism": "Adaptation of proposals progressively exploits high-weight regions while proposal diversity ensures exploration; no formal exploration/exploitation acquisition function.",
            "diversity_mechanism": "Multiple proposals (mixture) can maintain diversity; adaptation can include defensive mixtures to avoid premature collapse.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Number of Monte Carlo samples / computational sampling budget.",
            "budget_constraint_handling": "Adapt proposals to maximize estimator efficiency given sample budget; not framed as explicit cost-vs-information optimization with external cost units.",
            "breakthrough_discovery_metric": "Not applicable in optimization sense; goal is estimator accuracy/variance reduction.",
            "performance_metrics": "Estimator variance, effective sample size, accuracy of integral estimates (I(f)).",
            "comparison_baseline": "Standard importance sampling, defensive mixtures, non-adaptive proposals, MCMC.",
            "performance_vs_baseline": "Adaptive schemes improve sampling efficiency and estimator accuracy vs static proposals in many settings (literature cited).",
            "efficiency_gain": "Improved effective sample size and reduced variance per unit sampling cost versus non-adaptive samplers (no single-number claim in paper).",
            "tradeoff_analysis": "Paper contrasts AIS with surrogate-based adaptive learning: AIS allocates resources based solely on sample weighting without constructing a global surrogate and thus lacks goal-driven learning properties.",
            "optimal_allocation_findings": "AIS is effective for inference tasks to concentrate sampling where target density is high but is not considered a goal-driven learning method for optimization in this paper.",
            "uuid": "e2493.10",
            "source_info": {
                "paper_title": "Active Learning and Bayesian Optimization: a Unified Perspective to Learn with a Goal",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "ASC",
            "name_full": "Adaptive Stochastic Collocation",
            "brief_description": "An adaptive modeling approach that builds sparse-grid polynomial interpolants in stochastic input space by adaptively adding collocation nodes where an error indicator is high, improving surrogate approximation without the surrogate informing sampling for optimization.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Adaptive Stochastic Collocation (ASC)",
            "system_description": "Constructs sparse-grid tensor-product interpolants using difference formulas and adaptively refines the grid by adding nodes in subspaces where error indicators (or estimators) are largest; the surrogate is for uncertainty propagation, not used as a decision-making learner for optimization queries.",
            "application_domain": "Uncertainty quantification, propagation, and surrogate construction for PDE-based models and stochastic simulations.",
            "resource_allocation_strategy": "Allocate collocation points to subspaces with largest estimated interpolation error, concentrating computational resources where surrogate error is greatest.",
            "computational_cost_metric": "Number of collocation nodes and high-fidelity model solves required (growing exponentially with dimension if not sparse-adapted).",
            "information_gain_metric": "Error indicators/estimators (e.g., hierarchical surplus) used to guide node additions; not formulated as expected-information or mutual information.",
            "uses_information_gain": false,
            "exploration_exploitation_mechanism": "Refinement favors regions with high error (akin to exploitation of error hotspots) while sparse-grid design maintains some spread (representativeness); no explicit exploration/exploitation acquisition.",
            "diversity_mechanism": "Sparse-grid selection distributes nodes across coordinate directions, and adaptivity focuses on important directions to control growth; not an explicit diversity-promotion for hypothesis search.",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Fixed computational budget in number of collocation solves.",
            "budget_constraint_handling": "Adaptive refinement picks nodes that most reduce interpolation error per added node, thereby aiming to maximize surrogate quality under node budget.",
            "breakthrough_discovery_metric": "Not applicable; metric is surrogate error reduction rather than discovery of breakthroughs.",
            "performance_metrics": "Interpolation error, surrogate accuracy, error indicators per number of collocation points.",
            "comparison_baseline": "Full tensor grids, non-adaptive sparse grids, other surrogate constructions.",
            "performance_vs_baseline": "ASC reduces required nodes vs full tensor grids and non-adaptive approaches for many problems by targeting important directions.",
            "efficiency_gain": "Reduces computational burden compared to full tensor-product sampling, especially in moderate dimensions; efficiency depends on problem smoothness and anisotropy.",
            "tradeoff_analysis": "Paper contrasts adaptive modeling (ASC) with adaptive learning: ASC builds surrogates but does not use them to inform goal-driven sampling decisions, so it doesn't directly balance cost vs information for optimization objectives.",
            "optimal_allocation_findings": "ASC is recommended when the goal is surrogate accuracy for uncertainty quantification rather than goal-driven optimizer sampling; resource allocation focuses on error reduction per collocation node.",
            "uuid": "e2493.11",
            "source_info": {
                "paper_title": "Active Learning and Bayesian Optimization: a Unified Perspective to Learn with a Goal",
                "publication_date_yy_mm": "2023-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Efficient global optimization of expensive black-box functions",
            "rating": 2,
            "sanitized_title": "efficient_global_optimization_of_expensive_blackbox_functions"
        },
        {
            "paper_title": "Max-value entropy search for efficient bayesian optimization",
            "rating": 2,
            "sanitized_title": "maxvalue_entropy_search_for_efficient_bayesian_optimization"
        },
        {
            "paper_title": "Multi-fidelity Bayesian optimization with continuous approximations",
            "rating": 2,
            "sanitized_title": "multifidelity_bayesian_optimization_with_continuous_approximations"
        },
        {
            "paper_title": "Multi-fidelity Bayesian optimization with max-value entropy search and its parallelization",
            "rating": 2,
            "sanitized_title": "multifidelity_bayesian_optimization_with_maxvalue_entropy_search_and_its_parallelization"
        },
        {
            "paper_title": "A survey of multifidelity methods in uncertainty propagation, inference, and optimization",
            "rating": 2,
            "sanitized_title": "a_survey_of_multifidelity_methods_in_uncertainty_propagation_inference_and_optimization"
        },
        {
            "paper_title": "Adaptive importance sampling: The past, the present, and the future",
            "rating": 1,
            "sanitized_title": "adaptive_importance_sampling_the_past_the_present_and_the_future"
        },
        {
            "paper_title": "Pool-based active learning in approximate linear regression",
            "rating": 1,
            "sanitized_title": "poolbased_active_learning_in_approximate_linear_regression"
        },
        {
            "paper_title": "RRA: Resource aware active learning for multifidelity efficient optimization",
            "rating": 2,
            "sanitized_title": "rra_resource_aware_active_learning_for_multifidelity_efficient_optimization"
        },
        {
            "paper_title": "Non-myopic multifidelity bayesian optimization",
            "rating": 2,
            "sanitized_title": "nonmyopic_multifidelity_bayesian_optimization"
        }
    ],
    "cost": 0.0303105,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>ACTIVE LEARNING AND BAYESIAN OPTIMIZATION: A UNIFIED PERSPECTIVE TO LEARN WITH A GOAL
March 2, 2023</p>
<p>Francesco Di Fiore francesco.difiore@polito.it 
Politecnico di Torino</p>
<p>Michela Nardelli michela.nardelli@polito.it 
Politecnico di Torino</p>
<p>Laura Mainini l.mainini@imperial.ac.uk 
Imperial College London Politecnico di Torino Massachusetts Institute of Technology</p>
<p>ACTIVE LEARNING AND BAYESIAN OPTIMIZATION: A UNIFIED PERSPECTIVE TO LEARN WITH A GOAL
March 2, 202313053D09195A061E39AA5DCA7D4C2FCC10.1007/s11831-024-10064-zarXiv:2303.01560v4[cs.LG]
Science and Engineering applications are typically associated with expensive optimization problems to identify optimal design solutions and states of the system of interest.Bayesian optimization and active learning compute surrogate models through efficient adaptive sampling schemes to assist and accelerate this search task toward a given optimization goal.Both those methodologies are driven by specific infill/learning criteria which quantify the utility with respect to the set goal of evaluating the objective function for unknown combinations of optimization variables.While the two fields have seen an exponential growth in popularity in the past decades, their dualism and synergy have received relatively little attention to date.This paper discusses and formalizes the synergy between Bayesian optimization and active learning as symbiotic adaptive sampling methodologies driven by common principles.In particular, we demonstrate this unified perspective through the formalization of the analogy between the Bayesian infill criteria and active learning criteria as driving principles of both the goal-driven procedures.To support our original perspective, we propose a general classification of adaptive sampling techniques to highlight similarities and differences between the vast families of adaptive sampling, active learning, and Bayesian optimization.Accordingly, the synergy is demonstrated mapping the Bayesian infill criteria with the active learning criteria, and is formalized for searches informed by both a single information source and multiple levels of fidelity.In addition, we provide guidelines to apply those learning criteria investigating the performance of different Bayesian schemes for a variety of benchmark problems to highlight benefits and limitations over mathematical properties that characterize real-world applications.</p>
<p>Introduction</p>
<p>In science and engineering, the development of advanced technologies involves the formalization and solution of optimization problems to identify both optimal designs capable of satisfying competing requirements of performance [1], and states of the system to monitor their health status during the operational life [2].Depending on the specific application, the identification of optimal solutions requires the minimization of an objective function that measures the goodness of design configurations with respect to the requirements, or the accuracy of the estimated health status of the system with respect to the measurements.Typically, the scale of complexity of engineering systems requires several evaluations of this objective function through accurate computer simulations -e.g.Computational Fluid Dynamics (CFD) or Computational Structural Dynamics (CSD) -or physical experiments -e.g.lab-scale test benches or realworld testing -before assessing an optimal solution.The use of highly complicate representations of those systems leads to a significant bottleneck: the demand for resources to evaluate the objective function for all the combinations of optimization variables is difficult to be adequately satisfied.Indeed, the acquisition of data from these high-fidelity models involves huge non-trivial computational and economical costs that could arise from the computation of the objective function and its derivatives over ideally the entire optimization domain.</p>
<p>Surrogate models are computed on evaluations of the objective function acquired through computer codes and/or physical experiments of the system: these sources of information are mostly treated as purely input/output black-box relationships whose analytical form is unknown and not directly accessible to the optimizer.Thus, the accuracy and efficiency of the resulting surrogate are highly dependent on the sampling approach adopted to select informative combinations of optimization variables for the acquisition of data.Among the numerous sampling schemes available in literature, it is possible to identify two major families: one-shot, and sequential schemes.The one-shot strategy defines a grid of samples over the domain all at once.Examples include Latin Hypercube [3], factorial and fractional factorials designs [4,5], Placket-Burmann [6], and D-optimal [7].However, it is very hard to identify a priori the best design of those experiments to efficiently compute the most informative surrogate.To overcome these limitations, sequential sampling selects samples over the domain through an iterative process [8,9].Among these, adaptive sampling [10] provides resource-efficient techniques that seek to reduce as much as possible the evaluations of the objective function, and targets the improvement of the fitting quality across the domain and/or the acceleration of the optimization search [11,12,13].Popular adaptive samplings to address black-box optimization problems characterized by the expensive evaluation of the objective function are those realized through the Bayesian Optimization (BO) methodology [14,15].BO aims at efficiently elicit valuable data from models of the system to contain the computational expense of the optimization procedure.The Bayesian routine iteratively computes a surrogate model of the objective function, and defines a goal-driven sampling process through an acquisition function computed on the surrogate information.This acquisition function measures the merit of samples according to certain infill criteria, and permits to select the next sample that maximizes the query utility with respect to the given optimization goal.</p>
<p>The popular paradigms for Bayesian optimization show substantial synergy with active learning schemes which has not been explicitly discussed and formally described in literature to date.This paper proposes the explicit formalization of this synergy through an original perspective of Bayesian optimization and active learning as symbiotic expressions of adaptive sampling schemes.The aim of this unifying viewpoint is to support the use of those methodologies, and point out and discuss the analogies via their mathematical formalization.This unified interpretation is based on the formulation and demonstration of the analogy between the Bayesian infill criteria and the active learning criteria as the elements responsible for the decision on how to learn from samples to reach the given goal.In support of this unified perspective, this paper first clarifies the concept of goal-driven learning, and proposes a general classification of adaptive sampling methods that recognizes Bayesian optimization and active learning as methodologies characterized by goal-oriented search schemes.Thus, we elucidate the synergy between Bayesian optimization and active learning mapping the Bayesian learning features on the active learning properties.The mapping is discussed through the analysis of three popular Bayesian frameworks for both the case of a single information source, and when a spectrum of multiple sources are available to the search.In addition, we observe the capabilities introduced by the different learning criteria over a comprehensive set of benchmark problems specifically defined to stress test and validate goal-driven approaches [16].The objective is to discuss opportunities and limitations of different learning principles over a variety of challenging mathematical properties of optimization problems frequently encountered in complex scientific and engineering applications.This manuscript is organized as follows.Section 2 discusses goal-driven learning procedures and defines the concept of goal-driven learner according to surrogate modeling and optimization.In Section 3, we recognize that Bayesian optimization, active learning and adaptive sampling are not fully superimposable concepts, and propose a general classification to position Bayesian optimization and active learning with respect to the adaptive sampling methodologies.Then, Section 4 provides an overview on Bayesian optimization and multifidelity Bayesian optimization.Section 5 presents our perspective on the symbiotic relationship between Bayesian optimization and active learning.Then, in Section 6 popular Bayesian optimization and multifidelity Bayesian optimization algorithms are numerically investigated over a variety of benchmark problems.Finally, Section 7 provides concluding remarks.</p>
<p>Goal-Driven Learning</p>
<p>Goal-driven learning is a decision-making process in which each decision is made to acquire specific information about the system of interest that contribute the most to achieve a given goal [17,18,19,20,21,22].This learning goal can be the increase of the knowledge of the system behaviour over all the domain of application, or the acquisition of specific knowledge to enhance and accelerate the identification of optimization solutions.Accordingly, a goal-driven learner selects what to learn considering both the current knowledge and information needed, and determines how to learn quantifying the relative utility of alternative options in the current circumstances.This paper focuses on Bayesian optimization and active learning as goal-driven procedures where a surrogate model is built to accurately represent the behaviour of a system or effectively inform an optimization procedure to minimize given objectives.This goal-driven process is guided by learning principles that determine the "best" location of the domain to acquire information about the system, and refine the surrogate model toward the goal -improve the accuracy of the surrogate or minimize an objective function over the domain.Formally, these surrogate based modeling and optimization problems can be formulated as a minimization problem of the following form:
x * = arg min x∈χ f (R(x))(1)
where f (R(x)) denotes the objective function evaluated at the location x ∈ χ of the domain χ.The objective function is of the general form f = f (R(x)), where R(x) represents the response of the system of interest evaluated through a model -e.g.computer-based numerical simulations or real-world experiments.In surrogate based modeling, the objective function can be represented as the error between the approximation of the surrogate model and the response of the system: the goal is to minimize such error to improve the accuracy of the surrogate over all the domain.In surrogate based optimization, the objective function represents a performance indicator dependent on the system response: the goal is to minimize this indicator to improve the capabilities of the system according to given performance requirements.</p>
<p>Goal-driven techniques address Equation ( 1) through a decision-making iterative process where learning principles tailor the acquisition of specific knowledge about the objective function -evaluation of f at certain domain location xcurrently needed to update the surrogate and inform the learner toward the given goal.</p>
<p>In this context, the goal-driven learner is the agent that makes decisions based on the current knowledge of the system of interest, and acquires new information to accomplish a given goal while augmenting the awareness about the system itself.In practice, the learner queries the sample that maximizes the utility to achieve the desired goal: specific learning principles quantify this utility based on the surrogate estimate and in response to information needs.At the same time, the surrogate model is dynamically updated once new information are acquired, and informs the learner to focus and tailor on the fly the elicitation of samples to further overarching the goal.Thus, the distinguishing element of a goal-driven learning procedure is represented by the mutual exchange of information between the learner and the surrogate model: the learner assimilates the information from the surrogate to make a decision aimed at achieving the goal, and the approximation/prediction of the surrogate is enriched by the result of this decision.</p>
<p>Adaptive Sampling Classification</p>
<p>Bayesian optimization and active learning realize adaptive sampling schemes to efficiently accomplish a given goal while adapting to the previously collected information.In recent years, there has been a profusion of literature devoted to the general topic of adaptive sampling but arguably a blurring of focus: many contributions from different field provided a deal of interesting advancements, but also led to some degree of confusion around the concepts of adaptive sampling, active learning and Bayesian optimization.Figure 1 illustrates the use of the words "adaptive sampling", "active learning", and "Bayesian optimization" from 1990 to 2022.In addition, we report the combined use of all the three words over the same period of time.It can be appreciated both the general increasing trend of use of the three techniques and the associated increase of the use of the three terms combined.Many times the three concepts have been used as complete synonyms, with some growing abuse motivated by the difficulties to map the (shaded) boundaries.</p>
<p>Stemming from these considerations, this paper recognizes that adaptive sampling is not always superimposable with active learning and Bayesian optimization.Figure 2 illustrates the relationships between those three methodologies.</p>
<p>We propose a classification of adaptive sampling techniques in three main families, namely adaptive probing (Section 3.1), adaptive modeling (Section 3.2) and adaptive learning (Section 3.3).This classification is based on the concept of goal-driven learning as the distinctive element of adaptive learning methodologies: the learner assimilates the information from the surrogate model to make a decision aimed at achieving a goal, and the surrogate is enriched by the result of this decision following a mutual exchange of information.Conversely, adaptive probing and adaptive modeling classes do not realize a goal-driven learning: the former does not rely on a surrogate model to assist the sampling procedure while the latter computes a surrogate model that is not used to inform the search task.This classification permits to clarify the reciprocal positions between adaptive sampling, active learning and Bayesian optimization.</p>
<p>Accordingly, adaptive sampling and active learning do not completely overlap.Active learning strategies are categorized into population-based and pool-based algorithms according to the nature of the search procedure [23,24].In populationbased active learning, the distribution of the objective function is available: the learner seeks to determine the optimal training input density to generate training points without relying on a surrogate model of the objective function.Conversely, pool-based active learning computes a surrogate model of the unknown objective function that is used to inform the learner toward a given goal, and is updated during the procedure to refine the informative content supporting  the learning procedure.Thus, pool-based active learning methods realize goal-driven learning schemes and can be collocated in the adaptive learning class while population-based active learning techniques can not be considered as adaptive samplings.Following the proposed classification, Bayesian optimization represents the logic intersection between active learning and adaptive sampling since (i) BO realizes an adaptive sampling scheme toward a given goal, and (ii) the BO goal-driven learning procedure is guided by learning principles also traceable in active learning schemes.This synergy between Bayesian optimization and active learning is the main focus of our work, and the remaining of this manuscript is dedicated to formalize and discuss this dualism.To support this discussion, we provide additional details of the proposed classification for adaptive sampling, and review some popular approaches for each of the three classes.The literature on adaptive sampling is vast, and a complete review goes beyond the purpose of this work.Although our discussion will not be comprehensive, the objective is to highlight the distinguishing features of each class and clarify the relative positions of adaptive sampling, active learning and Bayesian optimization.</p>
<p>Adaptive Probing</p>
<p>Adaptive probing schemes exploit the observations of previous samples without computing any surrogate model.These sampling procedures are informed exclusively from the collected data to guide the selection of the next location to query, and exclude the adoption of emulators to support the search.Several adaptive probing frameworks have been developed based on the Monte Carlo method [25,26].Among these, adaptive importance samplings [27,28,29] and adaptive Markov Chain Monte Carlo samplings [30,31] represent popular methodologies adopted in different practical scenarios, from signal processing [32,33] to reliability analysis of complex systems [34,35].Adaptive importance sampling uses previously observed samples to adapt the proposal densities and locate the regions from which samples should be drawn;</p>
<p>this strategy permits to iteratively improve the quality of the samples distribution and enhance the accuracy of the relative inference from these observations.Adaptive Markov Chain Monte Carlo (MCMC) determines the parameters of the MCMC transition probabilities on the fly through already collected information.This adaptively generates new samples from an usually complex and high-dimensional distribution, and enhances the overall computational efficiency and reliability of the procedure.In the next paragraph, we report the mathematical formulation of adaptive importance sampling to illustrate the properties of adaptive probing methodologies and the elements that differentiate them from active learning paradigms.</p>
<p>Adaptive Importance Sampling</p>
<p>Adaptive Importance Sampling (AIS) usually considers a generic inference problem characterized by a certain probability density function (pdf) π(x) of a d x -dimensional vector of unknown statistic real parameters x ∈ χ.AIS frameworks aim to provide a numerical approximation of some particular moment of x:
I( f ) = E π [ f (x)] = f (x) π(x) dx(2)
where f : χ → R can be any function of x integrable with respect to the pdf π(x)</p>
<p>The integral I( f ) is representative of different mathematical problems, from Bayesian inference [36] to the estimate of rare events [37].In many practical scenarios, the integral I( f ) can not be computed in closed form.Adaptive importance sampling provides an algorithmic framework to efficiently address this problem.</p>
<p>Let us define a proposal probability density function q(x) to simulate samples under the restriction that q(x) &gt; 0 for all x where π(x) f (x) ̸ = 0. AIS provides an iterative procedure that improves the quality of one or multiple proposals q(x) to approximate a non-normalized non-negative target function π(x).At the beginning, AIS initializes N proposals {q n (x|θ n,1 )} N n=1 parameterized through the vector θ n,1 .Then, the procedure simulates K samples from each proposal x (k) n,1 , n = 1, ..., N, k = 1, ..., K, and assigns to each sample an associated importance weight formalized as follows:
w n = π(x n ) q(x n ) , n = 1, ..., N(3)
These importance weights measure the representativeness of each sample simulated from the proposal pdf q(x) with reference to the distribution of random variables π(x).</p>
<p>At this point, this set of N weighted samples (x
(k) n,1 , w(k)
n,1 ), n = 1, ..., N, k = 1, ..., K is used to define a self-normalized estimator:
ÎN ( f ) = N ∑ n=1 wn f (w n )(4)
where wn = w n /∑ N j=1 w j are the normalized weights.This permits to approximate the target function distribution as follows:
πN (x) = N ∑ n=1 wn δ (x − x N )(5)
where δ represents the Dirac measure.</p>
<p>Finally, AIS realizes the adaptation phase and updates the parameters of the n-th proposals from θ n,1 to θ n,2 using the last set of drawn parameters [38] or all the parameters evaluated so far [39].The whole procedure is repeated until a certain termination criteria is met (e.g.maximum number of iterations).</p>
<p>This adaptive policy permits to gradually evolve the single or multiple proposal densities to accurately approximate the target pdf.The generation of new samples is uniquely driven by the measurement of the importance of previous samples (weighting) that supports the updating of the proposal parameters (adaptation).Thus, AIS adaptively locates promising regions to query without benefiting from an overall quantification of the goodness of all the spectrum of samples available in the domain -e.g.through the construction of a surrogate model.On these basis, AIS and the general class of adaptive probing strategies is not considerable as a learning procedures since the adaptation phase is not informed by a surrogate model updated on the fly during the procedure, and is not guided by a "learner" that assimilates information from this emulator and adapts the next queries to achieve a given goal.</p>
<p>Adaptive Modeling</p>
<p>Adaptive modeling paradigms sample the domain supported by the information from previous queries, and use the collected data to build a surrogate model.However, the informative content encoded in the emulator is not used to guide the sampling and decide the next point to evaluate.Adaptive modeling approaches have been extensively developed for the reliable propagation and quantification of uncertainties [40,41], analysis of ordinary or partial differential equations [42,43], and inverse problems [44,45].One common approach is represented by adaptive stochastic collocation methodologies, which use an adaptive sparse grid approximation scheme to construct an interpolant polynomial in a multi-dimensional random space [46,47].The adaptive selection of collocation points is driven by an error indicator [48] or estimator [49] that evaluates a certain number of sparse admissible subspaces of the domain: the subspace that exhibits the higher error is included in the grid and the new set of subspaces is identified.Other well-known adaptive modeling approaches are residual-based samplings distribution [50].This family of techniques is mostly applied to improve the training efficiency of Physics-Informed Neural Networks (PINN) surrogate models.Residual-based approaches enhance the distribution of residual points by placing more samples according to certain properties of the residuals during the training of PINN.This decision can be made on the basis of locations where the residual of the partial differential equation is large [51], according to a probability density function of the residual points [52], and hybrid approaches of the above [50].This permits to achieve a better accuracy of the final PINN surrogate model while containing the computational burden associated with computations.Both stochastic collocation and residual based samplings are intended to build an efficient and accurate surrogate model over the domain of samples.However, the sampling procedure is adapted uniquely to previous evaluated samples without a learning procedure from data: the surrogate model is not used to inform the decision on where to sample, and is not progressively updated with previous information.In the following, we provide general mathematical details about adaptive stochastic collocation to analyze the peculiarities of the adaptive modeling class, and underline the absence of a learning process during the construction of the surrogate model.</p>
<p>Adaptive Stochastic Collocation</p>
<p>Adaptive Stochastic Collocation (ASC) builds an interpolation function to approximate the outputs from a model of interest.This emulator is constructed on the evaluations of the model at valuable collocation points of the stochastic inputs to obtain the moments and the probability density function of the outputs.</p>
<p>Consider any point x contained in the random space Γ ⊂ R N with probability distribution function ρ(x).The goal of ASC is to find an interpolating polynomial I ( f ) to approximate a smooth function f (x) : R N → R:
I ( f )(x k ) = f (x k ) , 1 ≤ k ≤ P(6)
for a given set of points {x k } P k=1 .The selection of the collocation points majorly influences the capability of the interpolating polynomial to be close to the original function f .For multivariate problems, the interpolation function is defined as follows using the tensor product grid:
I ( f ) = (U i 1 ⊗ • • • ⊗ U i N )( f ) = n i 1 ∑ j 1 =1 • • • n i N ∑ j N =1 f (x i 1 j 1 , ..., x i N j N ) • (L i 1 j 1 ⊗ • • • ⊗ L i N j N )(7)
where U i k is the univariate interpolation function for the level i k in the k-th coordinate, x i k j m is the j m -th node, and L j k are the Lagrange interpolating polynomials.Equation 7 demands for n i 1 × • • • × n i N nodes, which indicate an exponential rate of computational cost growth with the number of dimensions.Adaptive stochastic collocation targets the reduction of this computational effort through an adaptive sparse grid of collocation points: the objective is to wisely place more points of the grid in the important directions to prioritize the collection of highly informative data.This adaptive sparse grid is defined through a subset of the full tensor product grid as follows:
A q,N ( f ) = ∑ |i|≤q (∆U i 1 ⊗ • • • ⊗ ∆U i N )( f ) = A q−1,N ( f ) + ∑ |i|=q (∆U i 1 ⊗ • • • ⊗ ∆U i N )( f )(8)
where i = (i 1 , ..., i N ) ∈ R N , |i|= i 1 + ... + i N , q is the sparseness parameter, and the difference formulas are defined by
U 0 = 0 and ∆U i = U i − U i−1 .
Equation 8 leverages the previous results to extend the interpolation from level q − 1 to q through the evaluation of the multivariate function on the sparse grid:
H q,N = |i|≤q (∆ϑ i 1 × • • • × ∆ϑ i N ) = H q−1,N + |i|=q (∆ϑ i 1 × • • • × ∆ϑ i N )(9)
where ∆ϑ i = ϑ i \ϑ i−1 are the newly added set of univariate nodes ϑ i k for level i k in the k-th coordinate.</p>
<p>This scheme adapts the sampling procedure through the knowledge acquired on the fly, and efficiently leverages data to improve the quality of the interpolation function.In this case, the selection of the collocation points is intended to compute an emulator of the target function, but the adaptive sampling is not driven by the information acquired from this emulator.In addition, the acquisition of data is not used to learn and update the surrogate model.These considerations on ASC can be extended to the general class of adaptive modeling methods: even if the sampling scheme is conceived to construct surrogate models, the selection of promising locations to query is not delegated to a goal-driven learner that leverages a mutual exchange of information with the surrogate.</p>
<p>Adaptive Learning</p>
<p>Adaptive learning methodologies realize goal-driven learning processes characterized by the mutual exchange of information between the surrogate model and the goal-driven learner: the former is updated and refined after new evaluations of samples while the latter decides the next query based on the updated approximation given by the emulator.Bayesian optimization and pool-based active learning belong to this specific class of adaptive sampling techniques.Bayesian frameworks constitute a learning process driven by the mutual informative assimilation between an acquisition function -learner -and a surrogate model [53,15].The acquisition function commensurates the benefit of evaluating samples based on the prediction of the surrogate model, and selects the most useful sample to query toward the given goal -either to improve the accuracy of the surrogate over the domain or to effectively inform the optimization search; at the same time, the emulator is enriched with the data from the new query, and is updated to refine the approximation of the objective function over the domain.Similarly, pool-based active learning methods search the domain through a goal-driven learner informed by a classification model of samples [54,55].This process is characterized by the reciprocal flow of information between the learner and the emulator: the classification model is updated through the new evaluations of unsampled locations, and the learner uses these information to select the next query.Mathematical details about pool-based active learning are provided in the following section to better clarify the distinction between this class of adaptive learners, and the other classes which do not realize a goal-driven learning procedure.</p>
<p>Pool-Based Active Learning</p>
<p>Pool-based active learning commonly defines an optimal sampling strategy to improve the accuracy of a surrogate model adopted to classify data-points from a target distribution of labels over the domain of samples χ.Considering this general classification task, pool-based active learning routine is grounded on a probabilistic estimate of the distribution of features f over the entire domain χ through a surrogate model f .This emulator is trained on a set of collected data-points, and maps features to labels f N (x n ) = fn through a predicted probability p N ( f n = f |x n ) that estimates the distribution of features over the domain.Suppose we have collected from a large pool of unlabelled data χ the -smalldataset
D N {x n , f (x n )} N n=1
observing the label values f (x n ) in output from an observation model or oracle at some informative locations x n .Based on this dataset, the goal-driven procedure learns a surrogate model fN whose predictive framework emulates the behaviour of samples over the domain based on the previous collected information.</p>
<p>At this point, an utility function acts as the goal-driven learner informed by the surrogate model, and identifies the most promising sample to be labelled by the oracle according to a measure of utility with respect to the given goal -improve the accuracy of the classifier.The next query augments the dataset D N+1 = D N {x N+1 , f N+1 } and the surrogate model is updated.This utility function defines a learning policy that maps the current predictive distribution to a decision/action on where to sample in the next iteration as follows:
x N+1 = arg maxU(p N ( f n = f |x n ))(10)
Equation ( 10) mathematically formalizes the concept of goal-driven learning procedure: the learner leverages the predicted probability of the surrogate p N ( f n = f |x n ) to make an action x N+1 ; at the same time, the decision is used to enrich the dataset D{x n , f (x n )} N+1 n=1 and update the predicted probability p N+1 .This mutual exchange and assimilation between the learner and the surrogate represents the key aspect that defines a goal-driven learning process and the whole class of adaptive learning sampling schemes.</p>
<p>Bayesian Frameworks</p>
<p>Bayesian optimization constitutes the mid-point between adaptive sampling and active learning.This intersection represents the focal point of our work, and motivates the substantial synergy between Bayesian optimization and active learning as adaptive sampling schemes capable of learning from data and accomplish a certain learning goal.The remaining of this section is dedicated to the general overview of Bayesian optimization considering both a single source of information (Section 4.1) and when multiple sources are available to the learning procedure (Section 4.2).This will guide the reader into the next sections that make explicit the symbiosis between Bayesian frameworks and active learning through our original perspective of Bayesian optimization as a way to actively learn with acquisition functions (Section 5).</p>
<p>Bayesian Optimization</p>
<p>The birth of Bayesian optimization can be retraced in 1964 with the work of Kushner [56] where unconstrained one-dimensional optimization problems are addressed through a predictive framework based on the Wiener process surrogate model, and a sampling scheme guided by the probability of improvement acquisition function.Further contributions have been proposed by Zhilinskas [57] and Mockus [58], and the methodology has been extended to high dimensional optimization problems in the works of Stuckman [59] and Elder [60].Bayesian optimization achieved resounding success after the introduction of the Efficient Global Optimization (EGO) algorithm by Jones et al. [61].EGO uses a Kriging surrogate model to predict the distribution of the objective function, and adopts the expected improvement acquisition function to measure the improvement of the optimization procedure obtained evaluating unknown samples.</p>
<p>The EGO methodology paves the way to the application of Bayesian optimization over a wide range of problems in science and engineering.These research fields demand for the efficient management of the information from black-box representations of the objective function -the procedure is only aware of the input and output without a priori knowledge about the function -to guide the optimization search.Engineering has been a pioneer in the adoption of Bayesian optimization: the design optimization of complex systems is frequently characterized by computationally intensive black-box functions which require efficient global optimization methods.Early applications relate to engineering design optimization [62], computer vision [63] and combinatorial problems [64].Nowadays, the Bayesian framework becomes widely adopted in many fields including and not limited to engineering [65,66,67,68], robotics and reinforcement learning [69,70,71], finance and economics [72,73], automatic machine learning [74,75], and preference learning [76,77].In addition, significant advances have been made in the expansion of BO methodologies to higher-dimensional search spaces frequently encountered in science and engineering, where the effectiveness of the search procedure is usually correlated to an exponential growth of the required observations of the objective function and associated demand for computational resources and time.Within this context, BO techniques have been scaled to approach high-dimensional problems exploiting potential additive structures of the objective function [78,79], mapping highdimensional search spaces into low-dimensional subspaces [80,81], learning from observations of multiple input points evaluated through parallel computing [82,83], and through simultaneous local optimization approaches [84].</p>
<p>Given a black-box expensive objective function f : χ → R, Bayesian optimization seeks to identify the input x * ∈ min x∈χ f (x) that minimizes the objective f over an admissible set of queries χ with a reduced computational cost.To achieve this goal, Bayesian optimization relies on an adaptive learning scheme based on a surrogate model that provides a probabilistic representation of the objective f , and uses this information to compute an acquisition function U(x) : χ → R + that drives the selection of the most promising sample to query.Let us consider the available information regarding the objective function f stored in the dataset D N = {(x 1 , y 1 ), ..., (x n , y n )} where y n ∼ N ( f (x n ), σ ε (x n )) are the noisy observations of the objective function and σ ε is the standard deviation of the normally distributed noise.</p>
<p>At each iteration of the optimization procedure, the surrogate model depicts possible explanations of f as f ∼ p( f |D N ) applying a joint distribution over its behaviour at each sample x ∈ χ.Typically, Gaussian Processes (GPs) have been widely used as the surrogate model for Bayesian optimization [85,86].In GP regression, the prior distribution of the objective p( f ) is combined with the likelihood function p
(D N | f ) to compute the posterior distribution p( f |D N ) ∝ p(D N | f )p( f ), representing the updated belief about f . The GP posterior is a joint Gaussian distribution p( f |D N ) = N (µ(x), κ(x, x ′ )) completely specified by its mean µ(x) = E [ f (x)] and covariance (also referred as kernel) function κ(x, x ′ ) = E [( f (x) − µ(x))( f (x ′ ) − µ(x ′ ))],
where µ(x) represents the prediction of the GP model at x and κ(x, x ′ ) the associated uncertainty.BO uses this statistical belief to make the decision on where to sample assisted by an acquisition function U, which identifies the most informative sample x new ∈ χ that should be evaluated via maximization x new ∈ max x∈χ U(x).Then, the objective function is evaluated at x new and this information is used to update the dataset
D N = D N ∪ (x new , y(x new )).
Acquisition functions are designed to guide the search for the optimum solution according to different infill criteria which provide a measure of the improvement that the next query is likely to provide with respect to the current posterior distribution of the objective function.In engineering applications, we could retrieve different implementations proposed for the acquisition function, which differ for the infill schemes adopted to sample pursuing the optimization goal.Examples include the Probability of Improvement (PI) [87], Expected Improvement (EI) [61], Entropy Search (ES) [88] and Max-Value Entropy Search (MES) [89], Knowledge-Gradient (KG) [90], and non-myopic acquisition functions [91,92].</p>
<p>The Probability of Improvement (PI) acquisition function encourages the selection of samples that are likely to obtain larger improvements over the current minimum predicted by the surrogate model, while the Expected Improvement (EI) considers not only the PI but also the expected gain in the solution of the optimization problem achieved evaluating a certain sample.Other popular schemes are entropy-based acquisition functions such as the Entropy Search (ES) and Max-Value Entropy Search (MES), which rely on estimating the entropy of the location of the optimum and the minimum function value, respectively, to maximize the mutual information between the samples and the location of the global optimum.Knowledge-gradient sampling procedures are conceived for applications where the evaluations of the objective function are affected by noise, recommending the location that maximizes the increment of the expected value that would be acquired by taking a sample from the location.Through the adoption of non-myopic acquisition functions, the learner maximizes the predicted improvement over future iterations of the optimization procedure, overcoming myopic schemes where the improvement of the solution is measured at the immediate step ahead.</p>
<p>Multifidelity Bayesian Optimization</p>
<p>The evaluation of black-box functions in engineering and science frequently requires time-consuming lab experiments or expensive computer-based models, which would dramatically increase the computational burden for the optimization procedure.This is the case of large-scale design optimization problems, where the evaluation of the objective function for enough samples can not be afforded in practice.In many real-world applications, the objective function can be computed using multiple representations at different levels of fidelity { f (1) , ..., f (L) }, where the lower the level of fidelity the less accurate but also less time-consuming the evaluation procedure.Multifidelity methods recognize that different representative levels of fidelity and associated costs can be used to accelerate the optimization process, and enable a flexible trade-off between computational cost and accuracy of the solution.In particular, multifidelity optimization leverages low-fidelity data to massively query the domain, and uses a reduced number of high-fidelity observations to refine the belief about the objective function toward the optimum [93,94,95].</p>
<p>Accordingly, Multifidelity Bayesian Optimization (MFBO) learns a surrogate model that synthesizes through stochastic approximation the multiple levels of fidelity available, and uses an acquisition function as the learner that selects the most promising sample and associated level of fidelity to interrogate.This learning procedure provides potential accelerations of the optimization procedure that is reflected in the likely improvement of the surrogate accuracy.According to Godino et al. [96], the improvement in performance occurs usually if the acquisition of large amount of high-fidelity data is hampered by the computational expense, the correlation between high-fidelity and low-fidelity data is high, and low-fidelity models are sufficiently inexpensive; Under different circumstances, multifidelity optimization might not deliver substantial accelerations and quality of the surrogate: the relationship between dimension of the training set and surrogate accuracy is not monotonically increasing, as evidenced by [97].In recent years, multifidelity Bayesian optimization has been successfully adopted for optimization problems ranging from engineering design optimization [98,99,22,100,101,102], automatic machine learning [103,104], applied physics [105,106], and medical applications [107,108].In the context of high-dimensional problems, multifidelity Bayesian optimization capitalizes from fast low-fidelity models to alleviate the computational burden associated with the required numerous observations of the objective function to effectively direct the search toward the given goal, and achieved promising results in terms of accuracy and efficiency for applications in quantum control [109], aerospace engineering [110], and reinforcement learning [111].</p>
<p>Multifidelity Bayesian optimization determines a learning procedure informed by the surrogate model of the objective function constructed on the dataset of noisy objective observations
D N = {(x 1 , y (l 1 ) 1 ), ..., (x n , y (l n ) n )}, where y (l n ) n ∼ N ( f (l n ) (x n ), σ ε (x n ))
and σ ε has the same distribution over the fidelities.This multifidelity surrogate model defines an approximation of the objective f (l) ∼ p( f (l) |(x, l), D N ) at different level of fidelity, and represents the belief about the distribution of the objective function over the domain χ based on data.A popular practice for MFBO is to extend the Gaussian process surrogate model to a multifidelity setting through an autoregressive scheme [112]:
f (l) = ρ (l−1) f (l−1) (x) + ζ (l) (x) l = 2, ..., L(11)
where ρ (l−1) is a constant scaling factor that includes the contribution of the previous fidelity with respect to the following one, and ζ (l) ∼ GP(0, κ (l) (x, x ′ )) models the discrepancy between two adjoining levels of fidelity.The posterior of the multifidelity Gaussian process is completely specified by the multifidelity mean function µ (l) (x, l) = E f (l) (x) that represents the approximation of the objective function at different levels of fidelity, and the multifidelity covariance function
κ (l) ((x, l), (x ′ , l)) = E ( f (l) (x, l) − µ (l) (x, l))( f (l) (x ′ , l) − µ (l) (x ′ , l)
) that defines the associated uncertainty for each level of fidelity.</p>
<p>The availability of multiple representations of the objective function poses a further decision task that has to be accounted by the learner during the sampling of unknown locations: the selection of the most promising sample is effected with the simultaneous designation of the information source to be evaluated.This is obtained through a learner represented by the multifidelity acquisition function U(x, l) that extends the infill criteria of Bayesian optimization, and selects the pair of sample and the associated level of fidelity to query (x new , l new ) ∈ max x∈χ,l∈L U(x, l) that is likely to provide higher gains with a regard for the computational expenditure.Among different formulations, well known multifidelity acquisition functions to address optimization problems are the Multifidelity Probability of Improvement (MFPI) [113], Multifidelity Expected Improvement (MFEI) [114], Multifidelity Predictive Entropy Search (MFPES) [115], Multifidelity Max-Value Entropy Search (MFMES) [116], and non-myopic multifidelity expected improvement [21].These formulations of the acquisition function define adaptive learning schemes that retain the infill principles characterizing the single-fidelity counterpart, and account for the dual decision task balancing the gains achieved through accurate queries with the associated cost during the optimization procedure.</p>
<p>An Active Learning Perspective</p>
<p>Bayesian frameworks and Active learning schemes exhibit a strong synergy: in both cases the learner seeks to design an efficient sampling policy to accomplish the learning goal, and is guided by a surrogate model that informs the learner and is continuously updated during the learning procedure.Active learning literature is vast an include a multitude of approaches [117,118,119,120,121,122,123,24].According to the well accepted classification proposed by Sugiyama and Nakajima [23], active learning strategies can be categorized in population-based and pool-based active learning frameworks according to the nature of the sampling scheme defined by the learner.Population-based active learning targets the identification of the best optimal density of the samples for training known the target distribution.Conversely, pool-based active learning defines an efficient sampling scheme to improve the efficiency of a surrogate model of the unknown target distribution over the domain of samples.</p>
<p>This paper explicitly formalizes and discusses Bayesian frameworks as an active learning procedure realized through acquisition functions.In particular, pool-based active learning shows in essence a strong dualism with Bayesian frameworks.We emphasize this synergy through the dissertation on the correspondence between learning criteria and infill criteria; the former drive the sampling procedure in pool-based active learning, while the latter guide the search in Bayesian schemes through the acquisition function.This symbiosis is evidenced for the case of a single source of information adopted to query samples, and when multiple sources are at disposal of the learner to evaluate new input.Accordingly, we review and discuss popular sampling policies commonly adopted in pool-based active learning, and discern the learning criteria to accomplish a specific learning goal (Section 5.1).Then, the attention is dedicated to the identification of the infill criteria realized through popular acquisition functions in Bayesian optimization (Section 5.2).The objective is to explicitly formalize the synergy between Bayesian frameworks and Active learning as adaptive sampling schemes guided by common principles.The same avenue is followed to formalize this dualism for the case of multiple sources of information available during the learning procedure.In particular, we identify the learning criteria The objective is to clarify the shared principles and the mutual relationship that characterize the two adaptive learning schemes when the decision of the sample to query requires also the selection of the appropriate source of information to be evaluated.</p>
<p>Learning Criteria</p>
<p>Pool-based active learning determines a tailored sampling policy to ensure the maximum computational efficiency of the adaptive sampling procedure -limited and well selected amount of samples to query.This adaptive learning demands for principled guidelines to decide whether to evaluate or not a certain sample based on a measure of its goodness.</p>
<p>Learning criteria permit to establish a metric to quantify the gains of all the possible learner decisions, and prescribe an optimal decision based on the information acquired from the surrogate model.The vast majority of the literature concerning pool-based active learning identifies three essential learning criteria: informativeness, representativeness and diversity [124,125,126,24,127,55]:</p>
<ol>
<li>Informativeness measures the amount of information encoded by a certain sample.This means that the sampling policy is driven by the maximum likely contribution of queries that would significantly benefit the objective of the learning procedure.2. Representativeness quantifies the similarity of a sample or a group of samples with respect to a target sample representative of the target distribution.Thus, the sampling policy exploits the structure underlying the domain to direct the queries in locations where a sample can represent a large amount of neighbouring samples.3. Diversity estimates how well the queries are disseminated over the domain of samples.This is reflected in a sampling policy that selects samples scattering across the full domain, and prevents the concentration of queries in small local regions.</li>
</ol>
<p>Figure 3 illustrates a watering optimization problem that attempts to clarify the peculiarities of each learning criteria.This simple toy problem requires to identify the areas of a wheat field where the crop is ripe and where it is still unripe for irrigation purposes.The learning goal is formalized as the identification of the area where the wheat is lower, which means an unripe cultivation and maximum requirements for irrigation.We assume that the learner can explore a maximum of five sites on the field during the procedure.A learner driven by the pure informativeness criterion (Figure 3(a)) would uniquely sample the regions of the wheat field that are likely to provide the maximum amount of information to accomplish the given learning goal; accordingly, observations are placed where the height of the wheat is minimum and the demand for water is maximum: this maximizes the information on where it is strictly necessary to irrigate, but nothing is known about the regions where the wheat is higher and irrigation is not a priority.Conversely, a purely representative sampling (Figure 3(b)) would probe the field by agglomerating observations to ensure the representativeness of the samples.This allows to partially know even areas where copious irrigation is not necessary, but increases the overall uncertainty given the small amount of samples for each agglomeration.If the learner pursues only the diversity of queries (Figure 3(c)), samples would scatter the field minimizing the maximum distance between measurements.Although this allows the queries to be distributed across the entire domain, the uncertainty is high as only one sample covers a respective area of the field.The remaining of this section is dedicated to the revision and discussion of popular pool-based active learning schemes.</p>
<p>We aim to provide a broad spectrum of approaches that exemplify the implementation of different learning criteria both individually and in combination.This permits to highlight the driving principles of learning procedures, and will help to better clarify the existing synergy between active learning and Bayesian optimization accounted in the following sections.Figure 4 summarizes the relationship between the methodologies reviewed in the following and the three learning criteria.</p>
<p>Informativeness-Based</p>
<p>Learning procedures characterized by a pure informative criterion can be traced in uncertainty-based sampling policies.These approaches make the query decision based on the predictive uncertainty of the surrogate model, and seek to improve the density of samples in regions that exhibit the largest uncertainty with respect to a specific learning goal.Popular uncertainty-based active learning algorithms are uncertainty sampling and query-by-committee methods.Uncertainty sampling algorithms probe the domain to improve the overall accuracy of the surrogate model according to a measure of the predictive uncertainty.Examples include the quantification of the uncertainty associated with samples [128], and its alternatives as margin-based [129], least confident [130] and entropy-based [131] approaches.Other strategies define sampling policies which promote the minimization of the surrogate model predicted variance [132] to maximize, respectively, the decrease of loss augmenting the training set [54], and the gradient descend [133].Other uncertainty-based strategies are query-by-committee sampling schemes [118,134], where the most informative sample to query is selected through the maximization of the disagreement between the predictions of a committee of surrogate models computed on subsets of the locations.</p>
<p>Representativeness/Diversity-Based</p>
<p>Other pool-based active learning algorithms rely exclusively on representativeness and diversity learning frames: usually these learning criteria are implemented simultaneously in the learning procedure to drive the domain probing.This blend is justified by the mutual complementary relationship between representativeness and diversity: pure representativeness might concentrate the sampling in congregated representative domain regions without a proper dispersion of queries, while pure diversity might lead to the over-query of the domain and divert the learning procedure from the actual goals.The combination of both the learning criteria permits on one hand to leverage the representativeness of samples to accomplish a certain learning goal, on the other hand prevents the selection of redundant samples and the high densities of queries only in circumstanced regions of the domain.Representative/diversity-based algorithms include a multitude of approaches that are commonly classified in two main schemes: clustering methodology and optimal experimental design.The former clustering algorithms identify the most representative locations exploiting the underlying structures of the domain: the utility of samples is obtained as a function of their distance from the cluster centers.Popular examples include hierarchical clustering and k-center clustering.The former identifies a hierarchy of clusters based on the encoded information, and selects samples closer to the cluster centers [135]; the latter determines a subset of k congruent clusters that together cover the sampling space and whose radius is minimized, and the best sample minimizes the maximum distance of any point to a center [136].The latter optimal experimental design defines a sampling policy based on a transductive approach: the learning procedure conducts the queries through a data reconstruction framework that measures the samples representativeness based on the capacity to reconstruct the training dataset.The selection of the most representative sample comes from an optimization process that maximizes the local acquisition of information about the parameters of the surrogate model [137,138,139].</p>
<p>Hybrid</p>
<p>Recent avenues explore the combination of both informativeness and representativeness/diversity learning criteria to combine the goal oriented query of the first, and the use of underlying structures preventing over-density of the second.Accordingly, combined-based algorithms integrates multiple learning criteria to improve the overall sampling performance.Those approaches are commonly classified into three main classes [140,55]: serial-form, criteria selection, and parallel-form approaches.Serial-form algorithms use a switching approach to take advantages from all the three learning criteria: informativeness-based techniques are used to select a subset of highly informative samples, and then representativeness/diversity techniques identify the centers of the clusters on this subset as the querying locations [125].Criteria selection algorithms rely on a selection parameter informed by a measure of the learning improvement that suggests the appropriate learning criteria to be used during the procedure [141].Both serial-form and criteria selection strategies combine the three learning criteria through a sequential approach where each criteria is used consecutively during the learning procedure.Parallel-form methods combine simultaneously multiple learning criteria: the utility of each sample is judged by weighting informativeness and representativeness/diversity at the same time; then, valuable samples are selected through a multi-objective optimization of the weights to maximize at the same time the improvement in terms of learning goals and the exploitation of potentially useful structures of the domain [142,143,144].</p>
<p>Acquisition Functions and Infill Criteria</p>
<p>The synergy between active learning and Bayesian optimization relies on the substantial analogy between the learning criteria driving the active learning procedure and the infill criteria that characterize the Bayesian learning scheme.Infill criteria provide a measure of the information gain in terms of utility acquired evaluating a certain location of the domain.</p>
<p>In Bayesian optimization, the acquisition function is formalized according to a certain infill criterion: this permits to quantify the merit of each sample with respect to a specific learning goal.Accordingly, the sample that maximizes the querying utility is observed to enrich the learning procedure toward this goal.</p>
<p>In particular, Bayesian learning schemes rely on two main infill criteria: global exploration ad local exploitation toward the optimum.The former exploration criterion concentrates the samples in regions of the domain where the uncertainty predicted by the surrogate is higher; this enhances the global awareness about the distribution of the objective function over the domain, but the resources might not be directed toward the goal of the procedure -e.g.minimum of the objective function.The latter exploitation criterion condensates the samples in regions where the surrogate model indicates that the objective is likely to be located -e.g.minimum of the Gaussian process mean function; exploitation realizes a goal-oriented sampling procedure that privileges the search for the objective without a potentially accurate knowledge of the overall distribution of interest.The dilemma between exploration and exploitation represents a key challenge to be carefully addressed.On one hand, a learning procedure based on pure exploration might use a large amount of samples to improve the overall accuracy of the surrogate model without searching toward the learning goal.On the other hand, an exploitation-based learner might anchor a high density of samples to a suboptimal local solution as a consequence of the information from an unreliable surrogate model.These extreme behaviours demonstrate the need to find a compromise between exploration and exploitation criteria.</p>
<p>In principle, infill criteria in Bayesian optimization are strongly related to the learning criteria commonly adopted in active learning.In particular:</p>
<p>• The concept of exploration is close to the representativeness/diversity criterion: both these learning schemes leverage underlying structures of the target distribution predicted by an accurate surrogate model to improve the awareness about the objective over the domain.</p>
<p>• The concept of exploitation is close to the informativeness criterion: the learner directs the selection of samples toward the believed objective without considering the global behaviour of the objective over the domain.</p>
<p>Figure 5 summarizes the mapping between infill criteria and learning criteria.The following sections discuss the formalization of (infill) active learning criteria for three popular formulations of Bayesian acquisition functions, namely the expected improvement (Section 5.2.1), probability of improvement (Section 5.2.2), and max-value entropy search (Section 5.2.3).</p>
<p>Figure 5: Mapping of the learning criteria in active learning and infill criteria in Bayesian optimization.</p>
<p>Expected Improvement</p>
<p>The Expected Improvement (EI) acquisition function quantifies the expected value of the improvement of the solution of the optimization problem achieved evaluating a certain location of the domain [61,145].EI in the generic location x relies on the predicted improvement over the best solution of the optimization problem observed so far.Considering the Gaussian process as the surrogate model for Bayesian optimization, EI can be expressed as follows:
U EI (x) = σ (x)<a href="12">I(x)Φ(I(x)) + φ (I(x))</a>
where
I(x) = ( f (x * ) − µ(x))/σ (x)
is the predicted improvement, x * is the current location of the best value of the objective sampled so far, µ is the mean function and σ is the standard deviation of the GP, and Φ(•) and φ (•) are the cumulative distribution function and the probability density function of a standard normal distribution, respectively.The computation of U EI (x) requires limited computational resources and the first-order derivatives are easy to calculate:
∂U EI (x) µ(x) = −Φ(I(x))(13)∂U EI (x) σ (x) = φ (I(x)).(14)
Both Equation (13) and Equation ( 14) demonstrate that U EI (x) is monotonic with respect to the increase of both the mean and the uncertainty of the GP surrogate model.This highlights a form of trade-off between exploration and exploitation: the formulation of the EI permits to balance the sampling in locations of the domain where is likely to have a significant improvement of the solution with respect to the current best solution, and the observations of regions where the improvement might be contained but the prediction is highly uncertain.In principle, it is possible to state that EI is driven by a combination of informativeness and representativeness/diversity criteria adopted in active learning.On one hand, the learner seeks to direct the computational resources toward the maximization of the learning contribution and achievement of the goal -informativeness; on the other hand, the learner pursues the awareness of the objective distribution over the domain to improve the quality of the prediction and better drive the searchrepresentativeness/diversity.The predictive framework of the surrogate model regulates the learning thrusts privileging the one over the other on the basis of the information about the objective function acquired over the iterations.</p>
<p>Probability of Improvement</p>
<p>The Probability of Improvement (PI) acquisition function targets the locations characterized by the highest probability of achieving the goal, based on the information from the current surrogate model [87,146].PI measures the probability that the prediction of the surrogate model in the generic location is lower than the best observation of the objective function so far.Under the Gaussian process surrogate model, the PI acquisition function is computed in closed form as follows:
U PI (x) = Φ (I(x))(15)
where Φ(•) is the cumulative distribution function of a standard normal distribution and x * is the current location of the best value of the objective.Similarly to EI, also U PI (x) is inexpensive to compute and the evaluation of the first-order derivatives requires simple calculations:
∂U PI (x) ∂ µ(x) = − 1 σ (x) φ (I(x))(16
)
∂U PI (x) ∂ σ (x) = − I(x) σ (x) φ (I(x)) (17)
where φ is the standard Gaussian probability density function.As demonstrated by Equation ( 16), regions of the input space characterized by lower values of the posterior mean of the GP are preferred for sampling, at fixed uncertainty of the surrogate.Moreover, Equation (17) shows that if µ(x) &lt; f (x * ) the regions characterized by lower uncertainty are preferred and, conversely, PI increases with uncertainty.Overall, the PI acquisition function can be considered as an exploitative scheme that determines the most informative location as the one that potentially produces a larger reduction of the minimum value of the objective function observed so far.This is achieved sampling regions where the surrogate model is reliable and characterized by lower levels of uncertainty.In principle, this sampling scheme makes PI in accordance with the informativeness criterion: the search toward the optimum is uniquely directed in regions of the domain that exhibit the higher probability of achieving the goal according to the emulator prediction.</p>
<p>Entropy Search and Max-value Entropy Search</p>
<p>The Entropy Search (ES) acquisition function measures the differential entropy of the believed global minimum location of the objective function, and targets the reduction of uncertainty selecting the sample that maximizes the decrease of differential entropy [88].The ES acquisition function is formulated as follows:
U ES (x) = H(p(x * |D)) − E f (x)|D <a href="18">H(p(x * | f (x), D))</a>
where H(p(x * )) is the entropy of the posterior distribution at the current iteration in the location of the minimum of the objective function x * , and E f (x) [•] is the expectation over f (x) of the entropy of the posterior distribution at the next iteration on x * .Typically, the exact calculation of the second term of Equation ( 18) is not possible and requires complex and expensive computational techniques to provide an approximation of U ES (x).</p>
<p>The Max-value entropy search (MES) [89] acquisition function is derived from the ES acquisition function and allows to reduce the computational effort required to estimate Equation ( 18) measuring the differential entropy of the minimum-value of the objective function:
U MES (x) = H(p( f |D)) − E f (x)|D <a href="19">H(p( f | f * , D))</a>
where the first and the second term are now computed for the minimum value of the objective function f * .This permits to simplify the computations and to approximate the second term through a Monte Carlo strategy [89].The analysis of the derivatives is not possible for the MES acquisition function since the formulation of the second term of Equation ( 19) is intractable.</p>
<p>As reported by Wang et al. [89] in their experimental analysis, MES targets the balance between the exploration of locations characterized by higher uncertainty of the surrogate model, and the exploitation toward the believed optimum of the objective function.However, Nguyen et al. [147] demonstrate that MES might suffer from an imbalanced exploration/exploitation trade-off due to noisy observations of the objective function, and to the discrepancy in the computation of the mutual information in the second term of Equation ( 19).As a result, MES might over-exploit the domain in presence of noise in measurements, and over-explore when the discrepancy in the evaluation issue determines a pronounced sensitivity to the uncertainty of the surrogate model.Overall, the adaptive sampling scheme determined by the MES acquisition function follows both the informativeness and the representativeness/diversity learning criteria: the most promising sample is ideally selected targeting the balance between the search toward the believed minimum predicted by the emulator, and the decrease of uncertainty about the objective function distribution.</p>
<p>Learning Criteria with Multiple Oracles</p>
<p>Most of the active learning paradigms rely on a unique and supposed omniscient source of information about the target distribution.This oracle is iteratively queried by the learner to evaluate the value of the distribution in certain locations, and is assumed that its prediction is exact.In many other scenarios, the learner can elicit information from multiple imperfect oracles at different levels of reliability, accuracy and cost.Accordingly, the active learning community introduced a multitude of annotator-aware algorithms which are capable of efficiently learning from multiple sources of information.This requires to make an additional decision during the learning procedure: the learner has to select at each iteration the most useful sample and the associated information source to query.In this context, the original learning criteria of informativeness and representativeness/diversity (Section 5.1) evolve and extend to quantify the utility of querying the domain with a certain level of accuracy and associated cost:</p>
<ol>
<li>Informativeness seeks to maximize the amount of information to decide the sample and information source to query.Thus, the learner might privilege the evaluations from accurate and yet costly oracles to capitalize from high-quality information and potentially reach the objective.2. Representativeness attempts to identify underlying structures of the domain to better inform the search procedure.In this case, the decision making process might prefer to interrogate less expensive sources of information to contain the required effort, especially if cheap predictions of the target distribution exhibit good correlation with the estimate of the accurate oracle.3. Diversity scatters the sampling effort over the domain to pursue a proper distribution of evaluations and augment the awareness about the target distribution.This might be favored by a major use of less accurate predictions of the target distribution, which are more likely to well address the cost/effectiveness trade-off during the diversity sampling.</li>
</ol>
<p>The remaining of this section provides an overview of different multiple oracles active learning methodologies to present and further clarify popular extensions of the learning criteria to a multi-oracle setting.</p>
<p>Typically, active learning paradigms are extended to the multiple-oracle setting through relabeling, repeating-labeling, probabilistic and transfer knowledge, and cost-aware algorithms.Relabeling approaches query samples multiple times using the library of sources of information available, and the final query is obtained via majority voting [148].Popular methodologies following this scheme pursue the identification of a subset of oracles according to the proximity of their upper confidence bound to the maximum upper confidence bound, and apply the majority voting technique only considering the queries of this informative subset [149].Other multi-oracle active learning methods use a repeatinglabeling procedure: the learner integrates the repeated -often noisy -prediction of the oracles to improve the quality of the evaluation process and the accuracy of the surrogate model learned from data [150].Both relabeling and repeatinglabeling approaches share a common drawback: the same unknown sample is evaluated multiple times with different oracles, which results in a sub-optimal usage of the available sources of information.Probabilistic and transfer learning methodologies attempt to overcome this limitation.Probabilistic frameworks rely on surrogate models specifically conceived for the multi-source scenario that provides a predictive framework to estimate the accuracy of each oracle in the evaluation of samples over the domain [151,152].Transfer knowledge approaches enhance the simultaneous selection of the most informative location to sample and the associated most profitable source to query; this is achieved through the transfer of knowledge from samples not evaluated in auxiliary domains to support the estimate of the oracle reliability [153].Recent advancements in multiple oracles active learning are cost-effective algorithms, where the cost of an oracle is evaluated considering both the overall reliability of the prediction and the quality of samples in specific locations [154,155,156].The cost-effectiveness property enhances the use of computational resources for the evaluation of samples, and targets the search toward the learning objectives while guarantees an optimal trade-off between evaluation accuracy and computational cost.</p>
<p>From the examined literature, the three learning criteria appear frequently coupled together during the learning procedure with multiple sources to query.This appears as a natural evolution of what has already been observed in the literature for active learning with single information source: the overall learning procedure usually benefits from a balanced learning scheme driven by informativeness and representativeness/diversity.In particular, informativeness permits to direct the search toward the learning goal, while representativeness/diversity augments the learner awareness about the target distribution over the domain; the combination of these learning criteria -in different measures -contributes to improve the performance of the active learning algorithms by efficiently using the computational resources and the information from multiple oracles.</p>
<p>Multifidelity Acquisition Functions and Infill Criteria</p>
<p>This section further investigates and highlights the synergy between active learning and Bayesian optimization for the specific case of multiple sources of information used to accomplish the learning goal.Similarly to the single source setting, this symbiotic relationship is revealed through common principles characterizing the infill criteria in multifidelity Bayesian optimization and the learning criteria in active learning with multiple oracles.The multifidelity scenario imposes an additional decision to be made: the learner has to identify the appropriate information source to query according to an accuracy/cost trade-off.This is reflected in the formalization of infill criteria capable of defining an efficient and balanced sampling policy, targeting either the wise selection of the samples and the levels of fidelity which ensure the maximum benefits with the minimum cost.Accordingly, the multifidelity acquisition function formalizes an adaptive sampling scheme based on one or multiple infill criteria to quantify the utility of querying a location of the domain with a specific level of fidelity.</p>
<p>Based on these considerations, the exploration and exploitation infill strategies are extended according to the peculiarities of the multifidelity setting:</p>
<p>• Exploration is close to the representativeness/diversity criterion and defines a sampling policy that incentivizes the overall reduction of the surrogate uncertainty.Accordingly, the selection of the appropriate level of fidelity is driven by a trade-off between accuracy and evaluation cost.This might be accomplished through less-expensive low-fidelity information to contain the demand for computational resources during exploration.</p>
<p>• Exploitation is close to the informativeness criterion and concentrates the sampling process in the regions of the domain where optimal solutions are likely to be located.For this purpose, the learner might emphasize the use of accurate evaluations of the target function to refine the solution of the learning procedure toward the specific goal.</p>
<p>Similarly to the acquisition functions in Bayesian optimization (Section 5.2), the symmetry between informativeness and exploitation criteria, and between representativeness/diversity and exploration criteria is preserved in the multifidelity setting.The following sections are dedicated to the review and discussion of popular multifidelity acquisition function, namely the multifidelity expected improvement (Section 5.4.1),multifidelity probability of improvement (Section 5.4.2) and multifidelity max-value entropy search (Section 5.4.3).The goal is to highlight the equivalent principles driving both the learning schemes, and further clarify the elements that encode the symbiotic relationship that exists between multifidelity Bayesian optimization and multi-oracle active learning.</p>
<p>Multifidelity Expected Improvement</p>
<p>The Multifidelity Expected Improvement (MFEI) extends the expected improvement acquisition function to define a learning scheme in the multifidelity setting as follows [114]:
U MFEI (x, l) = U EI (x, L)α 1 (x, l)α 2 (x, l)α 3 (l)(20)
where U EI (x, L) is the expected improvement illustrated in Equation ( 12) and evaluated with the highest level of fidelity L, and the utility functions α 1 , α 2 and α 3 are defined as follows:
α 1 (x, l) = corr f (l) , f (L)(21)α 2 (x, l) = 1 − σ ε σ 2(l) (x) + σ 2 ε (22) α 3 (l) = λ (L) λ (l) .(23)
The first element α 1 is the posterior correlation coefficient between the level of fidelity l and the high-fidelity level L, and accounts for the reduction of the expected improvement when a sample is evaluated with a low fidelity model.This term reflects a measure of the informativeness of the l-th source of information in the location x, and balances the amount of improvement achievable evaluating the high-fidelity level L with the reliability of the prediction associated with the level of fidelity l.Accordingly, α 1 modifies the learning scheme by adding a penalty in the formulation that reduces the U MFEI when 1 ≤ l &lt; L: this includes awareness about the increase of uncertainty associated with a low-fidelity prediction.The second element α 2 is conceived to adjust the expected improvement when the output evaluated with the l-th level of fidelity contains random errors.This is equivalent to consider the reduction of the uncertainty of the Gaussian process prediction after a new evaluation of the objective function is added to the dataset D. This function allows to improve the robustness of U MFEI when the representation of f (l) at different levels of fidelity is affected by noise in the measurements.The third element α 3 is formulated as the ratio between the computational cost of the high-fidelity level L and the l-th level of fidelity.This permits to balance the informative contributions of high-and a lower-fidelity observations and the related computational resources required for the evaluation.The effect of this term is to encourage the use of low-fidelity representations if almost the same expected improvement can be achieved with a high-fidelity evaluation.This wisely directs the use of computational resources to achieve the representativeness/diversity of samples, and prevents a massive use of expensive accurate queries during the exploration phases.</p>
<p>Multifidelity Probability of Improvement</p>
<p>The Multifidelity Probability of Improvement (MFPI) acquisition function provides an extended formulation of the probability of improvement suitable for the multifidelity scenario as follows [113]:
U MFPI (x, l) = U PI (x, L)η 1 (x, l)η 2 (l)η 3 (x, l)(24)
where the PI acquisition function (Equation ( 15)) is computed considering the highest-fidelity level L available, and the utility function η 1 , η 2 and η 3 are defined as follows:
η 1 (x, l) = corr f (l) , f (L)(25)η 2 (l) = λ (L) λ (l)(26)η 3 (x, l) = n l ∏ i=1 1 − R x, x(l) i
.</p>
<p>The first term η 1 shares the same formalization of the utility function α 1 in Equation ( 21), and accounts for the increase of uncertainty associated with low-fidelity representations 1 ≤ l &lt; L if compared with the high-fidelity output L. This reduces the probability of improvement if a low-fidelity representation is queried in a specific location of the input space x.As already highlighted in Section 5.4.1, η 1 incentivizes a form of informativeness learning where the information source is selected according to its capability to accurately represent the objective function.Similarly, the second utility function η 2 is also included in the multifidelity expected improvement in Equation ( 23) as the α 3 term.This element balances the computational costs and the informative contributions achieved through the l-th level of fidelity.This prevents the rise of computational demand produced by the over-exploitative nature of the probability of improvement (Section 5.2.2): η 2 encourages the use of fast low-fidelity data if the discrepancy between the l-th level of fidelity and the high-fidelity L -quantified by η 1 -is not significant.The third element η 3 is the sample density function computed as the product of the complement to unity of the spatial correlation function R (•) [157] evaluated for the n l samples considering the l-th level of fidelity.This term reduces the probability of improvement in locations with an high sampling density -over exploitation of the domain -to prevent the clustering of data.Accordingly, η 3 promotes a form of representativeness/diversity learning scheme and encourages the exploration to augment the awareness about the domain structure.</p>
<p>Multifidelity Entropy Search and Multifidelity Max-Value Entropy Search</p>
<p>The Multifidelity Entropy Search (MFES) acquisition function is formulated extending the entropy search acquisition function to query multiple sources of information [115] U
MFES (x) = H(p(x * |D)) − E f (l) (x)|D <a href="28">H(p(x * | f (l) (x), D))</a>
where the expectation term E f (l) (x) [•] considers multiple levels of fidelity l = 1, ..., L. Similarly to the entropy search acquisition function, the computation of the expectation in Equation ( 28) is not possible in closed-form and requires an intensive procedure to provide a reliable approximation.</p>
<p>The Multifidelity Max-Value Entropy Search (MFMES) acquisition function can be formulated extending the max-value entropy search to a multifidelity setting as follows [116]:
U MFMES (x) = [H(p( f (l) |D)) − E f (l) (x)|D [H(p( f (l) | f * (L) , D))]]/λ (l)(29)
where the differential entropy is measured for the minimum value of the objective function f * (L) considering the high-fidelity representation L. In this case, the approximation of the expectation term in Equation ( 29) relies on a Monte Carlo strategy that allows to contain the computational cost if compared with the procedure used for the MFES acquisition function [116].</p>
<p>In the multifidelity scenario, the MFMES acquisition function measures the information gain obtained evaluating the objective function f (l) (x) in a certain location x and associated level of fidelity l with respect to the global minimum of the objective function.This can be interpreted as an informativeness-driven learning based on the reduction of the uncertainty associated with the minimum value of the objective f * (L) through the observation f (l) (x), where this uncertainty is measured as the differential entropy associated with the l-th level of fidelity.At the same time, the information gain is also sensitive to the accuracy of the surrogate predictive framework, and realizes a form of representativeness/diversity balance to improve the awareness about the distribution of the objective function over the domain.The sensitivity to the computational cost λ (l) of the l-th level of fidelity is introduced in Equation ( 29) to balance the quality of the source -quantified by the information gain -and the demand for computational resources.</p>
<p>Experiments</p>
<p>This section investigates and compares the performance of the acquisition functions for both single-fidelity and multifidelity Bayesian optimization considering a set of benchmark problems conceived to stress those algorithms.</p>
<p>The objective is to highlight advantages and opportunities offered by different learning principles over challenging mathematical properties of the objective function, which are frequently encountered in real-world engineering and scientific problems.[16].In particular, this comparative study considers the expected improvement (Section 5.2.1), probability of improvement (PI) (Section 5. We impose the same initialization conditions for both the single-fidelity and the multifidelity algorithms.This initial setting includes: (i) the initial dataset of N</p>
<p>0 samples for each level of fidelity l to compute the prior surrogate model of the objective function, (ii) the computational cost assigned to each level of fidelity λ (l) , and (iii) the maximum computational budget B max allocated for each benchmark problem defined linearly with the dimensionality D of the problem B max = 100D.The initial dataset N (l) 0 is obtained through Latin hypercube sampling for all the numerical experiments [158] to ensure the full coverage of the range of the optimization variables.The computational budget B = ∑ λ (l) i is quantified as the cumulative computational cost used during the optimization at each iteration i.All the methods are based on the Gaussian processes surrogate model and its extension to the multifidelity setting.We implement the squared exponential kernels for all the GP covariances, and use the maximum likelihood estimation approach to optimize the hyperparameters of the kernel and the mean function of the GP [159].</p>
<p>Benchmark Problems</p>
<p>The following set of benchmark problems is specifically conceived to investigate the capabilities of different learning criteria over challenging mathematical properties of the objective function [16].In particular, the experimental settings include a variety of attributes that can be traced in real-world optimization problems, namely local and global behaviours, non-linearities and discontinuities, multimodality and noise.The set of problems consist of several objective functions such as the Forrester continuous and discontinuous, the Rosenbrock increasing the domain dimensionality, the Rastrigin shifted and rotated, the Agglomeration of Locally Optimized Surrogate (ALOS), a coupled spring-mass optimization problem and the noisy Paciorek function.</p>
<p>Forrester Function</p>
<p>The Forrester function is a popular test-case to investigate the performance of different learning strategies over a non-linear one-dimensional distribution characterized by local behaviours.This benchmark problem guarantees an high interpretability of the results thanks to the one-dimensional nature of the objective function.The search domain is bounded between χ = [0, 1] and four levels of fidelity are available during the optimization:
f (4) (x) = (6x − 2) 2 sin(12x − 4)(30)
f (3) (x) = (5.5x− 2.5) 2 sin(12x − 4) ( 31)  where f (4) is the high-fidelity function and the levels of fidelity l = 1, 2, 3, 4 increase with the accuracy of the representations.Figure 6(a) reports the four levels of fidelity for the Forrester function over the search domain.
f (2) (x) = 0.75 f (4) (x) + 5(x − 0.5) − 2(32)
The analytical minimum of the Forrester function is equal to f * (4) = −6.0207and it is located at the domain point x * = 0.7572.</p>
<p>Jump Forrester Function</p>
<p>The jump Forrester function introduces a discontinuity in the formulation of the Forrester function to investigate the capabilities of the learning schemes to refine the surrogate model and capture the instantaneous variation of the objective function over the domain.This scenario can often occur in real problems where the phenomena of interest -e.g.physical quantity of interest in engineering -evolves over the domain and determines large variations of the objective function values.Figure 6(b) reports the two levels of fidelity that are available during the search procedure:
f (2) (x) = (6x − 2) 2 sin(12x − 4), 0 ≤ x ≤ 0.5 (6x − 2) 2 sin(12x − 4) + 10, 0.5 &lt; x ≤ 1(34)
f (1) (x) = 0.5 f (2) (x) + 10(x − 0.5) − 5, 0 ≤ x ≤ 0.5 0.5 f (2) (x) + 10(x − 0.5) − 2 0.5 &lt; x ≤ 1 (35) where f (2) is the high-fidelity information source.The optimum is located at x * = 0.75724876 corresponding to a value of the objective equal to f * (2) = −0.9863.</p>
<p>Rosenbrock Function</p>
<p>The Rosenbrock function permits to investigate the learning criteria over a non-convex objective function that allows for parametric scalability over the domain χ = [−2, 2] D where D is the dimensionality of the input space.A library of three levels of fidelity is available (Figure 7):
f (3) (x) = D−1 ∑ i=1 100(x i+1 − x 2 i ) 2 + (1 − x i ) 2(36)f (2) (x) = D−1 ∑ i=1 50(x i+1 − x 2 i ) 2 + (−2 − x i ) 2 − D ∑ i=1 0.5x i(37)f (1) (x) = f (3) (x) − 4 − ∑ D i=1 0.5x i 10 + ∑ D i=1 0.25x i (38) (a) Rosenbrock f (3) , f (2) (b) Rosenbrock f (3) , f(1)</p>
<p>ALOS Functions</p>
<p>The Agglomeration of Locally Optimized Surrogate (ALOS) is a heterogeneous and non-polynomial function defined on unit hypercubes up to three dimensions useful to assess the accuracy of surrogate models in presence of localized behaviours.In particular, the ALOS function reproduces a real-world scenario where the objective function is characterized by oscillatory phenomena at different frequency distributed along the domain.We consider two levels of fidelity and increasing dimensionality of the input space D = 1, 2, 3.For D = 1 the ALOS function is formalized as follows:</p>
<p>f (2) (x) = sin[30(x − 0.9) 4 ] cos[2(x − 0.9)] + (x − 0.9)/2 f (1) (x) = ( f (2) (x) − 1.0 + x)/(1.0+ 0.25x) (39) and for D = 2, 3 is formulated as:
f (2) (x) = sin[21(x 1 − 0.9) 4 ] cos[2(x 1 − 0.9)] + (x 1 − 0.7)/2 + ∑ D i=2 ix i i sin ∏ i j=1 x j f (1) (x) = ( f (2) (x) − 2.0 + ∑ D i=1 x i )/(5.0 + ∑ 2 i=1 0.25ix i − ∑ D i=3 0.25ix i )(40)</p>
<p>Shifted-Rotated Rastrigin Function</p>
<p>The Rastrigin function is commonly used as test function to represent real-world applications where the objective function might present a high multimodal behaviour.We adopt a benchmark problem based on the original formulation of the Rastrigin function shifted and rotated as follows (Figure 9):
f (z z z) = D ∑ i=1 (z 2 i + 1 − cos(10πz i )),(41)
where: z z z = R(θ )(x − x * ) and R(θ ) = cos θ − sin θ sin θ cos θ is the rotation matrix with the rotation angle fixed at θ = 0.2.</p>
<p>We define three levels of fidelity for this benchmark problem as follows:</p>
<p>where e r (z z z, φ i ) is the resolution error:
e r (z z z, φ ) = 2 ∑ i=1 a(φ ) cos 2 (w(φ )z i + b(φ ) + π).(43)
with Θ(φ ) = 1 − 0.0001φ , a(φ ) = Θ(φ ), w(φ ) = 10πΘ(φ ), and b(φ ) = 0.5πΘ(φ ).Thus, we define the high-fidelity function f (3) (φ = 10000), the intermediate fidelity function f (2) (φ = 5000) and the low-fidelity function f (1) (φ = 2500).For this benchmark, the input variables are defined within the interval χ = [−0.1,0.2] 2 and the analytical optimum is f * (3) = 0 located at x * = [0.1,0.1].</p>
<p>Spring-Mass System</p>
<p>This benchmark problem consists of a coupled spring mass system composed of two masses connected by two springs.</p>
<p>The challenges associated with this simple physical optimization problem are related to the intrinsic multimodality induced by the elastic behaviour of the system dynamics.We consider the masses m 1 and m 2 concentrated at their center of gravity and the elastic behaviour of the two spring modeled through the Hooke's law and characterized by the Hooke's constants k 1 and k 2 , respectively.Considering a friction-less dynamics, it is possible to define the equations of motion as follows Equation ( 44) can be solved using the fourth-order accurate Runge-Kutta time-marching method and varying the time-step dt to define two fidelity levels.Specifically, we define the high-fidelity model f (2) (dt = 0.01) and the low-fidelity model f (1) (dt = 0.6).The benchmark problem requires the identification of the combination of masses and Hooke's constants of springs x = [m 1 , m 2 , k 1 , k 2 ] that minimizes h 1 (t = 6) considering the domain χ = [1, 4] 4 and the initial conditions of motion h 1 = h 2 = 0 and ḣ1 = ḣ2 = 0.
m 1 ḧ1 (t) = (−k 1 − k 2 ) h 1 (t) + k 2 h 2 (t)(44)m 2 ḧ2 (t) = k 2 h 1 (t) + (−k 1 − k 2 ) h 2 (t).(45)</p>
<p>Paciorek Function with Noise</p>
<p>The Paciorek function reproduces an optimization setting where the objective function is affected by measurement noise and localized multimodal behaviour.This scenario is replicated through a random noise term in the low-fidelity Paciorek function as follows (Figure 10):
f (2) (x) = sin D ∏ i=1 x i −1(46)f (1) (x) = f (2) (x) − 9A 2 cos D ∏ i=1 x i −1 + rand.norm(0, α)(47)
where f (2) is the Paciorek function, A = 0.5, α = 0.2, and the input variable is defined across the input domain χ = [0, 3, 1] 2 .</p>
<p>Results and Discussion</p>
<p>First, we define the following evaluation metrics to assess the performances of the Bayesian schemes [16]:
ε x = ∥x * − x * ∥ √ N(48)ε f = f (x * ) − f * f max − f *(49)
where x * is the location of the analytical optimum, x * is the optimum identified by the algorithm, and f max and f * are the maximum and minimum of the objective function, respectively.The first metric ε x quantifies the search error in the domain of the objective function, while the second metric ε f evaluates the error associated with the learning goalminimum of the objective function [160].We evaluate the metrics ε x and ε f as functions of the computational budget B defined as the cumulative computational cost associated with observations of the objective function considering the l-th level of fidelity.We run 10 trails for each benchmark problem presented in Section 6.1 to compensate the influence of the random initial design of experiments, and to verify the sensitivity and robustness of the algorithms to the initialization setting.The results for all the experiments are reported in terms of median values of ε x and ε f .show that the multifidelity algorithms identify the optimum solution with a significant reduction of the computational budget if compared with the single fidelity counterparts.The best performing algorithm is the MFPI learner considering only the high-fidelity l = 4 and the lower-fidelity l = 1 levels, while the second best is the MFEI acquisition function considering available the complete spectrum of fidelities l = 1, 2, 3, 4.These outcomes suggest that multifidelity learning paradigms driven majorly by informativeness -MFPI acquisition function -are capable of efficiently directing the computational resources toward the optimum of low-dimensional objective functions in presence of continuous localized behaviours.Moreover, it should be noted that the MFEI capitalizes from all the information sources available and leverages the balance between informativeness and representativeness/diversity to effectively search toward the analytical optimum.The single fidelity Bayesian frameworks exhibit a lower convergence rate with respect to the multifidelity algorithms.The EI and PI use almost the same computational budget to identify the optimum solution, while the MES adopts more evaluations of the objective function.This confirms the observations for the multifidelity experiments.PI takes advantage from the purely exploitation of high-fidelity samples in the surrounding of the surrogate minimum to reach the optimum.This can be explained with the computation of an accurate surrogate model -at least close to the optimum -for low-dimensional objective functions.In contrast, EI balances an exploration phase to improve the overall accuracy of the surrogate with the exploitation toward the believed optimum.Particular attention should be dedicated to the MES and MFMES outcomes.In the single-fidelity frameworks, MES scores slightly worst both in terms of convergence rate and budget expenditure.This can be interpreted with an overall over-exploration behavior: MES distributes computational resources to explore the domain and refine the surrogate model, and directs lately efforts toward the optimum.This trend is considerably dampened in the multifidelity scenario, where MFMES shows good capabilities especially when all the sources of information are available during the search.In this case, cheap low-fidelity observations are used to explore the domain with contained computational expenditure, and high-fidelity data are mostly adopted to search toward the prescribed optimum location.</p>
<p>The discontinuous Forrester problem introduces a discontinuous local property of the objective function that further stresses the learning schemes.This can be explicitly observed with the average improvement of the budget required to achieve the optimum.Overall, it is possible to identify the same trends observed for the continuous Forrester function (Figure 11(b) and Figure 11(d)): either balancing exploration and exploitation -EI and MFEI -or a major exploitation search -PI and MFPI -lead to an efficient identification of the analytical optimum.In contrast, the over-exploration of MES and MFMES decelerates the optimization procedure with respect to the counterpart competing methods.This can be observed majorly for the MES which uses almost all the budget available to explore the domain and finally reach the optimum.Figure 12 illustrates the experiments conducted on the Rosenbrock benchmark function increasing the dimensionality D of the domain.This allows to investigate the performance of the learning scheme as the number of parameters to optimize increases.Overall, the multifidelity schemes deliver better convergences with a fraction of the computational budget required by single-fidelity algorithms for all the dimensions of the domain -D = 2, 5, 10.</p>
<p>For D = 2 (Figure 12(a) and Figure 12(d)), MFEI and MFPI implementing only the highest and lower levels of fidelity l = 1, 3 are the best performing algorithms, followed by the counterpart considering all the fidelities spectrum and the MFMES also learning from l = 1, 3. Two major observations can be made in this experimental setting.First, multifidelity learners are not capable of making advantage of the intermediate fidelity l = 2 during exploration leading to an increase of the computational expenditure.A possible explanation to these outcomes is the local behaviour of the intermediate fidelity that pushes the exploration in regions far from the optimum.Second, pure exploitation or a balanced search between exploration and exploitation are advantageous in low-dimensional domains, while pure exploration sacrifices valuable computational resources to improve the awareness about the global distribution of the objective instead of searching the optimum.</p>
<p>Increasing the dimension of the input space to D = 5 (Figure 12(b) and Figure 12(e)), only the MFEI and the MFPI using all the fidelities available are capable of identifying the optimum solution, while the other competing algorithms converge to suboptimal solutions.However, it should be noted the much faster convergence of MFEI with respect to the MFPI in the complete setting.These outcomes indicate that as the number of optimization variables increases, both exploration and exploitation are required for an efficient learning procedure.In particular, the exploration improves the accuracy of the surrogate over the domain which permits to better inform the learner during the exploitation phase.The utility of purely exploitation -MFPI -also continues to be observed, but the effectiveness is limited by the dimensionality of the domain that requires an exploration phase to better capture the distribution of the objective function.</p>
<p>Pushing further the dimensionality of the domain at D = 10 (Figure 12(c) and Figure 12(f)), all the algorithms are not capable of reaching the analytical optimum with the allocated budget.This can be explained with the unreliable prediction of the surrogate model that is not capable of correctly informing the learner with limited amount of datalimited allocated budget.However, the multifidelity paradigms achieve larger reductions of both the error in the domain ε x and the goal error ε f if compared with the single-fidelity outcomes.This suggests that learners capable of leveraging multiple information sources might produce higher gains in a limited budget scenario thanks to the massive use of cheap low-fidelity models to learn the objective function.Among the competing strategies, MFMES exhibits remarkable outcomes in terms of convergence values of the errors when all the library of fidelities is available.These results can be justified with the over-exploration properties of the MFMES acquisition function: the learner uses massive low-fidelity data to refine the approximation of the surrogate model and augment its predictive capabilities.This permits to better inform the procedure and direct computational resources toward the optimum.</p>
<p>The results obtained for the ALOS benchmark problem in Figure 13  The outcomes related to the multimodal benchmarks are reported in Figure 14.The multifidelity algorithms are capable of converging toward the analytical optimum with a fraction of the computational cost, if compared with the singlefidelity results.For the Rastrigin function (Figure 14(a) and Figure 14(d)), the multifidelity methods implementing all the levels of fidelity l = 1, 2, 3 outperform the multifidelity methods with l = 1, 3: the intermediate level of fidelity l = 2 is more accurate if compared with the low-fidelity output l = 3 and allows to improve the reliability of the Gaussian process in presence of a strong multimodal behaviour.The best performing method is MFPI using l = 1, 2, 3 denoting that the over-exploitation of the input space with lower-fidelity levels l = 2, 3 allows to take full advantage from low-fidelity data, improving the performance of the learning process.In contrast, we observe that the MES algorithm exhibits a more efficient convergence of the MFMES counterpart.This is related to the already noticed over-exploration of the domain: the MES uses accurate high-fidelity observations to refine the surrogate during the exploration, while the MFMES systematically adopts lower-levels of fidelity to massively query the domain and retard the exploitation with more accurate information sources.The results achieved for the mass spring benchmark problem (Figure 14(b) and Figure 14(e)) confirm the superior convergence performance of the multifidelity algorithms in presence of marked multimodal objective functions.In particular, the balance between exploration and exploitation delivered by the MFEI allows for superior accelerations and contained demand for computational resources.Similar results can be observed for the Paciorek benchmark problem (Figure 14(c) and Figure 14(f)): the multifidelity learning delivers efficient optimization procedures even in the simultaneous presence of multi-modality and noise.It should be noticed that in presence of noise both the MES and MFMES show an attenuation of the exploratory behaviour and a greater exploitation of the domain.This result is in agreement with what observed by Nguyen et al. [147].The overall outcomes for this subset of benchmark functions demonstrate that a learning scheme characterized by a balanced exploration and exploitation phase is essential in presence of multimodal behaviour and noise in the measurements of the objective function.</p>
<p>Advice on using Learning Criteria</p>
<p>Throughout the experiments in this paper and in our research experience, we can summarize several recommendations that are intended to provide a guideline to apply the different learning criteria in real-world optimization problems.Although these advice may not be suitable in general due to the vast and natural heterogeneity of the applications where optimization is relevant, we believe that these guidelines can be useful in directing researchers toward the effective use of learning schemes.</p>
<ol>
<li>
<p>Pure exploitation/informativeness learning schemes could be potentially beneficial for low-dimensional optimization problems.In our experience, the direct exploitation of data at the beginning of the optimization procedure can produce significant improvement in the solution with relatively contained computational resources.The reason behind this behaviour is due to the accurate prediction of the emulator with contained amount of data in low-dimensional domains.This contributes to better inform the learner and effectively direct resources toward the optimum.</p>
</li>
<li>
<p>Pure exploration/representativeness-diversity could impact considerably the optimization results for highdimensional optimization problems.The exploration reduces the uncertainty of the emulator over all the domain and leads to a more reliable predictive framework.This would better inform the learner and help directing the computational resources in regions of the domain where is more likely to achieve benefits in terms of solution.</p>
</li>
<li>
<p>The balance between exploration and exploitation guarantees consistent and satisfactory optimization performances over different mathematical properties of the objective function.In particular, our experiments suggest that pursuing the trade-off between exploration and exploitation often leads to satisfactory and in many cases better performance than implementing the learning criteria individually.Although the well performing behaviour in general, it should be privileged mainly in cases when there is no prior knowledge about the specific optimization problem considered to increase the chances of success.</p>
</li>
<li>
<p>When the computational resources are severely limited -e.g.engineering preliminary design phases or trade-off analysis -, there is a clear advantage of using multifidelity learning criteria and leverage a spectrum of information sources at different levels of fidelity.Indeed, the wise combination of fast low-fidelity data with expensive high-fidelity evaluations reduces the overall demand for computational resources, and shows more robust performance for challenging mathematical properties of the objective function such as local/global behaviours, non-linearities and discontinuities, multimodality, and noisy measurements.</p>
</li>
</ol>
<p>This paper proposes an original unified perspective of Bayesian optimization and active learning as adaptive sampling schemes guided by common learning principles toward a given optimization goal.Our arguments are based on the recognition of Bayesian optimization and active learning as goal-driven learning procedures characterized by the mutual information exchange between the learner and the surrogate model: the learner makes a decision based on the surrogate information to maximize the sampling utility with respect to the given goal, while the emulator is constantly updated through the results of this decision.Accordingly, we clarify and support our discussion through a general classification of adaptive sampling methodologies, and recognize Bayesian optimization as the logic intersection between active learning and adaptive sampling.This lays the foundations for the explicit formalization of the synergy between Bayesian optimization and active learning considering both a single information source and when a library of representations at different levels of fidelity is available to the learner.This unified perspective is based on the dualism between the active learning criteria of informativeness and representativeness/diversity, and the Bayesian infill criteria of exploration and exploitation as the driving elements to achieve the learning goal.To support our perspective, we reviewed and analysed popular formulations of the acquisition function for Bayesian optimization considering both single-fidelity and multifidelity settings.Accordingly, we formalize this synergy mapping the informativeness learning criterion with the exploitation infill criterion as driving components that direct the selection of samples toward the learning goal.Similarly, we formulate the substantial analogy between representativeness-diversity learning criterion and the exploration infill criterion as sampling policies that improve the awareness about the objective function over the domain.</p>
<p>Through stressfull analytical benchmark problems, the authors demonstrate the benefits of each learning/infill criteria over challenging mathematical properties of the objective function typically encountered in real-world applications.The results reveal that the balance between the learning/infill criteria ensures good performances and computational efficiency over all the benchmark problems.In addition, multifidelity learning schemes deliver significant accelerations of the learning procedure making them particularly attractive when the available computational resources are limited.The authors also include some advice and guidelines on the use of the different learning criteria based on the experimental results and their own experience in the field.</p>
<p>Figure 1 :
1
Figure 1: Citations of Bayesian Optimization (BO), Active Learning (AL), Adaptive Sampling (AS) and the three terms combined (BO+AL+AS).</p>
<p>Figure 2 :
2
Figure 2: Where adaptive sampling and active learning meet: this work focuses on the synergies between Bayesian optimization and active learning as goal-driven learning procedures driven by common learning principles.</p>
<p>Figure 3 :
3
Figure 3: Learning criteria: watering optimization problem.</p>
<p>Figure 4 :
4
Figure 4: Mapping methodologies to learning criteria.</p>
<p>2.2), and Max-Value Entropy Search (MES) (Section 5.2.3) for the singlefidelity frameworks, and their multifidelity counterparts Multifidelity Expected Improvement (MFEI) (Section 5.4.1),Multifidelity Probability of Improvement (MFPI) (Section 5.4.2) and Multifidelity Max-Value Entropy Search (MFMES) (Section 5.4.3).</p>
<p>Figure 6 :
6
Figure 6: Forrester function benchmark problems</p>
<p>Figure 7 :
7
Figure 7: Rosenbrock function benchmark problem over the D = 2 dimensional domain</p>
<p>For D = 1 ,
1
the analytical optimum is located at x * = 0.2755 corresponding to f * (2) = −0.6250while for D ≥ 2 the minimum is located at x * = [0, 0] D with value of the objective function f * (2) = −0.5627123.Figure 8 illustrates the high and low-fidelity ALOS function for D = 1 (Figure 8(a)) and D = 2 (Figure 8(b)).</p>
<p>2 Figure 8 :
28
Figure 8: ALOS function benchmark problems over the D = 1 and D = 2 dimensional domain</p>
<p>Figure 9 :
9
Figure 9: Rastrigin function shifted and rotated benchmark problem</p>
<p>Figure 10 :
10
Figure 10: Paciorek function benchmark problem</p>
<p>Figure 11 :
11
Figure 11: Performances of the competing algorithms for the Forrester and Jump Forrester benchmarks.</p>
<p>Figure 11
11
Figure11summarizes the outcomes obtained for the Forrester function and discontinuous Forrester function.The results for the Forrester benchmark (Figure11(a) and Figure11(c)) show that the multifidelity algorithms identify the optimum solution with a significant reduction of the computational budget if compared with the single fidelity counterparts.The best performing algorithm is the MFPI learner considering only the high-fidelity l = 4 and the lower-fidelity l = 1 levels, while the second best is the MFEI acquisition function considering available the complete spectrum of fidelities l = 1, 2, 3, 4.These outcomes suggest that multifidelity learning paradigms driven majorly by informativeness -MFPI acquisition function -are capable of efficiently directing the computational resources toward the optimum of low-dimensional objective functions in presence of continuous localized behaviours.Moreover, it should be noted that the MFEI capitalizes from all the information sources available and leverages the balance between informativeness and representativeness/diversity to effectively search toward the analytical optimum.The single fidelity Bayesian frameworks exhibit a lower convergence rate with respect to the multifidelity algorithms.The EI and PI use almost the same computational budget to identify the optimum solution, while the MES adopts more evaluations of the objective function.This confirms the observations for the multifidelity experiments.PI takes advantage from the purely exploitation of high-fidelity samples in the surrounding of the surrogate minimum to reach the optimum.This can be explained with the computation of an accurate surrogate model -at least close to the optimum -for low-dimensional objective functions.In contrast, EI balances an exploration phase to improve the overall accuracy of the surrogate with the exploitation toward the believed optimum.Particular attention should be dedicated to the MES and MFMES outcomes.In the single-fidelity frameworks, MES scores slightly worst both in terms of convergence rate and budget expenditure.This can be interpreted with an overall over-exploration behavior: MES distributes computational resources to explore the domain and refine the surrogate model, and directs lately efforts toward the optimum.This trend is considerably dampened in the multifidelity scenario, where MFMES shows good capabilities especially when all the sources of information are available during the search.In this case, cheap low-fidelity observations are used to explore the domain with contained computational expenditure, and high-fidelity data are mostly adopted to search toward the prescribed optimum location.</p>
<p>Figure 12 :
12
Figure 12: Performances of the competing algorithms for the Rosenbrock benchmarks.</p>
<p>Figure 13 :
13
Figure 13: Performances of the competing algorithms for the ALOS benchmarks.</p>
<p>confirm the previous observations about the different effectiveness of the learning schemes.In particular, the multifidelity strategies provide larger accelerations of the optimization procedure in presence of oscillations at different frequencies of the objective function for the one-(Figure 13(a) and Figure 13(d)), two-(Figure 13(b) and Figure 13(e)) and three-(Figure 13(c) and Figure 13(f)) dimensional ALOS problem.We observe that the best performances are delivered by either learners based on the balance between informativeness and representativeness/diversity -MFEI and EI -or a purely informativeness-driven -MFPI and PI -, while over-exploration performs relatively poorly -MFMES and MES.These results are justified with the low-dimensionality of the objective function.</p>
<p>Figure 14 :
14
Figure 14: Performances of the competing algorithms for the multimodal benchmarks.</p>
<p>Engineering design optimization. Rra Joaquim, Andrew Martins, Ning, 2021Cambridge University Press</p>
<p>Prognostics and health management of engineering systems. Nam-Ho Kim, Dawn An, Joo-Ho Choi, 2017Springer International PublishingSwitzerland</p>
<p>A comparison of three methods for selecting values of input variables in the analysis of output from a computer code. Richard J Michael D Mckay, William J Beckman, Conover, Technometrics. 4212000</p>
<p>Fractional factorial design. F Richard, Robert L Gunst, Mason, Wiley Interdisciplinary Reviews: Computational Statistics. 122009</p>
<p>Design and analysis of experiments. Montgomery Douglas, 2017John wiley &amp; sons</p>
<p>Conjoint measurement: Methods and applications. Anders Gustafsson, Andreas Herrmann, Frank Huber, 2013Springer Science &amp; Business Media</p>
<p>Response surface methodology: process and product optimization using designed experiments. Douglas C Raymond H Myers, Christine M Montgomery, Anderson-Cook, 2016John Wiley &amp; Sons</p>
<p>Sequential design of experiments. Herman Chernoff, The Annals of Mathematical Statistics. 3031959</p>
<p>On sequential sampling for global metamodeling in engineering design. Ruichen Jin, Wei Chen, Agus Sudjianto, International design engineering technical conferences and computers and information in engineering conference. 200236223</p>
<p>Efficient progressive sampling. Foster Provost, David Jensen, Tim Oates, Proceedings of the fifth ACM SIGKDD international conference on Knowledge discovery and data mining. the fifth ACM SIGKDD international conference on Knowledge discovery and data mining1999</p>
<p>Special section on multidisciplinary design optimization: metamodeling in multidisciplinary design optimization: how far have we really come?. Felipe Ac Viana, Timothy W Simpson, Vladimir Balabanov, Vasilli Toropov, AIAA journal. 5242014</p>
<p>A survey of adaptive sampling for global metamodeling in support of simulation-based complex engineering design. Structural and Multidisciplinary Optimization. Haitao Liu, Yew-Soon Ong, Jianfei Cai, 201857</p>
<p>Adaptive sampling approaches for surrogate-based optimization. Lisia Dias, Atharv Bhosekar, Mariathi Ierapetritou, Computer Aided Chemical Engineering. Elsevier201947</p>
<p>Taking the human out of the loop: A review of bayesian optimization. Bobak Shahriari, Kevin Swersky, Ziyu Wang, Ryan P Adams, Nando De Freitas, Proceedings of the IEEE. 10412015</p>
<p>Frazier Peter, arXiv:1807.02811A tutorial on bayesian optimization. 2018arXiv preprint</p>
<p>Analytical benchmark problems for multifidelity optimization methods. Mainini, Serani, Mp Rumpfkeil, Minisci, Quagliarella, Pehlivan, Yildiz, Ficini, F Pellegrini, Di Fiore, arXiv:2204.078672022arXiv preprint</p>
<p>Goal-driven learning. Ashwin Ram, David B Leake, 1995MIT press</p>
<p>Estimation of local modeling error and goal-oriented adaptive modeling of heterogeneous materials: I. error estimates and adaptive algorithms. Oden Tinsley, Kumar, Vemaganti, Journal of Computational Physics. 16412000</p>
<p>Goal-oriented, modelconstrained optimization for reduction of large-scale systems. Tan Bui-Thanh, Karen Willcox, Omar Ghattas, Bart Van Bloemen, Waanders, Journal of Computational Physics. 22422007</p>
<p>Goal-oriented inference: Approach, linear theory, and application to advection diffusion. siam REVIEW. Chad Lieberman, Karen Willcox, 201355</p>
<p>Francesco Di, Fiore , Laura Mainini, arXiv:2207.06325Non-myopic multifidelity bayesian optimization. 2022arXiv preprint</p>
<p>Raal: Resource aware active learning for multifidelity efficient optimization. Francesco Grassi, Giorgio Manganini, Michele Garraffa, Laura Mainini, AIAA Journal. 6162023</p>
<p>Pool-based active learning in approximate linear regression. Masashi Sugiyama, Shinichi Nakajima, Machine Learning. 200975</p>
<p>Pool-based sequential active learning for regression. Dongrui Wu, IEEE transactions on neural networks and learning systems. 201830</p>
<p>Monte carlo sampling methods. Alexander Shapiro, Handbooks in operations research and management science. 200310</p>
<p>Quasi-monte carlo sampling. Art B Owen, 2003Siggraph1Monte Carlo Ray Tracing</p>
<p>Adaptive importance sampling. Karamchandani, Bjerager, Cornell, Structural Safety and Reliability. ASCE1989</p>
<p>Adaptive importance sampling: The past, the present, and the future. Monica F Bugallo, Victor Elvira, Luca Martino, David Luengo, Joaquin Miguez, Petar M Djuric, IEEE Signal Processing Magazine. 3442017</p>
<p>Multifidelity importance sampling. Benjamin Peherstorfer, Tiangang Cui, Youssef Marzouk, Karen Willcox, Computer Methods in Applied Mechanics and Engineering. 3002016</p>
<p>On adaptive markov chain monte carlo algorithms. F Yves, Jeffrey S Atchadé, Rosenthal, Bernoulli. 1152005</p>
<p>Adaptive markov chain monte carlo: theory and methods. Bayesian time series models. Yves Atchade, Gersende Fort, Eric Moulines, Pierre Priouret, 20111</p>
<p>Adaptive importance sampling in signal processing. Luca Mónica F Bugallo, Jukka Martino, Corander, Digital Signal Processing. 472015</p>
<p>Abrupt motion tracking via intensively adaptive markov-chain monte carlo sampling. Xiuzhuang Zhou, Yao Lu, Jiwen Lu, Jie Zhou, IEEE Transactions on Image Processing. 2122011</p>
<p>A new reliability method for small failure probability problems by combining the adaptive importance sampling and surrogate models. Ning-Cong Xiao, Hongyou Zhan, Kai Yuan, Computer Methods in Applied Mechanics and Engineering. 3721133362020</p>
<p>An adaptive scheme for reliability-based global design optimization: A markov chain monte carlo approach. Ha Jensen, Jerez, Valdebenito, Mechanical Systems and Signal Processing. 2020143106836</p>
<p>Monte Carlo statistical methods. George Christian P Robert, George Casella, Casella, 1999Springer2</p>
<p>Weighted average importance sampling and defensive mixture distributions. Tim Hesterberg, Technometrics. 3721995</p>
<p>An adaptive population importance sampler: Learning from uncertainty. Luca Martino, Victor Elvira, David Luengo, Jukka Corander, IEEE Transactions on Signal Processing. 63162015</p>
<p>Efficient adaptive multiple importance sampling. Yousef El-Laham, Luca Martino, Elvira Víctor, Mónica F Bugallo, 2019 27th European Signal Processing Conference (EUSIPCO). IEEE2019</p>
<p>Local and dimension adaptive stochastic collocation for uncertainty quantification. D John, Stephen G Jakeman, Roberts, Sparse grids and applications. Springer2012</p>
<p>Adaptive multi-index collocation for uncertainty quantification and sensitivity analysis. John D Jakeman, Gianluca Michael S Eldred, Alex Geraci, Gorodetsky, International Journal for Numerical Methods in Engineering. 12162020</p>
<p>An adaptive wavelet stochastic collocation method for irregular solutions of partial differential equations with random input data. Max Gunzburger, Clayton G Webster, Guannan Zhang, Sparse Grids and Applications-Munich 2012. Springer2014</p>
<p>On the convergence of adaptive stochastic collocation for elliptic partial differential equations with affine diffusion. Martin Eigel, G Oliver, Bjorn Ernst, Lorenzo Sprungk, Tamellini, SIAM Journal on Numerical Analysis. 6022022</p>
<p>A stochastic collocation approach to bayesian inference in inverse problems. Youssef Marzouk, Dongbin Xiu, Communications in Computational Physics. 642009</p>
<p>An efficient bayesian inference approach to inverse problems based on an adaptive sparse grid collocation method. Xiang Ma, Nicholas Zabaras, Inverse Problems. 253350132009</p>
<p>Multi-index stochastic collocation for random pdes. Abdul-Lateef Haji-Ali, Fabio Nobile, Lorenzo Tamellini, Raúl Tempone, Computer Methods in Applied Mechanics and Engineering. 3062016</p>
<p>A fully adaptive multilevel stochastic collocation strategy for solving elliptic pdes with random data. Jens Lang, Robert Scheichl, David Silvester, Journal of Computational Physics. 4191096922020</p>
<p>Dimension-adaptive tensor-product quadrature. Thomas Gerstner, Michael Griebel, Computing. 712003</p>
<p>A posteriori error estimation for the stochastic collocation finite element method. Diane Guignard, Fabio Nobile, SIAM Journal on Numerical Analysis. 5652018</p>
<p>A comprehensive study of non-adaptive and residual-based adaptive sampling for physics-informed neural networks. Chenxi Wu, Min Zhu, Qinyang Tan, Yadhu Kartha, Lu Lu, Computer Methods in Applied Mechanics and Engineering. 4031156712023</p>
<p>Deepxde: A deep learning library for solving differential equations. Lu Lu, Xuhui Meng, Zhiping Mao, George Em Karniadakis, SIAM review. 6312021</p>
<p>Efficient training of physics-informed neural networks via importance sampling. Mohammad Amin Nabian, Rini , Jasmine Gladstone, Hadi Meidani, Computer-Aided Civil and Infrastructure Engineering. 3682021</p>
<p>Bayesian approach to global optimization: theory and applications. Jonas Mockus, 2012Springer Science &amp; Business Media37</p>
<p>Active learning literature survey. Burr Settles, TR-16482009Technical Report</p>
<p>A comparative survey: Benchmarking for pool-based active learning. Xueying Zhan, Huan Liu, Qing Li, Antoni B Chan, IJCAI. 2021</p>
<p>A new method of locating the maximum point of an arbitrary multipeak curve in the presence of noise. J Harold, Kushner, ASME. J. Basic Eng. 1964</p>
<p>Single-step bayesian search method for an extremum of functions of a single variable. Ag Zhilinskas, Cybernetics. 1111975</p>
<p>On bayesian methods for seeking the extremum. Jonas Močkus, Optimization Techniques IFIP Technical Conference. NovosibirskSpringerJuly 1-7, 1974. 1975</p>
<p>A global search method for optimizing nonlinear systems. Bruce E Stuckman, IEEE Transactions on Systems, Man, and Cybernetics. 1861988</p>
<p>Global r/sup d/optimization when probes are expensive: the grope algorithm. John F Elder, IEEE International Conference on Systems, Man, and Cybernetics. IEEE1992. 1992</p>
<p>Efficient global optimization of expensive black-box functions. Matthias Donald R Jones, William J Schonlau, Welch, Journal of Global optimization. 1341998</p>
<p>Needs and opportunities for uncertainty-based multidisciplinary design methods for aerospace vehicles. National Aeronautics and Space Administration. Zang Thomas, 2002Langley Research Center</p>
<p>A bayesian approach for shadow extraction from a single image. Tai-Pang Wu, Chi-Keung Tang, Tenth IEEE International Conference on Computer Vision (ICCV'05. 20051</p>
<p>A bayesian optimization algorithm for the nurse scheduling problem. Jingpeng Li, Uwe Aickelin, The 2003 Congress on Evolutionary Computation, 2003. CEC'03. IEEE20033</p>
<p>Bayesian optimization for materials design. Information science for materials discovery and design. I Peter, Jialei Frazier, Wang, 2016</p>
<p>Advances in bayesian optimization with applications in aerospace engineering. Rémi Lam, Matthias Poloczek, Peter Frazier, Karen E Willcox, AIAA Non-Deterministic Approaches Conference. 16562018. 2018</p>
<p>Energy management strategy for electric vehicles based on deep q-learning using bayesian optimization. Huifang Kong, Jiapeng Yan, Hai Wang, Lei Fan, Neural Computing and Applications. 322020</p>
<p>An efficient application of bayesian optimization to an industrial mdo framework for aircraft design. Remy Priem, Hugo Gagnon, Ian Chittick, Stephane Dufresne, Youssef Diouane, Nathalie Bartoli, AIAA Aviation 2020 Forum. 20203152</p>
<p>Bayesian optimization with safety constraints: safe and automatic parameter tuning in robotics. Felix Berkenkamp, Andreas Krause, Angela P Schoellig, Machine Learning. 2021</p>
<p>Efficient exploration of reward functions in inverse reinforcement learning via bayesian optimization. Sreejith Balakrishnan, Phong Quoc, Bryan Kian Hsiang Nguyen, Harold Low, Soh, Advances in Neural Information Processing Systems. 202033</p>
<p>Distributed bayesian optimization of deep reinforcement learning algorithms. Young Todd, Jacob D Hinkle, Ramakrishnan Kannan, Arvind Ramanathan, Journal of Parallel and Distributed Computing. 1392020</p>
<p>Joan Gonzalvez, Edmond Lezmi, Thierry Roncalli, Jiali Xu, arXiv:1903.04841Financial applications of gaussian processes and bayesian optimization. 2019arXiv preprint</p>
<p>Cryptocurrency price prediction with neural networks of lstm and bayesian optimization. Ehsan Sadeghi Pour, Hossein Jafari, Ali Lashgari, Elaheh Rabiee, Amin Ahmadisharaf, European Journal of Business and Management Research. 722022</p>
<p>Automatic tuning of hyperparameters using bayesian optimization. Helen Victoria, Ganesh Maragatham, Evolving Systems. 122021</p>
<p>Bayesian optimization is superior to random search for machine learning hyperparameter tuning: Analysis of the black-box optimization challenge. Ryan Turner, David Eriksson, Michael Mccourt, Juha Kiili, Eero Laaksonen, Zhen Xu, Isabelle Guyon, NeurIPS 2020 Competition and Demonstration Track. PMLR2020. 2021</p>
<p>Crowdsourcing interface feature design with bayesian optimization. John J Dudley, Jason T Jacques, Per Ola Kristensson, Proceedings of the 2019 chi conference on human factors in computing systems. the 2019 chi conference on human factors in computing systems2019</p>
<p>Sequential gallery for interactive visual design optimization. Yuki Koyama, Issei Sato, Masataka Goto, ACM Transactions on Graphics (TOG). 3942020</p>
<p>High dimensional bayesian optimisation and bandits via additive models. Kirthevasan Kandasamy, Jeff Schneider, Barnabás Póczos, International conference on machine learning. PMLR2015</p>
<p>Batched large-scale bayesian optimization in high-dimensional spaces. Zi Wang, Clement Gehring, Pushmeet Kohli, Stefanie Jegelka, International Conference on Artificial Intelligence and Statistics. PMLR2018</p>
<p>A framework for bayesian optimization in embedded subspaces. Amin Nayebi, Alexander Munteanu, Matthias Poloczek, International Conference on Machine Learning. PMLR2019</p>
<p>Bayesian optimization in a billion dimensions via random embeddings. Ziyu Wang, Frank Hutter, Masrour Zoghi, David Matheson, Nando De, Feitas , Journal of Artificial Intelligence Research. 552016</p>
<p>Parallel predictive entropy search for batch global optimization of expensive objective functions. Amar Shah, Zoubin Ghahramani, Advances in neural information processing systems. 282015</p>
<p>Parallel bayesian global optimization of expensive functions. Jialei Wang, Scott C Clark, Eric Liu, Peter I Frazier, Operations Research. 6862020</p>
<p>Scalable global optimization via local bayesian optimization. David Eriksson, Michael Pearce, Jacob Gardner, Ryan D Turner, Matthias Poloczek, Advances in neural information processing systems. 201932</p>
<p>Gaussian processes in machine learning. Carl Edward, Rasmussen , 2003SpringerSummer school on machine learning</p>
<p>Gaussian processes for global optimization. Roman Michael A Osborne, Stephen J Garnett, Roberts, 3rd international conference on learning and intelligent optimization (LION3). Citeseer2009</p>
<p>A new method of locating the maximum point of an arbitrary multipeak curve in the presence of noise. J Harold, Kushner, ASME. J. Basic Eng. 1964</p>
<p>Entropy search for information-efficient global optimization. Philipp Hennig, Christian J Schuler, Journal of Machine Learning Research. 1362012</p>
<p>Max-value entropy search for efficient bayesian optimization. Zi Wang, Stefanie Jegelka, International Conference on Machine Learning. PMLR2017</p>
<p>The correlated knowledge gradient for simulation optimization of continuous parameters using gaussian process regression. Warren Scott, Peter Frazier, Warren Powell, SIAM Journal on Optimization. 2132011</p>
<p>Lookahead bayesian optimization with inequality constraints. Remi Lam, Karen Willcox, Advances in Neural Information Processing Systems. 201730</p>
<p>Practical two-step lookahead bayesian optimization. Jian Wu, Peter Frazier, Advances in neural information processing systems. 201932</p>
<p>Multi-fidelity optimization via surrogate modelling. András Alexander Ij Forrester, Andy J Sóbester, Keane, Proceedings of the royal society a: mathematical, physical and engineering sciences. the royal society a: mathematical, physical and engineering sciences2088. 2007463</p>
<p>Survey of multifidelity methods in uncertainty propagation, inference, and optimization. Benjamin Peherstorfer, Karen Willcox, Max Gunzburger, Siam Review. 6032018</p>
<p>Comparison of multi-fidelity approaches for military vehicle design. Dean Philip S Beran, Bryson, Matteo Andrew S Thelen, Andrea Diez, Serani, AIAA AVIATION 2020 FORUM. 20203158</p>
<p>Issues in deciding whether to use multifidelity surrogates. Giselle Fernández-Godino, Chanyoung Park, Nam H Kim, Raphael T Haftka, Aiaa Journal. 5752019</p>
<p>Efficient surrogate model development: impact of sample size and underlying model dimensions. Sarah E Davis, Selen Cremaschi, Mario R Eden, Computer Aided Chemical Engineering. Elsevier201844</p>
<p>Multi-fidelity optimization of super-cavitating hydrofoils. Bonfiglio, Perdikaris, Brizzolara, Karniadakis, Computer Methods in Applied Mechanics and Engineering. 3322018</p>
<p>Multi-fidelity efficient global optimization: Methodology and application to airfoil shape design. Mostafa Meliani, Nathalie Bartoli, Thierry Lefebvre, Mohamed-Amine Bouhlel, Joaquim Rra Martins, Joseph Morlier, AIAA aviation 2019 forum. 20193236</p>
<p>Multifidelity do-main-aware learning for the design of re-entry vehicles. Structural and Multidisciplinary Optimization. Francesco Di Fiore, Paolo Maggiore, Laura Mainini, 202164</p>
<p>Nm-mf: Non-myopic multifidelity framework for constrained multiregime aerodynamic optimization. Francesco Di, Fiore , Laura Mainini, AIAA Journal. 6132023</p>
<p>Resistance and seakeeping optimization of a naval destroyer by multi-fidelity methods. Andrea Serani, Simone Ficini, Gregory Grigoropoulos, Chris Bakirtzogou, Riccardo Broglia, Matteo Diez, George Papadakis, Omer Goren, Thomas Devrim B Danisman, Scholcz, VCG, 1000. 2022</p>
<p>Practical multi-fidelity bayesian optimization for hyperparameter tuning. Jian Wu, Saul Toscano-Palmerin, Peter I Frazier, Andrew Gordon, Wilson , Uncertainty in Artificial Intelligence. PMLR2020</p>
<p>Multi-fidelity bayesian optimisation with continuous approximations. Kirthevasan Kandasamy, Gautam Dasarathy, Jeff Schneider, Barnabás Póczos, International Conference on Machine Learning. PMLR2017</p>
<p>Multi-objective and multi-fidelity bayesian optimization of laser-plasma acceleration. Faran Irshad, Stefan Karsch, Andreas Döpp, Physical Review Research. 51130632023</p>
<p>Multi-fidelity bayesian optimization to solve the inverse stefan problem. Winter, Abaidi, Kaiser, N A Adami, Adams, Computer Methods in Applied Mechanics and Engineering. 4101159462023</p>
<p>Model inversion via multi-fidelity bayesian optimization: a new paradigm for parameter estimation in haemodynamics, and beyond. Paris Perdikaris, George Em Karniadakis, Journal of The Royal Society Interface. 13118201511072016</p>
<p>Learning cardiac activation maps from 12-lead ecg with multi-fidelity bayesian optimization on manifolds. Simone Pezzuto, Paris Perdikaris, Francisco Sahli, Costabal , arXiv:2203.062222022arXiv preprint</p>
<p>High-dimensional multi-fidelity bayesian optimization for quantum control. Christian R Marjuka F Lazin, Simon N Shelton, Bryan M Sandhofer, Wong, Machine Learning: Science and Technology. 44450142023</p>
<p>Multifidelity and multiscale bayesian framework for high-dimensional engineering design and calibration. Soumalya Sarkar, Sudeepta Mondal, Michael Joly, Matthew E Lynch, Ranadip Shaunak D Bopardikar, Paris Acharya, Perdikaris, Journal of Mechanical Design. 141121210012019</p>
<p>Scalable inverse reinforcement learning through multifidelity bayesian optimization. Mahdi Imani, Seyede Fatemeh, Ghoreishi , IEEE transactions on neural networks and learning systems. 202133</p>
<p>Predicting the output from a complex computer code when fast approximations are available. C Marc, Anthony O' Kennedy, Hagan, Biometrika. 8712000</p>
<p>Variable-fidelity probability of improvement method for efficient global optimization of expensive black-box problems. Structural and Multidisciplinary Optimization. Xiongfeng Ruan, Ping Jiang, Qi Zhou, Jiexiang Hu, Leshi Shu, 202062</p>
<p>Sequential kriging optimization using multiple-fidelity evaluations. Structural and Multidisciplinary Optimization. Deng Huang, Theodore T Allen, William I Notz, Allen Miller, 200632</p>
<p>Information-based multi-fidelity bayesian optimization. Yehong Zhang, Trong Nghia Hoang, Bryan Kian Hsiang Low, Mohan Kankanhalli, NIPS Workshop on Bayesian Optimization. 2017</p>
<p>Multi-fidelity bayesian optimization with max-value entropy search and its parallelization. Shion Takeno, Hitoshi Fukuoka, Yuhki Tsukada, Toshiyuki Koyama, Motoki Shiga, Ichiro Takeuchi, Masayuki Karasuyama, International Conference on Machine Learning. PMLR2020</p>
<p>Query learning strategies using boosting and bagging. Naoki Abe, Proc. of 15ˆ&lt; th&gt; Int. Cmf. on Machine Learning (ICML98). of 15ˆ&lt; th&gt; Int. Cmf. on Machine Learning (ICML98)1998</p>
<p>Active learning for regression based on query by committee. Robert Burbidge, Jem J Rowland, Ross D King, International conference on intelligent data engineering and automated learning. Springer2007</p>
<p>Batch mode active learning for regression with expected model change. Wenbin Cai, Muhan Zhang, Ya Zhang, IEEE transactions on neural networks and learning systems. 201628</p>
<p>A multiple criteria active learning method for support vector regression. Beguem Demir, Lorenzo Bruzzone, Pattern recognition. 4772014</p>
<p>Minimisation of data collection by active learning. Tirthankar Raychaudhuri, Leonard, Hamey, Proceedings of ICNN'95-International Conference on Neural Networks. ICNN'95-International Conference on Neural NetworksIEEE19953</p>
<p>An analysis of active learning strategies for sequence labeling tasks. Burr Settles, Mark Craven, proceedings of the 2008 conference on empirical methods in natural language processing. the 2008 conference on empirical methods in natural language processing2008</p>
<p>Query by committee. Sebastian Seung, Manfred Opper, Haim Sompolinsky, Proceedings of the fifth annual workshop on Computational learning theory. the fifth annual workshop on Computational learning theory1992</p>
<p>An active learning approach with uncertainty, representativeness, and diversity. Tianxu He, Shukui Zhang, Jie Xin, Pengpeng Zhao, Jian Wu, Xuefeng Xian, Chunhua Li, Zhiming Cui, The Scientific World Journal. 2014. 2014</p>
<p>Multi-criteria-based active learning for named entity recognition. Dan Shen, Jie Zhang, Jian Su, Guodong Zhou, Chew Lim, Tan , Proceedings of the 42nd annual meeting of the Association for Computational Linguistics (ACL-04). the 42nd annual meeting of the Association for Computational Linguistics (ACL-04)2004</p>
<p>Offline eeg-based driver drowsiness estimation using enhanced batch-mode active learning (ebmal) for regression. Dongrui Wu, Stephen Vernon J Lawhern, Brent J Gordon, Chin-Teng Lance, Lin, 2016 IEEE International Conference on Systems, Man, and Cybernetics (SMC). IEEE2016</p>
<p>Human-in-the-Loop Machine Learning: Active learning and annotation for humancentered AI. Robert Munro, Monarch , 2021Simon and Schuster</p>
<p>Heterogeneous uncertainty sampling for supervised learning. D David, Jason Lewis, Catlett, Machine learning proceedings 1994. Elsevier1994</p>
<p>Margin based active learning. Maria-Florina Balcan, Andrei Broder, Tong Zhang, International Conference on Computational Learning Theory. Springer2007</p>
<p>Confidence-based active learning. Mingkun Li, Ishwar K Sethi, IEEE transactions on pattern analysis and machine intelligence. 200628</p>
<p>Entropy-based active learning for object recognition. Alex Holub, Pietro Perona, Michael C Burl, 2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops. IEEE2008</p>
<p>Neural network exploration using optimal experiment design. David Cohn, 19936Advances in neural information processing systems</p>
<p>Maximizing expected model change for active learning in regression. Wenbin Cai, Ya Zhang, Jun Zhou, 2013 IEEE 13th international conference on data mining. IEEE2013</p>
<p>Research on query-by-committee method of active learning and application. Yue Zhao, Ciwen Xu, Yongcun Cao, International Conference on Advanced Data Mining and Applications. Springer2006</p>
<p>Hierarchical sampling for active learning. Sanjoy Dasgupta, Daniel Hsu, Proceedings of the 25th international conference on Machine learning. the 25th international conference on Machine learning2008</p>
<p>A geometric approach to active learning for convolutional neural networks. Ozan Sener, Silvio Savarese, arXiv:1708.0048920177arXiv preprint</p>
<p>Batch mode active sampling based on marginal probability distribution matching. Rita Chattopadhyay, Zheng Wang, Wei Fan, Ian Davidson, Sethuraman Panchanathan, Jieping Ye, ACM Transactions on Knowledge Discovery from Data (TKDD). 732013</p>
<p>Active learning without knowing individual instance labels: a pairwise label homogeneity query approach. Yifan Fu, Bin Li, Xingquan Zhu, Chengqi Zhang, IEEE Transactions on Knowledge and Data Engineering. 2642013</p>
<p>Transductive active learning-a new semi-supervised learning approach based on iteratively refined generative models to capture structure in data. Tobias Reitmaier, Adrian Calma, Bernhard Sick, Information Sciences. 2932015</p>
<p>A novel active learning framework for classification: Using weighted rank aggregation to achieve multiple query criteria. Yu Zhao, Zhenhui Shi, Jingyang Zhang, Dong Chen, Lixu Gu, Pattern Recognition. 932019</p>
<p>Active learning by learning. Wei-Ning Hsu, Hsuan-Tien Lin, Twenty-Ninth AAAI conference on artificial intelligence. 2015</p>
<p>Adaptive active learning for image classification. Xin Li, Yuhong Guo, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2013</p>
<p>Querying discriminative and representative samples for batch mode active learning. Zheng Wang, Jieping Ye, ACM Transactions on Knowledge Discovery from Data (TKDD). 932015</p>
<p>Self-paced active learning: Query the right thing at the right time. Ying-Peng Tang, Sheng-Jun Huang, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence201933</p>
<p>On the bayes methods for seeking the extremal point. Mockus, IFAC Proceedings Volumes. 19758</p>
<p>A taxonomy of global optimization methods based on response surfaces. Donald R Jones, Journal of global optimization. 2142001</p>
<p>Rectified max-value entropy search for bayesian optimization. Phong Quoc, Bryan Kian Hsiang Nguyen, Patrick Low, Jaillet, arXiv:2202.135972022arXiv preprint</p>
<p>Incremental relabeling for active learning with noisy crowdsourced annotations. Liyue Zhao, Gita Sukthankar, Rahul Sukthankar, 2011 IEEE third international conference on privacy, security, risk and trust and 2011 IEEE third international conference on social computing. IEEE2011</p>
<p>Efficiently learning the accuracy of labeling sources for selective sampling. Pinar Donmez, Jaime G Carbonell, Jeff Schneider, Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining. the 15th ACM SIGKDD international conference on Knowledge discovery and data mining2009</p>
<p>Repeated labeling using multiple noisy labelers. Foster Panagiotis G Ipeirotis, Victor S Provost, Jing Sheng, Wang, Data Mining and Knowledge Discovery. 2822014</p>
<p>Active learning from crowds. Yan Yan, Romer Rosales, Glenn Fung, Jennifer G Dy, ICML. 2011</p>
<p>Active learning from multiple knowledge sources. Yan Yan, Rómer Rosales, Glenn Fung, Faisal Farooq, Bharat Rao, Jennifer Dy, Artificial Intelligence and Statistics. PMLR2012</p>
<p>Active learning for crowdsourcing using knowledge transfer. Meng Fang, Jie Yin, Dacheng Tao, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence201428</p>
<p>Cost-effective active learning from diverse labelers. Sheng-Jun Huang, Jia-Lve Chen, Xin Mu, Zhi-Hua Zhou, IJCAI. 2017</p>
<p>Cmal: Cost-effective multi-label active learning by querying subexamples. Guoxian Yu, Xia Chen, Carlotta Domeniconi, Jun Wang, Zhao Li, Zili Zhang, Xiangliang Zhang, IEEE Transactions on Knowledge and Data Engineering. 2020</p>
<p>Cost-accuracy aware adaptive labeling for active learning. Ruijiang Gao, Maytal Saar-Tsechansky, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202034</p>
<p>Sequential optimization using multi-level cokriging and extended expected improvement criterion. Structural and Multidisciplinary Optimization. Yixin Liu, Shishi Chen, Fenggang Wang, Fenfen Xiong, 201858</p>
<p>Latin hypercube sampling as a tool in uncertainty analysis of computer models. D Michael, Mckay, Proceedings of the 24th conference on Winter simulation. the 24th conference on Winter simulation1992</p>
<p>Engineering design via surrogate modelling: a practical guide. András Sobester, Alexander Forrester, Andy Keane, 2008John Wiley &amp; Sons</p>
<p>Parameter selection in synchronous and asynchronous deterministic particle swarm optimization for ship hydrodynamics problems. Andrea Serani, Cecilia Leotardi, Umberto Iemma, Emilio F Campana, Giovanni Fasano, Matteo Diez, Applied Soft Computing. 492016</p>            </div>
        </div>

    </div>
</body>
</html>
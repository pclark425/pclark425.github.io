<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-993 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-993</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-993</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-20.html">extraction-schema-20</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <p><strong>Paper ID:</strong> paper-0da0af5ed59661fe4f901cf330a851e95eac7ea4</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/0da0af5ed59661fe4f901cf330a851e95eac7ea4" target="_blank">Differentiable Causal Discovery from Interventional Data</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> This work proposes a neural network-based method for discovering causal relationships in data that can leverage interventional data and illustrates the flexibility of the continuous-constrained framework by taking advantage of expressive neural architectures such as normalizing flows.</p>
                <p><strong>Paper Abstract:</strong> Discovering causal relationships in data is a challenging task that involves solving a combinatorial problem for which the solution is not always identifiable. A new line of work reformulates the combinatorial problem as a continuous constrained optimization one, enabling the use of different powerful optimization techniques. However, methods based on this idea do not yet make use of interventional data, which can significantly alleviate identifiability issues. In this work, we propose a neural network-based method for this task that can leverage interventional data. We illustrate the flexibility of the continuous-constrained framework by taking advantage of expressive neural architectures such as normalizing flows. We show that our approach compares favorably to the state of the art in a variety of settings, including perfect and imperfect interventions for which the targeted nodes may even be unknown.</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e993.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e993.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DCDI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Differentiable Causal Discovery with Interventions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A differentiable, continuous-constrained causal structure learning method that leverages interventional data (perfect, imperfect, and unknown-target interventions) and expressive density estimators (e.g., normalizing flows) to recover the I-Markov equivalence class of the true DAG.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>DCDI (Differentiable Causal Discovery with Interventions)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Encodes a distribution over DAGs via a Bernoulli-masked adjacency matrix parameterized by sigmoid logits and optimizes a regularized maximum-likelihood score (likelihood across observational and interventional distributions minus an edge-count penalty) under an acyclicity constraint (Tr exp(A)-d=0) using an augmented Lagrangian procedure. Conditional densities are modeled by neural networks (optionally outputting parameters for normalizing flows). For unknown intervention targets, DCDI jointly learns an intervention-target mask (R) with an additional sparsity regularizer. Gradients through discrete masks are estimated via Straight-Through Gumbel / Gumbel-Softmax, and the optimization yields a point estimate by thresholding the learned mask probabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Synthetic interventional datasets and a real flow-cytometry dataset</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>The method is evaluated on synthetic data sets with multiple interventional contexts (observational + K interventional datasets) covering perfect and imperfect interventions, known and unknown targets; also applied to a real flow-cytometry dataset. The environment is not an active/interactive virtual lab (interventions are provided as datasets rather than selected online by the algorithm).</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Addresses spurious correlations that vary across environments by exploiting invariance of true conditionals across interventions; does NOT handle hidden confounders (assumes causal sufficiency).</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Implicit detection via enforcing conditional invariance across interventions: conditionals that must remain the same across environments (for non-targeted nodes) are modelled jointly, so mismatches indicate model misspecification/spurious associations.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>L1-type regularization on expected adjacency (||sigma(Lambda)||_1) to penalize extra edges; additional sparsity regularizer on learned intervention mask (lambda_R) to avoid spurious intervention attributions; mask probabilities allow downweighting edges during optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Score-based refutation: graphs implying invariances that contradict observed interventional distributions incur positive KL divergences to the data and thus lower regularized likelihood; theoretical identification results show wrong graphs score worse asymptotically under assumptions.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Across multiple synthetic settings (perfect/imperfect, known/unknown targets; linear, ANM, nonlinear NN mechanisms) DCDI-G (Gaussian) and DCDI-DSF (normalizing flows) achieve the best overall performance (lower SHD and SID) relative to baselines, especially for denser graphs; DCDI-DSF improves recovery in cases where simple densities lack capacity (paper reports consistent wins in Figures 2-4 and appendices).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Leveraging interventional data and enforcing invariance of non-targeted conditionals yields improved identifiability; jointly learning unknown intervention targets is possible via a sparsity-regularized mask; expressive density models (normalizing flows) can further improve recovery when mechanisms are complex; method assumes causal sufficiency and does not address hidden confounders.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Differentiable Causal Discovery from Interventional Data', 'publication_date_yy_mm': '2020-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e993.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e993.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ICP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Invariant Causal Prediction (ICP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A constraint/invariance-based causal discovery approach that finds subsets of predictors whose conditional distribution of the target is invariant across environments; used to identify direct causal parents and reject spurious predictors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Causal inference by using invariant prediction: identification and confidence intervals</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Invariant Causal Prediction (ICP)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Tests for invariance of the conditional distribution of a candidate target variable given a subset of predictors across multiple environments/contexts; variables failing invariance tests are rejected as non-causal. Typically implemented via hypothesis tests of equality of conditional distributions (or equality of residual distributions) across environments and yields confidence intervals for causal parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Multi-environment / interventional datasets (multiple contexts)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Designed for settings with multiple environments or interventions (observational and experimental datasets) where invariance of true causal conditionals can be tested; not inherently an interactive/active selection method though it can be applied to data from virtual labs.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Detects and rejects spurious predictors by testing conditional invariance across environments (variable selection via invariance tests).</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Non-causal predictors whose associations change across environments (spurious correlations due to environment-specific mechanisms); can give some robustness to certain types of distribution shifts; does not directly model hidden confounders unless extended.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Statistical hypothesis testing for equality of conditionals (or residual distributions) across environments; invariance violations flag spurious predictors.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Not a downweighting scheme per se — variables failing invariance tests are excluded from the causal parent set (hard rejection).</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Refutes candidate causal parents by statistically rejecting invariance hypotheses (confidence intervals and hypothesis tests).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Mentioned as an example of exploiting invariance across interventional distributions to avoid spurious associations; the authors note their own score also accounts for such invariances.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Differentiable Causal Discovery from Interventional Data', 'publication_date_yy_mm': '2020-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e993.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e993.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>JCI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Joint Causal Inference (JCI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework that encodes context (intervention) variables together with system variables to perform causal discovery across multiple contexts, supporting unknown targets and latent confounders when combined with suitable algorithms.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Joint causal inference from multiple contexts</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Joint Causal Inference (JCI)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Augments the variable set with context (intervention) variables and applies standard causal discovery / constraint-based methods (e.g., PC/FCI) to the augmented system; depending on modeling choices, JCI can handle unknown targets and latent confounders by treating contexts as observed variables that encode experimental regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Multiple contexts / interventional datasets</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Applicable to datasets collected under multiple contexts (observational and interventional); the framework is agnostic to whether contexts come from virtual labs or real experiments and does not itself actively plan experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Can handle latent confounders and unknown targets when used with algorithms designed for latent confounding (e.g., FCI) by modeling contexts jointly and exploiting conditional independencies across contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Latent confounding, unknown intervention targets, and environment-specific associations that can be disentangled via context variables.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Constraint-based conditional independence testing on the augmented graph including context variables; inconsistencies with context-conditioned independences reveal spurious links.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Uses conditional independence constraints and context-graph structure to rule out edges inconsistent with observed independences across contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as a framework that supports latent confounders and unknown intervention targets; highlighted as related work but not used in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Differentiable Causal Discovery from Interventional Data', 'publication_date_yy_mm': '2020-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e993.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e993.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>COmbINE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>COmbINE</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A constraint-based causal discovery method that uses Boolean satisfiability to combine constraints from multiple interventions (possibly overlapping variable sets) and can account for latent confounders.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Constraint-based causal discovery from multiple interventions over overlapping variable sets</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>COmbINE</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Formulates interventional conditional independence constraints across multiple (possibly overlapping) intervention datasets and uses SAT/Boolean solvers to find graphs that satisfy these constraints; explicitly models latent confounders in the constraint formulation.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Multiple interventional datasets with overlapping variable sets</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Designed for settings with several experiments/interventions that might affect different subsets of variables; not an active-selection algorithm — it consumes available datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Accounts for latent confounders in the constraint model, thereby handling certain spurious associations arising from hidden common causes.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Hidden confounding and spurious independencies arising from overlapping experimental designs.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Integrates independence constraints from multiple datasets and uses SAT solving to infer graph structures compatible with all constraints; incompatible edges are rejected.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Eliminates candidate edges that contradict aggregated independence/constraint specifications across interventions.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Mentioned as a constraint-based method supporting latent confounders when learning from multiple interventions; presented as contrasting approach to DCDI (which assumes causal sufficiency).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Differentiable Causal Discovery from Interventional Data', 'publication_date_yy_mm': '2020-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e993.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e993.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HEJ</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>HEJ (Hyttinen–Eberhardt–Järvisalo)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A constraint-based causal discovery approach using answer set programming (ASP) to resolve conflicts and learn causal graphs, and that can account for latent confounders.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Constraint-based causal discovery: Conflict resolution with answer set programming</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>HEJ (ASP-based constraint solver)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Converts conditional independence constraints into logical constraints and uses answer set programming to search for causal graphs that satisfy the constraints, enabling enumeration of compatible causal structures and the handling of latent confounders in the constraint representation.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Multiple interventional/observational datasets for constraint-based discovery</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Works with independence constraints from datasets collected under different interventions or contexts; not an interactive experimental design method.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Models latent confounders in the constraint encoding and resolves conflicts among constraints via logical solving, which helps avoid spurious edges due to confounding.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Hidden confounders and conflicting independence constraints across contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Logical/constraint-based conflict detection and resolution via ASP; constraints inconsistent with observed independences remove candidate edges.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Constraints and ASP search rule out graphs that imply independences inconsistent with data across contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Mentioned as an alternative constraint-based approach that can account for latent confounders; contrasted with the DCDI approach which assumes no hidden confounders.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Differentiable Causal Discovery from Interventional Data', 'publication_date_yy_mm': '2020-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e993.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e993.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IGSP / UT-IGSP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>IGSP and UT-IGSP (Interventional Greedy SP and Unknown-Target IGSP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Permutation-based (hybrid) causal discovery algorithms that use interventions to guide search; IGSP is consistent under faithfulness and UT-IGSP extends IGSP to unknown intervention targets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Permutation-based causal inference algorithms with interventions</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>IGSP / UT-IGSP</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>IGSP searches over permutations (variable orderings) and uses interventional information to greedily orient edges and optimize a score informed by conditional independence tests; UT-IGSP extends IGSP to handle unknown intervention targets by integrating inference over possible targets during the permutation search.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Interventional datasets (known and unknown targets)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Applied to batches of datasets corresponding to observational and interventional regimes; UT-IGSP targets settings where intervention targets are hidden. These are not interactive/online active-experiment selection methods in the experiments reported.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Accounts for spurious edge orientations by leveraging interventional data; does not explicitly handle hidden confounders (standard IGSP assumes causal sufficiency).</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Uses conditional independence testing adapted to each data type (partial correlation for linear Gaussian, KCI test for nonlinear) as part of permutation-based search; incorrect invariances reduce score and are avoided.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Rejects edge orientations that are inconsistent with conditional independences and interventional effects during the greedy permutation search.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Compared empirically: UT-IGSP used as baseline for unknown-target perfect intervention experiments; DCDI generally outperforms UT-IGSP except in some sparse linear cases (see Figure 4).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>UT-IGSP supports unknown-target interventions and is a strong baseline; DCDI typically outperforms UT-IGSP in experiments reported (especially for nonlinear or denser graphs).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Differentiable Causal Discovery from Interventional Data', 'publication_date_yy_mm': '2020-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e993.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e993.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Ke et al.</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Learning neural causal models from unknown interventions (Ke et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neural method for causal discovery from interventions gathered sequentially (online) that attempts to identify single-node interventions and learns models with a regularizer inspired by acyclicity constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning neural causal models from unknown interventions</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Ke et al. (neural causal learning with unknown interventions)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Learns neural causal models from interventions in an online/sequential setting, using random masking to encode graph structure and a regularizer inspired by continuous acyclicity constraints; identifies single-node interventions as hard predictions and estimates gradients with the score/likelihood log-trick (high variance).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Sequential/online interventional settings (single-node interventions)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Designed for interventions obtained sequentially; focuses on identifying the single intervened node per experiment and learning causal structure from streaming interventional data rather than batch interventional datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Targets spurious signals arising from unknown/ambiguous intervention attribution by explicitly modeling intervention targets, but does not address latent confounders.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Attempts to identify the single intervened node per experiment as a hard prediction; gradient estimation via log-trick for mask parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Hard assignment of intervention targets and regularization during learning to avoid spurious edges; no invariance-based refutation framework as in DCDI.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Mentioned as related work that also supports unknown interventions but in an online setting; contrasted with DCDI which learns distributions over targets (soft) and uses reparameterization gradients (lower variance).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Differentiable Causal Discovery from Interventional Data', 'publication_date_yy_mm': '2020-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e993.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e993.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NOTEARS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DAGs with NO TEARS (NOTEARS)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A continuous optimization approach for DAG learning that encodes acyclicity via a differentiable constraint (Tr exp(A)-d=0) enabling gradient-based optimization for structure learning in linear models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Dags with no tears: Continuous optimization for structure learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>NOTEARS</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Formulates structure learning as a continuous constrained optimization: minimize reconstruction error with an L1 sparsity penalty subject to an acyclicity constraint (Tr exp(A)-d=0). Originally proposed for linear Gaussian models where weights W parameterize both the structural equations and the adjacency representation (A = W ⊙ W). Solved by augmented Lagrangian and gradient-based optimizers.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Observational data (original); continuous-constrained framework applicable to interventional settings with extensions</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Originally designed for batch observational datasets and linear models; forms the basis of many continuous-constrained extensions to nonlinear and interventional settings but does not itself use interventions in the original formulation.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Does not explicitly handle distractors/hidden confounders; identifiability limitations under observational data are noted.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>L1 sparsity regularization on edge weights to discourage spurious edges.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Serves as the foundational continuous-constrained framework that DCDI extends to interventional data and expressive nonlinear density models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Differentiable Causal Discovery from Interventional Data', 'publication_date_yy_mm': '2020-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e993.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e993.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GIES</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Greedy Interventional Equivalence Search (GIES)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A greedy score-based method for learning DAGs from interventional data that optimizes BIC across interventional Markov equivalence classes, typically under linear-Gaussian assumptions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Characterization and greedy learning of interventional markov equivalence classes of directed acyclic graphs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>GIES</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Adaptation of the GES greedy search algorithm to interventional datasets; assumes linear Gaussian models and optimizes BIC across I-Markov equivalence classes using greedy edge insertions/deletions to maximize score.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Interventional datasets with known targets (batch)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Operates on batches of observational and interventional datasets, assuming linear-Gaussian mechanisms; not designed for interactive experiment selection.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Can be misled by model misspecification or nonlinearity; does not explicitly handle hidden confounders.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>BIC penalty for model complexity helps avoid spurious edges via model selection.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Often performs best on linear Gaussian synthetic data (when its model assumptions hold), but is outperformed by DCDI in nonlinear or denser graph settings.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Used as a baseline in experiments; strong when assumptions match data (linear Gaussian) but less flexible than DCDI for nonlinear or complex densities.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Differentiable Causal Discovery from Interventional Data', 'publication_date_yy_mm': '2020-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e993.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e993.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CAM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Causal Additive Models (CAM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A score-based method for nonlinear causal discovery that assumes additive noise models and conducts greedy order search with penalized regression.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Cam: Causal additive models, high-dimensional order search and penalized regression</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>CAM</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Assumes additive noise structural equations where each variable is a (nonlinear) function of its parents plus additive noise; performs order search and penalized regression to identify functional relationships and parent sets, with greedy optimization of a score based on penalized likelihood.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Observational (and adapted for interventions in code) datasets</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Originally developed for observational datasets under additive noise assumptions; authors use a modified CAM supporting interventions in comparisons, but CAM requires the additive noise structural assumption.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Targets spurious edges via model-based penalized regression and ordering search but assumes additive noise identifiability.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Penalized regression (sparsity-inducing penalties) to discourage spurious predictors.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Included as a baseline; performs well when data follow additive noise models but is less flexible than DCDI for general nonlinear/noise forms.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>CAM is a competitive baseline under its assumptions but lacks DCDI's ability to flexibly model general conditional densities and to jointly learn unknown intervention targets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Differentiable Causal Discovery from Interventional Data', 'publication_date_yy_mm': '2020-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Causal inference by using invariant prediction: identification and confidence intervals <em>(Rating: 2)</em></li>
                <li>Invariant causal prediction for nonlinear models <em>(Rating: 2)</em></li>
                <li>Joint causal inference from multiple contexts <em>(Rating: 2)</em></li>
                <li>Constraint-based causal discovery from multiple interventions over overlapping variable sets <em>(Rating: 2)</em></li>
                <li>Constraint-based causal discovery: Conflict resolution with answer set programming <em>(Rating: 2)</em></li>
                <li>Permutation-based causal inference algorithms with interventions <em>(Rating: 2)</em></li>
                <li>Permutation-based causal structure learning with unknown intervention targets <em>(Rating: 2)</em></li>
                <li>Learning neural causal models from unknown interventions <em>(Rating: 2)</em></li>
                <li>Dags with no tears: Continuous optimization for structure learning <em>(Rating: 2)</em></li>
                <li>A meta-transfer objective for learning to disentangle causal mechanisms <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-993",
    "paper_id": "paper-0da0af5ed59661fe4f901cf330a851e95eac7ea4",
    "extraction_schema_id": "extraction-schema-20",
    "extracted_data": [
        {
            "name_short": "DCDI",
            "name_full": "Differentiable Causal Discovery with Interventions",
            "brief_description": "A differentiable, continuous-constrained causal structure learning method that leverages interventional data (perfect, imperfect, and unknown-target interventions) and expressive density estimators (e.g., normalizing flows) to recover the I-Markov equivalence class of the true DAG.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "DCDI (Differentiable Causal Discovery with Interventions)",
            "method_description": "Encodes a distribution over DAGs via a Bernoulli-masked adjacency matrix parameterized by sigmoid logits and optimizes a regularized maximum-likelihood score (likelihood across observational and interventional distributions minus an edge-count penalty) under an acyclicity constraint (Tr exp(A)-d=0) using an augmented Lagrangian procedure. Conditional densities are modeled by neural networks (optionally outputting parameters for normalizing flows). For unknown intervention targets, DCDI jointly learns an intervention-target mask (R) with an additional sparsity regularizer. Gradients through discrete masks are estimated via Straight-Through Gumbel / Gumbel-Softmax, and the optimization yields a point estimate by thresholding the learned mask probabilities.",
            "environment_name": "Synthetic interventional datasets and a real flow-cytometry dataset",
            "environment_description": "The method is evaluated on synthetic data sets with multiple interventional contexts (observational + K interventional datasets) covering perfect and imperfect interventions, known and unknown targets; also applied to a real flow-cytometry dataset. The environment is not an active/interactive virtual lab (interventions are provided as datasets rather than selected online by the algorithm).",
            "handles_distractors": false,
            "distractor_handling_technique": null,
            "spurious_signal_types": "Addresses spurious correlations that vary across environments by exploiting invariance of true conditionals across interventions; does NOT handle hidden confounders (assumes causal sufficiency).",
            "detection_method": "Implicit detection via enforcing conditional invariance across interventions: conditionals that must remain the same across environments (for non-targeted nodes) are modelled jointly, so mismatches indicate model misspecification/spurious associations.",
            "downweighting_method": "L1-type regularization on expected adjacency (||sigma(Lambda)||_1) to penalize extra edges; additional sparsity regularizer on learned intervention mask (lambda_R) to avoid spurious intervention attributions; mask probabilities allow downweighting edges during optimization.",
            "refutation_method": "Score-based refutation: graphs implying invariances that contradict observed interventional distributions incur positive KL divergences to the data and thus lower regularized likelihood; theoretical identification results show wrong graphs score worse asymptotically under assumptions.",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "Across multiple synthetic settings (perfect/imperfect, known/unknown targets; linear, ANM, nonlinear NN mechanisms) DCDI-G (Gaussian) and DCDI-DSF (normalizing flows) achieve the best overall performance (lower SHD and SID) relative to baselines, especially for denser graphs; DCDI-DSF improves recovery in cases where simple densities lack capacity (paper reports consistent wins in Figures 2-4 and appendices).",
            "performance_without_robustness": null,
            "has_ablation_study": true,
            "number_of_distractors": null,
            "key_findings": "Leveraging interventional data and enforcing invariance of non-targeted conditionals yields improved identifiability; jointly learning unknown intervention targets is possible via a sparsity-regularized mask; expressive density models (normalizing flows) can further improve recovery when mechanisms are complex; method assumes causal sufficiency and does not address hidden confounders.",
            "uuid": "e993.0",
            "source_info": {
                "paper_title": "Differentiable Causal Discovery from Interventional Data",
                "publication_date_yy_mm": "2020-07"
            }
        },
        {
            "name_short": "ICP",
            "name_full": "Invariant Causal Prediction (ICP)",
            "brief_description": "A constraint/invariance-based causal discovery approach that finds subsets of predictors whose conditional distribution of the target is invariant across environments; used to identify direct causal parents and reject spurious predictors.",
            "citation_title": "Causal inference by using invariant prediction: identification and confidence intervals",
            "mention_or_use": "mention",
            "method_name": "Invariant Causal Prediction (ICP)",
            "method_description": "Tests for invariance of the conditional distribution of a candidate target variable given a subset of predictors across multiple environments/contexts; variables failing invariance tests are rejected as non-causal. Typically implemented via hypothesis tests of equality of conditional distributions (or equality of residual distributions) across environments and yields confidence intervals for causal parameters.",
            "environment_name": "Multi-environment / interventional datasets (multiple contexts)",
            "environment_description": "Designed for settings with multiple environments or interventions (observational and experimental datasets) where invariance of true causal conditionals can be tested; not inherently an interactive/active selection method though it can be applied to data from virtual labs.",
            "handles_distractors": true,
            "distractor_handling_technique": "Detects and rejects spurious predictors by testing conditional invariance across environments (variable selection via invariance tests).",
            "spurious_signal_types": "Non-causal predictors whose associations change across environments (spurious correlations due to environment-specific mechanisms); can give some robustness to certain types of distribution shifts; does not directly model hidden confounders unless extended.",
            "detection_method": "Statistical hypothesis testing for equality of conditionals (or residual distributions) across environments; invariance violations flag spurious predictors.",
            "downweighting_method": "Not a downweighting scheme per se — variables failing invariance tests are excluded from the causal parent set (hard rejection).",
            "refutation_method": "Refutes candidate causal parents by statistically rejecting invariance hypotheses (confidence intervals and hypothesis tests).",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "Mentioned as an example of exploiting invariance across interventional distributions to avoid spurious associations; the authors note their own score also accounts for such invariances.",
            "uuid": "e993.1",
            "source_info": {
                "paper_title": "Differentiable Causal Discovery from Interventional Data",
                "publication_date_yy_mm": "2020-07"
            }
        },
        {
            "name_short": "JCI",
            "name_full": "Joint Causal Inference (JCI)",
            "brief_description": "A framework that encodes context (intervention) variables together with system variables to perform causal discovery across multiple contexts, supporting unknown targets and latent confounders when combined with suitable algorithms.",
            "citation_title": "Joint causal inference from multiple contexts",
            "mention_or_use": "mention",
            "method_name": "Joint Causal Inference (JCI)",
            "method_description": "Augments the variable set with context (intervention) variables and applies standard causal discovery / constraint-based methods (e.g., PC/FCI) to the augmented system; depending on modeling choices, JCI can handle unknown targets and latent confounders by treating contexts as observed variables that encode experimental regimes.",
            "environment_name": "Multiple contexts / interventional datasets",
            "environment_description": "Applicable to datasets collected under multiple contexts (observational and interventional); the framework is agnostic to whether contexts come from virtual labs or real experiments and does not itself actively plan experiments.",
            "handles_distractors": true,
            "distractor_handling_technique": "Can handle latent confounders and unknown targets when used with algorithms designed for latent confounding (e.g., FCI) by modeling contexts jointly and exploiting conditional independencies across contexts.",
            "spurious_signal_types": "Latent confounding, unknown intervention targets, and environment-specific associations that can be disentangled via context variables.",
            "detection_method": "Constraint-based conditional independence testing on the augmented graph including context variables; inconsistencies with context-conditioned independences reveal spurious links.",
            "downweighting_method": null,
            "refutation_method": "Uses conditional independence constraints and context-graph structure to rule out edges inconsistent with observed independences across contexts.",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "Cited as a framework that supports latent confounders and unknown intervention targets; highlighted as related work but not used in experiments.",
            "uuid": "e993.2",
            "source_info": {
                "paper_title": "Differentiable Causal Discovery from Interventional Data",
                "publication_date_yy_mm": "2020-07"
            }
        },
        {
            "name_short": "COmbINE",
            "name_full": "COmbINE",
            "brief_description": "A constraint-based causal discovery method that uses Boolean satisfiability to combine constraints from multiple interventions (possibly overlapping variable sets) and can account for latent confounders.",
            "citation_title": "Constraint-based causal discovery from multiple interventions over overlapping variable sets",
            "mention_or_use": "mention",
            "method_name": "COmbINE",
            "method_description": "Formulates interventional conditional independence constraints across multiple (possibly overlapping) intervention datasets and uses SAT/Boolean solvers to find graphs that satisfy these constraints; explicitly models latent confounders in the constraint formulation.",
            "environment_name": "Multiple interventional datasets with overlapping variable sets",
            "environment_description": "Designed for settings with several experiments/interventions that might affect different subsets of variables; not an active-selection algorithm — it consumes available datasets.",
            "handles_distractors": true,
            "distractor_handling_technique": "Accounts for latent confounders in the constraint model, thereby handling certain spurious associations arising from hidden common causes.",
            "spurious_signal_types": "Hidden confounding and spurious independencies arising from overlapping experimental designs.",
            "detection_method": "Integrates independence constraints from multiple datasets and uses SAT solving to infer graph structures compatible with all constraints; incompatible edges are rejected.",
            "downweighting_method": null,
            "refutation_method": "Eliminates candidate edges that contradict aggregated independence/constraint specifications across interventions.",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "Mentioned as a constraint-based method supporting latent confounders when learning from multiple interventions; presented as contrasting approach to DCDI (which assumes causal sufficiency).",
            "uuid": "e993.3",
            "source_info": {
                "paper_title": "Differentiable Causal Discovery from Interventional Data",
                "publication_date_yy_mm": "2020-07"
            }
        },
        {
            "name_short": "HEJ",
            "name_full": "HEJ (Hyttinen–Eberhardt–Järvisalo)",
            "brief_description": "A constraint-based causal discovery approach using answer set programming (ASP) to resolve conflicts and learn causal graphs, and that can account for latent confounders.",
            "citation_title": "Constraint-based causal discovery: Conflict resolution with answer set programming",
            "mention_or_use": "mention",
            "method_name": "HEJ (ASP-based constraint solver)",
            "method_description": "Converts conditional independence constraints into logical constraints and uses answer set programming to search for causal graphs that satisfy the constraints, enabling enumeration of compatible causal structures and the handling of latent confounders in the constraint representation.",
            "environment_name": "Multiple interventional/observational datasets for constraint-based discovery",
            "environment_description": "Works with independence constraints from datasets collected under different interventions or contexts; not an interactive experimental design method.",
            "handles_distractors": true,
            "distractor_handling_technique": "Models latent confounders in the constraint encoding and resolves conflicts among constraints via logical solving, which helps avoid spurious edges due to confounding.",
            "spurious_signal_types": "Hidden confounders and conflicting independence constraints across contexts.",
            "detection_method": "Logical/constraint-based conflict detection and resolution via ASP; constraints inconsistent with observed independences remove candidate edges.",
            "downweighting_method": null,
            "refutation_method": "Constraints and ASP search rule out graphs that imply independences inconsistent with data across contexts.",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "Mentioned as an alternative constraint-based approach that can account for latent confounders; contrasted with the DCDI approach which assumes no hidden confounders.",
            "uuid": "e993.4",
            "source_info": {
                "paper_title": "Differentiable Causal Discovery from Interventional Data",
                "publication_date_yy_mm": "2020-07"
            }
        },
        {
            "name_short": "IGSP / UT-IGSP",
            "name_full": "IGSP and UT-IGSP (Interventional Greedy SP and Unknown-Target IGSP)",
            "brief_description": "Permutation-based (hybrid) causal discovery algorithms that use interventions to guide search; IGSP is consistent under faithfulness and UT-IGSP extends IGSP to unknown intervention targets.",
            "citation_title": "Permutation-based causal inference algorithms with interventions",
            "mention_or_use": "use",
            "method_name": "IGSP / UT-IGSP",
            "method_description": "IGSP searches over permutations (variable orderings) and uses interventional information to greedily orient edges and optimize a score informed by conditional independence tests; UT-IGSP extends IGSP to handle unknown intervention targets by integrating inference over possible targets during the permutation search.",
            "environment_name": "Interventional datasets (known and unknown targets)",
            "environment_description": "Applied to batches of datasets corresponding to observational and interventional regimes; UT-IGSP targets settings where intervention targets are hidden. These are not interactive/online active-experiment selection methods in the experiments reported.",
            "handles_distractors": false,
            "distractor_handling_technique": null,
            "spurious_signal_types": "Accounts for spurious edge orientations by leveraging interventional data; does not explicitly handle hidden confounders (standard IGSP assumes causal sufficiency).",
            "detection_method": "Uses conditional independence testing adapted to each data type (partial correlation for linear Gaussian, KCI test for nonlinear) as part of permutation-based search; incorrect invariances reduce score and are avoided.",
            "downweighting_method": null,
            "refutation_method": "Rejects edge orientations that are inconsistent with conditional independences and interventional effects during the greedy permutation search.",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "Compared empirically: UT-IGSP used as baseline for unknown-target perfect intervention experiments; DCDI generally outperforms UT-IGSP except in some sparse linear cases (see Figure 4).",
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "UT-IGSP supports unknown-target interventions and is a strong baseline; DCDI typically outperforms UT-IGSP in experiments reported (especially for nonlinear or denser graphs).",
            "uuid": "e993.5",
            "source_info": {
                "paper_title": "Differentiable Causal Discovery from Interventional Data",
                "publication_date_yy_mm": "2020-07"
            }
        },
        {
            "name_short": "Ke et al.",
            "name_full": "Learning neural causal models from unknown interventions (Ke et al.)",
            "brief_description": "A neural method for causal discovery from interventions gathered sequentially (online) that attempts to identify single-node interventions and learns models with a regularizer inspired by acyclicity constraints.",
            "citation_title": "Learning neural causal models from unknown interventions",
            "mention_or_use": "mention",
            "method_name": "Ke et al. (neural causal learning with unknown interventions)",
            "method_description": "Learns neural causal models from interventions in an online/sequential setting, using random masking to encode graph structure and a regularizer inspired by continuous acyclicity constraints; identifies single-node interventions as hard predictions and estimates gradients with the score/likelihood log-trick (high variance).",
            "environment_name": "Sequential/online interventional settings (single-node interventions)",
            "environment_description": "Designed for interventions obtained sequentially; focuses on identifying the single intervened node per experiment and learning causal structure from streaming interventional data rather than batch interventional datasets.",
            "handles_distractors": false,
            "distractor_handling_technique": null,
            "spurious_signal_types": "Targets spurious signals arising from unknown/ambiguous intervention attribution by explicitly modeling intervention targets, but does not address latent confounders.",
            "detection_method": "Attempts to identify the single intervened node per experiment as a hard prediction; gradient estimation via log-trick for mask parameters.",
            "downweighting_method": null,
            "refutation_method": "Hard assignment of intervention targets and regularization during learning to avoid spurious edges; no invariance-based refutation framework as in DCDI.",
            "uses_active_learning": null,
            "inquiry_strategy": null,
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "Mentioned as related work that also supports unknown interventions but in an online setting; contrasted with DCDI which learns distributions over targets (soft) and uses reparameterization gradients (lower variance).",
            "uuid": "e993.6",
            "source_info": {
                "paper_title": "Differentiable Causal Discovery from Interventional Data",
                "publication_date_yy_mm": "2020-07"
            }
        },
        {
            "name_short": "NOTEARS",
            "name_full": "DAGs with NO TEARS (NOTEARS)",
            "brief_description": "A continuous optimization approach for DAG learning that encodes acyclicity via a differentiable constraint (Tr exp(A)-d=0) enabling gradient-based optimization for structure learning in linear models.",
            "citation_title": "Dags with no tears: Continuous optimization for structure learning",
            "mention_or_use": "mention",
            "method_name": "NOTEARS",
            "method_description": "Formulates structure learning as a continuous constrained optimization: minimize reconstruction error with an L1 sparsity penalty subject to an acyclicity constraint (Tr exp(A)-d=0). Originally proposed for linear Gaussian models where weights W parameterize both the structural equations and the adjacency representation (A = W ⊙ W). Solved by augmented Lagrangian and gradient-based optimizers.",
            "environment_name": "Observational data (original); continuous-constrained framework applicable to interventional settings with extensions",
            "environment_description": "Originally designed for batch observational datasets and linear models; forms the basis of many continuous-constrained extensions to nonlinear and interventional settings but does not itself use interventions in the original formulation.",
            "handles_distractors": false,
            "distractor_handling_technique": null,
            "spurious_signal_types": "Does not explicitly handle distractors/hidden confounders; identifiability limitations under observational data are noted.",
            "detection_method": null,
            "downweighting_method": "L1 sparsity regularization on edge weights to discourage spurious edges.",
            "refutation_method": null,
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "Serves as the foundational continuous-constrained framework that DCDI extends to interventional data and expressive nonlinear density models.",
            "uuid": "e993.7",
            "source_info": {
                "paper_title": "Differentiable Causal Discovery from Interventional Data",
                "publication_date_yy_mm": "2020-07"
            }
        },
        {
            "name_short": "GIES",
            "name_full": "Greedy Interventional Equivalence Search (GIES)",
            "brief_description": "A greedy score-based method for learning DAGs from interventional data that optimizes BIC across interventional Markov equivalence classes, typically under linear-Gaussian assumptions.",
            "citation_title": "Characterization and greedy learning of interventional markov equivalence classes of directed acyclic graphs",
            "mention_or_use": "use",
            "method_name": "GIES",
            "method_description": "Adaptation of the GES greedy search algorithm to interventional datasets; assumes linear Gaussian models and optimizes BIC across I-Markov equivalence classes using greedy edge insertions/deletions to maximize score.",
            "environment_name": "Interventional datasets with known targets (batch)",
            "environment_description": "Operates on batches of observational and interventional datasets, assuming linear-Gaussian mechanisms; not designed for interactive experiment selection.",
            "handles_distractors": false,
            "distractor_handling_technique": null,
            "spurious_signal_types": "Can be misled by model misspecification or nonlinearity; does not explicitly handle hidden confounders.",
            "detection_method": null,
            "downweighting_method": "BIC penalty for model complexity helps avoid spurious edges via model selection.",
            "refutation_method": null,
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "Often performs best on linear Gaussian synthetic data (when its model assumptions hold), but is outperformed by DCDI in nonlinear or denser graph settings.",
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "Used as a baseline in experiments; strong when assumptions match data (linear Gaussian) but less flexible than DCDI for nonlinear or complex densities.",
            "uuid": "e993.8",
            "source_info": {
                "paper_title": "Differentiable Causal Discovery from Interventional Data",
                "publication_date_yy_mm": "2020-07"
            }
        },
        {
            "name_short": "CAM",
            "name_full": "Causal Additive Models (CAM)",
            "brief_description": "A score-based method for nonlinear causal discovery that assumes additive noise models and conducts greedy order search with penalized regression.",
            "citation_title": "Cam: Causal additive models, high-dimensional order search and penalized regression",
            "mention_or_use": "use",
            "method_name": "CAM",
            "method_description": "Assumes additive noise structural equations where each variable is a (nonlinear) function of its parents plus additive noise; performs order search and penalized regression to identify functional relationships and parent sets, with greedy optimization of a score based on penalized likelihood.",
            "environment_name": "Observational (and adapted for interventions in code) datasets",
            "environment_description": "Originally developed for observational datasets under additive noise assumptions; authors use a modified CAM supporting interventions in comparisons, but CAM requires the additive noise structural assumption.",
            "handles_distractors": false,
            "distractor_handling_technique": null,
            "spurious_signal_types": "Targets spurious edges via model-based penalized regression and ordering search but assumes additive noise identifiability.",
            "detection_method": null,
            "downweighting_method": "Penalized regression (sparsity-inducing penalties) to discourage spurious predictors.",
            "refutation_method": null,
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "Included as a baseline; performs well when data follow additive noise models but is less flexible than DCDI for general nonlinear/noise forms.",
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "CAM is a competitive baseline under its assumptions but lacks DCDI's ability to flexibly model general conditional densities and to jointly learn unknown intervention targets.",
            "uuid": "e993.9",
            "source_info": {
                "paper_title": "Differentiable Causal Discovery from Interventional Data",
                "publication_date_yy_mm": "2020-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Causal inference by using invariant prediction: identification and confidence intervals",
            "rating": 2
        },
        {
            "paper_title": "Invariant causal prediction for nonlinear models",
            "rating": 2
        },
        {
            "paper_title": "Joint causal inference from multiple contexts",
            "rating": 2
        },
        {
            "paper_title": "Constraint-based causal discovery from multiple interventions over overlapping variable sets",
            "rating": 2
        },
        {
            "paper_title": "Constraint-based causal discovery: Conflict resolution with answer set programming",
            "rating": 2
        },
        {
            "paper_title": "Permutation-based causal inference algorithms with interventions",
            "rating": 2
        },
        {
            "paper_title": "Permutation-based causal structure learning with unknown intervention targets",
            "rating": 2
        },
        {
            "paper_title": "Learning neural causal models from unknown interventions",
            "rating": 2
        },
        {
            "paper_title": "Dags with no tears: Continuous optimization for structure learning",
            "rating": 2
        },
        {
            "paper_title": "A meta-transfer objective for learning to disentangle causal mechanisms",
            "rating": 1
        }
    ],
    "cost": 0.02337525,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Differentiable Causal Discovery from Interventional Data</h1>
<p>Philippe Brouillard<em><br>Mila, Université de Montréal<br>Sébastien Lachapelle</em><br>Mila, Université de Montréal<br>Alexandre Lacoste<br>Element AI<br>Simon Lacoste-Julien<br>Mila, Université de Montréal<br>Alexandre Drouin<br>Canada CIFAR AI Chair<br>Element AI</p>
<h4>Abstract</h4>
<p>Learning a causal directed acyclic graph from data is a challenging task that involves solving a combinatorial problem for which the solution is not always identifiable. A new line of work reformulates this problem as a continuous constrained optimization one, which is solved via the augmented Lagrangian method. However, most methods based on this idea do not make use of interventional data, which can significantly alleviate identifiability issues. This work constitutes a new step in this direction by proposing a theoretically-grounded method based on neural networks that can leverage interventional data. We illustrate the flexibility of the continuousconstrained framework by taking advantage of expressive neural architectures such as normalizing flows. We show that our approach compares favorably to the state of the art in a variety of settings, including perfect and imperfect interventions for which the targeted nodes may even be unknown.</p>
<h2>1 Introduction</h2>
<p>The inference of causal relationships is a problem of fundamental interest in science. In all fields of research, experiments are systematically performed with the goal of elucidating the underlying causal dynamics of systems. This quest for causality is motivated by the desire to take actions that induce a controlled change in a system. Achieving this requires to answer questions, such as "what would be the impact on the system if this variable were changed from value $x$ to $y$ ?", which cannot be answered without causal knowledge [33].
In this work, we address the problem of data-driven causal discovery [16]. Our goal is to design an algorithm that can automatically discover causal relationships from data. More formally, we aim to learn a causal graphical model (CGM) [36], which consists of a joint distribution coupled with a directed acyclic graph (DAG), where edges indicate direct causal relationships. Achieving this based on observational data alone is challenging since, under the faithfulness assumption, the true DAG is only identifiable up to a Markov equivalence class [46]. Fortunately, identifiability can be improved by considering interventional data, i.e., the outcome of some experiments. In this case, the DAG is identifiable up to an interventional Markov equivalence class, which is a subset of the Markov equivalence class [48, 15], and, when observing enough interventions [9, 11], the DAG is exactly identifiable. In practice, it may be possible for domain experts to collect such interventional data, resulting in clear gains in identifiability. For instance, in genomics, recent advances in gene editing technologies have given rise to high-throughput methods for interventional gene expression data [6].</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Different intervention types (shown in red). In imperfect interventions, the causal relationships are altered. In perfect interventions, the targeted node is cut out from its parents.</p>
<p>Nevertheless, even with interventional data at hand, finding the right DAG is challenging. The solution space is immense and grows super-exponentially with the number of variables. Recently, Zheng et al. [52] proposed to cast this search problem as a constrained continuous-optimization problem, avoiding the computationally-intensive search typically performed by score-based and constraint-based methods [36]. The work of Zheng et al. [52] was limited to linear relationships, but was quickly extended to nonlinear ones via neural networks [27, 49, 53, 32, 21, 54]. Yet, these approaches do not make use of interventional data and must therefore rely on strong parametric assumptions (e.g., gaussian additive noise models). Bengio et al. [1] leveraged interventions and continuous optimization to learn the causal direction in the bivariate setting. The follow-up work of Ke et al. [23] generalized to the multivariate setting by optimizing an unconstrained objective with regularization inspired by Zheng et al. [52], but lacked theoretical guarantees. In this work, we propose a theoretically-grounded differentiable approach to causal discovery that can make use of interventional data (with potentially unknown targets) and that relies on the constrained-optimization framework of [52] without making strong assumptions about the functional form of causal mechanisms, thanks to expressive density estimators.</p>
<h3>1.1 Contributions</h3>
<ul>
<li>We propose Differentiable Causal Discovery with Interventions (DCDI): a general differentiable causal structure learning method that can leverage perfect, imperfect and unknown-target interventions (Section 3). We propose two instantiations, one of which is a universal density approximator that relies on normalizing flows (Section 3.4).</li>
<li>We show that the exact maximization of the proposed score will identify the $\mathcal{I}$-Markov equivalence class [48] of the ground truth graph (under regularity conditions) for both the known- and unknown-target settings (Thm. 1 in Section 3.1 &amp; Thm. 2 in Section 3.3, respectively).</li>
<li>We provide an extensive comparison of DCDI to state-of-the-art methods in a wide variety of conditions, including multiple functional forms and types of interventions (Section 4).</li>
</ul>
<h2>2 Background and related work</h2>
<h3>2.1 Definitions</h3>
<p><strong>Causal graphical models.</strong> A CGM is defined by a distribution $P_{X}$ over a random vector $X = (X_1, \cdots, X_d)$ and a DAG $\mathcal{G} = (V, E)$. Each node $i \in V = {1, \cdots, d}$ is associated with a random variable $X_i$ and each edge $(i, j) \in E$ represents a direct causal relation from variable $X_i$ to $X_j$. The distribution $P_{X}$ is Markov to the graph $\mathcal{G}$, which means that the joint distribution can be factorized as</p>
<p>$$p(x_1, \cdots, x_d) = \prod_{j=1}^{d} p_j(x_j | x_{\pi_j^G}), \tag{1}$$</p>
<p>where $\pi_j^G$ is the set of parents of the node $j$ in the graph $\mathcal{G}$, and $x_B$, for a subset $B \subseteq V$, denotes the entries of the vector $x$ with indices in $B$. In this work, we assume <em>causal sufficiency</em>, i.e., there is no hidden common cause that is causing more than one variable in $X$ [36].</p>
<p><strong>Interventions.</strong> In contrast with standard Bayesian Networks, CGMs support interventions. Formally, an intervention on a variable $x_j$ corresponds to replacing its conditional $p_j(x_j | x_{\pi_j^G})$ by a new</p>
<p>conditional $\tilde{p}<em j="j">{j}\left(x</em>\right)$, where $K$ is the number of interventions (including the observational setting). Finally, the $k$ th interventional joint density is} \mid x_{\pi_{j}^{\mathcal{G}}}\right)$ in Equation (1), thus modifying the distribution only locally. Interventions can be performed on multiple variables simultaneously and we call the interventional target the set $I \subseteq V$ of such variables. When considering more than one intervention, we denote the interventional target of the $k$ th intervention by $I_{k}$. Throughout this paper, we assume that the observational distribution (the original distribution without interventions) is observed, and denote it by $I_{1}:=\emptyset$. We define the interventional family by $\mathcal{I}:=\left(I_{1}, \cdots, I_{K</p>
<p>$$
p^{(k)}\left(x_{1}, \cdots, x_{d}\right):=\prod_{j \notin I_{k}} p_{j}^{(1)}\left(x_{j} \mid x_{\pi_{j}^{\mathcal{G}}}\right) \prod_{j \in I_{k}} p_{j}^{(k)}\left(x_{j} \mid x_{\pi_{j}^{\mathcal{G}}}\right)
$$</p>
<p>where the assumption of causal sufficiency is implicit to this definition of interventions.
Type of interventions. The general type of interventions described in (2) are called imperfect (or soft, parametric) [36, 7, 8]. A specific case that is often considered is (stochastic) perfect interventions (or hard, structural) [10, 48, 26] where $p_{j}^{(k)}\left(x_{j} \mid x_{\pi_{j}^{\mathcal{G}}}\right)=p_{j}^{(k)}\left(x_{j}\right)$ for all $j \in I_{k}$, thus removing the dependencies with their parents (see Figure 1). Real-world examples of these types of interventions include gene knockout/knockdown in biology. Analogous to a perfect intervention, a gene knockout completely suppresses the expression of one gene and removes dependencies to regulators of gene expression. In contrast, a gene knockdown hinders the expression of one gene without removing dependencies with regulators [55], and is thus an imperfect intervention.</p>
<h1>2.2 Causal structure learning</h1>
<p>In causal structure learning, the goal is to recover the causal DAG $\mathcal{G}$ using samples from $P_{X}$ and, when available, from interventional distributions. This problem presents two main challenges: 1) the size of the search space is super-exponential in the number of nodes [5] and 2) the true DAG is not always identifiable (more severe without interventional data). Methods for this task are often divided into three groups: constraint-based, score-based, and hybrid methods. We briefly review these below.
Constraint-based methods typically rely on conditional independence testing to identify edges in $\mathcal{G}$. The PC algorithm [41] is a classical example that works with observational data. It performs conditional independence tests with a conditioning set that increases at each step of the algorithm and finds an equivalence class that satisfies all independencies. Methods that support interventional data include COmbINE [45], HEJ [19], which both rely on Boolean satisfiability solvers to find a graph that satisfies all constraints; and [24], which proposes an algorithm inspired by FCI [41]. In contrast with our method, these methods account for latent confounders. The Joint causal inference framework (JCI) [31] supports latent confounders and can deal with interventions with unknown targets. This framework can be used with various observational constraint-based algorithms such as PC or FCI. Another type of constraint-based method exploits the invariance of causal mechanisms across interventional distributions, e.g., ICP [35, 17]. As will later be presented in Section 3, our loss function also accounts for such invariances.
Score-based methods formulate the problem of estimating the ground truth DAG $\mathcal{G}^{*}$ by optimizing a score function $\mathcal{S}$ over the space of DAGs. The estimated DAG $\hat{\mathcal{G}}$ is given by</p>
<p>$$
\hat{\mathcal{G}} \in \underset{\mathcal{G} \in \mathrm{DAG}}{\arg \max } \mathcal{S}(\mathcal{G})
$$</p>
<p>A typical choice of score in the purely observational setting is the regularized maximum likelihood:</p>
<p>$$
\mathcal{S}(\mathcal{G}):=\max <em P__X="P_{X" X="X" _sim="\sim">{\theta} \mathbb{E}</em>|
$$}} \log f_{\theta}(X)-\lambda|\mathcal{G</p>
<p>where $f_{\theta}$ is a density function parameterized by $\theta,|\mathcal{G}|$ is the number of edges in $\mathcal{G}$ and $\lambda$ is a positive scalar. ${ }^{1}$ Since the space of DAGs is super-exponential in the number of nodes, these methods often rely on greedy combinatorial search algorithms. A typical example is GIES [15], an adaptation of GES [5] to perfect interventions. In contrast with our method, GIES assumes a linear gaussian model and optimizes the Bayesian information criterion (BIC) over the space of $\mathcal{I}$-Markov equivalence</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>classes (see Definition 6 in Appendix A.1). CAM [4] is also a score-based method using greedy search, but it is nonlinear: it assumes an additive noise model where the nonlinear functions are additive. In the original paper, CAM only addresses the observational case where additive noise models are identifiable, however code is available to support perfect interventions.</p>
<p>Hybrid methods combine constraint and score-based approaches. Among these, IGSP [47; 48] is a method that optimizes a score based on conditional independence tests. Contrary to GIES, this method has been shown to be consistent under the faithfulness assumption. Furthermore, this method has recently been extended to support interventions with unknown targets (UT-IGSP) [42], which are also supported by our method.</p>
<h1>2.3 Continuous constrained optimization for structure learning</h1>
<p>A new line of research initiated by Zheng et al. [52], which serves as the basis for our work, reformulates the combinatorial problem of finding the optimal DAG as a continuous constrainedoptimization problem, effectively avoiding the combinatorial search. Analogous to standard scorebased approaches, these methods rely on a model $f_{\theta}$ parametrized by $\theta$, though $\theta$ also encodes the graph $\mathcal{G}$. Central to this class of methods are both the use a weighted adjacency matrix $A_{\theta} \in \mathbb{R}_{\geq 0}^{d \times d}$ (which depends on the parameters of the model) and the acyclicity constraint introduced by Zheng et al. [52] in the context of linear models:</p>
<p>$$
\operatorname{Tr} e^{A_{\theta}}-d=0
$$</p>
<p>The weighted adjacency matrix encodes the DAG estimator $\hat{\mathcal{G}}$ as $\left(A_{\theta}\right)<em _theta="\theta">{i j}&gt;0 \Longleftrightarrow i \rightarrow j \in \hat{\mathcal{G}}$. Zheng et al. [52] showed, in the context of linear models, that $\hat{\mathcal{G}}$ is acyclic if and only if the constraint $\operatorname{Tr} e^{A</em>-d=0$ is satisfied. The general optimization problem is then}</p>
<p>$$
\max <em P__X="P_{X" X="X" _sim="\sim">{\theta} \mathbb{E}</em>-d=0
$$}} \log f_{\theta}(X)-\lambda \Omega(\theta) \text { s.t. } \operatorname{Tr} e^{A_{\theta}</p>
<p>where $\Omega(\theta)$ is a regularizing term penalizing the number of edges in $\hat{\mathcal{G}}$. This problem is then approximately solved using an augmented Lagrangian procedure, as proposed by Zheng et al. [52]. Note that the problem in Equation (6) is very similar to the one resulting from Equations (3) and (4).</p>
<p>Continuous-constrained methods differ in their choice of model, weighted adjacency matrix, and the specifics of their optimization procedures. For instance, NOTEARS [52] assumes a Gaussian linear model with equal variances where $\theta:=W \in \mathbb{R}^{d \times d}$ is the matrix of regression coefficients, $\Omega(\theta):=|W|<em _theta="\theta">{1}$ and $A</em>$ is a function of the neural networks parameters [27; 53]. In terms of scoring, most methods rely on maximum likelihood or variants like implicit maximum likelihood [21] and evidence lower bound [49]. Zhu and Chen [54] also rely on the acyclicity constraint, but use reinforcement learning as a search strategy to estimate the DAG. Ke et al. [23] learn a DAG from interventional data by optimizing an unconstrained objective with a regularization term inspired by the acyclicity constraint, but that penalizes only cycles of length two. However, their work is limited to discrete distributions and single-node interventions. To the best of our knowledge, no work has investigated, in a general manner, the use of continuous-constrained approaches in the context of interventions as we present in the next section.}:=W \odot W$ is the weighted adjacency matrix. Several other methods use neural networks to model nonlinear relations via $f_{\theta}$ and have been shown to be competitive with classical methods [27; 53]. In some methods, the parameter $\theta$ can be partitioned into $\theta_{1}$ and $\theta_{2}$ such that $f_{\theta}=f_{\theta_{1}}$ and $A_{\theta}=A_{\theta_{2}}[21,32,23]$ while in others, such a decoupling is not possible, i.e., the adjacency matrix $A_{\theta</p>
<h2>3 DCDI: Differentiable causal discovery from interventional data</h2>
<p>In this section, we present a score for imperfect interventions, provide a theorem showing its validity, and show how it can be maximized using the continuous-constrained approach to structure learning. We also provide a theoretically grounded extension to interventions with unknown targets.</p>
<h3>3.1 A score for imperfect interventions</h3>
<p>The model we consider uses neural networks to model conditional densities. Moreover, we encode the $\operatorname{DAG} \mathcal{G}$ with a binary adjacency matrix $M^{\mathcal{G}} \in{0,1}^{d \times d}$ which acts as a mask on the neural networks</p>
<p>inputs. We similarly encode the interventional family $\mathcal{I}$ with a binary matrix $R^{\mathcal{I}}\in{0,1}^{K\times d}$, where $R_{kj}^{\mathcal{I}}=1$ means that $X_{j}$ is a target in $I_{k}$. In line with the definition of interventions in Equation (2), we model the joint density of the $k$th intervention by</p>
<p>$f^{(k)}(x;M^{\mathcal{G}},R^{\mathcal{I}},\phi):=\prod_{j=1}^{d}\tilde{f}(x_{j} ;\mathrm{NN}(M_{j}^{\mathcal{G}}\odot x;{\phi}<em kj="kj">{j}^{(1)}))^{1-R</em>}^{\mathcal{I}}}\tilde{f}(x_{j} ;\mathrm{NN}(M_{j}^{\mathcal{G}}\odot x;{\phi<em kj="kj">{j}^{(k)}))^{R</em>,$ (7)}^{\mathcal{I}}</p>
<p>where $\phi:=\left{\phi^{(1)},\cdots,\phi^{(K)}\right}$, the NN’s are neural networks parameterized by $\phi_{j}^{(1)}$ or $\phi_{j}^{(k)}$, the operator $\odot$ denotes the Hadamard product (element-wise) and $M_{j}^{\mathcal{G}}$ denotes the $j$th column of $M^{\mathcal{G}}$, which enables selecting the parents of node $j$ in the graph $\mathcal{G}$. The neural networks output the parameters of a density function $\tilde{f}$, which in principle, could be any density. We experiment with Gaussian distributions and more expressive normalizing flows (see Section 3.4).</p>
<p>We denote $\mathcal{G}^{<em>}$ and $\mathcal{I}^{</em>}:=\left(I_{1}^{<em>},...,I_{K}^{</em>}\right)$ to be the ground truth causal DAG and ground truth interventional family, respectively. In this section, we assume that $\mathcal{I}^{*}$ is known, but we will relax this assumption in Section 3.3. We propose maximizing with respect to $\mathcal{G}$ the following regularized maximum log-likelihood score:</p>
<p>$\mathcal{S}_{\mathcal{I}^{<em>}}(\mathcal{G}):=\sup_{\phi}\sum_{k=1}^{K}\mathbb{E}_{X\sim p^{(k)}}\log f^{(k)}(X;M^{\mathcal{G}},R^{\mathcal{I}^{</em>}},{\phi})-\lambda|\mathcal{G}|,$ (8)</p>
<p>where $p^{(k)}$ stands for the $k$th ground truth interventional distribution from which the data is sampled. A careful inspection of (7) reveals that the conditionals of the model are invariant across interventions in which they are not targeted. Intuitively, this means that maximizing (8) will favor graphs $\mathcal{G}$ in which a conditional $p(x_{j}|x_{\pi_{j}^{\mathcal{G}}})$ is invariant across all interventional distributions in which $x_{j}$ is not a target, i.e., $j\notin I_{k}^{*}$. This is a fundamental property of causal graphical models.</p>
<p>We now present our first theoretical result (see Appendix A.2 for the proof). This theorem states that, under appropriate assumptions, maximizing $\mathcal{S}_{\mathcal{I}^{<em>}}(\mathcal{G})$ yields an estimated DAG $\hat{\mathcal{G}}$ that is $\mathcal{I}^{</em>}$-Markov equivalent to the true DAG $\mathcal{G}^{<em>}$. We use the notion of $\mathcal{I}^{</em>}$-Markov equivalence introduced by [48] and recall its meaning in Definition 6 of Appendix A.1. Briefly, the $\mathcal{I}^{<em>}$-Markov equivalence class of $\mathcal{G}^{</em>}$ is a set of DAGs which are indistinguishable from $\mathcal{G}^{<em>}$ given the interventional targets in $\mathcal{I}^{</em>}$. This means identifying the $\mathcal{I}^{<em>}$-Markov equivalence class of $\mathcal{G}^{</em>}$ is the best one can hope for given the interventions $\mathcal{I}^{*}$ without making further distributional assumptions.</p>
<p>Theorem 1 (Identification via score maximization) Suppose the interventional family $\mathcal{I}^{<em>}$ is such that $I_{1}^{</em>}:=\emptyset$. Let $\mathcal{G}^{<em>}$ be the ground truth DAG and $\hat{\mathcal{G}}\in\operatorname{arg\,max}<em _mathcal_I="\mathcal{I">{\mathcal{G} \in \text { DAG } \mathcal{S}</em>^{</em>}}}(\mathcal{G})$. Assume that the density model has enough capacity to represent the ground truth distributions, that $\mathcal{I}^{<em>}$-faithfulness holds, that the density model is strictly positive and that the ground truth densities $p^{(k)}$ have finite differential entropy, respectively Assumptions 1, 2, 3 \&amp; 4 (see Appendix A.2 for precise statements). Then for $\lambda&gt;0$ small enough, we have that $\hat{\mathcal{G}}$ is $\mathcal{I}^{</em>}$-Markov equivalent to $\mathcal{G}^{*}$.</p>
<p>Proof idea. Using the graphical characterization of $\mathcal{I}$-Markov equivalence from Yang et al. [48], we verify that every graph outside the equivalence class has a lower score than that of the ground truth graph. We show this by noticing that any such graph will either have more edges than $\mathcal{G}^{*}$ or limit the distributions expressible by the model in such a way as to prevent it from properly fitting the ground truth. Moreover, the coefficient $\lambda$ must be chosen small enough to avoid too sparse solutions. $\square$</p>
<p>$\mathcal{I}^{<em>}$-faithfulness (Assumption 2) enforces two conditions. The first one is the usual faithfulness condition, i.e., whenever a conditional independence statement holds in the observational distribution, the corresponding d-separation holds in $\mathcal{G}^{</em>}$. The second one requires that the interventions are non-pathological in the sense that every variable that can be potentially affected by the intervention are indeed affected. See Appendix A.2 for more details and examples of $\mathcal{I}^{*}$-faithfulness violations.</p>
<p>To interpret this result, note that the $\mathcal{I}^{<em>}$-Markov equivalence class of $\mathcal{G}^{</em>}$ tends to get smaller as we add interventional targets to the interventional family $\mathcal{I}^{<em>}$. As an example, when $\mathcal{I}^{</em>}=(\emptyset,{1}, \cdots,{d})$, i.e., when each node is individually targeted by an intervention, $\mathcal{G}^{<em>}$ is alone in its equivalence class and, if assumptions of Theorem 1 hold, $\hat{\mathcal{G}}=\mathcal{G}^{</em>}$. See Corollary 11 in Appendix A.1 for details.</p>
<p>Perfect interventions. The score $\mathcal{S}_{\mathcal{I}^{*}}(\mathcal{G})$ can be specialized for perfect interventions, i.e., where the targeted nodes are completely disconnected from their parents. The idea is to leverage the fact that the</p>
<p>conditionals targeted by the intervention in Equation (7) should not depend on the graph $\mathcal{G}$ anymore. This means that these terms can be removed without affecting the maximization w.r.t. $\mathcal{G}$. We use this version of the score when experimenting with perfect interventions and present it in Appendix A.4.</p>
<h1>3.2 A continuous-constrained formulation</h1>
<p>To allow for gradient-based stochastic optimization, we follow [21, 32] and treat the adjacency matrix $M^{\mathcal{G}}$ as random, where the entries $M_{i j}^{\mathcal{G}}$ are independent Bernoulli variables with success probability $\sigma\left(\alpha_{i j}\right)$ ( $\sigma$ is the sigmoid function) and $\alpha_{i j}$ is a scalar parameter. We group these $\alpha_{i j}$ 's into a matrix $\Lambda \in \mathbb{R}^{d \times d}$. We then replace the score $\mathcal{S}_{\mathcal{I}^{*}}(\mathcal{G})$ (8) with the following relaxation:</p>
<p>$$
\hat{\mathcal{S}}<em 0="0">{\mathcal{I}^{<em>}}(\Lambda):=\sup <em k="1">{\phi} \underset{M \sim \sigma(\Lambda)}{\mathbb{E}}\left[\sum</em>^{}^{K} \underset{X \sim p^{(k)}}{\mathbb{E}} \log f^{(k)}\left(X ; M, R^{\mathcal{I</em>}}, \phi\right)-\lambda|M|</em>\right]
$$</p>
<p>where we dropped the $\mathcal{G}$ superscript in $M$ to lighten notation. This score tends asymptotically to $\mathcal{S}<em 1="1">{\mathcal{I}^{*}}(\mathcal{G})$ as $\sigma(\Lambda)$ progressively concentrates its mass on $\mathcal{G} .^{2}$ While the expectation of the loglikelihood term is intractable, the expectation of the regularizing term simply evaluates to $\lambda|\sigma(\Lambda)|</em>$. This score can then be maximized under the acyclicity constraint presented in Section 2.3:</p>
<p>$$
\sup <em _mathcal_I="\mathcal{I">{\Lambda} \hat{\mathcal{S}}</em>-d=0
$$}^{*}}(\Lambda) \quad \text { s.t. } \operatorname{Tr} e^{\sigma(\Lambda)</p>
<p>This problem presents two main challenges: it is a constrained problem and it contains intractable expectations. As proposed by [52], we rely on the augmented Lagrangian procedure to optimize $\phi$ and $\Lambda$ jointly under the acyclicity constraint. This procedure transforms the constrained problem into a sequence of unconstrained subproblems which can themselves be optimized via a standard stochastic gradient descent algorithm for neural networks such as RMSprop. The procedure should converge to a stationary point of the original constrained problem (which is not necessarily the global optimum due to the non-convexity of the problem). In Appendix B.3, we give details on the augmented Lagrangian procedure and show the learning process in details with a concrete example.</p>
<p>The gradient of the likelihood part of $\hat{\mathcal{S}}<em _theta="\theta">{\mathcal{I}^{*}}(\Lambda)$ w.r.t. $\Lambda$ is estimated using the Straight-Through Gumbel estimator. This amounts to using Bernoulli samples in the forward pass and Gumbel-Softmax samples in the backward pass which can be differentiated w.r.t. $\Lambda$ via the reparametrization trick [20, 29]. This approach was already shown to give good results in the context of continuous optimization for causal discovery in the purely observational case [32, 21]. We emphasize that our approach belongs to the general framework presented in Section 2.3 where the global parameter $\theta$ is ${\phi, \Lambda}$, the weighted adjacency matrix $A</em>$.}$ is $\sigma(\Lambda)$ and the regularizing term $\Omega(\theta)$ is $|\sigma(\Lambda)|_{1</p>
<h3>3.3 Interventions with unknown targets</h3>
<p>Until now, we have assumed that the ground truth interventional family $\mathcal{I}^{*}$ is known. We now consider the case were it is unknown and, thus, needs to be learned. To do so, we propose a simple modification of score (8) which consists in adding regularization to favor sparse interventional families.</p>
<p>$$
\mathcal{S}(\mathcal{G}, \mathcal{I}):=\sup <em k="1">{\phi} \sum</em>}^{K} \mathbb{E<em R="R">{X \sim p^{(k)}} \log f^{(k)}\left(X ; M^{\mathcal{G}}, R^{\mathcal{I}}, \phi\right)-\lambda|\mathcal{G}|-\lambda</em>|
$$}|\mathcal{I</p>
<p>where $|\mathcal{I}|=\sum_{k=1}^{K}\left|I_{k}\right|$. The following theorem, proved in Appendix A.3, extends Theorem 1 by showing that, under the same assumptions, maximizing $\mathcal{S}(\mathcal{G}, \mathcal{I})$ with respect to both $\mathcal{G}$ and $\mathcal{I}$ recovers both the $\mathcal{I}^{<em>}$-Markov equivalence class of $\mathcal{G}^{</em>}$ and the ground truth interventional family $\mathcal{I}^{*}$.</p>
<p>Theorem 2 (Unknown targets identification) Suppose $\mathcal{I}^{<em>}$ is such that $I_{1}^{</em>}:=\emptyset$. Let $\mathcal{G}^{<em>}$ be the ground truth DAG and $(\hat{\mathcal{G}}, \hat{\mathcal{I}}) \in \arg \max <em R="R">{\mathcal{G} \in D A G, \mathcal{I}} \mathcal{S}(\mathcal{G}, \mathcal{I})$. Under the same assumptions as Theorem 1 and for $\lambda, \lambda</em>^{}&gt;0$ small enough, $\hat{\mathcal{G}}$ is $\mathcal{I</em>}$-Markov equivalent to $\mathcal{G}^{<em>}$ and $\hat{\mathcal{I}}=\mathcal{I}^{</em>}$.</p>
<p>Proof idea. We simply append a few steps at the beginning of the proof of Theorem 1 which show that whenever $\mathcal{I} \neq \mathcal{I}^{<em>}$, the resulting score is worse than $\mathcal{S}\left(\mathcal{G}^{</em>}, \mathcal{I}^{*}\right)$, and hence is not optimal. This is done using arguments very similar to Theorem 1 and choosing $\lambda$ and $\lambda_{R}$ small enough.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Theorem 2 informs us that ignoring which nodes are targeted during interventions does not affect identifiability. However, this result assumes implicitly that the learner knows which data set is the observational one.</p>
<p>Similarly to the development of Section 3.2, the score $\mathcal{S}(\mathcal{G}, \mathcal{I})$ can be relaxed by treating entries of $M^{\mathcal{G}}$ and $R^{\mathcal{I}}$ as independent Bernoulli random variables parameterized by $\sigma\left(\alpha_{i j}\right)$ and $\sigma\left(\beta_{k j}\right)$, respectively. We thus introduced a new learnable parameter $\beta$. The resulting relaxed score is similar to (9), but the expectation is taken w.r.t. to $M$ and $R$. Similarly to $\Lambda$, the Straight-Through Gumbel estimator is used to estimate the gradient of the score w.r.t. the parameters $\beta_{k j}$. For perfect interventions, we adapt this score by masking all inputs of the neural networks under interventions.</p>
<p>The related work of Ke et al. [23], which also support unknown targets, bears similarity to DCDI but addresses a different setting in which interventions are obtained sequentially in an online fashion. One important difference is that their method attempts to identify the single node that has been intervened upon (as a hard prediction), whereas DCDI learns a distribution over all potential interventional families via the continuous parameters $\sigma\left(\beta_{k j}\right)$, which typically becomes deterministic at convergence. Ke et al. [23] also use random masks to encode the graph structure but estimates the gradient w.r.t. their distribution parameters using the log-trick which is known to have high variance [39] compared to reparameterized gradient [29].</p>
<h1>3.4 DCDI with normalizing flows</h1>
<p>In this section, we describe how the scores presented in Sections 3.2 \&amp; 3.3 can accommodate powerful density approximators. In the purely observational setting, very expressive models usually hinder identifiability, but this problem vanishes when enough interventions are available. There are many possibilities when it comes to the choice of the density function $\tilde{f}$. In this paper, we experimented with simple Gaussian distributions as well as normalizing flows [38] which can represent complex causal relationships, e.g., multi-modal distributions that can occur in the presence of latent variables that are parent of only one variable.
A normalizing flow $\tau(\cdot ; \omega)$ is an invertible function (e.g., a neural network) parameterized by $\omega$ with a tractable Jacobian, which can be used to model complex densities by transforming a simple random variable via the change of variable formula:</p>
<p>$$
\tilde{f}(z ; \omega):=\left|\operatorname{det}\left(\frac{\partial \tau(z ; \omega)}{\partial z}\right)\right| p(\tau(z ; \omega))
$$</p>
<p>where $\frac{\partial \tau(z ; \omega)}{\partial z}$ is the Jacobian matrix of $\tau(\cdot ; \omega)$ and $p(\cdot)$ is a simple density function, e.g., a Gaussian. The function $\tilde{f}(\cdot ; \omega)$ can be plugged directly into the scores presented earlier by letting the neural networks $\mathrm{NN}\left(\cdot ; \phi_{j}^{(k)}\right)$ output the parameter $\omega_{j}$ of the normalizing flow $\tau_{j}$ for each variable $x_{j}$. In our implementation, we use deep sigmoidal flows (DSF), a specific instantiation of normalizing flows which is a universal density approximator [18]. Details about DSF are relayed to Appendix B.2.</p>
<h2>4 Experiments</h2>
<p>We tested DCDI with Gaussian densities (DCDI-G) and with normalizing flows (DCDI-DSF) on a real-world data set and several synthetic data sets. The real-world task is a flow cytometry data set from Sachs et al. [40]. Our results, reported in Appendix C.1, show that our approach performs comparably to state-of-the-art methods. In this section, we focus on synthetic data sets, since these allow for a more systematic comparison of methods against various factors of variation (type of interventions, graph size, density, type of mechanisms).</p>
<p>We consider synthetic data sets with three interventional settings: perfect/known, imperfect/known, and perfect/unknown. Each data set has one of the three different types of causal mechanisms: i) linear [42], ii) nonlinear additive noise model (ANM) [4], and iii) nonlinear with non-additive noise using neural networks (NN) [21]. For each data set type, graphs vary in size ( $d=10$ or 20 ) and density ( $e=1$ or 4 where $e \cdot d$ is the average number of edges). For conciseness, we present results for 20-node graphs in the main text and report results on 10-node graphs in Appendix C.7; conclusions are similar for all sizes. For each condition, ten graphs are sampled with their causal mechanisms and then observational and interventional data are generated. Each data set has 10000 samples uniformly</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Perfect interventions. SHD and SID (lower is better) for 20-node graphs</p>
<p>distributed in the different interventional settings. A total of d interventions were performed, each by sampling up to 0.1d target nodes. For more details on the generation process, see Appendix B.1.</p>
<p>Most methods have an hyperparameter controlling DAG sparsity. Although performance is sensitive to this hyperparameter, many papers do not specify how it was selected. For score-based methods (GIES, CAM and DCDI), we select it by maximizing the held-out likelihood as explained in Appendix B.5 (without using the ground truth DAG). In contrast, since constraint-based methods (IGSP, UTIGSP, JCI-PC) do not yield a likelihood model to evaluate on held-out data, we use a fixed cutoff parameter (α = 1e−3) that leads to good results. We report additional results with different cutoff values in Appendix C.7. For IGSP and UT-IGSP, we always use the independence test well tailored to the data set type: partial correlation test for Gaussian linear data and KCI-test [50] for nonlinear data.</p>
<p>The performance of each method is assessed by two metrics comparing the estimated graph to the ground truth graph: i) the structural Hamming distance (SHD) which is simply the number of edges that differ between two DAGs (either reversed, missing or superfluous) and ii) the structural interventional distance (SID) which assesses how two DAGs differ with respect to their causal inference statements [34]. In Appendix C.6, we also report how well the graph can be used to predict the effect of unseen interventions [13]. Our implementation is available here and additional information about the baseline methods is provided in Appendix B.4.</p>
<h3>4.1 Results for different intervention types</h3>
<p><strong>Perfect interventions.</strong> We compare our methods to GIES [15], a modified version of CAM [4] that support interventions and IGSP [47]. The conditionals of targeted nodes were replaced by the marginal N(2,1) similarly to [15, 42]. Boxplots for SHD and SID over 10 graphs are shown in Figure 2. For all conditions, DCDI-G and DCDI-DSF shows competitive results in terms of SHD and SID. For graphs with a higher number of average edges, DCDI-G and DCDI-DSF outperform all methods. GIES often shows the best performance for the linear data set, which is not surprising given that it makes the right assumptions, i.e., linear functions with Gaussian noise.</p>
<p><strong>Imperfect interventions.</strong> Our conclusions are similar to the perfect intervention setting. As shown in Figure 3, DCDI-G and DCDI-DSF show competitive results and outperform other methods for graphs with a higher connectivity. The nature of the imperfect interventions are explained in Appendix B.1.</p>
<p><strong>Perfect unknown interventions.</strong> We compare to UT-IGSP [42], an extension of IGSP that deal with unknown interventions. The data used are the same as in the perfect intervention setting, but the intervention targets are hidden. Results are shown in Figure 4. Except for linear data sets with sparse graphs, DCDI-G and DCDI-DSF show an overall better performance than UT-IGSP.</p>
<p><strong>Summary.</strong> For all intervention settings, DCDI has overall the best performance. In Appendix C.5, we show similar results for different types of perfect/imperfect interventions. While the advantage of DCDI-DSF over DCDI-G is marginal, it might be explained by the fact that the densities can be sufficiently well modeled by DCDI-G. In Appendix C.2, we show cases where DCDI-G fails to detect the right causal direction due to its lack of capacity, whereas DCDI-DSF systematically succeeds. In Appendix C.4, we present an ablation study confirming the advantage of neural networks against linear models and the ability of our score to leverage interventional data.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Imperfect interventions. SHD and SID for 20-node graphs</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Unknown interventions. SHD and SID for 20-node graphs</p>
<h3>4.2 Scalability experiments</h3>
<p>So far the experiments focused on moderate size data sets, both in terms of number of variables (10 or 20) and number of examples (≈ 10<sup>4</sup>). In Appendix C.3, we compare the running times of DCDI to those of other methods on graphs of up to 100 nodes and on data sets of up to 1 million examples.</p>
<p>The augmented Lagrangian procedure on which DCDI relies requires the computation of the matrix exponential at each gradient step, which costs O(d<sup>3</sup>). We found this does not prevent DCDI from being applied to 100 nodes graphs. Several constraint-based methods use kernel-based conditional independence tests [50, 12], which scale poorly with the number of examples. For example, KCI-test scales in O(n<sup>3</sup>) [43] and HSIC in O(n<sup>2</sup>) [51]. On the other hand, DCDI is not greatly affected by the sample size since it relies on stochastic gradient descent which is known to scale well with the data set size [3]. Our comparison shows that, among all considered methods, DCDI is the only one supporting nonlinear relationships that can scale to as much as one million examples. We believe that this can open the way to new applications of causal discovery where data is abundant.</p>
<h2>5 Conclusion</h2>
<p>We proposed a general continuous-constrained method for causal discovery which can leverage various types of interventional data as well as expressive neural architectures, such as normalizing flows. This approach is rooted in a sound theoretical framework and is competitive with other state-of-the-art algorithms on real and simulated data sets, both in terms of graph recovery and scalability. This work opens interesting opportunities for future research. One direction is to extend DCDI to time-series data, where non-stationarities can be modeled as unknown interventions [37]. Another exciting direction is to learn representations of variables across multiple systems that could serve as prior knowledge for causal discovery in low data settings.</p>
<h1>Broader impact</h1>
<p>Causal structure learning algorithms are general tools that address two high-level tasks: understanding and acting. That is, they can help a user understand a complex system and, once such an understanding is achieved, they can help in recommending actions. We envision positive impacts of our work in fields such as scientific investigation (e.g., interpreting and anticipating the outcome of experiments), policy making for decision-makers (e.g., identifying actions that could stimulate economic growth), and improving policies in autonomous agents (e.g., learning causal relationships in the world via interaction). As a concrete example, consider the case of gene knockouts/knockdowns experiments in the field of genomics, which aim to understand how specific genes and diseases interact [55]. Learning causal models using interventions performed in this setting could help gain precious insight into gene pathways, which may catalyze the development of better pharmaceutic targets and broaden our understanding of complex diseases such as cancer. Of course, applications are likely to extend beyond these examples which seem natural from our current position.</p>
<p>Like any methodological contribution, our work is not immune to undesirable applications that could have negative impacts. For instance, it would be possible, yet unethical for a policy-maker to use our algorithm to understand how specific human-rights violations can reduce crime and recommend their enforcement. The burden of using our work within ethical and benevolent boundaries would rely on the user. Furthermore, even when used in a positive application, our method could have unintended consequences if used without understanding its assumptions.</p>
<p>In order to use our method correctly, it is crucial to understand the assumptions that it makes about the data. When such assumptions are not met, the results may still be valid, but should be used as a support to decision rather than be considered as the absolute truth. These assumptions are:</p>
<ul>
<li>Causal sufficiency: there are no hidden confounding variables</li>
<li>The samples for a given interventional distribution are independent and identically distributed</li>
<li>The causal relationships form an acyclic graph (no feedback loops)</li>
<li>Our theoretical results are valid in the infinite-data regime</li>
</ul>
<p>We encourage users to be mindful of this and to carefully analyze their results before making decisions that could have a significant downstream impact.</p>
<h2>Acknowledgments</h2>
<p>This research was partially supported by the Canada CIFAR AI Chair Program, by an IVADO excellence PhD scholarship and by a Google Focused Research award. The experiments were in part enabled by computational resources provided by Element AI, Calcul Quebec, Compute Canada. The authors would like to thank Nicolas Chapados, Rémi Lepriol, Damien Scieur and Assya Trofimov for their useful comments on the writing, Jose Gallego and Brady Neal for reviewing the proofs of Theorem $1 \&amp; 2$, and Grace Abuhamad for useful comments on the statement of broader impact. Simon Lacoste-Julien is a CIFAR Associate Fellow in the Learning in Machines \&amp; Brains program.</p>
<h2>References</h2>
<p>[1] Yoshua Bengio, Tristan Deleu, Nasim Rahaman, Nan Rosemary Ke, Sebastien Lachapelle, Olexa Bilaniuk, Anirudh Goyal, and Christopher Pal. A meta-transfer objective for learning to disentangle causal mechanisms. In International Conference on Learning Representations, 2020 .
[2] P. Billingsley. Probability and Measure. Wiley Series in Probability and Statistics. Wiley, 1995.
[3] L. Bottou. Large-scale machine learning with stochastic gradient descent. In Proceedings of COMPSTAT'2010. 2010.
[4] P. Bühlmann, J. Peters, and J. Ernest. Cam: Causal additive models, high-dimensional order search and penalized regression. The Annals of Statistics, 2014.</p>
<p>[5] D. M. Chickering. Optimal structure identification with greedy search. In Journal of Machine Learning Research, 2003.
[6] A. Dixit, O. Parnas, B. Li, J. Chen, C. P. Fulco, L. Jerby-Arnon, N. D. Marjanovic, D. Dionne, T. Burks, R. Raychndhury, T. M. Adamson, B. Norman, E. S. Lander, J. S. Weissman, N. Friedman, and A. Regev. Perturb-seq: dissecting molecular circuits with scalable single-cell rna profiling of pooled genetic screens. Cell, 2016.
[7] D. Eaton and K. Murphy. Exact bayesian structure learning from uncertain interventions. In Artificial intelligence and statistics, 2007.
[8] F. Eberhardt. Causation and intervention. Unpublished doctoral dissertation, Carnegie Mellon University, 2007.
[9] F. Eberhardt. Almost Optimal Intervention Sets for Causal Discovery. In Proceedings of the 24th Conference on Uncertainty in Artificial Intelligence, 2008.
[10] F. Eberhardt and R. Scheines. Interventions and causal inference. Philosophy of Science, 2007.
[11] F. Eberhardt, C. Glymour, and R. Scheines. On the Number of Experiments Sufficient and in the Worst Case Necessary to Identify all Causal Relations among N Variables. In Proceedings of the 21st Conference on Uncertainty in Artificial Intelligence, 2005.
[12] K. Fukumizu, A. Gretton, X. Sun, and B. Schölkopf. Kernel measures of conditional dependence. In Advances in neural information processing systems, 2008.
[13] Amanda Gentzel, Dan Garant, and David Jensen. The case for evaluating causal models using interventional measures and empirical data. In Advances in Neural Information Processing Systems, pages 11722-11732, 2019.
[14] X. Glorot and Y. Bengio. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, 2010.
[15] A. Hauser and P. Bühlmann. Characterization and greedy learning of interventional markov equivalence classes of directed acyclic graphs. Journal of Machine Learning Research, 2012.
[16] C. Heinze-Deml, M. H. Maathuis, and N. Meinshausen. Causal structure learning. Annual Review of Statistics and Its Application, 2018.
[17] C. Heinze-Deml, J. Peters, and N. Meinshausen. Invariant causal prediction for nonlinear models. Journal of Causal Inference, 2018.
[18] C.-W. Huang, D. Krueger, A. Lacoste, and A. Courville. Neural autoregressive flows. In Proceedings of the 35th International Conference on Machine Learning, 2018.
[19] A. Hyttinen, F. Eberhardt, and M. Järvisalo. Constraint-based causal discovery: Conflict resolution with answer set programming. In Proceedings of the Thirtieth Conference on Uncertainty in Artificial Intelligence, 2014.
[20] E. Jang, S. Gu, and B. Poole. Categorical reparameterization with gumbel-softmax. Proceedings of the 34th International Conference on Machine Learning, 2017.
[21] D. Kalainathan, O. Goudet, I. Guyon, D. Lopez-Paz, and M. Sebag. Sam: Structural agnostic model, causal discovery and penalized adversarial learning. arXiv preprint arXiv:1803.04929, 2018.
[22] Diviyan Kalainathan and Olivier Goudet. Causal discovery toolbox: Uncover causal relationships in python. arXiv preprint arXiv:1903.02278, 2019.
[23] N. R. Ke, O. Bilaniuk, A. Goyal, S. Bauer, H. Larochelle, C. Pal, and Y. Bengio. Learning neural causal models from unknown interventions. arXiv preprint arXiv:1910.01075, 2019.</p>
<p>[24] Murat Kocaoglu, Amin Jaber, Karthikeyan Shanmugam, and Elias Bareinboim. Characterization and learning of causal graphs with latent variables from soft interventions. In Advances in Neural Information Processing Systems 32. 2019.
[25] D. Koller and N. Friedman. Probabilistic Graphical Models: Principles and Techniques Adaptive Computation and Machine Learning. MIT Press, 2009.
[26] K. B. Korb, L. R. Hope, A. E. Nicholson, and K. Axnick. Varieties of causal intervention. In Pacific Rim International Conference on Artificial Intelligence, 2004.
[27] S. Lachapelle, P. Brouillard, T. Deleu, and S. Lacoste-Julien. Gradient-based neural DAG learning. In Proceedings of the 8th International Conference on Learning Representations, 2020.
[28] Steffen L. Lauritzen. Graphical Models. Oxford University Press, 1996.
[29] C. J. Maddison, A. Mnih, and Y. W. Teh. The concrete distribution: A continuous relaxation of discrete random variables. Proceedings of the 34th International Conference on Machine Learning, 2017.
[30] J. M. Mooij, S. Magliacane, and T. Claassen. Joint causal inference from multiple contexts. arXiv preprint arXiv:1611.10351, 2016.
[31] Joris M. Mooij, Sara Magliacane, and Tom Claassen. Joint causal inference from multiple contexts. Journal of Machine Learning Research, 2020.
[32] I. Ng, Z. Fang, S. Zhu, Z. Chen, and J. Wang. Masked gradient-based causal structure learning. arXiv preprint arXiv:1910.08527, 2019.
[33] J. Pearl. Causality. Cambridge university press, 2009.
[34] J. Peters and P. Bühlmann. Structural intervention distance (SID) for evaluating causal graphs. Neural Computation, 2015.
[35] J. Peters, P. Bühlmann, and N. Meinshausen. Causal inference by using invariant prediction: identification and confidence intervals. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 2016.
[36] J. Peters, D. Janzing, and B. Schölkopf. Elements of Causal Inference - Foundations and Learning Algorithms. MIT Press, 2017.
[37] N. Pfister, P. Bühlmann, and J. Peters. Invariant causal prediction for sequential data. Journal of the American Statistical Association, 2019.
[38] D. J. Rezende and S. Mohamed. Variational inference with normalizing flows. Proceedings of the 32nd International Conference on Machine Learning, 2015.
[39] D. J. Rezende, S. Mohamed, and D. Wierstra. Stochastic backpropagation and approximate inference in deep generative models. In Proceedings of the 31st International Conference on Machine Learning, 2014.
[40] K. Sachs, O. Perez, D. Pe'er, D. A. Lauffenburger, and G. P. Nolan. Causal protein-signaling networks derived from multiparameter single-cell data. Science, 2005.
[41] P. Spirtes, C. N. Glymour, R. Scheines, and D. Heckerman. Causation, prediction, and search. 2000.
[42] C. Squires, Y. Wang, and C. Uhler. Permutation-based causal structure learning with unknown intervention targets. Proceedings of the 36th Conference on Uncertainty in Artificial Intelligence, 2020.
[43] E. V. Strobl, K. Zhang, and S. Visweswaran. Approximate kernel-based conditional independence tests for fast non-parametric causal discovery. Journal of Causal Inference, 2019.</p>
<p>[44] T. Tieleman and G. Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning, 2012.
[45] S. Triantafillou and I. Tsamardinos. Constraint-based causal discovery from multiple interventions over overlapping variable sets. Journal of Machine Learning Research, 2015.
[46] T. Verma and J. Pearl. Equivalence and synthesis of causal models. In Proceedings of the Sixth Annual Conference on Uncertainty in Artificial Intelligence, 1990.
[47] Y. Wang, L. Solus, K. Yang, and C. Uhler. Permutation-based causal inference algorithms with interventions. In Advances in Neural Information Processing Systems, 2017.
[48] K. D. Yang, A. Katcoff, and C. Uhler. Characterizing and learning equivalence classes of causal DAGs under interventions. Proceedings of the 35th International Conference on Machine Learning, 2018.
[49] Y. Yu, J. Chen, T. Gao, and M. Yu. DAG-GNN: DAG structure learning with graph neural networks. In Proceedings of the 36th International Conference on Machine Learning, 2019.
[50] K. Zhang, J. Peters, D. Janzing, and B. Schölkopf. Kernel-based conditional independence test and application in causal discovery. Proceedings of the Twenty-Seventh Conference on Uncertainty in Artificial Intelligence, 2011.
[51] Q. Zhang, S. Filippi, A. Gretton, and D. Sejdinovic. Large-scale kernel methods for independence testing. Statistics and Computing, 2018.
[52] X. Zheng, B. Aragam, P.K. Ravikumar, and E.P. Xing. Dags with no tears: Continuous optimization for structure learning. In Advances in Neural Information Processing Systems 31, 2018.
[53] X. Zheng, C. Dan, B. Aragam, P. Ravikumar, and E. Xing. Learning sparse nonparametric dags. In Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics, 2020.
[54] S. Zhu and Z. Chen. Causal discovery with reinforcement learning. Proceedings of the 8th International Conference on Learning Representations, 2020.
[55] A. M. Zimmer, Y. K. Pan, T. Chandrapalan, R. WM Kwong, and S. F. Perry. Loss-of-function approaches in comparative physiology: is there a future for knockdown experiments in the era of genome editing? Journal of Experimental Biology, 2019.</p>
<h1>Appendix</h1>
<h2>Table of Contents</h2>
<p>A Theory ..... 14
A. 1 Theoretical Foundations for Causal Discovery with Imperfect Interventions ..... 14
A. 2 Proof of Theorem 1 ..... 16
A. 3 Theory for unknown targets ..... 23
A. 4 Adapting the score to perfect interventions ..... 25
B Additional information ..... 26
B. 1 Synthetic data sets ..... 26
B. 2 Deep Sigmoidal Flow: Architectural details ..... 26
B. 3 Optimization ..... 27
B. 4 Baseline methods ..... 30
B. 5 Default hyperparameters and hyperparameter search ..... 31
C Additional experiments ..... 32
C. 1 Real-world data set ..... 32
C. 2 Learning causal direction from complex distributions ..... 33
C. 3 Scalability experiments ..... 34
C. 4 Ablation study ..... 34
C. 5 Different kinds of interventions ..... 39
C. 6 Evaluation on unseen interventional distributions ..... 41
C. 7 Comprehensive results of the main experiments ..... 41</p>
<h2>Contents</h2>
<h2>A Theory</h2>
<h2>A. 1 Theoretical Foundations for Causal Discovery with Imperfect Interventions</h2>
<p>Before showing results about our regularized maximum likelihood score from Section 3.1, we start by briefly presenting useful definitions and results from Yang et al. [48]. We refer the reader to the original paper for a more comprehensive introduction to these notions, examples, and proofs. Throughout the appendix, we assume that the reader is comfortable with the concept of d-separation and immorality in directed graphs. These notions are presented in any standard book on probabilistic graphical models, e.g. Koller and Friedman [25]. Recall that $\mathcal{I}:=\left(I_{1}^{*}, \ldots, I_{K}\right)$ and that we always assume $I_{1}:=\emptyset$. Following the approach of Yang et al. [48] and to simplify the presentation, we consider only densities which are strictly positive everywhere throught this appendix. We also note that while we present proofs for the cases where the distributions have densities with respect to the Lebesgue measure, all our results also hold for discrete distributions by simply replacing the Lebesgue measure with the counting measure in the integrals. We use the notation $i \rightarrow j \in \mathcal{G}$ to indicate that the edge $(i, j)$ is in the edge set of $\mathcal{G}$. Given disjoint $A, B, C \subset V$, when $C$ d-separates $A$ from $B$ in graph $\mathcal{G}$, we write $A \Perp_{\mathcal{G}} B \mid C$ and when random variables $X_{A}$ and $X_{B}$ are independent given $X_{C}$ in distribution $f$, we write $X_{A} \Perp_{f} X_{B} \mid X_{C}$.</p>
<p>Definition 3 For a $D A G \mathcal{G}$, let $\mathcal{M}(\mathcal{G})$ be the set of strictly positive densities $f: \mathbb{R}^{d} \rightarrow \mathbb{R}$ such that</p>
<p>$$
f\left(x_{1}, \cdots, x_{d}\right)=\prod_{j} f_{j}\left(x_{j} \mid x_{x_{j}^{d}}\right)
$$</p>
<p>where $\int_{\mathbb{R}} f_{j}\left(x_{j} \mid x_{\pi_{j}^{g}}\right) d m\left(x_{j}\right)=1$ for all $x_{\pi_{j}^{g}} \in \mathbb{R}^{\left|\pi_{j}^{g}\right|}$ and all $j \in[d]$, where $m$ is the Lebesgue measure on $\mathbb{R}$.</p>
<p>Next proposition is adapted from Lauritzen [28, Theorem 3.27]. It relates the factorization of (13) to d-separation statements.</p>
<p>Proposition 4 For a DAG $\mathcal{G}$ and a strictly positive density $f,{ }^{3}$ we have $f \in \mathcal{M}(\mathcal{G})$ if and only if for any disjoint sets $A, B, C \subset V$ we have</p>
<p>$$
A \Perp_{\mathcal{G}} B \mid C \Longrightarrow X_{A} \Perp_{f} X_{B} \mid X_{C}
$$</p>
<p>Definition 5 For a DAG $\mathcal{G}$ and an interventional family $\mathcal{I}$, let</p>
<p>$$
\mathcal{M}<em _in_K_="\in[K]" k="k">{\mathcal{I}}(\mathcal{G}):=\left{\left(f^{(k)}\right)</em>\right)\right}
$$} \mid \forall k \in[K], f^{(k)} \in \mathcal{M}(\mathcal{G}) \text { and } \forall j \notin I_{k}, f_{j}^{(k)}\left(x_{j} \mid x_{\pi_{j}^{g}}\right)=f_{j}^{(1)}\left(x_{j} \mid x_{\pi_{j}^{g}</p>
<p>Definition 5 defines a set $\mathcal{M}<em _in_K_="\in[K]" k="k">{\mathcal{I}}(\mathcal{G})$ which contains all the sets of distributions $\left(f^{(k)}\right)</em>}$ which are coherent with the definition of interventions provided at Equation (2). ${ }^{4}$ Note that the assumption of causal sufficiency is implicit to this definition of interventions. Analogously to the observational case, two different DAGs $\mathcal{G<em 2="2">{1}$ and $\mathcal{G}</em>$ can induce the same interventional distributions.</p>
<p>Definition 6 ( $\mathcal{I}$-Markov Equivalence Class) Two DAGs $\mathcal{G}<em 2="2">{1}$ and $\mathcal{G}</em>}$ are $\mathcal{I}$-Markov equivalent iff $\mathcal{M<em 1="1">{\mathcal{I}}\left(\mathcal{G}</em>}\right)=\mathcal{M<em 2="2">{\mathcal{I}}\left(\mathcal{G}</em>}\right)$. We denote by $\mathcal{I}$-MEC $\left(\mathcal{G<em 1="1">{1}\right)$ the set of all DAGs which are $\mathcal{I}$-Markov equivalent to $\mathcal{G}</em>$.}$, this is the $\mathcal{I}$-Markov equivalence class of $\mathcal{G}_{1</p>
<p>We now define an augmented graph containing exactly one node for each intervention $k$.
Definition 7 Given a DAG $\mathcal{G}$ and an interventional family $\mathcal{I}$, the associated $\mathcal{I}$-DAG, denoted by $\mathcal{G}^{\mathcal{I}}$, is the graph $\mathcal{G}$ augmented with nodes $\zeta_{k}$ and edges $\zeta_{k} \rightarrow i$ for all $k \in[K] \backslash{1}$ and all $i \in I_{k}$.</p>
<p>In the observational case, we say that a distribution $f$ has the Markov property w.r.t. a graph $\mathcal{G}$ if whenever some d-separation holds in the graph, the corresponding conditional independence holds in $f$. We now define the $\mathcal{I}$-Markov property, which generalizes this idea to interventions. This property is important since it holds in causal graphical models, as Proposition 9 states.</p>
<p>Definition 8 ( $\mathcal{I}$-Markov property) Let $\mathcal{I}$ be interventional family such that $I_{1}:=\emptyset$ and $\left(f^{(k)}\right)<em _in_K_="\in[K]" k="k">{k \in[K]}$ be a family of strictly positive densities over $X$. We say that $\left(f^{(k)}\right)</em>$ iff}$ satisfies the $\mathcal{I}$-Markov property w.r.t. the $\mathcal{I}$-DAG $\mathcal{G}^{\mathcal{I}</p>
<ol>
<li>For any disjoint $A, B, C \subset V, A \Perp_{\mathcal{G}} B \mid C$ implies $X_{A} \Perp_{f^{(k)}} X_{B} \mid X_{C}$ for all $k \in[K]$.</li>
<li>For any disjoint $A, C \subset V$ and $k \in[K] \backslash{1}$,
$A \Perp_{\mathcal{G}^{\mathcal{I}}} \zeta_{k} \mid C \cup \zeta_{-k}$ implies $f^{(k)}\left(X_{A} \mid X_{C}\right)=f^{(1)}\left(X_{A} \mid X_{C}\right)$, where $\zeta_{-k}:=\zeta_{[K] \backslash{1, k}}$.
The next proposition relates the definition of interventions with the $\mathcal{I}$-Markov property that we just defined.</li>
</ol>
<p>Proposition 9 (Yang et al. [48]) Suppose the interventional family $\mathcal{I}$ is such that $I_{1}:=\emptyset$. Then $\left(f^{(k)}\right)<em _mathcal_I="\mathcal{I">{k \in[K]} \in \mathcal{M}</em>$.}}(\mathcal{G})$ iff $\left(f^{(k)}\right)_{k \in[K]}$ is $\mathcal{I}$-Markov to $\mathcal{G}^{\mathcal{I}</p>
<p>The next theorem gives a graphical characterization of $\mathcal{I}$-Markov equivalence classes, which will be crucial in the proof of Theorem 1.</p>
<p>Theorem 10 (Yang et al. [48]) Suppose the interventional family $\mathcal{I}$ is such that $I_{1}:=\emptyset$. Two DAGs $\mathcal{G}<em 2="2">{1}$ and $\mathcal{G}</em>}$ are $\mathcal{I}$-Markov equivalent iff their $\mathcal{I}$-DAGs $\mathcal{G<em 2="2">{1}^{\mathcal{I}}$ and $\mathcal{G}</em>$ share the same skeleton and immoralities.}^{\mathcal{I}</p>
<p>See Figure 5 for a simple illustration of this concept. We now present a very simple corollary which gives a situation where the $\mathcal{I}$-Markov equivalence class contains a unique graph.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Different $\mathcal{I}$-DAGs with a single intervention. The first graph is alone in its $\mathcal{I}$-Markov equivalence class since reversing the $1 \rightarrow 2$ edge would break the immorality $1 \rightarrow 2 \leftarrow \zeta$. The second graph is also alone in its equivalence class since reversing $1 \rightarrow 2$ would create a new immorality $\zeta \rightarrow 1 \leftarrow 2$. The third DAG is not alone in its equivalence class since reversing $1 \rightarrow 2$ would preserve the skeleton without adding or removing an immorality. It should become apparent that adding more interventions will likely reduce the size of the $\mathcal{I}$-Markov equivalence class by introducing more immoralities.</p>
<p>Corollary 11 Let $\mathcal{G}$ be a DAG and let $\mathcal{I}=(\emptyset,{1}, \cdots,{d})$. Then $\mathcal{G}$ is alone in its $\mathcal{I}$-Markov equivalence class.</p>
<p>Proof. By Theorem 10, all $\mathcal{I}$-Markov equivalent graphs will share its skeleton with $\mathcal{G}$, so we consider only graphs obtained by reversing edges in $\mathcal{G}$.
Consider any edge $i \rightarrow j$ in $\mathcal{G}$. We note that $i \rightarrow j \leftarrow \zeta_{j+1}$ forms an immorality in the $\mathcal{I}$-DAG $\mathcal{G}^{\mathcal{I}}$. Reversing $i \rightarrow j$ would break this immorality which would imply that the resulting DAG is not $\mathcal{I}$-Markov equivalent to $\mathcal{G}$, by Theorem 10. Hence, $\mathcal{G}$ is alone in its equivalence class.</p>
<h1>A. 2 Proof of Theorem 1</h1>
<p>We are now ready to present the main result of this section. We recall the score function introduced in Section 3.1:</p>
<p>$$
\mathcal{S}_{\mathcal{I}^{<em>}}(\mathcal{G}):=\sup <em k="1">{\phi} \sum</em>^{}^{K} \mathbb{E}_{X \sim p^{(k)}} \log f^{(k)}\left(X ; M^{\mathcal{G}}, R^{\mathcal{I</em>}}, \phi\right)-\lambda|\mathcal{G}|
$$</p>
<p>where</p>
<p>$$
f^{(k)}\left(x ; M^{\mathcal{G}}, R^{\mathcal{I}}, \phi\right):=\prod_{j=1}^{d} \tilde{f}\left(x_{j} ; \operatorname{NN}\left(M_{j}^{\mathcal{G}} \odot x ; \phi_{j}^{(1)}\right)\right)^{1-R_{k j}^{\mathcal{I}}} \tilde{f}\left(x_{j} ; \operatorname{NN}\left(M_{j}^{\mathcal{G}} \odot x ; \phi_{j}^{(k)}\right)\right)^{R_{k j}^{\mathcal{I}}}
$$</p>
<p>Recall that $\left(p^{(k)}\right)<em _mathcal_G="\mathcal{G">{k \in[K]}$ are the ground truth interventional distributions with ground truth graph $\mathcal{G}^{<em>}$ and ground truth interventional family $\mathcal{I}^{</em>}$. We will sometimes use the notation $f</em>} \mathcal{I} \phi}^{(k)}(x)$ to refer to $f^{(k)}\left(x ; M^{\mathcal{G}}, R^{\mathcal{I}}, \phi\right)$. We define $\mathcal{F<em _in_K_="\in[K]" k="k">{\mathcal{I}}(\mathcal{G})$ to be the set of all $\left(f^{(k)}\right)</em>$ which are expressible by the model specified in Equation (15). More precisely,</p>
<p>$$
\mathcal{F}<em _in_K_="\in[K]" k="k">{\mathcal{I}}(\mathcal{G}):=\left{\left(f^{(k)}\right)</em>\right}
$$} \mid \exists \phi \text { s.t. } \forall k \in[K] f^{(k)}=f_{\mathcal{G} \mathcal{I} \phi}^{(k)</p>
<p>Theorem 1 relies on four assumptions. The first one requires that the model is expressive enough to represent the ground truth distributions exactly.</p>
<p>Assumption 1 (Sufficient capacity) The ground truth interventional distributions $P^{(k)}$ all have a density $p^{(k)}$ w.r.t. the Lebesgue measure on $\mathbb{R}^{n}$ such that $\left(p^{(k)}\right)<em _mathcal_I="\mathcal{I">{k \in[K]} \in \mathcal{F}</em>^{<em>}}\left(\mathcal{G}^{</em>}\right)$, i.e. the model specified in Equation (15) is expressive enough to represent the ground truth distributions.</p>
<p>The second assumption is a generalization of faithfulness to interventions.</p>
<h2>Assumption 2 ( $\mathcal{I}^{*}$-Faithfulness)</h2>
<ol>
<li>For any disjoint $A, B, C \subset V$,</li>
</ol>
<p>$$
A \nmid \mathcal{G}<em A="A">{<em>}^{</em>} B \mid C \text { implies } X</em>
$$} \nmid p^{(1)} X_{B} \mid X_{C</p>
<ol>
<li>For any disjoint $A, C \subset V$ and $k \in[K]$,</li>
</ol>
<p>$$
A \Perp_{\mathcal{G}^{<em>} \mathcal{I}^{</em>}} \zeta_{k} \mid C \cup \zeta_{-k} \text { implies } p^{(k)}\left(X_{A} \mid X_{C}\right) \neq p^{(1)}\left(X_{A} \mid X_{C}\right)
$$</p>
<p>The first condition of Assumption 2 is exactly the standard faithfulness assumption for the ground truth observational distribution. The second condition is simply the converse of the second condition in the $\mathcal{I}$-Markov property (Definition 8) and can be understood as avoiding pathological interventions to make sure that every variables that can be potentially affected by the intervention are indeed affected. The simplest case is when $I_{k}:={j}, A:={j}$ and $C:=\pi_{j}^{\mathcal{G}^{<em>}}$. In this case the condition requires that the intervention actually change something. Another simple case is when $C:=\emptyset$. In this case, the condition requires that all descendants are affected, in the sense that their marginals change.
As we just saw, a trivial violation of $\mathcal{I}^{</em>}$-faithfulness would be when the intervention is not changing anything, not even the targeted conditional. We now present a non-trivial violation of $\mathcal{I}^{*}$-faithfulness.</p>
<p>Example 12 ( $\mathcal{I}^{<em>}$-Faithfulness violation) Suppose $\mathcal{G}^{</em>}$ is $X_{1} \rightarrow X_{2}$ where both variables are binary. Assume $p^{(1)}\left(X_{1}=1\right)=\frac{1}{2}, p^{(1)}\left(X_{2}=1 \mid X_{1}=0\right)=\frac{1}{4}$ and $p^{(1)}\left(X_{2}=1 \mid X_{1}=1\right)=\frac{3}{4}$. From this, we can compute $p^{(1)}\left(X_{2}=1\right)=\frac{1}{2}$. Consider the intervention targeting only $X_{2}$ which changes its conditional to $p^{(2)}\left(X_{2}=1 \mid X_{1}=0\right)=\frac{3}{4}$ and $p^{(2)}\left(X_{2}=1 \mid X_{1}=1\right)=\frac{1}{4}$. So the interventional family is $\mathcal{I}^{<em>}=(\emptyset,{2})$. A simple computation shows the new marginal on $X_{2}$ has not changed, i.e. $p^{(2)}\left(X_{2}\right)=p^{(1)}\left(X_{2}\right)$. This is a violation of $\mathcal{I}^{</em>}$-faithfulness since clearly $X_{2}$ is not d-separated from the interventional node $\zeta_{2}$ in $\mathcal{G}^{<em> \mathcal{I}^{</em>}}$.</p>
<p>The third assumption is a technicality to simplify the presentation of the proofs and to follow the presentation of Yang et al. [48]: we require the density model to be strictly positive.</p>
<p>Assumption 3 (Strict positivity) For all $k \in[K]$, the model density $f^{(k)}\left(x ; M^{\mathcal{G}}, R^{\mathcal{I}}, \phi\right)$ is strictly positive for all $\phi, D A G \mathcal{G}$ and interventional family $\mathcal{I}$.</p>
<p>Note that Assumption 3 is satisfied for example when for all $\theta$ in the image of NN, the density $\tilde{f}(\cdot ; \theta)$ is strictly positive. This happens when using a Gaussian density with variance strictly positive or a deep sigmoidal flow.
From Equation (16) and Assumption 3, it should be clear that $\mathcal{F}<em _mathcal_I="\mathcal{I">{\mathcal{I}}(\mathcal{G}) \subset \mathcal{M}</em>}}(\mathcal{G})$ (recall $\mathcal{M<em _in_K_="\in[K]" k="k">{\mathcal{I}}(\mathcal{G})$ contains only strictly positive densities). Thus, from Proposition 9 we see that the $\mathcal{I}$-Markov property holds for all $\left(f^{(k)}\right)</em>)$. This fact will be useful in the proof of Theorem 1.} \in \mathcal{F}_{\mathcal{I}}(\mathcal{G</p>
<p>The fourth assumption is purely technical. It requires the differential entropy of the densities $p^{(k)}$ to be finite, which, as we will see in Lemma 13, ensures that the score of the ground truth graph $\mathcal{S}<em _mathcal_I="\mathcal{I">{\mathcal{I}^{<em>}}\left(\mathcal{G}^{</em>}\right)$ is finite. This will be important to ensure that the score of any other graphs can be compared to it. In particular, this is avoiding the hypothetical situation where $\mathcal{S}</em>^{<em>}}\left(\mathcal{G}^{</em>}\right)$ and $\mathcal{S}_{\mathcal{I}^{*}}(\mathcal{G})$ are both equal to infinity, which means they cannot be easily compared without defining a specific limiting process.</p>
<p>Assumption 4 (Finite differential entropies) For all $k \in[K]$,</p>
<p>$$
\left|\mathbb{E}_{p^{(k)}} \log p^{(k)}(X)\right|&lt;\infty
$$</p>
<p>Lemma 13 (Finite scores) Under Assumptions 1 \&amp; 4, $\left|\mathcal{S}<em _mathcal_G="\mathcal{G">{\mathcal{I}^{<em>}}\left(\mathcal{G}^{</em>}\right)\right|&lt;\infty$.
Proof. Consider the Kullback-Leibler divergence between $p^{(k)}$ and $f</em>^{<em>} \mathcal{I}^{</em>} \phi}^{(k)}$ for an arbitrary $\phi$.</p>
<p>$$
0 \leq D_{K L}\left(p^{(k)} | f_{\mathcal{G}^{<em>} \mathcal{I}^{</em>} \phi}^{(k)}\right)=\mathbb{E}<em p_k_="p^{(k)">{p^{(k)}} \log p^{(k)}(X)-\mathbb{E}</em>^{}} \log f_{\mathcal{G<em>} \mathcal{I}^{</em>} \phi}^{(k)}(X)
$$</p>
<p>where we applied the linearity of the expectation (which holds because $\left|\mathbb{E}_{p^{(k)}} \log p^{(k)}(X)\right|&lt;\infty$ ). We thus have that</p>
<p>$$
\mathbb{E}<em _mathcal_G="\mathcal{G">{p^{(k)}} \log f</em>^{<em>} \mathcal{I}^{</em>} \phi}^{(k)}(X) \leq \mathbb{E}_{p^{(k)}} \log p^{(k)}(X)&lt;\infty
$$</p>
<p>Thus, $\sup <em p_k_="p^{(k)">{\phi} \mathbb{E}</em>^{}} \log f_{\mathcal{G<em>} \mathcal{I}^{</em>} \phi}^{(k)}(X)&lt;\infty$, which implies $\mathcal{S}_{\mathcal{I}^{<em>}}\left(\mathcal{G}^{</em>}\right)&lt;\infty$.</p>
<p>By the assumption of sufficient capacity, there exists some $\phi^{<em>}$ such that $f_{\mathcal{G}^{</em>} \phi^{<em>}}^{(k)}=p^{(k)}$ for all $k$, hence $\sup <em k="1">{\phi} \sum</em>}^{K} \mathbb{E<em _mathcal_G="\mathcal{G">{p^{(k)}} \log f</em>^{</em>} \mathcal{I}^{<em>} \phi}^{(k)}(X) \geq \sum_{k=1}^{K} \mathbb{E}<em _mathcal_G="\mathcal{G">{p^{(k)}} \log f</em>^{</em>} \phi^{<em>}}^{(k)}(X)=\sum_{k=1}^{K} \mathbb{E}<em _mathcal_I="\mathcal{I">{p^{(k)}} \log p^{(k)}(X)&gt;-\infty$. This implies that $\mathcal{S}</em>^{</em>}}\left(\mathcal{G}^{*}\right)&gt;-\infty$.</p>
<p>The next lemma shows that the difference $\mathcal{S}<em _mathcal_I="\mathcal{I">{\mathcal{I}^{<em>}}\left(\mathcal{G}^{</em>}\right)-\mathcal{S}</em>)$ can be rewritten as a minimization of a sum of KL divergences plus the difference in regularizing terms.}^{*}}(\mathcal{G</p>
<p>Lemma 14 (Rewriting of score differences) Under Assumption 1 \&amp; 4, we have</p>
<p>$$
\mathcal{S}<em _mathcal_I="\mathcal{I">{\mathcal{I}^{<em>}}\left(\mathcal{G}^{</em>}\right)-\mathcal{S}</em>^{<em>}}(\mathcal{G})=\inf <em _in_K_="\in[K]" k="k">{\phi} \sum</em>^{} D_{K L}\left(p^{(k)} | f_{\mathcal{G}_{\mathcal{I</em>}} \phi}^{(k)}\right)+\lambda\left(|\mathcal{G}|-\left|\mathcal{G}^{*}\right|\right)
$$</p>
<p>Proof. By Lemma 13, we have that $\left|\mathcal{S}<em _mathcal_I="\mathcal{I">{\mathcal{I}^{<em>}}\left(\mathcal{G}^{</em>}\right)\right|&lt;\infty$, which ensures the difference $\mathcal{S}</em>^{<em>}}\left(\mathcal{G}^{</em>}\right)-\mathcal{S}_{\mathcal{I}^{*}}(\mathcal{G})$ is well defined.</p>
<p>$$
\begin{aligned}
&amp; \mathcal{S}<em _mathcal_I="\mathcal{I">{\mathcal{I}^{<em>}}\left(\mathcal{G}^{</em>}\right)-\mathcal{S}</em>^{<em>}}(\mathcal{G}) \
= &amp; \mathcal{S}_{\mathcal{I}^{</em>}}\left(\mathcal{G}^{<em>}\right)-\sum_{k \in[K]} \mathbb{E}<em _mathcal_I="\mathcal{I">{p^{(k)}} \log p^{(k)}(X)-\mathcal{S}</em>^{</em>}}(\mathcal{G})+\sum_{k \in[K]} \mathbb{E}<em _phi="\phi">{p^{(k)}} \log p^{(k)}(X) \
= &amp; \sup </em>} \sum_{k \in[K]} \mathbb{E<em _mathcal_G="\mathcal{G">{p^{(k)}} \log f</em>^{<em>} \mathcal{I}^{</em>} \phi}^{(k)}(X)-\sum_{k \in[K]} \mathbb{E}<em _phi="\phi">{p^{(k)}} \log p^{(k)}(X) \
&amp; -\sup </em>} \sum_{k \in[K]} \mathbb{E<em _mathcal_G="\mathcal{G">{p^{(k)}} \log f</em><em _phi="\phi">{\mathcal{I}^{<em>}}^{(k)}}^{(k)}(X)+\sum_{k \in[K]} \mathbb{E}_{p^{(k)}} \log p^{(k)}(X) \
&amp; +\lambda\left(|\mathcal{G}|-\left|\mathcal{G}^{</em>}\right|\right) \
= &amp; \inf </em>}-\sum_{k \in[K]} \mathbb{E<em _mathcal_G="\mathcal{G">{p^{(k)}} \log f</em><em _phi="\phi">{\mathcal{I}^{<em>}}^{(k)}}^{(k)}(X)+\sum_{k \in[K]} \mathbb{E}<em _phi="\phi">{p^{(k)}} \log p^{(k)}(X) \
&amp; -\inf </em>}-\sum_{k \in[K]} \mathbb{E<em _mathcal_G="\mathcal{G">{p^{(k)}} \log f</em>^{</em>} \mathcal{I}^{<em>} \phi}^{(k)}(X)-\sum_{k \in[K]} \mathbb{E}_{p^{(k)}} \log p^{(k)}(X) \
&amp; +\lambda\left(|\mathcal{G}|-\left|\mathcal{G}^{</em>}\right|\right) \
= &amp; \inf </em>^{} \sum_{k \in[K]} D_{K L}\left(p^{(k)} | f_{\mathcal{G}_{\mathcal{I<em>}}^{(k)}}^{(k)}\right)-\inf <em _in_K_="\in[K]" k="k">{\phi} \sum</em>^{} D_{K L}\left(p^{(k)} | f_{\mathcal{G</em>} \mathcal{I}^{<em>} \phi}^{(k)}\right) \
&amp; +\lambda\left(|\mathcal{G}|-\left|\mathcal{G}^{</em>}\right|\right)
\end{aligned}
$$</p>
<p>The first equality holds since by Assumption 4 the differential entropy of $p^{(k)}$ is finite for all $k$. In (24), we use the linearity of the expectation, which holds because the entropy term is finite. By Assumption 1, $\left(p^{(k)}\right)<em _mathcal_I="\mathcal{I">{k \in[K]} \in \mathcal{F}</em>^{<em>}}\left(\mathcal{G}^{</em>}\right)$ which implies that $\inf <em _in_K_="\in[K]" k="k">{\phi} \sum</em>^{} D_{K L}\left(p^{(k)} | f_{\mathcal{G<em>} \mathcal{I}^{</em>} \phi}^{(k)}\right)=0$.</p>
<p>We will now prove three technical lemmas (Lemma 15, 16 \&amp; 18). Their proof can be safely skipped during a first reading.
Lemma 15 is adapted from Koller and Friedman [25, Theorem 8.7] to handle cases where infinite differential entropies might arise.</p>
<p>Lemma 15 Let $\mathcal{G}$ be a DAG. If $p \notin \mathcal{M}(\mathcal{G})$ and $p(x)&gt;0$ for all $x \in \mathbb{R}^{d}$, then</p>
<p>$$
\inf <em K="K" L="L">{f \in \mathcal{M}(\mathcal{G})} D</em>(p | f)&gt;0
$$</p>
<p>Proof. We consider a new density function defined as</p>
<p>$$
\hat{f}(x):=\prod_{j=1}^{d} p\left(x_{j} \mid x_{\pi_{j}^{0}}\right)
$$</p>
<p>where</p>
<p>$$
p\left(x_{j} \mid x_{\pi_{j}^{0}}\right):=\frac{p\left(x_{j}, x_{\pi_{j}^{0}}\right)}{p\left(x_{\pi_{j}^{0}}\right)}
$$</p>
<p>i.e. it is the conditional density. This should not be conflated with $p\left(x_{j} \mid x_{\pi_{j}^{\mathcal{G}^{*}}}\right)$. It should be clear from (25) and the fact that $p$ is strictly positive that $\hat{f} \in \mathcal{M}(\mathcal{G})$ hence $p \neq \hat{f}$. We will show that $\hat{f} \in \arg \min <em K="K" L="L">{f \in \mathcal{M}(\mathcal{G})} D</em>(p | f)$.
Pick an arbitrary $f \in \mathcal{M}(\mathcal{G})$. We first show that $\mathbb{E}_{p} \log \frac{\hat{f}(X)}{f(X)}$ can be written as a sum of KL divergences.</p>
<p>$$
\begin{aligned}
\mathbb{E}<em p="p">{p} \log \frac{\hat{f}(X)}{f(X)} &amp; =\mathbb{E}</em> \
&amp; =\sum_{j=1}^{d} \mathbb{E}} \sum_{j=1}^{d} \log \frac{p\left(X_{j} \mid X_{\pi_{j}^{\mathcal{G}}}\right)}{f\left(X_{j} \mid X_{\pi_{j}^{\mathcal{G}}}\right)<em j="j">{p} \log \frac{p\left(X</em>
\end{aligned}
$$} \mid X_{\pi_{j}^{\mathcal{G}}}\right)}{f\left(X_{j} \mid X_{\pi_{j}^{\mathcal{G}}}\right)</p>
<p>In Equation (28), we apply the linearity of the Lebesgue integral, which holds as long as we are not summing infinities of opposite signs (in which case the sum is undefined). ${ }^{5}$ We now show that it is not the case since each term is an expectation of a KL divergence, which is in $[0,+\infty]$ :</p>
<p>$$
\begin{aligned}
\mathbb{E}<em j="j">{p} \log \frac{p\left(X</em> \
&amp; =\int p\left(x_{\pi_{j}^{\mathcal{G}}}\right) D_{K L}\left(p\left(\cdot_{j} \mid x_{\pi_{j}^{\mathcal{G}}}\right) | f\left(\cdot_{j} \mid x_{\pi_{j}^{\mathcal{G}}}\right)\right)\right)
\end{aligned}
$$} \mid X_{\pi_{j}^{\mathcal{G}}}\right)}{f\left(X_{j} \mid X_{\pi_{j}^{\mathcal{G}}}\right)} &amp; =\int p\left(x_{\pi_{j}^{\mathcal{G}}}\right) \int p\left(x_{j} \mid x_{\pi_{j}^{\mathcal{G}}}\right) \log \frac{p\left(x_{j} \mid x_{\pi_{j}^{\mathcal{G}}}\right)}{f\left(x_{j} \mid x_{\pi_{j}^{\mathcal{G}}}\right)} d x_{j} d x_{\pi_{j}^{\mathcal{G}}</p>
<p>This implies that $\mathbb{E}<em _in="\in" _mathcal_M="\mathcal{M" f="f">{p} \log \frac{\hat{f}(X)}{f(X)} \in[0,+\infty]$. We can now show that $\hat{f} \in \arg \min </em>(p | f)$ :}(\mathcal{G})} D_{K L</p>
<p>$$
\begin{aligned}
D_{K L}(p | f) &amp; =\mathbb{E}<em p="p">{p} \log \frac{p(X)}{\hat{f}(X)} \frac{\hat{f}(X)}{f(X)} \
&amp; =\mathbb{E}</em>} \log \frac{p(X)}{\hat{f}(X)}+\mathbb{E<em K="K" L="L">{p} \log \frac{\hat{f}(X)}{f(X)} \
&amp; =D</em>}(p | \hat{f})+\mathbb{E<em K="K" L="L">{p} \log \frac{\hat{f}(X)}{f(X)} \
&amp; \geq D</em>)&gt;0
\end{aligned}
$$}(p | \hat{f</p>
<p>Equation (32) holds as long as we do not have $\infty-\infty$. It is not the case here since (i) the first term is a KL divergence, so it is in $[0,+\infty]$, and (ii) the second term was already shown to be in $[0,+\infty]$. The very last inequality holds because $p \neq \hat{f}$.
We conclude by noting that $\inf <em K="K" L="L">{f \in \mathcal{M}(\mathcal{G})} D</em>)&gt;0$.
The following lemma will make use of the following definition:}(p | f)=D_{K L}(p | \hat{f</p>
<p>$$
\mathcal{Z}(j, A):=\left{\left(f^{(1)}, f^{(2)}\right) \mid f^{(1)}\left(x_{j} \mid x_{A}\right)=f^{(2)}\left(x_{j} \mid x_{A}\right) \text { and } f^{(1)}, f^{(2)}&gt;0\right}
$$</p>
<p>Lemma 16 Let $j \in V$ and $A \subset V \backslash{j}$. If $\left(p^{(1)}, p^{(2)}\right) \notin \mathcal{Z}(j, A)$ and both $p^{(1)}$ and $p^{(2)}$ are strictly positive, then</p>
<p>$$
\inf <em K="K" L="L">{\left(f^{(1)}, f^{(2)}\right) \in \mathcal{Z}(j, A)} D</em>\right)&gt;0
$$}\left(p^{(1)} | f^{(1)}\right)+D_{K L}\left(p^{(2)} | f^{(2)</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Proof. The proof is very similar in spirit to the proof of Lemma 15.
We define new density functions:</p>
<p>$$
\begin{aligned}
&amp; p^{\text {mid }}(x):=\frac{p^{(1)}(x)+p^{(2)}(x)}{2} \
&amp; \hat{f}^{(k)}(x):=p^{(k)}\left(x_{A}\right) p^{\text {mid }}\left(x_{j} \mid x_{A}\right) p^{(k)}\left(x_{V \backslash A \backslash j} \mid x_{A \cup j}\right) \forall k \in{1,2}
\end{aligned}
$$</p>
<p>We note that $p^{\text {mid }}, \hat{f}^{(1)}$ and $\hat{f}^{(2)}$ are strictly positive since $p^{(1)}$ and $p^{(2)}$ are strictly positive. By construction, we have $\hat{f}^{(1)}\left(x_{j} \mid x_{A}\right)=\hat{f}^{(2)}\left(x_{j} \mid x_{A}\right)$, and thus $\left(\hat{f}^{(1)}, \hat{f}^{(2)}\right) \in \mathcal{Z}(i, A)$. This means that $\hat{f}^{(1)} \neq p^{(1)}$ or $\hat{f}^{(2)} \neq p^{(2)}$.
Pick an arbitrary $\left(f^{(1)}, f^{(2)}\right) \in \mathcal{Z}(j, A)$. We start by showing that the integral $\int p^{(1)}(x) \log \frac{\hat{f}^{(1)}(x)}{f^{(1)}(x)}+p^{(2)}(x) \log \frac{\hat{f}^{(2)}(x)}{f^{(2)}(x)} d x$ is in $[0,+\infty]$.</p>
<p>$$
\begin{aligned}
&amp; \int p^{(1)}(x) \log \frac{\hat{f}^{(1)}(x)}{f^{(1)}(x)}+p^{(2)}(x) \log \frac{\hat{f}^{(2)}(x)}{f^{(2)}(x)} d x \
= &amp; \int p^{(1)}(x)\left[\log \frac{p^{(1)}\left(x_{A}\right)}{f^{(1)}\left(x_{A}\right)}+\log \frac{p^{\text {mid }}\left(x_{j} \mid x_{A}\right)}{f^{(1)}\left(x_{j} \mid x_{A}\right)}+\log \frac{p^{(1)}\left(x_{V \backslash A \backslash j} \mid x_{A \cup j}\right)}{f^{(1)}\left(x_{V \backslash A \backslash j} \mid x_{A \cup j}\right)}\right] \
&amp; +p^{(2)}(x)\left[\log \frac{p^{(2)}\left(x_{A}\right)}{f^{(2)}\left(x_{A}\right)}+\log \frac{p^{\text {mid }}\left(x_{j} \mid x_{A}\right)}{f^{(1)}\left(x_{j} \mid x_{A}\right)}+\log \frac{p^{(2)}\left(x_{V \backslash A \backslash j} \mid x_{A \cup j}\right)}{f^{(2)}\left(x_{V \backslash A \backslash j} \mid x_{A \cup j}\right)}\right] d x \
= &amp; D_{K L}\left(p^{(1)}\left(\cdot_{A}\right) | f^{(1)}\left(\cdot_{A}\right)\right)+\mathbb{E}<em K="K" L="L">{p^{(1)}} D</em>\right)\right) \
&amp; +D_{K L}\left(p^{(2)}\left(\cdot_{A}\right) | f^{(2)}\left(\cdot_{A}\right)\right)+\mathbb{E}}\left(p^{(1)}\left(\cdot_{V \backslash A \backslash j} \mid X_{A \cup j}\right) | f^{(1)}\left(\cdot_{V \backslash A \backslash j} \mid X_{A \cup j<em K="K" L="L">{p^{(2)}} D</em>\right)\right) \
&amp; +2 \underbrace{\int p^{(1)}(x)+p^{(2)}(x)}}\left(p^{(2)}\left(\cdot_{V \backslash A \backslash j} \mid X_{A \cup j}\right) | f^{(2)}\left(\cdot_{V \backslash A \backslash j} \mid X_{A \cup j<em _mid="{mid" p_text="p^{\text">{=\mathbb{E}</em>}}} D_{K L}\left(p^{\text {mid }}\left(\cdot_{j} \mid x_{A}\right)\right.} d x<em _mid="{mid" p_text="p^{\text">{=\mathbb{E}</em>
\end{aligned}
$$}}} D_{K L}\left(p^{\text {mid }}\left(\cdot_{j} \mid x_{A}\right)\right.</p>
<p>In (39), we used the fact that $f^{(1)}\left(x_{j} \mid x_{A}\right)=f^{(2)}\left(x_{j} \mid x_{A}\right)$. In (40), we use the linearity of the integral (which can be safely apply because each resulting "piece" is in $[0,+\infty]$ ). Since each term in (40) is in $[0,+\infty]$, their sum is in $[0,+\infty]$ as well.
We can now look at the sum of KL-divergences we are interested in.</p>
<p>$$
\begin{aligned}
&amp; D_{K L}\left(p^{(1)} | f^{(1)}\right)+D_{K L}\left(p^{(2)} | f^{(2)}\right) \
&amp; =\int p^{(1)}(x) \log \frac{p^{(1)}}{f^{(1)}} d x+\int p^{(2)}(x) \log \frac{p^{(2)}}{f^{(2)}} d x \
&amp; =\int p^{(1)}(x) \log \frac{p^{(1)}}{f^{(1)}}+p^{(2)}(x) \log \frac{p^{(2)}}{f^{(2)}} d x \
&amp; =\int p^{(1)}(x) \log \frac{p^{(1)}(x)}{f^{(1)}(x)}+p^{(1)}(x) \log \frac{\hat{f}^{(1)}(x)}{f^{(1)}(x)}+p^{(2)}(x) \log \frac{p^{(2)}(x)}{\hat{f}^{(2)}(x)}+p^{(2)}(x) \log \frac{\hat{f}^{(2)}(x)}{f^{(2)}(x)} d x \
&amp; =D_{K L}\left(p^{(1)} | \hat{f}^{(1)}\right)+D_{K L}\left(p^{(2)} | \hat{f}^{(2)}\right)+\int p^{(1)}(x) \log \frac{\hat{f}^{(1)}(x)}{f^{(1)}(x)}+p^{(2)}(x) \log \frac{\hat{f}^{(2)}(x)}{f^{(2)}(x)} d x \
&amp; \geq D_{K L}\left(p^{(1)} | \hat{f}^{(1)}\right)+D_{K L}\left(p^{(2)} | \hat{f}^{(2)}\right)&gt;0
\end{aligned}
$$</p>
<p>In (42), we use the linearity of the integral (which can be safely applied given the initial integrals were in $[0,+\infty])$. In (44), we again use the linearity of the integral (which is, again, possible because each resulting piece are in $[0,+\infty])$. In (45), we use the fact that $\int p^{(1)}(x) \log \frac{\hat{f}^{(1)}(x)}{f^{(1)}(x)}+$ $p^{(2)}(x) \log \frac{\hat{f}^{(2)}(x)}{f^{(2)}(x)} d x \in[0,+\infty]$ to get the $\geq$ while the strict inequality holds because either $\hat{f}^{(1)} \neq$ $p^{(1)}$ or $\hat{f}^{(k)} \neq p^{(k)}$.
This implies that</p>
<p>$$
\inf <em K="K" L="L">{\left(f^{(1)}, f^{(2)}\right) \in \mathcal{Z}(j, A)} D</em>\right)&gt;0
$$}\left(p^{(1)} | f^{(1)}\right)+D_{K L}\left(p^{(2)} | f^{(2)}\right)=D_{K L}\left(p^{(1)} | \hat{f}^{(1)}\right)+D_{K L}\left(p^{(2)} | \hat{f}^{(2)</p>
<p>The following definition will be useful for the next lemma.</p>
<p>Definition 17 Given a DAG $\mathcal{G}$ with node set $V$ and two nodes $i, j \in V$, we define the following sets:</p>
<p>$$
\begin{aligned}
&amp; T_{i j}^{\mathcal{G}}:={\ell \in V \mid \text { the immorality } i \rightarrow \ell \leftarrow j \text { is in } \mathcal{G}} \
&amp; L_{i j}^{\mathcal{G}}:=\mathbf{D E}<em i="i" j="j">{\mathcal{G}}\left(T</em>\right) \cup{i, j}
\end{aligned}
$$}^{\mathcal{G}</p>
<p>where $\mathbf{D E}_{\mathcal{G}}(S)$ is the set of descendants of $S$ in $\mathcal{G}$, including $S$ itself.
Lemma 18 Let $\mathcal{G}$ be a DAG with node set $V$. When $i \rightarrow j \notin \mathcal{G}$ and $i \leftarrow j \notin \mathcal{G}$ we have</p>
<p>$$
i \Perp_{\mathcal{G}} j \mid V \backslash L_{i j}^{\mathcal{G}}
$$</p>
<p>Proof: By contradiction. Suppose there is a path from $\left(i=a_{0}, a_{1}, \ldots, a_{p}=j\right)$ with $p&gt;1$ which is not d-blocked by $V \backslash L_{i j}^{\mathcal{G}}$ in $\mathcal{G}$. We first consider the case where the path contains no colliders.
If the path contains no colliders, then $a_{0} \leftarrow a_{1}$ or $a_{p-1} \rightarrow a_{p}$. Moreover, since the path is not d-blocked and both $a_{1}$ and $a_{p-1}$ are not colliders, $a_{1}, a_{p-1} \in L_{i j}^{\mathcal{G}}$. But this implies that there is a directed path from $i=a_{0}$ to $a_{1}$ and a directed path from $j=a_{p}$ to $a_{p-1}$. This creates a directed cycle: either $a_{0} \rightarrow \cdots \rightarrow a_{1} \rightarrow a_{0}$ or $a_{p} \rightarrow \cdots \rightarrow a_{p-1} \rightarrow a_{p}$. This is a contradiction since $\mathcal{G}$ is acyclic.
Suppose there is a collider $a_{k}$, i.e. $a_{k-1} \rightarrow a_{k} \leftarrow a_{k+1}$. Since the path is not d-blocked, there must exists a node $z \in \mathbf{D E}<em k="k">{\mathcal{G}}\left(a</em>$.
We recall Theorem 1 from Section 3.1 and present its proof.
Theorem 1 (Identification via score maximization) Suppose the interventional family $\mathcal{I}^{}\right) \cup\left{a_{k}\right}$ such that $z \notin L_{i j}^{\mathcal{G}}$. If $i=a_{k-1}$ and $j=a_{k+1}$, then clearly $z \in L_{i j}^{\mathcal{G}}$, which is a contradiction. Otherwise, $i \neq a_{k-1}$ or $j \neq a_{k+1}$. Without loss of generality, assume $i \neq a_{k-1}$. Clearly, $a_{k-1}$ is not a collider and since the path is not d-blocked, $a_{k-1} \in L_{i j}^{\mathcal{G}}$. But by definition, $L_{i j}^{\mathcal{G}}$ also contains all the descendants of $a_{k-1}$ including $z$. Again, this is a contradiction with $z \notin L_{i j}^{\mathcal{G}<em>}$ is such that $I_{1}^{</em>}:=\emptyset$. Let $\mathcal{G}^{<em>}$ be the ground truth DAG and $\tilde{\mathcal{G}} \in \arg \max <em _mathcal_I="\mathcal{I">{\mathcal{G} \in D A G} \mathcal{S}</em>^{</em>}}(\mathcal{G})$. Assume that the density model has enough capacity to represent the ground truth distributions, that $\mathcal{I}^{<em>}$-faithfulness holds, that the density model is strictly positive and that the ground truth densities $p^{(k)}$ have finite differential entropy, respectively Assumptions 1, 2, 3 \&amp; 4. Then for $\lambda&gt;0$ small enough, we have that $\tilde{\mathcal{G}}$ is $\mathcal{I}^{</em>}$-Markov equivalent to $\mathcal{G}^{*}$.</p>
<p>Proof. It is sufficient to prove that, for all $\mathcal{G} \notin \mathcal{I}^{<em>}-\operatorname{MEC}\left(\mathcal{G}^{</em>}\right), \mathcal{S}<em _mathcal_I="\mathcal{I">{\mathcal{I}^{<em>}}\left(\mathcal{G}^{</em>}\right)&gt;\mathcal{S}</em>^{<em>}}(\mathcal{G})$. We use Theorem 10 which states that $\tilde{\mathcal{G}}$ is not $\mathcal{I}^{</em>}$-Markov equivalent to $\mathcal{G}^{<em>}$ if and only if $\tilde{\mathcal{G}}^{\mathcal{I}^{</em>}}$ does not share its skeleton or its immoralities with $\mathcal{G}^{<em> \mathcal{I}^{</em>}}$. The proof is organized in six cases. Cases 1-2 treat when $\mathcal{G}$ and $\mathcal{G}^{*}$ do not share the same skeleton, cases $3 \&amp; 4$ when their immoralities differ and cases $5 \&amp; 6$ when their immoralities implying interventional nodes $\zeta_{k}$ differ. In almost every cases, the idea is the same:</p>
<ol>
<li>Use Lemma 18 to find a d-separation which holds in $\mathcal{G}^{\mathcal{I}^{<em>}}$ and show it does not hold in $\mathcal{G}^{</em> \mathcal{I}^{*}}$;</li>
<li>Use the fact that $\mathcal{F}<em _mathcal_I="\mathcal{I">{\mathcal{I}}(\mathcal{G}) \subset \mathcal{M}</em>^{}}(\mathcal{G})$ (by strict positivity), Proposition 9 and the $\mathcal{I<em>}$-faithfulness assumption to obtain an invariance which holds for all $\left(f^{(k)}\right)<em _mathcal_I="\mathcal{I">{k \in[K]} \in \mathcal{F}</em>^{</em>}}(\mathcal{G})$ but not in $\left(p^{(k)}\right)_{k \in[K]}$;</li>
<li>Use the fact that the invariance forces $\inf <em _in_K_="\in[K]" k="k">{\phi} \sum</em>\right)$ to be greater than zero (by Lemma 15 or 16) and;} D_{K L}\left(p^{(k)} | f_{\mathcal{G} \mathcal{I}^{*} \phi}^{(k)</li>
<li>Conclude that $\mathcal{S}<em _mathcal_I="\mathcal{I">{\mathcal{I}^{<em>}}\left(\mathcal{G}^{</em>}\right)&gt;\mathcal{S}</em>)$ via Lemma 14.}^{*}}(\mathcal{G</li>
</ol>
<p>In this proof, we are exclusively referring to $\mathcal{I}^{<em>}$. Thus for notational convenience, we set $\mathcal{I}:=\mathcal{I}^{</em>}$.
Case 1: We consider the graphs $\mathcal{G}$ such that there exists $i \rightarrow j \in \mathcal{G}^{<em>}$ but $i \rightarrow j \notin \mathcal{G}$ and $i \leftarrow j \notin \mathcal{G}$. Let $\mathbb{G}$ be the set of all such $\mathcal{G}$. By Lemma 18, $i \Perp_{\mathcal{G}} j \mid V \backslash L_{i j}^{\mathcal{G}}$ but clearly $i \Perp_{\mathcal{G}^{</em>}} j \mid V \backslash L_{i j}^{\mathcal{G}}$. Hence, by $\mathcal{I}$-faithfulness (Assumption 2) we have $X_{i} \Perp_{p^{(1)}} X_{j} \mid X_{V \backslash L_{i j}^{\mathcal{G}}}$. It implies that $p^{(1)} \notin \mathcal{M}(\mathcal{G})$, by Proposition 4.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{5}$ The linearity of the Lebesgue integral is typically stated for Lebesgue integrable functions $f$ and $g$, i.e. $\int|f|, \int|g|&lt;\infty$. See for example Billingsley [2, Theorem 16.1]. However, it can be extended to cases where $f$ and $g$ are not integrable, as long as $\int f$ and $\int g$ are well-defined and are not infinities of opposite sign (which would yield the undefined expression $\infty-\infty$ ). The proof is a simple adaptation of Theorem 16.1 which makes use of Theorem 15.1 in Billingsley [2].&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-543 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-543</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-543</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-15.html">extraction-schema-15</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <p><strong>Paper ID:</strong> paper-316f980cfd2e217234386166a46eb080bf027cdd</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/316f980cfd2e217234386166a46eb080bf027cdd" target="_blank">Physically Grounded Vision-Language Models for Robotic Manipulation</a></p>
                <p><strong>Paper Venue:</strong> IEEE International Conference on Robotics and Automation</p>
                <p><strong>Paper TL;DR:</strong> It is demonstrated that fine-tuning a VLM on PhysObjects improves its understanding of physical object concepts, including generalization to held-out concepts, by capturing human priors of these concepts from visual appearance.</p>
                <p><strong>Paper Abstract:</strong> Recent advances in vision-language models (VLMs) have led to improved performance on tasks such as visual question answering and image captioning. Consequently, these models are now well-positioned to reason about the physical world, particularly within domains such as robotic manipulation. However, current VLMs are limited in their understanding of the physical concepts (e.g., material, fragility) of common objects, which restricts their usefulness for robotic manipulation tasks that involve interaction and physical reasoning about such objects. To address this limitation, we propose PHYSOBJECTS, an object-centric dataset of 39.6K crowd-sourced and 417K automated physical concept annotations of common household objects. We demonstrate that fine-tuning a VLM on PhysObjects improves its understanding of physical object concepts, including generalization to held-out concepts, by capturing human priors of these concepts from visual appearance. We incorporate this physically grounded VLM in an interactive framework with a large language model-based robotic planner, and show improved planning performance on tasks that require reasoning about physical object concepts, compared to baselines that do not leverage physically grounded VLMs. We additionally illustrate the benefits of our physically grounded VLM on a real robot, where it improves task success rates. We release our dataset and provide further details and visualizations of our results at https://iliad.stanford.edu/pg-vlm/.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e543.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e543.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (LLM planner)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (Generative Pre-trained Transformer 4) used as an LLM-based robotic planner</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large language model (GPT-4) is used as a high-level planner that receives a list of detected objects and available primitive actions, iteratively queries a vision-language model for object-centric physical facts, and outputs a sequence of primitive actions (a plan). The LLM itself has no direct access to raw sensory inputs and reasons over symbolic/probabilistic answers provided by the VLM.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gpt-4 technical report</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A pre-trained large transformer-based language model used zero-shot/few-shot as a planner with a chain-of-thought style prompt; receives textual object detections and VLM-provided answers (text + likelihoods), and emits plans as sequences of high-level primitives. The paper uses GPT-4 with temperature 0 and few-shot chain-of-thought examples encouraging iterative question-asking to the VLM.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Real-scene LLM-based robotic planning evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given an image processed by an object detector (OWL-ViT) the planner receives a list of object detections (letters + category labels), a human instruction, and a library of parameterized primitives (go to object X, pick up object X, bring to human object X, put down object X, done). The LLM iteratively asks a VLM (InstructBLIP / PG-InstructBLIP) object-centric questions (no constraint on question phrasing, but encouraged yes/no), obtains the VLM's top textual answers with likelihoods, and then outputs a numbered plan of primitives to satisfy the human instruction. Plans are evaluated by a human rater for task correctness and, separately, executed on a real Franka Panda for success-rate evaluation (primitives assumed executable).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step planning; object manipulation; instruction following</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>procedural + object-relational (primary); limited spatial reasoning via symbolic constraints (e.g., 'cannot move countertop')</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pre-training on text corpora (LLM priors) plus on-the-fly grounding from VLM answers (PG-InstructBLIP) and object detections from OWL-ViT; few-shot chain-of-thought examples in the prompt steer behavior</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>few-shot chain-of-thought prompting and iterative questioning of a VLM (Socratic interaction); the LLM uses textual/probabilistic answers to produce plans</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Implicit procedural knowledge and commonsense stored in LLM weights (natural-language reasoning); externalized object-relational facts are provided as natural-language answers with likelihood/confidence scores from the VLM; plans are explicit action sequences (lists of primitives).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Task plan accuracy (human-evaluated correctness of produced plans) across 51 real scenarios; real-robot execution success rate (number of successful executions / trials).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Planning accuracy across 51 real scenarios: No VLM baseline 33.3% (text-only planner), InstructBLIP (base VLM) 56.9%, PG-InstructBLIP (physically grounded VLM) 88.2% (Table V). Real-robot execution success (10 tasks total across 2 scenes): InstructBLIP 4/10 (40%), PG-InstructBLIP 9/10 (90%) (Table VII).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>When provided reliable object-centric physical facts from the VLM, GPT-4 successfully composes procedural action sequences and uses object-relational knowledge (mass, fragility, deformability, container affordances) to select appropriate target objects and order primitives (e.g., choose a container that 'can contain liquid', avoid moving large immovable items like countertops). It uses probabilistic/confidence outputs from the VLM to compare candidates and decide which primitives to apply.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Failures occur when the VLM supplies incorrect or overconfident answers (out-of-distribution questions/prompts), or when the task requires grounding in absolute physical quantities or geometric reasoning that the VLM/LLM setup does not provide (e.g., whether an object is too heavy to pick up, precise volume/fit reasoning). The LLM also cannot access raw spatial coordinates/continuous geometry and thus cannot perform fine-grained spatial planning beyond symbolic judgments provided by upstream modules.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Baselines: 'No VLM' planner (LLM only with object list) achieved 33.3% task accuracy; InstructBLIP (off-the-shelf VLM) achieved 56.9% task accuracy. PG-InstructBLIP yielded 88.2% task accuracy. On the real robot: InstructBLIP 4/10 vs PG-InstructBLIP 9/10.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Key ablation at planner level: removing VLM interaction (No VLM) dramatically reduced task accuracy from 88.2% to 33.3%. Replacing PG-InstructBLIP with base InstructBLIP reduced accuracy to 56.9%, indicating the importance of improved object-relational grounding in the VLM. (Paper also reports many ablations for VLM fine-tuning components — see PG-InstructBLIP entry.)</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A language-only planner (GPT-4) can perform embodied multi-step planning without direct sensory input by iteratively querying a VLM for object-relational physical facts and using returned probabilistic answers as symbolic grounding; improving the VLM's object-centric physical reasoning (via fine-tuning on human annotations) substantially increases planning accuracy and real-robot success. The LLM represents procedural knowledge implicitly and relies on explicit VLM-provided object facts (natural-language + confidence scores) to resolve object-selection and affordance reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Physically Grounded Vision-Language Models for Robotic Manipulation', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e543.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e543.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PG-InstructBLIP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Physically Grounded InstructBLIP (InstructBLIP fine-tuned on PhYsObjects)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A vision-language model (InstructBLIP with Flan-T5 backbone) fine-tuned on the PhYsObjects dataset of object-centric physical concept annotations to improve visual reasoning about material, mass, fragility, deformability, transparency, contents, and container affordances; used to answer LLM queries about objects in scenes and provide likelihood/confidence scores.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InstructBLIP (fine-tuned -> PG-InstructBLIP)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Base: InstructBLIP (Flan-T5-XXL backbone, Q-Former visual encoder). Fine-tuned on PhYsObjects (39.6K crowdsourced + 417K automated annotations) by framing categorical concepts as direct answers and continuous concepts as preference-derived binary queries (yes/no). At inference the model returns textual answers with likelihoods; continuous concepts are scored via the ratio s(o,c)=p(yes|o,c)/p(no|o,c) and used with a Bradley-Terry model for pairwise comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Object-centric physical concept recognition for robotic grounding</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given an object's bounding-box image and a natural-language question about a physical concept (e.g., 'Is this object heavy?', 'What is inside this container?', 'Is this container sealed?'), PG-InstructBLIP outputs textual answers plus likelihoods; for continuous concepts trained as preferences it produces p(yes)/p(no) scores that are used to rank objects in pairwise comparisons. These outputs are supplied to the LLM planner to ground object selection and affordance reasoning in manipulation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>object-relational perception; affordance recognition for manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>object-relational (physical properties and affordances); supports procedural planning by informing object selection and constraints</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>fine-tuning on the PhYsObjects dataset (human crowd-sourced and automated annotations) plus original InstructBLIP pretraining on large-scale image-text corpora</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>fine-tuning on labeled concept prompts (categorical and preference pairs); at runtime, prompted QA-style queries per object (sometimes including object category label) and outputs top textual answers with likelihoods/confidence scores</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Implicitly encoded in VLM weights as mappings from visual appearance to physical concepts; categorical concepts returned as discrete labels, continuous concepts represented relationally via log-likelihood ratios log s(o,c) = log p(yes) - log p(no), which serve as scalar scores for pairwise comparisons (Bradley-Terry). Outputs are natural-language answers plus likelihoods (probabilistic soft outputs).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy on PhYsObjects crowd-sourced test set per concept; generalization to held-out concepts; effect on downstream planning accuracy and real-robot success rate.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>PhYsObjects (crowd-sourced main concepts) average test accuracy: PG-InstructBLIP 87.5% vs InstructBLIP 66.2% (Table II). Held-out concepts average: PG-InstructBLIP 71.7% vs InstructBLIP 59.8% (Table III). Robustness to paraphrased prompts and limited VQA degradation reported. Downstream planning: replacing InstructBLIP with PG-InstructBLIP increased task planning accuracy from 56.9% to 88.2% and real-robot success from 40% to 90%.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Improved recognition of object-centric physical concepts that are visually inferable (mass comparisons, fragility, deformability, material, transparency, contents, container affordances). Particularly large gains on contents and container-related concepts. The model generalizes somewhat to held-out but related physical concepts, suggesting learned visual priors about physical properties.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Still makes errors relative to human judgment on some examples; sensitive to out-of-distribution question phrasing (e.g., different forms of the transparency question led to errors); continuous outputs are relational and not grounded in absolute physical units (cannot decide if an object is too heavy to be picked up by the robot); limited geometric reasoning (volume/fit) and some mislabels even after fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to base InstructBLIP (no PhYsObjects fine-tuning), Most Common label, and a Text-Only LLM baseline (InstructGPT with in-context examples), PG-InstructBLIP substantially outperformed them on visual physical concept recognition and improved downstream planning. Example: contents accuracy: PG-InstructBLIP 83.3% vs InstructBLIP 35.1% (Table II).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Ablations on fine-tuning components (Table XXI): removing automated data, filtering training data, enabling/disabling Q-Former text conditioning, and toggling inclusion of object category labels had limited effect on crowd-sourced test accuracy; removing VLM interaction at planner level hurt planning performance (see GPT-4 entry). The authors found removing Q-Former text conditioning improved general VQA robustness in some cases. Different scoring functions for preferences were tested and produced similar performance; they chose the p(yes)/p(no) ratio for interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Fine-tuning a VLM on a large, object-centric dataset of human physical concept annotations (PhYsObjects) leads to significantly improved visual understanding of object-relational physical concepts that are useful for manipulation planning; representing continuous physical concepts relationally via VLM-produced yes/no likelihood ratios combined with a Bradley-Terry model is an effective way to convert VLM outputs into comparator scores usable by an LLM planner; these object-relational signals enable an LLM planner without direct vision to perform embodied planning with high accuracy and real-robot success.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Physically Grounded Vision-Language Models for Robotic Manipulation', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e543.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e543.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Preference scoring + Bradley-Terry</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Likelihood-ratio preference scoring with Bradley–Terry pairwise model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method to convert VLM text-output likelihoods for yes/no prompts into scalar scores for continuous-valued physical concepts: s(o,c)=p(yes|o,c)/p(no|o,c) and log s(o,c) interpreted as a latent scalar; pairwise preference probabilities are then estimated via the Bradley–Terry model P(o1>o2|c)=s(o1,c)/(s(o1,c)+s(o2,c)). This enables relational grounding of continuous properties from a discrete-output VLM.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Applied to InstructBLIP / PG-InstructBLIP (VLM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Technique: prompt VLM with yes/no question for a continuous concept (e.g., 'Is this object heavy?'), extract p(yes) and p(no) likelihoods from the VLM's text output distribution, compute s(o,c)=p(yes)/p(no), use log s as a scalar latent value, and apply Bradley–Terry to obtain pairwise comparison probabilities for preference-labeled training and evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Relational grounding of continuous physical concepts (mass, fragility, deformability, held-out density, liquid capacity)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Transform VLM discrete textual outputs into continuous/ordinal comparative scores that can be trained on human-provided preference annotations and used to rank/compare objects by physical properties for downstream planning decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>object-relational representation; supports comparative selection in manipulation planning</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>object-relational (relational comparisons of continuous physical properties)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>VLM outputs (p(yes), p(no)) after fine-tuning on PhYsObjects; training uses human pairwise preference annotations</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>prompting VLM with yes/no style prompts for continuous concepts, extracting likelihoods, optimizing via cross-entropy on Bradley–Terry pairwise probabilities during fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Scalar latent values represented implicitly via log-likelihood differences of textual outputs; pairwise preference probabilities via Bradley–Terry; effectively encodes relational knowledge (ordinal comparisons) rather than absolute physical units.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy on pairwise preference test examples (mass, fragility, deformability, held-out density, liquid capacity), and downstream improvements in planning/robotic success when these comparative scores are used by the LLM planner.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>PG-InstructBLIP using this scheme achieved high accuracy on continuous/concept comparisons: Mass 80.0%, Fragility 94.6%, Deformability 93.0% on crowd-sourced test sets (Table II). Held-out concepts improved vs base (density 70.3% vs 54.2%, liquid capacity 73.0% vs 65.4% Table III). Using these scores in the planning loop contributed to raising planning accuracy from 56.9% (base VLM) to 88.2% (PG-InstructBLIP).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Enables reliable ordinal comparisons between objects for properties that are visually inferable (e.g., picking the heaviest among a set, choosing most deformable container), and affords direct use by an LLM planner to rank candidate objects for primitives.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Does not yield absolute physical measurements (no grounding to real-world units); sensitive to VLM miscalibration of p(yes)/p(no) for out-of-distribution prompts; equal/unclear labels were removed in training which can limit handling ambiguous comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Alternative naive baselines (Most Common, Text-Only LLM) are substantially worse; random guessing for pairwise preferences would be 50%, while PG-InstructBLIP substantially exceeds this (see above). The paper states that other choice of score functions were tried and gave similar test accuracy but authors preferred the likelihood-ratio for interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Authors experimented with other score functions and found similar performance; ablations on data composition (with/without automatic annotations, filtered labels) show small effects on preference-task accuracy (see Table XXI).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Converting VLM yes/no likelihoods into scalar scores and using a Bradley–Terry pairwise model is an effective and interpretable mechanism to represent continuous physical concepts relationally from a discrete-output VLM, enabling downstream LLM planners (which lack direct sensory input) to perform comparative object selection and improve embodied planning performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Physically Grounded Vision-Language Models for Robotic Manipulation', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Do as i can, not as i say: Grounding language in robotic affordances <em>(Rating: 2)</em></li>
                <li>PaLM-e: An embodied multimodal language model <em>(Rating: 2)</em></li>
                <li>Socratic models: Composing zero-shot multimodal reasoning with language <em>(Rating: 2)</em></li>
                <li>Inner monologue: Embodied reasoning through planning with language models <em>(Rating: 2)</em></li>
                <li>Matcha <em>(Rating: 1)</em></li>
                <li>InstructBLIP: Towards general-purpose vision-language models with instruction tuning <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-543",
    "paper_id": "paper-316f980cfd2e217234386166a46eb080bf027cdd",
    "extraction_schema_id": "extraction-schema-15",
    "extracted_data": [
        {
            "name_short": "GPT-4 (LLM planner)",
            "name_full": "GPT-4 (Generative Pre-trained Transformer 4) used as an LLM-based robotic planner",
            "brief_description": "A large language model (GPT-4) is used as a high-level planner that receives a list of detected objects and available primitive actions, iteratively queries a vision-language model for object-centric physical facts, and outputs a sequence of primitive actions (a plan). The LLM itself has no direct access to raw sensory inputs and reasons over symbolic/probabilistic answers provided by the VLM.",
            "citation_title": "Gpt-4 technical report",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": null,
            "model_description": "A pre-trained large transformer-based language model used zero-shot/few-shot as a planner with a chain-of-thought style prompt; receives textual object detections and VLM-provided answers (text + likelihoods), and emits plans as sequences of high-level primitives. The paper uses GPT-4 with temperature 0 and few-shot chain-of-thought examples encouraging iterative question-asking to the VLM.",
            "task_name": "Real-scene LLM-based robotic planning evaluation",
            "task_description": "Given an image processed by an object detector (OWL-ViT) the planner receives a list of object detections (letters + category labels), a human instruction, and a library of parameterized primitives (go to object X, pick up object X, bring to human object X, put down object X, done). The LLM iteratively asks a VLM (InstructBLIP / PG-InstructBLIP) object-centric questions (no constraint on question phrasing, but encouraged yes/no), obtains the VLM's top textual answers with likelihoods, and then outputs a numbered plan of primitives to satisfy the human instruction. Plans are evaluated by a human rater for task correctness and, separately, executed on a real Franka Panda for success-rate evaluation (primitives assumed executable).",
            "task_type": "multi-step planning; object manipulation; instruction following",
            "knowledge_type": "procedural + object-relational (primary); limited spatial reasoning via symbolic constraints (e.g., 'cannot move countertop')",
            "knowledge_source": "pre-training on text corpora (LLM priors) plus on-the-fly grounding from VLM answers (PG-InstructBLIP) and object detections from OWL-ViT; few-shot chain-of-thought examples in the prompt steer behavior",
            "has_direct_sensory_input": false,
            "elicitation_method": "few-shot chain-of-thought prompting and iterative questioning of a VLM (Socratic interaction); the LLM uses textual/probabilistic answers to produce plans",
            "knowledge_representation": "Implicit procedural knowledge and commonsense stored in LLM weights (natural-language reasoning); externalized object-relational facts are provided as natural-language answers with likelihood/confidence scores from the VLM; plans are explicit action sequences (lists of primitives).",
            "performance_metric": "Task plan accuracy (human-evaluated correctness of produced plans) across 51 real scenarios; real-robot execution success rate (number of successful executions / trials).",
            "performance_result": "Planning accuracy across 51 real scenarios: No VLM baseline 33.3% (text-only planner), InstructBLIP (base VLM) 56.9%, PG-InstructBLIP (physically grounded VLM) 88.2% (Table V). Real-robot execution success (10 tasks total across 2 scenes): InstructBLIP 4/10 (40%), PG-InstructBLIP 9/10 (90%) (Table VII).",
            "success_patterns": "When provided reliable object-centric physical facts from the VLM, GPT-4 successfully composes procedural action sequences and uses object-relational knowledge (mass, fragility, deformability, container affordances) to select appropriate target objects and order primitives (e.g., choose a container that 'can contain liquid', avoid moving large immovable items like countertops). It uses probabilistic/confidence outputs from the VLM to compare candidates and decide which primitives to apply.",
            "failure_patterns": "Failures occur when the VLM supplies incorrect or overconfident answers (out-of-distribution questions/prompts), or when the task requires grounding in absolute physical quantities or geometric reasoning that the VLM/LLM setup does not provide (e.g., whether an object is too heavy to pick up, precise volume/fit reasoning). The LLM also cannot access raw spatial coordinates/continuous geometry and thus cannot perform fine-grained spatial planning beyond symbolic judgments provided by upstream modules.",
            "baseline_comparison": "Baselines: 'No VLM' planner (LLM only with object list) achieved 33.3% task accuracy; InstructBLIP (off-the-shelf VLM) achieved 56.9% task accuracy. PG-InstructBLIP yielded 88.2% task accuracy. On the real robot: InstructBLIP 4/10 vs PG-InstructBLIP 9/10.",
            "ablation_results": "Key ablation at planner level: removing VLM interaction (No VLM) dramatically reduced task accuracy from 88.2% to 33.3%. Replacing PG-InstructBLIP with base InstructBLIP reduced accuracy to 56.9%, indicating the importance of improved object-relational grounding in the VLM. (Paper also reports many ablations for VLM fine-tuning components — see PG-InstructBLIP entry.)",
            "key_findings": "A language-only planner (GPT-4) can perform embodied multi-step planning without direct sensory input by iteratively querying a VLM for object-relational physical facts and using returned probabilistic answers as symbolic grounding; improving the VLM's object-centric physical reasoning (via fine-tuning on human annotations) substantially increases planning accuracy and real-robot success. The LLM represents procedural knowledge implicitly and relies on explicit VLM-provided object facts (natural-language + confidence scores) to resolve object-selection and affordance reasoning.",
            "uuid": "e543.0",
            "source_info": {
                "paper_title": "Physically Grounded Vision-Language Models for Robotic Manipulation",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "PG-InstructBLIP",
            "name_full": "Physically Grounded InstructBLIP (InstructBLIP fine-tuned on PhYsObjects)",
            "brief_description": "A vision-language model (InstructBLIP with Flan-T5 backbone) fine-tuned on the PhYsObjects dataset of object-centric physical concept annotations to improve visual reasoning about material, mass, fragility, deformability, transparency, contents, and container affordances; used to answer LLM queries about objects in scenes and provide likelihood/confidence scores.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "InstructBLIP (fine-tuned -&gt; PG-InstructBLIP)",
            "model_size": null,
            "model_description": "Base: InstructBLIP (Flan-T5-XXL backbone, Q-Former visual encoder). Fine-tuned on PhYsObjects (39.6K crowdsourced + 417K automated annotations) by framing categorical concepts as direct answers and continuous concepts as preference-derived binary queries (yes/no). At inference the model returns textual answers with likelihoods; continuous concepts are scored via the ratio s(o,c)=p(yes|o,c)/p(no|o,c) and used with a Bradley-Terry model for pairwise comparisons.",
            "task_name": "Object-centric physical concept recognition for robotic grounding",
            "task_description": "Given an object's bounding-box image and a natural-language question about a physical concept (e.g., 'Is this object heavy?', 'What is inside this container?', 'Is this container sealed?'), PG-InstructBLIP outputs textual answers plus likelihoods; for continuous concepts trained as preferences it produces p(yes)/p(no) scores that are used to rank objects in pairwise comparisons. These outputs are supplied to the LLM planner to ground object selection and affordance reasoning in manipulation tasks.",
            "task_type": "object-relational perception; affordance recognition for manipulation",
            "knowledge_type": "object-relational (physical properties and affordances); supports procedural planning by informing object selection and constraints",
            "knowledge_source": "fine-tuning on the PhYsObjects dataset (human crowd-sourced and automated annotations) plus original InstructBLIP pretraining on large-scale image-text corpora",
            "has_direct_sensory_input": true,
            "elicitation_method": "fine-tuning on labeled concept prompts (categorical and preference pairs); at runtime, prompted QA-style queries per object (sometimes including object category label) and outputs top textual answers with likelihoods/confidence scores",
            "knowledge_representation": "Implicitly encoded in VLM weights as mappings from visual appearance to physical concepts; categorical concepts returned as discrete labels, continuous concepts represented relationally via log-likelihood ratios log s(o,c) = log p(yes) - log p(no), which serve as scalar scores for pairwise comparisons (Bradley-Terry). Outputs are natural-language answers plus likelihoods (probabilistic soft outputs).",
            "performance_metric": "Accuracy on PhYsObjects crowd-sourced test set per concept; generalization to held-out concepts; effect on downstream planning accuracy and real-robot success rate.",
            "performance_result": "PhYsObjects (crowd-sourced main concepts) average test accuracy: PG-InstructBLIP 87.5% vs InstructBLIP 66.2% (Table II). Held-out concepts average: PG-InstructBLIP 71.7% vs InstructBLIP 59.8% (Table III). Robustness to paraphrased prompts and limited VQA degradation reported. Downstream planning: replacing InstructBLIP with PG-InstructBLIP increased task planning accuracy from 56.9% to 88.2% and real-robot success from 40% to 90%.",
            "success_patterns": "Improved recognition of object-centric physical concepts that are visually inferable (mass comparisons, fragility, deformability, material, transparency, contents, container affordances). Particularly large gains on contents and container-related concepts. The model generalizes somewhat to held-out but related physical concepts, suggesting learned visual priors about physical properties.",
            "failure_patterns": "Still makes errors relative to human judgment on some examples; sensitive to out-of-distribution question phrasing (e.g., different forms of the transparency question led to errors); continuous outputs are relational and not grounded in absolute physical units (cannot decide if an object is too heavy to be picked up by the robot); limited geometric reasoning (volume/fit) and some mislabels even after fine-tuning.",
            "baseline_comparison": "Compared to base InstructBLIP (no PhYsObjects fine-tuning), Most Common label, and a Text-Only LLM baseline (InstructGPT with in-context examples), PG-InstructBLIP substantially outperformed them on visual physical concept recognition and improved downstream planning. Example: contents accuracy: PG-InstructBLIP 83.3% vs InstructBLIP 35.1% (Table II).",
            "ablation_results": "Ablations on fine-tuning components (Table XXI): removing automated data, filtering training data, enabling/disabling Q-Former text conditioning, and toggling inclusion of object category labels had limited effect on crowd-sourced test accuracy; removing VLM interaction at planner level hurt planning performance (see GPT-4 entry). The authors found removing Q-Former text conditioning improved general VQA robustness in some cases. Different scoring functions for preferences were tested and produced similar performance; they chose the p(yes)/p(no) ratio for interpretability.",
            "key_findings": "Fine-tuning a VLM on a large, object-centric dataset of human physical concept annotations (PhYsObjects) leads to significantly improved visual understanding of object-relational physical concepts that are useful for manipulation planning; representing continuous physical concepts relationally via VLM-produced yes/no likelihood ratios combined with a Bradley-Terry model is an effective way to convert VLM outputs into comparator scores usable by an LLM planner; these object-relational signals enable an LLM planner without direct vision to perform embodied planning with high accuracy and real-robot success.",
            "uuid": "e543.1",
            "source_info": {
                "paper_title": "Physically Grounded Vision-Language Models for Robotic Manipulation",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Preference scoring + Bradley-Terry",
            "name_full": "Likelihood-ratio preference scoring with Bradley–Terry pairwise model",
            "brief_description": "A method to convert VLM text-output likelihoods for yes/no prompts into scalar scores for continuous-valued physical concepts: s(o,c)=p(yes|o,c)/p(no|o,c) and log s(o,c) interpreted as a latent scalar; pairwise preference probabilities are then estimated via the Bradley–Terry model P(o1&gt;o2|c)=s(o1,c)/(s(o1,c)+s(o2,c)). This enables relational grounding of continuous properties from a discrete-output VLM.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Applied to InstructBLIP / PG-InstructBLIP (VLM)",
            "model_size": null,
            "model_description": "Technique: prompt VLM with yes/no question for a continuous concept (e.g., 'Is this object heavy?'), extract p(yes) and p(no) likelihoods from the VLM's text output distribution, compute s(o,c)=p(yes)/p(no), use log s as a scalar latent value, and apply Bradley–Terry to obtain pairwise comparison probabilities for preference-labeled training and evaluation.",
            "task_name": "Relational grounding of continuous physical concepts (mass, fragility, deformability, held-out density, liquid capacity)",
            "task_description": "Transform VLM discrete textual outputs into continuous/ordinal comparative scores that can be trained on human-provided preference annotations and used to rank/compare objects by physical properties for downstream planning decisions.",
            "task_type": "object-relational representation; supports comparative selection in manipulation planning",
            "knowledge_type": "object-relational (relational comparisons of continuous physical properties)",
            "knowledge_source": "VLM outputs (p(yes), p(no)) after fine-tuning on PhYsObjects; training uses human pairwise preference annotations",
            "has_direct_sensory_input": true,
            "elicitation_method": "prompting VLM with yes/no style prompts for continuous concepts, extracting likelihoods, optimizing via cross-entropy on Bradley–Terry pairwise probabilities during fine-tuning",
            "knowledge_representation": "Scalar latent values represented implicitly via log-likelihood differences of textual outputs; pairwise preference probabilities via Bradley–Terry; effectively encodes relational knowledge (ordinal comparisons) rather than absolute physical units.",
            "performance_metric": "Accuracy on pairwise preference test examples (mass, fragility, deformability, held-out density, liquid capacity), and downstream improvements in planning/robotic success when these comparative scores are used by the LLM planner.",
            "performance_result": "PG-InstructBLIP using this scheme achieved high accuracy on continuous/concept comparisons: Mass 80.0%, Fragility 94.6%, Deformability 93.0% on crowd-sourced test sets (Table II). Held-out concepts improved vs base (density 70.3% vs 54.2%, liquid capacity 73.0% vs 65.4% Table III). Using these scores in the planning loop contributed to raising planning accuracy from 56.9% (base VLM) to 88.2% (PG-InstructBLIP).",
            "success_patterns": "Enables reliable ordinal comparisons between objects for properties that are visually inferable (e.g., picking the heaviest among a set, choosing most deformable container), and affords direct use by an LLM planner to rank candidate objects for primitives.",
            "failure_patterns": "Does not yield absolute physical measurements (no grounding to real-world units); sensitive to VLM miscalibration of p(yes)/p(no) for out-of-distribution prompts; equal/unclear labels were removed in training which can limit handling ambiguous comparisons.",
            "baseline_comparison": "Alternative naive baselines (Most Common, Text-Only LLM) are substantially worse; random guessing for pairwise preferences would be 50%, while PG-InstructBLIP substantially exceeds this (see above). The paper states that other choice of score functions were tried and gave similar test accuracy but authors preferred the likelihood-ratio for interpretability.",
            "ablation_results": "Authors experimented with other score functions and found similar performance; ablations on data composition (with/without automatic annotations, filtered labels) show small effects on preference-task accuracy (see Table XXI).",
            "key_findings": "Converting VLM yes/no likelihoods into scalar scores and using a Bradley–Terry pairwise model is an effective and interpretable mechanism to represent continuous physical concepts relationally from a discrete-output VLM, enabling downstream LLM planners (which lack direct sensory input) to perform comparative object selection and improve embodied planning performance.",
            "uuid": "e543.2",
            "source_info": {
                "paper_title": "Physically Grounded Vision-Language Models for Robotic Manipulation",
                "publication_date_yy_mm": "2023-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Do as i can, not as i say: Grounding language in robotic affordances",
            "rating": 2
        },
        {
            "paper_title": "PaLM-e: An embodied multimodal language model",
            "rating": 2
        },
        {
            "paper_title": "Socratic models: Composing zero-shot multimodal reasoning with language",
            "rating": 2
        },
        {
            "paper_title": "Inner monologue: Embodied reasoning through planning with language models",
            "rating": 2
        },
        {
            "paper_title": "Matcha",
            "rating": 1
        },
        {
            "paper_title": "InstructBLIP: Towards general-purpose vision-language models with instruction tuning",
            "rating": 2
        }
    ],
    "cost": 0.020027749999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Physically Grounded Vision-Language Models for Robotic Manipulation</h1>
<p>Jensen Gao ${ }^{1}$, Bidipta Sarkar ${ }^{1}$, Fei Xia ${ }^{2}$, Ted Xiao ${ }^{2}$, Jiajun Wu ${ }^{1}$, Brian Ichter ${ }^{2}$, Anirudha Majumdar ${ }^{2,3}$, Dorsa Sadigh ${ }^{1,2}$</p>
<h4>Abstract</h4>
<p>Recent advances in vision-language models (VLMs) have led to improved performance on tasks such as visual question answering and image captioning. Consequently, these models are now well-positioned to reason about the physical world, particularly within domains such as robotic manipulation. However, current VLMs are limited in their understanding of the physical concepts (e.g., material, fragility) of common objects, which restricts their usefulness for robotic manipulation tasks that involve interaction and physical reasoning about such objects. To address this limitation, we propose PhYsObjects, an object-centric dataset of 39.6 K crowdsourced and 417 K automated physical concept annotations of common household objects. We demonstrate that fine-tuning a VLM on PhYsObjects improves its understanding of physical object concepts, including generalization to held-out concepts, by capturing human priors of these concepts from visual appearance. We incorporate this physically grounded VLM in an interactive framework with a large language model-based robotic planner, and show improved planning performance on tasks that require reasoning about physical object concepts, compared to baselines that do not leverage physically grounded VLMs. We additionally illustrate the benefits of our physically grounded VLM on a real robot, where it improves task success rates. We release our dataset and provide further details and visualizations of our results at https://iliad.stanford. edu/pg-vlm/.</p>
<h2>I. INTRODUCTION</h2>
<p>Large language models (LLMs) have shown great promise for converting language instructions into task plans for embodied agents [1], [2]. The fundamental challenge in applying LLMs for this is grounding them to the physical world, through sensory input such as vision. Prior work has made progress towards grounding LLMs by using vision-language models (VLMs) to indicate the presence of objects in a scene, or to provide feedback about occurrences in a scene [3]-[7]. However, vision could be used to further improve grounding by extracting more detailed scene information. For robotic manipulation, understanding physical concepts of objects, such as their material composition or their fragility, would help planners identify relevant objects to interact with, and affordances based on physical or safety constraints. For example, if a human wants a robot to get a cup of water, the robot should be able to determine if a cup already has water or something else in it. Also, the robot should handle the cup with greater caution if it is more fragile.</p>
<p>How can we use vision to reason about physical object concepts? Prior work has studied this problem using more traditional vision techniques, such as self-supervised learning on object interaction data. However, object interaction data</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>can be challenging to collect when scaling up beyond a small set of objects in well-defined settings. While precise estimation of physical properties may sometimes be impossible without interaction data, humans can use their visual perception to reason at a high level about physical concepts without object interactions. For example, humans can reason that a glass cup is more fragile than a plastic bottle, and that it would be easier to use a bowl to hold water than a shallow plate. This reasoning is often based on prior semantic knowledge of visually similar objects, and can be done from static visual appearance alone.</p>
<p>Similarly, VLMs pre-trained using large-scale data have demonstrated broad visual reasoning abilities and generalization [8]-[13], and thus have the potential to physically reason about objects in a similar fashion as humans. Therefore, we propose to leverage VLMs as a scalable way of providing the kind of high-level physical reasoning that humans use to interact with the world, which can benefit a robotic planner, without the need for interaction data. The general and flexible nature of VLMs also removes the need to use separate taskspecific vision models for physical reasoning. VLMs have already been commonly incorporated into robotic planning systems [3]-[7], [13], making them a natural solution for endowing physical reasoning into robotic planning.</p>
<p>However, while modern VLMs have improved significantly on tasks such as visual question answering (VQA), and there has been evidence of their potential for objectcentric physical reasoning [14], we show in this work that their out-of-the-box performance for this still leaves much to be desired. Although VLMs have been trained on broad internet-scale data, this data does not contain many examples of object-centric physical reasoning. This motivates incorporating a greater variety and amount of such data when training VLMs. Unfortunately, prior visual datasets for physical reasoning are not well-suited for understanding common real-world objects, which is desirable for robotics. To address this, we propose PhYsObjects, an objectcentric dataset with human physical concept annotations of common household objects. Our annotations include categorical labels (e.g., object X is made of plastic) and preference pairs (e.g., object X is heavier than object Y ).</p>
<p>Our main contributions are PhYsObjects, a dataset of 39.6 K crowd-sourced and 417 K automated physical concept annotations of real household objects, and demonstrating that using it to fine-tune a VLM significantly improves physical reasoning. We show that our physically grounded VLM achieves improved test accuracy on our dataset, including on held-out physical concepts. Furthermore, to illustrate</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1: (a) We collect physical concept annotations of common household objects for fine-tuning VLMs. (b) We use the fine-tuned VLM in an LLM-based robotic planning framework, where the LLM queries the VLM about physical concepts of objects in the scene, before producing a plan. (c) We evaluate LLM-generated plans on a real Franka Emika Panda robot.</p>
<p>The utility of improved physical reasoning for robotics, we incorporate our physically grounded VLM with an LLM-based robotic planner, where the LLM queries the VLM about physical concepts of objects in its scene. Our system achieves improved planning performance on tasks that require physical reasoning, compared to baselines that do not use physically grounded VLMs. Finally, we demonstrate the benefits of our physically grounded VLM for planning with a real robot, where its usage improves task success rates.</p>
<h2>II. RELATED WORK</h2>
<p>We review prior work on physical reasoning, object attribute datasets, VLMs, using LLMs for robotic planning, and using LLMs and VLMs together in an interactive system.</p>
<p><strong>Physical Reasoning.</strong> Prior works have studied estimating physical object properties from vision by learning from interaction data [15]–[17]. Other works focus on learning representations that capture physical concepts, rather than direct estimation [18], [19]. Unlike these works, we use pre-trained VLMs and human annotations as a more scalable alternative to learning from interaction. Mind's Eye investigates physical reasoning using LLMs [20], but relies on grounding using a simulator, which would be difficult to scale to the real world. VEC investigates physical reasoning with LLMs and VLMs [21], but reasons from text descriptions, while we reason from real images. OpenScene uses CLIP [22] to identify objects in scenes using properties such as material and fragility, but these results are only qualitative in nature [14]. In our work, we propose PHYSOBJECTS to better quantify and improve object-centric physical reasoning, and leverage this reasoning for robotic manipulation.</p>
<p><strong>Object Attribute Datasets.</strong> There have been prior visual object attribute datasets with concepts included in PHYSOBJECTS, such as material and transparency [23]–[26]. However, they focus more on visual attributes such as color, while we focus on physical concepts. Physics 101 provides a dataset of object interaction videos and property measurements [16], but PHYSOBJECTS includes a greater variety of objects that are more relevant for household robotics.</p>
<p><strong>Vision-Language Models.</strong> VLMs have made large improvements on multi-modal tasks such as VQA, by leveraging internet-scale image and text data [8]–[10], [12]. In our experiments, we use InstructBLIP [11] as our base VLM for fine-tuning and comparison, as it was the state-of-the-art open-source VLM at the time of our experiments. PaLM-E has shown strong performance on general visual-language tasks and robotic planning [13], but there has not been focused evaluation of it for physical reasoning. SuccessVQA fine-tunes VLMs on human data for success detection by treating it as a VQA task, and achieves better generalization than models designed specifically for success detection [27]. We similarly fine-tune VLMs on human data for physical reasoning by casting it as a VQA problem, to benefit from the generalization abilities and versatility of VLMs.</p>
<p><strong>LLMs for Robotic Planning.</strong> Many recent works have used LLMs as robotic planners. SayCan uses visual value functions to provide affordances for grounding [2], but does not benefit from VLMs. Follow-up works have used VLMs for grounding LLM planners through object detection, or providing feedback about what has happened (e.g., success detection) [3]–[7]. Our work focuses on expanding the use of VLMs for grounding through physical reasoning, to let LLM-based planners perform tasks that require a deeper physical understanding of the world.</p>
<p><strong>LLM/VLM Interaction.</strong> Our planning evaluation falls in the framework of Socratic Models [28], where large models interact with each other through text to perform tasks such as VQA [29], [30] and image captioning [31]. Most similar to our evaluation is Matcha, where an LLM receives a task instruction, obtains object-centric feedback from its environment, and uses this for task planning [32]. However, this work does not focus on visual feedback, as their evaluation is in a simulated environment where physical concepts are not visually observable. In contrast, we focus on physical reasoning from vision in real-world scenes.</p>
<h2>III. PHYSOBJECTS DATASET</h2>
<p>To benchmark and improve VLMs for object-centric physical reasoning, we propose PHYSOBJECTS, a dataset of 39.6K crowd-sourced and 417K automated physical concept annotations for images of real household objects.</p>
<p>Image Source. We use the publicly released challenge version of the EgoObjects dataset [33] as our image source. To our knowledge, this was the largest object-centric dataset of real images that was publicly released when constructing PHYS OBJECTS. The dataset consists of frames from egocentric videos in realistic household settings, which makes it particularly relevant for household robotics. It includes 117,424 images, 225,466 object bounding boxes with corresponding category labels from 277 object categories, and 4,203 object instance IDs. PHYS OBJECTS consists of physical concept annotations for a large subset of this image data.</p>
<p>We construct random training, validation, and test sets based on object instance IDs. We split the dataset per object category to ensure each object category is represented in each set when possible. Our training, validation, and test sets consist of 73.0%, 14.8%, and 12.2% of objects, respectively.</p>
<table>
<thead>
<tr>
<th>Concept</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Mass</td>
<td>how heavy an object is</td>
</tr>
<tr>
<td>Fragility</td>
<td>how easily an object can be broken/damaged</td>
</tr>
<tr>
<td>Deformability</td>
<td>how easily an object can change shape without breaking</td>
</tr>
<tr>
<td>Material</td>
<td>what an object is primarily made of</td>
</tr>
<tr>
<td>Transparency</td>
<td>how much can be seen through an object</td>
</tr>
<tr>
<td>Contents</td>
<td>what is inside a container</td>
</tr>
<tr>
<td>Can Contain Liquid</td>
<td>if a container can be used to easily carry liquid</td>
</tr>
<tr>
<td>Is Sealed</td>
<td>if a container will not spill if rotated</td>
</tr>
<tr>
<td>Density (held-out)</td>
<td>how much mass per unit of volume of an object</td>
</tr>
<tr>
<td>Liquid Capacity (held-out)</td>
<td>how much liquid a container can contain</td>
</tr>
</tbody>
</table>
<p>TABLE I: Our physical concepts and brief descriptions
Physical Concepts. We collect annotations for eight main physical concepts and two additional concepts reserved for held-out evaluation. We select concepts based on prior work and what we believe to be useful for robotic manipulation, but do not consider all such concepts. For example, we do not include friction because this can be challenging to estimate without interaction, and we do not include volume because this requires geometric reasoning, which we do not focus on.</p>
<p>Of our main concepts, three are continuous-valued and applicable to all objects: mass, fragility, and deformability. Two are also applicable to all objects, but are categorical: material and transparency. Transparency could be considered continuous, but we use discrete values of transparent, translucent, and opaque. The other three are categorical and applicable only to container objects: contents, can contain liquid, and is sealed. We define which object categories are containers, resulting in 956 container object instances.</p>
<p>Our two held-out concepts are density, which is continuous and applicable to all objects, and liquid capacity, which is continuous and applicable only to containers. We only collect test data for these held-out concepts. We list all concepts and their brief descriptions in Table I.</p>
<p>For categorical concepts, we define a set of labels for each concept. Annotations consist of a label specified for a given object and concept. For the concepts material and contents, when crowd-sourcing, we allow for open-ended labels if none of the pre-defined labels are applicable.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>For continuous concepts, annotations are preference pairs, where given two objects, an annotation indicates that either one object has a higher level of a concept, the objects have roughly equal levels, or the relationship is unclear. We use preferences because it is generally more intuitive for humans to provide comparisons than continuous values [34], [35]. This is especially true when annotating static images with physical concepts, where it is difficult to specify precise grounded values. For example, it would be difficult to specify the deformability of a sponge as a value out of 10. Comparisons have also been used to evaluate LLMs and VLMs for physical reasoning in prior work [21]. Therefore, the kind of grounding studied in PHYS OBJECTS for continuous concepts is only relational in nature.
Automatic Annotations. Before crowd-sourcing, we first attempt to automate as many annotations as possible, so that crowd-workers only annotate examples that cannot be easily automated. For categorical concepts, we assign concept values to some of the defined object categories in EgoObjects, such that all objects in a category are labeled with that value. For continuous concepts, we define high and low tiers for each concept, such that all objects from a high tier category have a higher level of that concept than all objects from a low tier category. Then, we automate preference annotations for all object pairs between the two tiers.</p>
<p>Which side is more fragile?</p>
<p>Fig. 2: Annotation UI for fragility. Here, the label is right, i.e., the water glass is more fragile than the house/car key.</p>
<p>Crowd-Sourcing Annotations. We obtain additional annotations via crowd-sourcing, using 573 crowd-workers on the Prolific platform. Crowd-workers use a web-based user interface (example for fragility shown in Fig. 2) where they are presented with object bounding boxes in the context of their overall image, and provide annotations using onscreen buttons or their keyboard. For categorical concepts, we collect annotations for the majority of objects that were not automatically annotated. For continuous concepts, because it is impractical to annotate every pair of objects in the dataset, we randomly sample pairs to annotate. We enforce that 20% of the sampled pairs are between objects of the same category, to prioritize understanding differences between objects of the same category. We collect annotations from three crowd-workers for each example. To promote high-quality data, we include attention checks as 10% of provided examples, which have known labels, and only keep data from annotators that achieve 80% accuracy on these.</p>
<table>
<thead>
<tr>
<th></th>
<th>Most Common</th>
<th>Text Only</th>
<th>InstructBLIP</th>
<th>Single Concept FT (ours)</th>
<th>PG-InstructBLIP (ours)</th>
</tr>
</thead>
<tbody>
<tr>
<td>1 Mass</td>
<td>42.2</td>
<td>73.3</td>
<td>62.2</td>
<td>$\mathbf{8 0 . 0}$</td>
<td>$\mathbf{8 0 . 0}$</td>
</tr>
<tr>
<td>Fragility</td>
<td>64.9</td>
<td>64.9</td>
<td>78.4</td>
<td>91.2</td>
<td>$\mathbf{9 4 . 6}$</td>
</tr>
<tr>
<td>Deformability</td>
<td>46.5</td>
<td>62.8</td>
<td>67.4</td>
<td>$\mathbf{9 5 . 3}$</td>
<td>93.0</td>
</tr>
<tr>
<td>Material</td>
<td>37.1</td>
<td>73.9</td>
<td>67.1</td>
<td>83.7</td>
<td>$\mathbf{8 4 . 6}$</td>
</tr>
<tr>
<td>Transparency</td>
<td>77.6</td>
<td>82.2</td>
<td>85.8</td>
<td>89.4</td>
<td>$\mathbf{9 0 . 1}$</td>
</tr>
<tr>
<td>Contents</td>
<td>39.5</td>
<td>50.9</td>
<td>35.1</td>
<td>81.6</td>
<td>$\mathbf{8 3 . 3}$</td>
</tr>
<tr>
<td>Can Contain Liquid</td>
<td>56.3</td>
<td>$\mathbf{9 2 . 2}$</td>
<td>59.4</td>
<td>84.4</td>
<td>87.5</td>
</tr>
<tr>
<td>Is Sealed</td>
<td>80.6</td>
<td>80.6</td>
<td>74.2</td>
<td>80.6</td>
<td>$\mathbf{8 7 . 1}$</td>
</tr>
<tr>
<td>Average</td>
<td>55.6</td>
<td>72.6</td>
<td>66.2</td>
<td>85.8</td>
<td>$\mathbf{8 7 . 5}$</td>
</tr>
</tbody>
</table>
<p>TABLE II: Test accuracy for main concepts on crowd-sourced PHYSOBJECTS
Dataset Statistics. We crowd-source 39.6K annotations for 13.2 K examples, and automate annotations for 417 K additional examples. For crowd-sourced annotations, $93.7 \%$ of examples have at least $2 / 3$ annotator label agreement, and $58.1 \%$ have unanimous agreement.</p>
<h2>IV. Physically Grounding Vision-Language MODELS</h2>
<p>Fine-Tuning VLMs. We work with the FlanT5-XXL [36] version of InstructBLIP [11]. InstructBLIP takes as input a single RGB image and text prompt, and predicts text as output. In our setup, we choose the model inputs to be a single bounding box of an object, and a question text prompt corresponding to each concept.
Learning From Preferences. Learning for categorical concepts amounts to maximum likelihood of annotated labels. However, it is not as straightforward to train a VLM on preferences for continuous concepts, because preference learning requires a continuous score. To do this with VLMs, which naturally have discrete text outputs, we prompt the VLM with questions that can be answered with yes or no for continuous concepts. Then, we extract the following score function:</p>
<p>$$
s(o, c)=\frac{p(\text { yes } \mid o, c)}{p(\text { no } \mid o, c)}
$$</p>
<p>where $o$ is an object bounding box image, $c$ is a concept, and $p(\cdot \mid o, c)$ is the likelihood under the VLM of text, conditioned on the object image and concept. We use this as our score function because it can take any non-negative value, and $\log s(o, c)$ has the intuitive interpretation as the difference of log-likelihoods between yes and no. ${ }^{2}$ We then use the Bradley-Terry model [37] to estimate the probability of a human indicating that object $o_{1}$ has a higher value than object $o_{2}$ for concept $c$ as:</p>
<p>$$
P\left(o_{1}&gt;o_{2} \mid c\right)=\frac{s\left(o_{1}, c\right)}{s\left(o_{1}, c\right)+s\left(o_{2}, c\right)}
$$</p>
<p>We assume a dataset $\mathcal{D}$ of preference annotations $\left(o_{1}, o_{2}, c, y\right)$, where $y \in{[1,0],[0,1],[0.5,0.5]}$ corresponds</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>to if $o_{1}$ is preferred, $o_{2}$ is preferred, or if they are indicated to be equal. We then fine-tune the VLM by minimizing the following objective:</p>
<p>$$
\begin{aligned}
\mathcal{L}(\mathcal{D})= &amp; -\mathbb{E}<em 1="1">{\left(o</em> \mid c\right)\right. \
&amp; \left.+y_{2} \log \left(1-P\left(o_{1}&gt;o_{2} \mid c\right)\right]\right.
\end{aligned}
$$}, o_{2}, c, y\right) \sim \mathcal{D}}\left[y_{1} \log P\left(o_{1}&gt;o_{2</p>
<p>In practice, this is the binary cross-entropy objective where the logits for each object image $o$ is the difference of loglikelihoods $\log s(o, c)=\log p($ yes $\mid o, c)-\log p($ no $\mid o, c)$.</p>
<h2>V. EXPERIMENTAL RESULTS</h2>
<p>We evaluate VLMs for physical reasoning using 1) test accuracy on PHYSOBJECTS, 2) planning accuracy on real scenes for physical reasoning tasks, and 3) task success rate on a real robot.</p>
<h2>A. Dataset Evaluation</h2>
<p>We refer to InstructBLIP fine-tuned on all main concepts in PHYSOBJECTS as Physically Grounded InstructBLIP, or PG-InstructBLIP. ${ }^{3}$ We focus our evaluation on crowdsourced examples, because as described in Section III, these were collected with the intent for their labels to not be discernible from object category information alone, and thus they are generally more challenging. We report test accuracy on these examples in Table II. Our baselines include Most Common, where the most common label in the training data is predicted, Text Only, where an LLM makes predictions using in-context examples from PHYSOBJECTS, but using object category labels instead of images, and InstructBLIP. We also compare to versions of InstructBLIP fine-tuned on single concept data. We find that PG-InstructBLIP outperforms InstructBLIP on all concepts, with the largest improvement on contents, which InstructBLIP has the most difficulty with. We also find that PG-InstructBLIP performs slightly better than the single concept models, suggesting possible positive transfer from using a single general-purpose model compared to separate task-specific models, although we acknowledge the improvement here is not extremely significant. PG-InstructBLIP also generally outperforms Most Common and Text Only, suggesting that our evaluation benefits from reasoning beyond dataset statistics, and from using vision.</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Instruct- <br> BLIP</th>
<th style="text-align: center;">PG-InstructBLIP <br> (ours)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Density</td>
<td style="text-align: center;">54.2</td>
<td style="text-align: center;">$\mathbf{7 0 . 3}$</td>
</tr>
<tr>
<td style="text-align: left;">Liquid Capacity</td>
<td style="text-align: center;">65.4</td>
<td style="text-align: center;">$\mathbf{7 3 . 0}$</td>
</tr>
<tr>
<td style="text-align: left;">Average</td>
<td style="text-align: center;">59.8</td>
<td style="text-align: center;">$\mathbf{7 1 . 7}$</td>
</tr>
</tbody>
</table>
<p>TABLE III: Test accuracy for held-out concepts on crowd-sourced PhYsObjects</p>
<p>Generalization Results. We additionally evaluate both InstructBLIP and PG-InstructBLIP on test data for our heldout concepts, which we report in Table III. We find that PG-InstructBLIP improves upon InstructBLIP by $\mathbf{1 1 . 9 \%}$, despite having never seen these evaluated concepts nor object instances during fine-tuning. We believe this suggests that fine-tuning VLMs can offer possible generalization benefits to concepts that are related to those seen during fine-tuning.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Instruct- <br> BLIP</th>
<th style="text-align: center;">PG-InstructBLIP <br> (ours)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Mass</td>
<td style="text-align: center;">55.6</td>
<td style="text-align: center;">$\mathbf{8 2 . 2}$</td>
</tr>
<tr>
<td style="text-align: left;">Fragility</td>
<td style="text-align: center;">70.3</td>
<td style="text-align: center;">$\mathbf{8 3 . 8}$</td>
</tr>
<tr>
<td style="text-align: left;">Deformability</td>
<td style="text-align: center;">76.7</td>
<td style="text-align: center;">$\mathbf{8 8 . 4}$</td>
</tr>
<tr>
<td style="text-align: left;">Material</td>
<td style="text-align: center;">67.7</td>
<td style="text-align: center;">$\mathbf{8 3 . 4}$</td>
</tr>
<tr>
<td style="text-align: left;">Transparency</td>
<td style="text-align: center;">81.5</td>
<td style="text-align: center;">$\mathbf{8 3 . 8}$</td>
</tr>
<tr>
<td style="text-align: left;">Contents</td>
<td style="text-align: center;">32.5</td>
<td style="text-align: center;">$\mathbf{8 1 . 6}$</td>
</tr>
<tr>
<td style="text-align: left;">Can Contain Liquid</td>
<td style="text-align: center;">56.3</td>
<td style="text-align: center;">$\mathbf{8 9 . 1}$</td>
</tr>
<tr>
<td style="text-align: left;">Is Sealed</td>
<td style="text-align: center;">71.0</td>
<td style="text-align: center;">$\mathbf{8 0 . 6}$</td>
</tr>
<tr>
<td style="text-align: left;">Average</td>
<td style="text-align: center;">64.0</td>
<td style="text-align: center;">$\mathbf{8 4 . 1}$</td>
</tr>
</tbody>
</table>
<p>TABLE IV: Test accuracy for main concepts with paraphrased prompts In Table IV, we report results for main concepts on unseen paraphrased question prompts. We find that PG-InstructBLIP still outperforms InstructBLIP, with limited degradation from the original prompts, suggesting robustness to question variety from using a large pre-trained VLM.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 3: Performance scaling with dataset size Dataset Scaling. In Fig. 3, we illustrate how average performance scales with dataset size, by fine-tuning on different fractions of data from PhYsObjects. Performance scales positively, but the models still benefit significantly from only $10 \%$ of our dataset, suggesting that the physical reasoning of VLMs can be improved with relatively small amounts of annotated data. Additional Results. We include additional results in our Appendix (found on our website). These include showing that PG-InstructBLIP has limited degradation on general VQA benchmarks compared to InstructBLIP, suggesting that existing systems using VLMs can benefit from PhYsObjects for physical reasoning, without sacrificing other reasoning abilities. We also include results using different question prompts, using a smaller version of InstructBLIP, evaluating on automatically annotated data, transfer to held-out concepts, and ablations on our fine-tuning process.</p>
<h2>B. Real Scene Planning Evaluation</h2>
<p>Next, we evaluate the efficacy of PG-InstructBLIP for robotic planning on unseen images of real scenes. We provide an example scene in Fig. 4. We evaluate on tasks with language instructions, and assume a library of primitive robotic operations with language descriptions.</p>
<h2>Planning Framework.</h2>
<p>The LLM used in our planning framework is GPT-4 [38]. It is first given object detections in the scene, a list of primitives, and the task instruction, and then asks a VLM questions about objects in the scene. There are no constraints on the questions. Afterwards, the</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 4: Example scene in our planning evaluation LLM either indicates the task is not possible, or produces a plan consisting of primitives to execute.</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Task Category</td>
<td>No</td>
<td>Instruct-</td>
<td>PG-InstructBLIP</td>
</tr>
<tr>
<td></td>
<td>VLM</td>
<td>BLIP</td>
<td>(ours)</td>
</tr>
<tr>
<td>Single Concept</td>
<td>36.8</td>
<td>68.4</td>
<td>$\mathbf{8 4 . 1}$</td>
</tr>
<tr>
<td>Multi-Concept</td>
<td>27.8</td>
<td>27.8</td>
<td>$\mathbf{9 4 . 4}$</td>
</tr>
<tr>
<td>Common Knowledge</td>
<td>35.7</td>
<td>78.6</td>
<td>$\mathbf{8 5 . 7}$</td>
</tr>
<tr>
<td>Overall</td>
<td>33.3</td>
<td>56.9</td>
<td>$\mathbf{8 8 . 2}$</td>
</tr>
</tbody>
</table>
<p>TABLE V: Task plan accuracy on 51 real scenarios Results. We report task planning accuracy using InstructBLIP and PG-InstructBLIP in Table V. We also compare to a planner that does not use VLM interaction for grounding. We evaluate on 51 task scenarios across 8 scenes, using a non-author human to evaluate task plans. We divide our task scenarios into three categories. Single Concept requires identifying objects using one physical concept, e.g., finding the heaviest object. Multi-Concept requires reasoning about multiple physical concepts, e.g., asking for a metal container that can hold water. This may include concepts outside of PhYsObjects. Common Knowledge requires additional reasoning about common knowledge of objects, e.g., understanding the label of a container. While our tasks focus on physical concepts in PhYsObjects, the LLM can ask questions about other concepts that may also be useful, particularly for Common Knowledge tasks.</p>
<p>PG-InstructBLIP outperforms InstructBLIP on all task categories, especially Multi-Concept. It does slightly better on Common Knowledge, suggesting that it can reason about non-PHYSObjects concepts at a similar level as InstructBLIP. Using no VLM performs substantially worse than using VLM interaction, indicating that our tasks require additional grounding beyond object detection. We provide further details of results on our website.</p>
<h3>V-C Real Robot Evaluation</h3>
<p>Lastly, we evaluate plans on real scenes using a Franka Emika Panda robot. We use a similar planner as in the previous section, but with different prompts and primitives. We assume a library of primitives for pick-and-place tasks. We evaluate on two scenes, with five tasks per scene, which we provide in Table VI. We report success rates using InstructBLIP and PG-InstructBLIP in Table VII. We ensure the primitives execute successfully, so our success rates only reflect plan quality.</p>
<table>
<thead>
<tr>
<th>Scene Image</th>
<th>Task Instructions</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<ol>
<li>Move all objects that are not plastic to the side.</li>
<li>Find a container that has metals. Move all metal objects into that container.</li>
<li>Move all containers that can be used to carry water to the side.</li>
<li>Put the two objects with the least mass into the least deformable container.</li>
<li>Move the most fragile object to the side.
<img alt="img-3.jpeg" src="img-3.jpeg" /></li>
<li>Put all containers that can hold water to the side.</li>
<li>Put all objects that are not plastic to the side.</li>
<li>Put all objects that are translucent to the side.</li>
<li>Put the three heaviest objects to the side.</li>
<li>Put a plastic object that is not a container into a plastic container. Choose the container that you are most certain is plastic.</li>
</ol>
<p>TABLE VI: Scene images and task instructions for our real robot evaluation</p>
<p>We find that using PG-InstructBLIP leads to successful robot executions more often than InstructBLIP. For example, when asked "Is this object not plastic?" about the ceramic bowl in Fig. 5a, InstructBLIP incorrectly assigns a likelihood of 0.89 to yes, while PG-InstructBLIP only assigns 0.18. However, when asked "Is this object translucent?" about the glass jar in Fig. 5b, both InstructBLIP and PGInstructBLIP incorrectly assign likelihoods of 0.95 and 0.91 to yes, respectively. We note that while these questions relate to physical concepts in PHYS OBJECTS, neither are formatted like the training questions for PG-InstructBLIP. For example, the training prompt for transparency was "Is this object transparent, translucent, or opaque?". This suggests that despite using a large pre-trained VLM, PG-InstructBLIP may sometimes still fail due to out-of-distribution questions. We provide more results and visualizations on our website.</p>
<p>|   | Instruct-
BLIP | PG-InstructBLIP
(ours)  |
| --- | --- | --- |
|  Scene 1 | $2 / 5$ | $\mathbf{5 / 5}$  |
|  Scene 2 | $2 / 5$ | $\mathbf{4 / 5}$  |
|  Overall | $4 / 10$ | $\mathbf{9 / 1 0}$  |</p>
<p>TABLE VII: Success rates for real robot evaluation</p>
<table>
<thead>
<tr>
<th><img alt="img-4.jpeg" src="img-4.jpeg" /></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>(a) Ceramic bowl</td>
<td>(b) Glass jar</td>
</tr>
</tbody>
</table>
<p>Fig. 5: Objects from our real robot evaluation</p>
<h2>VI. DISCUSSION</h2>
<p>Summary. In this work, we propose PHYS OBJECTS, the first large-scale dataset of physical concept annotations of real household object images, and demonstrate that finetuning a VLM on it significantly improves its physical reasoning abilities, including on held-out physical concepts. We find that using the fine-tuned VLM for real-world robotic planning improves performance on tasks that require physical reasoning. We believe our work makes progress toward expanding the applicability of VLMs for robotics. Limitations and Future Work. While we show PHYS Ob JECTS can improve the physical reasoning of a VLM, it still makes errors relative to human judgment. Also, while our proposed methodology for continuous concepts improves relational grounding, which we show can be useful for robotic planning, the model outputs are not grounded in real physical quantities, which would be needed for some applications, e.g., identifying if an object is too heavy to be picked up. Future work can investigate incorporating data with real physical measurements to improve grounding.</p>
<p>While we believe the physical concepts in this work to have broad relevance for robotics, future work can expand on these for greater downstream applications. This could include expanding beyond physical reasoning, such as geometric reasoning (e.g., whether an object can fit inside a container), or social reasoning (e.g., what is acceptable to move off a table for cleaning). We believe our dataset is a first step towards this direction of using VLMs for more sophisticated reasoning in robotics.</p>
<h2>ACKNOWLEDGMENTS</h2>
<p>This work was supported by NSF Awards 2132847, 1941722, and 2338203, ONR N00014-23-1-2355 and YIP, DARPA YFA, and Ford. We thank Minae Kwon, Siddharth Karamcheti, Suvir Mirchandani, and other ILIAD lab members for helpful discussions and feedback, and Siddharth Karamcheti for helping to set up the real robot evaluation.</p>
<h2>REFERENCES</h2>
<p>[1] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. In International Conference on Machine Learning, pages 9118-9147. PMLR, 2022.
[2] Brian Ichter, Anthony Brohan, Yevgen Chebotar, Chelsea Finn, Karol Hausman, Alexander Herzog, Daniel Ho, Julian Ibarz, Alex Irpan, Eric Jang, Ryan Julian, Dmitry Kalashnikov, Sergey Levine, Yao Lu, Carolina Parada, Kanishka Rao, Pierre Sermanet, Alexander T Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Mengyuan Yan, Noah Brown, Michael Ahn, Omar Cortes, Nicolas Sievers, Clayton Tan, Sichun Xu, Diego Reyes, Jarek Rettinghouse, Jornell Quiambao, Peter Pastor, Linda Luu, Kuang-Huei Lee, Yuheng Kuang, Sally Jesmonth, Kyle Jeffrey, Rosario Jauregui Ruano, Jasmine Hsu, Keerthana Gopalakrishnan, Byron David, Andy Zeng, and Cheyuan Kelly Fu. Do as i can, not as i say: Grounding language in robotic affordances. In 6th Annual Conference on Robot Learning, 2022.
[3] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, Pierre Sermanet, Tomas Jackson, Noah Brown, Linda Luu, Sergey Levine, Karol Hausman, and Brian Ichter. Inner monologue: Embodied reasoning through planning with language models. In 6th Annual Conference on Robot Learning, 2022.
[4] Boyuan Chen, Fei Xia, Brian Ichter, Kanishka Rao, Keerthana Gopalakrishnan, Michael S Ryoo, Austin Stone, and Daniel Kappler. Open-vocabulary queryable scene representations for real world planning. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pages 11509-11522. IEEE, 2023.
[5] Wenlong Huang, Fei Xia, Dhruv Shah, Danny Driess, Andy Zeng, Yao Lu, Pete Florence, Igor Mordatch, Sergey Levine, Karol Hausman, et al. Grounded decoding: Guiding text generation with grounded models for embodied agents. Advances in Neural Information Processing Systems, 36, 2023.
[6] Satvik Sharma, Huang Huang, Kaushik Shivakumar, Lawrence Yunliang Chen, Ryan Hoque, brian ichter, and Ken Goldberg. Semantic mechanical search with large vision and language models. In 7th Annual Conference on Robot Learning, 2023.
[7] Jimmy Wu, Rika Antonova, Adam Kan, Marion Lepert, Andy Zeng, Shuran Song, Jeannette Bohg, Szymon Rusinkiewicz, and Thomas Funkhouser. Tidybot: Personalized robot assistance with large language models. Autonomous Robots, 2023.
[8] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for fewshot learning. Advances in Neural Information Processing Systems, 35:23716-23736, 2022.
[9] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Sale, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, Alexander Kolesnikov, Joan Puigcerver, Nan Ding, Keran Rong, Hassan Akbari, Gaurav Mishra, Linting Xue, Ashish V Thapliyal, James Bradbury, Weicheng Kuo, Mojtaba Seyedhooezin, Chao Jia, Burcu Karagol Ayan, Carlos Riquelme Ruiz, Andreas Peter Steiner, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, and Radu Soricut. PaLI: A jointly-scaled multilingual languageimage model. In The Eleventh International Conference on Learning Representations, 2023.
[10] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In Proceedings of the 40th International Conference on Machine Learning, pages 19730-19742, 2023.
[11] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. InstructBLIP: Towards general-purpose vision-language models with instruction tuning. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.
[12] Shikun Liu, Linxi Fan, Edward Johns, Zhiding Yu, Chaowei Xiao, and Anima Anandkumar. Prismer: A vision-language model with multitask experts. Transactions on Machine Learning Research, 2024.
[13] Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhury, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, and Pete Florence. PaLM-e: An embodied multimodal language
model. In Proceedings of the 40th International Conference on Machine Learning, pages 8469-8488, 2023.
[14] Songyou Peng, Kyle Genova, Chiyu "Max" Jiang, Andrea Tagliasacchi, Marc Pollefeys, and Thomas Funkhouser. Openscene: 3d scene understanding with open vocabularies. In CVPR, 2023.
[15] Jiajun Wu, Ilker Yildirim, Joseph J Lim, Bill Freeman, and Josh Tenenbaum. Galileo: Perceiving physical object properties by integrating a physics engine with deep learning. Advances in neural information processing systems, 28, 2015.
[16] Jiajun Wu, Joseph J Lim, Hongyi Zhang, Joshua B Tenenbaum, and William T Freeman. Physics 101: Learning physical object properties from unlabeled videos. In BMVC, volume 2, page 7, 2016.
[17] Yunzhu Li, Toru Lin, Kexin Yi, Daniel Bear, Daniel L.K. Yamins, Jiajun Wu, Joshua B. Tenenbaum, and Antonio Torralba. Visual grounding of learned physical models. In ICML, 2020.
[18] Michael Janner, Sergey Levine, William T. Freeman, Joshua B. Tenenbaum, Chelsea Finn, and Jiajun Wu. Reasoning about physical interactions with object-oriented prediction and planning. In International Conference on Learning Representations, 2019.
[19] Zhenjia Xu, Jiajun Wu, Andy Zeng, Joshua B Tenenbaum, and Shuran Song. Densephysnet: Learning dense physical object representations via multi-step dynamic interactions. In Robotics: Science and Systems (RSS), 2019.
[20] Ruibo Liu, Jason Wei, Shixiang Shane Gu, Te-Yen Wu, Soroush Vosoughi, Claire Cui, Denny Zhou, and Andrew M. Dai. Mind's eye: Grounded language model reasoning through simulation. In The Eleventh International Conference on Learning Representations, 2023.
[21] Lei Li, Jingjing Xu, Qingxiu Dong, Ce Zheng, Xu Sun, Lingpeng Kong, and Qi Liu. Can language models understand physical concepts? In The 2023 Conference on Empirical Methods in Natural Language Processing, 2023.
[22] Alec Radford, Jong Wook Kim, Chris Hallacy, A. Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In ICML, 2021.
[23] Genevieve Patterson and James Hays. Coco attributes: Attributes for people, animals, and objects. In Computer Vision-ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part VI 14, pages 85-100. Springer, 2016.
[24] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. International journal of computer vision, 123:32-73, 2017.
[25] Khoi Pham, Kushal Kafle, Zhe Lin, Ziuhong Ding, Scott Cohen, Quan Tran, and Abhinav Shrivastava. Learning to predict visual attributes in the wild. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13018-13028, 2021.
[26] Vignesh Ramanathan, Anmol Kalia, Vladan Petrovic, Yi Wen, Baixue Zheng, Baishan Guo, Rui Wang, Aaron Marquez, Rama Kovvuri, Abhishek Kadian, Amir Mousavi, Yiwen Song, Abhimanyu Dubey, and Dhruv Mahajan. Paco: Parts and attributes of common objects. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 7141-7151, June 2023.
[27] Yuqing Du, Ksenia Konyushkova, Misha Denil, Akhil Raju, Jessica Landon, Felix Hill, Nando de Freitas, and Serkan Cabi. Visionlanguage models as success detectors. In Proceedings of The 2nd Conference on Lifelong Learning Agents, pages 120-136, 2023.
[28] Andy Zeng, Maria Attarian, Brian Ichter, Krzysztof Marcin Choromanski, Adrian Wong, Surfan Welker, Federico Tombari, Aveek Purohit, Michael S Ryoo, Vikas Sindhwani, Johnny Lee, Vincent Vanhoucke, and Pete Florence. Socratic models: Composing zero-shot multimodal reasoning with language. In The Eleventh International Conference on Learning Representations, 2023.
[29] Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Yumao Lu, Zicheng Liu, and Lijuan Wang. An empirical study of gpt-3 for fewshot knowledge-based vqa. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 3081-3089, 2022.
[30] Zhenwei Shao, Zhou Yu, Meng Wang, and Jun Yu. Prompting large language models with answer heuristics for knowledge-based visual question answering. In Computer Vision and Pattern Recognition (CVPR), pages 14974-14983, 2023.
[31] Deyao Zhu, Jun Chen, Kilichbek Haydarov, Xiaoqian Shen, Wenxuan Zhang, and Mohamed Elhoseiny. Chatgpt asks, blip-2 answers:</p>
<p>Automatic questioning towards enriched visual descriptions. arXiv preprint arXiv:2303.06594, 2023.
[32] Xufeng Zhao, Mengdi Li, Cornelius Weber, Muhammad Burhan Hafez, and Stefan Wermter. Chat with the environment: Interactive multimodal perception using large language models. arXiv preprint arXiv:2303.08268, 2023.
[33] Meta. Egoobjects dataset. https://ai.facebook.com/ datasets/egoobjects-dataset/, Last accessed on 2023-0528.
[34] Dorsa Sadigh, Anca D. Dragan, S. Shankar Sastry, and Sanjit A. Seshia. Active preference-based learning of reward functions. In Proceedings of Robotics: Science and Systems (RSS), July 2017.
[35] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017.
[36] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In International Conference on Learning Representations, 2022.
[37] Ralph Allan Bradley and Milton E Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324-345, 1952.
[38] OpenAI. Gpt-4 technical report, 2023.
[39] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. Openclip. July 2021.
[40] Dongxu Li, Junnan Li, Hung Le, Guangsen Wang, Silvio Savarese, and Steven C. H. Hoi. Lavis: A library for language-vision intelligence, 2022.
[41] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Gray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022.
[42] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6904-6913, 2017.
[43] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: A visual question answering benchmark requiring external knowledge. In Proceedings of the IEEE/cvf conference on computer vision and pattern recognition, pages 3195-3204, 2019.
[44] Matthias Minderer, Alexey Gritsenko, Maxim Neumann Austin Stone, Dirk Weissenborn, Aravindh Mahendran Alexey Dosovitskiy, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, Xiao Wang, Xiaohua Zhai, Thomas Kipf, and Neil Houlsby. Simple open-vocabulary object detection with vision transformers. ECCV, 2022.
[45] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022.
[46] Yixin Lin, Austin S. Wang, Giovanni Sutanto, Akshara Rai, and Franziska Meier. Polymetis. https://facebookresearch. github.ierfairn/polymetis/, 2021.</p>
<h2>APPENDIX</h2>
<h2>A. Physical Concepts Details</h2>
<p>In this section, we provide details on how we define each of our ten physical concepts, which we communicate to crowd-workers before annotation. We also list the pre-defined options for categorical concepts.</p>
<h2>Continuous-Valued, Applicable to All Objects</h2>
<p>Mass: This refers to how heavy an object is. If an object has contents inside, this includes how heavy both the object and its contents are combined.
Fragility: This refers to how easily an object can be broken or damaged. An object has higher fragility than another if a person would handle it more carefully to avoid breaking it.
Deformability: This refers to how easily an object can change shape without breaking. An object has more deformability than another if less force is needed to change its shape without breaking it.
Density (held-out): This refers to the amount of mass per unit of volume of the object. If an object has contents inside, this only refers to the object, not the contents.</p>
<h2>Continuous-Valued, Applicable to Containers</h2>
<p>Liquid Capacity (held-out): This refers to the volume of liquid a container can contain without spilling.</p>
<h2>Categorical-Valued, Applicable to All Objects</h2>
<p>Material: This refers to what an object is made of. If an object is made of multiple materials, it refers to what material makes up the largest portion of the object that is visible. This does not refer to the contents of a container. The pre-defined options we include are plastic, glass, ceramic, metal, wood, paper, fabric, food, unknown, and other (annotator provides an open-ended response if this option is chosen).
Transparency: This refers to how much can be seen through an object. The pre-defined options we include are transparent, translucent, opaque, and unknown. Transparent refers to an object that can be seen clearly through, almost as if it was not there. Translucent refers to an object where some details can be seen through the object, but the details are not as clear as if it was transparent. Opaque refers to an object that cannot be seen through at all. This concept only refers to the object itself, and not the contents of a container. If different parts of an object have different levels of transparency, it refers what level applies to the largest visible portion of the object.</p>
<h2>Categorical-Valued, Applicable to Containers</h2>
<p>Contents: This refers to the contents of a container that are clearly visible and identifiable. The pre-defined options we include are nothing, water, food, oil, soap, unknown, and other (annotator provides an open-ended response if this option is chosen).
Can Contain Liquid: This refers to if a container can be used to transport a liquid across a room without a person
needing to be particularly careful about not spilling it. The pre-defined options we include are yes, no, and unknown. Is Sealed: This refers to if a container can be rotated by any amount in any direction without spilling its contents. The pre-defined options we include are yes, no, and unknown.</p>
<p>Container Categories. We define the following object categories from EgoObjects as containers: bottle, container, plate, bowl, mug, water glass, measuring cup, wine glass, tea cup, frying pan, flowerpot, tin can, kettle, vase, coffee cup, mixing bowl, saucer, jug, serving tray, pitcher (container), and picnic basket.</p>
<h2>B. Automatic Annotation Details</h2>
<p>We list the object categories we assign to high and low tiers for automating preference pair annotations for continuous concepts in Table VIII. We list the object categories for which we assign a concept label in Table IX. If a concept is not listed in these tables, we do not provide automatic annotations for that concept.</p>
<p>We originally assigned the label metal for material to the object category house/car key, but realized after crowdsourcing that not all instances of this category should have been given this assignment. Therefore, we manually labeled these examples for material, but still considered these to be automatic annotations for dataset purposes.</p>
<h2>C. Crowd-Sourcing Details</h2>
<p>Choosing Annotation Images. There are multiple bounding box images in EgoObjects for each object instance. To determine which to present for annotating an object, we choose the bounding box with the highest CLIP [22] similarity with the object's category label, as a heuristic for the object's visibility. We use the CLIP-ViT-H-14-laion2B-s32B-b79K model from OpenCLIP [39]. In Fig. 6, we show an example of randomly sampled bounding boxes for an instance of the object category guitar, arranged from left-to-right in decreasing order of CLIP similarity. The objects in bounding boxes with lower CLIP similarities tend to be less visible.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Fig. 6: Bounding boxes for an instance of guitar, in decreasing order of CLIP similarity
Attention Checks. We generate attention checks for crowdworkers by randomly sampling from the automatic annotations, which have known labels. For the concepts contents, density, and liquid capacity, for which there are no automatic annotations, we manually label a small set of objects for attention checks.
Other Details. Each annotation job on Prolific consisted of 250 annotations for a single concept, of which 25 are attention checks. Participants were paid an average of 15.50 US dollars per hour, and each annotation job took on average 20-30 minutes to complete, depending on the concept.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Concept</th>
<th style="text-align: center;">High</th>
<th style="text-align: center;">Low</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Mass</td>
<td style="text-align: center;">television, microwave oven, table, nightstand, chest of drawers</td>
<td style="text-align: center;">pen, paper, spoon, fork, glasses, sunglasses, scissors, watch, necklace, house/car key, pencil, earrings, ring, screwdriver, book, container, plate, bowl, pillow, remote control, clothing, mug, laptop, knife, mobile phone, toy, computer mouse, water glass, towel, headphones, spatula, frying pan, measuring cup, banana, wallet, blanket, candle, apple, wine glass, picture frame, computer keyboard, game controller/pad, tea cup, tin can, handbag, whisk, orange, belt, plastic bag, salt and pepper shakers, cutting board, perfume, stapler, footwear, tablet coputer, teddy bear, cookie, scarf, coffee cup, ball, mixing bowl, pear, alarm clock, light switch, bread, jacket, tennis ball, sandal, saucer, laptop charger, camera, yoga mat, power plugs and sockets, cream, shirt, baseball bat, sun hat, paper towel, kitchen knife, doll, can opener, sock, facial tissue holder, boot, hair dryer</td>
</tr>
<tr>
<td style="text-align: center;">Fragility</td>
<td style="text-align: center;">water glass, television</td>
<td style="text-align: center;">house/car key, dumbbell, screwdriver, kitchen knife</td>
</tr>
<tr>
<td style="text-align: center;">Deformability</td>
<td style="text-align: center;">pillow, clothing, towel, blanket, belt, plastic bag, scarf, jacket, yoga mat, shirt, paper towel, sock</td>
<td style="text-align: center;">remote control, mug, mobile phone, computer mouse, water glass, frying pan, flowerpot, scissors, wine glass, house/car key, dumbbell, cutting board, microwave oven, toaster, blender, pressure cooker, kitchen knife, table, spoon, laptop, knife, fork, glasses, spatula, sunglasses, chair, measuring cup, pencil, picture frame, computer keyboard, game controller/pad, tea cup, tin can, salt and pepper shakers, television, coffeemaker, stapler, tablet computer, kettle, vase, coffee cup, mixing bowl, computer monitor, stool, ring, alarm clock, light switch, saucer, printer, screwdriver, guitar, camera, jug, gas stove, baseball bat, humidifier, chest of drawers, sink, can opener, nightstand, hair dryer</td>
</tr>
</tbody>
</table>
<p>TABLE VIII: Object category assignments to high and low tiers for continuous concepts</p>
<table>
<thead>
<tr>
<th>Concept</th>
<th>Label</th>
<th>Categories</th>
</tr>
</thead>
<tbody>
<tr>
<td>Material</td>
<td>Plastic</td>
<td>remote control, computer mouse, computer keyboard, game controller/pad, plastic bag</td>
</tr>
<tr>
<td></td>
<td>Glass</td>
<td>water glass, wine glass</td>
</tr>
<tr>
<td></td>
<td>Metal</td>
<td>tin can, kitchen knife, can opener</td>
</tr>
<tr>
<td></td>
<td>Paper</td>
<td>book, paper, paper towel</td>
</tr>
<tr>
<td></td>
<td>Fabric</td>
<td>clothing, towel, blanket, scarf, sock</td>
</tr>
<tr>
<td></td>
<td>Food</td>
<td>banana, apple, orange, cookie, pear, bread</td>
</tr>
<tr>
<td>Transparency</td>
<td>Transparent</td>
<td>wine glass</td>
</tr>
<tr>
<td></td>
<td>Opaque</td>
<td>book, pillow, remote control, clothing, laptop, mobile phone, towel, headphones, spatula, chair, frying pan, banana, wallet, flowerpot, scissors, apple, houseplant, house/car key, pencil, computer keyboard, tin can, whisk, dumbbell, orange, belt, cutting board, toaster, teddy bear, tablet computer, cookie, pear, computer monitor, stool, light switch, bread, pressure cooker, scarf, laptop charger, guitar, camera, yoga mat, shirt, baseball bat, paper towel, kitchen knife, sink, chest of drawers, can opener, boot, nightstand, hair dryer</td>
</tr>
<tr>
<td>Can Contain Liquid</td>
<td>Yes</td>
<td>bottle, mug, water glass, measuring cup, wine glass, tea cup, kettle, coffee cup, mixing bowl, jug, pitcher (container), tin can</td>
</tr>
<tr>
<td></td>
<td>No</td>
<td>picnic basket, serving tray</td>
</tr>
<tr>
<td>Is Sealed</td>
<td>No</td>
<td>plate, bowl, mug, water glass, measuring cup, wine glass, tea cup, frying pan, flowerpot, kettle, vase, coffee cup, mixing bowl, saucer, jug, serving tray, pitcher (container), picnic basket</td>
</tr>
</tbody>
</table>
<p>TABLE IX: Concept label assignments of object categories for categorical concepts
In the annotation user interface, for each object example, the object is shown in the context of its surrounding scene, with the object indicated by its bounding box. We also provide the object's category label to help clarify which object is to be annotated. Crowd-workers can choose an annotation label by clicking on an associated button, or typing an associated keyboard key. We also provide a back option to go to the previous example to correct mistakes. For the concepts material and contents, the user may choose other as an option, whereupon they are presented with a text box to type an open-ended label. We do not annotate objects from the categories pet, cat, and dog, to omit objects that are living.</p>
<p>We provide instructions to annotators that are specific to each concept, to encourage annotations that agree with our concept definitions. We provide an image of the instruction page provided to annotators for the fragility concept, which also includes an example of the annotation user interface, in Fig. 7. The instructions for how to annotate each property are also repeated at the bottom of the annotation user interface.</p>
<p>We detail the number of examples per concept and dataset split for PhySObiectS in Table X. This is before any preprocessing of the data for annotator agreement or labels. For the crowd-sourced data, the count refers to the number of examples, not the number of annotations, for which there are three times as many. We also provide the percent of crowd-
sourced examples with majority agreement (at least 2/3) and unanimous agreement per concept in Table XI.</p>
<h2>D. Training Details</h2>
<p>Hyperparameters. We provide hyperparameters used for fine-tuning InstructBLIP in Table XII. These hyperparameters are largely derived from those proposed for fine-tuning BLIP-2 [10]. When fine-tuning, we only update the QFormer parameters, as done during instruction tuning for InstructBLIP. We use a linear warmup of the learning rate, followed by a linear decay with a minimum learning rate of 0 . We fine-tune using mixed precision bfloat16 training. We use a prompt template for questions, which is used both during training and inference. We load the InstructBLIP model using the LAVIS library [40]. We train and evaluate using the evaluation image processor provided by LAVIS, as we do not use image data augmentation.
Validation \&amp; Data Filtering. For most experiments, we evaluate on validation data every 250 gradient steps and choose the checkpoint with the lowest validation loss. For experiments fine-tuning for a single concept, we validate every 100 gradient steps. Our validation set consists of all validation data for all concepts without balancing, except we limit the number of automatically generated examples for mass and deformability to 100. For validation data, we only use the bounding box image with the highest CLIP object category similarity score for each object, which for</p>
<h1>Physical Properties Annotations</h1>
<h2>Instructions</h2>
<p>Thank you for participating in this user study! In this task, you will classify 250 pairs of images based on how fragile the objects are.
When you begin this study, you will see an interface similar to the one below:
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>You will be presented with a pair of images, each of which contains an object boxed in green. There is a label below the images to help confirm what specific object we want you to analyze. Only use the label to identify the object, not to inform your annotation. If there are multiple objects of the given label in the green box, choose the object that best matches the box.</p>
<p>Below these are options for you to click. You may also use your keyboard to indicate an option, using the letter in parenthesis for each button. If you make a mistake, you can click the green Back button (or press 'b'), which allows you to go back to the previous example.</p>
<p>If you consider one of the objects to be more fragile than the other, you should indicate which one is more fragile using either Left or Right. If you think both objects are roughly the same fragile, choose Equal. If you cannot determine from the images due to low resolution, bad lighting, or other reasons, choose Unclear.</p>
<p>We define fragility as how easily an object can be broken or damaged. You should consider an object more fragile than another if you would handle it more carefully to avoid breaking it.</p>
<p>In general, please try to give some annotation instead of Unclear if you are somewhat confident in your responses. Even if you may be wrong, try your best to give an answer.</p>
<p>Occasionally, you will randomly be given an example that has a known correct answer. You must achieve at least $80 \%$ accuracy on these examples to be compensated for the study.</p>
<p>Once you complete all 250 examples, you are done, and you will be given a link to return back to Prolific to confirm your submission.</p>
<h2>Ready to start?</h2>
<p>We see that your Prolific ID is EXAMPLE. Please contact the researchers if this is not the case.</p>
<h2>Consent Confirmation:</h2>
<p>Do you consent to the collection of your responses and a temporary collection of your Prolific ID?
O Yes
Start</p>
<p>Fig. 7: Instruction page for the fragility concept</p>
<table>
<thead>
<tr>
<th>Concept</th>
<th>Source</th>
<th>Train</th>
<th>Validation</th>
<th>Test</th>
</tr>
</thead>
<tbody>
<tr>
<td>Mass</td>
<td>Crowd-sourced</td>
<td>2108</td>
<td>86</td>
<td>56</td>
</tr>
<tr>
<td></td>
<td>Automatic</td>
<td>87269</td>
<td>4536</td>
<td>2688</td>
</tr>
<tr>
<td>Fragility</td>
<td>Crowd-sourced</td>
<td>2096</td>
<td>99</td>
<td>57</td>
</tr>
<tr>
<td></td>
<td>Automatic</td>
<td>2397</td>
<td>110</td>
<td>80</td>
</tr>
<tr>
<td>Deformability</td>
<td>Crowd-sourced</td>
<td>2101</td>
<td>84</td>
<td>65</td>
</tr>
<tr>
<td></td>
<td>Automatic</td>
<td>293540</td>
<td>13384</td>
<td>9888</td>
</tr>
<tr>
<td>Material</td>
<td>Crowd-sourced</td>
<td>2316</td>
<td>460</td>
<td>374</td>
</tr>
<tr>
<td></td>
<td>Automatic</td>
<td>612</td>
<td>130</td>
<td>119</td>
</tr>
<tr>
<td>Transparency</td>
<td>Crowd-sourced</td>
<td>1993</td>
<td>394</td>
<td>313</td>
</tr>
<tr>
<td></td>
<td>Automatic</td>
<td>1046</td>
<td>224</td>
<td>194</td>
</tr>
<tr>
<td>Contents</td>
<td>Crowd-sourced</td>
<td>641</td>
<td>134</td>
<td>125</td>
</tr>
<tr>
<td></td>
<td>Automatic</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>Can Contain Liquid</td>
<td>Crowd-sourced</td>
<td>318</td>
<td>68</td>
<td>64</td>
</tr>
<tr>
<td></td>
<td>Automatic</td>
<td>342</td>
<td>70</td>
<td>67</td>
</tr>
<tr>
<td>Is Sealed</td>
<td>Crowd-sourced</td>
<td>164</td>
<td>30</td>
<td>31</td>
</tr>
<tr>
<td></td>
<td>Automatic</td>
<td>444</td>
<td>91</td>
<td>86</td>
</tr>
<tr>
<td>Density (held-out)</td>
<td>Crowd-sourced</td>
<td>0</td>
<td>0</td>
<td>500</td>
</tr>
<tr>
<td>Liquid Capacity (held-out)</td>
<td>Crowd-sourced</td>
<td>0</td>
<td>0</td>
<td>500</td>
</tr>
</tbody>
</table>
<p>TABLE X: Number of examples per concept and dataset split</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Concept</th>
<th style="text-align: center;">\% Majority Agreement</th>
<th style="text-align: center;">\% Unanimous Agreement</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Mass</td>
<td style="text-align: center;">94.2</td>
<td style="text-align: center;">58.8</td>
</tr>
<tr>
<td style="text-align: left;">Fragility</td>
<td style="text-align: center;">93.6</td>
<td style="text-align: center;">53.1</td>
</tr>
<tr>
<td style="text-align: left;">Deformability</td>
<td style="text-align: center;">90.5</td>
<td style="text-align: center;">48.1</td>
</tr>
<tr>
<td style="text-align: left;">Material</td>
<td style="text-align: center;">93.7</td>
<td style="text-align: center;">59.4</td>
</tr>
<tr>
<td style="text-align: left;">Transparency</td>
<td style="text-align: center;">97.0</td>
<td style="text-align: center;">72.5</td>
</tr>
<tr>
<td style="text-align: left;">Contents</td>
<td style="text-align: center;">90.4</td>
<td style="text-align: center;">49.8</td>
</tr>
<tr>
<td style="text-align: left;">Can Contain Liquid</td>
<td style="text-align: center;">99.3</td>
<td style="text-align: center;">64.2</td>
</tr>
<tr>
<td style="text-align: left;">Is Sealed</td>
<td style="text-align: center;">98.2</td>
<td style="text-align: center;">74.7</td>
</tr>
<tr>
<td style="text-align: left;">Density (held-out)</td>
<td style="text-align: center;">93.3</td>
<td style="text-align: center;">50.7</td>
</tr>
<tr>
<td style="text-align: left;">Liquid Capacity (held-out)</td>
<td style="text-align: center;">89.1</td>
<td style="text-align: center;">46.0</td>
</tr>
</tbody>
</table>
<p>TABLE XI: Agreement among crowd-workers per concept
crowd-sourced data is also the bounding box image presented for annotation. For crowd-sourced validation data, we filter our data to only include examples with at least $2 / 3$ majority agreement among annotators, and only use the majority label. We do not apply this filtering for training data. For preference pair annotations, we remove data annotated with unclear.</p>
<p>Dataset Balancing. We construct sub-datasets for dataset balancing purposes. For the categorical concepts except is sealed, we combine the crowd-sourced and automatically annotated data for each concept into one sub-dataset per concept. For the other concepts, we keep separate subdatasets for crowd-sourced and automatically annotated data. We keep separate sub-datasets for is sealed because for its crowd-sourced data, we only train using the bounding box image for the object that was presented for annotation, rather than randomly sampling one of its bounding box images (as described in the below sub-section), as values for this concept may change for the same object instance. We keep
separate datasets for the continuous concepts because there is a large imbalance between the number of crowd-sourced and automatically annotated examples for these concepts. To balance these sub-datasets, we sample from each of them during training at a rate proportional to the square root of the number of annotations in the sub-dataset, as proposed in InstructBLIP for instruction tuning.
Additional Training Details. For most objects, each time we sample one for training, we randomly sample one of its bounding box images as input to the model, as a form of data augmentation. We do not do this with crowd-sourced data for the contents and is sealed concepts, because labels for these concepts may vary across different images of the same object. Instead, we only use the bounding box image that was presented for annotation.</p>
<p>To promote robustness to different queries to the VLM, we include object category labels in the question prompt for half of the training examples (e.g., asking "Is this bottle heavy?"),</p>
<table>
<thead>
<tr>
<th>Hyperparameter</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Max fine-tuning steps</td>
<td>10000</td>
</tr>
<tr>
<td>Warmup steps</td>
<td>1000</td>
</tr>
<tr>
<td>Learning rate</td>
<td>$1 \mathrm{e}-5$</td>
</tr>
<tr>
<td>Batch size</td>
<td>128</td>
</tr>
<tr>
<td>AdamW $\beta$</td>
<td>$(0.9,0.999)$</td>
</tr>
<tr>
<td>Weight decay</td>
<td>0.05</td>
</tr>
<tr>
<td>Image resolution</td>
<td>224</td>
</tr>
<tr>
<td>Prompt template</td>
<td>Question: $}$ Respond unknown if you are not sure. Short answer:</td>
</tr>
</tbody>
</table>
<p>TABLE XII: Hyperparameters for fine-tuning InstructBLIP</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Concept</th>
<th style="text-align: left;">Question Prompt</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Mass</td>
<td style="text-align: left;">Is this object heavy?</td>
</tr>
<tr>
<td style="text-align: left;">Fragility</td>
<td style="text-align: left;">Is this object fragile?</td>
</tr>
<tr>
<td style="text-align: left;">Deformability</td>
<td style="text-align: left;">Is this object deformable?</td>
</tr>
<tr>
<td style="text-align: left;">Material</td>
<td style="text-align: left;">What material is this object made of?</td>
</tr>
<tr>
<td style="text-align: left;">Transparency</td>
<td style="text-align: left;">Is this object transparent, translucent, or opaque?</td>
</tr>
<tr>
<td style="text-align: left;">Contents</td>
<td style="text-align: left;">What is inside this container?</td>
</tr>
<tr>
<td style="text-align: left;">Can Contain Liquid</td>
<td style="text-align: left;">Can this container hold a liquid inside easily?</td>
</tr>
<tr>
<td style="text-align: left;">Is Sealed</td>
<td style="text-align: left;">Is this container sealed?</td>
</tr>
<tr>
<td style="text-align: left;">Density (held-out)</td>
<td style="text-align: left;">Is this object dense?</td>
</tr>
<tr>
<td style="text-align: left;">Liquid Capacity (held-out)</td>
<td style="text-align: left;">Can this object hold a lot of liquid?</td>
</tr>
</tbody>
</table>
<p>TABLE XIII: Question prompts for each concept, without object category labels
and omit this information in the other half (e.g., asking "Is this object heavy?"). We experimented with training on one or multiple question prompts per concept, and found this to not significantly affect performance, so we only use one prompt per concept for simplicity. We include the question prompts for each concept in Table XIII. These are versions of the prompts without object category labels. When including category labels, we replace either the word "object" or "container" with the object's category label from EgoObjects. We also pluralize the prompt to have correct grammar if the category label is plural.</p>
<p>We experimented with removing Q-Former text conditioning in InstructBLIP while fine-tuning, and found this to improve results on general VQA evaluation and evaluation with held-out paraphrased question prompts, so we report results using models trained without this text conditioning. In our ablation results in Table XXI, we find that this does not significantly change performance for our main crowdsourced evaluation.</p>
<h2>E. Evaluation Details</h2>
<p>Further PhysObjects Evaluation Details. For crowdsourced test evaluation data, we only include examples with at least $2 / 3$ annotator agreement, and use the majority label as ground-truth. For categorical concepts, we predict by choosing the label with the highest likelihood out of all labels in PhYsObjects for the concept. For continuous concepts, we predict the object in a pair with the higher score from Section IV as the one with higher concept value. We only evaluate on preference examples with a definite,
non-equal preference label. For the Most Common baseline with continuous concepts, we also only include examples with a definite, non-equal preference when determining the most common label in the training data. We note that that Most Common baseline is not particularly meaningful for continuous concepts, because the preference labels and predictions are invariant to ordering in each preference pair. Therefore, a more natural baseline for these concepts would be random guessing, which would achieve $50 \%$ accuracy.</p>
<p>Similarly as with validation data, for test data we only evaluate using the bounding box image with the highest CLIP object category similarity per object, which for crowdsourced data is also the bounding box image presented for annotation. We evaluate using the same question prompts per concept as during training, which are listed in Table XIII. Unless stated otherwise, we report evaluation results without object category labels in the question prompt, because this gives slightly better results for the base InstructBLIP model.
Text Only Baseline. For this baseline, we use ground truth object category labels from EgoObjects. We use the 'text-davinci-003' InstructGPT model [41] as our LLM. For each concept, we use 128 in-context examples randomly sampled from the training data in PHYSObJects for that concept. Because in-context learning is limited by context length, and therefore it is desirable to use the best quality in-context examples when possible, we first apply to the training data the same majority filtering process used on crowd-sourced test data as described in the previous subsection. We also remove preference annotations with the label unclear, as</p>
<table>
<thead>
<tr>
<th>Concept</th>
<th>Question Prompt</th>
</tr>
</thead>
<tbody>
<tr>
<td>Mass</td>
<td>Does this object weigh a lot?</td>
</tr>
<tr>
<td>Fragility</td>
<td>Is this object easily breakable?</td>
</tr>
<tr>
<td>Deformability</td>
<td>Is this object easily bendable?</td>
</tr>
<tr>
<td>Material</td>
<td>What is this object made of?</td>
</tr>
<tr>
<td>Transparency</td>
<td>Would you describe this object as opaque, transparent, or translucent?</td>
</tr>
<tr>
<td>Contents</td>
<td>What does this container contain?</td>
</tr>
<tr>
<td>Can Contain Liquid</td>
<td>Is this container able to hold water inside easily?</td>
</tr>
<tr>
<td>Is Sealed</td>
<td>Is this container sealed shut?</td>
</tr>
</tbody>
</table>
<p>TABLE XIV: Paraphrased question prompts for main concepts, without object category labels
done in our VLM fine-tuning setup. We treat each example as a question answering task, using question prompts for each concept similar to those in Table XIII, but modified to refer to general object classes, rather than specific instances. We make predictions by selecting the most likely completion of the LLM conditioned on the in-context examples and test example. For categorical concepts, we first include in the LLM context all possible labels in PhysObjects for the concept. For continuous concepts, because we only evaluate on examples with definite preferences, we restrict predictions to only definite preferences using logit bias, although the incontext examples may include equal as a possible answer.
Paraphrased Question Prompts. In Table XIV, we list the paraphrased prompts used in the evaluation for Table IV.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">InstructBLIP</th>
<th style="text-align: center;">PG-InstructBLIP (ours)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">VQAv2</td>
<td style="text-align: center;">71.4</td>
<td style="text-align: center;">67.5</td>
</tr>
<tr>
<td style="text-align: left;">OK-VQA</td>
<td style="text-align: center;">52.4</td>
<td style="text-align: center;">48.7</td>
</tr>
</tbody>
</table>
<p>TABLE XV: Accuracy on existing VQA benchmarks
Limited VQA Degradation. Ideally, training on PhysObjects should be done while co-training on other vision and language datasets to preserve general reasoning abilities. In this work, we do not do this because we focus primarily on physical reasoning. However, we show that fine-tuning on only PhysObjects does not significantly degrade general VQA performance. In Table XV, we compare InstructBLIP to PG-InstructBLIP on VQAv2 [42] and OK-VQA [43]. These results suggest that existing systems using VLMs can benefit from PhysObjects for physical reasoning, without sacrificing other reasoning abilities.</p>
<p>We perform VQA evaluation using the LAVIS library, using their configurations for evaluation of BLIP-2. Although PG-InstructBLIP is fine-tuned without Q-Former text conditioning, we found that Q-Former text conditioning during VQA evaluation improved performance, so we report these results. We believe this is because InstructBLIP was instruction tuned with this text conditioning. We also experimented with VQA evaluation on PG-InstructBLIP fine-tuned with Q-Former text conditioning, but found this to have worse results, possibly due to overfitting on our limited variety of question prompts. We believe these issues can be mitigated by co-training on PhysObjects in combination with other
vision and language datasets, which we leave for future work.
Motivated by these VQA results, for our planning evaluations we also evaluate PG-InstructBLIP using Q-Former text conditioning, to avoid possible degradation when answering questions that do not pertain concepts in PhysObjects. We verified that evaluating PG-InstructBLIP using Q-Former text conditioning did not significantly affect test accuracy on PhysObjects.</p>
<h2>Including Object Category Labels in Question Prompts.</h2>
<p>We generally report evaluation results without ground-truth object category labels in the question prompt. In Table XVI, we compare including object category labels or not, and find that all models are not extremely sensitive to this.
Including Concept Definitions in Question Prompts. While we did not spend extensive effort designing the question prompts for each concept (shown in Table XIII), we aimed for them to be concise while still eliciting the desired concept. As seen in Table XVIII, the base InstructBLIP model achieves above chance performance on all concepts, suggesting that these prompts do elicit the desired concept to some extent. However, these prompts do not contain our definitions for each concept provided to annotators, as described in Appendix A. We analyze whether including concept definitions in the question prompt would improve base VLM performance in Table XVIII, which contains our original crowd-sourced test accuracy results, with additional evaluation of the base InstructBLIP model using modified prompts that contain concept definitions, which we provide in Table XVII. We find that while including concept definitions improves performance for some concepts (mass, deformability, contents, can contain liquid), this still does not match PG-InstructBLIP on these concepts, and overall performance in fact decreases compared to the original prompts. We believe this could be because InstructBLIP does not have strong enough language understanding to properly incorporate the concept definitions when providing responses. For this reason, and for simplicity, we use prompts without concept definitions in the rest of our experiments.
Using a Smaller VLM. To analyze the effect of VLM size on physical reasoning, in Table XIX we provide evaluation results using the InstructBLIP version with the smaller FlanT5 XL as its base LLM, compared to the Flan-T5 XXL version used in all other experiments. We find that while the smaller Flan-T5 XL version generally has worse base</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">InstructBLIP</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Single Concept FT (ours)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">PG-InstructBLIP (ours)</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Category Labels</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">No</td>
</tr>
<tr>
<td style="text-align: left;">Mass</td>
<td style="text-align: center;">60.0</td>
<td style="text-align: center;">62.2</td>
<td style="text-align: center;">$\mathbf{8 4 . 4}$</td>
<td style="text-align: center;">80.0</td>
<td style="text-align: center;">80.0</td>
<td style="text-align: center;">80.0</td>
</tr>
<tr>
<td style="text-align: left;">Fragility</td>
<td style="text-align: center;">75.7</td>
<td style="text-align: center;">78.4</td>
<td style="text-align: center;">91.2</td>
<td style="text-align: center;">91.2</td>
<td style="text-align: center;">$\mathbf{9 7 . 3}$</td>
<td style="text-align: center;">94.6</td>
</tr>
<tr>
<td style="text-align: left;">Deformability</td>
<td style="text-align: center;">69.8</td>
<td style="text-align: center;">67.4</td>
<td style="text-align: center;">88.4</td>
<td style="text-align: center;">$\mathbf{9 5 . 3}$</td>
<td style="text-align: center;">90.7</td>
<td style="text-align: center;">93.0</td>
</tr>
<tr>
<td style="text-align: left;">Material</td>
<td style="text-align: center;">73.3</td>
<td style="text-align: center;">67.1</td>
<td style="text-align: center;">$\mathbf{8 6 . 8}$</td>
<td style="text-align: center;">83.7</td>
<td style="text-align: center;">85.7</td>
<td style="text-align: center;">84.6</td>
</tr>
<tr>
<td style="text-align: left;">Transparency</td>
<td style="text-align: center;">84.5</td>
<td style="text-align: center;">85.8</td>
<td style="text-align: center;">89.1</td>
<td style="text-align: center;">89.4</td>
<td style="text-align: center;">89.8</td>
<td style="text-align: center;">$\mathbf{9 0 . 1}$</td>
</tr>
<tr>
<td style="text-align: left;">Contents</td>
<td style="text-align: center;">34.2</td>
<td style="text-align: center;">35.1</td>
<td style="text-align: center;">80.7</td>
<td style="text-align: center;">81.6</td>
<td style="text-align: center;">82.5</td>
<td style="text-align: center;">$\mathbf{8 3 . 3}$</td>
</tr>
<tr>
<td style="text-align: left;">Can Contain Liquid</td>
<td style="text-align: center;">57.8</td>
<td style="text-align: center;">59.4</td>
<td style="text-align: center;">84.4</td>
<td style="text-align: center;">84.4</td>
<td style="text-align: center;">82.8</td>
<td style="text-align: center;">$\mathbf{8 7 . 5}$</td>
</tr>
<tr>
<td style="text-align: left;">Is Sealed</td>
<td style="text-align: center;">71.0</td>
<td style="text-align: center;">74.2</td>
<td style="text-align: center;">80.6</td>
<td style="text-align: center;">80.6</td>
<td style="text-align: center;">$\mathbf{8 7 . 1}$</td>
<td style="text-align: center;">$\mathbf{8 7 . 1}$</td>
</tr>
<tr>
<td style="text-align: left;">Average</td>
<td style="text-align: center;">65.8</td>
<td style="text-align: center;">66.2</td>
<td style="text-align: center;">85.7</td>
<td style="text-align: center;">85.8</td>
<td style="text-align: center;">87.0</td>
<td style="text-align: center;">$\mathbf{8 7 . 5}$</td>
</tr>
</tbody>
</table>
<p>TABLE XVI: Test accuracy for main concepts on crowd-sourced PHYSObJECTS, with and without object category labels</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Concept</th>
<th style="text-align: left;">Question Prompt</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Mass</td>
<td style="text-align: left;">The heaviness of an object refers to its mass. It includes the contents of the <br> object if it has something inside it. Is this object heavy?</td>
</tr>
<tr>
<td style="text-align: left;">Fragility</td>
<td style="text-align: left;">Fragility refers to how easily an object can be broken or damaged. Is this <br> object fragile?</td>
</tr>
<tr>
<td style="text-align: left;">Deformability</td>
<td style="text-align: left;">Deformability refers to how easily an object can change shape without <br> breaking. Is this object deformable?</td>
</tr>
<tr>
<td style="text-align: left;">Material</td>
<td style="text-align: left;">The material of an object refers to what material makes up the largest portion <br> of the object that is visible. It does not refer to the contents of a container. <br> What material is this object made of?</td>
</tr>
<tr>
<td style="text-align: left;">Transparency</td>
<td style="text-align: left;">Transparency refers to how much can be seen through an object. A transparent <br> object can be clearly seen through, almost as if it was not there. A translucent <br> object can be seen through with some details, but not as clearly as if it was <br> transparent. An opaque object cannot be seen through at all. The transparency <br> of an object does not refer to the transparency of its contents if it has anything <br> inside it. Is this object transparent, translucent, or opaque? If different portions <br> of the object have different levels of transparency, respond with the level that <br> applies to the largest visible portion of the object.</td>
</tr>
<tr>
<td style="text-align: left;">Contents</td>
<td style="text-align: left;">What is inside this container? Only respond with contents that are clearly <br> visible and identifiable.</td>
</tr>
<tr>
<td style="text-align: left;">Can Contain Liquid</td>
<td style="text-align: left;">A container can contain liquid if it can be used to transport a liquid across a <br> room without a person needing to be particularly careful about not spilling it. <br> Can this container contain liquid?</td>
</tr>
<tr>
<td style="text-align: left;">Is Sealed</td>
<td style="text-align: left;">A container is sealed if it can be rotated by any amount in any direction <br> without spilling its contents if it has anything inside. Is this container sealed?</td>
</tr>
</tbody>
</table>
<p>TABLE XVII: Question prompts with definitions for each main concept, without object category labels</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">InstructBLIP</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">PG-InstructBLIP (ours)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Prompt Type</td>
<td style="text-align: center;">Original</td>
<td style="text-align: center;">w/ Concept Definitions</td>
<td style="text-align: center;">Original</td>
</tr>
<tr>
<td style="text-align: left;">Mass</td>
<td style="text-align: center;">62.2</td>
<td style="text-align: center;">71.1</td>
<td style="text-align: center;">$\mathbf{8 0 . 0}$</td>
</tr>
<tr>
<td style="text-align: left;">Fragility</td>
<td style="text-align: center;">78.4</td>
<td style="text-align: center;">67.6</td>
<td style="text-align: center;">$\mathbf{9 4 . 6}$</td>
</tr>
<tr>
<td style="text-align: left;">Deformability</td>
<td style="text-align: center;">67.4</td>
<td style="text-align: center;">69.8</td>
<td style="text-align: center;">$\mathbf{9 3 . 0}$</td>
</tr>
<tr>
<td style="text-align: left;">Material</td>
<td style="text-align: center;">67.1</td>
<td style="text-align: center;">66.0</td>
<td style="text-align: center;">$\mathbf{8 4 . 6}$</td>
</tr>
<tr>
<td style="text-align: left;">Transparency</td>
<td style="text-align: center;">85.8</td>
<td style="text-align: center;">65.3</td>
<td style="text-align: center;">$\mathbf{9 0 . 1}$</td>
</tr>
<tr>
<td style="text-align: left;">Contents</td>
<td style="text-align: center;">35.1</td>
<td style="text-align: center;">36.0</td>
<td style="text-align: center;">$\mathbf{8 3 . 3}$</td>
</tr>
<tr>
<td style="text-align: left;">Can Contain Liquid</td>
<td style="text-align: center;">59.4</td>
<td style="text-align: center;">64.1</td>
<td style="text-align: center;">$\mathbf{8 7 . 5}$</td>
</tr>
<tr>
<td style="text-align: left;">Is Sealed</td>
<td style="text-align: center;">74.2</td>
<td style="text-align: center;">64.5</td>
<td style="text-align: center;">$\mathbf{8 7 . 1}$</td>
</tr>
<tr>
<td style="text-align: left;">Average</td>
<td style="text-align: center;">66.2</td>
<td style="text-align: center;">63.0</td>
<td style="text-align: center;">$\mathbf{8 7 . 5}$</td>
</tr>
</tbody>
</table>
<p>TABLE XVIII: Test accuracy for main concepts on crowd-sourced PHYSObJECTS, with additional base InstructBLIP evaluation on prompts with definitions for each concept</p>
<table>
<thead>
<tr>
<th></th>
<th>InstructBLIP</th>
<th></th>
<th>PG-InstructBLIP (ours)</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>LLM Version</td>
<td>Flan-T5 XL</td>
<td>Flan-T5 XXL</td>
<td>Flan-T5 XL</td>
<td>Flan-T5 XXL</td>
</tr>
<tr>
<td>Mass</td>
<td>62.2</td>
<td>62.2</td>
<td>$\mathbf{8 2 . 2}$</td>
<td>80.0</td>
</tr>
<tr>
<td>Fragility</td>
<td>64.9</td>
<td>78.4</td>
<td>$\mathbf{9 7 . 3}$</td>
<td>94.6</td>
</tr>
<tr>
<td>Deformability</td>
<td>48.8</td>
<td>67.4</td>
<td>$\mathbf{9 7 . 7}$</td>
<td>93.0</td>
</tr>
<tr>
<td>Material</td>
<td>69.1</td>
<td>67.1</td>
<td>82.6</td>
<td>$\mathbf{8 4 . 6}$</td>
</tr>
<tr>
<td>Transparency</td>
<td>74.0</td>
<td>85.8</td>
<td>87.5</td>
<td>$\mathbf{9 0 . 1}$</td>
</tr>
<tr>
<td>Contents</td>
<td>18.4</td>
<td>35.1</td>
<td>$\mathbf{8 6 . 8}$</td>
<td>83.3</td>
</tr>
<tr>
<td>Can Contain Liquid</td>
<td>68.8</td>
<td>59.4</td>
<td>$\mathbf{8 7 . 5}$</td>
<td>$\mathbf{8 7 . 5}$</td>
</tr>
<tr>
<td>Is Sealed</td>
<td>67.7</td>
<td>74.2</td>
<td>80.6</td>
<td>$\mathbf{8 7 . 1}$</td>
</tr>
<tr>
<td>Average</td>
<td>59.2</td>
<td>66.2</td>
<td>$\mathbf{8 7 . 8}$</td>
<td>87.5</td>
</tr>
</tbody>
</table>
<p>TABLE XIX: Test accuracy for main concepts on crowd-sourced PHYSOBJECTS, using different VLM versions performance, after fine-tuning on PHYSOBJECTS, we see that performance is comparable between the two model sizes. This suggests that for physical reasoning, fine-tuning on human data such as PHYSOBJECTS could reduce the need for larger model sizes. While fine-tuned evaluation performance is similar across model sizes, for simplicity of comparison, we only report results using the larger Flan-T5 XXL models in all other experiments.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">InstructBLIP</th>
<th style="text-align: center;">PG-InstructBLIP (ours)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Mass</td>
<td style="text-align: center;">72.8</td>
<td style="text-align: center;">$\mathbf{9 9 . 9}$</td>
</tr>
<tr>
<td style="text-align: left;">Fragility</td>
<td style="text-align: center;">95.0</td>
<td style="text-align: center;">$\mathbf{1 0 0}$</td>
</tr>
<tr>
<td style="text-align: left;">Deformability</td>
<td style="text-align: center;">96.0</td>
<td style="text-align: center;">$\mathbf{9 8 . 8}$</td>
</tr>
<tr>
<td style="text-align: left;">Material</td>
<td style="text-align: center;">89.1</td>
<td style="text-align: center;">$\mathbf{9 8 . 3}$</td>
</tr>
<tr>
<td style="text-align: left;">Transparency</td>
<td style="text-align: center;">97.4</td>
<td style="text-align: center;">$\mathbf{1 0 0}$</td>
</tr>
<tr>
<td style="text-align: left;">Can Contain Liquid</td>
<td style="text-align: center;">98.5</td>
<td style="text-align: center;">$\mathbf{1 0 0}$</td>
</tr>
<tr>
<td style="text-align: left;">Is Sealed</td>
<td style="text-align: center;">$\mathbf{1 0 0}$</td>
<td style="text-align: center;">$\mathbf{1 0 0}$</td>
</tr>
<tr>
<td style="text-align: left;">Average</td>
<td style="text-align: center;">92.7</td>
<td style="text-align: center;">$\mathbf{9 9 . 6}$</td>
</tr>
</tbody>
</table>
<p>TABLE XX: Test accuracy for main concepts on automatically annotated PHYSOBJECTS</p>
<p>Results on Automatically Annotated Data. We report evaluation results on automatically annotated data in Table XX. Performance is generally much higher on this data compared to the crowd-sourced data, because these are easier examples that can be determined from object categories alone.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Fig. 8: Performance scaling on held-out concepts</p>
<p>Held-Out Concept Scaling. In these experiments, we evaluate the transfer abilities of InstructBLIP across different concepts when fine-tuning on PHYSOBJECTS. We fine-tune models on data from PHYSOBJECTS for all concepts except one, and then report results of additional fine-tuning on the held-out concept. We compare to fine-tuning base InstructBLIP without training on the other concepts, and base InstructBLIP without any fine-tuning. Results for three concepts are shown in Fig. 8. We chose these concepts because we believed they had the most generalization potential from the other concepts. We find that there are some signs of positive transfer on mass and fragility, although we see slight negative transfer on material. We believe that more positive transfer could be attained by co-training with other vision and language datasets.
Ablations. We report additional ablation results on crowdsourced PHYSOBJECTS examples in Table XXI. We list each ablation below:</p>
<p>1) No Auto Data: Instead of training on both crowdsourced and automatically annotated data, we train on only crowd-sourced data.
2) Filtered: Instead of training on all annotations for crowd-sourced data, we filter the data similarly as during evaluation: we only include examples with at least $2 / 3$ annotator agreement, and use the majority label as ground-truth.
3) Q-Former Text: Instead of removing Q-Former text conditioning during fine-tuning, we include it, as done for the original InstructBLIP model.
4) No Category Info: Instead of training on both question prompts with and without object category information, we only train on question prompts without it.
5) Only Category Info: Instead of training on both question prompts with and without object category information, we only train on question prompts with it. Here, unlike the rest of the evaluations, we evaluate with object category information to match the training setup.
We find that overall performance for each ablated version of our model does not change significantly, suggesting some robustness of our fine-tuning process to different design decisions. In particular, we find that including automatically</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">PG-InstructBLIP (ours)</th>
<th style="text-align: center;">No Auto Data</th>
<th style="text-align: center;">Filtered</th>
<th style="text-align: center;">Q-Former Text</th>
<th style="text-align: center;">No Category Info</th>
<th style="text-align: center;">Only Category Info</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Mass</td>
<td style="text-align: center;">80.0</td>
<td style="text-align: center;">$\mathbf{8 4 . 4}$</td>
<td style="text-align: center;">75.6</td>
<td style="text-align: center;">80.0</td>
<td style="text-align: center;">75.6</td>
<td style="text-align: center;">77.8</td>
</tr>
<tr>
<td style="text-align: left;">Fragility</td>
<td style="text-align: center;">94.6</td>
<td style="text-align: center;">94.6</td>
<td style="text-align: center;">97.3</td>
<td style="text-align: center;">97.3</td>
<td style="text-align: center;">$\mathbf{1 0 0}$</td>
<td style="text-align: center;">$\mathbf{1 0 0}$</td>
</tr>
<tr>
<td style="text-align: left;">Deformability</td>
<td style="text-align: center;">$\mathbf{9 3 . 0}$</td>
<td style="text-align: center;">$\mathbf{9 3 . 0}$</td>
<td style="text-align: center;">90.7</td>
<td style="text-align: center;">90.7</td>
<td style="text-align: center;">$\mathbf{9 3 . 0}$</td>
<td style="text-align: center;">88.4</td>
</tr>
<tr>
<td style="text-align: left;">Material</td>
<td style="text-align: center;">84.6</td>
<td style="text-align: center;">83.4</td>
<td style="text-align: center;">83.4</td>
<td style="text-align: center;">85.4</td>
<td style="text-align: center;">84.6</td>
<td style="text-align: center;">$\mathbf{8 6 . 5}$</td>
</tr>
<tr>
<td style="text-align: left;">Transparency</td>
<td style="text-align: center;">90.1</td>
<td style="text-align: center;">89.4</td>
<td style="text-align: center;">89.1</td>
<td style="text-align: center;">$\mathbf{9 2 . 1}$</td>
<td style="text-align: center;">89.8</td>
<td style="text-align: center;">91.7</td>
</tr>
<tr>
<td style="text-align: left;">Contents</td>
<td style="text-align: center;">83.3</td>
<td style="text-align: center;">81.6</td>
<td style="text-align: center;">85.1</td>
<td style="text-align: center;">$\mathbf{8 7 . 7}$</td>
<td style="text-align: center;">84.2</td>
<td style="text-align: center;">86.8</td>
</tr>
<tr>
<td style="text-align: left;">Can Contain Liquid</td>
<td style="text-align: center;">87.5</td>
<td style="text-align: center;">89.1</td>
<td style="text-align: center;">85.9</td>
<td style="text-align: center;">84.4</td>
<td style="text-align: center;">$\mathbf{9 0 . 6}$</td>
<td style="text-align: center;">84.4</td>
</tr>
<tr>
<td style="text-align: left;">Is Sealed</td>
<td style="text-align: center;">$\mathbf{8 7 . 1}$</td>
<td style="text-align: center;">83.9</td>
<td style="text-align: center;">83.9</td>
<td style="text-align: center;">71.0</td>
<td style="text-align: center;">80.6</td>
<td style="text-align: center;">$\mathbf{8 7 . 1}$</td>
</tr>
<tr>
<td style="text-align: left;">Average</td>
<td style="text-align: center;">87.5</td>
<td style="text-align: center;">87.4</td>
<td style="text-align: center;">86.4</td>
<td style="text-align: center;">86.1</td>
<td style="text-align: center;">87.3</td>
<td style="text-align: center;">$\mathbf{8 7 . 8}$</td>
</tr>
</tbody>
</table>
<p>TABLE XXI: Ablation results for main concepts on crowd-sourced PHYSÖBJECTS
annotated data does not significantly impact performance on crowd-sourced data, which perhaps is not surprising because base InstructBLIP already performs well on automatically annotated examples, as seen in Table XX. Only Category Info very slightly improves upon PG-InstructBLIP, but uses privileged object category information at evaluation time.</p>
<h2>F. Real Scene Planning Evaluation Details</h2>
<p>Planning Framework. Our planning framework consists of first providing the scene image to an OWL-ViT ViT-L/14 open-vocabulary object detector [44], which produces object bounding boxes and category labels from the EgoObjects categories. We then provide the list of detected objects and the task instruction to our LLM, which is GPT-4 [38] with temperature 0 . The LLM is additionally provided with the robotic primitives, and a few-shot chain-of-thought prompt [45] with instructions to ask questions about objects in the scene to determine how to complete the task, and then produce a plan using the primitives. There is no constraint on the questions that the LLM can ask, except for encouragement in the prompt to ask questions that can be answered with yes or no. The same prompt is used for all scenes and tasks, which we provide in Listing 1.</p>
<p>After the LLM asks a set of object-centric questions, a VLM answers each question prompted with the bounding box of the object indicated by the LLM, and then provides the LLM with its highest likelihood responses and their associated likelihoods/confidence scores, as done in prior work for VQA [30]. This continues until the LLM decides it has enough information, whereupon it either indicates that the task is not possible, or produces a plan consisting of a list of primitives to execute for the task. The few-shot examples in Listing 1 illustrate how interaction between the LLM and VLM for planning is structured.
Primitives. We list the primitives for our real scene planning evaluation below:</p>
<ul>
<li>go to object $[\mathrm{X}]$</li>
<li>pick up object $[\mathrm{X}]$</li>
<li>bring to human object $[\mathrm{X}]$</li>
<li>put down object $[\mathrm{X}]$</li>
<li>done</li>
</ul>
<p>The primitives (except done) are parameterized by a letter (in place of $[\mathrm{X}]$ ) that identifies each detected object in the
scene. The assignment of letters is provided in the list of object detections given to the LLM planner.
Scenes and Tasks. In Table XXII, we provide the scene images in our evaluation, and the detected objects and task instructions for each scene. We also indicate the task type for each instruction.
Prompts. We provide the prompts used by our LLM-based planning framework for our scene planning evaluation. The version with VLM interaction is in Listing 1 and the version without VLM interaction is in Listing 2. The parts of the prompts in square brackets are replaced with the corresponding information specific to the task, in the same format as the prompt example.</p>
<p>Listing 1: Prompt for LLM planner with VLM interaction.</p>
<div class="codehilite"><pre><span></span><code><span class="n">You</span><span class="w"> </span><span class="k">are</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">household</span><span class="w"> </span><span class="n">robot</span><span class="p">.</span><span class="w"> </span><span class="n">You</span><span class="w"> </span><span class="k">are</span><span class="w"> </span><span class="n">able</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">move</span><span class="w"> </span><span class="n">most</span>
<span class="w">    </span><span class="n">household</span><span class="w"> </span><span class="n">objects</span><span class="p">,</span><span class="w"> </span><span class="n">but</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="k">large</span><span class="w"> </span><span class="ow">or</span><span class="w"> </span><span class="n">heavy</span><span class="w"> </span><span class="n">furniture</span><span class="p">.</span>
<span class="w">    </span><span class="n">You</span><span class="w"> </span><span class="k">are</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">safe</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="k">break</span><span class="w"> </span><span class="n">anything</span><span class="p">.</span>
<span class="n">You</span><span class="w"> </span><span class="n">will</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">given</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">list</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">objects</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">scene</span><span class="p">.</span>
<span class="n">A</span><span class="w"> </span><span class="n">human</span><span class="w"> </span><span class="n">will</span><span class="w"> </span><span class="n">give</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">task</span><span class="w"> </span><span class="n">instruction</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">perform</span><span class="p">.</span>
<span class="k">First</span><span class="p">,</span><span class="w"> </span><span class="n">ask</span><span class="w"> </span><span class="n">questions</span><span class="w"> </span><span class="n">about</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">objects</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">learn</span><span class="w"> </span><span class="n">more</span><span class="w"> </span><span class="n">about</span>
<span class="w">    </span><span class="n">them</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">determine</span><span class="w"> </span><span class="n">how</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">properly</span><span class="w"> </span><span class="n">complete</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">task</span><span class="p">.</span>
<span class="n">Indicate</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="k">object</span><span class="w"> </span><span class="n">letters</span><span class="w"> </span><span class="k">before</span><span class="w"> </span><span class="n">asking</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">question</span><span class="p">,</span>
<span class="w">    </span><span class="ow">and</span><span class="w"> </span><span class="n">do</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="n">provide</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">answer</span><span class="p">.</span>
<span class="nf">Format</span><span class="w"> </span><span class="k">each</span><span class="w"> </span><span class="n">question</span><span class="w"> </span><span class="ow">like</span><span class="w"> </span><span class="ss">&quot;Question about object [A, B]:</span>
<span class="ss">    Is this object heavy?&quot;</span><span class="p">.</span>
<span class="k">Only</span><span class="w"> </span><span class="n">ask</span><span class="w"> </span><span class="n">informative</span><span class="w"> </span><span class="n">questions</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">help</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">understand</span>
<span class="w">    </span><span class="n">how</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">properly</span><span class="w"> </span><span class="n">complete</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">task</span><span class="p">.</span>
<span class="ow">Some</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="k">object</span><span class="w"> </span><span class="n">descriptions</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">list</span><span class="w"> </span><span class="n">may</span><span class="w"> </span><span class="n">be</span>
<span class="w">    </span><span class="n">inaccurate</span><span class="p">,</span><span class="w"> </span><span class="n">so</span><span class="w"> </span><span class="n">it</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">good</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">ask</span><span class="w"> </span><span class="n">questions</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">confirm</span>
<span class="w">    </span><span class="n">information</span><span class="w"> </span><span class="n">about</span><span class="w"> </span><span class="n">them</span><span class="p">.</span>
<span class="n">Ask</span><span class="w"> </span><span class="n">questions</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">can</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">answered</span><span class="w"> </span><span class="k">with</span><span class="w"> </span><span class="n">yes</span><span class="w"> </span><span class="ow">or</span><span class="w"> </span><span class="k">no</span><span class="w"> </span><span class="k">when</span>
<span class="w">    </span><span class="n">possible</span><span class="p">.</span>
<span class="k">Only</span><span class="w"> </span><span class="n">ask</span><span class="w"> </span><span class="n">one</span><span class="w"> </span><span class="n">question</span><span class="w"> </span><span class="k">at</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="nc">time</span><span class="p">.</span>
<span class="k">After</span><span class="w"> </span><span class="k">each</span><span class="w"> </span><span class="n">question</span><span class="p">,</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">will</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">given</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">response</span>
<span class="w">    </span><span class="n">containing</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="mi">3</span><span class="w"> </span><span class="n">most</span><span class="w"> </span><span class="n">likely</span><span class="w"> </span><span class="n">possible</span><span class="w"> </span><span class="n">answers</span><span class="w"> </span><span class="k">for</span>
<span class="w">    </span><span class="k">each</span><span class="w"> </span><span class="k">object</span><span class="p">,</span><span class="w"> </span><span class="k">with</span><span class="w"> </span><span class="k">corresponding</span><span class="w"> </span><span class="n">confidence</span><span class="w"> </span><span class="n">scores</span><span class="p">.</span>
<span class="n">An</span><span class="w"> </span><span class="n">example</span><span class="w"> </span><span class="n">response</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">one</span><span class="w"> </span><span class="k">object</span><span class="w"> </span><span class="n">would</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="ss">&quot;A: yes (0.8),</span>
<span class="ss">    no (0.1), unknown (0.1)&quot;</span><span class="p">.</span>
<span class="n">Otherwise</span><span class="p">,</span><span class="w"> </span><span class="k">use</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">information</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">have</span><span class="w"> </span><span class="n">obtained</span><span class="w"> </span><span class="k">to</span>
<span class="w">    </span><span class="n">produce</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">numbered</span><span class="w"> </span><span class="n">list</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">actions</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">perform</span><span class="w"> </span><span class="n">the</span>
<span class="w">    </span><span class="n">task</span><span class="p">,</span><span class="w"> </span><span class="n">starting</span><span class="w"> </span><span class="k">with</span><span class="w"> </span><span class="ss">&quot;Plan:&quot;</span><span class="p">.</span>
<span class="n">You</span><span class="w"> </span><span class="k">are</span><span class="w"> </span><span class="k">only</span><span class="w"> </span><span class="n">able</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">perform</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">following</span><span class="w"> </span><span class="n">actions</span><span class="p">,</span><span class="w"> </span><span class="k">where</span>
<span class="w">    </span><span class="ss">&quot;X&quot;</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">placeholder</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="k">object</span><span class="w"> </span><span class="n">letter</span><span class="w"> </span><span class="k">from</span><span class="w"> </span><span class="n">the</span>
<span class="w">    </span><span class="n">given</span><span class="w"> </span><span class="n">list</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="nl">objects</span><span class="p">:</span>
<span class="mf">1.</span><span class="w"> </span><span class="k">go</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="k">object</span><span class="w"> </span><span class="n">X</span>
<span class="mf">2.</span><span class="w"> </span><span class="n">pick</span><span class="w"> </span><span class="n">up</span><span class="w"> </span><span class="k">object</span><span class="w"> </span><span class="n">X</span>
<span class="mf">3.</span><span class="w"> </span><span class="n">bring</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">human</span><span class="w"> </span><span class="k">object</span><span class="w"> </span><span class="n">X</span>
<span class="mf">4.</span><span class="w"> </span><span class="n">put</span><span class="w"> </span><span class="n">down</span><span class="w"> </span><span class="k">object</span><span class="w"> </span><span class="n">X</span>
<span class="mf">5.</span><span class="w"> </span><span class="n">done</span>
<span class="k">Only</span><span class="w"> </span><span class="n">perform</span><span class="w"> </span><span class="n">actions</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="k">are</span><span class="w"> </span><span class="n">possible</span><span class="p">.</span><span class="w"> </span><span class="n">You</span><span class="w"> </span><span class="k">are</span><span class="w"> </span><span class="n">able</span><span class="w"> </span><span class="k">to</span>
<span class="w">    </span><span class="n">move</span><span class="w"> </span><span class="n">most</span><span class="w"> </span><span class="n">household</span><span class="w"> </span><span class="n">objects</span><span class="p">,</span><span class="w"> </span><span class="n">including</span><span class="w"> </span><span class="n">most</span>
<span class="w">    </span><span class="n">electronic</span><span class="w"> </span><span class="n">appliances</span><span class="p">,</span><span class="w"> </span><span class="n">but</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="k">large</span><span class="w"> </span><span class="ow">or</span><span class="w"> </span><span class="n">heavy</span>
<span class="w">    </span><span class="n">furniture</span><span class="w"> </span><span class="ow">or</span><span class="w"> </span><span class="n">parts</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">room</span><span class="w"> </span><span class="p">(</span><span class="ow">like</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">door</span><span class="w"> </span><span class="ow">or</span><span class="w"> </span><span class="n">heater</span><span class="p">).</span>
<span class="w">    </span><span class="n">You</span><span class="w"> </span><span class="k">are</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">safe</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="k">break</span><span class="w"> </span><span class="n">anything</span><span class="p">.</span><span class="w"> </span><span class="nf">Choose</span>
<span class="w">    </span><span class="n">the</span><span class="w"> </span><span class="ss">&quot;done&quot;</span><span class="w"> </span><span class="k">action</span><span class="w"> </span><span class="k">when</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="k">plan</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">complete</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="k">then</span>
<span class="w">    </span><span class="n">stop</span><span class="p">.</span><span class="w"> </span><span class="k">If</span><span class="w"> </span><span class="k">no</span><span class="w"> </span><span class="k">plan</span><span class="w"> </span><span class="n">can</span><span class="w"> </span><span class="n">satisfy</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">task</span><span class="p">,</span><span class="w"> </span><span class="n">tell</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">human</span>
<span class="w">    </span><span class="p">,</span><span class="w"> </span><span class="nf">choose</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="ss">&quot;done&quot;</span><span class="w"> </span><span class="k">action</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">stop</span><span class="p">.</span>
</code></pre></div>

<p>Only respond with the plan list and do not include any other text.</p>
<h2>Scene 1:</h2>
<p>The following objects are in the scene: A (bottle), B ( bowl), C (countertop), D (shirt), E (lock)</p>
<p>Instruction: Bring me the heaviest object.
Thought: I can ask which object is heavy out of all the objects, and I can find the one with the largest weight that I can carry.
Question about [A, B, C, D, E]: Is this object heavy? Answer:
A) No (0.50), Yes (0.24), Unknown (0.21)
B) No (0.90), Unknown (0.05), Yes (0.04)
C) Yes (0.80), Unknown (0.10), No (0.05)
D) No (0.73), Unknown (0.20), Yes (0.06)
E) No (0.41), Yes (0.35), Unknown (0.19)</p>
<p>Thought: The heaviest object is the one with the highest Yes value or the lowest No value (if none are Yes). The heaviest object is C. However, I cannot carry a countertop since it is a heavy piece of furniture. Therefore, I will to choose the next heaviest, which would be E, a lock that I can carry easily.
Plan:
1. Go to object E
2. Pick up object E
3. Bring to human object E
4. Done</p>
<p>Instruction: Bring me the most deformable container. Thought: I can first ask which objects are containers out of all the objects I can carry. Of these containers, I can ask which is deformable.
Question about [A, B, C, D, E]: Is this object a container ?
Answer:
A) Yes (0.55), No (0.20), Unknown (0.19)
B) Yes (0.90), Unknown (0.05), No (0.04)
C) No (0.85), Unknown (0.06), Yes (0.05)
D) No (0.62), Unknown (0.20), Yes (0.06)
E) No (0.41), Yes (0.35), Unknown (0.19)</p>
<p>Thought: The only objects that are confidently Yes are B and A. I should ask which is deformable next.
Question about [A, B]: Is this object deformable? Answer:
A) Yes (0.80), Unknown (0.15), No (0.04)
B) No (0.55), Unknown (0.26), Yes (0.05)</p>
<p>Thought: The most deformable object is the one with the highest Yes value or the lowest No value (if none are Yes). Since some of the answers are Yes, the answer is A (the highest yes value).
Plan:
1. Go to object A
2. Pick up object A
3. Bring to human object A
4. Done</p>
<p>Scene 2:
The following objects are in the scene: [list of objects in the scene]</p>
<p>Instruction: [instruction specified here]</p>
<p>Listing 2: Prompt for LLM planner without VLM interaction.</p>
<div class="codehilite"><pre><span></span><code><span class="n">You</span><span class="w"> </span><span class="k">are</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">household</span><span class="w"> </span><span class="n">robot</span><span class="p">.</span><span class="w"> </span><span class="n">You</span><span class="w"> </span><span class="k">are</span><span class="w"> </span><span class="n">able</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">move</span><span class="w"> </span><span class="n">most</span>
<span class="w">    </span><span class="n">household</span><span class="w"> </span><span class="n">objects</span><span class="p">,</span><span class="w"> </span><span class="n">but</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="k">large</span><span class="w"> </span><span class="ow">or</span><span class="w"> </span><span class="n">heavy</span><span class="w"> </span><span class="n">furniture</span><span class="p">.</span>
<span class="w">    </span><span class="n">You</span><span class="w"> </span><span class="k">are</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">safe</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="k">break</span><span class="w"> </span><span class="n">anything</span><span class="p">.</span>
<span class="n">You</span><span class="w"> </span><span class="n">will</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">given</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">list</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">objects</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">scene</span><span class="p">.</span>
<span class="n">A</span><span class="w"> </span><span class="n">human</span><span class="w"> </span><span class="n">will</span><span class="w"> </span><span class="n">give</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">task</span><span class="w"> </span><span class="n">instruction</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">perform</span><span class="p">.</span>
<span class="n">The</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="k">object</span><span class="w"> </span><span class="n">information</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">produce</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">numbered</span><span class="w"> </span><span class="n">list</span><span class="w"> </span><span class="k">of</span>
<span class="w">    </span><span class="n">actions</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">perform</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">task</span><span class="p">,</span><span class="w"> </span><span class="n">starting</span><span class="w"> </span><span class="k">with</span><span class="w"> </span><span class="ss">&quot;Plan:&quot;</span><span class="p">.</span>
<span class="n">You</span><span class="w"> </span><span class="k">are</span><span class="w"> </span><span class="k">only</span><span class="w"> </span><span class="n">able</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">perform</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">following</span><span class="w"> </span><span class="n">actions</span><span class="p">,</span><span class="w"> </span><span class="k">where</span>
<span class="w">    </span><span class="ss">&quot;X&quot;</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">placeholder</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="k">object</span><span class="w"> </span><span class="n">letter</span><span class="w"> </span><span class="k">from</span><span class="w"> </span><span class="n">the</span>
<span class="w">    </span><span class="n">given</span><span class="w"> </span><span class="n">list</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="nl">objects</span><span class="p">:</span>
<span class="mf">1.</span><span class="w"> </span><span class="k">go</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="k">object</span><span class="w"> </span><span class="n">X</span>
<span class="mf">2.</span><span class="w"> </span><span class="n">pick</span><span class="w"> </span><span class="n">up</span><span class="w"> </span><span class="k">object</span><span class="w"> </span><span class="n">X</span>
<span class="mf">3.</span><span class="w"> </span><span class="n">bring</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">human</span><span class="w"> </span><span class="k">object</span><span class="w"> </span><span class="n">X</span>
<span class="mf">4.</span><span class="w"> </span><span class="n">put</span><span class="w"> </span><span class="n">down</span><span class="w"> </span><span class="k">object</span><span class="w"> </span><span class="n">X</span>
<span class="mf">5.</span><span class="w"> </span><span class="n">done</span>
</code></pre></div>

<p>Only perform actions that are possible. You are able to
move most household objects, including most electronic appliances, but not large or heavy furniture or parts of a room (like a door or heater). You are to be safe and not break anything. Choose the "done" action when the plan is complete and then stop. If no plan can satify the task, tell the human, choose the "done" action and stop.
Only respond with the plan list and do not include any other text.</p>
<p>Scene 1:
The following objects are in the scene: A (bottle), B ( bowl), C (countertop), D (shirt), E (lock)</p>
<p>Instruction: Bring me the heaviest object.
Thought: I cannot carry a countertop since it is a heavy piece of furniture. Out of the rest, a good guess would be the lock.
Plan:
E. Go to object E
2. Pick up object E
3. Bring to human object E
4. Done</p>
<p>Instruction: Bring me the most deformable container. Thought: Typically shirts are easy to fold, so a good choice for the most deformable object would be the shirt.
Plan:
1. Go to object D
2. Pick up object D
3. Bring to human object D
4. Done</p>
<p>Scene 2:
The following objects are in the scene: [list of objects in the scene]</p>
<p>Instruction: [instruction specified here]</p>
<p>Evaluation Procedure. We evaluate task planning accuracy using a non-author human evaluator. For each evaluation, the evaluator is given the task instruction, the image of the scene, the list of detected objects in the scene and their bounding boxes, and the generated task plan, and they are asked to evaluate whether the task plan successfully performed the task instruction for the given scene. We provide the following instructions to the evaluator on what to consider when evaluating whether a plan was correct:
Instructions: For each scene, there is a list of objects (under 'Options:'). Below that is a table of tasks for that scene. The instruction given to a robot is on the left. On the right are the choices from 3 different robots. You need to mark which ones are correct or incorrect. It may be possible that multiple robots got it right or none of them got it right. Be aware that in tasks that involve moving objects, the robot should not plan to move an object that is very heavy, like large furniture.</p>
<p>While the planner usually creates plans using only the provided primitives, it sometimes specifies primitives that were not provided. Because the purpose of this evaluation is on assessing if a LLM planner can benefit from physical reasoning using a VLM, and not on creating a functional planning system, we do not do anything to handle these cases. We the evaluator to judge if these plans satisfy the task instruction like the others. We provide example executions for different versions of our planning framework on our website.</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>1) buttle
2) pitcher (container)
3) bowl [flatter bowl]
4) towel [shirt]
5) countertop
6) bowl [taller ceramic bowl]
7) measuring cup [lock]</p>
<h2>1) Bring me the heaviest object. [S]</h2>
<p>2) Bring me the most deformable object. [S]
3) Bring me the most fragile object. [S]
4) Bring me all containers that you can confidently determine have water. [M]
5) Bring me the container with oil. [M]
6) Among all empty containers, bring me the ones that cannot be used to carry water. [M]
7) Bring me the metal object. [S]</p>
<p>1) suitcase [blue crate]
2) stool
3) hair dryer [mirror]
4) chair [chair that the mirror is on]
5) dishwasher [metal cabinet in top right]
6) chair [blue chair]
7) bottle [Elmer glue container]
8) bottle [Mod Podge container]
9) container [paint thinner container]
10) desk
11) mug [mug with paintbrushes]
12) facial tissue holder [container with glitter]
13) pencil
1) Bring me the heaviest object. [S]
2) Bring me a metal container. [M]
3) Bring me a small, empty cup that I can fill with water to clean my paintbrushes. If there are none, tell me that there are no small empty cups. [M]
4) Bring me the clear container with art supplies. [C]
5) Bring me the metal object that is reflective. [M]
6) Bring me paint thinner. [C]
7) Bring me a wooden object. [S]</p>
<p>TABLE XXII: Scene images, object detections, and task instructions for our real scene planning evaluation (scenes 1 and 2). The object category labels given by OWL-ViT are sometimes inaccurate or ambiguous, in which case we provide more precise labels in square brackets. Note that the planner only has access to the original OWL-ViT labels. Tasks are labeled with S, M, or C for Single Concept, Multi-Concept, or Common Knowledge, respectively.</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<table>
<thead>
<tr>
<th style="text-align: left;">1)</th>
<th style="text-align: left;">clothing [green hoodie]</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">2)</td>
<td style="text-align: left;">towel</td>
</tr>
<tr>
<td style="text-align: left;">3)</td>
<td style="text-align: left;">clothing [striped shirt]</td>
</tr>
<tr>
<td style="text-align: left;">4)</td>
<td style="text-align: left;">bottle [sunscreen bottle]</td>
</tr>
<tr>
<td style="text-align: left;">5)</td>
<td style="text-align: left;">towel [socks]</td>
</tr>
<tr>
<td style="text-align: left;">6)</td>
<td style="text-align: left;">mouse [ear thermometer]</td>
</tr>
<tr>
<td style="text-align: left;">7)</td>
<td style="text-align: left;">suitcase</td>
</tr>
<tr>
<td style="text-align: left;">8)</td>
<td style="text-align: left;">bottle [hand sanitizer]</td>
</tr>
<tr>
<td style="text-align: left;">9)</td>
<td style="text-align: left;">hair dryer [dumbbell]</td>
</tr>
<tr>
<td style="text-align: left;">10)</td>
<td style="text-align: left;">clothing [blue shirt]</td>
</tr>
</tbody>
</table>
<p><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<table>
<thead>
<tr>
<th style="text-align: center;">1)</th>
<th style="text-align: center;">facial tissue holder [paper towel dispenser]</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">2)</td>
<td style="text-align: center;">light switch [left electric outlet]</td>
</tr>
<tr>
<td style="text-align: center;">3)</td>
<td style="text-align: center;">light switch [right electric outlet]</td>
</tr>
<tr>
<td style="text-align: center;">4)</td>
<td style="text-align: center;">mixer</td>
</tr>
<tr>
<td style="text-align: center;">5)</td>
<td style="text-align: center;">toaster</td>
</tr>
<tr>
<td style="text-align: center;">6)</td>
<td style="text-align: center;">kettle</td>
</tr>
<tr>
<td style="text-align: center;">7)</td>
<td style="text-align: center;">paper towel</td>
</tr>
<tr>
<td style="text-align: center;">8)</td>
<td style="text-align: center;">water glass [plastic cup]</td>
</tr>
<tr>
<td style="text-align: center;">9)</td>
<td style="text-align: center;">salt and pepper shakers [salt]</td>
</tr>
<tr>
<td style="text-align: center;">10)</td>
<td style="text-align: center;">bottle [jam container]</td>
</tr>
<tr>
<td style="text-align: center;">11)</td>
<td style="text-align: center;">frying pan [baking pan]</td>
</tr>
<tr>
<td style="text-align: center;">12)</td>
<td style="text-align: center;">container [salmon-colored container]</td>
</tr>
<tr>
<td style="text-align: center;">13)</td>
<td style="text-align: center;">salt and pepper shakers [pepper]</td>
</tr>
<tr>
<td style="text-align: center;">14)</td>
<td style="text-align: center;">countertop</td>
</tr>
</tbody>
</table>
<p>1) Bring me the heaviest object. [S]
2) Bring me all clear containers. [M]
3) Bring me the hard plastic object. [M]
4) Bring me the lightest piece of clothing. [S]
5) Bring me the object I can pack my clothes into. [C]
6) It is cold outside. Bring me something that can keep me warm. [C]
7) It is sunny outside. Bring me the container of sunscreen. [C]
8) Bring me the heaviest object. [S]
9) Bring me the heavier glass container. [M]
10) Bring me something that is easy to tear. [C]
11) Bring me the lightest container that is empty but can be filled with water. [M]
12) Bring me the most deformable container with a lid. [M]
13) Bring me all metal containers that can be used to carry water. [M]
14) Bring me the object that can be used in an oven. [C]</p>
<p>TABLE XXII: Scene images, object detections, and task instructions for our real scene planning evaluation (scenes 3 and 4). The object category labels given by OWL-ViT are sometimes inaccurate or ambiguous, in which case we provide more precise labels in square brackets. Note that the planner only has access to the original OWL-ViT labels. Tasks are labeled with S, M, or C for Single Concept, Multi-Concept, or Common Knowledge, respectively.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ We experimented with other choices of score functions, and found that while all performed similarly with respect to test accuracy on PHYSOBJECTS, we found this score function to produce the most interpretable range of likelihoods for different responses, which we hypothesize to be beneficial for downstream planning.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{3}$ We release the model weights for PG-InstructBLIP on our website.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
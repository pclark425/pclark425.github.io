<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1095 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1095</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1095</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-24.html">extraction-schema-24</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <p><strong>Paper ID:</strong> paper-417edb826b9dbf8e142e9358b78da70b0bbc7177</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/417edb826b9dbf8e142e9358b78da70b0bbc7177" target="_blank">Structured agents for physical construction</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Machine Learning</p>
                <p><strong>Paper TL;DR:</strong> A suite of challenging physical construction tasks inspired by how children play with blocks are introduced, such as matching a target configuration, stacking blocks to connect objects together, and creating shelter-like structures over target objects.</p>
                <p><strong>Paper Abstract:</strong> Physical construction---the ability to compose objects, subject to physical dynamics, to serve some function---is fundamental to human intelligence. We introduce a suite of challenging physical construction tasks inspired by how children play with blocks, such as matching a target configuration, stacking blocks to connect objects together, and creating shelter-like structures over target objects. We examine how a range of deep reinforcement learning agents fare on these challenges, and introduce several new approaches which provide superior performance. Our results show that agents which use structured representations (e.g., objects and scene graphs) and structured policies (e.g., object-centric actions) outperform those which use less structured representations, and generalize better beyond their training when asked to reason about larger scenes. Model-based agents which use Monte-Carlo Tree Search also outperform strictly model-free agents in our most challenging construction problems. We conclude that approaches which combine structured representations and reasoning with powerful learning are a key path toward agents that possess rich intuitive physics, scene understanding, and planning.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1095.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1095.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GN-DQN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph-Network Deep Q-Network (discrete relative)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph-structured, object-centric discrete DQN agent that represents scenes as fully-connected graphs and outputs Q-values on edges corresponding to relative placement actions; uses discrete relative object-centric actions and a DQN learning algorithm.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GN-DQN</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Graph-network based deep Q-learning agent (DQN) that takes object-state graphs as input and produces Q-values per edge for discrete relative actions (pick block, choose reference object edge, choose discretized horizontal offset, stickiness). Learning algorithm: model-free Q-learning (DQN) with replay and adaptive epsilon exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Physical construction task suite (Silhouette, Connecting, Covering, Covering Hard)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>A procedurally generated 2D physics-based construction environment (Unity + Box2D) containing movable rectangular blocks, immovable obstacles, and targets/floors; tasks require stacking/attaching blocks to satisfy functional objectives (match silhouette, connect targets to floor, build shelters). Complexity arises from many objects (up to ~40-50 in scenes), layered obstacles, varying target elevations, limited block supplies (Covering Hard), and combinatorially large action spaces; high stochastic variation from random obstacle positions, target placements, and procedural curricula.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Number of objects (up to ~40-50), number of targets (training 1-8, test up to 16), number of obstacle layers (training up to 3, test up to 4), limited block supply (Covering Hard), discretized action space size (e.g., typical discrete relative agent ~2e4 actions shown in Table D.1 depending on task).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>Varies by curriculum: training distribution medium (e.g. avg 4.5 targets in Silhouette), hardest scenes high (e.g. 8 targets, 3-4 obstacle layers, up to 40-50 objects).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Procedural generation parameters (random target positions, random obstacles, random lengths of obstacles), curriculum levels (increasing #targets, obstacle layers, elevations), randomized scene seeds (evaluation on 10,000 scenes per seed).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>High (procedural/randomized generation across many scene instances and curriculum levels).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Task reward (task-specific scalar reward), and derived measures like fraction of targets connected / covered (success rate-style), median reward across seeds, and best/worst seed ranges.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported as top-performing model-free relative agent across tasks; relative-action agents (including GN-DQN) achieve up to 1.7x more reward than best absolute agents averaged across curricula, and up to 2.4x on hardest curricula levels (relative vs absolute comparison). GN-DQN is noted as constructing more economical solutions and achieving best overall seed on Covering Hard (qualitative and seed-level numeric comparisons reported in figures).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Paper reports structured, object-centric GN-DQN scales better with increased scene complexity and variation: GN-DQN retains substantially better performance and generalization when scene complexity or variation increases (more targets, multiple elevation levels, more obstacle layers) compared to less-structured agents. The authors emphasize that structured representations + relative actions provide invariances that help reuse local strategies across locations and scales, mitigating the negative effects of increased complexity/variation.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td>Qualitatively strong: GN-DQN and GN-DQN-MCTS maintain performance on harder, more varied scenes (e.g., in Connecting with multi-level targets GN-DQN drops only slightly while other agents drop near 0); exact per-task medians for GN-DQN seeds shown in figures (examples: Connecting median rewards in some settings ~2.5-2.8 depending on recurrences).</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Curriculum learning (curricula progressively increase #targets, obstacle layers, target elevation); distributed DQN training with replay; adaptive epsilon schedule to address many invalid actions.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>GN-DQN generalizes substantially better than less-structured agents: in Silhouette it can cover nearly twice as many targets as seen during training; in Connecting with targets at multiple levels its performance drops only slightly while other agents' performance falls to near zero; performance declines moderately with increased obstacle layers but remains better than unstructured agents.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Model-free GN-DQN agents trained for ~1e7 learner steps (â‰ˆ2.5e6 actor steps) in reported experiments; uses replay ratio 4; training with curricula over 4e4 learner-step stages. Empirical learning stability depends on number of GN recurrences and graph connectivity.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Graph-structured (object-centric) DQN with discrete relative actions strongly outperforms unstructured/absolute-action agents on complex construction tasks and generalizes better to higher complexity and variation; relative, object-centric actions plus per-edge Q-values align policy structure with problem structure and improve learning and reuse of local solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Structured agents for physical construction', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1095.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1095.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GN-DQN-MCTS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph-Network DQN with Monte-Carlo Tree Search (model-based planning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A GN-DQN agent augmented with Monte-Carlo Tree Search that uses the DQN as a prior during planning; planning can be applied at test time, training time, or both, using either a perfect simulator or learned predictive model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GN-DQN-MCTS</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Graph-network DQN (as above) augmented with MCTS planning; uses the learned Q-network as a prior term in node-value estimates, and performs MCTS expansions with varying budgets; planning can use a perfect environment simulator (used in main experiments) or a learned graph-network transition model.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent (model-based planning augmentation)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Physical construction task suite (Silhouette, Connecting, Covering, Covering Hard)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same procedurally-generated 2D physics environment; planning is particularly valuable in tasks requiring long-term resource allocation and foresight (Covering Hard with limited blocks, Connecting requiring multi-block structures and precise layouts).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>As GN-DQN: number of objects up to ~40-50, targets up to 8 in training and up to 16 in tests, obstacle layers 3-4, limited block supply in Covering Hard; additionally search budget (MCTS budgets varied: 0, 5, 10, 50, 100, up to 1000) is a planning complexity parameter.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>High for hardest curricula and Covering Hard (resource constraints, dense obstacles); MCTS budgets control effective search computation.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Procedural generation + curricula; experiments vary training MCTS budget and test MCTS budget independently to study robustness to planning-time computation and environment variation.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>High (procedural scenes across curriculum levels); evaluation on hard scenes unseen in training.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Median episode reward across seeds, fraction of optimal reward (e.g., Connecting has optimal reward 3), success counts (e.g., number of targets connected), and qualitative structure correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Planning is generally helpful: in Connecting, using train budget 10 and test budget 100 increases median reward from 2.17 to 2.72 on hardest scenes (i.e., from 72.5% to 90.6% of optimal 3); in Covering Hard, planning with train & test budget 10 increases median reward from 3.60 to 4.61. Pure-planning agent with budget 1000 still performs poorly compared to learned policies, indicating large combinatorial search difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Paper shows planning provides the largest gains in tasks with high complexity and resource constraints (e.g., Covering Hard) and helps handle higher variation; however, planning alone (pure MCTS) is insufficient without learned priors due to combinatorial search space. Planning at training time can improve stability and performance for difficult tasks but transferring planning gains into the Q-function is imperfect.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td>With moderate MCTS budgets, GN-DQN-MCTS achieves large gains (e.g., Connecting median reward 2.72 / 3.0 with train=10/test=100 on hardest scenes).</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td>Shown to improve performance on hardest, highly varied Covering Hard scenes (reward improved from 3.60 to 4.61 with train/test budget 10); also GN-DQN-MCTS generalizes well to increased number of targets and multi-level targets in Silhouette and Connecting.</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Curriculum learning; MCTS used optionally during training (train budget) and/or testing (test budget); experiments with perfect simulator and learned model for rollouts; distribution of experience modified when MCTS used during training.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>GN-DQN-MCTS generalizes strongly: GN-DQN-MCTS agents cover nearly twice as many targets in Silhouette as in training; handle multi-level targets in Connecting with small drops; retain better performance under increased obstacle layers relative to less-structured agents. MCTS at test time often boosts performance even if not used during training.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Model-based agents run up to ~4e6 learner steps (~1e6 actor steps) in reported model-based runs; pre-trained learned models + MCTS can yield improved early training performance but imperfections limit long-run gains.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Combining learned structured policies (GN-DQN) with planning (MCTS) yields substantial gains in high-complexity, high-variation construction tasks, especially when resource constraints and long-horizon planning are required; learned priors are essential for tractable planning in large combinatorial spaces, and planning budgets/training-time planning produce trade-offs in performance and stability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Structured agents for physical construction', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1095.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1095.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GN-RS0</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph-Network RS0 (continuous relative, model-free actor-critic)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Graph-network policy trained with RS0 (retrace + stochastic value gradients) producing continuous actions via global graph features; object-centric encoder with continuous relative actions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GN-RS0</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Graph-network policy trained with RS0 (actor-critic variant for continuous actions). Uses object-state graphs as input, pools information to a global attribute which parameterizes a continuous policy (relative or absolute continuous actions).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Physical construction task suite (Silhouette, Connecting, Covering, Covering Hard)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same procedurally-generated 2D physics tasks; continuous-action variant (placement coordinates continuous or relative continuous offsets) with same complexity sources (many objects, layered obstacles, varying elevations, limited resources in Covering Hard).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Same measures (object counts up to 40-50, targets 1-8 training, obstacle layers up to 3, continuous action dimensionality).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>Medium-to-High depending on task and curriculum level; Covering Hard high.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Procedural generation + curricula over targets/obstacles/elevations; environment stochasticity in placements and physics.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>High</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Episode reward (median across seeds), qualitative solution structures.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>GN-RS0 has the best median performance on Covering Hard among agents reported (median-best), though GN-DQN achieved the best overall seed; overall GN-RS0 performs well but generally below discrete GN-DQN on many tasks due to action-decoding differences (global output vs per-edge outputs). Specific numeric medians not enumerated for all tasks in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>GN-RS0 benefits from graph-based representations but is disadvantaged by needing to pool object-centric information into a global vector for continuous action outputs; this affects performance as complexity/variation increases compared to per-edge action GN-DQN.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td>Median best in Covering Hard (qualitatively stronger than many other continuous agents), but still weaker than planning-augmented GN-DQN-MCTS in hardest settings.</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Curriculum learning with dynamic progression for RS0 agents; Gaussian policy + epsilon exploration injections; trained with RS0 off-policy algorithm, retrace length 5.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Generalization results favor structured GN agents; GN-RS0 generalized less robustly than GN-DQN(-MCTS), consistent with architectural differences (global pooling for action output).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>RS0 agents trained with dynamic curricula; RS0 training slower than DQN with larger variance; exact learner-step budgets vary but model-free runs described as up to 1e7 learner steps for model-free agents generally.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Graph-structured continuous actor-critic agents can handle complex construction tasks and sometimes obtain strong median performance (e.g., Covering Hard), but discrete per-edge GN-DQN architectures provided advantages in action representation and yielded better overall seeds and generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Structured agents for physical construction', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1095.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1095.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CNN-RS0</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CNN + RS0 (pixel-based continuous)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pixel-based convolutional encoder combined with RS0 continuous actor-critic learning; learns vector embeddings from images then MLP policy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>CNN-RS0</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Image-observation agent: CNN encoder produces a vector embedding from RGB renders (plus xy channels); RS0 actor-critic learns continuous absolute or relative placements. Learning algorithm: RS0 (stochastic value gradients with retrace).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent (pixel-based)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Physical construction task suite (image observations)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same construction tasks rendered as images; added difficulty of requiring perception (no parsed object lists), in addition to spatial and physical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Same scene complexity; additional perceptual complexity from image encoding and segmentation requirement.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>Medium-to-High (depends on task); generally more difficult than object-state agents due to perceptual burden.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Procedural generation + curricula; image rendering adds variation across seeds and positions.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>High</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Episode reward (median across seeds), qualitative behavior</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>CNN-RS0 performance generally poorer than GN-based agents; parsing pixels into object representations (e.g., CNN-GN-DQN) improves performance toward object-based GN agents. Exact numeric values not provided in main text beyond aggregate comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Pixel-based agents suffer more from increased scene complexity and variation because the encoder must learn segmentation and object-centric structure in addition to control; explicitly parsing images into objects (segmentation) helps close the gap.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td>Performs worse than object-based GN agents on hardest curricula; does not generalize as well.</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Curriculum learning; CNN encoder followed by MLP policy trained with RS0.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>CNN-GN-DQN (segmented-image + GN) outperforms raw CNN-RS0, indicating that object segmentation and graph structure are key for generalizing to more complex/varied scenes.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Similar training budgets described for RS0 agents; learning slower and less sample-efficient relative to structured GN agents.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Raw pixel-based continuous agents struggle relative to object-centric GN agents; introducing object segmentation and GN processing recovers much of the performance, underscoring the importance of structured representations for scaling to high complexity and variation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Structured agents for physical construction', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1095.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1095.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CNN-GN-DQN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Per-object CNN + Graph-Network DQN (segmented-image GN-DQN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Agent that parses segmented images into per-object CNN embeddings, builds a graph, and applies GN-DQN on that object-centric representation; combines visual perception with structured relational reasoning and discrete relative actions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>CNN-GN-DQN</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Segmentation-based visual pipeline: per-object CNNs convert segmented object images into node features, these are processed by a graph-network policy (GN) and a DQN head that outputs per-edge Q-values for discrete relative actions. Learning algorithm: DQN (model-free).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent (vision + GN)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Physical construction task suite (segmented image observations)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same tasks but with segmented image observations (one per object) rather than ground-truth object vectors; requires perception (via per-object CNN) followed by relational reasoning in GN.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Same scene complexity measures; added perception pipeline complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>Medium-to-High; performance approaches object-state GN agents when segmentation is available.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Procedural generation and curricula; segmentation-based inputs reduce variation burden on GN compared to raw pixels.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>High (but structured by segmentation masks)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Episode reward (median), generalization metrics</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>CNN-GN-DQN achieves performance close to object-state GN-DQN and clearly above CNN-RS0; in supplemental comparisons it performs almost as well as object-based GN counterparts.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Parsing pixels into per-object representations reduces the negative impact of complexity and variation; structured GN reasoning on top of per-object visual embeddings generalizes better than end-to-end pixel agents.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td>Performs near object-based GN-DQN on harder/varied scenes when segmentation is provided; outperforms raw CNN agents.</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Curriculum learning; DQN on graph outputs; vision module trained end-to-end (or preprocessed segments provided) depending on experiment.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>CNN-GN-DQN generalizes substantially better than raw CNN agents and approaches GN-DQN generalization, demonstrating that object-centric visual parsing enables structured agents to scale to more complex/varied scenes.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Comparable to GN-DQN when segmentation reduces perceptual burden; training budgets similar to other DQN agents (1e7 learner steps for model-free experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Integrating per-object perception (segmentation + per-object CNN) with graph-network policies yields nearly the benefits of object-state GN agents: strong performance and robustness to increased environment complexity and variation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Structured agents for physical construction', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1095.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1095.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Relative-action agents</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Relative (object-centric) action agents</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Agents whose action parameterization is object-relative: choose a reference object and place a block at an offset relative to that object, thereby providing translational invariance and compositionality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Relative-action agents (discrete and continuous variants)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Action space that expresses placement relative to a reference object (discrete relative: edge (u,v), discretized horizontal offsets, stickiness; continuous relative: choose reference via cursor, output Î”x offset and stickiness). Implemented with GN policies and DQN/RS0 learning.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent (action-space class)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Physical construction task suite (all tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Relative actions reduce the effective complexity of decision-making by making actions invariant to absolute locations; beneficial in large scenes and for compositional construction behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Action-space size comparison: relative discrete agents typical action counts ~2e4 (task-dependent) vs absolute discrete agents up to 9.1e5 (see Table D.1) â€” a direct measure of action-space complexity reduction.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>Relative actions lower effective action-space complexity compared to absolute actions across environments.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Relative agents evaluated across the same procedural curricula; variation measured by scene randomness and curriculum levels.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>High (same as other agents), but relative parameterization provides invariance to spatial variation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Episode reward (median), scaling/generalization (reward ratios vs absolute agents), qualitative solution types.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Relative-action agents consistently outperform absolute-action agents: best relative agents achieve up to 1.7x more reward averaged across curricula and up to 2.4x on hardest curriculum levels; relative agents construct more economical and reusable solutions (e.g., arches, reused local motifs).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Relative actions reduce sensitivity to absolute spatial complexity (location-specific re-learning), improving generalization across variations (different target numbers, elevations, obstacle arrangements). The paper demonstrates relative actions are a key factor enabling agents to scale to larger, more variable scenes.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td>Relative-action GN agents (e.g., GN-DQN) maintain comparatively high performance under high complexity and high variation (see generalization and hardest-level results), outperforming absolute-action counterparts by large margins.</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Used across agents with curriculum learning; discrete GN-DQN paired with discrete relative actions; continuous GN-RS0 uses continuous relative actions.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Relative agents generalize better zero-shot to more targets, multi-level targets, and increased obstacle layers than absolute-action agents; qualitative examples show reuse of local structures at different heights/locations.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Relative action parameterization reduces action-space dimensionality and leads to faster learning and better sample efficiency relative to absolute action agents (empirical improvement in convergence speed reported).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Object-centric relative actions are a core contribution: they yield large improvements in reward, learning speed, stability, and generalization when environments vary in size, layout, or difficulty, by providing translational invariance and compositionality.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Structured agents for physical construction', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1095.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1095.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Absolute-action agents</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Absolute (world-frame) action agents</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Agents that parameterize actions in the absolute coordinate frame (e.g., place block at (x,y) world coordinates); require learning policies specialized to locations and scale poorly with environment size.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Absolute-action agents (continuous and discrete variants)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agents that directly output absolute placement coordinates (continuous or discretized) along with stickiness; implemented for both RS0 (continuous absolute) and discrete absolute DQN variants.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent (action-space class)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Physical construction task suite (all tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same procedurally-generated tasks but with action choices specified in global coordinates, which increases effective policy complexity because location-specific policies must be learned across the scene.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Large absolute action spaces (Table D.1: absolute discrete actions up to ~9.1e5 in some tasks), increasing sample complexity as environment size grows.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>Higher effective complexity than relative actions for same environments.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Procedural scene variation and curriculum; absolute actions are more sensitive to location variation (lack of translational invariance).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>High, but absolute parameterization less robust to that variation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Episode reward (median across seeds), qualitative solution quality</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Absolute-action agents underperform relative-action agents consistently; best relative agents achieved up to 1.7x more reward averaged across curricula and up to 2.4x on hardest curriculum levels relative to absolute agents. Absolute agents often fall into poor local minima, fail to anticipate long-term consequences, and produce qualitatively worse structures.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Absolute actions scale poorly as environment complexity and variation increase because the agent effectively must re-learn policies for different locations; this leads to worse generalization and lower performance on harder, more varied test distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td>Perform substantially worse than relative-action agents on hardest, most varied scenes (quantified as up to 2.4x lower reward on hardest levels).</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Curriculum learning; absolute-action variants trained with RS0 (continuous) or DQN (discrete) where applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Absolute agents generalize poorly to increased number of targets, multi-level targets, and additional obstacle layers; performance drops dramatically on harder scenes relative to relative-action GN agents.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Worse sample efficiency due to vastly larger effective action spaces; training takes longer and converges to lower-performance policies.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Absolute, world-frame action parameterizations are ill-suited for scalable construction tasks; they exacerbate the burden of generalization across spatial variation and increase sample complexity substantially compared to relative object-centric actions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Structured agents for physical construction', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Relational inductive bias for physical construction in humans and machines <em>(Rating: 2)</em></li>
                <li>Graph networks as learnable physics engines for inference and control <em>(Rating: 2)</em></li>
                <li>Reasoning about physical interactions with object-oriented prediction and planning <em>(Rating: 2)</em></li>
                <li>Interaction networks for learning about objects, relations and physics <em>(Rating: 1)</em></li>
                <li>Learning to attach blocks together to stabilize an unstable stack <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1095",
    "paper_id": "paper-417edb826b9dbf8e142e9358b78da70b0bbc7177",
    "extraction_schema_id": "extraction-schema-24",
    "extracted_data": [
        {
            "name_short": "GN-DQN",
            "name_full": "Graph-Network Deep Q-Network (discrete relative)",
            "brief_description": "A graph-structured, object-centric discrete DQN agent that represents scenes as fully-connected graphs and outputs Q-values on edges corresponding to relative placement actions; uses discrete relative object-centric actions and a DQN learning algorithm.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "GN-DQN",
            "agent_description": "Graph-network based deep Q-learning agent (DQN) that takes object-state graphs as input and produces Q-values per edge for discrete relative actions (pick block, choose reference object edge, choose discretized horizontal offset, stickiness). Learning algorithm: model-free Q-learning (DQN) with replay and adaptive epsilon exploration.",
            "agent_type": "simulated agent",
            "environment_name": "Physical construction task suite (Silhouette, Connecting, Covering, Covering Hard)",
            "environment_description": "A procedurally generated 2D physics-based construction environment (Unity + Box2D) containing movable rectangular blocks, immovable obstacles, and targets/floors; tasks require stacking/attaching blocks to satisfy functional objectives (match silhouette, connect targets to floor, build shelters). Complexity arises from many objects (up to ~40-50 in scenes), layered obstacles, varying target elevations, limited block supplies (Covering Hard), and combinatorially large action spaces; high stochastic variation from random obstacle positions, target placements, and procedural curricula.",
            "complexity_measure": "Number of objects (up to ~40-50), number of targets (training 1-8, test up to 16), number of obstacle layers (training up to 3, test up to 4), limited block supply (Covering Hard), discretized action space size (e.g., typical discrete relative agent ~2e4 actions shown in Table D.1 depending on task).",
            "complexity_level": "Varies by curriculum: training distribution medium (e.g. avg 4.5 targets in Silhouette), hardest scenes high (e.g. 8 targets, 3-4 obstacle layers, up to 40-50 objects).",
            "variation_measure": "Procedural generation parameters (random target positions, random obstacles, random lengths of obstacles), curriculum levels (increasing #targets, obstacle layers, elevations), randomized scene seeds (evaluation on 10,000 scenes per seed).",
            "variation_level": "High (procedural/randomized generation across many scene instances and curriculum levels).",
            "performance_metric": "Task reward (task-specific scalar reward), and derived measures like fraction of targets connected / covered (success rate-style), median reward across seeds, and best/worst seed ranges.",
            "performance_value": "Reported as top-performing model-free relative agent across tasks; relative-action agents (including GN-DQN) achieve up to 1.7x more reward than best absolute agents averaged across curricula, and up to 2.4x on hardest curricula levels (relative vs absolute comparison). GN-DQN is noted as constructing more economical solutions and achieving best overall seed on Covering Hard (qualitative and seed-level numeric comparisons reported in figures).",
            "complexity_variation_relationship": "Paper reports structured, object-centric GN-DQN scales better with increased scene complexity and variation: GN-DQN retains substantially better performance and generalization when scene complexity or variation increases (more targets, multiple elevation levels, more obstacle layers) compared to less-structured agents. The authors emphasize that structured representations + relative actions provide invariances that help reuse local strategies across locations and scales, mitigating the negative effects of increased complexity/variation.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": "Qualitatively strong: GN-DQN and GN-DQN-MCTS maintain performance on harder, more varied scenes (e.g., in Connecting with multi-level targets GN-DQN drops only slightly while other agents drop near 0); exact per-task medians for GN-DQN seeds shown in figures (examples: Connecting median rewards in some settings ~2.5-2.8 depending on recurrences).",
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Curriculum learning (curricula progressively increase #targets, obstacle layers, target elevation); distributed DQN training with replay; adaptive epsilon schedule to address many invalid actions.",
            "generalization_tested": true,
            "generalization_results": "GN-DQN generalizes substantially better than less-structured agents: in Silhouette it can cover nearly twice as many targets as seen during training; in Connecting with targets at multiple levels its performance drops only slightly while other agents' performance falls to near zero; performance declines moderately with increased obstacle layers but remains better than unstructured agents.",
            "sample_efficiency": "Model-free GN-DQN agents trained for ~1e7 learner steps (â‰ˆ2.5e6 actor steps) in reported experiments; uses replay ratio 4; training with curricula over 4e4 learner-step stages. Empirical learning stability depends on number of GN recurrences and graph connectivity.",
            "key_findings": "Graph-structured (object-centric) DQN with discrete relative actions strongly outperforms unstructured/absolute-action agents on complex construction tasks and generalizes better to higher complexity and variation; relative, object-centric actions plus per-edge Q-values align policy structure with problem structure and improve learning and reuse of local solutions.",
            "uuid": "e1095.0",
            "source_info": {
                "paper_title": "Structured agents for physical construction",
                "publication_date_yy_mm": "2019-04"
            }
        },
        {
            "name_short": "GN-DQN-MCTS",
            "name_full": "Graph-Network DQN with Monte-Carlo Tree Search (model-based planning)",
            "brief_description": "A GN-DQN agent augmented with Monte-Carlo Tree Search that uses the DQN as a prior during planning; planning can be applied at test time, training time, or both, using either a perfect simulator or learned predictive model.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "GN-DQN-MCTS",
            "agent_description": "Graph-network DQN (as above) augmented with MCTS planning; uses the learned Q-network as a prior term in node-value estimates, and performs MCTS expansions with varying budgets; planning can use a perfect environment simulator (used in main experiments) or a learned graph-network transition model.",
            "agent_type": "simulated agent (model-based planning augmentation)",
            "environment_name": "Physical construction task suite (Silhouette, Connecting, Covering, Covering Hard)",
            "environment_description": "Same procedurally-generated 2D physics environment; planning is particularly valuable in tasks requiring long-term resource allocation and foresight (Covering Hard with limited blocks, Connecting requiring multi-block structures and precise layouts).",
            "complexity_measure": "As GN-DQN: number of objects up to ~40-50, targets up to 8 in training and up to 16 in tests, obstacle layers 3-4, limited block supply in Covering Hard; additionally search budget (MCTS budgets varied: 0, 5, 10, 50, 100, up to 1000) is a planning complexity parameter.",
            "complexity_level": "High for hardest curricula and Covering Hard (resource constraints, dense obstacles); MCTS budgets control effective search computation.",
            "variation_measure": "Procedural generation + curricula; experiments vary training MCTS budget and test MCTS budget independently to study robustness to planning-time computation and environment variation.",
            "variation_level": "High (procedural scenes across curriculum levels); evaluation on hard scenes unseen in training.",
            "performance_metric": "Median episode reward across seeds, fraction of optimal reward (e.g., Connecting has optimal reward 3), success counts (e.g., number of targets connected), and qualitative structure correctness.",
            "performance_value": "Planning is generally helpful: in Connecting, using train budget 10 and test budget 100 increases median reward from 2.17 to 2.72 on hardest scenes (i.e., from 72.5% to 90.6% of optimal 3); in Covering Hard, planning with train & test budget 10 increases median reward from 3.60 to 4.61. Pure-planning agent with budget 1000 still performs poorly compared to learned policies, indicating large combinatorial search difficulty.",
            "complexity_variation_relationship": "Paper shows planning provides the largest gains in tasks with high complexity and resource constraints (e.g., Covering Hard) and helps handle higher variation; however, planning alone (pure MCTS) is insufficient without learned priors due to combinatorial search space. Planning at training time can improve stability and performance for difficult tasks but transferring planning gains into the Q-function is imperfect.",
            "high_complexity_low_variation_performance": "With moderate MCTS budgets, GN-DQN-MCTS achieves large gains (e.g., Connecting median reward 2.72 / 3.0 with train=10/test=100 on hardest scenes).",
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": "Shown to improve performance on hardest, highly varied Covering Hard scenes (reward improved from 3.60 to 4.61 with train/test budget 10); also GN-DQN-MCTS generalizes well to increased number of targets and multi-level targets in Silhouette and Connecting.",
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Curriculum learning; MCTS used optionally during training (train budget) and/or testing (test budget); experiments with perfect simulator and learned model for rollouts; distribution of experience modified when MCTS used during training.",
            "generalization_tested": true,
            "generalization_results": "GN-DQN-MCTS generalizes strongly: GN-DQN-MCTS agents cover nearly twice as many targets in Silhouette as in training; handle multi-level targets in Connecting with small drops; retain better performance under increased obstacle layers relative to less-structured agents. MCTS at test time often boosts performance even if not used during training.",
            "sample_efficiency": "Model-based agents run up to ~4e6 learner steps (~1e6 actor steps) in reported model-based runs; pre-trained learned models + MCTS can yield improved early training performance but imperfections limit long-run gains.",
            "key_findings": "Combining learned structured policies (GN-DQN) with planning (MCTS) yields substantial gains in high-complexity, high-variation construction tasks, especially when resource constraints and long-horizon planning are required; learned priors are essential for tractable planning in large combinatorial spaces, and planning budgets/training-time planning produce trade-offs in performance and stability.",
            "uuid": "e1095.1",
            "source_info": {
                "paper_title": "Structured agents for physical construction",
                "publication_date_yy_mm": "2019-04"
            }
        },
        {
            "name_short": "GN-RS0",
            "name_full": "Graph-Network RS0 (continuous relative, model-free actor-critic)",
            "brief_description": "Graph-network policy trained with RS0 (retrace + stochastic value gradients) producing continuous actions via global graph features; object-centric encoder with continuous relative actions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "GN-RS0",
            "agent_description": "Graph-network policy trained with RS0 (actor-critic variant for continuous actions). Uses object-state graphs as input, pools information to a global attribute which parameterizes a continuous policy (relative or absolute continuous actions).",
            "agent_type": "simulated agent",
            "environment_name": "Physical construction task suite (Silhouette, Connecting, Covering, Covering Hard)",
            "environment_description": "Same procedurally-generated 2D physics tasks; continuous-action variant (placement coordinates continuous or relative continuous offsets) with same complexity sources (many objects, layered obstacles, varying elevations, limited resources in Covering Hard).",
            "complexity_measure": "Same measures (object counts up to 40-50, targets 1-8 training, obstacle layers up to 3, continuous action dimensionality).",
            "complexity_level": "Medium-to-High depending on task and curriculum level; Covering Hard high.",
            "variation_measure": "Procedural generation + curricula over targets/obstacles/elevations; environment stochasticity in placements and physics.",
            "variation_level": "High",
            "performance_metric": "Episode reward (median across seeds), qualitative solution structures.",
            "performance_value": "GN-RS0 has the best median performance on Covering Hard among agents reported (median-best), though GN-DQN achieved the best overall seed; overall GN-RS0 performs well but generally below discrete GN-DQN on many tasks due to action-decoding differences (global output vs per-edge outputs). Specific numeric medians not enumerated for all tasks in main text.",
            "complexity_variation_relationship": "GN-RS0 benefits from graph-based representations but is disadvantaged by needing to pool object-centric information into a global vector for continuous action outputs; this affects performance as complexity/variation increases compared to per-edge action GN-DQN.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": "Median best in Covering Hard (qualitatively stronger than many other continuous agents), but still weaker than planning-augmented GN-DQN-MCTS in hardest settings.",
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Curriculum learning with dynamic progression for RS0 agents; Gaussian policy + epsilon exploration injections; trained with RS0 off-policy algorithm, retrace length 5.",
            "generalization_tested": true,
            "generalization_results": "Generalization results favor structured GN agents; GN-RS0 generalized less robustly than GN-DQN(-MCTS), consistent with architectural differences (global pooling for action output).",
            "sample_efficiency": "RS0 agents trained with dynamic curricula; RS0 training slower than DQN with larger variance; exact learner-step budgets vary but model-free runs described as up to 1e7 learner steps for model-free agents generally.",
            "key_findings": "Graph-structured continuous actor-critic agents can handle complex construction tasks and sometimes obtain strong median performance (e.g., Covering Hard), but discrete per-edge GN-DQN architectures provided advantages in action representation and yielded better overall seeds and generalization.",
            "uuid": "e1095.2",
            "source_info": {
                "paper_title": "Structured agents for physical construction",
                "publication_date_yy_mm": "2019-04"
            }
        },
        {
            "name_short": "CNN-RS0",
            "name_full": "CNN + RS0 (pixel-based continuous)",
            "brief_description": "A pixel-based convolutional encoder combined with RS0 continuous actor-critic learning; learns vector embeddings from images then MLP policy.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "CNN-RS0",
            "agent_description": "Image-observation agent: CNN encoder produces a vector embedding from RGB renders (plus xy channels); RS0 actor-critic learns continuous absolute or relative placements. Learning algorithm: RS0 (stochastic value gradients with retrace).",
            "agent_type": "simulated agent (pixel-based)",
            "environment_name": "Physical construction task suite (image observations)",
            "environment_description": "Same construction tasks rendered as images; added difficulty of requiring perception (no parsed object lists), in addition to spatial and physical reasoning.",
            "complexity_measure": "Same scene complexity; additional perceptual complexity from image encoding and segmentation requirement.",
            "complexity_level": "Medium-to-High (depends on task); generally more difficult than object-state agents due to perceptual burden.",
            "variation_measure": "Procedural generation + curricula; image rendering adds variation across seeds and positions.",
            "variation_level": "High",
            "performance_metric": "Episode reward (median across seeds), qualitative behavior",
            "performance_value": "CNN-RS0 performance generally poorer than GN-based agents; parsing pixels into object representations (e.g., CNN-GN-DQN) improves performance toward object-based GN agents. Exact numeric values not provided in main text beyond aggregate comparisons.",
            "complexity_variation_relationship": "Pixel-based agents suffer more from increased scene complexity and variation because the encoder must learn segmentation and object-centric structure in addition to control; explicitly parsing images into objects (segmentation) helps close the gap.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": "Performs worse than object-based GN agents on hardest curricula; does not generalize as well.",
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Curriculum learning; CNN encoder followed by MLP policy trained with RS0.",
            "generalization_tested": true,
            "generalization_results": "CNN-GN-DQN (segmented-image + GN) outperforms raw CNN-RS0, indicating that object segmentation and graph structure are key for generalizing to more complex/varied scenes.",
            "sample_efficiency": "Similar training budgets described for RS0 agents; learning slower and less sample-efficient relative to structured GN agents.",
            "key_findings": "Raw pixel-based continuous agents struggle relative to object-centric GN agents; introducing object segmentation and GN processing recovers much of the performance, underscoring the importance of structured representations for scaling to high complexity and variation.",
            "uuid": "e1095.3",
            "source_info": {
                "paper_title": "Structured agents for physical construction",
                "publication_date_yy_mm": "2019-04"
            }
        },
        {
            "name_short": "CNN-GN-DQN",
            "name_full": "Per-object CNN + Graph-Network DQN (segmented-image GN-DQN)",
            "brief_description": "Agent that parses segmented images into per-object CNN embeddings, builds a graph, and applies GN-DQN on that object-centric representation; combines visual perception with structured relational reasoning and discrete relative actions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "CNN-GN-DQN",
            "agent_description": "Segmentation-based visual pipeline: per-object CNNs convert segmented object images into node features, these are processed by a graph-network policy (GN) and a DQN head that outputs per-edge Q-values for discrete relative actions. Learning algorithm: DQN (model-free).",
            "agent_type": "simulated agent (vision + GN)",
            "environment_name": "Physical construction task suite (segmented image observations)",
            "environment_description": "Same tasks but with segmented image observations (one per object) rather than ground-truth object vectors; requires perception (via per-object CNN) followed by relational reasoning in GN.",
            "complexity_measure": "Same scene complexity measures; added perception pipeline complexity.",
            "complexity_level": "Medium-to-High; performance approaches object-state GN agents when segmentation is available.",
            "variation_measure": "Procedural generation and curricula; segmentation-based inputs reduce variation burden on GN compared to raw pixels.",
            "variation_level": "High (but structured by segmentation masks)",
            "performance_metric": "Episode reward (median), generalization metrics",
            "performance_value": "CNN-GN-DQN achieves performance close to object-state GN-DQN and clearly above CNN-RS0; in supplemental comparisons it performs almost as well as object-based GN counterparts.",
            "complexity_variation_relationship": "Parsing pixels into per-object representations reduces the negative impact of complexity and variation; structured GN reasoning on top of per-object visual embeddings generalizes better than end-to-end pixel agents.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": "Performs near object-based GN-DQN on harder/varied scenes when segmentation is provided; outperforms raw CNN agents.",
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Curriculum learning; DQN on graph outputs; vision module trained end-to-end (or preprocessed segments provided) depending on experiment.",
            "generalization_tested": true,
            "generalization_results": "CNN-GN-DQN generalizes substantially better than raw CNN agents and approaches GN-DQN generalization, demonstrating that object-centric visual parsing enables structured agents to scale to more complex/varied scenes.",
            "sample_efficiency": "Comparable to GN-DQN when segmentation reduces perceptual burden; training budgets similar to other DQN agents (1e7 learner steps for model-free experiments).",
            "key_findings": "Integrating per-object perception (segmentation + per-object CNN) with graph-network policies yields nearly the benefits of object-state GN agents: strong performance and robustness to increased environment complexity and variation.",
            "uuid": "e1095.4",
            "source_info": {
                "paper_title": "Structured agents for physical construction",
                "publication_date_yy_mm": "2019-04"
            }
        },
        {
            "name_short": "Relative-action agents",
            "name_full": "Relative (object-centric) action agents",
            "brief_description": "Agents whose action parameterization is object-relative: choose a reference object and place a block at an offset relative to that object, thereby providing translational invariance and compositionality.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Relative-action agents (discrete and continuous variants)",
            "agent_description": "Action space that expresses placement relative to a reference object (discrete relative: edge (u,v), discretized horizontal offsets, stickiness; continuous relative: choose reference via cursor, output Î”x offset and stickiness). Implemented with GN policies and DQN/RS0 learning.",
            "agent_type": "simulated agent (action-space class)",
            "environment_name": "Physical construction task suite (all tasks)",
            "environment_description": "Relative actions reduce the effective complexity of decision-making by making actions invariant to absolute locations; beneficial in large scenes and for compositional construction behaviors.",
            "complexity_measure": "Action-space size comparison: relative discrete agents typical action counts ~2e4 (task-dependent) vs absolute discrete agents up to 9.1e5 (see Table D.1) â€” a direct measure of action-space complexity reduction.",
            "complexity_level": "Relative actions lower effective action-space complexity compared to absolute actions across environments.",
            "variation_measure": "Relative agents evaluated across the same procedural curricula; variation measured by scene randomness and curriculum levels.",
            "variation_level": "High (same as other agents), but relative parameterization provides invariance to spatial variation.",
            "performance_metric": "Episode reward (median), scaling/generalization (reward ratios vs absolute agents), qualitative solution types.",
            "performance_value": "Relative-action agents consistently outperform absolute-action agents: best relative agents achieve up to 1.7x more reward averaged across curricula and up to 2.4x on hardest curriculum levels; relative agents construct more economical and reusable solutions (e.g., arches, reused local motifs).",
            "complexity_variation_relationship": "Relative actions reduce sensitivity to absolute spatial complexity (location-specific re-learning), improving generalization across variations (different target numbers, elevations, obstacle arrangements). The paper demonstrates relative actions are a key factor enabling agents to scale to larger, more variable scenes.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": "Relative-action GN agents (e.g., GN-DQN) maintain comparatively high performance under high complexity and high variation (see generalization and hardest-level results), outperforming absolute-action counterparts by large margins.",
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Used across agents with curriculum learning; discrete GN-DQN paired with discrete relative actions; continuous GN-RS0 uses continuous relative actions.",
            "generalization_tested": true,
            "generalization_results": "Relative agents generalize better zero-shot to more targets, multi-level targets, and increased obstacle layers than absolute-action agents; qualitative examples show reuse of local structures at different heights/locations.",
            "sample_efficiency": "Relative action parameterization reduces action-space dimensionality and leads to faster learning and better sample efficiency relative to absolute action agents (empirical improvement in convergence speed reported).",
            "key_findings": "Object-centric relative actions are a core contribution: they yield large improvements in reward, learning speed, stability, and generalization when environments vary in size, layout, or difficulty, by providing translational invariance and compositionality.",
            "uuid": "e1095.5",
            "source_info": {
                "paper_title": "Structured agents for physical construction",
                "publication_date_yy_mm": "2019-04"
            }
        },
        {
            "name_short": "Absolute-action agents",
            "name_full": "Absolute (world-frame) action agents",
            "brief_description": "Agents that parameterize actions in the absolute coordinate frame (e.g., place block at (x,y) world coordinates); require learning policies specialized to locations and scale poorly with environment size.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Absolute-action agents (continuous and discrete variants)",
            "agent_description": "Agents that directly output absolute placement coordinates (continuous or discretized) along with stickiness; implemented for both RS0 (continuous absolute) and discrete absolute DQN variants.",
            "agent_type": "simulated agent (action-space class)",
            "environment_name": "Physical construction task suite (all tasks)",
            "environment_description": "Same procedurally-generated tasks but with action choices specified in global coordinates, which increases effective policy complexity because location-specific policies must be learned across the scene.",
            "complexity_measure": "Large absolute action spaces (Table D.1: absolute discrete actions up to ~9.1e5 in some tasks), increasing sample complexity as environment size grows.",
            "complexity_level": "Higher effective complexity than relative actions for same environments.",
            "variation_measure": "Procedural scene variation and curriculum; absolute actions are more sensitive to location variation (lack of translational invariance).",
            "variation_level": "High, but absolute parameterization less robust to that variation.",
            "performance_metric": "Episode reward (median across seeds), qualitative solution quality",
            "performance_value": "Absolute-action agents underperform relative-action agents consistently; best relative agents achieved up to 1.7x more reward averaged across curricula and up to 2.4x on hardest curriculum levels relative to absolute agents. Absolute agents often fall into poor local minima, fail to anticipate long-term consequences, and produce qualitatively worse structures.",
            "complexity_variation_relationship": "Absolute actions scale poorly as environment complexity and variation increase because the agent effectively must re-learn policies for different locations; this leads to worse generalization and lower performance on harder, more varied test distributions.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": "Perform substantially worse than relative-action agents on hardest, most varied scenes (quantified as up to 2.4x lower reward on hardest levels).",
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Curriculum learning; absolute-action variants trained with RS0 (continuous) or DQN (discrete) where applicable.",
            "generalization_tested": true,
            "generalization_results": "Absolute agents generalize poorly to increased number of targets, multi-level targets, and additional obstacle layers; performance drops dramatically on harder scenes relative to relative-action GN agents.",
            "sample_efficiency": "Worse sample efficiency due to vastly larger effective action spaces; training takes longer and converges to lower-performance policies.",
            "key_findings": "Absolute, world-frame action parameterizations are ill-suited for scalable construction tasks; they exacerbate the burden of generalization across spatial variation and increase sample complexity substantially compared to relative object-centric actions.",
            "uuid": "e1095.6",
            "source_info": {
                "paper_title": "Structured agents for physical construction",
                "publication_date_yy_mm": "2019-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Relational inductive bias for physical construction in humans and machines",
            "rating": 2
        },
        {
            "paper_title": "Graph networks as learnable physics engines for inference and control",
            "rating": 2
        },
        {
            "paper_title": "Reasoning about physical interactions with object-oriented prediction and planning",
            "rating": 2
        },
        {
            "paper_title": "Interaction networks for learning about objects, relations and physics",
            "rating": 1
        },
        {
            "paper_title": "Learning to attach blocks together to stabilize an unstable stack",
            "rating": 1
        }
    ],
    "cost": 0.0215,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Structured agents for physical construction</h1>
<p>Victor Bapst ${ }^{<em> 1}$ Alvaro Sanchez-Gonzalez ${ }^{</em> 1}$ Carl Doersch ${ }^{1}$ Kimberly L. Stachenfeld ${ }^{1}$ Pushmeet Kohli ${ }^{1}$ Peter W. Battaglia ${ }^{1}$ Jessica B. Hamrick ${ }^{1}$</p>
<h4>Abstract</h4>
<p>Physical construction-the ability to compose objects, subject to physical dynamics, to serve some function-is fundamental to human intelligence. We introduce a suite of challenging physical construction tasks inspired by how children play with blocks, such as matching a target configuration, stacking blocks to connect objects together, and creating shelter-like structures over target objects. We examine how a range of deep reinforcement learning agents fare on these challenges, and introduce several new approaches which provide superior performance. Our results show that agents which use structured representations (e.g., objects and scene graphs) and structured policies (e.g., object-centric actions) outperform those which use less structured representations, and generalize better beyond their training when asked to reason about larger scenes. Model-based agents which use Monte-Carlo Tree Search also outperform strictly model-free agents in our most challenging construction problems. We conclude that approaches which combine structured representations and reasoning with powerful learning are a key path toward agents that possess rich intuitive physics, scene understanding, and planning.</p>
<h2>1. Introduction</h2>
<p>Humans are a "construction species"-we build forts out of couch cushions as children, pyramids in our deserts, and space stations that orbit hundreds of kilometers above our heads. What abilities do artificial intelligence (AI) agents need to possess to perform such achievements? This question frames the high-level purpose of this paper: to explore a range of tasks more complex than those typically studied in AI, and to develop approaches for learning to solve them.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Physical construction involves composing multiple elements under physical dynamics and constraints to achieve rich functional objectives. We introduce a suite of simulated physical construction tasks (Fig. 1), similar in spirit to how children play with toy blocks, which involve stacking and attaching together multiple blocks in configurations that satisfy functional objectives. For example, one task requires stacking blocks around obstacles to connect target locations to the ground. Another task requires building shelters to cover up target blocks and keep them dry in the rain. These tasks are representative of real-world construction challenges: they emphasize problem-solving and functionality rather than simply replicating a given target configuration, reflecting the way human construction involves forethought and purpose.</p>
<p>Real-world physical construction assumes many forms and degrees of complexity, but a few basic skills are typically involved: spatial reasoning (e.g. concepts like "empty" vs "occupied"), relational reasoning (e.g. concepts like "next to" or "on top of"), knowledge of physics (e.g., predicting physical interactions among objects), and planning the allocation of resources to different parts of the structure. Our simulated task environment (Fig. 1) is designed to exercise these skills, while still being simple enough to allow careful experimental control and tractable agent training.</p>
<p>While classic AI studied physical reasoning extensively (Chen, 1990; Pfalzgraf, 1997), construction has not been well-explored using modern learning-based approaches. We draw on a number of techniques from modern AI, combining and extending them in novel ways to make them more applicable and effective for construction. Our family of deep reinforcement learning (RL) agents can support: (1) vector, sequence, image, and graph-structured representations of scenes; (2) continuous and discrete actions, in absolute or object-centric coordinates; (3) model-free learning via deep Q-learning (Mnih et al., 2015), or actor-critic methods (Heess et al., 2015; Munos et al., 2016); and (4) planning via Monte-Carlo Tree Search (MCTS) (Coulom, 2006).</p>
<p>We find that graph-structured representations and reasoning, object-centric policies, and model-based planning are crucial for solving our most difficult tasks, outperforming standard approaches which combine unstructured represen-</p>
<p>tations with policies that take absolute actions. Our results demonstrate the value of integrating rich structure and powerful learning approaches as a key path toward complex construction behavior.</p>
<h2>2. Related Work</h2>
<p>Physical reasoning has been of longstanding interest in AI. Early work explored physical concepts with an emphasis on descriptions that generalize across diverse settings (Winston, 1970). Geometric logical reasoning was a major topic in symbolic logic research (Chou, 1987; Arnon, 1988), leading to geometric theorem-provers (Bouma et al., 1995), rulebased geometric constraint solvers for computer-aided design (Aldefeld, 1988; Schreck et al., 2012), and logic-based optimization for open-ended objectives in robotics (Toussaint, 2015). Classic work often focused on rules and structured representations rather than learning because the sample complexity of learning was often prohibitive for contemporary computers.</p>
<p>Modern advances in learning-based approaches have opened new avenues for using vector and convolutional representations for physical reasoning (Wu et al., 2015; 2016; 2017; Mottaghi et al., 2016; Fragkiadaki et al., 2016; Finn et al., 2016; Agrawal et al., 2016; Lerer et al., 2016; Li et al., 2016; Groth et al., 2018; Bhattacharyya et al., 2018; Ebert et al., 2018). A common limitation, however, is that due to their relatively unstructured representations of space and objects, these approaches tend not to scale up to complex scenes, or generalize to scenes with different numbers of objects, etc.</p>
<p>Several recent studies have explored learning construction, including learning to stack blocks by placing them at predicted stable points (Li et al., 2017), learning to attach blocks together to stabilize an unstable stack (Hamrick et al., 2018), learning basic block-stacking by predicting shortest paths between current and goal states via a transition model (Zhang et al., 2018), and learning object representations and coarse-grained physics models for stacking blocks (Janner et al., 2019). Though promising, in these works the physical structures the agents construct are either very simple, or provided explicitly as an input rather than being designed by the agent itself. A key open challenge, which this paper begins to address, is how to learn to design and build complex structures to satisfy rich functional objectives.</p>
<p>A main direction we explore is object-centric representations of the scene and agent's actions (Diuk et al., 2008; Scholz et al., 2014), implemented with graph neural networks (Scarselli et al., 2009; Bronstein et al., 2017; Gilmer et al., 2017; Battaglia et al., 2018). Within the domain of physical reasoning, graph neural networks have been used as forward models for predicting future states and images (Battaglia et al., 2016; Chang et al., 2017; Watters et al.,
2017; van Steenkiste et al., 2018), and can allow efficient learning and rich generalization. These models have also begun to be incorporated into model-free and model-based RL, in domains such as combinatorial optimization, motor control, and game playing (Dai et al., 2017; Kool \&amp; Welling, 2018; Hamrick et al., 2018; Wang et al., 2018; SanchezGonzalez et al., 2018; Zambaldi et al., 2019). There are several novel aspects to our graph network policies beyond these existing works, including the use of multiple actions per edge and graphs that change size during an episode.</p>
<h2>3. Physical Construction Tasks</h2>
<p>Our simulated task environment is a continuous, procedurally-generated 2D world implemented in Unity (Juliani et al., 2018) with the Box2D physics engine (Catto, 2013). Each episode contains unmoveable obstacles, target objects, and floor, plus movable rectangular blocks which can be picked up and placed.</p>
<p>On each step of an episode, the agent chooses an available block (from below the floor), and places it in the scene (above the floor) by specifying its position. In all but one task (Covering Hard-see below), there is an unlimited supply of blocks of each size, so the same block can be picked up and placed multiple times. The agent may also attach objects together by assigning the property of "stickiness" to the block it is placing. Sticky objects form unbreakable, nearly rigid bonds with objects they contact. In all but one task (Connecting) the agent pays a cost to make a block sticky. After the agent places a block, the environment runs physics forward until all blocks come to rest.</p>
<p>An episode terminates when: (1) a movable block makes contact with an obstacle, either because it is placed in an overlapping location, or because they collide under physical dynamics; (2) a maximum number of actions is exceeded; or (3) the task-specific termination criterion is achieved (described below). The episode always yields zero reward when a movable block makes contact with an obstacle.</p>
<p>Silhouette task (Fig. 1a). The agent must place blocks to overlap with target blocks in the scene, while avoiding randomly positioned obstacles. The reward function is: +1 for each placed block which overlaps at least $90 \%$ with a target block of the same size; and -0.5 for each block set as sticky. The task-specific termination criterion is achieved when there is at least $90 \%$ overlap with all targets.</p>
<p>This is similar to the task in Janner et al. (2019), and challenges agents to reason about physical support of complex arrangements of objects and to select, position, and attach sequences of objects accordingly. However, by fully specifying the target configuration, Silhouette does not require the agent to design a structure to satisfy a functional objective, which is an important component of our other tasks.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. Construction task suite. In all tasks, dark blue objects are regular blocks, light blue blocks are "sticky", red objects are obstacles which cannot be touched, and grey circles indicate points where blocks have stuck together. The black line indicates the floor separating the scene above, which is subject to physics, from the blocks below, which can be picked up and placed. (a) Silhouette task. The agent stacks blocks to match the target blocks (depicted as light green blocks). (b) Connecting task. The agent stacks blocks to connect the small blue target objects to the floor. (c) Covering task. The agent stacks blocks to shelter the obstacles from above. (d) Covering Hard task. Similar to Covering, but crucially the agent has a limited supply of movable blocks. Videos of agent behaviors in these tasks are available at https://tinyurl.com/y7wtfen9 and in Supplemental Table G.1.</p>
<p>Connecting task (Fig. 1b). The agent must stack blocks to connect the floor to three different target locations, avoiding randomly positioned obstacles arranged in layers. The reward function is: +1 for each target whose center is touched by at least one block, and 0 (no penalty) for each block set to sticky. The task-specific termination criterion is achieved when all targets are connected to the floor.</p>
<p>By not fully specifying the target configuration, the Connecting task requires the agent to design a structure with a basic function-connecting targets to the floor-rather than simply implementing it as in the Silhouette task. A wider variety of structures could achieve success in Connecting than Silhouette, and the solution space is much larger because the task is tailored so that solutions usually require many more blocks.</p>
<p>Covering task (Fig. 1c). The agent must build a shelter that covers all obstacles from above, without touching them. The reward function is: $+L$, where $L$ is the sum of the lengths of the top surfaces of the obstacles which are sheltered by blocks placed by the agent; and -2 for each block set as sticky. The task-specific termination criterion is achieved when at least $99 \%$ of the summed obstacle surfaces are covered. The layers of obstacles are well-separated vertically so that the agent can build structures between them.</p>
<p>The Covering task requires richer reasoning about function than the previous tasks: the purpose of the final construction is to provide shelter to a separate object in the scene. The task is also demanding because the obstacles may be elevated far from the floor, and the cost of stickiness essentially prohibits its use.</p>
<p>Covering Hard task (Fig. 1d). Similar to Covering, the
agent must build a shelter, but the task is modified to encourage longer term planning: there is a finite supply of movable blocks, the distribution of obstacles is denser, and the cost of stickiness is lower ( -0.5 per sticky block). It thus incorporates key challenges of the Silhouette task (reasoning about which blocks to make sticky), the Connecting task (reasoning about precise block layouts), and the Covering task (reasoning about arch-like structures). The limited number of blocks necessitates foresight in planning (e.g. reserving long blocks to cover long obstacles). The reward function and termination criterion are the same as in Covering.</p>
<h2>4. Agents</h2>
<p>With our suite of construction tasks, we can now tackle the question we posed at the top of the Introduction: what would an agent need to perform complex construction behaviors? We expect agents which have explicit structured representations to perform better, due to their capacity for relational reasoning, compositionality, and combinatorial generalization. We implement seven construction agents which vary in the degree of structure in their observation types, internal representations, learning algorithms, and action specifications, as summarized in Table 1 and Fig. 2.</p>
<h3>4.1. Observation formats</h3>
<p>Each construction task (Sec. 3) provides object state and/or image observations. Both types are important for construction agents to be able to handle: we ultimately want agents that can use symbolic inputs, e.g., the representations in computer-aided design programs, as well as raw sensory inputs, e.g., photographs of a construction site.</p>
<table>
<thead>
<tr>
<th>Agent</th>
<th>Observation</th>
<th>Encoder</th>
<th>Policy</th>
<th>Planning</th>
<th>Learning alg.</th>
<th>Action space</th>
</tr>
</thead>
<tbody>
<tr>
<td>RNN-RS0</td>
<td>Object</td>
<td>RNN</td>
<td>MLP/vector</td>
<td>-</td>
<td>RS0</td>
<td>Continuous</td>
</tr>
<tr>
<td>CNN-RS0</td>
<td>Image</td>
<td>CNN</td>
<td>MLP/vector</td>
<td>-</td>
<td>RS0</td>
<td>Continuous</td>
</tr>
<tr>
<td>GN-RS0</td>
<td>Object</td>
<td>-</td>
<td>GN/graph</td>
<td>-</td>
<td>RS0</td>
<td>Continuous</td>
</tr>
<tr>
<td>GN-DQN</td>
<td>Object</td>
<td>-</td>
<td>GN/graph</td>
<td>-</td>
<td>DQN</td>
<td>Discrete</td>
</tr>
<tr>
<td>GN-DQN-MCTS</td>
<td>Object</td>
<td>-</td>
<td>GN/graph</td>
<td>MCTS</td>
<td>DQN</td>
<td>Discrete</td>
</tr>
<tr>
<td>CNN-GN-DQN</td>
<td>Seg. image</td>
<td>Per-object CNN</td>
<td>GN/graph</td>
<td>-</td>
<td>DQN</td>
<td>Discrete</td>
</tr>
<tr>
<td>CNN-GN-DQN-MCTS</td>
<td>Seg. image</td>
<td>Per-object CNN</td>
<td>GN/graph</td>
<td>MCTS</td>
<td>DQN</td>
<td>Discrete</td>
</tr>
</tbody>
</table>
<p>Table 1. Full agent architectures. Each component is as described in Sec. 4 and also illustrated in Fig. 2. All agents can be trained with either relative or absolute actions.</p>
<p>Object state: These observations contain a set of feature vectors that communicate the objectsâ€™ positions, orientations, sizes, types (e.g., obstacle, movable, sticky, etc.). Contact information between objects is also provided, as well as the order in which objects were placed in the scene (see Supplemental Sec. C).</p>
<p>Image: Observed images are RGB renderings of the scene, with $(x, y)$ coordinates appended as two extra channels.</p>
<p>Segmented images: The RGB scene image is combined with a segmentation mask for each object, thus comprising a set of segmented images (similar to Janner et al., 2019).</p>
<h3>4.2 Encoders</h3>
<p>We use two types of internal representations for computing policies from inputs: fixed-length vectors and directed graphs with attributes.</p>
<p>CNN encoder: The convolutional neural network (CNN) embeds an input image as a vector representation.</p>
<p>RNN encoder: Object state input vectors are processed sequentially with a recurrent neural network (RNN)â€”a gated recurrent unit (GRU) (Cho et al., 2014)â€”in the order they were placed in the scene, and the final hidden state vector is used as the embedding.</p>
<p>Graph encoder: To convert a set of state input vectors into a graph, we create a node for each input object, and add edges either between all nodes or a subset of them (see Supplemental Sec. C.2).</p>
<p>Per-object CNN encoder: To generate a graph-based representation from images, we first split the input image into segments, and generate new images with only single objects. Each of these are passed to a CNN, and the output vectors are used as nodes in a graph, with edges added as above.</p>
<h3>4.3 Policies</h3>
<p>MLP policy: Given a vector representation, we obtain a policy using a multi-layer perceptron (MLP), which outputs actions or Q-values depending on the learning algorithm.</p>
<p>GN policy: Given a graph-based representation from a graph encoder or a per-object CNN, we apply a stack of three graph networks (GN) (Battaglia et al., 2018) arranged in series, where the second net performs some number of recurrent steps, consistent with the â€œencode-process-decodeâ€ architecture described in Battaglia et al. (2018). Unless otherwise noted, we used three recurrent steps.</p>
<h3>4.4 Actions</h3>
<p>In typical RL and control settings that involve placing objects, the agent takes absolute actions in the frame of reference of the observation (e.g. Silver et al., 2016; 2018; Zhang et al., 2018; Ganin et al., 2018; Janner et al., 2019). We implement this approach in our â€œabsolute actionâ€ agents, where, for example, the agent might choose to â€œplace block D at coordinates $(5.3,7.2)$â€. However, learning absolute actions scales poorly as the size of the environment grows, because the agent must effectively re-learn its construction policy at every location.</p>
<p>To support learning compositional behaviors which are more invariant to the location in the scene (e.g. stacking one block on top of another), we develop an object-centric alternative to absolute actions which we term relative actions. With relative actions, the agent takes actions in a reference frame relative to one of the objects in the scene. This is a natural way of expressing actions, and is similar to how humans are thought to choose actions in some behavioral domains (Ballard et al., 1997; Botvinick &amp; Plaut, 2004).</p>
<p>The different types of actions are shown at the bottom of Fig. 2, with details in Supplemental Sec. B.</p>
<p>Continuous absolute actions are 4-tuples $(X, x, y, s)$, where $X$ is a horizontal cursor to choose a block from the available blocks at the bottom of the scene, â€œsnappingâ€ to the closest one, $(x, y)$ determines its placement in the scene and the sign of $s$ indicates stickiness (see Sec. 3).</p>
<p>Continuous relative actions are 5-tuples, $(X, x, y, \Delta x, s)$,</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. Summary of our construction agents' components. Each agent is defined by an observation format, encoder, policy, learning algorithm, and output action space. We evaluated many of the compatible combinations of these components, as indicated by the grey arrows. For "continuous absolute" actions, the agent picks a block from the bottom (solid green circle), and places it in the scene (empty green circle). For "continuous relative" actions, the agent picks a block from the bottom (solid green circle), and places it in the scene (empty green circle) with a relative horizontal offset from the nearest block it snaps to. "Discrete absolute" actions are similar to continuous absolute actions, except with discrete coordinates. For "discrete relative" actions, the agent picks an edge between a block at the bottom (solid green circle) and block in the scene (empty green circle), and a relative horizontal offset.
where $X$ and $s$ are as before, $(x, y)$ is used to choose a reference block (by snapping to the closest one), and $\Delta x$ determines where to place the objects horizontally relatively to the reference object, the vertical positioning being automatically adjusted.</p>
<p>Discrete absolute actions are 4-tuples $(u, i, j, s)$ where $u$ is an index over the available objects, $i, j$ indicate the discrete index at which to place the object in a grid-like 2D discretization of space, and $s$ indicates stickiness.</p>
<p>Absolute actions and continuous relative actions are easily
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3. Structure of the GN-DQN agents. The agent takes in a graph-structured representation of the scene where each object corresponds to a node in the graph, and passes this representation through a GN. The GN produces a vector of Q-values for each edge, corresponding to relative actions for picking a block (the start node of the edge) and placing it on another object (the end node of the edge) at a given offset (the edge attribute). To choose actions, the agent takes an $\arg \max$ across all edges' $Q$-values and then converts the edge and offset into $(x, y)$ positions.
implemented by any agent that outputs a single fixed-length continuous vector, such as that output by an MLP or the global output feature of a GN.</p>
<p>Discrete relative actions are triplets, $(e, i, s)$, where $e:=$ $(u, v)$ is an edge in the input graph between the to-be-placed block $u$ and the selected reference block $v, i$ is an index over finely discretized horizontal offsets to place the chosen block relatively to the reference block's top surface, and $s$ is as before.</p>
<p>Discrete relative actions are straightforward to implement with a graph-structured internal representation: if the nodes represent objects, then the edges can represent pairwise functions over the objects, such as "place block D on top of block B" (see Fig. 3).</p>
<h3>4.5. Learning algorithms</h3>
<p>The internal vector and graph representations are used to produce actions either by an explicit policy or a Q-function.</p>
<p>RS0 learning algorithm: For continuous action outputs, we use an actor-critic learning algorithm that combines retrace with stochastic value gradients (denoted RS0) (Munos et al., 2016; Heess et al., 2015; Riedmiller et al., 2018).</p>
<p>DQN learning algorithm: For discrete action outputs, we use Q-learning implemented as a deep Q network (DQN) from Mnih et al. (2015), with Q-values on the edges, similar to Hamrick et al. (2018). See Sec. 4.4 and Fig. 3.</p>
<p>MCTS: Because the DQN agent outputs discrete actions, it is straightforward to combine it with standard planning techniques like Monte-Carlo Tree Search (Coulom, 2006; Silver et al., 2016) (see Fig. 3). We use the base DQN</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4. Comparison of absolute and relative actions for model-free agents. (a) Comparison of rewards, averaged across all levels of the curricula. (b) The same as in (a), but for the hardest level of each curricula. (c-d) Qualitative comparison between the best-performing absolute and relative seeds at the hardest curriculum levels in Silhouette, Connecting, and Covering.
agent as a prior for MCTS, and use MCTS with various budgets (either only at test time, only during training, or both), thereby modifying the distribution of experience fed to the learner. As a baseline, we also perform MCTS without the model-free policy prior. In all results reported in the main text, we use the environment simulator as our model; we also explored using learned models with mixed success (see Supplemental Sec. E.3).</p>
<h2>5. Experiments and results</h2>
<p>We ran experiments to evaluate the effectiveness of different agent architectures (see Table 1) on our construction tasks; we also tested several heuristic baselines to estimate bounds on our tasks (see Supplemental Sec. A.2). We focused on quantifying the effect of structured actions (Sec. 5.1), the effect of planning both during training and at decision time (Sec. 5.2), zero-shot generalization performance on larger and more complex scenes (Sec. 5.3). In all experiments, we report results for 10 randomly initialized agents (termed "seeds") which were trained until convergence. Each seed is evaluated on 10,000 scenes, and in all figures we report median performance across seeds as well as errorbars indicating worst and best seed performance.</p>
<p>For efficient training, we found it was important to apply a curriculum which progressively increases the complex-
ity of the task across training episodes. In Silhouette, the curriculum increases the number of targets. In Connecting, it increases the elevation of the targets. In the Covering tasks, it increases the elevation of the obstacles. Details are available in Supplemental Sec. A.2. In our analysis, we evaluated each seed on scenes generated either uniformly at random across all difficulty levels, or only at the hardest difficulty level for each task.</p>
<h3>5.1. Relative versus absolute actions</h3>
<p>We find that agents which use relative actions consistently outperform those which use absolute actions. Across tasks, almost every relative action agent converges at a similar or higher median performance level (see Fig. 4a), and the best relative agents achieve up to 1.7 times more reward than the best absolute agents when averaging across all curriculum levels. When considering only the most advanced level, the differences are larger with factors of up to 2.4 (Fig. 4b).</p>
<p>Fig. 4c shows examples of the best absolute agents' constructions. These outcomes are qualitatively worse than the best relative agents' (Fig. 4d). The absolute agents do not anticipate the long term consequences of their actions as well, sometimes failing to make blocks sticky when necessary, or failing to place required objects at the base of a structure, as in Fig. 4c's Silhouette example. They also fall into poor local minima, building stacks of blocks on the</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5. (a-d) Comparison of different training and testing budgets for the model-based GN-DQN-MCTS agents on the hardest curricula levels. The gray dashed line corresponds to a pure-planning agent with a search budget of 1000. (e-h) Representative structures built by GN-DQN-MCTS agents, chosen from a set of 10 random episodes for each task. The Silhouette and Connecting agents use training budgets of 0 and test budgets of 50; the Covering agent uses a training budget of 0 and test budget of 5, and the Covering Hard agent uses a train and test budget of 10. An example of sub-optimal behavior has been chosen for the third row when available. The entire set of random episodes are shown in Supplemental Sec. H.
sides of the scene which fail to reach or cover objects in the center, as in Fig. 4c's Connecting and Covering examples.</p>
<p>By contrast, the best relative agents (which, across all tasks, were GN-DQN) construct more economical solutions (e.g., Fig. 4d, Connecting) and discover richer strategies, such as building arches (Fig. 4d, Covering). The GN-DQN agent's superior performance suggests that structured representations and relative, object-centric actions are powerful tools for construction. Our qualitative results suggest that these tools provide invariance to dimensions such as spatial location, which can be seen in cases where the GN-DQN agent re-uses local block arrangements at different heights and locations, such as the T structures in Fig. 4g.</p>
<p>Most agents achieve similar levels of performance of Covering Hard: GN-RS0 has the best median performance, while GN-DQN has the best overall seed. But inspecting the qualitative results (Fig. 4), even the best relative agent does not give very strong performance. Though Covering Hard involves placing fewer blocks than other tasks because of their limited supply, reasoning about the sequence of blocks to use, which to make sticky, etc. is indeed a challenge, which we will address in the next section with our planning agent.</p>
<p>Interestingly, the GN-RS0 and GN-DQN agents have markedly different performance despite both using the same structured GN policy. There are a number of subtle differences, but notably, the object-centric information contained in the graph of the GN-RS0 agent must be pooled and passed through the global attribute to produce actions, while the GN-DQN agent directly outputs actions via the graph's edges. This may allow its policy to be more analogous to the actual structure of the problem than the GN-RS0 agent.</p>
<p>The CNN-RS0 agent's performance is generally poorer than the GN-based agents', but the observation formats are also different: the CNN agent must learn to encode images, and it does not receive distinct, parsed objects. To better control for this, we train a GN-based agent from pixels, labelled CNN-GN-DQN, described in Sec. 4. The CNN-GN-DQN agent achieves better performance than the CNN-RS0 agent (see Supplemental Fig. C.2). This suggests that parsing images into objects is valuable, and should be investigated further in future work.</p>
<h3>5.2. Model-based versus model-free</h3>
<p>Generally, complex construction should require longerterm planning, rather than simply reactive decision-making. Given a limited set of blocks, for example, it may be crucial to reserve certain blocks for roles they uniquely satisfy in the future. We thus augment our GN-DQN agent with a planning mechanism based on MCTS (see Sec. 4.5) and evaluate its performance in several conditions, varying the search budget at training and testing time independently (a search budget of 0 corresponds to no planning).</p>
<p>Our results (Fig. 5) show that planning is generally helpful, especially in Connecting and Covering Hard. In Connecting, planning with a train budget of 10 and test budget of 100 improves the agent's median reward from 2.17 to 2.72 on the hardest scenes, or from $72.5 \%$ to $90.6 \%$ of the optimal reward of 3. In Covering Hard, planning with a train and test budget of 10 improves the agent's median reward from 3.60 to 4.61 . Qualitatively, the planning agent appears to be close to ceiling (Fig. 5h). Note that a pure-planning agent (Fig. 5a-d, gray dashed line) with a budget of 1000 still performs poorly compared to learned policies, underscoring</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6. Zero-shot generalization performance of various agents. In all cases, asterisks indicate values seen during training. (a) In Silhouette, we varied the number of targets from 8 to 16. (b) In Connecting, we first varied the location of the target locations to be either on the same level or on different levels. (c) In Connecting, we also varied the number of obstacle layers from three to four. (df) Examples of the GN-DQN-MCTS generalizing to new scenes. In each case, the agent has a train budget of 0 and a test budget of 50. (d) Generalization to 16 targets in Silhouette. (e) Generalization to multi-level targets in Connecting. (f) Generalization to 4 layers of obstacles and higher targets in Connecting.
the difficulty of the combinatorially large search space in construction. In Supplemental Sec. E, we discuss of the trade-offs of planning during training, testing, or both.</p>
<h3>5.3. Generalization</h3>
<p>We next ask: how do our agents generalize to conditions beyond those on which they were trained? In Silhouette, our agents only experience 1-8 targets during training, so we test them on 9 and 16 targets. In Connecting, agents always experience targets at the same elevation within one scene during training, so we test them on targets appearing at multiple different levels in the same scene (in one condition) and all at a higher elevation than experienced during training.</p>
<p>We find that the GN-DQN and especially GN-DQN-MCTS agents with relative actions generalize substantially better than others. In Silhouette, the GN-DQN-* agents cover nearly twice as many targets as seen during training, while the other agents' performances plateau or fall off dramatically (Fig. 6a). In Connecting with targets at multiple different levels, the GN-DQN and GN-DQN-MCTS agentsâ€™ performances drops only slightly, while other agentsâ€™ per-
formance drops to near 0 (Fig. 6b). With increased numbers of obstacle layers in Connecting, both agentsâ€™ performances drop moderately but remain much better than the less structured agents (Fig. 6c). Fig. 6d-f show the qualitative generalization behavior of the GN-DQN-MCTS agent. Overall, these generalization results provide evidence that structured agents are more robust to scenarios which are more complex than those in their training distribution. This is likely a consequence of their ability to recognize structural similarity and re-use learned strategies.</p>
<h3>5.4. Iterative relational reasoning</h3>
<p>Recurrent GNs support iterative relational reasoning by propagating information across the scene graph. We vary the number of recurrent steps in our GN-DQN agent to understand how its relational reasoning capacity affects task performance. We find that increasing the number of propagation steps from 1 to 3 to 5 generally improves performance (to a point) across all tasks: in Silhouette, the median rewards were 3.75, 4.04 and 4.06; in Connecting, 2.49, 2.84, and 2.81; in Covering, 3.41, 3.96, and 4.01; and in Covering Hard, 2.62, 3.03, and 3.02, respectively.</p>
<h2>6. Discussion</h2>
<p>We introduced a suite of representative physical construction challenges, and a family of RL agents to solve them. Our results suggest that graph-structured representations, model-based planning under model-free search policies, and object-relative actions are valuable ingredients for achieving strong performance and effective generalization. We believe this work is the first to demonstrate agents that can learn rich construction behaviors in complex settings with large numbers of objects (up to 40-50 in some cases), and can satisfy challenging functional objectives that go beyond simply matching a pre-specified goal configuration.</p>
<p>Given the power of object-centric policies, future work should seek to integrate methods for detecting and segmenting objects from computer vision with learned relational reasoning. Regarding planning, this work only scratches the surface, and future efforts should explore learned models and more sophisticated search strategies, perhaps using policy improvement (Silver et al., 2018) and gradient-based optimization via differentiable world models (Sanchez-Gonzalez et al., 2018). Finally, procedurally generating problem instances that require complex construction solutions is challenging, and adversarial or other learned approaches may be promising future directions.</p>
<p>Our work is only a first step toward agents which can construct complex, functional structures. However we expect approaches that combine rich structure and powerful learning will be key making fast, durable progress.</p>
<h2>7. Acknowledgements</h2>
<p>We would like to thank Yujia Li, Hanjun Dai, Matt Botvinick, Andrea Tacchetti, Tobias Pfaff, CÃ©dric Hauteville, Thomas Kipf, Andrew Bolt, Piotr Trochim, Victoria Langston, Nicole Hurley, Tejas Kulkarni, Vlad Mnih, Catalin Ionescu, Tina Zhu, Thomas Hubert, and Vinicius Zambaldi for helpful discussions, input, and feedback on this work.</p>
<h2>References</h2>
<p>Agrawal, P., Nair, A., Abbeel, P., Malik, J., and Levine, S. Learning to poke by poking: Experiential learning of intuitive physics. In Proceedings of the 30th Conference on Neural Information Processing Systems (NeurIPS 2016), 2016.</p>
<p>Aldefeld, B. Variation of geometries based on a geometricreasoning method. Computer-Aided Design, 20(3):117126, 1988.</p>
<p>Arnon, D. Geometric reasoning with logic and algebra. Artificial Intelligence, 37(1-3):37-60, 1988.</p>
<p>Azizzadenesheli, K., Yang, B., Liu, W., Lipton, E. B. Z. C., and Anandkumar, A. Surprising negative results for generative adversarial tree search. arXiv preprint arXiv:1806.05780, pp. 1-25, 2018.</p>
<p>Ballard, D. H., Hayhoe, M. M., Pook, P. K., and Rao, R. P. Deictic codes for the embodiment of cognition. Behavioral and Brain Sciences, 20(4):723-742, 1997.</p>
<p>Battaglia, P., Pascanu, R., Lai, M., Rezende, D., and Kavukcuoglu, K. Interaction networks for learning about objects, relations and physics. In Proceedings of the 30th Conference on Neural Information Processing Systems (NeurIPS 2016), 2016.</p>
<p>Battaglia, P. W., Hamrick, J. B., Bapst, V., SanchezGonzalez, A., Zambaldi, V., Malinowski, M., Tacchetti, A., Raposo, D., Santoro, A., Faulkner, R., et al. Relational inductive biases, deep learning, and graph networks. arXiv preprint arXiv:1806.01261, pp. 1-38, 2018.</p>
<p>Bhattacharyya, A., Malinowski, M., Schiele, B., and Fritz, M. Long-term image boundary prediction. In Proceedings of the 32nd AAAI Conference on Artificial Intelligence (AAAI-18), 2018.</p>
<p>Botvinick, M. and Plaut, D. C. Doing without schema hierarchies: a recurrent connectionist approach to normal and impaired routine sequential action. Psychological review, 111(2):395, 2004.</p>
<p>Bouma, W., Fudosa, I., Hoffmann, C., Cai, J., and Paige, R. Geometric constraint solver. Computer-Aided Design, 27 (6):487-501, 1995.</p>
<p>Bronstein, M. M., Bruna, J., LeCun, Y., Szlam, A., and Vandergheynst, P. Geometric deep learning: going beyond euclidean data. IEEE Signal Processing Magazine, 34(4): $18-42,2017$.</p>
<p>Catto, E. Box2D. https://box2d.org/, 2013.
Chang, M. B., Ullman, T., Torralba, A., and Tenenbaum, J. B. A compositional object-based approach to learning physical dynamics. In Proceedings of the 5th International Conference on Learning Representations (ICLR 2017), 2017.</p>
<p>Chen, S.-s. Advances in Spatial Reasoning, volume 2. Intellect Books, 1990.</p>
<p>Cho, K., Van MerriÃ«nboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., and Bengio, Y. Learning phrase representations using rnn encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014.</p>
<p>Chou, S.-C. Mechanical geometry theorem proving. Kluwer Academic, 1987.</p>
<p>Coulom, R. Efficient selectivity and backup operators in monte-carlo tree search. In International conference on computers and games, pp. 72-83. Springer, 2006.</p>
<p>Dai, H., Khalil, E., Zhang, Y., Dilkina, B., and Song, L. Learning combinatorial optimization algorithms over graphs. In Advances in Neural Information Processing Systems, pp. 6348-6358, 2017.</p>
<p>Diuk, C., Cohen, A., and Littman, M. L. An object-oriented representation for efficient reinforcement learning. In Proceedings of the 25th international conference on Machine learning, pp. 240-247. ACM, 2008.</p>
<p>Ebert, F., Finn, C., Dasari, S., Xie, A., Lee, A., and Levine, S. Visual foresight: Model-based deep reinforcement learning for vision-based robotic control. arXiv preprint arXiv:1812.00568, 2018.</p>
<p>Finn, C., Goodfellow, I., and Levine, S. Unsupervised learning for physical interaction through video prediction. In Proceedings of the 30th Conference on Neural Information Processing Systems (NeurIPS 2016), 2016.</p>
<p>Fragkiadaki, K., Agrawal, P., Levine, S., and Malik, J. Learning visual predictive models of physics for playing billiards. In Proceedings of the 4th International Conference on Learning Representations (ICLR 2016), 2016.</p>
<p>Ganin, Y., Kulkarni, T., Babuschkin, I., Eslami, S., and Vinyals, O. Synthesizing programs for images using reinforced adversarial learning. arXiv preprint arXiv:1804.01118, 2018.</p>
<p>Gilmer, J., Schoenholz, S. S., Riley, P. F., Vinyals, O., and Dahl, G. E. Neural message passing for quantum chemistry. arXiv preprint arXiv:1704.01212, 2017.</p>
<p>Groth, O., Fuchs, F. B., Posner, I., and Vedaldi, A. Shapestacks: Learning vision-based physical intuition for generalised object stacking. In Proceedings of the European Conference on Computer Vision (ECCV), 2018.</p>
<p>Guo, X., Singh, S., Lee, H., Lewis, R. L., and Wang, X. Deep learning for real-time atari game play using offline monte-carlo tree search planning. In Ghahramani, Z., Welling, M., Cortes, C., Lawrence, N. D., and Weinberger, K. Q. (eds.), Advances in Neural Information Processing Systems 27, pp. 3338-3346. Curran Associates, Inc., 2014.</p>
<p>Hamrick, J. B., Ballard, A. J., Pascanu, R., Vinyals, O., Heess, N., and Battaglia, P. W. Metacontrol for adaptive imagination-based optimization. In Proceedings of the 5th International Conference on Learning Representations (ICLR 2017), 2017.</p>
<p>Hamrick, J. B., Allen, K. R., Bapst, V., Zhu, T., McKee, K. R., Tenenbaum, J. B., and Battaglia, P. W. Relational inductive bias for physical construction in humans and machines. In Proceedings of the 40th Annual Conference of the Cognitive Science Society, 2018.</p>
<p>Heess, N., Wayne, G., Silver, D., Lillicrap, T., Tassa, Y., and Erez, T. Learning continuous control policies by stochastic value gradients. In Proceedings of the 29th Conference on Neural Information Processing Systems (NeurIPS 2015), 2015.</p>
<p>Janner, M., Levine, S., Freeman, W. T., Tenenbaum, J. B., Finn, C., and Wu, J. Reasoning about physical interactions with object-oriented prediction and planning. In International Conference on Learning Representations, 2019.</p>
<p>Juliani, A., Berges, V.-P., Vckay, E., Gao, Y., Henry, H., Mattar, M., and Lange, D. Unity: A general platform for intelligent agents. arXiv preprint arXiv:1809.02627, 2018.</p>
<p>Kocsis, L. and SzepesvÃ¡ri, C. Bandit based monte-carlo planning. In FÃ¼rnkranz, J., Scheffer, T., and Spiliopoulou, M. (eds.), Machine Learning: ECML 2006, pp. 282-293, Berlin, Heidelberg, 2006. Springer Berlin Heidelberg.</p>
<p>Kool, W. and Welling, M. Attention solves your TSP. arXiv preprint arXiv:1803.08475, 2018.</p>
<p>Lerer, A., Gross, S., and Fergus, R. Learning physical intuition of block towers by example. In Proceedings of the 33rd International Conference on Machine Learning (ICML 2016), 2016.</p>
<p>Li, W., Azimi, S., Leonardis, A., and Fritz, M. To fall or not to fall: A visual approach to physical stability prediction. arXiv preprint arXiv:1604.00066, pp. 1-20, 2016.</p>
<p>Li, W., Leonardis, A., and Fritz, M. Visual stability prediction and its application to manipulation. In Proceedings of the 31st AAAI Conference on Artificial Intelligence (AAAI-17), 2017.</p>
<p>Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G., et al. Human-level control through deep reinforcement learning. Nature, 518(7540): 529, 2015.</p>
<p>Mottaghi, R., Bagherinezhad, H., Rastegari, M., and Farhadi, A. Newtonian image understanding: Unfolding the dynamics of objects in static images. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2016), 2016.</p>
<p>Munos, R., Stepleton, T., Harutyunyan, A., and Bellemare, M. Safe and efficient off-policy reinforcement learning. In Advances in Neural Information Processing Systems, pp. 1054-1062, 2016.</p>
<p>Pascanu, R., Li, Y., Vinyals, O., Heess, N., Buesing, L., RacaniÃ¨re, S., Reichert, D., Weber, T., Wierstra, D., and Battaglia, P. Learning model-based planning from scratch. arXiv preprint arXiv:1707.06170, pp. 1-13, 2017.</p>
<p>Pfalzgraf, J. On geometric and topological reasoning in robotics. Annals of Mathematics and Artificial Intelligence, 19:279, 1997.</p>
<p>Riedmiller, M., Hafner, R., Lampe, T., Neunert, M., Degrave, J., van de Wiele, T., Mnih, V., Heess, N., and Springenberg, J. T. Learning by playing solving sparse reward tasks from scratch. In Dy, J. and Krause, A. (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 4344-4353, Stockholmsmssan, Stockholm Sweden, 10-15 Jul 2018. PMLR.</p>
<p>Sanchez-Gonzalez, A., Heess, N., Springenberg, J. T., Merel, J., Riedmiller, M., Hadsell, R., and Battaglia, P. Graph networks as learnable physics engines for inference and control. In Proceedings of the 35th International Conference on Machine Learning (ICML 2018), 2018.</p>
<p>Santoro, A., Raposo, D., Barrett, D. G., Malinowski, M., Pascanu, R., Battaglia, P., and Lillicrap, T. A simple neural network module for relational reasoning. In Advances in neural information processing systems, pp. 4967-4976, 2017.</p>
<p>Scarselli, F., Gori, M., Tsoi, A., Hagenbuchner, M., and Monfardini, G. The graph neural network model. IEEE Transactions on Neural Networks, 20:61-80, 2009.</p>
<p>Scholz, J., Levihn, M., Isbell, C., and Wingate, D. A physicsbased model prior for object-oriented mdps. In International Conference on Machine Learning, pp. 1089-1097, 2014.</p>
<p>Schreck, P., Mathis, P., and Narboux, J. Geometric construction problem solving in computer-aided learning. In Proceedings of the IEEE International Conference on Tools with Artificial Intelligence (ICTAI), volume 24, pp. 1139-1144, 2012.</p>
<p>Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., van den Driessche, G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M., Dieleman, S., Grewe, D., Nham, J., Kalchbrenner, N., Sutskever, I., Lillicrap, T., Leach, M., Kavukcuoglu, K., Graepel, T., and Hassabis, D. Mastering the game of Go with deep neural networks and tree search. Nature, 529:484-489, 2016.</p>
<p>Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., Hubert, T., Baker, L., Lai, M., Bolton, A., Chen, Y., Lillicrap, T., Hui, F., Sifre, L., van den Driessche, G., Graepel, T., and Hassabis, D. Mastering the game of Go without human knowledge. Nature, 550:354-359, 2017.</p>
<p>Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A., Lanctot, M., Sifre, L., Kumaran, D., Graepel, T., et al. A general reinforcement learning algorithm that masters chess, shogi, and go through self-play. Science, 362(6419):1140-1144, 2018.</p>
<p>Sutton, R. S. Dyna, an integrated architecture for learning, planning, and reacting. SIGART Bull., 2(4):160-163, July 1991. ISSN 0163-5719. doi: 10.1145/122344.122377.</p>
<p>Toussaint, M. Logic-geometric programming: an optimization-based approach to combined task and motion planning. In Proceedings of the International Conference on Artificial Intelligence (IJCAI), volume 24, pp. 1930-1936, 2015.
van Steenkiste, S., Chang, M., Greff, K., and Schmidhuber, J. Relational neural expectation maximization: unsupervised discovery of objects and their interactions. In Proceedings of the 6th International Conference on Learning Representations (ICLR 2018), 2018.</p>
<p>Wang, T., Liao, R., Ba, J., and Fidler, S. Nervenet: Learning structured policy with graph neural networks. In Proceedings of the International Conference on Learning Representations (ICLR), 2018.</p>
<p>Watters, N., Tacchetti, A., Weber, T., Pascanu, R., Battaglia, P., and Zoran, D. Visual interaction networks: Learning a physics simulator from video. In Proceedings of the 31st Conference on Neural Information Processing Systems (NeurIPS 2017), 2017.</p>
<p>Winston, P. Learning structural descriptions from examples. AI Technical Reports (1964 - 2004), 1970.</p>
<p>Wu, J., Yildirim, I., Lim, J. J., Freeman, W. T., and Tenenbaum, J. B. Galileo: Perceiving physical object properties by integrating a physics engine with deep learning. In Proceedings of the 29th Conference on Neural Information Processing Systems (NeurIPS 2015), 2015.</p>
<p>Wu, J., Lim, J. J., Zhang, H., Tenenbaum, J. B., and Freeman, W. T. Physics 101: Learning physical object properties from unlabeled videos. In Proceedings of the British Machine Vision Conference (BMVC), 2016.</p>
<p>Wu, J., Lu, E., Kohli, P., Freeman, W. T., and Tenenbaum, J. B. Learning to see physics via visual de-animation. In Proceedings of the 31st Conference on Neural Information Processing Systems (NeurIPS 2017), 2017.</p>
<p>Zambaldi, V., Raposo, D., Santoro, A., Bapst, V., Li, Y., Babuschkin, I., Tuyls, K., Reichert, D., Lillicrap, T., Lockhart, E., Shanahan, M., Langston, V., Pascanu, R., Botvinick, M., Vinyals, O., and Battaglia, P. Deep reinforcement learning with relational inductive biases. In International Conference on Learning Representations, 2019.</p>
<p>Zhang, A., Lerer, A., Sukhbaatar, S., Fergus, R., and Szlam, A. Composable planning with attributes. In Proceedings of the 35th International Conference on Machine Learning (ICML 2018), 2018.</p>
<h1>Supplementary Material: Structured agents for physical construction</h1>
<h2>A. Tasks details</h2>
<h2>A.1. Observation formats</h2>
<p>For each task, the agent could use either an image-based observation, an object-based observation, or a combination of both as a segmentation-masks-based observation.</p>
<p>Object state observations are a list of vectors (one for each block), where each vector of size 15 contains information about the corresponding block position $(x, y)$, orientation $(\cos (\theta), \sin (\theta))$, size (width, height), linear $\left(v_{x}, v_{y}\right)$ and angular $\left(v_{\theta}\right)$ velocities, whether it is sticky or not, and one-hot information about its type (available block, placed block, target or obstacle). The list is ordered by the order under which objects appeared in the scene, but this information is discarded for the graph based agents. Information about which objects are in contact is also provided and is used when constructing the input for the graph based networks (see Sec. C).</p>
<p>Image observations start as $128 \times 128$ RGB renders of the scenes and are re-scaled down to $64 \times 64$ by averaging $2 \times 2$ patches, with the color channels normalized to $[0,1]$. The $x$ and $y$ coordinate is also supplied for each point in the image and is normalized in the $[-1,1]$ interval. The re-scaling procedure helps preserve spatial information at a sub-pixel level as color fading at the boundaries between the objects and the background.</p>
<p>Segmented images observations are a list of images, one for each block. They are obtained using a segmentation of the $128 \times 128$ render that maps each pixel to zero or more blocks that may be present at that pixel. Using this segmentation, we build a $128 \times 128$ binary mask for each block, re-scale it down to $64 \times 64$ by averaging $2 \times 2$ patches, and multiply it with the unsegmented RGB render to obtain per-block renders. We also add the mask as an additional alpha channel to the masked the RGB image, as well as coordinate channels.</p>
<h2>A.2. Full scene generation and reward specifications</h2>
<p>The full rendered scene spans a region a size of $16 \times 16$ (meters, unless otherwise indicated).
At the beginning of the episode, the agent has access to 7 available blocks: three small, three medium and one large block (corresponding to respective widths of $0.7,2.1$ and 3.5 , all with height 0.7 ).</p>
<p>The physics simulation is run for 20 seconds after the agent places each block to make sure that the scene is at an equilibrium position before the score is evaluated, and before the agent can place the next block.</p>
<p>Silhouette: Each scene is comprised of 1 to 8 targets and 0 to 6 obstacles, arranged in up to 6 layers, with a curriculum over the maximum number of targets, maximum number of obstacles, and number of layers (see Fig. H.1). Levels are generated by (1) tessellating the scene into layers of blocks of the same sizes as the available blocks, with a small separation of 0.35 , (2) sequentially (up to the required number of targets) finding the set of target candidates and sampling targets from this set (blocks in the tessellation that are directly on top of the floor or an existing target block) (3) sampling obstacles using a similar procedure. Both obstacles and targets that are further from the floor are sampled with higher probability to favor the generation of harder-to-construct towers and inverted pyramids. The average number of targets is 4.5 on the training distribution, and the number of targets goes up to 8 for the hardest levels. These numbers set an upper bound on the total reward that can be obtained. However, the average reward for an optimal agent is lower than that due to the cost of glue (silhouettes generated using this procedure are not guaranteed to be stable, thus the best possible solution may require glue). We ran a baseline (using the action interface of the Relative GN-DQN agent) consisting of two heuristics for deciding (a) where to place blocks: at target positions, sequentially layer by layer, and from center to the sides and (b) when to use sticky blocks: whenever none of the existing blocks that would touch the new block are sticky and the center of mass of the new block would not be supported by the blocks in the previous layer. This baseline achieves a reward of 3.42 on the training distribution and 5.27 on the hardest levels. For comparison our best GN-DQN-MCTS agent seed on this task achieves 4.15 and 7.18 , respectively.</p>
<p>Connecting: There are at most three vertical layers of obstacles above the floor and a layer of three targets above the highest obstacles. Each layer consists of up to three obstacles, whose lengths are uniformly and independently sampled from the interval $[0.7,2.8]$. The layers of obstacles separated by enough distance for one block can be placed between any two layers of obstacles. The curriculum is comprised of scenes fewer obstacle layers, while the number of targets is unchanged (see Fig. H. 4 for examples). Since glue is unpenalized, the maximum reward available to the agent is exactly 3 . We expect a heuristic based on path finding would achieve the total reward of 3.
Covering: There are at most three vertical layers of obstacles above the floor at any location, and up to 2 obstacles in each layer, with lengths uniformly and independently sampled from the interval $[0.7,2.8]$. As in Connecting, these layers are well separated so that one block can be placed between any two layers of obstacles. The curriculum is comprised of scenes with obstacles only in the first two lower layers (see Fig. H.2). The total available length to cover is 5.25 on the training distribution and 7.88 for the hardest levels. This provides a tight upper-bound on the maximal reward and the agent could be expected to achieve this. We ran a baseline (using the action interface of the Relative GN-DQN agent) consisting of placing objects layer by layer, prioritizing large blocks and using the following heuristics for (a) odd layers (those with obstacles): place as many blocks as possible in gaps between obstacles, (b) even layers: placing objects to cover the obstacles from the previous layer, then to fill the remaining gaps when possible. This baseline achieves a reward of 3.85 on the training distribution and 5.31 on the hardest levels. For comparison our best GN-DQN-MCTS agent seed on this task achieves 4.65 and 7.18 respectively.
Covering Hard: There are at most two vertical layers of obstacles above the floor at any location, and up to 2 obstacles in each layer, with lengths uniformly and independently sampled in $[0.7,3.5]$. The curriculum is comprised of scenes with only one layer of obstacles (see Fig. H. 3 for examples). The layers of obstacles are closer to each other than in they were in Connecting or Covering Hard. The maximum length that can be covered is 4.2 on the training distribution and 6.3 on the hardest levels, but this only gives a weak upper bound on the possible reward because of the cost of glue and limited supply of blocks. Given the additional complexity of this task, involving resource planning (which blocks to save for later), and balancing the use of sticky blocks (pay price) with the use of arches (use more blocks), we did not find a simple heuristic that could provide a relevant baseline. In particular, note that our heuristic for Covering is not relevant here as it assumes an infinite supply of blocks.
Curriculum complexity: Curricula were designed to increase in complexity while preserving instances of scenes from previous points in training to avoid catastrophic forgetting. This allows us to make a distinction, for any task and curriculum level, between Hardest Scenes (scenes types that are introduced for the first time at the present level) and All Scenes (training distribution, including hardest scenes at the current level and lower level scenes). Additional details about the conditions for advancing through the curricula are given in Sec. D for the DQN agents and Sec. F for the RS0 agents.</p>
<h1>B. Implementation details of the action specification.</h1>
<p>Continuous absolute actions are 4-tuples $(X, x, y, s) . X$ is compared to the $x$-coordinates of each of the available blocks and the closest block $c$ is chosen. A new block identical to it is then spawned with its center at location $(x, y)$. The resulting object is sticky if and only if the continuous action $s \in[-1,1]$ is positive.
Discrete absolute actions are 4-tuples $(u, i, j, s) . u \in{1,2, \ldots, 7}$ is an index within the set of available blocks to decide which block will be placed on the scene. $i, j$ are discrete index to place this block on the scene. Specifically the scene was discretized in height $\times$ width using different sizes from $8 \times 64$ to $256 \times 256$, finding the best results for $256 \times 256$ in Silhouette and $8 \times 64$ for the other tasks. $s$ is a discrete variable in ${-1,1}$ indicating whether the placed object should be made sticky or not.
Continuous relative actions are 5-tuples, $(X, x, y, \Delta x, s)$. Here $X, s$ have identical meaning to the absolute case, and $c$ is again the object selected by $X$. The object $r$ whose center is closest to $(x, y)$ is then selected as reference. Then, the $x$-coordinate of the placed block $x_{p}$ is determined by $\Delta x \in[-1,1]$, such that $x_{p}=x_{r}+\Delta x\left(\frac{w_{c}+w_{c}}{2}+\epsilon_{x}\right)$, where $x_{r}$ is the $x$-coordinate of the center of $r, w_{r}$ and $w_{c}$ are the widths of the objects $r$ and $c$, and $\epsilon_{x}$ is a small offset so that the objects are not touching laterally.
If $r$ is a target object centered at $\left(x_{r}, y_{r}\right)$, the $y$-coordinate of the center of $c$ will be placed at $y_{p}=y_{r}+\epsilon_{y}$ so that $c$ is vertically overlapping with $r$ (where $\epsilon_{y}$ is a small offset so that the objects are not perfectly flush). If $r$ is a solid object, $c$ is placed just above $r$, i.e. $y_{p}=y_{r}+\frac{h_{r}+h_{c}}{2}+\epsilon_{y}$, where $h_{c}$ and $h_{r}$ are the heights of the objects $c$ and $r$, respectively. If the agent chooses an invalid edge (where $c$ is not an available block, or where $r$ is not a block in the scene), then the episode is</p>
<p>terminated with a reward of zero. We use $\epsilon_{x}=\epsilon_{y}=0.04$ throughout.
Discrete relative actions are triplets, $(e, i, s)$, where $e:=(c, r)$ is an edge in the structured observation graph between the chosen new block and the selected reference block, $i$ is an index over fine discretization of discrete horizontal offsets to place the chosen block relatively to the reference block, and $s$ is as before. If the blocks $c$ are not an available block or that the block $r$ is not a block already in the scene, then the episode is terminated with a reward of 0.
For the $x$ offsets, we use a uniform grid with $n$ bins in the range $\left[-\frac{w_{r}+w_{c}}{2}\left(1+\frac{1}{n-3}\right), \frac{w_{r}+w_{c}}{2}\left(1+\frac{1}{n-3}\right)\right]$, where $w_{r}$ and $w_{c}$ are as before (this is such that there are exactly $n$ segments in the range $\left[-\frac{w_{r}+w_{c}}{2}, \frac{w_{r}+w_{c}}{2}\right]$ ). We then pick the $i$-th value in this grid as the relative $x$ position. The $y$ coordinate of the placed block is computed as before, but we also experimented with predicting the relative offset $\Delta y$, and varying the number of discrete offsets $n$ (see Sec. D for details).</p>
<h1>C. Architecture details</h1>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure C.1. Overall comparison of all agents tested, including MLP and relation network (Santoro et al., 2017) baselines, averaging across curriculum levels. Bars show median performance across seeds, and errorbars are min and max seeds.</p>
<h2>C.1. MLP-based architectures</h2>
<p>MLP: The pre-processor of the MLP model consists of concatenating the list of blocks as given by the environment (blocks, available blocks, obstacles, targets) padded with zero blocks (up to the total maximum number of objects in each task with a 1 hot indicator of padding), and normalizing it with a LayerNorm layer into a fixed set 100 features. This fixed size vector is then processed by the core MLP consisting of four hidden layers of 256 units with ReLU non-linearity, and an output layer to match the required output size. We found this MLP model to have equal or worse performance to the RNN agent, and thus did not report results on it in the main text; however, Fig. C. 1 includes results for the MLP agent across the tasks.</p>
<p>RNN: The RNN model pre-processor uses a GRU (hidden size of 256) to sequentially process the objects in the the scene (including padding objects up to a maximum size as described in the MLP). The output of the GRU after processing the last object is then used as input for the core MLP (identical in size to the on described in the MLP model). In some generalization settings, where the total number of objects increased drastically, we found better generalization performance by clipping/ignoring some of the objects in the input, than by allowing the network to process a longer sequence of objects than used at training time.</p>
<p>CNN: The CNN model pre-processor passes the $64 \times 64$ input image through a 4-layer convolution network (output channels $=[16,32,32,32])$ followed by a ReLU activation, a linear layer on the flattened outputs into embedding size of 256, and another ReLU activation. Each layer is comprised of a 2 d convolution layer (size=3, stride=1, padding="same") and a max pooling layer (size=3, stride=2, padding="same"). The vector embedding of the image is then processed by and MLP core (identical in size to the on described in the MLP model, except that it uses 3 layers instead of 4).</p>
<p>CNN-RN: We found this CNN-RN model to have equal or worse performance to the vanilla CNN agent, and thus did not report results on it in the main text; however, Fig. C. 1 includes results for the CNN-RN agent across the tasks. We use a higher-resolution convolutional feature map, using residual connections to increase depth and ease training. Each residual block with N channels consists of a N-channel (size=3, stride=1, padding="same") convolution and a max pool (size=3, stride=2, padding="same"). This is followed by a N-channel convolution (size=3, stride=1, padding="same"), a ReLU, and</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure C.2. Comparison of object- and pixel-based agents, averaged across curriculum levels. Bars indicate median performance across seeds, and errorbars are min and max seeds. The CNN-GN-DQN agent augmented with a vision module is able to perform almost as well as its object-based counterparts, and clearly above the CNN-RS0 agent. The same holds for the CNN-GN-DQN agent trained with MCTS, which sometimes even outperforms the object based, model free DQN.
another N-channel convolution (size=3, stride=2, padding="same"), the output of which is added to the max pool output to get the block output. We apply 3 such blocks with $\mathrm{N}=[16,32,8]$. This gives us a vector of length 8 at every spatial location, to which we apply the standard Relation Net architecture [Santoro et al., 2017]: we contatenate each pairs of these vectors, and feed the length-16 vector into a 2-layer MLP with ReLU activations ( 64 and 128 units), before applying an average pool operation over all pair representations. This 128-length vector is a linear layer to produce the final embedding of size 256.</p>
<h1>C.2. GN-based architectures</h1>
<p>Graph pre-processing: We use the list of objects or segmentation masks to construct the graphs that are input to the RS0-GN and DQN-GN agents, only discarding the information about the order of appearance of the object in the scene.</p>
<p>For the RS0 agent, we then construct a sparse graph from this set of nodes by connecting (1) available objects to all other objects in the scene; (2) targets and obstacles to all blocks in the scenes; and (3) blocks that are in contact. The DQN agent takes a fully-connected graph as input but we also experimented with feeding it the sparse representation (see Sec. D. 3 for details).</p>
<p>GN architecture: We use the encode-process-decode architecture described by Battaglia et al. (2018). comprised of an independent graph encoder, a recurrent graph core with separate MLPs as node, edge, and global functions followed by three GRUs, respectively, and finally as a decoder either a graph network (for the RS0 agent) or graph independent (for the DQN agent). In symbols, given a graph observation $o$, we process it as</p>
<p>$$
\begin{aligned}
e &amp; =E(o) \
o_{n} &amp; =G\left(\left[o, o_{n-1}, o_{n-1}^{\prime}\right]\right) \
d &amp; =D\left(o_{n}^{\prime}\right)
\end{aligned}
$$</p>
<p>$$
\begin{aligned}
o_{0} &amp; =e \quad o_{0}^{\prime}=e \
o_{n}^{\prime} &amp; =R\left(o_{n-1}\right) \
\left(1 \leq n \leq n_{\mathrm{rec}}\right)
\end{aligned}
$$</p>
<p>where $E$ and $D$ are independent graph network (see Battaglia et al. (2018)), $G$ is a full graph network, and $R$ is a recurrent independent graph network. We use two hidden layers of 64 units with ReLU non-linearity within all our graph networks.</p>
<p>For this discrete agent, the Q values are finally decoded from $d$ as</p>
<p>$$
q=M\left(\left[x, d_{\text {globals }}\right]<em _objes="{objes" _text="\text">{x \in d</em>\right)
$$}}</p>
<p>similarly to the approach of Dai et al. (2017).
For the RS0 agent we find that having more than 1 recurrent steps in the recurrent graph core did not improve performance so we use a single recurrent step, and disabled the GRU (no longer needed without recurrent steps).</p>
<p>Segmented images pre-processing: In the case of the Segmented images observations, each of the nodes in the graph contains an image, which we process independently using a pre-processor similar to that of the CNN model, but smaller (three layers with $[8,16,8]$ output channels, followed by two activated linear layers with sizes $[64,32]$ ). This produces a graph with 32 embedded features for each node.</p>
<h1>C.3. Comparison of pixel based methods with objects based methods</h1>
<p>We compare pixel based approaches with object based approaches on Fig. C.2, emphasizing that the graphical networks that take segmented images as input fare closer to their object based graphical counterparts than to raw CNNs, making their usage an exciting avenue for future work.</p>
<p>We also implemented use a CNN followed by a relation network (Santoro et al., 2017), but it fared worse than the vanilla CNN in all experiments so we excluded it from the main text (see Sec.C.1 of the Supplemental).</p>
<h2>D. Further study on the GN-DQN agent</h2>
<h2>D.1. General implementation</h2>
<p>We implement a DQN agent with a structured graph input and graph output (roughly similar to Dai et al. (2017)), but where the Q-function is defined on the edges of the graph. This agent takes a fully-connected graph as input. The actions are decoded from the edges (resp. the global features) of the network's output in the case of the discrete relative (resp. absolute) agent. The learner pulls experience from a replay containing graphs, with a fixed replay ratio of 4 . The curriculum over scene difficulty is performed on a fixed, short schedule of $4 \times 10^{4}$ learner steps. The main difference with respect to a vanilla DQN is the way we perform $\epsilon$-exploration, which we explain in more detail below.</p>
<p>We use a distributed setup with up to 128 actors (for the largest MCTS budgets) and 1 learner. Our setup is synchronized to keep the replay ratio constant, i.e. the learner only replays each transition a maximum number of times, and conversely actors may wait for the learner to be done processing transitions. This results in an algorithm which has similar learning dynamics to a non-distributed one.</p>
<h2>D.2. $\epsilon$-exploration schedule</h2>
<p>The majority of actions of the discrete agent are invalid, either because they (1) do not correspond to an edge starting from an available block and reaching to an already placed object; (2) because the resulting configuration would have overlapping objects; or (3) because the resulting scene would be unstable. This has the consequence that doing standard $\epsilon$-exploration strongly reduces the length of an episode (longer episodes are exponentially suppressed), effectively performing more exploration at the beginning of an episode than at its end. To counteract this effect, we use an adaptive $\epsilon$-schedule, where the probability of taking a random action at the $n$-th step of an episode is given by $p_{n}=\frac{\epsilon}{\min (L-n, 1)}$, where $\widetilde{L}$ is an empirical estimate of an episodes typical length, and we use $\epsilon=0.3$ throughout the paper. The final performance is mostly unchanged, but we observe that this makes learning faster and helps with model training (see Sec. E.3).</p>
<h2>D.3. Effect of the number of propagation steps and graph connectivity in the graph network</h2>
<p>The results reported elsewhere in this text for the discrete agent were all obtained with $n_{\text {rec }}=3$ (see Sec. C.2) and a fully-connected input graph, but we experimented with varying $n_{\text {rec }}$ and changing the graph connectivity. In Fig. D. 1 we show that performance improves with the number of recurrences, but that training is also more unstable, as demonstrated by the wider shaded area around the curve. Empirically, $n_{\text {rec }}=3$ provides the better compromise between performance and stability.</p>
<p>Those results were all obtained with a fully-connected graph, with a number of edges therefore equal to the number of objects squared. Many of those edges do not however correspond to valid actions or to directly actionable connections, and we experimented with removing those edges from the graph, using the same sparse graph used by the RS0 agent and described in Sec. C (note that this graph typically has about 4 times fewer edges than the fully-connected one). What we observe is that this reduces the reasoning capacities of the discrete agent and therefore decreases performance. Augmenting the number of recurrences can partially correct this effect: the best seed with a sparse graph and $n_{\text {rec }}=7$ can get to the same level of performance as a seed of the fully-connected graph with $n_{\text {rec }}=3$ ), but this then happens at the detriment of training stability.</p>
<h2>D.4. Relative actions on both the $x$ and $y$ axes</h2>
<p>Our discrete relative agents must choose a block to place, an object to use as a reference, and an offset relative to that reference. Thus far, that offset is only in the $x$-direction, since a small $y$-offset above the reference block is almost always</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure D.1. Top panel: Influence of the $n_{\text {rec }}$ parameter on the discrete agent's performance. Bottom panel: Influence of using a sparse graph or the fully-connected graph. Results are qualitatively similar when considering the agents trained with MCTS. In all plots, solid lines are median performance across seeds and shaded regions extend between min and max seeds.
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure D.2. Influence of predicting both the $x$ and $y$ relative coordinates or not. Observe that best performance reaches comparable values in both cases. In all plots, solid lines are median performance across seeds and shaded regions extend between min and max seeds.
sufficient. However, what happens if we allow the agent to choose $y$ offset as well? We observe that this multiplies the size of the action space by the number of discretization points (in our case, 15), therefore making learning harder. On the other hand, for seeds that manage to start learning, the final performance is equivalent that of the agent which only predicts the relative $x$ position (see Fig. D.2), despite a number of actions much larger than that of a typical discrete agent, as shown in Table D.1.</p>
<h1>D.5. Number of discretization steps</h1>
<p>The architecture of the GN-DQN agent naturally represents discrete quantities (i.e., choosing blocks out of a fix set), but using a discrete $x$-offset loses precision over outputting a continuous value. In order to probe the effect of this approximation, we varied the number of discrete locations that the agent is allowed to choose as the second dimension of the action (Fig. D.3). We observe that a finer discretization of the space allows for slightly better final performance on some problems, but also implies a slower and more unstable learning. Empirically, the 15 steps of discretization used in this paper offers the best compromise. An interesting avenue for further research would be to create an agent that can produce continuous actions attached to a particular edge or vertex of the input graph.
Other parameters: In all the paper, and unless otherwise specified, we fix the learning rate of the discrete agent to $10^{-4}$ (resp. $210^{-4}$ ) for the model-free (resp. model based) agent and use the Adam optimizer. We use a batch size of 16 and a</p>
<p><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure D.3. Influence of the number of discretization steps for the relative horizontal positioning of the discrete agent. In all plots, solid lines are median performance across seeds and shaded regions extend between min and max seeds.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Silhouette</th>
<th style="text-align: center;">Reaching</th>
<th style="text-align: center;">Covering</th>
<th style="text-align: center;">Covering Hard</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">absolute</td>
<td style="text-align: center;">$910^{5}$</td>
<td style="text-align: center;">$710^{3}$</td>
<td style="text-align: center;">$710^{3}$</td>
<td style="text-align: center;">$710^{3}$</td>
</tr>
<tr>
<td style="text-align: left;">relative</td>
<td style="text-align: center;">$210^{4}$</td>
<td style="text-align: center;">$210^{4}$</td>
<td style="text-align: center;">$210^{4}$</td>
<td style="text-align: center;">$310^{3}$</td>
</tr>
<tr>
<td style="text-align: left;">relative $(x, y)$</td>
<td style="text-align: center;">$210^{5}$</td>
<td style="text-align: center;">$310^{5}$</td>
<td style="text-align: center;">$310^{5}$</td>
<td style="text-align: center;">$510^{4}$</td>
</tr>
</tbody>
</table>
<p>Table D.1. Typical number of actions for the variations of the tasks, for the discrete agents. The first line reports the number of actions for the best performing discrete absolute agent, the second line for the main discrete relative agent, and the third line for the agents predicting both $x$ and $y$ relative positions.
replay ratio of 4 . We perform a linear curriculum over the problem difficulty over a short amount of steps ( $4 \times 10^{4}$ learner steps). We run all model free agents for $10^{7}$ learner steps, i.e. approximately $2.5 \times 10^{6}$ actor steps. Model based agent are run for up to $4 \times 10^{6}$ learner steps ( $10^{6}$ actor steps). Every experiment is run with 10 different seeds.</p>
<h1>E. Further study on the GN-DQN-MCTS agent</h1>
<h2>E.1. Details of MCTS</h2>
<p>The efficiency of Monte-Carlo Tree Search (MCTS) (Coulom, 2006) planning in RL has recently been highlighted in (Guo et al., 2014; Silver et al., 2016; 2017; 2018). Here we combine our DQN agent with MCTS, in the spirit of Sutton (1991) and Azizzadenesheli et al. (2018). We define a state $s$ in the tree by the sequence of actions that led to it. In other words, given an episode starting with a configuration $s_{0}$ and a sequence of actions $\left(a_{0}, . ., a_{t}\right)$, we simply define $s_{t}:=\left(a_{0}, . ., a_{t}\right)$ (we do not try to regroup states that would correspond to the same observation if coming from different actions sequences). Each node in the tree has a value estimated as</p>
<p>$$
V(s):=\frac{1}{N(s)}\left(\max <em _="{" _in="\in" _text="\text" r="r" rollouts_="rollouts,">{a} Q(s, a)+\sum</em> N(s, a)
$$} s \in r} \hat{Q}(r, s)\right), \quad N(s, a)=1+\sum_{r \in \text { rollouts, } s, a \in r} 1, \quad N(s)=\sum_{a</p>
<p>(observe that the resulting Monte-Carlo tree has a variable connectivity). In this expression, $\hat{Q}(r, s)$ is the standard MonteCarlo return of a rollout after state $s$. The left term $\max _{a} Q(s, a)$ acts as a prior on the value of a node $s$. It is essential to include this term to obtain learning with MCTS, even if using a large budget (see Fig. E.1). We interpret this as being due to the large number of actions stemming from each node and to the fact that many of these actions are actually invalid.</p>
<p>We then perform MCTS exploration by picking the action $a$ that maximizes $\mathcal{V}(s, a)$, where for common MCTS with UCT exploration (Kocsis \&amp; SzepesvÃ¡ri, 2006) one would have</p>
<p>$$
\mathcal{V}(s, a)=V\left(\left(a_{0}, \ldots, a_{t}, a\right)\right)+c \sqrt{\frac{\ln N(s)}{N\left(\left(a_{0}, \ldots, a_{t}, a\right)\right)}}
$$</p>
<p>Remembering that the action $a$ can be decomposed as $(\alpha, \beta)$, where $\alpha$ represents an edge index and $\beta$ all the remaining</p>
<p><img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure E.1. Comparison of the learning based approaches with non-guided MCTS. Model free denotes the relative DQN agent, model based the relative DQN agent with MCTS at training time (budget of 10), Raw MCTS (10) (resp. Raw MCTS (100), Raw MCTS (1000)) denotes a pure MCTS search (without prior on an action value) with a budget of 10 (resp. 100, 1000). For each of the MCTS result, we show the result when performing the search over the actions in two stages (as described in E) or in the usual way. The non-visible bars correspond to zero reward.
dimensions of the action (relative $x$ placement, use of glue or not, ..), we instead first pick $\alpha$ as the maximizer of</p>
<p>$$
\mathcal{V}^{\prime}(x, \alpha):=\max <em 0="0">{\beta}\left[V\left(\left(a</em>
$$}, \ldots, a_{t},(\alpha, \beta)\right)\right)\right]+c \sqrt{\frac{\ln N(s)}{\sum_{\beta} N\left(\left(a_{0}, \ldots, a_{t},(\alpha, \beta)\right)\right)}</p>
<p>and then $\beta$ as the maximizer of $\beta \rightarrow \mathcal{V}(s,(\alpha, \beta))$. We find this approach to yield slightly better results (see Fig. E.1), and to offer better invariance to changes in the second dimension of the action (e.g. when introducing two dimensional relative placement or changing the number of discretization steps). We use a value of $c=2$ for the UCT constant, and do not find a strong influence of this value on our results.</p>
<p>We then use a transition model to deduce the observation, reward and discount obtained when transitioning from $s$ to $s^{\prime}$. For the results presented in the main part of this work, we focused on using a perfect transition model, obtained from reseeding the environment every time with the initial state of an episode and reapplying the same sequence of actions. While this is impractical for the large MCTS budgets used in some other works, this provides an upper-bound on the performance that can be obtained with a learnt model and allows to separate hyper-parameters analysis. Also, as we will show in the next paragraph, it is possible to obtain significant gains even when performing the MCTS expansion only at test time.</p>
<h1>E.2. Results with the environment simulator</h1>
<p>We incorporate planning in two ways to our relative discrete agent. In the first variation, we only perform MCTS at test time, using an independently trained Q-network to act as a prior in our MCTS expansion (cf. Eq.(1)). We observe that this improves the results on almost all problems but for Covering. In particular, in Reaching, the fraction of the hardest scenes where the agent does not reach all three targets is decreased by a factor of 4 (from $55 \%$ down to $16 \%$ ).</p>
<p>In the second variation, we also perform MCTS at training time: the actor generates trajectories using MCTS expansions using its current Q-function, and the resulting trajectories are then fed to the learner (which does not do any Monte-Carlo sampling). We observe that this second approach yield slightly more stable learning and higher performance on Covering Hard, the task that requires the more reasoning (see the last panel of Fig. 5). On the other hand, on other problems, it yields a similar or even decreased performance.</p>
<p>An interesting point to note is that, when training with a perfect simulator, the transfer into the Q-function is very imperfect, as demonstrated by the low value of the left most point on the darker curve of Fig. 5. As it turns out, the agent is relying on the model to select the best action out of the few candidates selected by the Q-function. This may explain why the performance does not necessarily increase when testing with more budget, as the Q-function does not in this case provide a good prior when doing a deeper exploration of the MCTS tree. This is, in essence, also similar to the hypothesis put forward in (Azizzadenesheli et al., 2018).</p>
<h1>E.3. Results with a learned model</h1>
<p>Finally, we extend the previous model-based results by performing the MCTS expansion with a learnt model rather than a perfect simulator of the environment. Using a learnt object-based model was recently put forward as an efficient method for planning in RL (Pascanu et al., 2017; Hamrick et al., 2017) and for predicting physical dynamics (Sanchez-Gonzalez et al., 2018; Janner et al., 2019). Note, however, that none of these approaches have attempted to use MCTS with a graph network-based model.</p>
<p>The model is an operator taking as an input a graph observation and an action, and outputting a new graph observation alongside a reward and discount for the transition:</p>
<p>$$
M: \mathcal{O} \times \mathcal{A} \rightarrow \mathcal{O} \times \mathbb{R} \times[0,1]
$$</p>
<p>Given a sequence of observations $o$, actions $a$, rewards $r$ and discounts $\gamma$ belonging to a single episode, we train this model with an unrolled loss</p>
<p>$$
L_{n_{\text {unroll }}}\left(\left(o_{t^{\prime}}\right)<em _text="\text" _unroll="{unroll">{t \leq t^{\prime} \leq t+n</em>\right)}}},\left(r_{t^{\prime}}, a_{t^{\prime}}, \gamma_{t^{\prime}<em _text="\text" _unroll="{unroll">{t \leq t^{\prime} \leq t+n</em>\right)}}-1}\right):=\sum_{n=0}^{n_{\text {unroll }}-1} l\left(M^{(n)}\left(o_{t},\left(a_{t^{\prime}<em t_n="t+n">{t \leq t^{\prime}&lt;t+n}\right), r</em>\right)
$$}, \gamma_{t+n</p>
<p>where we defined the predicted observation after $n$ steps</p>
<p>$$
M^{(n)}\left(o_{t},\left(a_{t^{\prime}}\right)<em t="t">{t \leq t^{\prime}&lt;t+n}\right)=M\left(M^{(n-1)}\left(o</em>\right)},\left(a_{t^{\prime}<em t_n-1="t+n-1">{t \leq t^{\prime}&lt;t+n-1}\right), a</em>\right)<em t="t">{0}, \quad M^{(0)}\left(o</em>
$$}\right)=o_{t</p>
<p>and the single step loss</p>
<p>$$
l\left((o, r, \gamma), o^{\prime}, r^{\prime}, \gamma^{\prime}\right):=\left|o-o^{\prime}\right|<em 2="2">{2}+\left|r-r^{\prime}\right|</em>\right)\right)
$$}+D_{\mathrm{KL}}\left((\gamma, 1-\gamma)\right|\left(\gamma^{\prime}, 1-\gamma^{\prime</p>
<p>In practice we varied the number of unrolls $n$ between 1 and 4 . The model training is slower with a larger number of unrolls, but it yields more consistent unrolls when used within the MCTS expansion (ideally, the number of unrolls should probably match the typical depth of a MCTS unroll). The model architecture is similar to the one of the main Q-network described in Sec. C.</p>
<p>Model pre-training: At first, we experiment with using a pretrained, learnt model to then perform Q-learning with MCTS. The setup is therefore as follows:
(1) Train an agent model free, or with a perfect environment simulator.
(2) Train a model on trajectories generated by this agent.
(3) Train a second agent with the model learnt in (2)</p>
<p>We observe in Fig. E. 2 that this allows to obtain an improved performance at the beginning of training, matching the results obtained with a perfect environment simulator. However, on longer timescales, the performance plateaus and does slightly worse than a model free agent. We interpret this as being due to the rigidity of the model on longer timescales, which is not able to generalize enough to the data distribution that would be required to obtain larger rewards.</p>
<p>Model learnt online: Finally, we try to learn a model online. In this case the agent is trained with a model which is learnt at the same time on trajectories generated by the agent. As shown on Fig. E.3, we are able to slightly outperform the model free agent on short timescales in two of the problems (Silhouette and Covering Hard), while the noise introduced by the model is prohibitive again in Covering. On longer timescales, the imperfections of the model make the agent trained with a learnt model converge to the same rewards as the one trained without a model, rather than with a perfect model.</p>
<p>We believe that both in this case and when pre-training the model, understanding how to better train the model so that it generalizes better and yields sharper predictions are important areas of future research, and we see the positive results described here at the beginning of training as a strong motivation to pursue work in this direction.</p>
<p><img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Figure E.2. Performance of agents with pre-trained models at small number of steps. Pretrained (a) uses a model pre-trained on a model free agent (the continuation of the light grey curve), while Pretrained (b) uses a model pre-trained on a model based agent (the continuation of the dark grey curve). Observe how the pre-trained curves match the perfect model curves at short times. All these curve use a same learning rate of $210^{-4}$ for fair comparison on short timescales. In all plots, solid lines are median performance across seeds and shaded regions extend between min and max seeds.
<img alt="img-13.jpeg" src="img-13.jpeg" /></p>
<p>Figure E.3. Performance of agents learning a model of the environment at small number of steps. The model based agent use an MCTS budget of 5 that worked best in this setting. All these curve use a same learning rate of $210^{-4}$ for fair comparison on short timescales. In all plots, solid lines are median performance across seeds and shaded regions extend between min and max seeds.</p>
<h1>F. Model-free continuous agent</h1>
<h2>F.1. RS0</h2>
<p>We use a Retraced Stochastic Value Gradients (RS0, Heess et al. (2015); Munos et al. (2016); Riedmiller et al. (2018)) off-policy agent, with a shared observation pre-processor and independent actor and critic core models (MLP or GN). The critic is conditioned on the actions by concatenating them with the input to the model core (either MLP input features of graph globals). The actor learns a Gaussian distribution over each of the actions by outputting parameters using a linear policy head, conditioned on the last layer of the MLP or output globals of the GN. We use a value of 0.98 for the discount and calculated the retrace loss on sequences of length 5 .</p>
<h2>F.2. Exploration</h2>
<p>While the Gaussian policy noise is often sufficient as a source of exploration, due to the highly multi-modal nature of placing objects, we injected additional fixed $\epsilon$ exploration by sampling a continuous action uniformly over the action range with probability $\epsilon$, and sampling from the Gaussian otherwise. We set $\epsilon=.08$ for tasks with shorter episodes (Silhouette and Covering Hard) and $\epsilon=.03$ otherwise.</p>
<h2>F.3. Dynamic curriculum</h2>
<p>Due to the slower training of RS0 compared to DQN, and the large variance in learning time across the different configurations, we use a dynamic curriculum, only allowing agents to progress through the curriculum once they had achieved a certain performance in the current level.</p>
<p>The criteria for progressing through the curriculum is to obtain at least $50 \%$ of the maximum reward in at least $50 \%$ of the episodes (Silhouette, Covering Hard) or at least $25 \%$ of the maximum reward in at least $25 \%$ of the episodes (Connecting,</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{*}$ Equal contribution ${ }^{1}$ DeepMind, London, UK. Correspondence to: Jessica Hamrick $&lt;$ jhamrick@google.com $&gt;$.</p>
<p>Proceedings of the $36^{\text {th }}$ International Conference on Machine Learning, Long Beach, California, PMLR 97, 2019. Copyright 2019 by the author(s).&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
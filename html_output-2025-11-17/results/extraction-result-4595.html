<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4595 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4595</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4595</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-100.html">extraction-schema-100</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <p><strong>Paper ID:</strong> paper-263908992</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2310.07984v1.pdf" target="_blank">Large Language Models for Scientific Synthesis, Inference and Explanation</a></p>
                <p><strong>Paper Abstract:</strong> Large language models are a form of artificial intelligence systems whose primary knowledge consists of the statistical patterns, semantic relationships, and syntactical structures of language1. Despite their limited forms of"knowledge", these systems are adept at numerous complex tasks including creative writing, storytelling, translation, question-answering, summarization, and computer code generation. However, they have yet to demonstrate advanced applications in natural science. Here we show how large language models can perform scientific synthesis, inference, and explanation. We present a method for using general-purpose large language models to make inferences from scientific datasets of the form usually associated with special-purpose machine learning algorithms. We show that the large language model can augment this"knowledge"by synthesizing from the scientific literature. When a conventional machine learning system is augmented with this synthesized and inferred knowledge it can outperform the current state of the art across a range of benchmark tasks for predicting molecular properties. This approach has the further advantage that the large language model can explain the machine learning system's predictions. We anticipate that our framework will open new avenues for AI to accelerate the pace of scientific discovery.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4595.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4595.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM4SD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large Language Models for Scientific Discovery (LLM4SD)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pipeline that leverages general-purpose and science-pretrained LLMs to (1) synthesize rules from scientific literature, (2) infer rules from labeled datasets, (3) convert rules into measurable features to train interpretable models, and (4) generate textual explanations linking rules to predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLM4SD</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A four-stage pipeline: (a) Knowledge Synthesis from Literature — the LLM is prompted (persona prompting) to extract domain-relevant, measurable rules from its pretraining knowledge; (b) Knowledge Inference from Data — the LLM is given batches of labeled SMILES examples and asked to infer discriminative rules, then summarization removes duplicates; (c) Interpretable Model Training — all rules are transcribed into functions mapping instances (SMILES) to numeric/categorical features and used to train interpretable models (random forest or linear classifier/regressor); (d) Interpretable Explanation Generation — the LLM consumes prediction, vector representation, top rules and importance scores to produce human-readable explanations. The pipeline converts LLM-generated knowledge into symbolic/functional features enabling standard ML training and human validation.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Open-source LLM backbones used in experiments: Galactica-6.7b, Galactica-30b, Falcon-7b, Falcon-40b (paper reports ablation across these models); general description also mentions domain models like BioBERT/SciBERT/Med-PALM as related work.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Structured prompting: persona/roleplay prompts to elicit literature-derived rules; few-shot / batch labeled-instance prompting for data-driven rule inference; summarization/deduplication of generated rules into canonical feature definitions; no fixed external paper-retrieval pipeline — relies on LLM pretraining knowledge for literature synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Rule-level synthesis: aggregate generated rules into a final rule set via LLM summarization; combine literature-synthesized rules with data-inferred rules to form a joint feature set; vectorize instances using these rule-functions and train interpretable models; explanations produced by LLMs integrate rule importance scores.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Molecular property prediction across Physiology (e.g., BBBP, ClinTox, Tox21, SIDER), Biophysics (BACE, HIV), Physical Chemistry (ESOL, FreeSolv, Lipophilicity), and Quantum Mechanics (QM9 tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Rule-based feature sets, trained interpretable models (random forest / linear), and textual explanations for individual predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>AUC-ROC for classification tasks (physiology, biophysics), RMSE for physical chemistry regression, MAE for quantum mechanics regression; statistical validation of rules with Mann-Whitney U test (classification) and linear regression t-test (regression); literature review of synthesized rules.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Across 58 tasks LLM4SD achieved SOTA: Physiology AUC-ROC improved from 74.43% to 76.60%; Biophysics AUC-ROC improved from 81.70% to 83.40%; Quantum mechanics average MAE reduced from 11.2450 to 5.8233 (48.2% improvement); Physical chemistry MAE improved from 1.57 to 1.28 (18.5% improvement). Additionally, 85% of literature-synthesized rules were statistically significant; 91.3% of inferred rules were statistically significant.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>State-of-the-art Graph Neural Networks (AttrMask, GraphCL, MolCLR, 3DInfomax, GraphMVP, MoleBERT) and Random Forest with ECFP4 features.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>LLM4SD outperformed all listed baselines on physiology and biophysics domains, and delivered substantial improvements on quantum mechanics and physical chemistry regression tasks (quantified above).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Combining literature-synthesized rules with data-inferred rules yields better performance than either alone; LLMs can both summarize established literature rules and infer statistically significant, sometimes previously under-documented, 'second-order' features from datasets; interpretable models trained on LLM-derived features can surpass black-box SOTA GNNs while providing explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Relies on LLM pretraining coverage (no explicit external literature retrieval was used); some inferred rules are not found in literature and require expert validation; model-scale and pretraining-domain strongly affect capability (domain-pretrained smaller models can outperform larger general models in some tasks); ethical concerns and potential for misleading/incomplete rules are noted.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Ablation shows architecture- and scale-dependent trends: Falcon family required larger scale (40b > 7b) to approach scientific-task performance; Galactica (science-pretrained) shows that smaller models (6.7b) can rival larger ones (30b) in many domains, though Galactica-30b outperforms 6.7b substantially in Quantum Mechanics (≈14% margin), indicating non-monotonic scaling depending on pretraining domain and task complexity.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4595.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4595.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Knowledge Synthesis (Literature)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Knowledge Synthesis from Scientific Literature (LLM component)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method where LLMs are prompted (persona of an expert) to mine their pretraining knowledge and propose measurable rules/features from the scientific literature relevant to a target prediction task.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Knowledge Synthesis from Literature</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>The LLM is instructed to adopt the persona of an experienced chemist/biologist and to produce 20–30 rules (depending on LLM size) that are structural or property-based and measurable (numerical/categorical). Generated rules are then transcribed into deterministic functions to compute feature values from SMILES strings.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Primarily Galactica (6.7b, 30b) and Falcon models in ablation experiments; general approach applicable to any pretrained LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Persona/roleplay prompting to elicit literature-derived knowledge from the LLM's pretraining; constraints require measurable outputs enabling transcription to functions.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>LLM-generated rules are summarized and deduplicated; rules are validated statistically and via literature review; combined with data-derived rules for final feature set.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Molecular property prediction (BBBP, toxicity, solubility, QM properties, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Measurable rules/features (e.g., TPSA, molecular weight, H-bond counts, specific substructures).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Statistical significance of rules (Mann-Whitney U for classification, t-test for regression) and cross-check against literature prevalence.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>85% of synthesized rules were statistically significant across tasks and largely supported by existing literature (except a few tasks like BACE and Tox21-NR-Ahr where divergence was observed).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Implicit comparison against purely data-driven feature discovery; no direct external system baseline for literature synthesis reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Literature-synthesized rules were generally validated and aligned with known determinants (e.g., BBBP determinants: molecular weight, logP, TPSA, H-bonds).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLMs can reliably summarize domain literature into measurable predictive rules; these synthesized rules are often present in the scientific literature and statistically meaningful for prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Synthesis depends on pretraining coverage; tends to reproduce established knowledge and is less likely to discover novel rules without data; no explicit external retrieval of contemporary papers was implemented.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Effectiveness depends on LLM pretraining domain; domain-specific pretraining (Galactica) improves quality even at smaller scales compared to general LLMs.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4595.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4595.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Knowledge Inference (Data)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Knowledge Inference from Data (LLM component)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A procedure where LLMs are given labeled example batches and asked to analyze patterns to infer empirically discriminative, measurable rules which are then consolidated into features.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Knowledge Inference from Data</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>LLMs receive instructions plus several batches of sampled instances (SMILES) with class labels or property values and are prompted to infer stepwise rules that discriminate classes or predict continuous values. Generated rules across batches are summarized and duplicates removed to yield a final inferred-rule set which is converted into feature functions.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Galactica-6.7b and other LLM backbones evaluated (Falcon-7b/40b, Galactica-30b) in ablation.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Few-shot / batch labeled-instance prompting; pattern recognition and chain-of-thought-style reasoning elicited from LLMs; post-generation summarization to unify rules.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Aggregate inferred rules from multiple batches and use LLM summarization to produce canonical rules; combine with literature-synthesized rules for training features.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Molecular property prediction tasks across the four domains used in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Empirically inferred rules/features (often substructures, counts, combined descriptors) with associated measures.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Statistical significance (Mann-Whitney U / t-test) and literature cross-check: authors report percent significant and percent found in literature.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Average 91.3% of inferred rules were statistically significant; ~74% of inferred rules were documented in literature while ~17.3% were not (indicating potentially novel or dataset-specific rules). For BBBP, 38% of significant rules were not identified in the literature by authors.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against synthesis-only features and against standard feature baselines (ECFP4 + RandomForest) in downstream performance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Combining inferred features with literature-synthesized features improved downstream predictive performance across domains versus either alone; inferred features alone sometimes outperformed synthesis in some domains.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLMs can infer statistically robust rules from labeled datasets, including less-documented 'second-order' features; these can complement literature-driven knowledge to improve predictive models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Some inferred rules are dataset-specific and not supported by literature — require domain expert validation; possibility of LLM overfitting to idiosyncratic dataset patterns if batches are not representative.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Quality of inference depends on model ability (impacted by scale and pretraining): larger general models can bridge knowledge gaps but domain-pretrained models may be more sample-efficient for scientific datasets.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4595.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4595.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Interpretable Explanation Generation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Interpretable Explanation Generation (LLM-based)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLM-driven summarization that combines model predictions, feature vectors, and rule importance scores from interpretable models to produce human-readable explanations for individual predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Interpretable Explanation Generation</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>After training interpretable models on LLM-derived features, the system provides the LLM with (i) the model's prediction, (ii) the instance feature vector, and (iii) the list of important rules with their importance scores (from random forest or linear coefficients). The LLM then synthesizes these inputs into a textual explanation that links rules and their contributions to the final prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Same LLM backbones used throughout (Galactica and Falcon variants); the paper does not claim a single required model for explanation generation.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Structured summarization / prompted conditioning on model internals (feature values and importance scores).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Natural-language synthesis that maps numeric importance to qualitative causal-style explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Explanations for molecular property predictions in drug discovery and chemistry domains.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Textual, instance-level explanations describing which rules/features contributed to a prediction and why.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Qualitative expert validation; no formal automated explanation metric reported beyond example analyses and user-facing web application.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Paper reports that LLM-generated explanations make model decisions transparent and align with known scientific determinants (examples provided, e.g., for BBBP explanations citing TPSA, heteroatoms), but quantitative explanation fidelity scores are not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Not directly compared against other explanation systems; contrasted conceptually with black-box GNNs which lack comparable transparent explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>LLM explanations increase interpretability compared to black-box baselines; direct numerical comparison not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLMs can convert model- and feature-level information into human-interpretable explanations that reflect domain knowledge and allow expert scrutiny.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Explanation fidelity depends on correctness of feature importances and LLM summarization; potential for LLM phrasing to overstate confidence or introduce narrative not strictly supported by numeric evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not explicitly quantified for explanation quality; general observation that better LLM backbones produced more coherent outputs in ablation experiments.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4595.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4595.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Persona Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Persona / Roleplaying Prompting for Knowledge Elicitation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique that asks an LLM to adopt the persona of an experienced domain expert (e.g., ‘experienced chemist’) to improve the quality and domain specificity of extracted rules and hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Persona / Roleplaying Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Prompts instruct the LLM to 'assume you are an experienced chemist/biologist' and request a specified number of measurable rules relevant to a prediction task; used both for literature-synthesis prompts (20/30 rules depending on LLM size) and data-inference prompts (step-by-step rule inference).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Applied with Galactica and Falcon models in this paper; technique is model-agnostic.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Structured prompting (persona framing) to bias output toward expert-like, domain-relevant assertions that are measurable.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Persona outputs are summarized/deduplicated by LLM and transcribed to feature functions.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Used in molecular property prediction tasks; generalizable to other scientific domains.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Expert-style rule lists and stepwise explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Indirect: downstream predictive performance and statistical validation of resulting rules.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Persona prompting contributed to generating literature-consistent rules that were statistically significant and improved model performance when converted to features; concrete ablation of persona framing not isolated.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Implicitly compared to unstructured prompting; no explicit baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Not quantitatively isolated in paper; the method is presented as an effective prompt engineering choice for eliciting domain rules.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Persona framing helps LLMs produce domain-appropriate, measurable rules amenable to feature engineering.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Effectiveness depends on LLM pretraining; risk of authoritative-sounding but incorrect 'expert' assertions if not validated.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>No explicit scaling analysis for persona prompting; quality tied to base LLM capability.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4595.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4595.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Galactica (used)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Galactica (science-pretrained large language model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer LLM pretrained heavily on scientific literature and used in this paper as a backbone to synthesize literature rules and infer data-derived rules (Galactica-6.7b and Galactica-30b were evaluated).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Galactica: A large language model for science</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Galactica (6.7b, 30b)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A science-focused pretrained LLM designed to encode scientific text corpora; used here to generate literature-synthesized rules and data-inferred rules. The paper used Galactica-6.7b for rule validation due to reproducibility and observed good performance; Galactica-30b sometimes outperformed 6.7b on more abstract QM tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Galactica-6.7b and Galactica-30b</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Pretraining-based knowledge recall accessed via structured prompting (persona prompts and direct instructions).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>LLM summarization of generated rules across prompts and batches to produce canonical features.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Scientific literature (domain-pretrained), applied to molecular / chemistry tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Domain rules/features and textual explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Downstream predictive performance in the LLM4SD pipeline and statistical significance of produced rules; ablation contrasts 6.7b vs 30b performance per domain.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Galactica-6.7b produced many statistically significant rules and was chosen for rule validation. Galactica-30b outperformed 6.7b in Quantum Mechanics by ~14% in the ablation study, while in other domains 6.7b rivaled 30b.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to Falcon family LLMs (general-pretrained) in ablation; compared as backbones within LLM4SD versus GNN baselines in overall experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Galactica models consistently outperformed Falcon models of comparable sizes on many scientific tasks, highlighting the value of science-focused pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Domain-specific pretraining (scientific literature) yields strong performance on scientific synthesis and inference tasks; smaller, domain-pretrained models can rival or exceed much larger general models in science applications.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Even domain-pretrained models show scale-task interactions (larger models better on very abstract QM tasks), and performance depends on alignment between pretraining content and target task.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Non-monotonic: Galactica-6.7b often matched Galactica-30b except in highly abstract QM tasks where the 30b model improved performance by ≈14%.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4595.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4595.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Falcon (used)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Falcon (general-purpose LLM family)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>General-purpose transformer LLMs pretrained on large web corpora (including arXiv/Wikipedia) evaluated as backbones (Falcon-7b and Falcon-40b) in the LLM4SD ablation study.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Falcon-40B: an open large language model with state-of-the-art performance</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Falcon (7b, 40b)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Open LLM family trained for broad applications; in this paper Falcon-7b underperformed on scientific tasks relative to Falcon-40b and Galactica variants, indicating that general pretraining requires larger scale to match science-specialized models.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Falcon-7b, Falcon-40b</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Pretraining recall via prompting; used similarly for literature-synthesis and data-inference prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Summarization of generated rules; used in LLM4SD pipeline parallel to Galactica variants.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General web and scientific text (arXiv/Wikipedia included in pretraining), applied to molecular prediction tasks in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Generated rules/features and explanations when used within LLM4SD.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Downstream predictive performance in LLM4SD; ablation contrasts across model sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Falcon-40b outperformed Falcon-7b; Falcon-7b failed some tasks (physiology and quantum mechanics) whereas Falcon-40b had sufficient scale to perform; overall Galactica models often outperformed Falcon models of similar size.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against Galactica family in ablation and against GNN baselines in downstream tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Falcon series required larger scale to approach scientific-task performance; Falcon-40b bridged much of the gap compared to Falcon-7b.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>General-purpose LLMs can be adapted to scientific synthesis tasks but need significant scale; domain-specific pretraining is a major advantage.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Scale sensitivity — smaller general models may fail on domain-specific scientific reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Clear scaling trend within Falcon family: 40b substantially outperforms 7b on the evaluated scientific tasks.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4595.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e4595.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought / Least-to-Most Prompting (cited)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought and Least-to-Most Prompting Techniques</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompting strategies (Chain-of-Thought, Least-to-Most) cited as techniques that elicit stepwise reasoning from LLMs and thereby help them perform complex multi-step inference, cited as relevant background methods for eliciting reasoning in LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain-of-thought prompting elicits reasoning in large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Chain-of-Thought / Least-to-Most Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Techniques that present or prompt LLMs to generate intermediate reasoning steps (chain-of-thought) or decompose complex tasks into easier subproblems (least-to-most) to improve multi-step reasoning outputs; cited in paper as methodological background for eliciting LLM reasoning during rule inference.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>General LLM prompting techniques; not tied to a single model in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Stepwise prompting / decompositional prompts to encourage transparent intermediate reasoning when inferring rules from examples.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Use of intermediate steps to produce robust, explainable rules which can be summarized and converted into features.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General LLM reasoning techniques relevant to scientific knowledge elicitation.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Stepwise rationales and intermediate reasoning chains that support rule generation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not directly evaluated in this paper; cited as enabling techniques in prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as important methods to elicit complex reasoning in LLMs that underpin the plausibility of data-inference prompts used in LLM4SD.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Cited literature discusses potential for spurious or unfaithful chains; not directly analyzed in LLM4SD.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Prior work shows chain-of-thought emerges more reliably in larger LLMs; LLM4SD ablation is consistent with scale-dependent emergent reasoning.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Galactica: A large language model for science <em>(Rating: 2)</em></li>
                <li>Falcon-40B: an open large language model with state-of-the-art performance <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Can ChatGPT be used to generate scientific hypotheses? <em>(Rating: 1)</em></li>
                <li>Science in the age of large language models <em>(Rating: 1)</em></li>
                <li>Large language models encode clinical knowledge <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4595",
    "paper_id": "paper-263908992",
    "extraction_schema_id": "extraction-schema-100",
    "extracted_data": [
        {
            "name_short": "LLM4SD",
            "name_full": "Large Language Models for Scientific Discovery (LLM4SD)",
            "brief_description": "A pipeline that leverages general-purpose and science-pretrained LLMs to (1) synthesize rules from scientific literature, (2) infer rules from labeled datasets, (3) convert rules into measurable features to train interpretable models, and (4) generate textual explanations linking rules to predictions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "LLM4SD",
            "system_description": "A four-stage pipeline: (a) Knowledge Synthesis from Literature — the LLM is prompted (persona prompting) to extract domain-relevant, measurable rules from its pretraining knowledge; (b) Knowledge Inference from Data — the LLM is given batches of labeled SMILES examples and asked to infer discriminative rules, then summarization removes duplicates; (c) Interpretable Model Training — all rules are transcribed into functions mapping instances (SMILES) to numeric/categorical features and used to train interpretable models (random forest or linear classifier/regressor); (d) Interpretable Explanation Generation — the LLM consumes prediction, vector representation, top rules and importance scores to produce human-readable explanations. The pipeline converts LLM-generated knowledge into symbolic/functional features enabling standard ML training and human validation.",
            "llm_model_used": "Open-source LLM backbones used in experiments: Galactica-6.7b, Galactica-30b, Falcon-7b, Falcon-40b (paper reports ablation across these models); general description also mentions domain models like BioBERT/SciBERT/Med-PALM as related work.",
            "extraction_technique": "Structured prompting: persona/roleplay prompts to elicit literature-derived rules; few-shot / batch labeled-instance prompting for data-driven rule inference; summarization/deduplication of generated rules into canonical feature definitions; no fixed external paper-retrieval pipeline — relies on LLM pretraining knowledge for literature synthesis.",
            "synthesis_technique": "Rule-level synthesis: aggregate generated rules into a final rule set via LLM summarization; combine literature-synthesized rules with data-inferred rules to form a joint feature set; vectorize instances using these rule-functions and train interpretable models; explanations produced by LLMs integrate rule importance scores.",
            "number_of_papers": null,
            "domain_or_topic": "Molecular property prediction across Physiology (e.g., BBBP, ClinTox, Tox21, SIDER), Biophysics (BACE, HIV), Physical Chemistry (ESOL, FreeSolv, Lipophilicity), and Quantum Mechanics (QM9 tasks).",
            "output_type": "Rule-based feature sets, trained interpretable models (random forest / linear), and textual explanations for individual predictions.",
            "evaluation_metrics": "AUC-ROC for classification tasks (physiology, biophysics), RMSE for physical chemistry regression, MAE for quantum mechanics regression; statistical validation of rules with Mann-Whitney U test (classification) and linear regression t-test (regression); literature review of synthesized rules.",
            "performance_results": "Across 58 tasks LLM4SD achieved SOTA: Physiology AUC-ROC improved from 74.43% to 76.60%; Biophysics AUC-ROC improved from 81.70% to 83.40%; Quantum mechanics average MAE reduced from 11.2450 to 5.8233 (48.2% improvement); Physical chemistry MAE improved from 1.57 to 1.28 (18.5% improvement). Additionally, 85% of literature-synthesized rules were statistically significant; 91.3% of inferred rules were statistically significant.",
            "comparison_baseline": "State-of-the-art Graph Neural Networks (AttrMask, GraphCL, MolCLR, 3DInfomax, GraphMVP, MoleBERT) and Random Forest with ECFP4 features.",
            "performance_vs_baseline": "LLM4SD outperformed all listed baselines on physiology and biophysics domains, and delivered substantial improvements on quantum mechanics and physical chemistry regression tasks (quantified above).",
            "key_findings": "Combining literature-synthesized rules with data-inferred rules yields better performance than either alone; LLMs can both summarize established literature rules and infer statistically significant, sometimes previously under-documented, 'second-order' features from datasets; interpretable models trained on LLM-derived features can surpass black-box SOTA GNNs while providing explanations.",
            "limitations_challenges": "Relies on LLM pretraining coverage (no explicit external literature retrieval was used); some inferred rules are not found in literature and require expert validation; model-scale and pretraining-domain strongly affect capability (domain-pretrained smaller models can outperform larger general models in some tasks); ethical concerns and potential for misleading/incomplete rules are noted.",
            "scaling_behavior": "Ablation shows architecture- and scale-dependent trends: Falcon family required larger scale (40b &gt; 7b) to approach scientific-task performance; Galactica (science-pretrained) shows that smaller models (6.7b) can rival larger ones (30b) in many domains, though Galactica-30b outperforms 6.7b substantially in Quantum Mechanics (≈14% margin), indicating non-monotonic scaling depending on pretraining domain and task complexity.",
            "uuid": "e4595.0"
        },
        {
            "name_short": "Knowledge Synthesis (Literature)",
            "name_full": "Knowledge Synthesis from Scientific Literature (LLM component)",
            "brief_description": "A method where LLMs are prompted (persona of an expert) to mine their pretraining knowledge and propose measurable rules/features from the scientific literature relevant to a target prediction task.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Knowledge Synthesis from Literature",
            "system_description": "The LLM is instructed to adopt the persona of an experienced chemist/biologist and to produce 20–30 rules (depending on LLM size) that are structural or property-based and measurable (numerical/categorical). Generated rules are then transcribed into deterministic functions to compute feature values from SMILES strings.",
            "llm_model_used": "Primarily Galactica (6.7b, 30b) and Falcon models in ablation experiments; general approach applicable to any pretrained LLM.",
            "extraction_technique": "Persona/roleplay prompting to elicit literature-derived knowledge from the LLM's pretraining; constraints require measurable outputs enabling transcription to functions.",
            "synthesis_technique": "LLM-generated rules are summarized and deduplicated; rules are validated statistically and via literature review; combined with data-derived rules for final feature set.",
            "number_of_papers": null,
            "domain_or_topic": "Molecular property prediction (BBBP, toxicity, solubility, QM properties, etc.).",
            "output_type": "Measurable rules/features (e.g., TPSA, molecular weight, H-bond counts, specific substructures).",
            "evaluation_metrics": "Statistical significance of rules (Mann-Whitney U for classification, t-test for regression) and cross-check against literature prevalence.",
            "performance_results": "85% of synthesized rules were statistically significant across tasks and largely supported by existing literature (except a few tasks like BACE and Tox21-NR-Ahr where divergence was observed).",
            "comparison_baseline": "Implicit comparison against purely data-driven feature discovery; no direct external system baseline for literature synthesis reported.",
            "performance_vs_baseline": "Literature-synthesized rules were generally validated and aligned with known determinants (e.g., BBBP determinants: molecular weight, logP, TPSA, H-bonds).",
            "key_findings": "LLMs can reliably summarize domain literature into measurable predictive rules; these synthesized rules are often present in the scientific literature and statistically meaningful for prediction.",
            "limitations_challenges": "Synthesis depends on pretraining coverage; tends to reproduce established knowledge and is less likely to discover novel rules without data; no explicit external retrieval of contemporary papers was implemented.",
            "scaling_behavior": "Effectiveness depends on LLM pretraining domain; domain-specific pretraining (Galactica) improves quality even at smaller scales compared to general LLMs.",
            "uuid": "e4595.1"
        },
        {
            "name_short": "Knowledge Inference (Data)",
            "name_full": "Knowledge Inference from Data (LLM component)",
            "brief_description": "A procedure where LLMs are given labeled example batches and asked to analyze patterns to infer empirically discriminative, measurable rules which are then consolidated into features.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Knowledge Inference from Data",
            "system_description": "LLMs receive instructions plus several batches of sampled instances (SMILES) with class labels or property values and are prompted to infer stepwise rules that discriminate classes or predict continuous values. Generated rules across batches are summarized and duplicates removed to yield a final inferred-rule set which is converted into feature functions.",
            "llm_model_used": "Galactica-6.7b and other LLM backbones evaluated (Falcon-7b/40b, Galactica-30b) in ablation.",
            "extraction_technique": "Few-shot / batch labeled-instance prompting; pattern recognition and chain-of-thought-style reasoning elicited from LLMs; post-generation summarization to unify rules.",
            "synthesis_technique": "Aggregate inferred rules from multiple batches and use LLM summarization to produce canonical rules; combine with literature-synthesized rules for training features.",
            "number_of_papers": null,
            "domain_or_topic": "Molecular property prediction tasks across the four domains used in experiments.",
            "output_type": "Empirically inferred rules/features (often substructures, counts, combined descriptors) with associated measures.",
            "evaluation_metrics": "Statistical significance (Mann-Whitney U / t-test) and literature cross-check: authors report percent significant and percent found in literature.",
            "performance_results": "Average 91.3% of inferred rules were statistically significant; ~74% of inferred rules were documented in literature while ~17.3% were not (indicating potentially novel or dataset-specific rules). For BBBP, 38% of significant rules were not identified in the literature by authors.",
            "comparison_baseline": "Compared against synthesis-only features and against standard feature baselines (ECFP4 + RandomForest) in downstream performance.",
            "performance_vs_baseline": "Combining inferred features with literature-synthesized features improved downstream predictive performance across domains versus either alone; inferred features alone sometimes outperformed synthesis in some domains.",
            "key_findings": "LLMs can infer statistically robust rules from labeled datasets, including less-documented 'second-order' features; these can complement literature-driven knowledge to improve predictive models.",
            "limitations_challenges": "Some inferred rules are dataset-specific and not supported by literature — require domain expert validation; possibility of LLM overfitting to idiosyncratic dataset patterns if batches are not representative.",
            "scaling_behavior": "Quality of inference depends on model ability (impacted by scale and pretraining): larger general models can bridge knowledge gaps but domain-pretrained models may be more sample-efficient for scientific datasets.",
            "uuid": "e4595.2"
        },
        {
            "name_short": "Interpretable Explanation Generation",
            "name_full": "Interpretable Explanation Generation (LLM-based)",
            "brief_description": "LLM-driven summarization that combines model predictions, feature vectors, and rule importance scores from interpretable models to produce human-readable explanations for individual predictions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Interpretable Explanation Generation",
            "system_description": "After training interpretable models on LLM-derived features, the system provides the LLM with (i) the model's prediction, (ii) the instance feature vector, and (iii) the list of important rules with their importance scores (from random forest or linear coefficients). The LLM then synthesizes these inputs into a textual explanation that links rules and their contributions to the final prediction.",
            "llm_model_used": "Same LLM backbones used throughout (Galactica and Falcon variants); the paper does not claim a single required model for explanation generation.",
            "extraction_technique": "Structured summarization / prompted conditioning on model internals (feature values and importance scores).",
            "synthesis_technique": "Natural-language synthesis that maps numeric importance to qualitative causal-style explanations.",
            "number_of_papers": null,
            "domain_or_topic": "Explanations for molecular property predictions in drug discovery and chemistry domains.",
            "output_type": "Textual, instance-level explanations describing which rules/features contributed to a prediction and why.",
            "evaluation_metrics": "Qualitative expert validation; no formal automated explanation metric reported beyond example analyses and user-facing web application.",
            "performance_results": "Paper reports that LLM-generated explanations make model decisions transparent and align with known scientific determinants (examples provided, e.g., for BBBP explanations citing TPSA, heteroatoms), but quantitative explanation fidelity scores are not provided.",
            "comparison_baseline": "Not directly compared against other explanation systems; contrasted conceptually with black-box GNNs which lack comparable transparent explanations.",
            "performance_vs_baseline": "LLM explanations increase interpretability compared to black-box baselines; direct numerical comparison not provided.",
            "key_findings": "LLMs can convert model- and feature-level information into human-interpretable explanations that reflect domain knowledge and allow expert scrutiny.",
            "limitations_challenges": "Explanation fidelity depends on correctness of feature importances and LLM summarization; potential for LLM phrasing to overstate confidence or introduce narrative not strictly supported by numeric evidence.",
            "scaling_behavior": "Not explicitly quantified for explanation quality; general observation that better LLM backbones produced more coherent outputs in ablation experiments.",
            "uuid": "e4595.3"
        },
        {
            "name_short": "Persona Prompting",
            "name_full": "Persona / Roleplaying Prompting for Knowledge Elicitation",
            "brief_description": "A prompting technique that asks an LLM to adopt the persona of an experienced domain expert (e.g., ‘experienced chemist’) to improve the quality and domain specificity of extracted rules and hypotheses.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Persona / Roleplaying Prompting",
            "system_description": "Prompts instruct the LLM to 'assume you are an experienced chemist/biologist' and request a specified number of measurable rules relevant to a prediction task; used both for literature-synthesis prompts (20/30 rules depending on LLM size) and data-inference prompts (step-by-step rule inference).",
            "llm_model_used": "Applied with Galactica and Falcon models in this paper; technique is model-agnostic.",
            "extraction_technique": "Structured prompting (persona framing) to bias output toward expert-like, domain-relevant assertions that are measurable.",
            "synthesis_technique": "Persona outputs are summarized/deduplicated by LLM and transcribed to feature functions.",
            "number_of_papers": null,
            "domain_or_topic": "Used in molecular property prediction tasks; generalizable to other scientific domains.",
            "output_type": "Expert-style rule lists and stepwise explanations.",
            "evaluation_metrics": "Indirect: downstream predictive performance and statistical validation of resulting rules.",
            "performance_results": "Persona prompting contributed to generating literature-consistent rules that were statistically significant and improved model performance when converted to features; concrete ablation of persona framing not isolated.",
            "comparison_baseline": "Implicitly compared to unstructured prompting; no explicit baseline.",
            "performance_vs_baseline": "Not quantitatively isolated in paper; the method is presented as an effective prompt engineering choice for eliciting domain rules.",
            "key_findings": "Persona framing helps LLMs produce domain-appropriate, measurable rules amenable to feature engineering.",
            "limitations_challenges": "Effectiveness depends on LLM pretraining; risk of authoritative-sounding but incorrect 'expert' assertions if not validated.",
            "scaling_behavior": "No explicit scaling analysis for persona prompting; quality tied to base LLM capability.",
            "uuid": "e4595.4"
        },
        {
            "name_short": "Galactica (used)",
            "name_full": "Galactica (science-pretrained large language model)",
            "brief_description": "A transformer LLM pretrained heavily on scientific literature and used in this paper as a backbone to synthesize literature rules and infer data-derived rules (Galactica-6.7b and Galactica-30b were evaluated).",
            "citation_title": "Galactica: A large language model for science",
            "mention_or_use": "use",
            "system_name": "Galactica (6.7b, 30b)",
            "system_description": "A science-focused pretrained LLM designed to encode scientific text corpora; used here to generate literature-synthesized rules and data-inferred rules. The paper used Galactica-6.7b for rule validation due to reproducibility and observed good performance; Galactica-30b sometimes outperformed 6.7b on more abstract QM tasks.",
            "llm_model_used": "Galactica-6.7b and Galactica-30b",
            "extraction_technique": "Pretraining-based knowledge recall accessed via structured prompting (persona prompts and direct instructions).",
            "synthesis_technique": "LLM summarization of generated rules across prompts and batches to produce canonical features.",
            "number_of_papers": null,
            "domain_or_topic": "Scientific literature (domain-pretrained), applied to molecular / chemistry tasks.",
            "output_type": "Domain rules/features and textual explanations.",
            "evaluation_metrics": "Downstream predictive performance in the LLM4SD pipeline and statistical significance of produced rules; ablation contrasts 6.7b vs 30b performance per domain.",
            "performance_results": "Galactica-6.7b produced many statistically significant rules and was chosen for rule validation. Galactica-30b outperformed 6.7b in Quantum Mechanics by ~14% in the ablation study, while in other domains 6.7b rivaled 30b.",
            "comparison_baseline": "Compared to Falcon family LLMs (general-pretrained) in ablation; compared as backbones within LLM4SD versus GNN baselines in overall experiments.",
            "performance_vs_baseline": "Galactica models consistently outperformed Falcon models of comparable sizes on many scientific tasks, highlighting the value of science-focused pretraining.",
            "key_findings": "Domain-specific pretraining (scientific literature) yields strong performance on scientific synthesis and inference tasks; smaller, domain-pretrained models can rival or exceed much larger general models in science applications.",
            "limitations_challenges": "Even domain-pretrained models show scale-task interactions (larger models better on very abstract QM tasks), and performance depends on alignment between pretraining content and target task.",
            "scaling_behavior": "Non-monotonic: Galactica-6.7b often matched Galactica-30b except in highly abstract QM tasks where the 30b model improved performance by ≈14%.",
            "uuid": "e4595.5"
        },
        {
            "name_short": "Falcon (used)",
            "name_full": "Falcon (general-purpose LLM family)",
            "brief_description": "General-purpose transformer LLMs pretrained on large web corpora (including arXiv/Wikipedia) evaluated as backbones (Falcon-7b and Falcon-40b) in the LLM4SD ablation study.",
            "citation_title": "Falcon-40B: an open large language model with state-of-the-art performance",
            "mention_or_use": "use",
            "system_name": "Falcon (7b, 40b)",
            "system_description": "Open LLM family trained for broad applications; in this paper Falcon-7b underperformed on scientific tasks relative to Falcon-40b and Galactica variants, indicating that general pretraining requires larger scale to match science-specialized models.",
            "llm_model_used": "Falcon-7b, Falcon-40b",
            "extraction_technique": "Pretraining recall via prompting; used similarly for literature-synthesis and data-inference prompts.",
            "synthesis_technique": "Summarization of generated rules; used in LLM4SD pipeline parallel to Galactica variants.",
            "number_of_papers": null,
            "domain_or_topic": "General web and scientific text (arXiv/Wikipedia included in pretraining), applied to molecular prediction tasks in experiments.",
            "output_type": "Generated rules/features and explanations when used within LLM4SD.",
            "evaluation_metrics": "Downstream predictive performance in LLM4SD; ablation contrasts across model sizes.",
            "performance_results": "Falcon-40b outperformed Falcon-7b; Falcon-7b failed some tasks (physiology and quantum mechanics) whereas Falcon-40b had sufficient scale to perform; overall Galactica models often outperformed Falcon models of similar size.",
            "comparison_baseline": "Compared against Galactica family in ablation and against GNN baselines in downstream tasks.",
            "performance_vs_baseline": "Falcon series required larger scale to approach scientific-task performance; Falcon-40b bridged much of the gap compared to Falcon-7b.",
            "key_findings": "General-purpose LLMs can be adapted to scientific synthesis tasks but need significant scale; domain-specific pretraining is a major advantage.",
            "limitations_challenges": "Scale sensitivity — smaller general models may fail on domain-specific scientific reasoning tasks.",
            "scaling_behavior": "Clear scaling trend within Falcon family: 40b substantially outperforms 7b on the evaluated scientific tasks.",
            "uuid": "e4595.6"
        },
        {
            "name_short": "Chain-of-Thought / Least-to-Most Prompting (cited)",
            "name_full": "Chain-of-Thought and Least-to-Most Prompting Techniques",
            "brief_description": "Prompting strategies (Chain-of-Thought, Least-to-Most) cited as techniques that elicit stepwise reasoning from LLMs and thereby help them perform complex multi-step inference, cited as relevant background methods for eliciting reasoning in LLMs.",
            "citation_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "mention_or_use": "mention",
            "system_name": "Chain-of-Thought / Least-to-Most Prompting",
            "system_description": "Techniques that present or prompt LLMs to generate intermediate reasoning steps (chain-of-thought) or decompose complex tasks into easier subproblems (least-to-most) to improve multi-step reasoning outputs; cited in paper as methodological background for eliciting LLM reasoning during rule inference.",
            "llm_model_used": "General LLM prompting techniques; not tied to a single model in this paper.",
            "extraction_technique": "Stepwise prompting / decompositional prompts to encourage transparent intermediate reasoning when inferring rules from examples.",
            "synthesis_technique": "Use of intermediate steps to produce robust, explainable rules which can be summarized and converted into features.",
            "number_of_papers": null,
            "domain_or_topic": "General LLM reasoning techniques relevant to scientific knowledge elicitation.",
            "output_type": "Stepwise rationales and intermediate reasoning chains that support rule generation.",
            "evaluation_metrics": "Not directly evaluated in this paper; cited as enabling techniques in prior work.",
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": "Cited as important methods to elicit complex reasoning in LLMs that underpin the plausibility of data-inference prompts used in LLM4SD.",
            "limitations_challenges": "Cited literature discusses potential for spurious or unfaithful chains; not directly analyzed in LLM4SD.",
            "scaling_behavior": "Prior work shows chain-of-thought emerges more reliably in larger LLMs; LLM4SD ablation is consistent with scale-dependent emergent reasoning.",
            "uuid": "e4595.7"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Galactica: A large language model for science",
            "rating": 2
        },
        {
            "paper_title": "Falcon-40B: an open large language model with state-of-the-art performance",
            "rating": 2
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2
        },
        {
            "paper_title": "Can ChatGPT be used to generate scientific hypotheses?",
            "rating": 1
        },
        {
            "paper_title": "Science in the age of large language models",
            "rating": 1
        },
        {
            "paper_title": "Large language models encode clinical knowledge",
            "rating": 1
        }
    ],
    "cost": 0.020789749999999996,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Large Language Models for Scientific Synthesis, Inference and Explanation</p>
<p>Yizhen Zheng yizhen.zheng1@monash.edu 
Department of Data Science and Artificial Intelligence
Monash University
VictoriaAustralia</p>
<p>indicates equal contribution</p>
<p>Huan Yee Koh 
Department of Data Science and Artificial Intelligence
Monash University
VictoriaAustralia</p>
<p>Drug Discovery Biology
Monash Institute of Pharmaceutical Sciences
Monash University
VictoriaAustralia</p>
<p>indicates equal contribution</p>
<p>Jiaxin Ju 
School of Information and Communication Technology and Institute for Integrated and Intelligent Systems
Griffith University
QueenslandAustralia</p>
<p>indicates equal contribution</p>
<p>Anh T N Nguyen 
Drug Discovery Biology
Monash Institute of Pharmaceutical Sciences
Monash University
VictoriaAustralia</p>
<p>Lauren T May 
Drug Discovery Biology
Monash Institute of Pharmaceutical Sciences
Monash University
VictoriaAustralia</p>
<p>Victorian Heart Institute
Monash University
VictoriaAustralia</p>
<p>Geoffrey I Webb geoff.webb@monash.edu 
Department of Data Science and Artificial Intelligence
Monash University
VictoriaAustralia</p>
<p>Shirui Pan s.pan@griffith.edu.au 
School of Information and Communication Technology and Institute for Integrated and Intelligent Systems
Griffith University
QueenslandAustralia</p>
<p>Large Language Models for Scientific Synthesis, Inference and Explanation
FB149F66E1424304933513260ABB2681
Large language models are a form of artificial intelligence systems whose primary knowledge consists of the statistical patterns, semantic relationships, and syntactical structures of language 1 .Despite their limited forms of 'knowledge,' these systems are adept at numerous complex tasks including creative writing, storytelling, translation, question-answering, summarization, and computer code generation 2,3 .However, they have yet to demonstrate advanced applications in natural science 4,5 .Here we show how large language models can perform scientific synthesis, inference, and explanation.We present a method for using general purpose large language models to make inferences from scientific datasets of the form usually associated with special purpose machine learning algorithms.We show that the large language model can augment this 'knowledge' by synthesizing from the scientific literature.When a conventional machine learning system is augmented with this synthesized and inferred knowledge it can outperform the current state of the art across a range of benchmark tasks for predicting molecular properties.This approach has the further advantage that the large language model can explain the machine learning system's predictions.We anticipate that our framework will open new avenues for AI to accelerate the pace of scientific discovery.</p>
<p>Introduction</p>
<p>Scientific productivity is in precipitous decline, with the rate of progress in many fields approximately halving every 13 years 6 .As scientific discovery becomes increasingly complex and challenging, traditional methodologies struggle to keep pace, necessitating innovative approaches.Meanwhile, Large Language Model (LLM) Artificial Intelligence systems have shown remarkable capabilities in a wide range of tasks.From creative writing to translating languages, from answering intricate queries 7,8 to code generation 9 , their capabilities have been transformative in various domains 1,2,3,10,11 .In this work we show that these LLMs have similar transformational potential in the natural sciences.Particularly, we demonstrate that LLMs can synthesize postulates from the scientific literature, make inferences from scientific data, and elucidate their conclusions with explanations.</p>
<p>LLMs are trained on large corpuses of text, including much of the scientific literature.Notable models like BioBert 12 , SciBERT 13 , Med-PALM 11 , and Galactica 14 are specifically tailored to the scientific domain.Meanwhile, general-purpose LLMs like Falcon 15 integrate extensive scientific literature in their pretraining, including sources such as arXiv and Wikipedia.We demonstrate that these systems have acquired deep abilities to interpret and manipulate the formal scientific language for describing molecules, SMILES strings, along with capability to apply information from the scientific literature in their interpretation.We present a scientific discovery pipeline LLM4SD (Large Language Models for Scientific Discovery) designed to tackle complex molecular property prediction tasks.LLM4SD operates by specifying rules for deriving features from SMILES strings that are relevant to predicting a target feature.Some of these rules are synthesized from the scientific literature that the LLMs encode.Others are inferred from training sets of SMILES strings each labelled with the relevant classes or property values.A standard machine learning model can then be learned from the training data using the rule-based features.Finally, our pipeline utilizes LLMs to produce interpretable outcomes, allowing human experts to ascertain the specific factors influencing the final predictions.We show that this pipeline achieves the current state of the art across 58 benchmark tasks spanning four domains -Physiology, Biophysics, Physical Chemistry and Quantum Mechanics.Despite these auspicious outcomes, we acknowledge the vastness and intricacy of the scientific discovery landscape; our endeavours have merely scratched the surface.Nonetheless, the strides made by LLM4SD pave the way for deeper exploration, heralding an era where AI-driven insights interweave with human ingenuity to redress the current decline in scientific productivity.Looking ahead, we are optimistic about AI's potential role as a linchpin in the future of scientific discovery, revolutionizing processes and expediting breakthroughs.</p>
<p>Large Language Models for Scientific Discovery</p>
<p>Our scientific discovery pipeline, LLM4SD, shown in Fig. 1 consists of 4 main components: Knowledge Synthesis from the Scientific Literature, Knowledge Inference from Data, Interpretable Model Training and Interpretable Explanation Generation.We demonstrate the application of our pipeline to 58 specialized property prediction tasks across four scientific domains: Physiology, Biophysics, Physical Chemistry, and Quantum Mechanics.</p>
<p>In the Knowledge Synthesis from Literature phase (Fig. 1a), LLMs use pre-trained knowledge from an extensive literature amassed from LLMs' pretraining 14,15 to synthesize domain-specific molecular property prediction rules.Then, in the Knowledge Inference from Data phase (Fig. 1b), LLMs harness their inferential and analytical skills to infer molecular property prediction rules from the patterns in the datasets.These rules can generate features that effectively distinguish between different class instances or predict specific properties, such as a molecule's lipophilicity.This process mirrors how human scientists formulate hypotheses based on observation.In both the knowledge synthesis and inference stages, we require that the rules have either a numerical or categorical measure associated with them.This ensures that the rules can be readily transformed into corresponding functions, which in turn can convert each data instance into a vector of values.</p>
<p>Rules, independently defined by LLMs, transform data instances into vectorized representations, i.e., features.These rule-based features facilitate the training of an interpretable model, e.g., random forest or linear classifier (Fig. 1c).Our preference for training these interpretable models stems from a desire to enhance transparency during predictions.Remarkably, we noted that when enhanced with LLM4SD, traditional interpretable models like random forests can surpass stateof-the-art baselines.These interpretable models, once trained, are adeptly employed for downstream application, encompassing both classification and regression scientific tasks.This entire workflow draws parallels with the methodical approach of human scientists-designing experiments to validate their proposed hypotheses.Please summarise the following information and generate response to explain why "C1=CC=C(C=C1)CCNN" is BBBP:</p>
<p>Step c result</p>
<p>Step a, b output</p>
<p>Step c output</p>
<p>Step c analysis</p>
<p>Classification Task</p>
<p>Regression Task</p>
<p>Linear Classifier Random Forest</p>
<p>Models</p>
<p>LLMs</p>
<p>In the final stages (Fig. 1d), we tap into the LLMs' adeptness at information summarization.They are tasked to demystify the decision-making mechanism, illuminating how these interpretable models arrive at prediction outcomes based on instance representations, rules, and their respective significance.This clarity and transparency positions LLMs as intuitive partners, enabling scientists to seamlessly interface with and derive insights from the system's decisionmaking processes.To improve usability for researchers, we have created a web-based application that offers knowledge synthesis, inference, and prediction with explanation functions (see Supplementary Information 3).</p>
<p>By fostering this symbiotic relationship, we not only amplify the efficacy of scientific investigations but also elevate the confidence and trust in AI-assisted conclusions, driving forward the frontier of collaborative research.</p>
<p>Experiment Results</p>
<p>In this section, we offer a synopsis of LLM4SD's pivotal results spanning the 4 domains of physiology, biophysics, quantum mechanics and physical chemistry.Notably, all results of LLM4SD are obtained based on open-source LLM backbones to ensure reproducibility.Subsequently, we delved into an ablation study of LLM4SD, examining its performance across various LLM backbones 14,15 of differing scales and pretraining datasets.</p>
<p>Overall Performance on Four Domains</p>
<p>To evaluate the versatility of LLM4SD's application, we conducted a comprehensive analysis of its performance across 58 molecular prediction tasks across the 4 domains (Fig. 2).Specifically, the physiology domain comprised (Blood-Brain Barrier Penetration) BBBP 16 , ClinTox 17 , Tox21 18 with 12 tasks, and SIDER 19 with 27 tasks.Biophysics had two tasks, BACE 20 and HIV 18 , while physical chemistry had three regression tasks: ESOL 21 , FreeSolv 22 and Lipophilicity 18 .Quantum mechanics presented 12 regression tasks under QM9 23 .The detailed description of these tasks is illustrated in the method section (see Methods, 'Datasets').We compared LL4SD's performance with specialized, state-of-the-art supervised machine learning techniques.These are advanced Graph Neural Networks (GNNs), namely AttrMask 24 , GraphCL 25 , MolCLR 26 , 3DInfomax 27 , GraphMVP 28 , and MoleBERT 29 .Each model was pretrained on large datasets with diverse molecular knowledge and then fine-tuned for specific tasks (see Methods).As a standard baseline, we implemented Random Forest 30 with ECFP4 31 as input set features.Benchmarking LLM4SD against the baseline, LLM4SD demonstrated its superior efficacy and performance (Fig 2).This exemplary performance spanned 58 diverse tasks, from physiology (Extended Data Fig. 1-3) and biophysics (Extended Data Fig. 4) to physical chemistry (Extended Data Fig. 5) and quantum mechanics (Extended Fig. 6).</p>
<p>In both physiology and biophysics, our model outperformed all existing baselines (Fig. 2a).Notably, we attained state-of-the-art (SOTA) results in Physiology, raising the AUC-ROC from a previous best of 74.43% to 76.60%, a gain of 2.8%.In Biophysics, our model further enhanced performance, advancing the AUC-ROC from 81.7% to 83.4%, marking a 2.0% improvement.These advancements in physiology and biophysics emphasize the robustness and precision of LLM4SD in tasks that demand intricate biological understanding and modeling.On tasks in quantum mechanics and physical chemistry, LLM4SD demonstrated substantial advancements (Fig. 2b).In the domain of quantum mechanics, it showed a profound improvement of 48.2% over the best performed baseline, registering an average MAE of 5.8233 across 12 tasks as opposed to 11.2450.Similarly, in physical chemistry, LLM4SD observed a noteworthy enhancement, with the model reaching a MAE of 1.28 marking an 18.5% advancement over the baseline MAE of 1.57.These significant improvements in regression tasks affirm the refined capability of our approach in continuous prediction.</p>
<p>Overall, LLM4SD's marked improvements not only affirm its supremacy over specialized, and often black-box, state-of-the-art models but also highlight its unparalleled ability to synthesize postulates, infer scientific data, and provide insightful explanations.This offers a fresh perspective in computational research and heralds a new direction in scientific endeavors.</p>
<p>Ablation Study</p>
<p>To delve deeper into the intricacies of the LLM4SD pipeline, we conducted an ablation study, focusing on discerning the influence of scale and pretraining datasets on the performance of Large Language Models (LLMs).In addition, we assessed the relative contributions of knowledge synthesis and inference.Our evaluation spanned across a spectrum of foundational LLM backbones, notably the Falcon 7b 15 , Falcon 40b 15 , Galactica-6.7b 14, and Galactica-30b 14 .</p>
<p>Here, we selected open-source LLM backbones to ensure the reproducibility of our work.It is worth noting the distinct differences between the Falcon and Galactica series of LLMs.In particular, the Falcon models are trained for a broad range of applications, imbibing a more general context during their pretraining phase, while the Galactica models are pretrained on mainly scientific literature, making them particularly suitable for science.</p>
<p>Effect of Scale</p>
<p>The ablation study of LLM4SD, which compared four open-source LLM backbones, revealed substantial differences among the different LLMs (Fig. 3a, b).Particularly within the Falcon series, performance disparities were conspicuous.The Falcon 7b, a smaller model, fell short compared to the Falcon 40b in its range of domain expertise.Notably, it failed to conduct tasks in two key areas: physiology and quantum mechanics, indicating a weaker understanding of scientific challenges and data interpretation.Conversely, the Galactica series painted a more nuanced picture.Unlike with the Falcon series, a larger model did not necessarily translate to superior performance.In disciplines such as Physiology, Biophysics, and Physical Chemistry, Galactica 6.7b rivaled the performance of Galactica-30b, despite the latter having more than 4 times the number of parameters.However, a b c Fail Fail in the domain of Quantum Mechanics, the larger Galactica 30b surged ahead, outperforming Galactica 6.7b by a margin of 14%.This variance could be attributed to the intricate and abstract nature of Quantum Mechanics, where the depth and breadth of knowledge encapsulated in the larger model might offer a discernible advantage.</p>
<p>Effect of Pretraining Datasets of LLMs</p>
<p>From these observations it becomes evident that an LLM steeped in scientific literature, even if smaller in scale, exhibits a commendable prowess in scientific tasks (Fig. 3a, b).Conversely, the Falcon series, designed for general utility, necessitates a more substantial scale to effectively navigate scientific challenges.We postulate that this phenomenon is underpinned by the emergent capabilities 32 inherent to large-scale LLMs.These capabilities empower the more expansive Falcon-40b to bridge the knowledge gap and adapt to scientific tasks.In a broader perspective, despite their relatively modest scale, the Galactica models consistently outperformed the Falcon series, underscoring the pivotal role of domain-specific pretraining.</p>
<p>Contributions of knowledge synthesis and inference</p>
<p>In our exploration of LLM4SD with respect to various knowledge sources, we discerned the performance variance arising from the use of rule-based features synthesized from literature, rule-based features inferred from data, and a combined approach.Overall values for these categories were obtained by averaging over results for all tasks in a domain (Fig. 3c).</p>
<p>In a comprehensive assessment of various scientific domains, the combination of synthesis and inference features consistently outperformed individual methods.Specifically, in the field of physiology, an AUC-ROC of 76.38 was achieved using both methods, compared to 72.15 with synthesis alone and 72.12 with inference.Similarly, in biophysics, combining both methods yielded an AUC-ROC of 80.95, surpassing the scores of 75.62 and 77.23 obtained from synthesis and inference features, respectively.In physical chemistry, the combined approach resulted in an RMSE of 1.38, which is notably better than the 1.72 from synthesis features and 1.92 from inference features.Lastly, in Quantum Mechanics, the use of both synthesis and inference features produced a MAE of 6.82, improving upon the values of 9.81 and 7.18 recorded with synthesis and inference alone.Notably, comparing just synthesis with just inference, each outperformed the other in 2 out of the 4 domains.</p>
<p>These observations highlight the value of combining knowledge synthesis from scientific literature with inference from data.Literature imparts foundational theoretical insights, while empirical data identifies further regularities.The fusion of these knowledge facets equips the models with a comprehensive understanding, empowering them to excel across varied tasks and domains.</p>
<p>Statistical Analysis and Literature Review: Validating Established Rules</p>
<p>With LLM4SD outperforming specialized, state-of-the-art methods, we further validated the rules generated by Galactica-6.7bdue to its superior performance and ease of reproducibility.The rules were validated in two ways: statistical tests to confirm the significance of these rules, and literature review to assess whether the rules are discussed in existing scientific literature.</p>
<p>For statistical tests of rules, we employed the Mann-Whitney U test 33 for classification tasks and the linear regression t-test for regression tasks.The Mann-Whitney U test 33 compared the distributions of chosen rule across the two classes of the target variable, thereby evaluating the statistical relevance of the rule's ability to split and distinguish classes.Conversely, the linear regression t-test treated the chosen rule as the independent variable and examined whether its coefficient significantly deviated from 0, reflecting whether the rule contributes to regression prediction.Chemistry.In the statistical analysis, the significance of a rule is determined based on the task type: for classification, the Mann-Whitney U test 33 compares the difference in distributions of chosen rule across the two classes of the target variable; and for regression, the linear regression t-test 26 treats the chosen rule as the independent variable and examined whether its coefficient significantly deviated from 0, reflecting whether the rule contributes to prediction.In both cases, we used a 0.05 p-value threshold to determine rule significance.In the literature review, we assessed the prevalence of a rule in existing literature.With statistical analysis and literature review, each rule is categorized into one of three classes: statistically significant and literature supported; statistically significant and not found in literature; and statistically insignificant.Across all tasks, literaturesynthesized knowledge rules were generally both prevalent in existing literature and statistically significant.In contrast, empirically inferred data rules yielded mixed results, with some easily found in existing literature and others not identified by the researchers.We further carried out a comprehensive review with in-domain experts to evaluate the prevalence of a rule in existing literature (see Supplementary Information 4: Literature Review(Example)).After cross-referencing the rules with scientific literature, we categorized each rule into one of three classes: statistically significant and literature supported; statistically significant and not found in literature; and statistically insignificant (Fig. 4).</p>
<p>Knowledge Synthesis from Scientific Literature</p>
<p>We discovered that most of the synthesized rules we examined are readily available in existing scholarly works.Notably, an overwhelming majority (85%) of these rules were statistically significant in indicating the target labels across all selected tasks, affirming our pipeline's ability to summarize rules from scientific literature that were the most important for different tasks and domains.</p>
<p>Importantly, except for BACE 20 and Tox21-NR-Ahr 18 , we found no instances where statistically significant rules were absent from existing literature (Fig. 4).This aligns with the design of our pipeline: without analyzing the data, LLMs tend to aggregate and summarize existing knowledge.To illustrate, in the context of BBBP, the rules generated by our pipeline were consistent with well-established determinants such as molecular weight, lipophilicity, distribution coefficient, topological polar surface area, and hydrogen bonds [34][35][36] .These findings validate the robustness and reliability of our pipeline in leveraging LLMs to summarize existing scientific literature.</p>
<p>Knowledge Inference from Data</p>
<p>We found that an average of 91.3% of the inferred rules were statistically significant, higher than synthesized rules (Fig. 4).Of these, an average of 74% rules were already documented in existing scientific literature, while 17.3% were not identified by researchers.These latter rules were primarily associated with data patterns that are not widely discussed in the literature but can be inferred from our task dataset, such as obscure molecular substructures that influence target labels like the Gibbs free energy (ΔG°) of a molecule.</p>
<p>In contrast, to the knowledge synthesized from literature, we found that 6 out of 8 tasks have statistically significant rules that were absent from existing literature.This suggests that the rules produced by LLM4SD are not merely a result of the LLM's textual memorization during pretraining.Instead, the inferred rules reflect a genuine capability to derive meaningful rules from data based on the specific task.</p>
<p>Our case studies further substantiated the utility of these unidentified but significant rules.For instance, in BBBP where 38% of rules are significant but unidentified, Galactica 6.7B pinpointed the carbonyl functional group and fragment rings as key determinants of a molecule's BBBP.We hypothesize that these features are crucial for calculating a molecule's cross-sectional area, which in turn influences its orientation in lipid-water interfaces-factors vital for membrane partitioning and permeation 37 .Intriguingly, this suggests that our pipeline enables LLMs to infer what we term as second-order features.These are features that may not be immediately obvious or widely recognized but are consistent with established scientific principles in literature.In doing so, LLMs not only corroborate existing knowledge but also apply existing knowledge in interpreting data, thereby enriching the current scientific discourse.</p>
<p>By leveraging LLMs, our pipeline not only validates well-established scientific principles but also uncovers less documented and even potentially novel rules.This facilitates a more effective and transparent interaction between scientists and the AI system, enhancing both the quality and trustworthiness of the research output.Moreover, the statistically robust but underrepresented rules we identified could serve as promising avenues for future scientific exploration, thereby advancing the frontier of collaborative, AI-assisted research.</p>
<p>Discussion</p>
<p>In our exploration, we unveil the capabilities of LLM4SD through our specially designed pipeline, enabling LLMs to excel in scientific synthesis, inference, and explanation.Through seamless integration with our proposed architecture, LLMs exhibit state-of-the-art (SOTA) performance across a vast expanse of four domains.The inherent versatility of LLM4SD stands as a testament to its potential, making it poised for broader applications across varied domains, thus magnifying its relevance in the current scientific landscape.</p>
<p>Scientific discovery, vast in scope, is constantly evolving with our expanding understanding of the universe.Our study, ambitious in its intent, captures 58 tasks across four distinct domains, providing a glimpse into the immense reservoir of scientific knowledge.While this study serves as a pioneering beacon, demonstrating LLMs' transformative capabilities, it also signals the beginning of a broader exploration.We envision further expansion, integrating more diverse tasks and domains, pushing LLMs to their full potential and reshaping the boundaries of scientific inquiry.</p>
<p>Harnessing the immense power of AI-driven models for scientific discovery brings along its ethical challenges.The vast capabilities of such models, while revolutionizing our understanding, also raise concerns of potential misuse, especially in sensitive domains like biophysics and quantum mechanics.The reliance on machine synthesis and interpretation might overshadow the indispensable human element of scrutiny and ethics in research.As we plunge deeper into the AI era, it's crucial to tread with caution, balancing advancements with rigorous oversight and an unwavering commitment to ethical rigor.</p>
<p>As we gaze towards the horizon, the potential trajectory for LLM4SD is compelling.We anticipate a future where the nexus between LLMs and advanced scientific toolkits deepens.As computational capabilities grow and scientific knowledge expands, our pipeline stands poised for evolutionary enhancements.Our steadfast goal is to harmoniously fuse artificial intelligence with myriad scientific arenas, unlocking novel insights and pioneering avenues previously unimagined.</p>
<p>Methods</p>
<p>Datasets:</p>
<p>We conducted a thorough evaluation of LLM4SD, covering 58 subtasks across four unique domains for a robust assessment.The physiology domain included 41 tasks like BBBP, ClinTox, and the 12-task Tox21, ranging from NR-AR to SR-p53, along with the 27-task SIDER suite covering various medical conditions.Biophysics offered 2 classification tasks: BACE and HIV.</p>
<p>In physical chemistry, we addressed three regression tasks: ESOL, FreeSolv, and Lipophilicity, while the Quantum mechanics domain presented 12 regression tasks within the QM9 dataset, exploring properties from mu to G, providing a comprehensive insight into LLM4SD's capabilities.</p>
<p>Physiology.</p>
<p>BBBP:</p>
<p>The BBBP dataset contains 2,039 instances, each representing unique compounds labeled based on their permeability properties.Predicting which molecules can cross this barrier is paramount for drug development, especially for neurological conditions.</p>
<p>ClinTox:</p>
<p>The ClinTox dataset, with 1,478 instances, provides comprehensive information on the toxicological properties of various compounds.</p>
<p>Biophysics.</p>
<p>HIV: With a collection of 17,930 instances, the HIV dataset offers a comprehensive repository of molecules, represented in the SMILES format.This dataset is instrumental in the classification of compounds based on their potential inhibitory effects against HIV.</p>
<p>BACE:</p>
<p>The BACE dataset, comprising 11,908 instances, is a curated collection of molecules, each represented in the SMILES format.This dataset is tailored for classification tasks, aiming to discern molecules that can inhibit the BACE-1 enzyme.By analyzing the molecules within this dataset, researchers can glean insights into the structural features that confer inhibitory properties against BACE-1.</p>
<p>Physical Chemistry.</p>
<p>ESOL:</p>
<p>The ESOL dataset, comprising 1,128 instances, is a curated collection that delves into the solubility of molecules in water.By analyzing the ESOL dataset, researchers can gain a deeper understanding of the molecular features that dictate solubility, thereby aiding in the design of compounds with optimal solubility profiles.Each entry in this dataset is represented using the SMILES notation, a universal language for describing the structure of chemical species.</p>
<p>FreeSolv: With 642 instances, the FreeSolv dataset provides comprehensive data on the hydration free energy of molecules.This dataset is pivotal for researchers aiming to predict how molecules interact with water, which has implications for drug solubility and stability.Each molecule in the FreeSolv dataset is also represented using the SMILES notation.</p>
<p>Lipophilicity: Lipophilicity is a fundamental property that influences the absorption, distribution, metabolism, and excretion of drugs.The Lipophilicity dataset with 4200 compounds offers a rich resource for understanding this property.Analyzing this dataset allows researchers to discern the molecular attributes that contribute to a compound's lipophilicity, guiding the synthesis of molecules with desired pharmacokinetic properties.Like the other datasets in this domain, each entry is denoted using the SMILES notation.</p>
<p>Quantum</p>
<p>Baselines:</p>
<p>We rigorously assessed our pipeline in comparison to specialized, state-of-the-art supervised machine learning methods.For conventional approaches, we employed Random Forest 30 , using ECFP4 31 as the input feature set.We also considered state-of-the-art Graph Neural Networks (GNNs), including Attribute Masking (AttrMask) 24 , GraphCL 25 , MolCLR 26 , 3DInfomax 27 , GraphMVP 28 , and MoleBERT 29 .Each of these models was initialized with pre-trained weights and subsequently fine-tuned for specific tasks.</p>
<p>In summary, AttrMask pre-training involves teaching the GNN to predict randomly masked atom and bond attributes within molecular graphs.GraphCL and MolCLR use graph augmentations for a contrastive learning objective, aimed at maximizing the similarity between augmentations originating from the same molecule while minimizing similarity between augmentations from different molecules.GraphMVP and 3DInfomax leverage existing 3D molecular datasets to pretrain models capable of deducing 3D molecular geometry from 2D graphs, by optimizing mutual information between 3D summary vectors and GNN graph representations.Finally, MoleBERT, the recent state-of-the-art method, employs a VQ-VAE-based tokenizer to diversify atom vocabulary, thereby balancing dominant and rare atoms.It uses Masked Atoms Modeling (MAM) and Triplet Masked Contrastive Learning (TMCL) for node and graph-level pre-training, respectively.</p>
<p>LLM for Scientific Discovery Pipeline</p>
<p>In this section, we detail the proposed pipeline and the techniques used to align them with the requirements of molecular property prediction tasks.Instead of merely prompting LLMs to generate scientific hypotheses 38 or training them for direct predictions 39 , LLM4SD emulates how human experts conduct scientific research.This includes synthesizing knowledge from literature, inferring hypotheses from datasets, validating findings through experiments, and elucidating the rationale behind predictions.</p>
<p>Knowledge Synthesis from the Scientific Literature</p>
<p>LLMs are usually pretrained on large corpora of text data that include books, articles, websites and other written content.This extensive pretraining helps LLMs to learn the structure of the language, recognize patterns, understand context and acquire a wide-ranging knowledge of facts and concepts.Thus, the goal of the knowledge synthesis process is to extract relevant features from the vast pool of the knowledge that a LLM possesses from the pretraining stage.</p>
<p>To achieve this, we first instruct the LLM to adopt the persona of an experienced chemist, and then engage it to identify pertinent features based on its existing knowledge.This form of roleplaying prompt facilitates the knowledge mining process to mimic how human experts solve real-world challenges.For example, when a chemist needs to predict the bioactivity or BBBP of a molecule, they often apply feature-related rules such as number of hydrogen bond donors/acceptors, molecular weight, and logP.We require that the features identified by LLMs can be measured with a numerical or categorical measure to enable their transcription into corresponding functions.</p>
<p>Knowledge Inference from Data</p>
<p>The objective of knowledge inference form data is to harness the powerful reasoning skills of LLMs to identify relevant features by analyzing the given data.Given their impressive ability to solve mathematical problems and identify patterns, we conjecture that LLMs have the capacity to discern common patterns within groups of molecules based on its scientific understanding.To validate this hypothesis, we provide LLMs with an instruction and several batches of sampled instances with their corresponding class labels or instance property values.In the instruction, the LLM is tasked with analyzing patterns from provided data to identify features that effectively discriminate between two classes of instances or predict their property values.As a result, LLMs will come up with rules distilled from the analysis for each batch.Since the generated rules in different batches may contain duplicates, we ultimately employ the LLMs' summarization capability to condense the rules and eliminate duplicates, resulting in the final list of features.</p>
<p>Interpretable Model Training</p>
<p>In this stage, all the features identified in the first two stages are transcribed into corresponding functions.All these functions take a scientific instance as input, e.g., a SMILES string for molecules, and return a feature value.Consequently, the final representation of an instance resides in an r-dimensional space, where r is the number of features that have been identified.These vector representations function as the feature vectors for the model training.Employing interpretable models like a linear layer or random forest enables quantification of each rule's importance in prediction, thus elucidating their contribution to the model's final decision.This transparency fosters an intuitive comprehension of the decision-making process, enhancing trust and usability among domain experts.</p>
<p>Interpretable Explanation Generation</p>
<p>The final stage in our pipeline involves generating interpretable explanations for the predictions.Specifically, we furnish the LLMs with salient information, including the model prediction, the vector representation, important rules, and their importance scores derived from the random forest or linear layer.Utilizing the inference and summarization ability of the LLMs, the provided information is transformed into a text-based explanation.This stage is pivotal in rendering the results in an accessible manner.It ensures that users can seamlessly understand the decision-making process and each rule's contribution to the overall prediction, thereby enhancing trust and transparency.This accessibility not only facilitates user interaction with the model but also empowers experts in the field to utilize the generated insights for further analysis and decision-making.</p>
<p>Metrics</p>
<p>We assessed LLM4SD across 58 molecular property prediction tasks spanning four domains, utilizing distinct evaluation metrics tailored to each task's nature.For the domains of physiology and biophysics, the Area Under the Receiver Operating Characteristic curve (AUC-ROC) metric was employed.AUC-ROC, measures the ability of the model to distinguish between classes, with a range from 0 to 1, where a higher value indicates better performance.In the domain of physical chemistry, the Root Mean Square Error (RMSE) was used.RMSE quantifies the difference between predicted and observed values, with a lower value indicating a closer fit to the true data.For quantum mechanics, we utilized the Mean Absolute Error (MAE) metric.MAE measures the average magnitude of errors between predicted and true values, with smaller values denoting better accuracy.</p>
<p>Experiment setting</p>
<p>In our experimental setting, we partitioned the data into an 80/10/10 split for training, validation, and test sets.For the domains of physiology, biophysics, and physical chemistry, we employed a scaffold split for molecular compounds.The scaffold split method in these three domains ensures that molecules with similar structures are grouped together, providing a more challenging and realistic evaluation of model generalization.To ensure reproducibility and facilitate further research, the datasets split using this scaffold method are made available in our open-source GitHub repository.In the realm of quantum mechanics, we opted for a random split."without access to 3D information" is added only for QM9 dataset.Extended Data Table 3: Regression task descriptions for the general prompt in Extended Data Table 1.</p>
<p>Extended Data</p>
<p>Extended Data</p>
<p>Regression Task Name Task Description for Table X General Prompt</p>
<p>Fig. 1 .
1
Fig.1.LLM for Scientific Discovery Pipeline.(a) Knowledge synthesis from the scientific literature.In this phase, LLMs leverage their pre-trained understanding, amassed from pretraining on a massive body of literature, to synthesize domain-specific rules.(b) Knowledge inference from data.LLMs analyse the intrinsic patterns in the task-specific datasets to identify labelled patterns and infer empirical rules.(c) Interpretable Model Training.Rules formulated from LLMs are harnessed to convert data instances into features, enabling the development of models that are both effective and readily interpretable.(d) Interpretable Explanation Generation.Culminating the process, LLMs assimilate insights from the preceding steps to articulate comprehensive textual explanations, elucidating the rationales behind predictions.In the figure, the percentage value for each factor indicates its importance in concluding the prediction.</p>
<p>Prompt</p>
<p>C=C1)CCNN", is predicted to be blood-brain barrier permeable (BBBP) based on several key factors:1.TPSA (Topological Polar Surface Area): 8.29% The TPSA of this molecule is found to be 38.05, which is moderately low.A lower TPSA generally means better BBB permeability.2. Number of Heteroatoms: 5.78% This molecule possesses 2 heteroatoms, which suggests a moderate capacity to establish diverse chemical interactions.......... LLMs can perform Explanation for the prediction a. Knowledge Synthesis from the Scientific Literature LLMs LLMs can perform Synthesis from Literature Knowledge Assume you are an experienced Chemist.Please come up with 20/30 rules that are important to predict [Task Description] Assume you are a very experienced chemist.The following data includes molecules and their corresponding value [Task Description].Please infer step-by-step to come up with 3 rules that directly relate the properties/structures of a molecule to predict [Task Description].</p>
<p>Presence of Halogens Presence of Sulfur groups Presence of Nitro groups Vectorization Presence of Alcohol groups c.Interpretable Model Training Assume you are a very experienced Chemist.In the following data, with label 1, it means the smiles string is [Task Description].With label 0, it means the smiles string is not [Task Description].Please infer stepby-step to come up with 3 rules that directly relate the properties/ structures of a molecule to predict [Task Description].</p>
<p>Fig. 2 .
2
Fig.2.Comparison between LLM4SD and baselines across 4 domains.The red dotted line represents the average performance of all baselines.(a) Comparative analysis of model performance versus baselines in physiology and biophysics.(b) Comparative analysis of regression performance: LLM4SD vs. baselines in quantum mechanics and physical chemistry.</p>
<p>Fig. 3 .
3
Fig.3.Ablation study of LLM4SD.(a) Performance comparison of four open-source LLM backbones for physiology and biophysics.The vertical dashed line in the figure separates the two LLM series: Falcon on the left and Galactica series on the right.(b) Performance comparison of four open-source LLM backbones for physical chemistry and quantum mechanics.(c) Examining the influence of both synthesized and inferred knowledge on the average model performance across four domains.The triangle's color signifies the metric employed for domain-specific tasks.A (+) next to the metric name indicates higher values yield better results, while a (-) suggests the contrary.</p>
<p>Fig. 4 .
4
Fig.4.Literature Review and Statistical Analysis of LLM Rules.a-d.We conducted statistical analysis and a comprehensive literature review on rules generated by Galactica 6.7b across all four scientific domains, with two tasks evaluated for each domain: (a) Physiology, (b) Biophysics, (c) Quantum Mechanics, and (d) Physical Chemistry.In the statistical analysis, the significance of a rule is determined based on the task type: for classification, the Mann-Whitney U test33 compares the difference in distributions of chosen rule across the two classes of the target variable; and for regression, the linear regression t-test26 treats the chosen rule as the independent variable and examined whether its coefficient significantly deviated from 0, reflecting whether the rule contributes to prediction.In both cases, we used a 0.05 p-value threshold to determine rule significance.In the literature review, we assessed the prevalence of a rule in existing literature.With statistical analysis and literature review, each rule is categorized into one of three classes: statistically significant and literature supported; statistically significant and not found in literature; and statistically insignificant.Across all tasks, literaturesynthesized knowledge rules were generally both prevalent in existing literature and statistically significant.In contrast, empirically inferred data rules yielded mixed results, with some easily found in existing literature and others not identified by the researchers.</p>
<p>Tox21: With 7 ,
7
831 instances, the Tox21 dataset is a collaborative effort to identify environmental toxicants.Its 12 classification tasks focus on specific biological targets or pathways.The Nuclear Receptor (NR) tasks, namely NR-AhR, NR-AR, NR-AR-LBD, NR-Aromatase, NR-ER, NR-ER-LBD, and NR-PPAR-gamma, examine interactions with intracellular proteins influencing gene expression and potential toxic effects.The Stress Response (SR) tasks, including SR-ARE, SR-ATAD5, SR-HSE, SR-MMP, and SR-p53, explore the impact of chemicals on stress-related cellular pathways.SIDER:The SIDER dataset, with 1,427 instances, offers detailed data on medication side effects.Each task in this dataset relates to a specific adverse drug reaction, aiding researchers in understanding and predicting potential drug side effects.All 27 classification tasks are: 1) Hepatobiliary disorders 2) Metabolism and nutrition disorders 3) Product issues 4) Eye disorders 5) Investigations 6) Musculoskeletal and connective tissue disorders 7) Gastrointestinal disorders 8) Social circumstances 9) Immune system disorders 10) Reproductive system and breast disorders 11) Neoplasms benign, malignant and unspecified (incl cysts and polyps).12) General disorders and administration site conditions 13) Endocrine disorders 14) Surgical and medical procedures 15) Vascular disorders 16) Blood and lymphatic system disorders 17) Skin and subcutaneous tissue disorders 18) Congenital, familial and genetic disorders 19) Infections and infestations 20) Respiratory, thoracic and mediastinal disorders 21) Psychiatric disorders 22) Renal and urinary disorders 23) Pregnancy, puerperium and perinatal conditions 24) Ear and labyrinth disorders 25) Cardiac disorders 26) Nervous system disorders 27) Injury, poisoning and procedural complications.</p>
<p>Mechanics. QM9 :
QM9
The Quantum Mechanics domain, central to understanding the fundamental properties of matter, is exemplified in our evaluation through the QM9 dataset.Comprising 133,885 instances, the QM9 dataset provides a comprehensive exploration of molecules' quantum mechanical attributes, essential for diverse applications from material science to pharmaceuticals.It includes 12 tasks: (Dipole Moment), (Polarizability), (Squared Radius), ZPVE (Zero-Point Vibrational Energy), (Heat Capacity at Constant Volume), (Energy Gap), (Highest Occupied Molecular Orbital Energy), (Lowest Unoccupied Molecular Orbital Energy), (Internal Energy at 0 Kelvin), U (Internal Energy at Standard State), H (Enthalpy), G (Gibbs Free Energy).</p>
<p>Prompts for Knowledge Inference from Data Classification Tasks (General):Assume you are a very experienced biologist/Chemist.In the following data, with label 1, it means [Task Description].With label 0, it means it is not.Please infer step-by-step to come up with 3 rules that directly relate the properties/structures of a molecule.Regression Tasks (General):Assume you are a very experienced biologist/chemist.The following data includes molecules and their corresponding value [Task Description].Please infer step-by-step to come up with 3 rules that directly relate the properties/structures of a molecule (without access to 3D information).Note:</p>
<p>ESOL the water solubility data (log solubility in mols per litre) FreeSolv the octanol/water distribution coefficient (logD at pH 7.4) Lipophilicity the hydration free energy of a molecule in water Quantum Mechanics dipole moment (Mu) of a molecule Isotropic polarizability of a molecule electronic spatial extent of a molecule Zero-Point Vibrational Energy (ZPVE) of a molecule the heat capacity at constant volume of a molecule the HUMO-LUMO gap of a molecule the highest occupied molecular orbital (HOMO) energy of a molecule Lowest Unoccupied Molecular Orbital (LUMO) energy of a molecule internal energy at absolute zero temperature (0 Kelvin), U0, of a molecule U internal energy of a molecule at a specific temperature, specifically at 298.15 Kelvin (approximately room temperature), (U) of a molecule H enthalpy of the molecule at a specific temperature, specifically at 298.15 Kelvin (approximately room temperature), (U) of a molecule G Gibbs free energy of the molecule at a specific temperature, specifically at 298.15 Kelvin (approximately room temperature), (U) of a molecule</p>
<p>Table 1 :
1
General prompt for Knowledge Synthesis from the Scientific Literature and Knowledge Inference from Data.
Prompts</p>
<p>for Knowledge Synthesis from the Scientific Literature Classification Tasks (General):</p>
<p>Assumed you are an experienced biologist/chemist.Please come up with [20 rules/30 rules] that you think are very important to predict [Task Description].Each rule is either about the structure or property of a molecule.Assumed you are an experienced biologist/chemist.Please come up with [20 rules/30 rules] that you think are very important to predict [Task Description].Each rule is either about the structure or property of a molecule (
Regression Tasks (General):</p>
<p>without access to 3D information).</p>
<p>is for smaller LLMs prompt while "30 rules" is for larger LLMs prompt."without access to 3D information" is added only for QM9 dataset.
Note:"20 rules"</p>
<p>Table 2 :
2
Classification task descriptions for the general prompt in Extended Data Table1.
Classification</p>
<p>Task Name Task Description for Table X General Prompt Note: add 'if' for Knowledge Synthesis from the Scientific Literature prompt
BBBP(if) a molecule is blood brain barrier permeable (BBBP)ClinTox(if) a molecule will be approved by the FDABACE(if) a molecule can inhibit human β-secretase 1(BACE-1)HIV(if) a molecule can inhibit HIV replication.
Note:</p>
<p>add 'it is related to' for Knowledge Inference from Data prompt Tox21 (it is related to</p>
<p>) the toxicity activity of a molecule against the [subtask description] in the [nuclear receptor (NR)/ stress response (SR)] signalling pathway.
nr-arandrogen receptornr-ar-lbdandrogen receptor ligand-binding domainnr-ahraryl hydrocarbon receptorSubtask Descriptionnr-aromatase nr-er nr-er-lbd nr-ppar-gamma sr-are sr-atad5 sr-hsearomatase estrogen receptor estrogen receptor ligand-binding domain peroxisome proliferator activated receptor nuclear factor (erythroid-derived 2)-like 2 antioxidant responsive element genotoxicity indicated by ATAD5 heat shock factor response elementsr-mmpmitochondrial membrane potentialsr-p53DNA damage p53-pathwaySider(it is related to) the side-effect activity of a molecule in causing [subtask name].respiratory, thoracic and mediastinal disordersmetabolism and nutrition disordersproduct issues eye disordersinvestigationsnames Subtaskmusculoskeletal and connective tissue disorders general disorders and administration site conditions skin and subcutaneous tissue disorders pregnancy, puerperium and perinatal conditionsblood and lymphatic system disorders surgical and medical procedures congenital, familial and genetic disorders reproductive system and breast disordersimmune system disorders cardiac disorders infections and infestations ear and labyrinth disorderssocial circumstances vascular disorders renal and urinary disorders gastrointestinal disordershepatobiliary disorders endocrine disorders psychiatric disorders nervous system disordersinjury, poisoning andproceduralneoplasms benign, malignant and unspecified (incl cysts and polyps)complications
Acknowledgements:H.Y.K. scholarship is supported by the Australian Government Research Training Program (RTP) Scholarship and Monash University as a co-contribution to Australian Research Council grant ARC DP210100072.L.T.M, G.W and A.T.N.N research into artificial intelligence applications for drug discovery is supported by a National Health and Medical Research Council (NHMRC) of Australia Ideas grant (APP2013629).Computational resources were generously provided by the Nectar Research Cloud, a collaborative Australian research platform supported by the NCRIS-funded Australian Research Data Commons (ARDC) and the MASSIVE HPC facility.We also gratefully acknowledge the support of the Griffith University eResearch Service &amp; Specialized Platforms Team and the use of the High-Performance Computing Cluster "Gowonda".S.R.P is supported by ARC Future Fellowship (No. FT210100097).Data availabilityThe datasets utilized in this study are entirely open-source and have been made publicly available to ensure straightforward replication of our findings.For research related to quantum mechanics, physical chemistry, biophysics, and physiology, the datasets can be accessed at https://moleculenet.org/datasets-1.Code availabilityIn our commitment to transparency and reproducibility, we will release our code showing our implementation in https://github.com/zyzisastudyreallyhardguy/LLM4SD.This encompasses methodologies for literature knowledge mining, knowledge inference rule mining, interpretable model training, and interpretable explanation generation.Throughout this work, we have employed several open-source libraries, including Hugging Face, numpy, rdkit, pytorch, scipy, bitsandbytes, and accelerate.Furthermore, we are in the process of deploying a website to facilitate scientists in utilizing LLM4SD.The site features three core functionalities for scientific users: knowledge synthesis, knowledge inference, and prediction with explanations.Examples of user interactions with the website can be found in the supplementary information.As part of our ongoing commitment, we anticipate the inclusion of additional tasks in the future development phases.Author Contribution:These authors contributed equally: Y.Z.Z., H.Y.K., J.X.J.These authors jointly supervised this work: S.R.P., G.I.W. S.R.P. and G.I.W. supervised the project.Y.Z.Z., H.Y.K., J.X.J. contributed to the conception and design of the work.Y.Z.Z., H.Y.K., J.X.J. contributed to the technical implementation.Y.Z.Z., H.Y.K., J.X.J. prepared the figures.Y.Z.Z.contributed to the design of the web-based application.A.T.N.N. and L.T.M. provided domain expertise for the literature review and validation of rules.Y.Z.Z., H.Y.K., A.T.N.N. and L.T.M. contributed to the design of the rule validation test.All authors edited and revised the manuscript.Competing Interests:The authors declare no competing interests.Extended Figures and TablesExtended Data Fig.1|Detailedperformance comparison between "LLM4SD" and eight baselines in the physiology domain.The red dashed line shows the average result across all methods.Each marker's error bar denotes the method's standard deviation, which is obtained via 10 runs.LLM4SD outperformed other models in 3 out of 4 datasets using the AUC-ROC metric, and consistently surpassing the average across all datasets.The results for Tox21 and SIDER are average scores from 12 and 27 tasks respectively (see Extended Data Fig.2 and 3for detailed breakdown).Extended Data Fig. 2|Detailed performance comparison between "LLM4SD" and eight baselines on Tox21Dataset.The red dashed line shows the average result across all methods.Each marker's error bar denotes the method's standard deviation, which is obtained via 10 runs.LLM4SD ranks among the top three methods in 10 out of 12 tasks, and consistently outperformed the average in all tasks.Extended Data Fig.3|Detailedperformance comparison between "LLM4SD" and eight baselines on Sider Dataset.The red dashed line shows the average result across all methods.Each marker's error bar denotes the method's standard deviation, which is obtained via 10 runs.LLM4SD ranks among the top three methods in 22 out of 27 tasks, and consistently outperforms the average in all tasks with the exception of the "Psychiatric disorders" task.Extended Data Fig.4|Detailed performance comparison between "LLM4SD" and eight baselines in the biophysics domain.The red dashed line shows the average result across all methods, in terms of AUC-ROC.Each marker's error bar denotes the method's standard deviation, which is obtained via 10 runs.LLM4SD outperformed the top-performing baseline by roughly 2% on the HIV dataset and closely matched the best performing method, RandomForest.In both cases, LLM4SD delivered a visibly superior outcome compared to the average performance.Extended Data Fig.5| Detailed performance comparison between "LLM4SD" and eight baselines in the physical chemistry domain.The red dashed line shows the average result across all methods.Each marker's error bar denotes the method's standard deviation, which is obtained via 10 runs.LLM4SD significantly outperformed all baseline methods on ESOL, demonstrating a 56% improvement over the average outcome for that dataset, and achieved stateof-the-art results on the additional datasets, FreeSolv and Lipophilicity.Extended Data Fig.6|Detailed performance comparison between "LLM4SD" and eight baselines in the quantum mechanics domain.The red dashed line shows the average result across all methods.Each marker's error bar denotes the method's standard deviation, which is obtained via 10 runs.LLM4SD excelled in predicting properties such as U0, U, H, and G, showing significant enhancements.In other tasks, the results from LLM4SD were comparable to the average of all methods.
Baby steps in evaluating the capacities of large language models. M C Frank, Nat Rev Psychol. 2023</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, S Agarwal, Adv Neural Inf Process Syst. 332020</p>
<p>GPT-4. 2023OpenAITechnical Report. preprint</p>
<p>Science in the age of large language models. A Birhane, A Kasirzadeh, D Leslie, Nat Rev Phys. 52023</p>
<p>Large language model AI chatbots require approval as medical devices. S Gilbert, H Harvey, T Melvin, Nat Med. 2023</p>
<p>Are ideas getting harder to find?. Nicholas Bloom, Charles I Jones, John Van Reenen, Michael Webb, American Economic Review. 11042020</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, Advances in Neural Information Processing Systems. 352022</p>
<p>Least-to-most prompting enables complex reasoning in large language models. Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, ICLR. 2023</p>
<p>Code Llama: Open Foundation Models for Code. Rozière, Jonas Baptiste, Fabian Gehring, Sten Gloeckle, Itai Sootla, Xiaoqing Gat, Yossi Ell En Tan, Adi, arXiv:2308.129502023arXiv preprint</p>
<p>Health system-scale language models are all-purpose prediction engines. Lavender Jiang, Xujin Yao, Chris Liu, Nima Pour Nejatian, Mustafa Nasir-Moin, Duo Wang, Anas Abidin, Kevin Eaton, Nature. 2023</p>
<p>Large language models encode clinical knowledge. Karan Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Nature. 2023</p>
<p>BioBERT: a pre-trained biomedical language representation model for biomedical text mining. Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, Jaewoo Kang, Bioinformatics. 3642020</p>
<p>SciBERT: A pretrained language model for scientific text. Iz Beltagy, Kyle Lo, Arman Cohan, ACL. 2019</p>
<p>Galactica: A large language model for science. Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, Robert Stojnic, 2022Preprint</p>
<p>Falcon-40B: an open large language model with state-of-the-art performance. Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Merouane Debbah, Etienne Goffinet, Technology Innovation Institute. 2023Technical report</p>
<p>A Bayesian approach to in silico blood-brain barrier penetration modeling. Ines Martins, Ana L Filipa, Luis Teixeira, Andre O Pinheiro, Falcao, Journal of Chemical Information and Modeling. 5262012</p>
<p>A data-driven approach to predicting successes and failures of clinical trials. Kaitlyn M Gayvert, S Neel, Olivier Madhukar, Elemento, Cell Chemical Biology. 23102016</p>
<p>MoleculeNet: a benchmark for molecular machine learning. Z Wu, B Ramsundar, E N Feinberg, J Gomes, C Geniesse, A S Pappu, . . Pande, V , Chemical Science. 922018</p>
<p>The SIDER database of drugs and side effects. Michael Kuhn, Ivica Letunic, Lars Juhl Jensen, Peer Bork, Nucleic Acids Research. 44D12016</p>
<p>Computational modeling of β-secretase 1 (BACE-1) inhibitors using ligand based approaches. Govindan Subramanian, Bharath Ramsundar, Vijay Pande, Rajiah Aldrin Denny, Journal of Chemical Information and Modeling. 5610252016</p>
<p>ESOL: estimating aqueous solubility directly from molecular structure. John S Delaney, Journal of Chemical Information and Computer Sciences. 4432004</p>
<p>FreeSolv: a database of experimental and calculated hydration free energies, with input files. David L Mobley, J Peter Guthrie, Journal of Computer-Aided Molecular Design. 282014</p>
<p>Quantum chemistry structures and properties of 134 kilo molecules. Ramakrishnan, Pavlo O Raghunathan, Matthias Dral, O Rupp, Von Anatole, Lilienfeld, Scientific Data. 112014</p>
<p>Strategies for Pre-training Graph Neural Networks. Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay Pande, Jure Leskovec, International Conference on Learning Representations. 2019</p>
<p>Graph contrastive learning with augmentations. Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, Yang Shen, Advances in Neural Information Processing Systems. 332020</p>
<p>Molecular contrastive learning of representations via graph neural networks. Yuyang Wang, Jianren Wang, Zhonglin Cao, Amir Barati, Farimani , Nature Machine Intelligence. 432022</p>
<p>3d infomax improves gnns for molecular property prediction. Hannes Stärk, Dominique Beaini, Gabriele Corso, Prudencio Tossou, Christian Dallago, Stephan Günnemann, Pietro Liò, International Conference on Machine Learning. PMLR2022</p>
<p>Pre-training Molecular Graph Representation with 3D Geometry. Shengchao Liu, Hanchen Wang, Weiyang Liu, Joan Lasenby, Hongyu Guo, Jian Tang, International Conference on Learning Representations. 2021</p>
<p>Mole-bert: Rethinking pre-training graph neural networks for molecules. Jun Xia, Chengshuai Zhao, Bozhen Hu, Zhangyang Gao, Cheng Tan, Yue Liu, Siyuan Li, Stan Z Li, The Eleventh International Conference on Learning Representations. 2022</p>
<p>Random forests. Leo Breiman, Machine Learning. 452001</p>
<p>Extended-connectivity fingerprints. David Rogers, Mathew Hahn, Journal of Chemical Information and Modeling. 5052010</p>
<p>Emergent abilities of large language models. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, TMLR. 2022</p>
<p>Mann-Whitney U Test. Patrick E Mcknight, Julius Najab, The Corsini Encyclopedia of Psychology. 2010</p>
<p>Defining desirable central nervous system drug space through the alignment of molecular properties, in vitro adme, and safety attributes. T T Wager, ACS Chemical Neuroscience. 12010</p>
<p>Moving beyond rules: the development of a central nervous system multiparameter optimization (cns mpo) approach to enable alignment of druglike properties. T T Wager, X Hou, P R Verhoest, A Villalobos, ACS Chemical Neuroscience. 12010</p>
<p>Molecular determinants of blood-brain barrier permeation. W J Geldenhuys, A S Mohammad, C E Adkins, P R Lockman, Therapetuic. Delivery. 62015</p>
<p>In silico prediction of blood-brain barrier permeation using the calculated molecular cross-sectional area as main parameter. G Gerebtzoff, A Seelig, J. Chemical Information Modeling. 462006</p>
<p>Can ChatGPT be used to generate scientific hypotheses?. Yang Park, Daniel Jeong, Zhichu Kaplan, Chia-Wei Ren, Changhao Hsu, Haowei Li, Sipei Xu, Ju Li, Li, arXiv:2304.122082023arXiv preprint</p>
<p>Smiles transformer: Pre-trained molecular fingerprint for low data drug discovery. Shion Honda, Shoi Shi, Hiroki R Ueda, arXiv:1911.047382019arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
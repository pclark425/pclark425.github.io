<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2023 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2023</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2023</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-48.html">extraction-schema-48</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of compositional generalization, systematic generalization, or out-of-distribution generalization experiments, including model architectures, task characteristics, performance metrics, and comparisons across different conditions.</div>
                <p><strong>Paper ID:</strong> paper-278911205</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.20278v1.pdf" target="_blank">The Coverage Principle: A Framework for Understanding Compositional Generalization</a></p>
                <p><strong>Paper Abstract:</strong> Large language models excel at pattern matching, yet often fall short in systematic compositional generalization. We propose the coverage principle: a data-centric framework showing that models relying primarily on pattern matching for compositional tasks cannot reliably generalize beyond substituting fragments that yield identical results when used in the same contexts. We demonstrate that this framework has a strong predictive power for the generalization capabilities of Transformers. First, we derive and empirically confirm that the training data required for two-hop generalization grows at least quadratically with the token set size, and the training data efficiency does not improve with 20x parameter scaling. Second, for compositional tasks with path ambiguity where one variable affects the output through multiple computational paths, we show that Transformers learn context-dependent state representations that undermine both performance and interoperability. Third, Chain-of-Thought supervision improves training data efficiency for multi-hop tasks but still struggles with path ambiguity. Finally, we outline a \emph{mechanism-based} taxonomy that distinguishes three ways neural networks can generalize: structure-based (bounded by coverage), property-based (leveraging algebraic invariances), and shared-operator (through function reuse). This conceptual lens contextualizes our results and highlights where new architectural ideas are needed to achieve systematic compositionally. Overall, the coverage principle provides a unified lens for understanding compositional reasoning, and underscores the need for fundamental architectural or training innovations to achieve truly systematic compositionality.</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2023.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2023.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of compositional generalization, systematic generalization, or out-of-distribution generalization experiments, including model architectures, task characteristics, performance metrics, and comparisons across different conditions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>2-HOP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>2-HOP synthetic compositional task</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Synthetic two-hop function-composition task f(x1,x2,x3)=f2(f1(x1,x2),x3) used to study compositional generalization and the coverage principle; datasets built from random maps over a finite token set and evaluated for ID/OOD generalization and representation clustering.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 (GPT-2-style autoregressive Transformer variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Auto-regressive GPT-2 style Transformer (decoder-only) trained from task data; attention-based layers with standard transformer blocks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-2 variants (68M, 96M, 1.5B)</td>
                        </tr>
                        <tr>
                            <td><strong>is_pretrained</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>standard autoregressive Transformer (multi-head self-attention), no explicit variable binding or module decomposition</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>synthetic function-composition (discrete token mapping)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>2-HOP</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Inputs are three discrete tokens (x1,x2,x3). Two random primitive functions f1:X^2->X and f2:X^2->X are sampled; target t = f2(f1(x1,x2),x3). Training sets sample in-domain triples (seen portions of primitive domains). The task isolates composition as the only generalization source by construction.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_depth</strong></td>
                            <td>two-hop</td>
                        </tr>
                        <tr>
                            <td><strong>composition_type</strong></td>
                            <td>function composition (nested application of primitive functions)</td>
                        </tr>
                        <tr>
                            <td><strong>split_type</strong></td>
                            <td>In-Domain (ID) = unseen combinations of seen primitives; Out-of-Domain (OOD) = includes unseen primitive applications</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>supervised training from scratch on synthetic examples using AdamW, large batch; training until >99% training accuracy then monitor ID performance</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inoculation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>iid_performance</strong></td>
                            <td>Models reach >99% training accuracy; with sufficient dataset size (N >= Nreq) ID test accuracy reaches 0.99 (measurement threshold used by authors).</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_performance</strong></td>
                            <td>Performance depends on dataset coverage (k-coverage). Empirical Nreq scales with token set size |X| as a power law with exponent c ≈ 2.26 for 2-HOP (log-log fit R^2>0.99). When dataset is below Nreq, ID accuracy fails to reach 0.99 within the practical epoch window.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap</strong></td>
                            <td>Practical gap observed: training accuracy >99% while ID accuracy remains below 0.99 until dataset size meets Nreq (authors use 0.99 ID accuracy as the reliable-generalization threshold).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_depth</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_composition_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_baseline_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Compared across GPT-2 sizes (68M,96M,1.5B) and hyperparameter ablations (LR, weight decay); compared standard target supervision vs. Chain-of-Thought (CoT) supervision and partial-computation supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_comparison</strong></td>
                            <td>Only GPT-2 variants were used; no distinct architectural families tested in main 2-HOP experiment (but CoT supervision and partial-computation examples were contrasted).</td>
                        </tr>
                        <tr>
                            <td><strong>scale_effects</strong></td>
                            <td>Power-law exponent for Nreq (2-HOP) is essentially invariant across model sizes (68M to 1.5B); scaling model parameters (up to 20x) did not materially reduce dataset-size scaling exponent or lower Nreq.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>For 2-HOP, reliable compositional generalization by Transformers is predicted by k-coverage; required dataset size Nreq grows approximately as |X|^{2.26} for the experimental regime studied, implying at-least-quadratic data scaling for two-hop composition when generalization is driven by pattern-matching.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_analysis</strong></td>
                            <td>Failures occur when training data does not provide robust k evidence of functional equivalence (low k); rare combinations remain effectively outside practical coverage and fail to generalize despite high training accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>success_conditions</strong></td>
                            <td>Successful generalization requires robust k-coverage (multiple distinct shared contexts supporting functional equivalence). Partial-computation examples or CoT supervision improve interpretability/efficiency but coverage remains necessary for Type-I generalization.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2023.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2023.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of compositional generalization, systematic generalization, or out-of-distribution generalization experiments, including model architectures, task characteristics, performance metrics, and comparisons across different conditions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PARALLEL-2-HOP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PARALLEL 2-HOP synthetic compositional task</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Synthetic task where two separate first-hop computations produce b1=f1(x1,x2) and b2=f2(x3,x4), then a final f3(b1,b2) produces the output; used to test higher-dimensional coverage effects.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 (GPT-2-style autoregressive Transformer variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Auto-regressive GPT-2 style Transformer trained on synthetic parallel two-hop examples.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-2 variants (68M, 96M, 1.5B)</td>
                        </tr>
                        <tr>
                            <td><strong>is_pretrained</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>standard Transformer, attention mechanisms; no explicit module-sharing or recurrence beyond parameter sharing of Transformer layers</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>synthetic function-composition (parallel composition)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>PARALLEL-2-HOP</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Inputs are four tokens (x1,x2,x3,x4). Two independent first-hop functions produce b1=f1(x1,x2) and b2=f2(x3,x4), and a final f3(b1,b2) yields t. Designed to study combinatorial explosion when two first-hop spaces must be covered.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_depth</strong></td>
                            <td>two-hop (parallel branches)</td>
                        </tr>
                        <tr>
                            <td><strong>composition_type</strong></td>
                            <td>parallel function composition combining two first-hop outputs</td>
                        </tr>
                        <tr>
                            <td><strong>split_type</strong></td>
                            <td>ID vs OOD defined analogously (unseen in-domain combinations vs unseen primitive outputs)</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>supervised training on uniformly sampled in-domain combinations from the seen domains of primitives</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inoculation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>iid_performance</strong></td>
                            <td>Models can reach high training accuracy; ID generalization requires larger N compared to simple 2-HOP due to higher effective dimensionality.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_performance</strong></td>
                            <td>Measured Nreq scales as a power law with exponent c ≈ 2.43 (empirical), indicating steeper data scaling than 2-HOP.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap</strong></td>
                            <td>Same qualitative gap as 2-HOP: training accuracy can be high while ID generalization lags until N exceeds Nreq.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_depth</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_composition_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_baseline_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Compared across GPT-2 sizes and hyperparameter variants; baseline is standard supervision without CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scale_effects</strong></td>
                            <td>Exponent measured across model sizes remains consistent (authors report invariance similar to 2-HOP).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>PARALLEL-2-HOP exhibits a steeper power-law dependence (c≈2.43) on token set size for required dataset Nreq, reflecting additional combinatorial dimensions introduced by parallel branches.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_analysis</strong></td>
                            <td>Higher exponent implies greater sample inefficiency; models fail to generalize on many in-domain combinations unless training dataset size is substantially increased to provide k-evidence across both branches.</td>
                        </tr>
                        <tr>
                            <td><strong>success_conditions</strong></td>
                            <td>Sufficient cross-context evidence (k) across both first-hop spaces; CoT/partial supervision may help but does not remove the need for coverage evidence.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2023.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2023.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of compositional generalization, systematic generalization, or out-of-distribution generalization experiments, including model architectures, task characteristics, performance metrics, and comparisons across different conditions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>3-HOP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>3-HOP synthetic compositional task</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Three-step compositional chain f(x1,x2,x3,x4)=f3(f2(f1(x1,x2),x3),x4) used to measure how compositional depth increases data requirements for pattern-matching Transformers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 (GPT-2-style autoregressive Transformer variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Auto-regressive GPT-2 style Transformer trained on synthetic three-hop chains.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-2 variants (68M, 96M, 1.5B)</td>
                        </tr>
                        <tr>
                            <td><strong>is_pretrained</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>standard Transformer architecture (attention-based), no explicit variable binding</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>synthetic multi-hop function composition</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>3-HOP</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Inputs follow a three-step composition chain (three nested primitive applications). Built from random primitive maps to isolate structure-based generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_depth</strong></td>
                            <td>three-hop</td>
                        </tr>
                        <tr>
                            <td><strong>composition_type</strong></td>
                            <td>nested function composition (multi-step chain)</td>
                        </tr>
                        <tr>
                            <td><strong>split_type</strong></td>
                            <td>ID vs OOD (in-domain = combinations of seen primitive-domain inputs; out-of-domain includes unseen primitives)</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>supervised training on uniformly sampled in-domain triples/quads as appropriate</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inoculation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>iid_performance</strong></td>
                            <td>High training accuracy achievable; ID generalization requires larger datasets than 2-HOP/PARALLEL-2-HOP.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_performance</strong></td>
                            <td>Empirical power-law exponent c ≈ 2.58 for Nreq vs |X| without CoT; with Chain-of-Thought supervision the exponent for 3-HOP drops to ≈1.76 (authors' measured value), indicating substantial improvement in data efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap</strong></td>
                            <td>Substantial until dataset size meets Nreq; CoT supervision reduces the data required for ID generalization but does not remove coverage requirements.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_depth</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_composition_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_baseline_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Compared standard supervision vs Chain-of-Thought supervision; also across model sizes and hyperparameter ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scale_effects</strong></td>
                            <td>Empirical exponent is invariant across tested GPT-2 sizes; model scaling alone does not reduce the compositional data-scaling exponent.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Increasing compositional depth increases required dataset size super-quadratically (empirical c≈2.58 for 3-HOP). Chain-of-Thought (CoT) supervision flattens multi-hop computation into single-hop steps and reduces the exponent (e.g., to ≈1.76 for 3-HOP), improving sample efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_analysis</strong></td>
                            <td>Without CoT or sufficient coverage evidence, Transformer models fail to generalize across deeper compositions unless dataset size grows rapidly with |X|.</td>
                        </tr>
                        <tr>
                            <td><strong>success_conditions</strong></td>
                            <td>Explicit intermediate-state supervision (CoT) improves data efficiency by decomposing the task into shallower steps, but robust coverage per primitive is still required.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2023.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2023.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of compositional generalization, systematic generalization, or out-of-distribution generalization experiments, including model architectures, task characteristics, performance metrics, and comparisons across different conditions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NON-TREE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NON-TREE synthetic compositional task (path ambiguity)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A synthetic task where a single input variable affects output through multiple computational paths (x2 influences both f1 and f2), creating path ambiguity that prevents formation of unified intermediate state representations and hinders compositional generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 (GPT-2-style autoregressive Transformer variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Auto-regressive GPT-2 style Transformer trained on NON-TREE synthetic data to probe path ambiguity effects.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-2 variants (96M and 1.5B explicitly reported; 68M also used elsewhere)</td>
                        </tr>
                        <tr>
                            <td><strong>is_pretrained</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>standard Transformer (attention-based) without explicit mechanisms for binding or unifying intermediate states across different computational paths</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>synthetic compositional task with path ambiguity</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>NON-TREE</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Task structure where x2 influences output both as input to a first-hop primitive and directly to the second-hop primitive, so two different computational paths share variables; this creates path ambiguity because functional equivalence of first-hop fragments depends on x2 value.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_depth</strong></td>
                            <td>two-hop with cross-path dependency (non-tree structure)</td>
                        </tr>
                        <tr>
                            <td><strong>composition_type</strong></td>
                            <td>function composition with shared variable affecting multiple paths (path ambiguity)</td>
                        </tr>
                        <tr>
                            <td><strong>split_type</strong></td>
                            <td>ID vs OOD (ID defined analogously), but many ID examples remain outside k-coverage due to path conditioning</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>standard supervised training on in-domain sampled combinations; extended prolonged training performed in ablations (e.g., 36k epochs) and near-exhaustive ID combinations tested</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inoculation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>iid_performance</strong></td>
                            <td>Models reach high training accuracy (>99%) but ID test accuracy remains substantially lower under same dataset conditions where 2-HOP succeeds; with near-exhaustive ID combinations and extremely prolonged training (36k epochs, N up to 50k) ID accuracy can reach ~0.96 but only after extensive exposure.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_performance</strong></td>
                            <td>Significant generalization failure compared to 2-HOP: under typical dataset sizes/models, NON-TREE does not reach perfect ID generalization; increasing model size to 1.5B yields no significant improvement in ID accuracy under comparable training budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap</strong></td>
                            <td>Large: training >99% but ID accuracy substantially below 0.99 under conditions that suffice for 2-HOP; even after extensive training, representations remain context-dependent (not unified) despite high ID accuracy approaching 0.96 only with near-exhaustive data.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_depth</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_composition_type</strong></td>
                            <td>Representation analyses show failure to form unified intermediate-state representations by b alone; models form (b,x2)-conditioned clusters instead.</td>
                        </tr>
                        <tr>
                            <td><strong>has_baseline_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Compared to 2-HOP and same models with CoT; model scaling (96M vs 1.5B) and prolonged training evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scale_effects</strong></td>
                            <td>Scaling model parameters up to 1.5B did not materially improve NON-TREE ID generalization under same training conditions; indicates limitation is structural (coverage/path ambiguity) rather than capacity.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Path ambiguity prevents Transformers from forming unified intermediate-state representations; instead models produce context-dependent states conditioned on the shared variable, degrading both generalization and interpretability. CoT helps but cannot fully resolve path ambiguity without near-exhaustive combinations.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_analysis</strong></td>
                            <td>Failures concentrate on cases where a variable participates in multiple computational paths: functional equivalence cannot be established across different contexts unless the shared variable's value is identical, so k-coverage is much harder to achieve and representational unification fails.</td>
                        </tr>
                        <tr>
                            <td><strong>success_conditions</strong></td>
                            <td>Only with near-exhaustive exposure to combinations (practically prohibitive) or architectural mechanisms that perform explicit variable binding/abstraction can the model reliably unify intermediate states across paths.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2023.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2023.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of compositional generalization, systematic generalization, or out-of-distribution generalization experiments, including model architectures, task characteristics, performance metrics, and comparisons across different conditions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought (CoT) supervision</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought supervision (explicit intermediate-state supervision)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Supervision format where models are trained to sequentially generate intermediate computation results (e.g., b then t), converting multi-hop tasks into sequences of single-hop predictions and improving data efficiency for multi-step reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 (GPT-2-style autoregressive Transformer variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same Transformer architecture trained with target sequences that include intermediate computation tokens (explicit CoT targets).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-2 variants (68M, 96M, 1.5B)</td>
                        </tr>
                        <tr>
                            <td><strong>is_pretrained</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>standard autoregressive Transformer; supervision changes target sequence, not architecture</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>synthetic multi-hop compositional tasks (2-HOP, 3-HOP, 5-HOP experiments reported)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>CoT-supervised variants of 2-HOP / 3-HOP / 5-HOP</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same compositional tasks but the target completion contains the intermediate token(s) explicitly (e.g., predict b then t), turning a single-token final prediction into multi-token supervised trace of computation.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_depth</strong></td>
                            <td>applied to multi-hop tasks (2- to 5-hop tested)</td>
                        </tr>
                        <tr>
                            <td><strong>composition_type</strong></td>
                            <td>function composition with explicit intermediate-state targets</td>
                        </tr>
                        <tr>
                            <td><strong>split_type</strong></td>
                            <td>ID/OOD analogous to standard tasks; coverage computed with respect to the same functional equivalence definition but now intermediate tokens are supervised</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>supervised training with explicit intermediate-state tokens in targets (CoT); otherwise same optimizer/hyperparameters</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inoculation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>iid_performance</strong></td>
                            <td>CoT substantially improves ID data efficiency; models with CoT reach ID accuracy targets with smaller N than non-CoT models.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_performance</strong></td>
                            <td>CoT reduces required dataset-size scaling exponent for deeper tasks: e.g., 3-HOP exponent drops from ≈2.58 to ≈1.76 in the authors' measurements; CoT makes scaling exponents for 2/3/5-HOP nearly identical in their experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap</strong></td>
                            <td>CoT reduces the sample-efficiency gap: fewer examples are needed to reach high ID accuracy, but CoT does not fully eliminate failures arising from path ambiguity.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_depth</strong></td>
                            <td>Authors report that with CoT the exponents across 2/3/5-HOP converge to similar (lower) values, indicating flattened depth effect on Nreq.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_composition_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_baseline_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Directly compared standard supervision vs CoT supervision on same tasks and models; CoT yields markedly better sample efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scale_effects</strong></td>
                            <td>CoT improvements in data efficiency observed across model sizes; the underlying coverage limitations for path-ambiguous tasks remain.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Chain-of-Thought supervision 'flattens' multi-hop problems into sequential single-hop predictions, substantially improving sample efficiency (reducing measured power-law exponents), but CoT alone does not resolve path ambiguity where the same variable feeds multiple paths.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_analysis</strong></td>
                            <td>Although CoT helps form sequential computation behavior, it does not provide cross-context evidence that different first-hop fragments with same b but different x2 should behave identically when path ambiguity exists.</td>
                        </tr>
                        <tr>
                            <td><strong>success_conditions</strong></td>
                            <td>CoT helps when intermediate states are valid abstractions across contexts (i.e., when coverage evidence exists); it is most effective at reducing compounding data needs for deep chain tasks but requires coverage to generalize systematically.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2023.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2023.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of compositional generalization, systematic generalization, or out-of-distribution generalization experiments, including model architectures, task characteristics, performance metrics, and comparisons across different conditions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Representation & coverage measures</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Representation clustering (IICG) and causal tracing analyses tied to k-coverage</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Analyses and metrics used to connect training data coverage (k-coverage) to internal representation unification and causal importance: Intra-Inter Cosine Gap (IICG) measures clustering by intermediate state, and causal tracing (Indirect Effect) quantifies causal role of activations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 (trained variants used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Standard GPT-2 Transformer whose hidden states are probed by IICG and causal tracing interventions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (68M, 96M, 1.5B)</td>
                        </tr>
                        <tr>
                            <td><strong>is_pretrained</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>standard Transformer; analyses are architectural-agnostic probes into learned representations</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>applied to synthetic compositional tasks (2-HOP, PARALLEL-2-HOP, 3-HOP, NON-TREE)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>representation analyses on compositional tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Probing internal representations to see whether fragments that are functionally equivalent (same intermediate b) are clustered (IICG) and whether intervention/patching on hidden states changes outputs towards corrupted runs (causal tracing/Indirect Effect).</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_depth</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>composition_type</strong></td>
                            <td>analysis methodology rather than a task</td>
                        </tr>
                        <tr>
                            <td><strong>split_type</strong></td>
                            <td>applied to ID and OOD groups, and to examples grouped by k-cutoff</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inoculation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>iid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compositional_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_depth</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_composition_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_baseline_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architectural_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scale_effects</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>When examples fall inside k-coverage, representations of functionally equivalent fragments form tight latent clusters (high IICG); causal tracing shows these clustered states have high Indirect Effect on predictions. OOD examples (outside coverage) show no clustering and are not reliably predicted. Path ambiguity causes context-conditioned clusters (by (b,x2)) instead of unified b clusters.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_analysis</strong></td>
                            <td>IICG and causal tracing reveal that failures to generalize correspond to absence of representation unification; in path-ambiguous tasks the model forms multiple context-dependent representations instead of a single intermediate abstraction.</td>
                        </tr>
                        <tr>
                            <td><strong>success_conditions</strong></td>
                            <td>High k evidence in training (multiple distinct shared contexts) correlates with representation unification and causal circuits that support compositional generalization.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2023.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2023.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of compositional generalization, systematic generalization, or out-of-distribution generalization experiments, including model architectures, task characteristics, performance metrics, and comparisons across different conditions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Model-scaling effect</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of model parameter scaling (GPT-2 68M -> 1.5B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical assessment of whether increasing Transformer parameter count improves sample efficiency or overcomes coverage limits for compositional generalization; finds little benefit for Type-I coverage-limited generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 (68M, 96M, 1.5B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same GPT-2-style architectures scaled in depth/heads/hidden size as described in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>68M, 96M, 1.5B</td>
                        </tr>
                        <tr>
                            <td><strong>is_pretrained</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>standard Transformer scaled in depth/width; no novel inductive biases introduced</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>synthetic compositional tasks (2-HOP, PARALLEL-2-HOP, 3-HOP, NON-TREE)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>scale comparisons on compositional tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Train identical task setups on increasing model sizes to examine whether parameter scale reduces required dataset size Nreq or improves handling of path ambiguity.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_depth</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>composition_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>split_type</strong></td>
                            <td>ID / OOD comparisons preserved across scales</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>same supervised protocol across model sizes</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inoculation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>iid_performance</strong></td>
                            <td>All model sizes reach high training accuracy; convergence speed may vary but main effect studied is ID generalization vs dataset size.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_performance</strong></td>
                            <td>Power-law exponent for Nreq vs |X| for 2-HOP remains essentially invariant across sizes (e.g., c ≈ 2.13..2.28 across 68M-1.5B reported entries); for NON-TREE scaling to 1.5B did not materially improve ID accuracy under comparable conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap</strong></td>
                            <td>Model scale does not eliminate the gap induced by lack of coverage or path ambiguity; increasing parameters alone does not substantially lower Nreq exponent.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_depth</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_composition_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_baseline_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Direct comparisons across three GPT-2 sizes under identical tasks/hyperparameters.</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scale_effects</strong></td>
                            <td>Little to no improvement in required dataset-scaling exponent with up to 20x parameter increase; capacity scaling does not substitute for coverage evidence in Type-I generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Parameter scaling (68M->1.5B) does not substantially change the power-law scaling relationship between |X| and required dataset size for compositional generalization and does not solve failures caused by path ambiguity.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_analysis</strong></td>
                            <td>Model capacity increases fail to resolve structural limits imposed by lack of k-coverage or path ambiguity.</td>
                        </tr>
                        <tr>
                            <td><strong>success_conditions</strong></td>
                            <td>Architectural or training innovations (CoT, partial supervision, explicit binding mechanisms) rather than mere parameter scaling are needed to overcome coverage-limited systematicity.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-thought prompting elicits reasoning in large language models. <em>(Rating: 2)</em></li>
                <li>Grokking of implicit reasoning in transformers: A mechanistic journey to the edge of generalization. <em>(Rating: 2)</em></li>
                <li>Universal transformers <em>(Rating: 1)</em></li>
                <li>The reversal curse: LLMs trained on "a is b" fail to learn "b is a" <em>(Rating: 1)</em></li>
                <li>Do large language models latently perform multi-hop reasoning? <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2023",
    "paper_id": "paper-278911205",
    "extraction_schema_id": "extraction-schema-48",
    "extracted_data": [
        {
            "name_short": "2-HOP",
            "name_full": "2-HOP synthetic compositional task",
            "brief_description": "Synthetic two-hop function-composition task f(x1,x2,x3)=f2(f1(x1,x2),x3) used to study compositional generalization and the coverage principle; datasets built from random maps over a finite token set and evaluated for ID/OOD generalization and representation clustering.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2 (GPT-2-style autoregressive Transformer variants)",
            "model_description": "Auto-regressive GPT-2 style Transformer (decoder-only) trained from task data; attention-based layers with standard transformer blocks.",
            "model_size": "GPT-2 variants (68M, 96M, 1.5B)",
            "is_pretrained": false,
            "architectural_features": "standard autoregressive Transformer (multi-head self-attention), no explicit variable binding or module decomposition",
            "task_domain": "synthetic function-composition (discrete token mapping)",
            "task_name": "2-HOP",
            "task_description": "Inputs are three discrete tokens (x1,x2,x3). Two random primitive functions f1:X^2-&gt;X and f2:X^2-&gt;X are sampled; target t = f2(f1(x1,x2),x3). Training sets sample in-domain triples (seen portions of primitive domains). The task isolates composition as the only generalization source by construction.",
            "compositional_depth": "two-hop",
            "composition_type": "function composition (nested application of primitive functions)",
            "split_type": "In-Domain (ID) = unseen combinations of seen primitives; Out-of-Domain (OOD) = includes unseen primitive applications",
            "training_strategy": "supervised training from scratch on synthetic examples using AdamW, large batch; training until &gt;99% training accuracy then monitor ID performance",
            "curriculum_details": null,
            "inoculation_details": null,
            "iid_performance": "Models reach &gt;99% training accuracy; with sufficient dataset size (N &gt;= Nreq) ID test accuracy reaches 0.99 (measurement threshold used by authors).",
            "compositional_performance": "Performance depends on dataset coverage (k-coverage). Empirical Nreq scales with token set size |X| as a power law with exponent c ≈ 2.26 for 2-HOP (log-log fit R^2&gt;0.99). When dataset is below Nreq, ID accuracy fails to reach 0.99 within the practical epoch window.",
            "generalization_gap": "Practical gap observed: training accuracy &gt;99% while ID accuracy remains below 0.99 until dataset size meets Nreq (authors use 0.99 ID accuracy as the reliable-generalization threshold).",
            "performance_by_depth": null,
            "performance_by_composition_type": null,
            "has_baseline_comparison": true,
            "baseline_comparisons": "Compared across GPT-2 sizes (68M,96M,1.5B) and hyperparameter ablations (LR, weight decay); compared standard target supervision vs. Chain-of-Thought (CoT) supervision and partial-computation supervision.",
            "architectural_comparison": "Only GPT-2 variants were used; no distinct architectural families tested in main 2-HOP experiment (but CoT supervision and partial-computation examples were contrasted).",
            "scale_effects": "Power-law exponent for Nreq (2-HOP) is essentially invariant across model sizes (68M to 1.5B); scaling model parameters (up to 20x) did not materially reduce dataset-size scaling exponent or lower Nreq.",
            "transfer_results": null,
            "key_findings": "For 2-HOP, reliable compositional generalization by Transformers is predicted by k-coverage; required dataset size Nreq grows approximately as |X|^{2.26} for the experimental regime studied, implying at-least-quadratic data scaling for two-hop composition when generalization is driven by pattern-matching.",
            "failure_analysis": "Failures occur when training data does not provide robust k evidence of functional equivalence (low k); rare combinations remain effectively outside practical coverage and fail to generalize despite high training accuracy.",
            "success_conditions": "Successful generalization requires robust k-coverage (multiple distinct shared contexts supporting functional equivalence). Partial-computation examples or CoT supervision improve interpretability/efficiency but coverage remains necessary for Type-I generalization.",
            "uuid": "e2023.0"
        },
        {
            "name_short": "PARALLEL-2-HOP",
            "name_full": "PARALLEL 2-HOP synthetic compositional task",
            "brief_description": "Synthetic task where two separate first-hop computations produce b1=f1(x1,x2) and b2=f2(x3,x4), then a final f3(b1,b2) produces the output; used to test higher-dimensional coverage effects.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2 (GPT-2-style autoregressive Transformer variants)",
            "model_description": "Auto-regressive GPT-2 style Transformer trained on synthetic parallel two-hop examples.",
            "model_size": "GPT-2 variants (68M, 96M, 1.5B)",
            "is_pretrained": false,
            "architectural_features": "standard Transformer, attention mechanisms; no explicit module-sharing or recurrence beyond parameter sharing of Transformer layers",
            "task_domain": "synthetic function-composition (parallel composition)",
            "task_name": "PARALLEL-2-HOP",
            "task_description": "Inputs are four tokens (x1,x2,x3,x4). Two independent first-hop functions produce b1=f1(x1,x2) and b2=f2(x3,x4), and a final f3(b1,b2) yields t. Designed to study combinatorial explosion when two first-hop spaces must be covered.",
            "compositional_depth": "two-hop (parallel branches)",
            "composition_type": "parallel function composition combining two first-hop outputs",
            "split_type": "ID vs OOD defined analogously (unseen in-domain combinations vs unseen primitive outputs)",
            "training_strategy": "supervised training on uniformly sampled in-domain combinations from the seen domains of primitives",
            "curriculum_details": null,
            "inoculation_details": null,
            "iid_performance": "Models can reach high training accuracy; ID generalization requires larger N compared to simple 2-HOP due to higher effective dimensionality.",
            "compositional_performance": "Measured Nreq scales as a power law with exponent c ≈ 2.43 (empirical), indicating steeper data scaling than 2-HOP.",
            "generalization_gap": "Same qualitative gap as 2-HOP: training accuracy can be high while ID generalization lags until N exceeds Nreq.",
            "performance_by_depth": null,
            "performance_by_composition_type": null,
            "has_baseline_comparison": true,
            "baseline_comparisons": "Compared across GPT-2 sizes and hyperparameter variants; baseline is standard supervision without CoT.",
            "architectural_comparison": null,
            "scale_effects": "Exponent measured across model sizes remains consistent (authors report invariance similar to 2-HOP).",
            "transfer_results": null,
            "key_findings": "PARALLEL-2-HOP exhibits a steeper power-law dependence (c≈2.43) on token set size for required dataset Nreq, reflecting additional combinatorial dimensions introduced by parallel branches.",
            "failure_analysis": "Higher exponent implies greater sample inefficiency; models fail to generalize on many in-domain combinations unless training dataset size is substantially increased to provide k-evidence across both branches.",
            "success_conditions": "Sufficient cross-context evidence (k) across both first-hop spaces; CoT/partial supervision may help but does not remove the need for coverage evidence.",
            "uuid": "e2023.1"
        },
        {
            "name_short": "3-HOP",
            "name_full": "3-HOP synthetic compositional task",
            "brief_description": "Three-step compositional chain f(x1,x2,x3,x4)=f3(f2(f1(x1,x2),x3),x4) used to measure how compositional depth increases data requirements for pattern-matching Transformers.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2 (GPT-2-style autoregressive Transformer variants)",
            "model_description": "Auto-regressive GPT-2 style Transformer trained on synthetic three-hop chains.",
            "model_size": "GPT-2 variants (68M, 96M, 1.5B)",
            "is_pretrained": false,
            "architectural_features": "standard Transformer architecture (attention-based), no explicit variable binding",
            "task_domain": "synthetic multi-hop function composition",
            "task_name": "3-HOP",
            "task_description": "Inputs follow a three-step composition chain (three nested primitive applications). Built from random primitive maps to isolate structure-based generalization.",
            "compositional_depth": "three-hop",
            "composition_type": "nested function composition (multi-step chain)",
            "split_type": "ID vs OOD (in-domain = combinations of seen primitive-domain inputs; out-of-domain includes unseen primitives)",
            "training_strategy": "supervised training on uniformly sampled in-domain triples/quads as appropriate",
            "curriculum_details": null,
            "inoculation_details": null,
            "iid_performance": "High training accuracy achievable; ID generalization requires larger datasets than 2-HOP/PARALLEL-2-HOP.",
            "compositional_performance": "Empirical power-law exponent c ≈ 2.58 for Nreq vs |X| without CoT; with Chain-of-Thought supervision the exponent for 3-HOP drops to ≈1.76 (authors' measured value), indicating substantial improvement in data efficiency.",
            "generalization_gap": "Substantial until dataset size meets Nreq; CoT supervision reduces the data required for ID generalization but does not remove coverage requirements.",
            "performance_by_depth": null,
            "performance_by_composition_type": null,
            "has_baseline_comparison": true,
            "baseline_comparisons": "Compared standard supervision vs Chain-of-Thought supervision; also across model sizes and hyperparameter ablations.",
            "architectural_comparison": null,
            "scale_effects": "Empirical exponent is invariant across tested GPT-2 sizes; model scaling alone does not reduce the compositional data-scaling exponent.",
            "transfer_results": null,
            "key_findings": "Increasing compositional depth increases required dataset size super-quadratically (empirical c≈2.58 for 3-HOP). Chain-of-Thought (CoT) supervision flattens multi-hop computation into single-hop steps and reduces the exponent (e.g., to ≈1.76 for 3-HOP), improving sample efficiency.",
            "failure_analysis": "Without CoT or sufficient coverage evidence, Transformer models fail to generalize across deeper compositions unless dataset size grows rapidly with |X|.",
            "success_conditions": "Explicit intermediate-state supervision (CoT) improves data efficiency by decomposing the task into shallower steps, but robust coverage per primitive is still required.",
            "uuid": "e2023.2"
        },
        {
            "name_short": "NON-TREE",
            "name_full": "NON-TREE synthetic compositional task (path ambiguity)",
            "brief_description": "A synthetic task where a single input variable affects output through multiple computational paths (x2 influences both f1 and f2), creating path ambiguity that prevents formation of unified intermediate state representations and hinders compositional generalization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2 (GPT-2-style autoregressive Transformer variants)",
            "model_description": "Auto-regressive GPT-2 style Transformer trained on NON-TREE synthetic data to probe path ambiguity effects.",
            "model_size": "GPT-2 variants (96M and 1.5B explicitly reported; 68M also used elsewhere)",
            "is_pretrained": false,
            "architectural_features": "standard Transformer (attention-based) without explicit mechanisms for binding or unifying intermediate states across different computational paths",
            "task_domain": "synthetic compositional task with path ambiguity",
            "task_name": "NON-TREE",
            "task_description": "Task structure where x2 influences output both as input to a first-hop primitive and directly to the second-hop primitive, so two different computational paths share variables; this creates path ambiguity because functional equivalence of first-hop fragments depends on x2 value.",
            "compositional_depth": "two-hop with cross-path dependency (non-tree structure)",
            "composition_type": "function composition with shared variable affecting multiple paths (path ambiguity)",
            "split_type": "ID vs OOD (ID defined analogously), but many ID examples remain outside k-coverage due to path conditioning",
            "training_strategy": "standard supervised training on in-domain sampled combinations; extended prolonged training performed in ablations (e.g., 36k epochs) and near-exhaustive ID combinations tested",
            "curriculum_details": null,
            "inoculation_details": null,
            "iid_performance": "Models reach high training accuracy (&gt;99%) but ID test accuracy remains substantially lower under same dataset conditions where 2-HOP succeeds; with near-exhaustive ID combinations and extremely prolonged training (36k epochs, N up to 50k) ID accuracy can reach ~0.96 but only after extensive exposure.",
            "compositional_performance": "Significant generalization failure compared to 2-HOP: under typical dataset sizes/models, NON-TREE does not reach perfect ID generalization; increasing model size to 1.5B yields no significant improvement in ID accuracy under comparable training budgets.",
            "generalization_gap": "Large: training &gt;99% but ID accuracy substantially below 0.99 under conditions that suffice for 2-HOP; even after extensive training, representations remain context-dependent (not unified) despite high ID accuracy approaching 0.96 only with near-exhaustive data.",
            "performance_by_depth": null,
            "performance_by_composition_type": "Representation analyses show failure to form unified intermediate-state representations by b alone; models form (b,x2)-conditioned clusters instead.",
            "has_baseline_comparison": true,
            "baseline_comparisons": "Compared to 2-HOP and same models with CoT; model scaling (96M vs 1.5B) and prolonged training evaluated.",
            "architectural_comparison": null,
            "scale_effects": "Scaling model parameters up to 1.5B did not materially improve NON-TREE ID generalization under same training conditions; indicates limitation is structural (coverage/path ambiguity) rather than capacity.",
            "transfer_results": null,
            "key_findings": "Path ambiguity prevents Transformers from forming unified intermediate-state representations; instead models produce context-dependent states conditioned on the shared variable, degrading both generalization and interpretability. CoT helps but cannot fully resolve path ambiguity without near-exhaustive combinations.",
            "failure_analysis": "Failures concentrate on cases where a variable participates in multiple computational paths: functional equivalence cannot be established across different contexts unless the shared variable's value is identical, so k-coverage is much harder to achieve and representational unification fails.",
            "success_conditions": "Only with near-exhaustive exposure to combinations (practically prohibitive) or architectural mechanisms that perform explicit variable binding/abstraction can the model reliably unify intermediate states across paths.",
            "uuid": "e2023.3"
        },
        {
            "name_short": "Chain-of-Thought (CoT) supervision",
            "name_full": "Chain-of-Thought supervision (explicit intermediate-state supervision)",
            "brief_description": "Supervision format where models are trained to sequentially generate intermediate computation results (e.g., b then t), converting multi-hop tasks into sequences of single-hop predictions and improving data efficiency for multi-step reasoning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2 (GPT-2-style autoregressive Transformer variants)",
            "model_description": "Same Transformer architecture trained with target sequences that include intermediate computation tokens (explicit CoT targets).",
            "model_size": "GPT-2 variants (68M, 96M, 1.5B)",
            "is_pretrained": false,
            "architectural_features": "standard autoregressive Transformer; supervision changes target sequence, not architecture",
            "task_domain": "synthetic multi-hop compositional tasks (2-HOP, 3-HOP, 5-HOP experiments reported)",
            "task_name": "CoT-supervised variants of 2-HOP / 3-HOP / 5-HOP",
            "task_description": "Same compositional tasks but the target completion contains the intermediate token(s) explicitly (e.g., predict b then t), turning a single-token final prediction into multi-token supervised trace of computation.",
            "compositional_depth": "applied to multi-hop tasks (2- to 5-hop tested)",
            "composition_type": "function composition with explicit intermediate-state targets",
            "split_type": "ID/OOD analogous to standard tasks; coverage computed with respect to the same functional equivalence definition but now intermediate tokens are supervised",
            "training_strategy": "supervised training with explicit intermediate-state tokens in targets (CoT); otherwise same optimizer/hyperparameters",
            "curriculum_details": null,
            "inoculation_details": null,
            "iid_performance": "CoT substantially improves ID data efficiency; models with CoT reach ID accuracy targets with smaller N than non-CoT models.",
            "compositional_performance": "CoT reduces required dataset-size scaling exponent for deeper tasks: e.g., 3-HOP exponent drops from ≈2.58 to ≈1.76 in the authors' measurements; CoT makes scaling exponents for 2/3/5-HOP nearly identical in their experiments.",
            "generalization_gap": "CoT reduces the sample-efficiency gap: fewer examples are needed to reach high ID accuracy, but CoT does not fully eliminate failures arising from path ambiguity.",
            "performance_by_depth": "Authors report that with CoT the exponents across 2/3/5-HOP converge to similar (lower) values, indicating flattened depth effect on Nreq.",
            "performance_by_composition_type": null,
            "has_baseline_comparison": true,
            "baseline_comparisons": "Directly compared standard supervision vs CoT supervision on same tasks and models; CoT yields markedly better sample efficiency.",
            "architectural_comparison": null,
            "scale_effects": "CoT improvements in data efficiency observed across model sizes; the underlying coverage limitations for path-ambiguous tasks remain.",
            "transfer_results": null,
            "key_findings": "Chain-of-Thought supervision 'flattens' multi-hop problems into sequential single-hop predictions, substantially improving sample efficiency (reducing measured power-law exponents), but CoT alone does not resolve path ambiguity where the same variable feeds multiple paths.",
            "failure_analysis": "Although CoT helps form sequential computation behavior, it does not provide cross-context evidence that different first-hop fragments with same b but different x2 should behave identically when path ambiguity exists.",
            "success_conditions": "CoT helps when intermediate states are valid abstractions across contexts (i.e., when coverage evidence exists); it is most effective at reducing compounding data needs for deep chain tasks but requires coverage to generalize systematically.",
            "uuid": "e2023.4"
        },
        {
            "name_short": "Representation & coverage measures",
            "name_full": "Representation clustering (IICG) and causal tracing analyses tied to k-coverage",
            "brief_description": "Analyses and metrics used to connect training data coverage (k-coverage) to internal representation unification and causal importance: Intra-Inter Cosine Gap (IICG) measures clustering by intermediate state, and causal tracing (Indirect Effect) quantifies causal role of activations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2 (trained variants used in experiments)",
            "model_description": "Standard GPT-2 Transformer whose hidden states are probed by IICG and causal tracing interventions.",
            "model_size": "various (68M, 96M, 1.5B)",
            "is_pretrained": false,
            "architectural_features": "standard Transformer; analyses are architectural-agnostic probes into learned representations",
            "task_domain": "applied to synthetic compositional tasks (2-HOP, PARALLEL-2-HOP, 3-HOP, NON-TREE)",
            "task_name": "representation analyses on compositional tasks",
            "task_description": "Probing internal representations to see whether fragments that are functionally equivalent (same intermediate b) are clustered (IICG) and whether intervention/patching on hidden states changes outputs towards corrupted runs (causal tracing/Indirect Effect).",
            "compositional_depth": null,
            "composition_type": "analysis methodology rather than a task",
            "split_type": "applied to ID and OOD groups, and to examples grouped by k-cutoff",
            "training_strategy": null,
            "curriculum_details": null,
            "inoculation_details": null,
            "iid_performance": null,
            "compositional_performance": null,
            "generalization_gap": null,
            "performance_by_depth": null,
            "performance_by_composition_type": null,
            "has_baseline_comparison": false,
            "baseline_comparisons": null,
            "architectural_comparison": null,
            "scale_effects": null,
            "transfer_results": null,
            "key_findings": "When examples fall inside k-coverage, representations of functionally equivalent fragments form tight latent clusters (high IICG); causal tracing shows these clustered states have high Indirect Effect on predictions. OOD examples (outside coverage) show no clustering and are not reliably predicted. Path ambiguity causes context-conditioned clusters (by (b,x2)) instead of unified b clusters.",
            "failure_analysis": "IICG and causal tracing reveal that failures to generalize correspond to absence of representation unification; in path-ambiguous tasks the model forms multiple context-dependent representations instead of a single intermediate abstraction.",
            "success_conditions": "High k evidence in training (multiple distinct shared contexts) correlates with representation unification and causal circuits that support compositional generalization.",
            "uuid": "e2023.5"
        },
        {
            "name_short": "Model-scaling effect",
            "name_full": "Effect of model parameter scaling (GPT-2 68M -&gt; 1.5B)",
            "brief_description": "Empirical assessment of whether increasing Transformer parameter count improves sample efficiency or overcomes coverage limits for compositional generalization; finds little benefit for Type-I coverage-limited generalization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2 (68M, 96M, 1.5B)",
            "model_description": "Same GPT-2-style architectures scaled in depth/heads/hidden size as described in the paper.",
            "model_size": "68M, 96M, 1.5B",
            "is_pretrained": false,
            "architectural_features": "standard Transformer scaled in depth/width; no novel inductive biases introduced",
            "task_domain": "synthetic compositional tasks (2-HOP, PARALLEL-2-HOP, 3-HOP, NON-TREE)",
            "task_name": "scale comparisons on compositional tasks",
            "task_description": "Train identical task setups on increasing model sizes to examine whether parameter scale reduces required dataset size Nreq or improves handling of path ambiguity.",
            "compositional_depth": null,
            "composition_type": null,
            "split_type": "ID / OOD comparisons preserved across scales",
            "training_strategy": "same supervised protocol across model sizes",
            "curriculum_details": null,
            "inoculation_details": null,
            "iid_performance": "All model sizes reach high training accuracy; convergence speed may vary but main effect studied is ID generalization vs dataset size.",
            "compositional_performance": "Power-law exponent for Nreq vs |X| for 2-HOP remains essentially invariant across sizes (e.g., c ≈ 2.13..2.28 across 68M-1.5B reported entries); for NON-TREE scaling to 1.5B did not materially improve ID accuracy under comparable conditions.",
            "generalization_gap": "Model scale does not eliminate the gap induced by lack of coverage or path ambiguity; increasing parameters alone does not substantially lower Nreq exponent.",
            "performance_by_depth": null,
            "performance_by_composition_type": null,
            "has_baseline_comparison": true,
            "baseline_comparisons": "Direct comparisons across three GPT-2 sizes under identical tasks/hyperparameters.",
            "architectural_comparison": null,
            "scale_effects": "Little to no improvement in required dataset-scaling exponent with up to 20x parameter increase; capacity scaling does not substitute for coverage evidence in Type-I generalization.",
            "transfer_results": null,
            "key_findings": "Parameter scaling (68M-&gt;1.5B) does not substantially change the power-law scaling relationship between |X| and required dataset size for compositional generalization and does not solve failures caused by path ambiguity.",
            "failure_analysis": "Model capacity increases fail to resolve structural limits imposed by lack of k-coverage or path ambiguity.",
            "success_conditions": "Architectural or training innovations (CoT, partial supervision, explicit binding mechanisms) rather than mere parameter scaling are needed to overcome coverage-limited systematicity.",
            "uuid": "e2023.6"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models.",
            "rating": 2
        },
        {
            "paper_title": "Grokking of implicit reasoning in transformers: A mechanistic journey to the edge of generalization.",
            "rating": 2
        },
        {
            "paper_title": "Universal transformers",
            "rating": 1
        },
        {
            "paper_title": "The reversal curse: LLMs trained on \"a is b\" fail to learn \"b is a\"",
            "rating": 1
        },
        {
            "paper_title": "Do large language models latently perform multi-hop reasoning?",
            "rating": 2
        }
    ],
    "cost": 0.02281875,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>The Coverage Principle: A Framework for Understanding Compositional Generalization
26 May 2025</p>
<p>Hoyeon Chang 
LG AI Research</p>
<p>Jinho Park 
LG AI Research</p>
<p>Hanseul Cho 
LG AI Research</p>
<p>Sohee Yang 
LG AI Research</p>
<p>Miyoung Ko 
LG AI Research</p>
<p>Hyeonbin Hwang 
LG AI Research</p>
<p>Seungpil Won 
LG AI Research</p>
<p>Dohaeng Lee 
LG AI Research</p>
<p>Youbin Ahn 
LG AI Research</p>
<p>Minjoon Seo minjoon@kaist.ac.kr 
LG AI Research</p>
<p>Ucl 
LG AI Research</p>
<p>The Coverage Principle: A Framework for Understanding Compositional Generalization
26 May 2025778C5F6813EBA0F28866BDDFEC84B2CBarXiv:2505.20278v1[cs.LG]
Large language models excel at pattern matching, yet often fall short in systematic compositional generalization.We propose the coverage principle: a data-centric framework showing that models relying primarily on pattern matching for compositional tasks cannot reliably generalize beyond substituting fragments that yield identical results when used in the same contexts.We demonstrate that this framework has a strong predictive power for the generalization capabilities of Transformers.First, we derive and empirically confirm that the training data required for two-hop generalization grows at least quadratically with the token set size, and the training data efficiency does not improve with 20x parameter scaling.Second, for compositional tasks with path ambiguity where one variable affects the output through multiple computational paths, we show that Transformers learn context-dependent state representations that undermine both performance and interpretability.Third, Chain-of-Thought supervision improves training data efficiency for multi-hop tasks but still struggles with path ambiguity.Finally, we outline a mechanism-based taxonomy that distinguishes three ways neural networks can generalize: structure-based (bounded by coverage), property-based (leveraging algebraic invariances), and shared-operator (through function reuse).This conceptual lens contextualizes our results and highlights where new architectural ideas are needed to achieve systematic compositionally.Overall, the coverage principle provides a unified lens for understanding compositional reasoning, and underscores the need for fundamental architectural or training innovations to achieve truly systematic compositionality. 2 * Equal contribution. 2All code and resources are available at: https://github.com/kaistAI/coverage-principle 3We refer to systematicity as an ability to generalize to unseen outcomes by exploiting their structural relations.For a more thorough discussion, see Fodor and Pylyshyn [9], Marcus[10],and Szabó [11].Preprint.Under review.</p>
<p>Introduction</p>
<p>Human generalization capability is often attributed to our ability to manipulate symbols compositionally [1,2].Despite the remarkable performance of Large Language Models (LLMs) [3][4][5][6][7][8], our understanding of how they represent and manipulate compositional structures is still limited.Meanwhile, a growing body of work points to the brittleness of LLMs in performing systematic 3
(x ′ 1 , x ′ 2 , x3) (x1, x2, x ′ 3 ) (x ′ 1 , x ′ 2 , x ′ 3 ) (x1, x2, x ′′ 3 ) (x ′ 1 , x ′ 2 , x ′′ 3 ) (x1, x ′ 2 , x3) (x ′′ 1 , x ′′ 2 , x3)
Figure 1: Left: In a two-hop task (x 1 , x 2 , x 3 ) → t with t = f 2 (f 1 (x 1 , x 2 ), x 3 ), two fragments (x 1 , x 2 ) and (x ′ 1 , x ′ 2 ) satisfying f 1 (x 1 , x 2 ) = f 1 (x ′ 1 , x ′ 2 ) = b consistently yield the same final output when combined with the same context x 3 , supporting their functional equivalence.Right: Among all possible inputs (few shown), we draw an edge between any two inputs that differ only by functionally equivalent fragments to form a substitution graph.Then, coverage is the set of observed inputs (highlighted as blue) and all inputs connected to them.The coverage principle states that a learner relying on pattern matching can reliably generalize only to inputs that are reachable from the observations by substituting functionally equivalent fragments.</p>
<p>compositional reasoning [12][13][14][15][16][17][18][19][20][21][22][23][24][25].These observations suggest that LLMs succeed primarily through pattern matching, i.e., by exploiting surface-level statistical correlations between input fragments and outputs, rather than systematic compositional reasoning.However, prior analyses of the patternmatching behavior remain largely observational and task-specific, leaving us without a unified framework to predict when they will succeed or fail.</p>
<p>We show that Transformers' [26] generalization on synthetic compositional tasks is predictable through a unified framework grounded in one observation: Transformers generalize by utilizing functionally equivalent fragments supported by input-output relations.This insight allows us to move beyond observing success or failure cases to systematically predicting generalization boundaries.To this end, we first formalize the concept of functional equivalence, where two fragments (i.e., components of input sequences) are regarded as equivalent when they yield identical results in all shared contexts observed in the training data.Then, we introduce coverage, the set of inputs reachable from training data by substituting functionally equivalent fragments.Finally, we propose the coverage principle, which states that coverage is a necessary condition for compositional generalization, i.e., model predictions become unreliable beyond this fundamental boundary, as long as the only generalization mechanism is pattern matching.</p>
<p>From this conceptual formulation, we derive specific predictions about generalization in Transformers.First, we derive and empirically confirm that reliable generalization on a two-hop reasoning task requires training data to scale at least quadratically with the token set size.Remarkably, our experiments suggest that this scaling relationship remains invariant even for 20x larger model, underscoring that the limitation is inherent to data properties rather than model capacity.</p>
<p>Second, our framework also predicts specific complications in tasks with path ambiguities, where the same variable influences outputs through multiple computational paths.We show that Transformers cannot form unified representations of identical intermediate states in such cases, instead developing context-dependent state representations that fragment across different paths.This degrades both performance and interpretability, explaining why seemingly simple compositional tasks can remain challenging even for large models.Third, while Chain-of-Thought (CoT) supervision [27,28] improves data efficiency by 'flattening' tasks into sequences of single-hop problems, our analysis reveals a fundamental limitation: CoTtrained models still cannot resolve path ambiguities without showing nearly all possible in-domain combinations.This finding provides insights into understanding the Transformers' failure modes in complex reasoning tasks with path ambiguities (e.g., planning [29][30][31]).</p>
<p>We place these findings within a broader, mechanism-based taxonomy that distinguishes three routes to generalization: (i) structure-based generalization via functional equivalence (the focus of this paper), (ii) property-based generalization that exploits algebraic invariances of individual primitive functions, and (iii) shared-operator generalization that reuses the same computation across positions.This taxonomy clarifies how certain model behaviours that occur outside the coverage region can Mechanistic interpretability Mechanistic interpretability studies aim to understand how submechanisms implement models' behaviors [61][62][63][64].Recent work traces how Transformer components coordinate on compositional tasks [65][66][67][68][69]. Wang et al. [19] demonstrate that in-domain compositional generalization emerges through grokking, but do not identify necessary data conditions for such emergence.Our data-centric framework complements this optimization-centric view by pinpointing when generalizing circuits can exist: only when training data provides sufficient evidence of functional equivalence, essentially formalizing when the pattern matching mechanisms that Fodor and Pylyshyn [9] critique can succeed or must fail.Our findings also explain why standard interpretability techniques like logit lens [70,71] may fail to identify compositional structure in models trained on tasks with path ambiguities.</p>
<p>A formulation of the coverage principle</p>
<p>We now develop a formal framework of the coverage principle.We first provide an intuitive illustration with a two-hop compositional task, then generalize the intuitions to formalize the coverage principle for more general compositional tasks.</p>
<p>Imagine a learner 4 observing input-output pairs determined by a function f : X 3 → X .The input x = (x 1 , x 2 , x 3 ) ∈ X 3 is a sequence of three discrete tokens and the output is a single token, where each token is sampled from a finite set X . 5Being unknown to the learner, let f be a function that can be factorized as the composition of two primitive functions as f (x) = f 2 (f 1 (x 1 , x 2 ), x 3 ), where f 1 : X 2 → X and f 2 : X 2 → X , as illustrated in Fig. 2a.How can the learner generalize by only seeing the input-output pairs?</p>
<p>Our key intuition is that a learner exploits the compositional structure only when observations show that two fragments behave identically.For instance, assume that f
1 (x 1 , x 2 ) = f 1 (x ′ 1 , x ′ 2 ) = b for (x 1 , x 2 ), (x ′ 1 , x ′ 2 ) ∈ X 2 .
From the underlying structure, these fragments are functionally equivalent, i.e., substituting one for the other never changes the output.If observations consistently support their equivalence, i.e., f (x 1 , x 2 , x 3 ) = f (x ′ 1 , x ′ 2 , x 3 ) for observed x 3 values, this equivalence can be supported (Fig. 1 (Left)).Intuitively, the learner would be able to harness this equivalence to generalize to an unseen input (x ′ 1 , x ′ 2 , x ′′ 3 ), provided the learner has seen f (x 1 , x 2 , x ′′ 3 ).In short, the learner can utilize the observed functional equivalence to correctly infer the output of an unseen input, if it can reach an observed input by safe substitutions supported by observations (Fig. 1 (Right)).Coverage is a set of such inputs that are reachable from an observed input through chains of functionally equivalent substitutions.The essence of our framework, the coverage principle, is that a learner can only generalize inside the coverage when the only source of compositional generalization is the observation and utilization of functional equivalence.</p>
<p>We now formalize these concepts for an arbitrary fixed-length task with an arbitrary set of observations.We restrict our attention to single-token prediction tasks defined as a deterministic mapping f : X n → X , where X is a finite set of tokens.We also consider a fixed observation set D ⊂ X n , a collection of inputs that are allowed to be observed by the learner.Write x = (x 1 , . . ., x n ) ∈ X n and, for a subset I ⊂ [n] := {1, . . ., n}, let x I := (x i ) i∈I be a subsequence of x.The first step is to formalize what it means for two subsequences to be functionally equivalent.Definition 3.1 (Functional k-equivalence).Fix a nonempty proper subset of indices I ⊊ [n].Given a pair of subsequences a, a ′ ∈ X |I| , we say a pair of inputs {x, x ′ } ⊂ D to be an I-co-occurrence of a and a ′ in D if it satisfies x I = a, x ′ I = a ′ , and
x [n]\I = x ′ [n]\I
. Also, we say a and a ′ are functionally k-equivalent at I in D, denoted by a ≡ I D a ′ , if it satisfies:</p>
<ol>
<li>(Sufficiency of co-occurrences.)There are k or more distinct I-co-occurrences of a and a ′ in D;</li>
</ol>
<p>(Consistency.) Every
I-co-occurrence {x, x ′ } of a and a ′ in D satisfies f (x) = f (x ′ ).
In other words, two subsequences are functionally k-equivalent if they behave identically in the same contexts at least k times.The hyperparameter k represents the strength of evidence required to establish functional equivalence between two subsequences.The minimum value k = 1 corresponds to the weakest form of evidence, meaning a single shared context is sufficient to establish equivalence, whereas higher values of k demand more robust evidence.</p>
<p>Next, we ask: which inputs are reachable from observed data utilizing functional equivalence?To formalize this, we define the substitution graph: Let G D,k = (V, E) be an undirected graph with a vertex set V = X n of all possible inputs.Two vertices x, x ′ ∈ V are connected with an edge in E if and only if there exists an index set I ⊂ [n] such that {x, x ′ } is an I-co-occurrence (in V ) of a pair of functionally k-equivalent sequences at I in D. With this substitution graph G D,k , we formally define the k-coverage as a set of inputs which are connected 6 to at least one observed input as follows: Definition 3.2 (k-coverage).The k-coverage of D, denoted by Cover k (D), is the set of all inputs in X n that is connected to an input in D on the substitution graph G D,k .</p>
<p>Note that the notion of coverage is a stricter condition of the conventional definition of in-domain, which is obtained by random train/test split [19] or taking combinations of observed internal computations [17].We also emphasize that coverage is a property of a dataset and is independent of model architectures and learning algorithms.Now, we formally phrase the coverage principle: This establishes a fundamental boundary: no learner can reliably generalize beyond the coverage as long as the only source of the generalization is functional equivalence.In Sec.5.1, we show that Transformers indeed fail to generalize beyond coverage when our random mapping dataset construction isolates functional equivalence as the sole generalization source.However, deeper insights come from asking when generalization success occurs in practice.While theory suggests that deep learning methods implicitly favor simpler solutions [72][73][74], which may involve leveraging functional equivalence, we still lack a systematic picture of how task structure, dataset, and model size interact to determine when such solutions emerge.We study this through experiments in Sec. 5.</p>
<p>Experimental setup
x 1 x 2 f 1 b x 3 f 2 t (a) 2-HOP x 1 x 2 x 3 x 4 f 1 f 2 b 1 b 2 f 3 t (b) PARALLEL 2-HOP x1 x2 f1 b1 x3 f2 b2 x4 f3 t (c) 3-HOP x 1 x 2 f 1 b x 3 f 2 t (d) NON-TREE
Figure 2: Four synthetic compositional structures we study.</p>
<p>Dataset construction</p>
<p>We construct four synthetic compositional tasks: 2-HOP, PARALLEL 2-HOP, 3-HOP, and NON-TREE (Fig. 2).We create random mappings from the product space of a token set to control generalization sources not attributable to compositional structures (i.e., commutativity). 7e explain the dataset construction process using 2-HOP task (Fig. 2a), (x 1 , x 2 , x 3 ) → t with t = f 2 (f 1 (x 1 , x 2 ), x 3 ), as an example.We construct training datasets by defining a token set with size |X |, and creating two random maps for the primitive functions f 1 : X 2 → X and f 2 : X 2 → X .We mark a fraction p seen = 0.</p>
<p>Results</p>
<p>Predictive power of k-coverage on Transformer's compositional generalization</p>
<p>We now empirically validate the predictive power of the coverage principle by quantifying the correlation between k-coverage and model generalization performance.shows that at N = 5k, every ID test example is already covered with minimal evidence (k = 1).</p>
<p>Ideally, a single witness of functional equivalence could suffice.However, experiments show that such minimal coverage alone is insufficient for efficient generalization in practice.To demonstrate this, we define a sample's k-cutoff as the lowest k for which an input lies in k-coverage, measuring the strength of evidence for functional equivalence.For example, a k-cutoff of 3 means that an example is inside coverage with k = 3 but not with k = 4.For 2-HOP dataset with N = 10k, we classify each ID test instance according to its k-cutoff, and track accuracy development for each group across 50k training epochs.As shown in Fig. 3  These results yield two important insights.First, successful generalization in practice requires robust coverage so the model can confidently identify and utilize functional equivalence relationships.The k parameter effectively quantifies this evidence strength, directly impacting generalization speed and reliability.Second, while our experiments use uniformly distributed data, the coverage principle explains why models struggle with generalizing long-tail distribution in imbalanced real-world data [76][77][78].Rare combinations naturally receive limited functional equivalence evidence (low k), placing them effectively outside practical coverage, despite technically being in-distribution.Next, we investigate how the model internally represents functional equivalence for k-covered inputs.Specifically, we inspect a GPT-2 trained on 2-HOP task (|X |= 50, N = 10k) for 50k epochs (corresponding to the yellow line in Fig. 3 (Right)). 8We observe that when a model successfully generalizes to ID test data, it maps functionally equivalent components into tight latent clusters, thereby encoding the equivalence relationships needed for compositional generalization.</p>
<p>The clustering of latent representation drives generalization on coverage</p>
<p>To quantify this representation clustering phenomenon, we develop a metric that captures how distinctly the model separates functionally equivalent fragments from others.Specifically, we measure the difference between the average pairwise cosine similarity of latent vectors that share the same intermediate state b = f 1 (x
(x ′ 1 , x ′ 2 ) whenever f 1 (x 1 , x 2 ) = f 1 (x ′ 1 , x ′ 2 )
. Assuming a learner requires at least k distinct pairs of evidence to establish functional equivalence of two fragments, how does the required dataset size scale with the token set size |X |?In practical terms, this question addresses how much data is fundamentally needed for compositional generalization, which is central to understanding data scaling requirements for compositional tasks.For 2-HOP task, we derive the following result (full derivation in App.E): Result 5.1 (Power-law lower bound).Let f : X 3 → X be a 2-HOP composition.Assume a learner recognizes functional equivalence of two subsequences only after observing them in at least k distinct pairs of evidence (i.e., functionally k-equivalent).theoretical predictions of at least quadratic scaling.Although we derive the theoretical bound only for 2-HOP, we observe clear power-law relationships for more complex structures as well.The higher exponents for PARALLEL-2-HOP (c = 2.43) and 3-HOP (c = 2.58) tasks suggest that extra computational steps essentially add another dimension of relationships that require robust coverage, driving the steeper power-law scaling.</p>
<p>Strikingly, these exponents remain invariant across three different GPT-2 model sizes spanning a 20x range in parameters (from 68M to 1.5B) for all three tasks (Fig. 5 (Right) and Tab. 2 in App.F).This supports that the scaling relationship is primarily determined by data properties rather than model capacity, reinforcing our framework's data-centric perspective.The observed scaling relationships are robust across different hyperparameters (weight decay and learning rate) and empirical decision criteria for Nreq (see App. F).Overall, these results suggest that compositional generalization in Transformers requires dataset sizes scaling as a power-law with token set size.This has significant implications for how we approach complex reasoning tasks, suggesting that data curation strategies informed by coverage considerations may be more effective than raw parameter scaling.</p>
<p>Path ambiguity hinders generalization and interpretability in Transformers Identical Intermediate States</p>
<p>Functional Equivalence
(x1, x2) (x ′ 1 , x2) (x1, x ′ 2 ) (x ′ 1 , x ′ 2 ) (x1, x ′′ 2 ) (x ′ 1 , x ′′ 2 )
5k 10k 15k 20k 25k 30k 35k 40k 45k 50k</p>
<p>Dataset Size(N) Many real-world reasoning tasks involve computational structures where a single variable affects the output through multiple paths, creating what we call path ambiguity.Our framework predicts that in such cases, Transformers struggle with forming unified representations of intermediate states that are theoretically equivalent, instead developing context-dependent state representations that vary based on the input context.In this section, we analyze NON-TREE task (Fig. 2d) as a case study, where x 2 affects the output through two paths: as input to f 1 and directly to f 2 .Following the definition of functional equivalence (Def.3.1), this path ambiguity prevents the model from establishing the functional equivalence of two subsequences (x 1 , x 2 ) and (x ′ 1 , x ′ 2 ) that produce the same intermediate state b, unless they also share the same x 2 value (x 2 = x ′ 2 ).Consequently, the model creates context-dependent state representations rather than unifying them to represent true intermediate state b, as illustrated in Fig. 6 (Left).</p>
<p>In consequence, the path ambiguity hinders both generalization performance and efficiency, as the model must establish functional equivalence for each x 2 -conditioned equivalent pair.Fig. 6 (Middle) shows that GPT-2 can fully generalize on ID test set of 2-HOP task within a reasonable time with increasing data size, but fails with NON-TREE task, even provided with a near-exhaustive amount of possible ID combinations as training data. 11Notably, scaling to 1.5B parameters does not show significant improvement in the performance (Fig. 17  This context-dependence raises an interpretability concern, as standard linear probing-based techniques like logit lens [70,71] would not reliably identify intermediate states.Moreover, our analysis provides insights into why LLMs fall short in planning tasks [29][30][31], as planning often requires correct tracking of intermediate states, which can influence outcomes through multiple paths.CoT supervision [27,28] dramatically improves performance on multi-step reasoning tasks.We investigate how CoT interacts with the coverage principle and whether it can address the challenges observed in Sections 5.3 and 5.4.Specifically, we train models to sequentially generate intermediate states before final outputs, making 2-HOP a two-token prediction task: (x 1 , x 2 , x 3 ) → (b, t), for example.This explicit supervision substantially improves data efficiency (Fig. 7 (Left)), with the power-law exponent dropping from 2.58 to 1.76 in 3-HOP task, aligning with previous studies on the sample efficiency of CoT [79][80][81].Remarkably, the scaling exponents measured for 2-HOP, 3-HOP, and even 5-HOP tasks become nearly identical with CoT supervision.We interpret this as CoT effectively 'flattening' multi-hop structures into sequences of single-hop tasks, reducing the compounding data requirements of deeper compositional structures.</p>
<p>CoT supervision improves data efficiency but still struggles with path ambiguity</p>
<p>However, we find models trained with CoT supervision still struggle with the path ambiguity in NON-TREE task.Despite showing improvements with CoT supervision, the models fail to achieve perfect ID generalization under the same training conditions that yield perfect performance in 2-HOP task (Fig. 7 (Middle).IICG analysis (Right) reveals that the model's representations remain partially context-dependent.For 2-HOP task, the representations cluster purely by intermediate states b, as indicated by the result that IICG measurement with x 2 -conditioned states does not significantly shift the curve.In contrast, the IICG score for NON-TREE task is significantly elevated at every layer with the same conditioning, suggesting the absence of disentangled state representation inside the model.We hypothesize this arises since CoT supervision does not give enough evidence that different (x 1 , x 2 ) pairs sharing the same b should yield identical second-step outputs, as functional equivalence holds only when x 2 = x ′ 2 .Hence, while CoT supervision helps with sequential computation by breaking down multi-hop structures, it may partially inherit the limitations on handling tasks with path ambiguities we describe in Sec.5.4.Our analysis likely explains why LLMs struggle with complex planning tasks even when using CoT techniques and massive training data [82].</p>
<p>A taxonomy for understanding generalization mechanisms</p>
<p>Our coverage analysis reveals fundamental limits of pattern matching for compositional generalization.This raises a broader question: What other mechanisms allow neural networks to go beyond coverage boundaries?We sketch a preliminary taxonomy that distinguishes three complementary mechanisms and offers hypotheses about why models sometimes succeed beyond coverage 12 .</p>
<p>Type-I: Structure-based generalization occurs when models identify and utilize functionally equivalent components based on how primitive functions are composed.This is precisely what we formalize through the coverage principle: models learn that different input fragments yield identical results in shared contexts, enabling generalization to new fragment combinations.Crucially, this generalization remains bounded by coverage, and reliable generalization fails without sufficient functional equivalence evidence.Type-I describes the ceiling of pattern matching without explicit variable binding.</p>
<p>Type-II: Function property-based generalization exploits intrinsic properties of individual primitive functions (e.g., algebraic invariances such as commutativity), group-theoretic structure in modular arithmetic [63], or input irrelevance, where certain arguments never affect the output (e.g., f (x 1 , x 2 ) = f (x 1 ) even when distractor x 2 is present [81]).Unlike Type-I, this can transcend coverage limitations by leveraging properties that hold across all inputs of a primitive.The Reversal Curse exemplifies the layered nature of compositional challenges across multiple generalization types.The coverage principle (Type-I) explains the fundamental failure: training on "A is B" provides no functional equivalence evidence for "B is A −1 ".Property-based approaches like bidirectional training partially succeed [83] by exploiting relationship invertibility (Type-II), using architectural modifications to learn inverse mappings from the same training data.However, recent evidence [42] shows that models still struggle once entities switch syntactic roles, indicating a remaining challenge in variable binding. 13ype-III: Shared-operator generalization emerges through reuse of identical primitive functions across computational positions (e.g., when f 1 = f 2 ).Recurrent architectures [84] exemplify this through weight sharing across time steps, enabling processing of variable-length sequences [85].In Transformers, inductive biases towards reuse of the same computation through parameter sharing [19,86,87] can improve generalization on compositional tasks beyond coverage.We interpret this mechanism as exploiting structural repetition without requiring Type-II's algebraic property understanding.</p>
<p>Distinguishing mechanisms from phenomena Prior categorizations focus on observed phenomena, for example, "systematic" versus "mix-and-match" [12].We instead categorize the underlying mechanisms, as our coverage principle offers quantitative predictions for Type-I boundaries while identifying when Type-II or Type-III mechanisms become necessary.Many influential studies have examined tasks mixing functional equivalence, primitives' intrinsic properties, and operator reuse within the same benchmark, making it difficult to pinpoint the true source of success or failure.We therefore advocate clearer experimental control and community discussion around this mechanistic distinction to sharpen future analyses of neural generalization.</p>
<p>Implications and future directions Real compositional tasks typically involve combinations of all three types, making it difficult to isolate why models succeed or fail.We suggest diagnostic approaches based on the proposed taxonomy: coverage ablation tests for Type-I, algebraic identity probes for Type-II, and parameter-sharing ablations for Type-III.While preliminary, this taxonomy provides a conceptual scaffold for understanding the fundamental capabilities and limitations of neural networks on compositional reasoning.This work isolates and formalizes structure-based generalization (Type-I) to clarify its specific boundaries.When models succeed beyond our coverage predictions, e.g., through ignoring distractors without seeing all combinations or leveraging parameter sharing, we view these as exploiting Type-II or Type-III mechanisms rather than contradicting our framework.Our focus on Type-I suggests that fundamental systematicity challenges remain as long as models rely primarily on pattern matching, requiring architectural innovations that harness all three mechanisms and, crucially, incorporate explicit variable binding.We hope this preliminary taxonomy serves as a conversation starter, and confirming or refuting its utility is an empirical matter that we invite the community to explore.</p>
<p>Discussion</p>
<p>Systematicity without variable binding The coverage principle pinpoints a fundamental question in the systematicity debate: How far can a neural network generalize compositionally when it relies only on pattern matching and has no mechanism for explicit variable binding?By formalizing the exact conditions under which pattern matching succeeds, we draw a principled boundary around what can be learned from input-output pairs alone.This boundary clarifies the limitations that Fodor and Pylyshyn [9] and Marcus [37] highlighted decades ago.Our results therefore complement approaches that encode compositional structure directly, whether through symbolic interfaces, slotbased representations, or neural modules [56][57][58][59][60]88], and delineate what must still be overcome to reach truly systematic generalization.</p>
<p>Practical implications</p>
<p>Our framework provides valuable insights into various observed LLM behaviors.First, it accounts for the data-hungry nature of compositional tasks by demonstrating that robust coverage is required for reliable generalization [43].Next, our framework explains why models struggle with generalizing long-tail knowledge [76][77][78], as low-frequency combinations naturally receive limited functional equivalence evidence.Similarly, failures in complex planning tasks [30,31,82,89], even with CoT techniques, might be attributed to path ambiguities.Moreover, our coverage principle predicts the reversal curse phenomenon [16,42,83], as pattern-matching models fundamentally cannot generalize to reversed relationships without explicit functional equivalence evidence in training data.</p>
<p>Beyond explaining failures, our framework helps understand when and why standard interpretability techniques like linear probing or logit lens can fail.In addition, our analysis suggests targeted data augmentation strategies that maximize coverage by ensuring diverse shared contexts for functionally equivalent components.This can explain the success of strategic data augmentation methods [48,49,90,91] and offers principled guidance.Finally, although our empirical study focuses on GPT-2-style Transformers, the coverage principle itself makes no architectural assumptions, and it applies to any learning system whose generalization mechanism is primarily pattern matching.Extending the analysis to recurrent (e.g., LSTM [84]), state-space [92], or convolutional [93] architectures, and more realistic data where multiple computational structures coexist within the same dataset [12,15,94] remains promising future work.</p>
<p>Conclusion</p>
<p>We introduce the coverage principle, a data-centric framework that specifies when pattern-matching learners can and cannot generalize compositionally.Our theoretical analysis and controlled experiments show that a Transformer's success is tightly predicted by coverage: two-hop tasks obey a superquadratic data scaling law that does not improve with 20x parameter scaling, path ambiguity fragments internal representations, and CoT supervision helps only inside the coverage boundary.</p>
<p>These results isolate Type I structure-based generalization and reveal its limits.Our taxonomy then situates two additional mechanisms, Type II property exploitation and Type III shared-operator reuse, that help explain coverage "violations" reported in the literature.Yet all three mechanisms may still rely on sophisticated pattern matching rather than explicit variable binding, leaving the fundamental systematicity challenge unresolved.</p>
<p>Hence, the systematicity challenge posed by Fodor and Pylyshyn [9] and by Marcus [37] remains open.Matching human-like compositionality will likely require architectures that can bind and manipulate symbols independently of surface form, not merely scale up or refine current patternmatching strategies.We hope the coverage principle and the accompanying taxonomy provide a clear target and a roadmap for the next stage of innovation.We now provide detailed information about our dataset construction process.While we primarily explain this process for the 2-HOP task, we follow similar procedures for the other compositional structures.</p>
<p>Contents of the Appendix</p>
<p>Vocabulary and Token Representation For a task with token set size |X |, we create |X | special tokens of the form <t_0>, <t_1>, . .., <t_(|X |−1)>, which we append to the standard GPT-2 vocabulary.We also add special tokens </a> to mark the end of sequences.For Chain-of-Thought (CoT) experiments, intermediate computations are represented in the target sequence as the actual intermediate token.</p>
<p>Function Construction For the 2-HOP task, we construct two primitive functions f 1 : X 2 → X and f 2 : X 2 → X by randomly mapping from their respective domains to the codomain X .We create the domain by taking the Cartesian product of the token set with itself.For each function, we randomly designate a fraction p seen = 0.7 of its domain as the "seen" portion, resulting in sets S f1 and S f2 .</p>
<p>Dataset Generation Algorithm To generate the training dataset, we first identify all possible combinations where both primitive operations come from their respective "seen" domains.Specifically, we find all valid tuples (x 1 , x 2 , x 3 , t) such that:
(x 1 , x 2 ) ∈ domain(S f1 ) (1) (f 1 (x 1 , x 2 ), x 3 ) ∈ domain(S f2 ) (2) t = f 2 (f 1 (x 1 , x 2 ), x 3 )(3)
From this set of all possible in-domain combinations, we uniformly sample N examples to form our training dataset.When the number of possible combinations exceeds N , this sampling ensures the model sees only a subset of possible in-domain combinations.</p>
<p>Test Set Construction</p>
<p>We carefully construct test sets to evaluate the model's generalization capabilities across different coverage conditions.Our test sets contain:</p>
<p>• In-Domain (ID) Test Set: Combinations not seen during training but where both primitive operations were observed in other contexts.These examples may lie within the coverage as defined by our framework.</p>
<p>• Out-of-Domain (OOD) Test Set: Examples where at least one primitive operation was never observed in training.These fall outside the coverage boundary.</p>
<p>Input-Output Format</p>
<p>The dataset is formatted for auto-regressive token prediction.For the standard 2-HOP task, inputs comprise three tokens representing x 1 , x 2 , and x 3 , while the target includes these input tokens followed by the prediction t and an end marker.Below are the examples of the dataset format for different settings.</p>
<p>• Standard Format:</p>
<p>-Input: <t_5><t_12><t_3> -Target Completion: <t_17></a> -The model must predict the final output token followed by the end marker.</p>
<p>• Chain-of-Thought Format:</p>
<p>-Input: <t_5><t_12><t_3> -Target Completion: <t_9><t_17></a> -The model must first predict the intermediate computation result <t_9> (where <t_9> = f 1 (<t_5>, <t_12>)), followed by the final output.</p>
<p>• Partial Computation Format (f 1 ):</p>
<p>-Input: <t_5><t_12> -Target Completion: <t_9></a> -These examples represent the primitive function applications used to construct the full compositional task.</p>
<p>For the other compositional tasks, we follow analogous construction procedures, adjusting the number of input tokens and the composition structure based on the specific task's requirements.For example, PARALLEL 2-HOP requires four input tokens, while 3-HOP follows a three-step composition chain requiring appropriate modifications to the function construction and sampling procedures.For our experiments, we employ three GPT-2 model variants of increasing size: GPT-2-Small (68M parameters), GPT-2 (96M parameters), and GPT-2-XL (1.5B parameters).As shown in Tab. 1, GPT-2-Small consists of 4 layers with 6 attention heads and a hidden dimension of 768.The standard GPT-2 configuration used in most experiments features 8 layers with 12 attention heads while maintaining the same hidden dimension of 768.Our largest model, GPT-2-XL, significantly scales up the architecture with 48 layers, 25 attention heads, and an increased hidden dimension of 1600.The implementation follows the codebase from [19].</p>
<p>B.2 Training details</p>
<p>We train all models using the AdamW optimizer with beta values of (0.9, 0.999) and epsilon of 1e-8.We set the learning rate to 8e-4 with a weight decay of 0.1.A batch size of 16,384 is used, with full gradient descent applied for datasets smaller than the batch size.All training is conducted with mixed precision (fp16) on 4 NVIDIA A100 GPUs with 80GB memory each.We employ a constant learning rate schedule with a linear warmup period of 2,000 steps.This standardized training configuration is maintained across all experiments to ensure fair comparisons between different task structures and dataset sizes, unless explicitly varied in specific ablation studies.Stage 2: Equivalence class construction For each subset of indices I, we build equivalence classes of subsequences that exhibit functionally identical behavior.Two subsequences are considered equivalent only if: (1) they share at least k distinct complements where they produce the same output, and (2) they never produce different outputs when given the same complement (no contradictions).We use a Union-Find data structure to efficiently track and merge these equivalence classes.The Union-Find (or Disjoint-Set) data structure efficiently maintains a collection of disjoint sets, supporting two key operations: (1) Find -determine which set an element belongs to, and (2) Union -merge two sets.</p>
<p>C Implementation details for the coverage determination algorithm</p>
<p>Stage 3: Substitution Graph Construction</p>
<p>We construct a graph where nodes represent input sequences from our training and test sets, rather than the entire domain space (which would be computationally prohibitive for large token sets).We add an edge between two inputs x and y if and only if: (1) they produce the same output, (2) they differ only in one subsequence position set I, and (3) their differing subsequences belong to the same equivalence class.This graph represents the space of safe substitutions where one can replace a subsequence with a functionally equivalent alternative without changing the expected output.Our implementation uses parallel processing to efficiently construct this graph even for large datasets.</p>
<p>Stage 4: Coverage computation Finally, we compute the coverage set by taking the union of all connected components in the substitution graph that contain at least one training example.This set comprises all inputs that are reachable from the training data through chains of equivalent subsequence substitutions.</p>
<p>D Detailed analysis for representation unification experiments D.1 Causal Tracing Methodology</p>
<p>To analyze the causal role of specific hidden representations in our Transformer model, we employ causal tracing, a technique that measures the effect of intervening on intermediate activations during inference [66,67].Specifically, we measure the causal effect using the indirect effect metric defined in [95].This methodology allows us to identify which components and positions in the model most strongly contribute to compositional generalization.We illustrate the measurement with 2-HOP task.</p>
<p>Our analysis begins by collecting three types of computational traces:</p>
<ol>
<li>Clean run (G): We run the model on a compositional task with input (x 1 , x 2 , x 3 ) where the corresponding output is t = f 2 (f 1 (x 1 , x 2 ), x 3 ).</li>
</ol>
<p>Corrupted run (G * ):</p>
<p>We replace the original input with a corrupted version by changing the first two tokens
(x 1 , x 2 ) to (x ′ 1 , x ′ 2 ), where f 1 (x ′ 1 , x ′ 2 ) ̸ = f 1 (x 1 , x 2 )
. This ensures that the model produces a different final output t * ̸ = t.During this run, we cache all hidden states h * (ℓ) i for each token position i and layer ℓ.</p>
<p>Patched run (G[← h * (ℓ)</p>
<p>i ]): We run the model on the input from the clean run, but at a specific token position i and layer ℓ, we replace the hidden state with the corresponding state from the corrupted run.</p>
<p>To quantify the causal effect of a specific hidden state h (ℓ) i on the model's prediction, we measure the Indirect Effect (IE):
IE h (ℓ) i = p<a href="t *">← h * (ℓ) i </a> − p(t * ) p * (t * ) − p(t * )(4)
where:</p>
<p>• p(t * ) is the probability assigned to the corrupted output t * in the clean run G • p * (t * ) is the probability assigned to the corrupted output t * in the corrupted run G *</p>
<p>• p<a href="t *">← h * (ℓ) i </a> is the probability assigned to the corrupted output t * in the patched run G[← h *  (ℓ)  i ] This metric quantifies how much corruption in a particular state affects the overall outcome.An IE value close to 1 indicates that the corruption of the state h
(ℓ) i to h * (ℓ)
i alone almost completely changes the prediction to that of the corrupted run, suggesting that this state is causally important for the computation.Conversely, an IE value close to 0 indicates that the state has minimal causal impact on the prediction.</p>
<p>In our experiments, we apply causal tracing to analyze different subsets of test data categorized by their k-cutoff values, where k represents the minimum evidence threshold required for functional equivalence (as defined in Sec. 3 of the main text).This allows us to correlate the strength of functional equivalence evidence with the formation of unified internal representations.</p>
<p>D.2 Causal tracing results for each k-cutoff value in 2-HOP task</p>
<p>Figure 8 displays the causal tracing results for the 2-HOP task, broken down by different k-cutoff values.We observe that the causal patterns are similar across different k-cutoff values, with slight differences in where and how strongly the causal effects manifest in the model.This suggests that once an example falls within coverage (even with minimal evidence, k = 1), the model forms internal representations that play similar causal roles in prediction.</p>
<p>D.3 Token set size ablation</p>
<p>We show that the observed patterns of cosine similarity analysis and causal tracing in</p>
<p>E Derivation of Result 5.1 (power-law lower bound)</p>
<p>Problem setting Let X be a finite token set with cardinality |X |.The target mapping Functional k-equivalence and learner model For two first-hop fragments a, a ′ ∈ X 2 define
f : X 3 −→ X , f (x 1 , x 2 , x 3 ) = f 2 (f 1 (x 1 , x 2 ), x 3 ) is a two-hop composition of unknown primitives f 1 : X 2 → X and f 2 : X 2 → X . Write b = f 1 (x 1 , x 2 ) ∈ Xa ∼ a ′ :⇐⇒ f 1 (a) = f 1 (a ′ ).
They are functionally k-equivalent w.r.t.D (denoted a ≡ k D a ′ ) when there exist k distinct contexts c 1 , . . ., c k ∈ X such that for every r ≤ k both (a, c r ) and (a ′ , c r ) appear in D and f (a, c r ) = f (a ′ , c r ).We call each (a, c r ), (a ′ , c r ) an evidence pair.</p>
<p>The coverage principle by itself is only a necessity statement: outside the k-coverage region a purely pattern matching learner's predictions are unconstrained.To convert this into a sufficient data condition we adopt an explicit inductive bias, matching the premise of Result 5.1:</p>
<p>Learner assumption: Whenever two fragments become linked by k independent (i.e., pairwise from distinct contexts) evidence pairs, the learner treats them as functionally equivalent; the learner also propagates equivalence transitively along chains of such links.</p>
<p>With this rule in place the relevant structure inside each class E b is the k-evidence graph: vertices are the |X | first-hop pairs in the class and an edge connects two vertices whenever the pair is observed at least k times in shared contexts.If that graph is connected (every vertex reachable from every other), then every fragment in E b is linked by a chain of k-evidence steps and is therefore recognized as equivalent by the learner.Hence Data sufficiency criterion: If the k-evidence graph of each class E b is connected, the learner generalizes perfectly to all in-domain (ID) inputs.This criterion requires connectivity, and we aim to derive the condition to yield the connectivity with high probability using Erdős-Rényi model [96,97].The minimal dataset size achieving this with high probability is denoted N req (|X |, k).</p>
<p>Note that the k-evidence graph on E b is the restriction of the substitution graph G D,k (Def.3.2) to the vertex set E b × {x 3 } for any fixed x 3 .Connectivity of every class therefore implies that every first-hop fragment lies in the same connected component as some training input, i.e., the entire in-domain set is contained in k-coverage.Under the learner assumption, this is both necessary and sufficient for perfect ID generalization, yielding Res. 5.1.</p>
<p>E.1 Step 1: probability of a single evidence pair</p>
<p>Evidence pair probability Fix two distinct first-hop fragments i, j ∈ E b and a context c ∈ X .We want to find p 1 , the probability that context c provides an evidence pair for the functional equivalence of fragments i and j:
p 1 := Pr[(i, c) ∈ D and (j, c) ∈ D]
Let q := 1/|X | 3 denote the probability of drawing any specific triple in a single draw.Using the inclusion-exclusion principle: Negatively correlated counts and an i.i.d.surrogate Because all N draws come from a single multinomial (N, 1/|X | 3 ), the indicators {Z ij (c)} c∈X are negatively correlated: drawing many triples with one context leaves fewer draws for the others.Negative correlation decreases the probability that several Z ij (c) equal 1 simultaneously.Hence the tail probability for the true count
Y ij := c∈X Z ij (c)
is upper-bounded by the tail of an i.i.d.binomial variable with the same single-trial success probability p 1 .Concretely, define
Y ⋆ ij ∼ Binom(|X |, p 1 ), p 1 = N 2 |X | 6 (1 + o(1)).
Then for every real t
Pr[Y ij ≥ t] ≤ Pr[Y ⋆ ij ≥ t]. Using Y ⋆
ij therefore overestimates the chance of obtaining k or more distinct evidence contexts, which is conservative for our goal of deriving a lower bound on the required dataset size N .</p>
<p>Since Y ⋆ ij is binomial with mean
µ := E[Y ⋆ ij ] = |X | p 1 = N 2 |X | 5 (1 + o(1)),(S2.2)
we may work henceforth with Y ⋆ ij alone; the resulting bounds apply verbatim to the original Y ij .</p>
<p>Poisson tail via Le Cam Le Cam's theorem [98] states that the total-variation distance between Binom(n, p) and Poisson(µ = np) is at most 2np 2 .At the scaling that will emerge in Step 3
(N = |X | 2.5− 0.5 k ), one has 2|X |p 2 1 = 2|X | −1− 2 k → 0 and µ = |X | − 1 k → 0. Thus Pr[Y ij ≥ k] ≤ Pr[Y ⋆ ij ≥ k] = ∞ r=k e −µ µ r r! ≤ µ k k! (1 + o(1)). (S2.3)
This upper bound p k := µ k /k! (1 + o(1)) will be the edge probability used in the connectivity threshold of Step 3.</p>
<p>E.3 Step 3: connectivity inside each equivalence class</p>
<p>With the edge probability p k from Step 2, we now analyze when the k-evidence graphs become connected.Recall that under our learner assumption, perfect generalization requires every equivalence class to form a connected component in the k-evidence graph.We model this as a random graph connectivity problem.</p>
<p>Random graph construction
p k = Pr[Y ij ≥ k] = µ k k! , µ = N 2 |X | 5 . (S3.1)
Why G b can be approximated as Erdős-Rényi As mentioned earlier, the dependence among distinct edges in G b arises from the constraint that the total sample size is N , which induces a negative correlation.Such negative dependence reduces the likelihood of simultaneously creating many edges.Consequently, viewing G b as an independent Erdős-Rényi graph G(n, p k ) provides a conservative model, and any threshold we derive for connectivity under independence remains valid (or becomes easier to satisfy).</p>
<p>Classwise connectivity threshold For Erdős-Rényi graphs, the classical result of Erdős-Rényi [97] states
Pr[G(n, p) is connected] − −−− → n→∞ 1 ⇐⇒ p ≥ log n + ω(1) n .
Setting n = |X | and p = p k yields the requirement
p k = µ k k! ≥ log|X | |X | (1 + o(1)).
Substituting µ = N 2 /|X | 5 and rearranging gives
N 2k |X | 5k k! ≳ log|X | |X | , =⇒ N ≳ |X | 5k−1 2k (k! log|X |) 1/(2k) . Because (k! log|X |) 1/(2k) = (log|X |) O(1/k)
grows only poly-logarithmically, we hide it inside a Ω( • ):
N = Ω(|X | 2.5− 0.5 k ).
Since every class must be connected to achieve the data-sufficiency criterion in the problem setting, we conclude
N req (|X |, k) = Ω(|X | 2.5− 0.5 k ) (with high probability).
As a remark, we note that Erdős-Rényi theory also shows that if
N ≤ |X | 2.5− 0.5 k /(log|X |) 1/(2k) then each G b is disconnected with high probability, so the exponent 2.5 − 0.5
k is in fact sharp (up to poly-logarithmic factors).</p>
<p>F Additional results for power-law scaling analysis F.1 Measurement protocol for N req</p>
<p>To empirically determine the minimum dataset size required for reliable compositional generalization (N req ), we develop a measurement protocol that accounts for practical computational constraints while ensuring robustness.For each token set size |X | and task structure, we test multiple dataset sizes until we identify the threshold point where the model successfully generalizes to the ID test set.Specifically, our criterion for "reliable generalization" on ID is defined as:</p>
<p>• The model must reach ID test accuracy of 0.99 within 100 epochs after achieving training accuracy &gt; 0.99.</p>
<p>This protocol balances several considerations:  Table 2 presents the power-law exponents obtained by linear fitting log(|X |) vs. log(N req ) plots, all with R 2 &gt; 0.99.The consistency of exponents across model sizes suggests that the observed power-law scaling relates to properties of the compositional tasks themselves, rather than model capacity.This observation aligns with our theoretical derivation in Section 5.1, which predicts that the required dataset size scales at least quadratically with token set size.</p>
<p>F.3 Robustness to hyperparameter variations</p>
<p>To verify that our observed power-law scaling relationship is not an artifact of specific hyperparameter choices, we conduct ablation studies with modified training configurations.Figure 15 demonstrates that for the 2-HOP task with |X |= 50, the following changes did not significantly affect the measured N req or the derived power-law exponent:</p>
<ol>
<li>Learning rate reduction: Halving the learning rate from 8e-4 to 4e-4  This robustness to hyperparameter variations suggests that the power-law relationship between token set size and required dataset size is primarily a property of the compositional generalization process, rather than an artifact of specific optimization settings.</li>
</ol>
<p>G Detailed analysis for NON-TREE task</p>
<p>This section provides additional analyses that support our findings in Sec.5.4 regarding the challenges of path ambiguity in the NON-TREE task.</p>
<p>G.1 Coverage analysis   Fig. 17 shows that scaling up the model size to GPT-2-XL (1.5B parameters) does not significantly improve generalization performance on the NON-TREE task, even when measured 100 epochs after reaching training accuracy &gt; 0.99.This suggests that the challenges posed by path ambiguity cannot be overcome simply by increasing model capacity, supporting our claim that the limitation is structural rather than related to model capacity.This analysis demonstrates that even models achieving high accuracy on NON-TREE tasks do so by developing context-dependent representations rather than unified abstractions of intermediate states.The model forms separate computational pathways conditioned on the x 2 value, rather than learning a single unified representation of the intermediate state b = f 1 (x 1 , x 2 ).This represents a fundamentally different solution strategy compared to the 2-HOP task, with implications for both generalization capability and interpretability.</p>
<p>G.3 Representation analysis in successful generalization</p>
<p>H Detailed discussion on the taxonomy of generalization</p>
<p>In Section 7, we introduce a taxonomy of generalization that extends beyond the coverage principle.We propose this taxonomy as an initial conceptual framework to disentangle different mechanisms of generalization that are often conflated in the literature.This categorization offers hypotheses for why neural networks succeed or fail at compositional tasks, connecting empirical observations to theoretical principles.We emphasize that the taxonomy is preliminary and that App.H.5 outlines its current limits, open questions, and future directions.In Type-I generalization, a model learns to recognize when different input fragments yield identical results in shared contexts, allowing it to generalize to new combinations of these fragments.However, without exposure to sufficient functional equivalence evidence during training, the model cannot reliably generalize to novel compositions.</p>
<p>Type-I generalization effectively describes what a pattern matching learner can achieve through the functional equivalence observed by input-output relationships, without requiring abstract variable binding mechanisms.This limitation aligns with the classic argument by Fodor and Pylyshyn [9] and Marcus [37] that connectionist networks lacking explicit variable binding capabilities14 cannot exhibit fully systematic generalization.We introduce this perspective as a conceptual framing of the coverage principle: it describes the ceiling of pattern-based generalization in neural networks that rely on pattern matching rather than symbolic variable binding.</p>
<p>H.2 Type-II: Function property-based generalization</p>
<p>Type-II generalization occurs when a model exploits specific properties intrinsic to individual primitive functions, enabling generalization beyond patterns observed during training.Unlike Type-I generalization which requires explicit evidence of functional equivalence, Type-II generalization leverages invariant properties that hold for all inputs of the primitive, making the function's output fully determined by those invariances even in unobserved contexts.</p>
<p>We identify several sub-families of property-based generalization:</p>
<p>• Algebraic invariances: commutativity, associativity, and similar properties • Group-theoretic structure: as in Fourier basis representations for modular arithmetic [63] • Input irrelevance: attention sparsity that ignores distractors [81] The reversal curse [16] exemplifies the layered nature of compositional challenges across multiple generalization types.The coverage principle (Type-I) explains the fundamental failure: training on "A is B" provides no functional equivalence evidence for "B is A −1 ".Property-based approaches like bidirectional training [83] partially succeed by exploiting relationship invertibility (Type-II), using architectural modifications to learn inverse mappings from the same training data.However, recent work [42] reveals deeper representational binding constraints: even though models can exploit inverse mapping properties in idealized conditions (abstract concept-level tasks), they struggle to maintain consistent concept representations when moving to realistic surface-level predictions, where entities switch roles between subject and object positions.This demonstrates how real compositional tasks involve multiple mechanism layers, where achieving one capability may be insufficient due to representational binding limitations.</p>
<p>Type-II generalization fundamentally differs from Type-I in that it can, in principle, transcend the limitations imposed by the coverage principle, allowing models to generalize correctly to inputs that lie outside the coverage of their training data by leveraging invariant properties of the underlying functions.Systematic characterization of pure Type-II extrapolation remains limited and is an important direction for future work.</p>
<p>H.3 Type-III: Shared-operator generalization</p>
<p>Type-III generalization emerges through reuse of identical primitive functions across different computational positions, e.g., when f 1 = f 2 in a multi-hop reasoning task.Unlike Type-I (bounded by coverage) and Type-II (exploiting algebraic properties), Type-III generalization leverages the structural repetition of identical operations.</p>
<p>This mechanism is not unique to Transformers.Recurrent architectures [84] exemplify Type-III generalization through their recurrent weight sharing: the same transformation is applied at each time step, enabling the network to process sequences of varying lengths despite training on fixed-length examples.This architectural inductive bias directly encodes the assumption that the same operation should apply across temporal positions, allowing recurrent architectures to extrapolate to longer sequences [85].</p>
<p>In the Transformer context, Wang et al. [19] demonstrate that Universal Transformer layers [86] enable generalization beyond in-domain data in two-hop tasks, while [87] show improved compositional reasoning with weight-shared layers.These findings suggest parameter sharing as a key enabling mechanism, where a single learned implementation of a function can be applied across multiple contexts.</p>
<p>A prominent manifestation of Type-III generalization appears in depth extrapolation and length generalization tasks, where models generalize to deeper or longer compositional structures than seen during training [99][100][101][102].While length generalization tasks fundamentally involve applying the same computation to longer inputs (Type-III), successful generalization often requires additional mechanisms like specialized positional encodings that facilitate computation reuse across positions.The observed brittleness of current approaches [103] suggests that such inductive biases are necessary but may not be sufficient for robust systematicity.</p>
<p>In summary, Type-III generalization extends beyond the coverage-bound limitations of Type-I by exploiting identical operations, without requiring the algebraic property understanding characteristic of Type-II.Type-III therefore complements Types I and II, but may still operate without explicit variable binding.</p>
<p>H.4 Relationship to prior taxonomies of generalization</p>
<p>Prior taxonomies of generalization in neural networks have primarily categorized the phenomena of compositional generalization rather than the underlying mechanisms.Lake and Baroni [12] distinguish between "mix-and-match" generalization (for small training-test differences) and systematic compositional generalization (requiring abstract rule application).Other frameworks focus on empirical evaluations through benchmarks [23] or decomposing compositional abilities into constituent skills [38].</p>
<p>Our taxonomy differs by categorizing the mechanisms that enable generalization rather than categorizing based on the phenomena.By distinguishing between structure-based pattern matching (Type-I), algebraic property exploitation (Type-II), and shared-operator transfer (Type-III), our framework provides a lens for interpreting why models succeed or fail at certain compositional tasks.This mechanistic approach not only explains observed behaviors but also offers hypotheses about when Types II or III become necessary for certain generalization challenges.</p>
<p>H.5 Open questions and future directions</p>
<p>To operationalize this taxonomy, future work could develop diagnostics: coverage ablation tests for Type-I (systematically removing training examples to test coverage boundaries), algebraic identity probes for Type-II (testing whether models recognize properties like commutativity without explicit training), and parameter-sharing ablations for Type-III (comparing shared vs. independent weights across positions).Current examples in this paper are illustrative rather than definitive classifications; real tasks often involve combinations of mechanisms, and our taxonomy provides a scaffold for disentangling them rather than enforcing mutually exclusive categories.</p>
<p>Several fundamental questions remain.First, practical computations often involve mixed ingredients of generalization, spanning multiple types.When classification is ambiguous, we suggest identifying the dominant mix of types that contribute, as most realistic tasks involve combinations rather than pure instances of any single type.Second, identifying the true compositional structure underlying a task can be ambiguous, as Locatello et al. [52] demonstrate regarding multiple valid decompositions.Third, real-world datasets contain heterogeneous computational structures, and it remains unclear how neural networks accommodate this diversity.Fourth, none of the three mechanisms solves the variable binding problem, and devising diagnostics and architectures for binding remains crucial.</p>
<p>Future work could explore connections to complementary frameworks:</p>
<p>• Minimum Description Length [73]: Type-II may compress regularities that lower description length.Similarly, Type-III can be viewed as compression through parameter reuse.</p>
<p>• Formal language learning [104,105]: Connections to grammatical inference and automata learning • Cognitive science [106]: Human compositional learning strategies that may inspire architectural innovations This taxonomy is not intended as a complete classification but rather as a conceptual framework to organize our understanding of generalization mechanisms and stimulate further research into the fundamental capabilities and limitations of neural networks on compositional tasks.Ultimately, developing architectural and optimization strategies that enable neural networks to leverage all three types more robustly and ultimately addressing the variable binding problem represents an important frontier for addressing the longstanding challenge of systematicity in connectionist systems.</p>
<p>I Partial computation observation drives the alignment of functional equivalence representation and vocabulary space</p>
<p>In this section, we investigate how exposure to partial computations affects the interpretability of  This finding has important implications for mechanistic interpretability research: the absence of interpretable representations through logit lens does not indicate the absence of structured internal computation.Furthermore, it suggests that interpretability techniques may need to account for how training data shapes the alignment between internal representations and vocabulary space, rather than assuming such alignment emerges naturally from task performance.</p>
<p>Claim 3 . 3 (
33
Coverage principle).Let a learner output f that is consistent with the training set D. If generalization is derived solely from observed functional k-equivalences and x / ∈ Cover k (D), the value f (x) is unconstrained by D.</p>
<p>Figure 4 :
4
Figure 4: Left: Heatmap of Intra-Inter Cosine Gap (IICG) across layers and positions, sliced by k-cutoff.Higher IICG values indicate stronger clustering of representations that share the same intermediate state.The positions with the highest IICG values are marked with squares.Right: PCA visualization of latent representations at position x 2 and layer 3. Datapoints are classified by their intermediate states b = f 1 (x 1 , x 2 ).</p>
<p>9
9
Let N req (|X |, k) be the smallest training dataset size that enables complete generalization to ID examples under this evidence threshold.Then, up to poly-logarithmic factors in |X |, N req (|X |, k) = Ω(|X | α(k) ), where α(k) = 2.5 − 0.5 k .Res. 5.1 predicts that a learner relying on pattern matching requires training dataset size scaling at least quadratically with respect to the token set size, to fully generalize on ID test data.To empirically confirm this, we define a practical threshold Nreq to estimate N req (|X |, k), as a minimal amount of training data required to exceed ID accuracy of 0.99 within 100 epochs after reaching the same level on training data. 10Fig. 5 (Left) shows the measured power-law exponents for Nreq vs. |X | across different task structures.The measured exponent for 2-HOP (c = 2.26) aligns well with our</p>
<p>Figure 6 :
6
Figure 6: Left: In NON-TREE task, the coverage principle predicts that representations of input subsequences with the same intermediate state b = f 1 (x 1 , x 2 ) split into multiple context-dependent state representations, conditioned on x 2 value.Middle: ID test accuracy after standard training (100 epochs post training accuracy&gt;0.99),showing NON-TREE task significantly underperforms 2-HOP task (|X |= 50).Right: IICG heatmap from a model that achieved near-perfect ID accuracy (0.96) after extended training (36k epochs, |X |= 50, N = 50k).</p>
<p>in App.G).Extremely prolonged training (36k epochs) with near-exhaustive ID combinations eventually achieves ID accuracy of 0.96.However, IICG analysis reveals no evidence of a unified intermediate state representation formation, with near-zero IICG scores when grouping by the intermediate state value b (Fig. 6 (Right)).In contrast, grouping by x 2 -conditioned intermediate state ((b, x 2 )) leads to high IICG scores, suggesting the formation of context-dependent state representations.</p>
<p>(b, x2) Non-tree (b) Non-tree (b, x2)</p>
<p>Figure 7 :
7
Figure 7: Left: Power-law scaling of required dataset size vs. token set size for tasks with CoT supervision.R 2 &gt; 0.98 for all linear fits.Middle: Comparison of ID test Accuracy of NON-TREE task (|X |= 50) with and without CoT supervision (100 epochs post training accuracy&gt;0.99).Right: IICG score comparison for NON-TREE and 2-HOP task with CoT supervision (|X |= 50, N = 10k).The scores are measured at each layer of intermediate state position b, based on two grouping strategies: b and (b, x 2 ).Models are trained for 100 epochs after reaching training accuracy&gt;0.99.</p>
<p>A Limitations 23 B Detailed experimental setup 24 B. 1 25 C 26 D Detailed analysis for representation unification experiments 28 D. 1 28 D. 2 28 D. 3 30 E 32 E. 1 32 E. 2 33 E. 3 34 F 36 F. 1 36 F. 2 36 F. 3 36 G 38 G. 1 38 G. 2 38 G. 3 39 H 40 H. 1 40 H. 2 40 H. 3 41 H. 4 41 H. 5 41 I 43 B
232412526281282283303213223333436136236336381382383394014024034144154143
Dataset construction details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .24 B.2 Training details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .Implementation details for the coverage determination algorithm Causal Tracing Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . .Causal tracing results for each k-cutoff value in 2-HOP task . . . . . . . . . . . .Token set size ablation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .28 D.4 Task ablation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .Derivation of Result 5.1 (power-law lower bound) Step 1: probability of a single evidence pair . . . . . . . . . . . . . . . . . . . . .Step 2: probability of observing k evidences for one fixed pair . . . . . . . . . . .Step 3: connectivity inside each equivalence class . . . . . . . . . . . . . . . . . .Additional results for power-law scaling analysis Measurement protocol for N req . . . . . . . . . . . . . . . . . . . . . . . . . . . .Measured power-law scaling constants across task structures and model sizes . . .Robustness to hyperparameter variations . . . . . . . . . . . . . . . . . . . . . . .Detailed analysis for NON-TREE task Coverage analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .Effect of model scaling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .Representation analysis in successful generalization . . . . . . . . . . . . . . . . .Detailed discussion on the taxonomy of generalization Type-I: Structure-based generalization . . . . . . . . . . . . . . . . . . . . . . . .Type-II: Function property-based generalization . . . . . . . . . . . . . . . . . . .Type-III: Shared-operator generalization . . . . . . . . . . . . . . . . . . . . . . .Relationship to prior taxonomies of generalization . . . . . . . . . . . . . . . . . .Open questions and future directions . . . . . . . . . . . . . . . . . . . . . . . . .Partial computation observation drives the alignment of functional equivalence representation and vocabulary space Detailed experimental setup B.1 Dataset construction details</p>
<p>Algorithm 1 :
1
k-Coverage Determination Algorithm Input: Training examples D = {(x i , f (x i ))}, where x i ∈ X n and f (x i ) ∈ X Minimum evidence threshold k ≥ 1 Output: Coverage set Cover(D) /<em> STEP 1: Build behavior maps for each subsequence pattern </em>/ foreach subset I ⊂ [n], I ̸ = ∅, I ̸ = [n] do Behavior I ← map from subsequence x I to the mapping {x [n]\I → f (x) | x ∈ D} end /<em> STEP 2: Identify functionally equivalent subsequences </em>/ foreach subset I ⊂ [n], I ̸ = ∅, I ̸ = [n] do U F I ← new UnionFind() foreach pair of subsequences (α, β) in Behavior I do SharedComplements ← complements observed with both α and β if No contradictions in SharedComplements and matching evidence ≥ k then U F I .Union(α, β) ; // Mark as functionally equivalent end end EquivClasses I ← U F I end /<em> STEP 3: Build substitution graph </em>/ G ← empty graph with nodes for all x ∈ X n foreach pair of inputs (x, y) with f (x) = f (y) do foreach subset I where x and y differ only on indices in I do if EquivClasses I .Find(x I ) = EquivClasses I .Find(y I ) then Add edge (x, y) to G break end end end /<em> STEP 4: Determine coverage </em>/ Cover(D) ← x∈D ConnectedComponent(G, x) return Cover(D) Algorithm 1 presents our approach to computing the coverage set with a minimum evidence threshold k.The algorithm works in four main stages: Stage 1: Behavior mapping We first analyze the training data to create a mapping of behaviors for each possible subsequence of the input.For each subset of indices I, we record how different subsequences x I behave when paired with their complements x [n]\I , essentially mapping each subsequence to a function from complements to outputs.</p>
<p>Figures 11 and 12 show the results for the PARALLEL-2-HOP task.The IICG patterns reveal strong representation clustering at mid-layers: at positions x 2 and x 3 when grouped by b 1 = f 1 (x 1 , x 2 ), and at position x 4 when grouped by b 2 = f 2 (x 3 , x 4 ).Causal tracing confirms the causal role of these clustered representations in the model's predictions.</p>
<p>Figure 11 :Figure 12 :
1112
Figure 11: IICG heatmap for PARALLEL-2-HOP task with grouping strategies based on b 1 = f 1 (x 1 , x 2 ) (Left), b 2 = f 2 (x 3 , x 4 ) (Middle), and t = f 3 (b 1 , b 2 ) (Right).</p>
<p>OODFigure 13 :Figure 14 :
1314
Figure 13: IICG heatmap for 3-HOP task with grouping strategies based on b 1 = f 1 (x 1 , x 2 ) (Left), b 2 = f 2 (b 1 , x 3 ) (Middle), and t = f 3 (b 2 , x 4 ) (Right).</p>
<p>for the intermediate state.Throughout the proof we impose two assumptions.(A1) Balanced classes: Each intermediate value b ∈ X is realized by exactly |X | first-hop pairs, i.e., the sets E b := {(x 1 , x 2 ) ∈ X 2 : f 1 (x 1 , x 2 ) = b} all have size |X | and form a partition of X 2 .(A2) Uniform training sampler: The training set D contains N triples drawn uniformly with replacement from the domain X 3 .</p>
<p>Fix one balanced class E b of size n := |X | (Assumption (A1)).Create a graph G b whose vertices are the first-hop pairs in E b , and place an undirected edge {i, j} exactly when the pair (i, j) has been observed in at least k distinct shared contexts.By Step E.2, each potential edge appears with probability</p>
<p>F. 2
2
Measured power-law scaling constants across task structures and model sizes Using our measurement protocol, we measure the required dataset size N req across three different compositional structures (2-HOP, PARALLEL-2-HOP, and 3-HOP) and three model scales (68M, 96M, and 1.5B parameters).For each task structure, we vary the token set size |X | from 50 to 200, allowing us to observe the scaling relationship.</p>
<p>Figure 15 : 2 .
152
Figure 15: Robustness of power-law scaling relationship to hyperparameter variations in the 2-HOP task with |X |= 50.Each line shows the training and test accuracy curves for a different configuration: (1) baseline, (2) reduced learning rate (4e-4, half of baseline), (3) reduced weight decay (0.01, one-tenth of baseline), and (4) changed generalization criteria (test accuracy &gt; 0.95 within 10 epochs after training accuracy &gt; 0.95).R 2 &gt; 0.99 for all linear fitting.</p>
<p>Figure 16 :
16
Figure 16: Coverage analysis for NON-TREE task with |X |= 50.The graph shows the percentage of ID test data covered at different k values across various dataset sizes (N ).Compared to the 2-HOP task (Fig. 3, left), NON-TREE has significantly lower coverage at equivalent dataset sizes, indicating that path ambiguity impedes the formation of functional equivalence relationships.</p>
<p>Fig. 16 demonstrates
16
Fig. 16 demonstrates that with equivalent training dataset sizes, a smaller percentage of ID test examples fall inside k-coverage for the NON-TREE task compared to the 2-HOP task shown in Fig. 3 (Left).This aligns with our theoretical analysis in Sec.5.4, which predicts that path ambiguity limits the establishment of functional equivalence relationships between input subsequences, as the model cannot generalize across different x 2 values in the NON-TREE structure even when they produce the same intermediate state b = f 1 (x 1 , x 2 ).</p>
<p>Figure 17 :
17
Figure 17: ID test accuracy comparison between GPT-2 (96M parameters) and GPT-2-XL (1.5B parameters) on the NON-TREE task with |X |= 50, measured 100 epochs after training accuracy exceeds 0.99.Despite the 15x increase in parameter count, the accuracy does not increase.</p>
<p>For a model thatIEFigure 18 :
18
Figure 18: Causal tracing analysis for the NON-TREE model after extended training.The heatmap shows indirect effect values across different layer-token positions.Left: perturbation leading to different intermediate state b = f 1 (x 1 , x 2 ).Middle: same b but different x 2 .Right: different b and x 2 .The causal tracing results in Fig.18reveal how the model achieves generalization in the presence of path ambiguity.Across all perturbation strategies, the model's predictions show strong causal dependence on representations at both the x 1 and x 2 positions, indicating reliance on direct access to both input tokens rather than an abstracted intermediate computation.This pattern contrasts sharply with the 2-HOP task, where causal effects concentrate primarily at positions corresponding to clustered functional equivalence representations.</p>
<p>H. 1
1
Type-I: Structure-based generalization Type-I generalization occurs when a model succeeds by identifying and utilizing functionally equivalent components based on how primitive functions are composed.This form of generalization is precisely what we have formalized and studied via the coverage principle.Crucially, this kind of generalization remains strictly bounded by coverage as defined in Section 3.</p>
<p>1 . 2 .
12
intermediate state representations through vocabulary space alignment.We compare two training conditions on a modified 2-HOP task with |X |= 50 and N = 10k, after 40k epochs of training: Standard Training: f 1 ̸ = f 2 , model only sees complete two-hop examples (x 1 , x 2 , x 3 ) → t.With Partial Computation: f 1 = f 2 , model additionally sees all possible partial computations (x 1 , x 2 ) → b where b = f 1 (x 1 , x 2 ) (2,500 partial examples, not counted toward the N = 10k two-hop training data).To assess interpretability, we measure the Mean Reciprocal Rank (MRR) of intermediate state representations when projected to vocabulary space using the unembedding matrix.Low MRR indicates that the model's internal representation of intermediate state b aligns with the corresponding vocabulary token.</p>
<p>Fig. 19 Figure 19 :
1919
Fig. 19 shows a striking contrast between the two conditions.Under standard training, the MRR score remains very high throughout training, indicating that intermediate representations are not aligned with vocabulary space despite the model successfully learning the compositional task.However, when partial computations are included, the MRR score becomes very high, demonstrating clear vocabulary alignment.</p>
<p>1 , x 2 ) (cos intra ), and those that do not (cos inter ), for each position and layer of the model.We term this difference the Intra-Inter Cosine Gap IICG = cos intra − cos inter , where higher values indicate stronger within-group clustering relative to between-group separation.
Fig. 4 (Left) reveals a clear relationship: higher k-cutoff values yield higher IICG scores at certainpositions, indicating that stronger functional equivalence evidence leads to more coherent internalrepresentations. Critically, OOD examples show no clustering pattern at all, since they lack anyfunctional equivalence evidence in the training data, as predicted by the coverage principle. ThePCA visualization at position x 2 and layer 3 (Right) shows this trend visually. We verify that therepresentation clusters play a causal role in ID generalization with causal tracing [66, 67], a widelyused mechanistic interpretability technique to identify Transformer circuits (see Fig. 8 in App. D).Our findings extend the insights of Wang et al. [19] in several ways. First, we demonstrate thatunified circuit formation is driven by functional equivalence evidence in the training data, not byexplicit exposure to intermediate computation steps. Consequently, out-of-coverage inputs lack theseunified circuits by construction, explaining their systematic failure on OOD examples. Moreover,we find that these clustered representations are not necessarily aligned with vocabulary embeddings,implying that standard interpretability methods like logit lens [70] may fail to detect these functionalequivalence representations despite their presence. See App. I for a detailed analysis.5.3 When does coverage lead to full ID generalization? A power-law lower bound6.05.50 5.75log(Nreq)4.5 5.5 5.03-Hop (c = 2.58) Parallel-2-Hop (c = 2.43) 2-Hop (c = 2.26)log(Nreq)5.00 5.25 4.50 4.752-Hop 1.5B (c=2.28) 2-Hop 96M (c=2.26) 2-Hop 68M (c=2.13)1.7 1.8 1.9 2.0 2.1 2.2 2.3 log(| |)1.7 1.8 1.9 2.0 2.1 2.2 2.3 log(| |)Figure 5: Left: Log-log plot of measured Nreq vs. token set size (|X |) across three compositionaltasks. The slope c corresponds to the empirical power-law scaling exponent. Omitted points for 3-HOP are due to prohibitively large dataset requirements. Right: Power-law scaling remains invariantacross GPT-2 model sizes (68M to 1.5B parameters) for 2-HOP task. R 2 &gt; 0.99 for all linear fitting.Our analysis of k-coverage and representation clustering demonstrates that stronger functionalequivalence evidence leads to better generalization. A natural follow-up question arises: Howlarge should the training set be to enable full generalization on all ID test data? Intuitively, thisrequires the training set to support the functional equivalence of every pair of inputs that shares thesame intermediate state b. Formally, for a 2-HOP task we need (x 1 , x 2 ) ≡{1,2} D</p>
<p>Table 1 :
1
Model configurations for different GPT-2 variants used in our experiments
ConfigurationGPT-2-Small GPT-2 GPT-2-XLNumber of Attention Heads61225Number of Layers4848Hidden Dimension7687681600Total Parameters68M96M1.5B</p>
<p>1 .
1
Training-to-generalization delay: Larger datasets naturally require more iterations to fit training data.By measuring epochs after reaching training accuracy &gt; 0.99, we focus on the generalization gap rather than conflating it with initial training difficulty.2.Epoch-based measurement: Using epochs rather than raw training steps ensures that the model sees each functional equivalence evidence approximately the same number of times, regardless of dataset size.This provides a fairer comparison across different dataset sizes.3.Practical time constraints: While indefinite training might eventually yield generalization with smaller datasets, we established a reasonable upper bound (100 epochs post-training convergence) to reflect practical limitations.4.Measurement precision:For each identified N req , we verified that 75% of this dataset size consistently failed to meet our generalization criterion.This establishes that our measurement error is at most − log(0.75)= 0.125 in log scale, providing confidence in the derived power-law exponents.</p>
<p>Table 2 :
2
Power law exponents for different tasks and GPT-2 sizes, obtained by linear fitting log(|X |) vs. log(N req ) plots.R 2 &gt; 0.99 for all linear fitting.
Model Size 2-HOP PARALLEL-2-HOP 3-HOP68M2.132.472.6196M2.262.352.501.5B2.282.172.60
By "learner", we mean any system that learns from data (e.g., neural networks).
For brevity, we use a shared token set X . Position-specific domains Xi can be embedded as subsets of an enlarged token set X = X1 ∪ X2 ∪ X3 without loss of generality.
For an undirected graph G, two vertices u, v are connected if G contains a path between u and v.
Such external generalization sources will be discussed in Sec. 7 and App. H.
Analyses for varying factors including task structures, entity set size (|X |), dataset size (N ), and training steps give consistent results; see App. D.
Note that this assumption is equivalent to modeling Fig.3(Right) as a step function.
See App. F measurement details.
For |X |= 50 and pseen = 0.7, our largest run (N = 50k) includes virtually the entire domain (≈ 0.7 2 × |X | 3 ≈ 61k distinct ID triples).
The taxonomy is not exhaustive; Appendix H details limitations and open questions.
See[42] for controlled concept-surface dissociation experiments.
We expand on the variable binding concept defined in Sec. 2 beyond flexibly associating variables with values, and we distinguish between self-replicative expansion (e.g., 2-hop → 3-hop via Type-III) and complex compositional binding (e.g., substituting noun phrases for nouns), where the latter may require explicit variable binding mechanisms beyond parameter sharing.
Acknowledgments and Disclosure of FundingWe would like to thank Jaewon Oh, Jihoon Ha, Hyowon Cho, Yunjae Won, Seongyun Lee, and Boshi Wang for their valuable feedback on our work.A LimitationsWe deliberately restrict to synthetic tasks to isolate structure-based limits without confounds from lexical or domain priors.We leave extending the coverage analysis to natural data as a future work.Additionally, our experiments focus on autoregressive Transformer architectures, and the applicability of the coverage principle to other architectures (e.g., state-space models) remains to be validated.For q ≪ 1 (which holds when |X | is large), we can use Taylor expansion:Substituting these approximations:Equation (S1.4) gives an exact expression for the probability (up to lower-order terms) that the single context c provides an evidence pair for the functional equivalence of fragments i and j.Remark on "lucky coincidences" Because functional k-equivalence demands consistency across all k evidences, a single coincidental equality f (i, c) = f (j, c) with i ̸ ∼ j can only masquerade as evidence when k = 1 and the dataset lacks any contradicting context c ′ .For k ≥ 2 the joint probability that two independent contexts simultaneously produce such coincidences is |X | −k per fragment pair and hence negligible relative to the isolate probability once N = Ω(|X | 2 ).Consequently, the effects of such "lucky coincidences" on the lower bound in Res.5.1 can be neglected.E.2 Step 2: probability of observing k evidences for one fixed pairHaving established the probability p 1 for a single evidence pair in Step 1, we now derive p k , the probability that a dataset provides at least k distinct contexts as evidence for the functional equivalence of two fragments i and j.This probability will determine the edge probability in the random graph model analyzed in Step 3. Thus Z ij (c) = 1 exactly when the single context c supplies an evidence pair for the functional equivalence of i and j.Indicators for one fragment pairSingle-context success probability From (S1.4),
The language of thought. Jerry Fodor, 1975</p>
<p>Computer science as empirical inquiry: symbols and search. Allen Newell, Herbert A Simon, 10.1145/360018.360022Commun. ACM. 0001-07821931976</p>
<p>Language models are few-shot learners. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam Mccandlish, Alec Radford, Ilya Sutskever, Dario Amodei, Annual Conference on Neural Information Processing Systems. Hugo Larochelle, Marc ' , Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, Hsuan-Tien Lin, NeurIPS2020. 2020. December 6-12, 2020, virtual, 202033Advances in Neural Information Processing Systems</p>
<p>. Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, abs/2303.087742023Gpt-4 technical report. ArXiv preprint</p>
<p>The llama 3 herd of models. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, abs/2407.217832024ArXiv preprint</p>
<p>Llama: Open and efficient foundation language models. Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Azhar, abs/2302.139712023ArXiv preprint</p>
<p>Gemini: a family of highly capable multimodal models. Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, Katie Millican, abs/2312.118052023ArXiv preprint</p>
<p>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, abs/2501.129482025ArXiv preprint</p>
<p>Connectionism and cognitive architecture: A critical analysis. Jerry A Fodor, Zenon W Pylyshyn, Cognition. 281-21988</p>
<p>Rethinking eliminative connectionism. Marcus Gary, Cognitive psychology. 3731998</p>
<p>Zoltán Gendler, Szabó , The Stanford Encyclopedia of Philosophy. Edward N Zalta, Uri Nodelman, 2024Metaphysics Research Lab, Stanford UniversityFall 2024 edition</p>
<p>Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks. M Brenden, Marco Lake, Baroni, Proceedings of the 35th International Conference on Machine Learning, ICML 2018. Jennifer G Dy, Andreas Krause, the 35th International Conference on Machine Learning, ICML 2018Stockholmsmässan, Stockholm, SwedenPMLRJuly 10-15, 2018. 201880of Proceedings of Machine Learning Research</p>
<p>Rearranging the familiar: Testing compositional generalization in recurrent networks. João Loula, Marco Baroni, Brenden Lake, 10.18653/v1/W18-5413Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP. Tal Linzen, Grzegorz Chrupała, Afra Alishahi, the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLPBrussels, BelgiumAssociation for Computational Linguistics2018</p>
<p>Improving text-to-SQL evaluation methodology. Catherine Finegan-Dollak, Jonathan K Kummerfeld, Li Zhang, Karthik Ramanathan, Sesh Sadasivam, Rui Zhang, Dragomir Radev, 10.18653/v1/P18-1033Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. Long Papers. Iryna Gurevych, Yusuke Miyao, the 56th Annual Meeting of the Association for Computational LinguisticsMelbourne, AustraliaAssociation for Computational Linguistics20181</p>
<p>CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning. Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, C Lawrence Zitnick, Ross B Girshick, 10.1109/CVPR.2017.2152017 IEEE Conference on Computer Vision and Pattern Recognition. Honolulu, HI, USAIEEE Computer Society2017. July 21-26, 2017. 2017</p>
<p>The reversal curse: LLMs trained on "a is b" fail to learn "b is a. Lukas Berglund, Meg Tong, Maximilian Kaufmann, Mikita Balesni, Asa Cooper Stickland, Tomasz Korbak, Owain Evans, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Faith and fate: Limits of transformers on compositionality. Nouha Dziri, Ximing Lu, Melanie Sclar, Lorraine Xiang, Liwei Li, Bill Jiang, Sean Yuchen Lin, Peter Welleck, Chandra West, Bhagavatula, Le Ronan, Jena D Bras, Soumya Hwang, Xiang Sanyal, Allyson Ren, Zaïd Ettinger, Yejin Harchaoui, Choi, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023. Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, Sergey Levine, NeurIPS; New Orleans, LA, USA2023. December 10 -16, 2023. 2023</p>
<p>Do large language models latently perform multi-hop reasoning?. Sohee Yang, Elena Gribovskaya, Nora Kassner, Mor Geva, Sebastian Riedel, 10.18653/v1/2024.acl-long.550Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. Lun-Wei Ku, Andre Martins, Vivek Srikumar, the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational Linguistics20241</p>
<p>Grokking of implicit reasoning in transformers: A mechanistic journey to the edge of generalization. Boshi Wang, Xiang Yue, Yu Su, Huan Sun ; Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, M Jakub, Cheng Tomczak, Zhang, Editors, Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems. NeurIPS; Vancouver, BC, Canada2024. 2024. December 10 -15, 2024. 2024</p>
<p>GSM-symbolic: Understanding the limitations of mathematical reasoning in large language models. Keivan Seyed Iman Mirzadeh, Hooman Alizadeh, Shahrokhi, The Thirteenth International Conference on Learning Representations. 2025Oncel Tuzel, Samy Bengio, and Mehrdad Farajtabar</p>
<p>Measuring compositional generalization: A comprehensive method on realistic data. Daniel Keysers, Nathanael Schärli, Nathan Scales, Hylke Buisman, Daniel Furrer, Sergii Kashubin, Nikola Momchev, Danila Sinopalnikov, Lukasz Stafiniak, Tibor Tihon, Dmitry Tsarkov, Xiao Wang, Marc Van Zee, Olivier Bousquet, 8th International Conference on Learning Representations. Addis Ababa, EthiopiaApril 26-30, 2020. 20202020OpenReview.net</p>
<p>CTL++: Evaluating generalization on never-seen compositional patterns of known functions, and compatibility of neural representations. Róbert Csordás, Kazuki Irie, Juergen Schmidhuber, 10.18653/v1/2022.emnlp-main.662Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. Yoav Goldberg, Zornitsa Kozareva, Yue Zhang, the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022</p>
<p>COGS: A compositional generalization challenge based on semantic interpretation. Najoung Kim, Tal Linzen, 10.18653/v1/2020.emnlp-main.731Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). Bonnie Webber, Trevor Cohn, Yulan He, Yang Liu, the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Association for Computational Linguistics2020</p>
<p>Measuring and narrowing the compositionality gap in language models. Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah Smith, Mike Lewis, 10.18653/v1/2023.findings-emnlp.378Findings of the Association for Computational Linguistics: EMNLP 2023. Houda Bouamor, Juan Pino, Kalika Bali, Singapore2023Association for Computational Linguistics</p>
<p>Do large language models have compositional ability? an investigation into limitations and scalability. Zhuoyan Xu, Zhenmei Shi, Yingyu Liang, ICLR 2024 Workshop on Mathematical and Empirical Understanding of Foundation Models. 2024</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, Illia Polosukhin, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017. Isabelle Guyon, Samy Ulrike Von Luxburg, Hanna M Bengio, Rob Wallach, S V N Fergus, Roman Vishwanathan, Garnett, Long Beach, CA, USADecember 4-9, 2017. 2017</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H Chi, V Quoc, Denny Le, Zhou, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems. S Sanmi Koyejo, A Mohamed, Danielle Agarwal, K Belgrave, A Cho, Oh, NeurIPS; New Orleans, LA, USA2022. 2022. November 28 -December 9, 2022. 2022</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems. S Sanmi Koyejo, A Mohamed, Danielle Agarwal, K Belgrave, A Cho, Oh, New Orleans, LA, USA2022. November 28 -December 9, 2022. 20222022</p>
<p>A benchmark for systematic generalization in grounded language understanding. Laura Ruis, Jacob Andreas, Marco Baroni, Diane Bouchacourt, Brenden M Lake, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems. Hugo Larochelle, Marc ' , Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, Hsuan-Tien Lin, NeurIPS2020. 2020. December 6-12, 2020, virtual, 2020</p>
<p>Planbench: An extensible benchmark for evaluating large language models on planning and reasoning about change. Karthik Valmeekam, Matthew Marquez, Alberto Olmo Hernandez, Sarath Sreedharan, Subbarao Kambhampati, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023. Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, Sergey Levine, NeurIPS; New Orleans, LA, USA2023. December 10 -16, 2023. 2023</p>
<p>Position: Llms can't plan, but can help planning in llm-modulo frameworks. Subbarao Kambhampati, Karthik Valmeekam, Lin Guan, Mudit Verma, Kaya Stechly, Siddhant Bhambri, Lucas Saldyt, Anil Murthy, Forty-first International Conference on Machine Learning, ICML 2024. Vienna, AustriaJuly 21-27, 2024. 2024OpenReview.net</p>
<p>Concepts and compositionality: in search of the brain's language of thought. M Steven, Joshua D Frankland, Greene, Annual review of psychology. 7112020</p>
<p>Compositional generalization through abstract representations in human and artificial neural networks. Takuya Ito, Tim Klinger, Douglas Schultz, John Murray, Michael W Cole, Mattia Rigotti, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems. S Sanmi Koyejo, A Mohamed, Danielle Agarwal, K Belgrave, A Cho, Oh, NeurIPS; New Orleans, LA, USA2022. 2022. November 28 -December 9, 2022. 2022</p>
<p>The computational origin of representation. Minds and machines. Steven T Piantadosi, 202131</p>
<p>Modelling meaning composition from formalism to mechanism. Andrea E Martin, Giosuè Baggio, Philosophical Transactions of the Royal Society B. 3752019</p>
<p>Tensor product variable binding and the representation of symbolic structures in connectionist systems. Paul Smolensky, Artificial intelligence. 461-21990</p>
<p>The algebraic mind: Integrating connectionism and cognitive science. Marcus Gary, 2003MIT press</p>
<p>Compositionality decomposed: How do neural networks generalise. Dieuwke Hupkes, Verna Dankers, Mathijs Mul, Elia Bruni, Journal of Artificial Intelligence Research. 672020</p>
<p>A complexitybased theory of compositionality. Eric Elmoznino, Thomas Jiralerspong, Yoshua Bengio, Guillaume Lajoie, abs/2410.148172024ArXiv preprint</p>
<p>The neural binding problem(s). Jerome Feldman, 10.1007/s11571-012-9219-8Cognitive Neurodynamics. 72013</p>
<p>On the binding problem in artificial neural networks. Klaus Greff, Sjoerd Van Steenkiste, Jürgen Schmidhuber, abs/2012.052082020ArXiv preprint</p>
<p>Is the reversal curse a binding problem? uncovering limitations of transformers from a basic generalization failure. Boshi Wang, Huan Sun, abs/2504.019282025ArXiv preprint</p>
<p>Building machines that learn and think like people. Brenden M Lake, Joshua B Tomer D Ullman, Samuel J Tenenbaum, Gershman, Behavioral and brain sciences. 40e2532017</p>
<p>Limits of transformer language models on learning to compose algorithms. Jonathan Thomm, Giacomo Camposampiero, Aleksandar Terzic, Michael Hersche, Bernhard Schölkopf, Abbas Rahimi, Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems. Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, M Jakub, Cheng Tomczak, Zhang, NeurIPS; Vancouver, BC, Canada2024. 2024. December 10 -15, 2024. 2024</p>
<p>Visual representation learning does not generalize strongly within the same domain. Lukas Schott, Julius Von Kügelgen, Frederik Träuble, Peter Vincent Gehler, Chris Russell, Matthias Bethge, Bernhard Schölkopf, Francesco Locatello, Wieland Brendel, The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event. April 25-29, 2022. 2022OpenReview.net</p>
<p>Exploring compositional generalization of large language models. Haoran Yang, Hongyuan Lu, Wai Lam, Deng Cai, Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Yang , Trista ) Cao, Isabel Papadimitriou, Anaelia Ovalle, the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMexico City, MexicoAssociation for Computational Linguistics20244Student Research Workshop)</p>
<p>Unveiling the mechanisms of explicit cot training: How chain-of-thought enhances reasoning generalization. Xinhao Yao, Ruifeng Ren, Yun Liao, Yong Liu, abs/2502.046672025ArXiv preprint</p>
<p>Good-enough compositional data augmentation. Jacob Andreas, 10.18653/v1/2020.acl-main.676Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Dan Jurafsky, Joyce Chai, Natalie Schluter, Joel Tetreault, the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational Linguistics2020</p>
<p>Solving SCAN tasks with data augmentation and input embeddings. Michal Auersperger, Pavel Pecina, Proceedings of the International Conference on Recent Advances in Natural Language Processing. Ruslan Mitkov, Galia Angelova, the International Conference on Recent Advances in Natural Language ProcessingHeld Online, 2021. INCOMA Ltd</p>
<p>Scaling laws for neural language models. Jared Kaplan, Sam Mccandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, Dario Amodei, abs/2001.083612020ArXiv preprint</p>
<p>Training compute-optimal large language models. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego De Las, Lisa Anne Casas, Johannes Hendricks, Aidan Welbl, Tom Clark, Eric Hennigan, Katie Noland, George Millican, Bogdan Van Den Driessche, Aurelia Damoc, Simon Guy, Karen Osindero, Erich Simonyan, Oriol Elsen, Jack W Vinyals, Laurent Rae, Sifre, Proceedings of the 36th International Conference on Neural Information Processing Systems, NIPS '22. the 36th International Conference on Neural Information Processing Systems, NIPS '22Red Hook, NY, USACurran Associates Inc2022ISBN 9781713871088</p>
<p>Challenging common assumptions in the unsupervised learning of disentangled representations. Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Rätsch, Sylvain Gelly, Bernhard Schölkopf, Olivier Bachem, Proceedings of the 36th International Conference on Machine Learning, ICML 2019. Kamalika Chaudhuri, Ruslan Salakhutdinov, the 36th International Conference on Machine Learning, ICML 2019Long Beach, California, USAPMLR9-15 June 2019. 201997of Proceedings of Machine Learning Research</p>
<p>When does compositional structure yield compositional generalization? a kernel theory. Samuel Lippl, Kim Stachenfeld, The Thirteenth International Conference on Learning Representations. 2025</p>
<p>A theory of independent mechanisms for extrapolation in generative models. Michel Besserve, Rémy Sun, Dominik Janzing, Bernhard Schölkopf, Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event. AAAI PressFebruary 2-9, 2021. 2021</p>
<p>The role of disentanglement in generalisation. Milton Llera Montero, J H Casimir, Ludwig, Ponte Rui, Gaurav Costa, Jeffrey Malhotra, Bowers, 9th International Conference on Learning Representations, ICLR 2021, Virtual Event. AustriaMay 3-7, 2021. OpenReview.net, 2021</p>
<p>Human-like systematic generalization through a metalearning neural network. M Brenden, Marco Lake, Baroni, Nature. 62379852023</p>
<p>How to represent part-whole hierarchies in a neural network. Geoffrey Hinton, Neural Computation. 3532023</p>
<p>Neural module networks. Jacob Andreas, Marcus Rohrbach, Trevor Darrell, Dan Klein, 10.1109/CVPR.2016.122016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016. Las Vegas, NV, USAIEEE Computer SocietyJune 27-30, 2016. 2016</p>
<p>A path towards autonomous machine intelligence version 0. Yann Lecun, Open Review. 912022</p>
<p>Inductive biases for deep learning of higher-level cognition. Anirudh Goyal, Yoshua Bengio, Proceedings of the Royal Society A. 478202100682266. 2022</p>
<p>A mathematical framework for transformer circuits. Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Transformer Circuits Thread. 11122021</p>
<p>In-context learning and induction heads. Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova Dassarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, abs/2209.118952022ArXiv preprint</p>
<p>Progress measures for grokking via mechanistic interpretability. Neel Nanda, Lawrence Chan, Tom Lieberum, Jess Smith, Jacob Steinhardt, The Eleventh International Conference on Learning Representations, ICLR 2023. Kigali, RwandaMay 1-5, 2023. 2023OpenReview.net</p>
<p>Toy models of superposition. Nelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan, Shauna Kravec, Zac Hatfield-Dodds, Robert Lasenby, Dawn Drain, Carol Chen, abs/2209.106522022ArXiv preprint</p>
<p>Locating and editing factual associations in GPT. Kevin Meng, David Bau, Alex Andonian, Yonatan Belinkov, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems. S Sanmi Koyejo, A Mohamed, Danielle Agarwal, K Belgrave, A Cho, Oh, New Orleans, LA, USA2022. November 28 -December 9, 2022. 20222022</p>
<p>How does GPT-2 compute greaterthan?: Interpreting mathematical abilities in a pre-trained language model. Michael Hanna, Ollie Liu, Alexandre Variengien, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023. Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, Sergey Levine, Editors, NeurIPS; New Orleans, LA, USA2023. December 10 -16, 2023. 2023</p>
<p>Localizing model behavior with path patching. Nicholas Goldowsky-Dill, Chris Macleod, Lucas Sato, Aryaman Arora, abs/2304.05969ArXiv preprint. 2023</p>
<p>An explainable transformer circuit for compositional generalization. Cheng Tang, Brenden Lake, Mehrdad Jazayeri, abs/2502.158012025ArXiv preprint</p>
<p>Out-of-distribution generalization via composition: a lens through induction heads in transformers. Jiajun Song, Zhuoyan Xu, Yiqiao Zhong, e2417182122. 2024122Proceedings of the National Academy of Sciences of the United States of America</p>
<p>gpt: the logit lens. LessWrong. 2020</p>
<p>Eliciting latent predictions from transformers with the tuned lens. Nora Belrose, Zach Furman, Logan Smith, Danny Halawi, Igor Ostrovsky, Lev Mckinney, Stella Biderman, Jacob Steinhardt, abs/2303.081122023ArXiv preprint</p>
<p>Neural networks are a priori biased towards boolean functions with low entropy. Chris Mingard, Joar Skalse, Guillermo Valle-Pérez, David Martínez-Rubio, Vladimir Mikulik, Ard A Louis, ArXiv preprint. 1909.11522. 2019</p>
<p>Revisiting minimum description length complexity in overparameterized models. Raaz Dwivedi, Chandan Singh, Bin Yu, Martin Wainwright, Journal of Machine Learning Research. 242682023</p>
<p>On the implicit bias in deep-learning algorithms. Gal Vardi, Communications of the ACM. 6662023</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI blog. 1892019</p>
<p>When not to trust language models: Investigating effectiveness of parametric and non-parametric memories. Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, Hannaneh Hajishirzi, 10.18653/v1/2023.acl-long.546Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. Anna Rogers, Jordan Boyd-Graber, Naoaki Okazaki, the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics20231</p>
<p>Large language models struggle to learn long-tail knowledge. Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, Colin Raffel, International Conference on Machine Learning, ICML 2023. Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, Jonathan Scarlett, Honolulu, Hawaii, USAPMLRJuly 2023. 2023202of Proceedings of Machine Learning Research</p>
<p>How do large language models acquire factual knowledge during pretraining?. Hoyeon Chang, Jinho Park, Seonghyeon Ye, Sohee Yang, Youngkyung Seo, Du-Seong Chang, Minjoon Seo, Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems. Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, M Jakub, Cheng Tomczak, Zhang, NeurIPS; Vancouver, BC, Canada2024. 2024. December 10 -15, 2024. 2024</p>
<p>Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal, Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, Agnieszka Kluska, Aitor Lewkowycz, Akshat Agarwal, Alethea Power, Alex Ray, Alex Warstadt, Alexander W Kocurek, Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Parrish, Allen Nie, Aman Hussain, Amanda Askell, Amanda Dsouza, Ambrose Slone, Ameet Rahane, Anantharaman S Iyer, Anders Johan Andreassen, Andrea Madotto, Andrea Santilli, Andreas Stuhlmüller, Andrew M Dai, Andrew La, Andrew Kyle Lampinen, Andy Zou, Angela Jiang, Angelica Chen, Anh Vuong, Animesh Gupta, Anna Gottardi, Antonio Norelli, Anu Venkatesh, Arash Gholamidavoodi, Arfa Tabassum, Arul Menezes, Arun Kirubarajan, Asher Mullokandov, Ashish Sabharwal, Austin Herrick, Avia Efrat, Aykut Erdem, Ayla Karakaş, B Ryan Roberts, Bao Sheng Loe, Barret Zoph, Bartłomiej Bojanowski, Batuhan Özyurt, Behnam Hedayatnia, Behnam Neyshabur, Benjamin Inden, Benno Stein, Berk Ekmekci, Bill Yuchen Lin, Blake Howald, Bryan Orinion, Cameron Diao, Cameron Dour, Catherine Stinson, Cedrick Argueta, Cesar Ferri, Chandan Singh, Charles Rathkopf, Chenlin Meng, Chitta Baral, Chiyu Wu, Chris Callison-Burch, Christopher Waites, Christian Voigt, Christopher D Manning, Christopher Potts, Cindy Ramirez, Clara E Rivera, Clemencia Siro, Colin Raffel, Courtney Ashcraft, Cristina Garbacea, Damien Sileo, Dan Garrette, Dan Hendrycks, Dan Kilman, Dan Roth, C Daniel Freeman, Daniel Khashabi, Daniel Levy, Daniel Moseguí González, Danielle Perszyk, Danny Hernandez, Danqi Chen, Daphne Ippolito, Dar Gilboa, David Dohan, David Drakard, David Jurgens, Debajyoti Datta, Deep Ganguli, Denis Emelin, Denis Kleyko, Deniz Yuret, Derek Chen, Derek Tam, Dieuwke Hupkes, Diganta Misra, Dilyar Buzan, Dimitri Coelho Mollo, Diyi Yang, Dong-Ho Lee, Dylan Schrader, Ekaterina Shutova, Ekin Dogus Cubuk, Elad Segal, Eleanor Hagerman, Elizabeth Barnes, Elizabeth Donoway, Ellie Pavlick, Emanuele Rodolà, Emma Lam, Eric Chu, Eric Tang, Erkut Erdem, Ernie Chang, Ethan A Chi, Ethan Dyer, Ethan Jerzak, Ethan Kim, Eunice Engefu Manyasi, Evgenii Zheltonozhskii, Fanyue Xia, Fatemeh Siar, Fernando Martínez-Plumed, Francesca Happé, Francois Chollet, Frieda Rong, Gaurav Mishra, Genta Indra Winata, Gerard De Melo, Germàn Kruszewski, Giambattista Parascandolo, Giorgio Mariani, Gloria Xinyue Wang, Gonzalo Jaimovitch-Lopez, Gregor Betz, Guy Gur-Ari, Hana Galijasevic, Hannah Kim, Hannah Rashkin, Hannaneh Hajishirzi, Harsh Mehta, Hayden Bogar, Henry Francis, Anthony Shevlin, Hinrich Schuetze, Hiromu Yakura, Hongming Zhang, Hugh Mee Wong, Ian Ng, Isaac Noble, Jaap Jumelet, Jack Geissinger, Jackson Kernion, Jacob Hilton, Jaehoon Lee, Jaime Fernández Fisac, James B Simon, James Koppel, James Zheng, James Zou, Jan Kocon, Jana Thompson, Janelle Wingfield, Jared Kaplan, Jarema Radom, Jascha Sohl-Dickstein, Jason Phang, Jason Wei, Jason Yosinski, Jekaterina Novikova, Jelle Bosscher, Jennifer Marsh, Jeremy Kim, Jeroen Taal, Jesse Engel, Jesujoba Alabi, Jiacheng Xu, Jiaming Song, Jillian Tang, Joan Waweru, John Burden, John Miller, John U Balis, Jonathan Batchelder, Jonathan Berant, Jörg Frohberg, Jos Rozen, Jose Hernandez-Orallo, Joseph Boudeman, Joseph Guerr, Joseph Jones, Joshua B Tenenbaum, Joshua S Rule, Joyce Chua, Kamil Kanclerz, Karen Livescu, Karl Krauth, Karthik Gopalakrishnan, Katerina Ignatyeva, Katja Markert, Kaustubh Dhole, Kevin Gimpel, Kevin Omondi, Kory Wallace Mathewson, Kristen Chiafullo, Ksenia Shkaruta, Kumar Shridhar, Kyle Mcdonell, Kyle Richardson, Laria Reynolds, Leo Gao, Li Zhang, Liam Dugan, Lianhui Qin, Lidia Contreras-Ochando, Louis-Philippe Morency, Luca Moschella, Lucas Lam, Lucy Noble, Ludwig Schmidt, Luheng He, Luis Oliveros-Colón, Luke Metz, Lütfi Kerem Senel, Maarten Bosma, Maarten Sap, Maartje Ter Hoeve, Maheen Farooqi, Manaal Faruqui, Mantas Mazeika, Marco Baturan, Marco Marelli, Marco Maru, Maria Jose Ramirez-Quintana, Marie Tolkiehn, Mario Giulianelli, Martha Lewis, Martin Potthast, Matthew L Leavitt, Matthias Hagen, Mátyás Schubert, Medina Orduna Baitemirova, Melody Arnaud, Melvin Mcelrath, Michael Andrew Yee, Michael Cohen, Michael Gu, Michael Ivanitskiy, Michael Starritt, Michael Strube, Michał Swędrowski, Michele Bevilacqua, Michihiro Yasunaga, Mihir Kale, Mike Cain, Mimee Xu, Mirac Suzgun, Mitch Walker, Mo Tiwari, Mohit Bansal, Moin Aminnaseri, Mor Geva, Mozhdeh Gheini, Mukund Varma, T , Nanyun Peng, Nathan Andrew Chi, Nayeon Lee, Neta Gur-, Ari Krakover, ; Niveditha, S Iyer, Noah Constant, Noah Fiedel, Nuan Wen, Oliver Zhang, Omar Agha, Omar Elbaghdadi, Sam Shleifer, Sam Wiseman, Samuel Gruetter, R Samuel, Samuel Bowman, Sanghyun Stern Schoenholz, Sanjeev Han, Sarah A Kwatra, Sarik Rous, Sayan Ghazarian, Sean Ghosh, Sebastian Casey, Sebastian Bischoff, Sebastian Gehrmann, Sepideh Schuster, Shadi Sadeghi, Sharon Hamdan, Shashank Zhou, Sherry Srivastava, Shikhar Shi, Shima Singh, Asaadi, Shane Shixiang, Shubh Gu, Shubham Pachchigar, Shyam Toshniwal, Upadhyay, Shammie Shyamolima, Siamak Debnath, Simon Shakeri, Simone Thormeyer, Siva Melzi, Reddy, Priscilla Sneha, Soo-Hwan Makini, Spencer Lee, Sriharsha Torene, Stanislas Hatwar, Stefan Dehaene, Stefano Divic, Stella Ermon, Stephanie Biderman, Stephen Lin, Steven Prasad, Stuart Piantadosi, Summer Shieber, Svetlana Misherghi, Swaroop Kiritchenko, Tal Mishra, Tal Linzen, Tao Schuster, Tao Li, Tariq Yu, Tatsunori Ali, William Hashimoto, William Fedus, William Saunders, Wout Zhang, Xiang Vossen, Xiaoyu Ren, Xinran Tong, Xinyi Zhao, Xudong Wu, Yadollah Shen, Yair Yaghoobzadeh, Yangqiu Lakretz, Yasaman Song, Yejin Bahri, Yichi Choi, Sophie Yang, Yifu Hao, Yonatan Chen, Yu Belinkov, Yufang Hou, Yuntao Hou, Zachary Bai, Zhuoye Seid, Zijian Zhao, Zijie J Wang, Zirui Wang, Ziyi Wang, Wu, Vikas Raunak, Vinay Venkatesh Ramasesh, vinay uday prabhu, Vishakh Padmakumar. Trishala Neeraj, Tushar Khot, Tyler Shultz, Uri Shaham, Vedant Misra, Vera Demberg, Victoria Nyamai, Nicholas Cameron, Nicholas Roberts, Nick Doiron, Nicole Martinez, Nikita Nangia, Niklas Deckers, Niklas Muennighoff; Omer Levy, Owain Evans, Pablo Antonio Moreno Casares; Théo Desbordes, Theodore Rothschild, Thomas Phan, Tianle Wang, Tiberius Nkinyili, Timo Schick, Timofei Kornev, Titus Tunduny, Tobias Gerstenberg, Trenton ChangVivek Srikumar2023Parth Doshi, Pascale Fung, Paul Pu Liang, Paul Vicol, Pegah Alipoormolabashi. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Transactions on Machine Learning Research</p>
<p>Transformers provably solve parity efficiently with chain of thought. Juno Kim, Taiji Suzuki, The Thirteenth International Conference on Learning Representations. 2025</p>
<p>From sparse dependence to sparse attention: Unveiling how chain-of-thought enhances transformer sample efficiency. Kaiyue Wen, Huaqing Zhang, Hongzhou Lin, Jingzhao Zhang, The Thirteenth International Conference on Learning Representations. 2025</p>
<p>Chain of thoughtlessness? an analysis of cot in planning. Kaya Stechly, Karthik Valmeekam, Subbarao Kambhampati, Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems. Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, M Jakub, Cheng Tomczak, Zhang, NeurIPS; Vancouver, BC, Canada2024. 2024. December 10 -15, 2024. 2024</p>
<p>An analysis and mitigation of the reversal curse. Ang Lv, Kaiyi Zhang, Shufang Xie, Quan Tu, Yuhan Chen, Ji-Rong Wen, Rui Yan, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language Processing2024</p>
<p>Long short-term memory. Sepp Hochreiter, Jürgen Schmidhuber, Neural computation. 981997</p>
<p>Alex Graves, Greg Wayne, Ivo Danihelka, arXiv:1410.5401Neural turing machines. 2014arXiv preprint</p>
<p>Universal transformers. Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, Lukasz Kaiser, ICLR 20197th International Conference on Learning Representations. New Orleans, LA, USAMay 6-9, 2019. 2019OpenReview.net</p>
<p>The devil is in the detail: Simple tricks improve systematic generalization of transformers. Róbert Csordás, Kazuki Irie, Juergen Schmidhuber, 10.18653/v1/2021.emnlp-main.49Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. Marie-Francine Moens, Xuanjing Huang, Lucia Specia, Scott , Wen-Tau Yih, the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican RepublicAssociation for Computational Linguistics2021Online and Punta Cana</p>
<p>Object-centric learning with slot attention. Francesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahendran, Georg Heigold, Jakob Uszkoreit, Alexey Dosovitskiy, Thomas Kipf, Advances in Neural Information Processing Systems. H Larochelle, M Ranzato, R Hadsell, M F Balcan, H Lin, Curran Associates, Inc202033</p>
<p>Ontheplanning abilities of openAI's o1 models: Feasibility, optimality, and generalizability. Kevin Wang, Junbo Li, Neel P Bhatt, Yihan Xi, Zhangyang Wang, Language Gamification -NeurIPS 2024 Workshop. 2024</p>
<p>Reverse training to nurse the reversal curse. Olga Golovneva, Zeyuan Allen-Zhu, Jason E Weston, Sainbayar Sukhbaatar, First Conference on Language Modeling. 2024</p>
<p>Physics of language models: Part 3.1, knowledge storage and extraction. Zeyuan Allen, -Zhu , Yuanzhi Li, Forty-first International Conference on Machine Learning, ICML 2024. Vienna, AustriaJuly 21-27, 2024. 2024OpenReview.net</p>
<p>Mamba: Linear-time sequence modeling with selective state spaces. Albert Gu, Tri Dao, First Conference on Language Modeling. 2024</p>
<p>Gradient-based learning applied to document recognition. Yann Lecun, Léon Bottou, Yoshua Bengio, Patrick Haffner, Proceedings of the IEEE. 86111998</p>
<p>Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and textto-SQL task. Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, Zilin Zhang, Dragomir Radev, 10.18653/v1/D18-1425Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Ellen Riloff, David Chiang, Julia Hockenmaier, Jun'ichi Tsujii, the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational Linguistics2018</p>
<p>Locating and editing factual associations in mamba. Sen Arnab, David Sharma, David Atkinson, Bau, First Conference on Language Modeling. 2024</p>
<p>On random graphs i. Paul Erdős, Alfréd Rényi, Publ. math. debrecen. 6181959</p>
<p>On the evolution of random graphs. Paul Erdős, Alfréd Rényi, Publ. Math. Inst. Hungar. Acad. Sci. 51960</p>
<p>An approximation theorem for the poisson binomial distribution. Lucien Le, Cam , 10.2140/pjm.1960.10.1181Pacific Journal of Mathematics. 1041960</p>
<p>Generalization on the unseen, logic reasoning and degree curriculum. Emmanuel Abbe, Samy Bengio, Aryo Lotfi, Kevin Rizk, Journal of Machine Learning Research. 253312024</p>
<p>Arithmetic transformers can length-generalize in both operand length and count. Hanseul Cho, Jaeyoung Cha, Srinadh Bhojanapalli, Chulhee Yun, The Thirteenth International Conference on Learning Representations. 2025</p>
<p>What algorithms can transformers learn? A study in length generalization. Hattie Zhou, Arwen Bradley, Etai Littwin, Noam Razin, Omid Saremi, Joshua M Susskind, Samy Bengio, Preetum Nakkiran, ICLR 2024The Twelfth International Conference on Learning Representations. Vienna, AustriaMay 7-11, 2024. 2024OpenReview.net</p>
<p>Positional description matters for transformers arithmetic. Ruoqi Shen, Sébastien Bubeck, Ronen Eldan, Yin Tat Lee, Yuanzhi Li, Yi Zhang, arXiv:2311.147372023arXiv preprint</p>
<p>Transformers can achieve length generalization but not robustly. Yongchao Zhou, Uri Alon, Xinyun Chen, Xuezhi Wang, Rishabh Agarwal, Denny Zhou, ICLR 2024 Workshop on Mathematical and Empirical Understanding of Foundation Models. 2024</p>
<p>Neural networks and the chomsky hierarchy. Gregoire Deletang, Anian Ruoss, Jordi Grau-Moya, Tim Genewein, Kevin Li, Elliot Wenliang, Chris Catt, Marcus Cundy, Shane Hutter, Joel Legg, Pedro A Veness, Ortega, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Transformers learn shortcuts to automata. Bingbin Liu, Jordan T Ash, Surbhi Goel, Akshay Krishnamurthy, Cyril Zhang, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Curriculum learning for human compositional generalization. Fabian Ronald B Dekker, Christopher Otto, Summerfield, Proceedings of the National Academy of Sciences. 11941e22055821192022</p>            </div>
        </div>

    </div>
</body>
</html>
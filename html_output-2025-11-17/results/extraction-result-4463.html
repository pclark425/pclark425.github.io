<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4463 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4463</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4463</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-101.html">extraction-schema-101</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <p><strong>Paper ID:</strong> paper-272600006</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2409.08027v1.pdf" target="_blank">From Explanations to Action: A Zero-Shot, Theory-Driven LLM Framework for Student Performance Feedback</a></p>
                <p><strong>Paper Abstract:</strong> Recent advances in eXplainable AI (XAI) for education have highlighted a critical challenge: ensuring that explanations for state-of-the-art AI models are understandable for non-technical users such as educators and students. In response, we introduce iLLuMinaTE, a zero-shot, chain-of-prompts LLM-XAI pipeline inspired by Miller's cognitive model of explanation. iLLuMinaTE is designed to deliver theory-driven, actionable feedback to students in online courses. iLLuMinaTE navigates three main stages - causal connection, explanation selection, and explanation presentation - with variations drawing from eight social science theories (e.g. Abnormal Conditions, Pearl's Model of Explanation, Necessity and Robustness Selection, Contrastive Explanation). We extensively evaluate 21,915 natural language explanations of iLLuMinaTE extracted from three LLMs (GPT-4o, Gemma2-9B, Llama3-70B), with three different underlying XAI methods (LIME, Counterfactuals, MC-LIME), across students from three diverse online courses. Our evaluation involves analyses of explanation alignment to the social science theory, understandability of the explanation, and a real-world user preference study with 114 university students containing a novel actionability simulation. We find that students prefer iLLuMinaTE explanations over traditional explainers 89.52% of the time. Our work provides a robust, ready-to-use framework for effectively communicating hybrid XAI-driven insights in education, with significant generalization potential for other human-centric fields.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4463.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4463.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Decomposed-question rubric</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Decomposed Binary Yes/No Annotation Rubric for Explanation Evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A structured set of binary ('yes'/'no') decomposed questions used to evaluate whether generated explanations align with explainer outputs, student feature values, course context, and theory-specific selection criteria; validated by experts and used to drive automated LLM annotation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Decomposed yes/no annotation rubric</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>The rubric breaks down explanation quality into a small set of concrete binary questions (four general questions applicable across theories plus up to six theory-specific questions). Annotators (human experts and an LLM annotator) answer each as YES only if the generated text fully satisfies the condition; otherwise NO. Examples: 'Is the generated text using the provided data extensively?', 'Is the generated text correctly using the model's predicted outcome?', and theory-specific items like 'Is the generated text selecting the causes based on controllability?'. Aggregate scores are computed as the ratio of YES answers per item and averaged across responses.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>General: data usage, reliance on explainer results, correct use of model prediction, use of course content/structure; Theory-specific: e.g., Abnormality, Temporality, Controllability (Abnormal Conditions); minimality and causal relevance (Pearl); necessity and robustness (Necessity & Robustness); foil selection and contrast relevance (Contrastive); statistical relevance (SR); stepwise reasoning (CoT).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Educational data / XAI for education</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Evaluation rubric for explanatory text and theory-aligned selections (applies to causal/contrastive/statistical explanations)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Applied to 315 human-annotated responses (plus GPT-4o annotations on the same block) producing average agreement metrics (overall >0.82 YES ratios across explainers/theories). Used to compute instruction-following scores per theory and explainer; Q4 (using course content) had lower scores and lower annotation reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Hybrid: human experts annotated an initial block and validated rubric; then GPT-4o was instructed to annotate all generated explanations using the same decomposed questions. Comparison between human and GPT-4o annotations showed 94.68% ± 5.20 agreement (percentage agreement) on the 42-response block.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Initial three-expert annotation on 42 responses with inter-rater Cohen's kappa (κ = 0.71 ± 0.13); subsequent comparison of GPT-4o annotations to human annotations on the same block yielding percent-agreement metrics and per-theory agreement table.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Binary decomposition can be brittle (YES requires near-perfect conformity), some items (e.g., use of course content) were subjective and had lower inter-rater reliability (Q4 κ = 0.52). Decomposed criteria may miss gradations of partial correctness and may be sensitive to prompt wording.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Annotations were performed over 21,915 generated natural language explanations from 315 students across three MOOCs (DSP, Geo, VA); 315 human-annotated responses used for rubric validation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From Explanations to Action: A Zero-Shot, Theory-Driven LLM Framework for Student Performance Feedback', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4463.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4463.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-as-judge prompt</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-as-a-judge Evaluation Prompt</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A structured instruction used to make an LLM (GPT-4o) act as an annotator/judge answering decomposed yes/no evaluation questions, with strict rules for YES/NO assignments to mirror expert annotation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>LLM-as-judge automated annotation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>An instruction template given to an LLM that asks it to answer each decomposed question with YES only if the generated text entirely fulfills the condition (even minor inaccuracies disqualify a YES), otherwise NO. This enforces conservative judgments intended to mimic stringent human expert annotation. Used to annotate all 21,915 explanations after initial human validation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Same decomposed questions as the rubric (general and theory-specific). The LLM annotator follows strict binary decision rules emphasizing complete correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Educational data / XAI</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Annotation/evaluation framework for text explanations</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>GPT-4o annotation matched human expert annotations with 94.68% ± 5.20 agreement on the validation block; used to scale annotations to all generated explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated LLM annotation validated against human annotations (hybrid approach).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Direct comparison with three human expert annotators on identical 42-response block; percent agreement and per-theory agreement reported (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Bias toward harsher grading by GPT-4o in some categories (e.g., Base Contrastive explanations) observed; strict YES threshold can undercount partial correctness; relies on LLM reliability as annotator which may introduce systematic biases.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Same set of generated explanations from the iLLuMinaTE experiments; initial validation on 42 responses then full annotation on 21,915 NLEs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From Explanations to Action: A Zero-Shot, Theory-Driven LLM Framework for Student Performance Feedback', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4463.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4463.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Readability suite</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Readability and Grammar Metrics (Flesch-Kincaid, Gunning Fog, SMOG, LanguageTool Grammar Issues)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A set of standard readability and grammar-check metrics used to quantify the comprehensibility of final presented explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Readability and grammar scoring</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute Flesch-Kincaid Grade Level, Gunning Fog Index, SMOG Index, and number of LanguageTool grammar issues for each generated explanation; lower scores indicate easier readability and fewer grammar problems. Used to compare LLMs (GPT-4o, Gemma2-9B, Llama3-70B), explainers, and courses.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Estimated years of schooling required, sentence/vocabulary complexity, and grammar correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP / Educational communication</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Text comprehensibility metrics</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>GPT-4o achieved the best readability performance (no overlap in 95% CIs), Llama3 produced the fewest grammatical errors. No significant readability differences across course domains or source explainers.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated metric computation (no human raters for readability metrics reported).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Standard well-known metrics with statistical comparisons across conditions and 95% confidence intervals reported.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Readability formulas are coarse proxies for human understandability; do not measure truthfulness, actionability, or alignment to explainer causal content.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Final presented iLLuMinaTE explanations for ~21,915 NLEs, aggregated to representative samples (300 students for presentation stage).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From Explanations to Action: A Zero-Shot, Theory-Driven LLM Framework for Student Performance Feedback', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4463.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4463.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>User Preference Likert</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Five-Criterion User Preference Evaluation (Usefulness, Trustworthiness, Actionability, Completeness, Conciseness)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A human-subjects evaluation where participants rate explanations on five Likert-scale criteria and choose preferred explanations among alternatives; used to measure perceived quality and actionability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>User preference study with five Likert criteria</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>114 participants each viewed sets of four explanations (three iLLuMinaTE variations + one baseline) per student instance and chose their preferred explanation and rated each on five 1–5 Likert scales: Usefulness, Trustworthiness, Actionability, Completeness, Conciseness. Participants also selected an action for the next week based on their preferred explanation and identified weeks to focus on. Open-ended reasons for choices were collected.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Usefulness, Trustworthiness, Actionability, Completeness, Conciseness (definitions provided to participants). Preference counts and Likert distributions used as evaluation measures.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Educational technology / HCI</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Human judgment of explanation quality and actionability</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Students preferred iLLuMinaTE explanations over baseline post-hoc explainers 89.52% of the time. iLLuMinaTE explanations scored higher across Likert criteria (example: Usefulness contrastive = 4.18 vs BASE = 3.59). Students ranked criteria in importance: usefulness > actionability > trustworthiness > completeness ≈ conciseness.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Human-subjects evaluation (114 participants) with qualitative and quantitative responses.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Randomized presentation order, practice question for attention check (3 participants excluded), balanced mixture of pass/fail cases and different confidence levels; statistical analyses reported (distributions/violins).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Presentation choices (graphical/text simplifications) may influence perceptions; participants were external students (Prolific) not the original course students; self-reported judgments may reflect prior beliefs and not objective effectiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>User study materials: 6 sampled student cases (2 per course) with explanations generated across explainers/theories; aggregated participant ratings and chosen actions used for simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From Explanations to Action: A Zero-Shot, Theory-Driven LLM Framework for Student Performance Feedback', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4463.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4463.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Actionability simulation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Actionability Simulation via BiLSTM-based Feature Manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simulation that applies participant-selected interventions to student feature vectors and re-runs a BiLSTM student-success predictor to estimate potential performance improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Actionability simulation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Participants choose one of ten suggested actions aligned to model behavioral features. The experiment simulates increases in relevant features by 25th percentile (or 25%? paper states 'increasing the relevant features by 25% percentile') for week 6, then runs an independently trained BiLSTM (trained on 6-week student data) to predict outcome under the simulated action. The average change in predicted performance (balanced accuracy or predicted pass probability aggregated) is reported per explainer and per theory.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Estimated percent improvement in predicted success after simulated action; per-explainer and per-theory averages.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Educational modeling / intervention simulation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Simulation of intervention effects on predictive outcome</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>For participants preferring iLLuMinaTE, simulated average performance improvements were 13.5% (LIME), 14.2% (CEM), and 20.7% (MC-LIME). Across theories, Contrastive explanations produced ~28.2% and Necessity & Robustness ~24.9% average performance improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated simulation using an existing BiLSTM student success model informed by human-chosen actions.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Used BiLSTM models trained on historical data with train-val-test splits; performance of baseline models reported (balanced accuracies per course: 90.8, 80.3, 76.8).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Simulation assumes that participant-selected actions translate to specific measurable feature changes and that changing features alone captures causal effects on success; may overestimate real-world intervention efficacy; choice of 25% percentile step and how to map actions to features introduces modeling assumptions.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Simulations applied to synthetic modifications of the same MOOC student datasets; actions chosen by 114 participants mapped to features and applied to BiLSTM trained on six weeks of data.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From Explanations to Action: A Zero-Shot, Theory-Driven LLM Framework for Student Performance Feedback', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4463.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4463.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Instruction-following score</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Instruction-Following Measurement (ratio of 'Yes' answers)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An aggregated scoring method measuring how well LLM-generated explanations follow the provided social-science-theory instructions, computed as the ratio of 'Yes' answers across decomposed rubric questions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Instruction-following score (decomposed-YES ratio)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>For each generated explanation, annotators (GPT-4o and humans in validation) answer decomposed questions; scores are averaged per question, per theory, per explainer to yield instruction-following performance (values between 0 and 1). Statistical comparisons across LLMs and explainers performed with 95% CIs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Proportion of decomposed questions answered YES, per general and theory-specific items.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o, Gemma2-9B, Llama3-70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-4o: unspecified; Gemma2-9B: 9B; Llama3-70B: 70B</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP / XAI</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Evaluation of adherence to instruction-based explanation policies</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Overall YES ratios exceeded 0.82 across explainers and theories; GPT-4o scored highest in explanation selection, followed by Gemma2-9B and Llama3-70B, with overlapping 95% CIs. No significant differences between explainers or theories in aggregate.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated annotation (GPT-4o) validated against human expert annotations for a subset; used to scale scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Inter-rater human κ reported for initial block (0.71 ± 0.13); GPT-4o vs human percent agreement 94.68% ±5.20 used to justify automated scaling.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Binary nature may mask partial compliance; dependence on initial human-validated block; confidence intervals overlapped so differences across LLMs/explainers not strongly significant.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Representative sample of 105 students per course (315 total) used for instruction-following analyses; full set of 21,915 NLEs generated.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From Explanations to Action: A Zero-Shot, Theory-Driven LLM Framework for Student Performance Feedback', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4463.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4463.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Theory-aligned prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Theory-Driven Explanation Selection & Presentation Prompts (iLLuMinaTE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework of zero-shot chain-of-prompts where LLMs are instructed with social-science theories (eight variants) to select, condense, and present model-based explanations that are aligned to theoretical criteria (e.g., Abnormal Conditions, Pearl, Necessity & Robustness, Contrastive, SR, Relevance Selection, CoT).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Theory-aligned instruction-following evaluation within iLLuMinaTE</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>LLMs receive structured prompts containing model prediction, explainer importance scores, student feature values, course context, and a specific social-science theory instruction. The LLM outputs a selected explanation following the theory's criteria; a presentation prompt then composes concise actionable feedback using Hattie & Timperley feedback framework and Grice's maxims. Evaluation uses the decomposed-question rubric, readability metrics, and user preferences.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Theory-specific selection criteria (e.g., Abnormality/Temporality/Controllability for Abnormal Conditions; minimality and causal sufficiency for Pearl; necessity and robustness for NR; foil selection for Contrastive; statistical likelihood for SR), plus presentation criteria (Where am I going? How am I doing? Where to next?).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o, Gemma2-9B, Llama3-70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-4o: unspecified; Gemma2-9B: 9B; Llama3-70B: 70B</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Educational XAI; explanation generation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Causal/contrastive/statistical explanations and communicative presentation</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>iLLuMinaTE explanations aligned well with instructed theories (overall scores >0.82 YES ratios); students preferred iLLuMinaTE 89.52% of the time; different LLMs performed slightly differently across selection vs presentation (GPT-4o best for selection; Llama3 best on presentation according to rubric/readability).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Hybrid: LLM outputs evaluated via LLM-as-judge and human experts for initial validation; user study captured human subjective preferences.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Human expert annotation of a validation block (κ = 0.71 ± 0.13), GPT-4o annotation agreement with humans (94.68%), readability metrics, and a 114-participant user study.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Dependence on the quality of underlying post-hoc explainers; difficulty in incorporating course content fully (Q4 lower scores); variability across explainers and LLMs; evaluating NLEs independently of prior beliefs is challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>21,915 generated natural language explanations over three explainers, three LLMs, eight theories, and 315 students from three MOOCs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From Explanations to Action: A Zero-Shot, Theory-Driven LLM Framework for Student Performance Feedback', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4463.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e4463.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Inter-rater reliability</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Inter-rater Agreement Metrics (Cohen's Kappa and Percent Agreement)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of Cohen's Kappa to measure agreement among human annotators on decomposed questions and percent agreement to compare human and GPT-4o annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Inter-rater reliability measurement</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute Cohen's Kappa across three human annotators on an initial block of 42 responses to assess rubric reliability; compute percent agreement between human experts and GPT-4o on the same block. Report per-question and per-theory kappas when available.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>κ values reported with standard deviations; percent agreement with standard deviation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Annotation quality assessment</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Evaluation of annotation reliability</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Human inter-rater agreement: κ = 0.71 ± 0.13 overall; lower κ for Q4 (using course content) = 0.52. Human vs GPT-4o agreement: 94.68% ± 5.20 overall; per-theory agreements ranged from 82.22% (Base Contrastive) to ~98.89% (Abnormal Conditions).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Human inter-rater calculation plus human vs automated LLM annotator percent agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Statistical calculation of κ and percent agreement on validation block; used to justify scaling of automated annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Some questions/theories are harder to annotate reliably (contextual Q4); percent agreement can mask chance agreement; small validation block (42 responses) limits precision of κ estimates.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>42-response validation block annotated by three experts and GPT-4o; extended human annotations totaling 315 human-annotated responses used for broader checks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From Explanations to Action: A Zero-Shot, Theory-Driven LLM Framework for Student Performance Feedback', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4463.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e4463.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Baseline explainers</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Post-hoc Explainers (LIME, CEM, MC-LIME) used as causal-connection sources</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Three post-hoc explainability methods used to extract feature importance/contrastive/counterfactual explanations from BiLSTM student models; served as inputs for LLM-based explanation selection and presentation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Comparative evaluation across source explainers</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>LIME provides local feature weights; CEM (Contrastive Explanation Method) provides pertinent positives/negatives; MC-LIME finds minimal feature sets that flip predictions. These outputs are passed to LLMs which are then evaluated by the rubric, readability metrics, user preferences, and actionability simulations to assess how source explainer affects final explanation quality and impact.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Instruction-following scores, presentation/readability metrics, user preference ratings, and simulated actionability improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Explainable AI in education</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Feature-attribution and counterfactual explanations</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Choice of source explainer had limited impact on instruction-following/readability/user preference overall (overlapping CIs), but MC-LIME produced larger simulated actionability gains (20.7% average improvement vs 13.5% LIME, 14.2% CEM).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Evaluated via hybrid pipeline: LLM-generated explanations derived from each explainer were annotated and judged by GPT-4o/humans and rated by users.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Standard settings for each explainer aligned with prior work, consistent extraction parameters (e.g., LIME num_samples=5000, top-20 features) and replication scripts described in appendix.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Post-hoc explainers are not fully coherent with each other and may produce differing feature lists; reliance on post-hoc explainers means explanation reliability depends on explainer faithfulness to model internals.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Explainer outputs derived from BiLSTM models trained on three MOOCs' clickstream features for 315 students.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From Explanations to Action: A Zero-Shot, Theory-Driven LLM Framework for Student Performance Feedback', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4463.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e4463.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Presentation protocol</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Explanation Presentation Protocol (Hattie & Timperley + Grice)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A presentation prompting scheme that condenses selected explanations into concise actionable student feedback using Hattie & Timperley's 'Where am I going? How am I doing? Where to next?' and Grice's conversational maxims.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Presentation quality evaluation using feedback and conversational maxims</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Prompts instruct the LLM to produce feedback structured into the three Hattie & Timperley steps; Grice's maxims (Quality, Quantity, Relation, Manner) further constrain truthfulness, relevance, conciseness, and clarity. Output JSON-format feedback is then assessed using decomposed presentation questions and readability metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Presentation-specific decomposed questions (e.g., conciseness/readability within 5 minutes), readability scores, human preference (actionability/trustworthiness/usefulness).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o, Gemma2-9B, Llama3-70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-4o: unspecified; Gemma2-9B: 9B; Llama3-70B: 70B</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Educational feedback communication</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Communicative presentation of explanations</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Presentation-level rubric scores averaged >0.82; Llama3 achieved highest presentation-stage rubric scores while GPT-4o achieved best readability metrics; no significant differences across courses or source explainers for presentation quality.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Hybrid: presentation outputs annotated by GPT-4o (validated earlier) and rated by human participants in the user study.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Presentation rubric validated via human annotation and inter-rater checks; readability metrics applied for objective measures.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Adherence to maxims may conflict with pedagogical detail needs; condensing verbose selection outputs may omit relevant nuance; measuring actual impact beyond perceived actionability requires longitudinal intervention testing.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Presentation outputs for 21,915 generated explanations with focused readability evaluation on representative samples (300 students for presentation-stage scores).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From Explanations to Action: A Zero-Shot, Theory-Driven LLM Framework for Student Performance Feedback', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>The Structure and Function of Explanations <em>(Rating: 2)</em></li>
                <li>Conversational Processes and Causal Explanation <em>(Rating: 2)</em></li>
                <li>Causes and explanations: A structural-model approach. Part II: Explanations <em>(Rating: 2)</em></li>
                <li>The Power of Feedback <em>(Rating: 2)</em></li>
                <li>Contrastive Explanation <em>(Rating: 2)</em></li>
                <li>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models <em>(Rating: 1)</em></li>
                <li>A New Readability Yardstick <em>(Rating: 1)</em></li>
                <li>On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? <em>(Rating: 1)</em></li>
                <li>Why Should I Trust You? (LIME) <em>(Rating: 2)</em></li>
                <li>Explanations based on the Missing: Towards Contrastive Explanations with Pertinent Negatives <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4463",
    "paper_id": "paper-272600006",
    "extraction_schema_id": "extraction-schema-101",
    "extracted_data": [
        {
            "name_short": "Decomposed-question rubric",
            "name_full": "Decomposed Binary Yes/No Annotation Rubric for Explanation Evaluation",
            "brief_description": "A structured set of binary ('yes'/'no') decomposed questions used to evaluate whether generated explanations align with explainer outputs, student feature values, course context, and theory-specific selection criteria; validated by experts and used to drive automated LLM annotation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Decomposed yes/no annotation rubric",
            "evaluation_method_description": "The rubric breaks down explanation quality into a small set of concrete binary questions (four general questions applicable across theories plus up to six theory-specific questions). Annotators (human experts and an LLM annotator) answer each as YES only if the generated text fully satisfies the condition; otherwise NO. Examples: 'Is the generated text using the provided data extensively?', 'Is the generated text correctly using the model's predicted outcome?', and theory-specific items like 'Is the generated text selecting the causes based on controllability?'. Aggregate scores are computed as the ratio of YES answers per item and averaged across responses.",
            "evaluation_criteria": "General: data usage, reliance on explainer results, correct use of model prediction, use of course content/structure; Theory-specific: e.g., Abnormality, Temporality, Controllability (Abnormal Conditions); minimality and causal relevance (Pearl); necessity and robustness (Necessity & Robustness); foil selection and contrast relevance (Contrastive); statistical relevance (SR); stepwise reasoning (CoT).",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Educational data / XAI for education",
            "theory_type": "Evaluation rubric for explanatory text and theory-aligned selections (applies to causal/contrastive/statistical explanations)",
            "human_comparison": true,
            "evaluation_results": "Applied to 315 human-annotated responses (plus GPT-4o annotations on the same block) producing average agreement metrics (overall &gt;0.82 YES ratios across explainers/theories). Used to compute instruction-following scores per theory and explainer; Q4 (using course content) had lower scores and lower annotation reliability.",
            "automated_vs_human_evaluation": "Hybrid: human experts annotated an initial block and validated rubric; then GPT-4o was instructed to annotate all generated explanations using the same decomposed questions. Comparison between human and GPT-4o annotations showed 94.68% ± 5.20 agreement (percentage agreement) on the 42-response block.",
            "validation_method": "Initial three-expert annotation on 42 responses with inter-rater Cohen's kappa (κ = 0.71 ± 0.13); subsequent comparison of GPT-4o annotations to human annotations on the same block yielding percent-agreement metrics and per-theory agreement table.",
            "limitations_challenges": "Binary decomposition can be brittle (YES requires near-perfect conformity), some items (e.g., use of course content) were subjective and had lower inter-rater reliability (Q4 κ = 0.52). Decomposed criteria may miss gradations of partial correctness and may be sensitive to prompt wording.",
            "benchmark_dataset": "Annotations were performed over 21,915 generated natural language explanations from 315 students across three MOOCs (DSP, Geo, VA); 315 human-annotated responses used for rubric validation.",
            "uuid": "e4463.0",
            "source_info": {
                "paper_title": "From Explanations to Action: A Zero-Shot, Theory-Driven LLM Framework for Student Performance Feedback",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "LLM-as-judge prompt",
            "name_full": "LLM-as-a-judge Evaluation Prompt",
            "brief_description": "A structured instruction used to make an LLM (GPT-4o) act as an annotator/judge answering decomposed yes/no evaluation questions, with strict rules for YES/NO assignments to mirror expert annotation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "LLM-as-judge automated annotation",
            "evaluation_method_description": "An instruction template given to an LLM that asks it to answer each decomposed question with YES only if the generated text entirely fulfills the condition (even minor inaccuracies disqualify a YES), otherwise NO. This enforces conservative judgments intended to mimic stringent human expert annotation. Used to annotate all 21,915 explanations after initial human validation.",
            "evaluation_criteria": "Same decomposed questions as the rubric (general and theory-specific). The LLM annotator follows strict binary decision rules emphasizing complete correctness.",
            "model_name": "GPT-4o",
            "model_size": null,
            "scientific_domain": "Educational data / XAI",
            "theory_type": "Annotation/evaluation framework for text explanations",
            "human_comparison": true,
            "evaluation_results": "GPT-4o annotation matched human expert annotations with 94.68% ± 5.20 agreement on the validation block; used to scale annotations to all generated explanations.",
            "automated_vs_human_evaluation": "Automated LLM annotation validated against human annotations (hybrid approach).",
            "validation_method": "Direct comparison with three human expert annotators on identical 42-response block; percent agreement and per-theory agreement reported (Table 4).",
            "limitations_challenges": "Bias toward harsher grading by GPT-4o in some categories (e.g., Base Contrastive explanations) observed; strict YES threshold can undercount partial correctness; relies on LLM reliability as annotator which may introduce systematic biases.",
            "benchmark_dataset": "Same set of generated explanations from the iLLuMinaTE experiments; initial validation on 42 responses then full annotation on 21,915 NLEs.",
            "uuid": "e4463.1",
            "source_info": {
                "paper_title": "From Explanations to Action: A Zero-Shot, Theory-Driven LLM Framework for Student Performance Feedback",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Readability suite",
            "name_full": "Readability and Grammar Metrics (Flesch-Kincaid, Gunning Fog, SMOG, LanguageTool Grammar Issues)",
            "brief_description": "A set of standard readability and grammar-check metrics used to quantify the comprehensibility of final presented explanations.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_method_name": "Readability and grammar scoring",
            "evaluation_method_description": "Compute Flesch-Kincaid Grade Level, Gunning Fog Index, SMOG Index, and number of LanguageTool grammar issues for each generated explanation; lower scores indicate easier readability and fewer grammar problems. Used to compare LLMs (GPT-4o, Gemma2-9B, Llama3-70B), explainers, and courses.",
            "evaluation_criteria": "Estimated years of schooling required, sentence/vocabulary complexity, and grammar correctness.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "NLP / Educational communication",
            "theory_type": "Text comprehensibility metrics",
            "human_comparison": false,
            "evaluation_results": "GPT-4o achieved the best readability performance (no overlap in 95% CIs), Llama3 produced the fewest grammatical errors. No significant readability differences across course domains or source explainers.",
            "automated_vs_human_evaluation": "Automated metric computation (no human raters for readability metrics reported).",
            "validation_method": "Standard well-known metrics with statistical comparisons across conditions and 95% confidence intervals reported.",
            "limitations_challenges": "Readability formulas are coarse proxies for human understandability; do not measure truthfulness, actionability, or alignment to explainer causal content.",
            "benchmark_dataset": "Final presented iLLuMinaTE explanations for ~21,915 NLEs, aggregated to representative samples (300 students for presentation stage).",
            "uuid": "e4463.2",
            "source_info": {
                "paper_title": "From Explanations to Action: A Zero-Shot, Theory-Driven LLM Framework for Student Performance Feedback",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "User Preference Likert",
            "name_full": "Five-Criterion User Preference Evaluation (Usefulness, Trustworthiness, Actionability, Completeness, Conciseness)",
            "brief_description": "A human-subjects evaluation where participants rate explanations on five Likert-scale criteria and choose preferred explanations among alternatives; used to measure perceived quality and actionability.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "User preference study with five Likert criteria",
            "evaluation_method_description": "114 participants each viewed sets of four explanations (three iLLuMinaTE variations + one baseline) per student instance and chose their preferred explanation and rated each on five 1–5 Likert scales: Usefulness, Trustworthiness, Actionability, Completeness, Conciseness. Participants also selected an action for the next week based on their preferred explanation and identified weeks to focus on. Open-ended reasons for choices were collected.",
            "evaluation_criteria": "Usefulness, Trustworthiness, Actionability, Completeness, Conciseness (definitions provided to participants). Preference counts and Likert distributions used as evaluation measures.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Educational technology / HCI",
            "theory_type": "Human judgment of explanation quality and actionability",
            "human_comparison": true,
            "evaluation_results": "Students preferred iLLuMinaTE explanations over baseline post-hoc explainers 89.52% of the time. iLLuMinaTE explanations scored higher across Likert criteria (example: Usefulness contrastive = 4.18 vs BASE = 3.59). Students ranked criteria in importance: usefulness &gt; actionability &gt; trustworthiness &gt; completeness ≈ conciseness.",
            "automated_vs_human_evaluation": "Human-subjects evaluation (114 participants) with qualitative and quantitative responses.",
            "validation_method": "Randomized presentation order, practice question for attention check (3 participants excluded), balanced mixture of pass/fail cases and different confidence levels; statistical analyses reported (distributions/violins).",
            "limitations_challenges": "Presentation choices (graphical/text simplifications) may influence perceptions; participants were external students (Prolific) not the original course students; self-reported judgments may reflect prior beliefs and not objective effectiveness.",
            "benchmark_dataset": "User study materials: 6 sampled student cases (2 per course) with explanations generated across explainers/theories; aggregated participant ratings and chosen actions used for simulation.",
            "uuid": "e4463.3",
            "source_info": {
                "paper_title": "From Explanations to Action: A Zero-Shot, Theory-Driven LLM Framework for Student Performance Feedback",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Actionability simulation",
            "name_full": "Actionability Simulation via BiLSTM-based Feature Manipulation",
            "brief_description": "A simulation that applies participant-selected interventions to student feature vectors and re-runs a BiLSTM student-success predictor to estimate potential performance improvements.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Actionability simulation",
            "evaluation_method_description": "Participants choose one of ten suggested actions aligned to model behavioral features. The experiment simulates increases in relevant features by 25th percentile (or 25%? paper states 'increasing the relevant features by 25% percentile') for week 6, then runs an independently trained BiLSTM (trained on 6-week student data) to predict outcome under the simulated action. The average change in predicted performance (balanced accuracy or predicted pass probability aggregated) is reported per explainer and per theory.",
            "evaluation_criteria": "Estimated percent improvement in predicted success after simulated action; per-explainer and per-theory averages.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Educational modeling / intervention simulation",
            "theory_type": "Simulation of intervention effects on predictive outcome",
            "human_comparison": false,
            "evaluation_results": "For participants preferring iLLuMinaTE, simulated average performance improvements were 13.5% (LIME), 14.2% (CEM), and 20.7% (MC-LIME). Across theories, Contrastive explanations produced ~28.2% and Necessity & Robustness ~24.9% average performance improvement.",
            "automated_vs_human_evaluation": "Automated simulation using an existing BiLSTM student success model informed by human-chosen actions.",
            "validation_method": "Used BiLSTM models trained on historical data with train-val-test splits; performance of baseline models reported (balanced accuracies per course: 90.8, 80.3, 76.8).",
            "limitations_challenges": "Simulation assumes that participant-selected actions translate to specific measurable feature changes and that changing features alone captures causal effects on success; may overestimate real-world intervention efficacy; choice of 25% percentile step and how to map actions to features introduces modeling assumptions.",
            "benchmark_dataset": "Simulations applied to synthetic modifications of the same MOOC student datasets; actions chosen by 114 participants mapped to features and applied to BiLSTM trained on six weeks of data.",
            "uuid": "e4463.4",
            "source_info": {
                "paper_title": "From Explanations to Action: A Zero-Shot, Theory-Driven LLM Framework for Student Performance Feedback",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Instruction-following score",
            "name_full": "Instruction-Following Measurement (ratio of 'Yes' answers)",
            "brief_description": "An aggregated scoring method measuring how well LLM-generated explanations follow the provided social-science-theory instructions, computed as the ratio of 'Yes' answers across decomposed rubric questions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Instruction-following score (decomposed-YES ratio)",
            "evaluation_method_description": "For each generated explanation, annotators (GPT-4o and humans in validation) answer decomposed questions; scores are averaged per question, per theory, per explainer to yield instruction-following performance (values between 0 and 1). Statistical comparisons across LLMs and explainers performed with 95% CIs.",
            "evaluation_criteria": "Proportion of decomposed questions answered YES, per general and theory-specific items.",
            "model_name": "GPT-4o, Gemma2-9B, Llama3-70B",
            "model_size": "GPT-4o: unspecified; Gemma2-9B: 9B; Llama3-70B: 70B",
            "scientific_domain": "NLP / XAI",
            "theory_type": "Evaluation of adherence to instruction-based explanation policies",
            "human_comparison": true,
            "evaluation_results": "Overall YES ratios exceeded 0.82 across explainers and theories; GPT-4o scored highest in explanation selection, followed by Gemma2-9B and Llama3-70B, with overlapping 95% CIs. No significant differences between explainers or theories in aggregate.",
            "automated_vs_human_evaluation": "Automated annotation (GPT-4o) validated against human expert annotations for a subset; used to scale scoring.",
            "validation_method": "Inter-rater human κ reported for initial block (0.71 ± 0.13); GPT-4o vs human percent agreement 94.68% ±5.20 used to justify automated scaling.",
            "limitations_challenges": "Binary nature may mask partial compliance; dependence on initial human-validated block; confidence intervals overlapped so differences across LLMs/explainers not strongly significant.",
            "benchmark_dataset": "Representative sample of 105 students per course (315 total) used for instruction-following analyses; full set of 21,915 NLEs generated.",
            "uuid": "e4463.5",
            "source_info": {
                "paper_title": "From Explanations to Action: A Zero-Shot, Theory-Driven LLM Framework for Student Performance Feedback",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Theory-aligned prompting",
            "name_full": "Theory-Driven Explanation Selection & Presentation Prompts (iLLuMinaTE)",
            "brief_description": "A framework of zero-shot chain-of-prompts where LLMs are instructed with social-science theories (eight variants) to select, condense, and present model-based explanations that are aligned to theoretical criteria (e.g., Abnormal Conditions, Pearl, Necessity & Robustness, Contrastive, SR, Relevance Selection, CoT).",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Theory-aligned instruction-following evaluation within iLLuMinaTE",
            "evaluation_method_description": "LLMs receive structured prompts containing model prediction, explainer importance scores, student feature values, course context, and a specific social-science theory instruction. The LLM outputs a selected explanation following the theory's criteria; a presentation prompt then composes concise actionable feedback using Hattie & Timperley feedback framework and Grice's maxims. Evaluation uses the decomposed-question rubric, readability metrics, and user preferences.",
            "evaluation_criteria": "Theory-specific selection criteria (e.g., Abnormality/Temporality/Controllability for Abnormal Conditions; minimality and causal sufficiency for Pearl; necessity and robustness for NR; foil selection for Contrastive; statistical likelihood for SR), plus presentation criteria (Where am I going? How am I doing? Where to next?).",
            "model_name": "GPT-4o, Gemma2-9B, Llama3-70B",
            "model_size": "GPT-4o: unspecified; Gemma2-9B: 9B; Llama3-70B: 70B",
            "scientific_domain": "Educational XAI; explanation generation",
            "theory_type": "Causal/contrastive/statistical explanations and communicative presentation",
            "human_comparison": false,
            "evaluation_results": "iLLuMinaTE explanations aligned well with instructed theories (overall scores &gt;0.82 YES ratios); students preferred iLLuMinaTE 89.52% of the time; different LLMs performed slightly differently across selection vs presentation (GPT-4o best for selection; Llama3 best on presentation according to rubric/readability).",
            "automated_vs_human_evaluation": "Hybrid: LLM outputs evaluated via LLM-as-judge and human experts for initial validation; user study captured human subjective preferences.",
            "validation_method": "Human expert annotation of a validation block (κ = 0.71 ± 0.13), GPT-4o annotation agreement with humans (94.68%), readability metrics, and a 114-participant user study.",
            "limitations_challenges": "Dependence on the quality of underlying post-hoc explainers; difficulty in incorporating course content fully (Q4 lower scores); variability across explainers and LLMs; evaluating NLEs independently of prior beliefs is challenging.",
            "benchmark_dataset": "21,915 generated natural language explanations over three explainers, three LLMs, eight theories, and 315 students from three MOOCs.",
            "uuid": "e4463.6",
            "source_info": {
                "paper_title": "From Explanations to Action: A Zero-Shot, Theory-Driven LLM Framework for Student Performance Feedback",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Inter-rater reliability",
            "name_full": "Inter-rater Agreement Metrics (Cohen's Kappa and Percent Agreement)",
            "brief_description": "Use of Cohen's Kappa to measure agreement among human annotators on decomposed questions and percent agreement to compare human and GPT-4o annotations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Inter-rater reliability measurement",
            "evaluation_method_description": "Compute Cohen's Kappa across three human annotators on an initial block of 42 responses to assess rubric reliability; compute percent agreement between human experts and GPT-4o on the same block. Report per-question and per-theory kappas when available.",
            "evaluation_criteria": "κ values reported with standard deviations; percent agreement with standard deviation.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Annotation quality assessment",
            "theory_type": "Evaluation of annotation reliability",
            "human_comparison": true,
            "evaluation_results": "Human inter-rater agreement: κ = 0.71 ± 0.13 overall; lower κ for Q4 (using course content) = 0.52. Human vs GPT-4o agreement: 94.68% ± 5.20 overall; per-theory agreements ranged from 82.22% (Base Contrastive) to ~98.89% (Abnormal Conditions).",
            "automated_vs_human_evaluation": "Human inter-rater calculation plus human vs automated LLM annotator percent agreement.",
            "validation_method": "Statistical calculation of κ and percent agreement on validation block; used to justify scaling of automated annotations.",
            "limitations_challenges": "Some questions/theories are harder to annotate reliably (contextual Q4); percent agreement can mask chance agreement; small validation block (42 responses) limits precision of κ estimates.",
            "benchmark_dataset": "42-response validation block annotated by three experts and GPT-4o; extended human annotations totaling 315 human-annotated responses used for broader checks.",
            "uuid": "e4463.7",
            "source_info": {
                "paper_title": "From Explanations to Action: A Zero-Shot, Theory-Driven LLM Framework for Student Performance Feedback",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Baseline explainers",
            "name_full": "Post-hoc Explainers (LIME, CEM, MC-LIME) used as causal-connection sources",
            "brief_description": "Three post-hoc explainability methods used to extract feature importance/contrastive/counterfactual explanations from BiLSTM student models; served as inputs for LLM-based explanation selection and presentation.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_method_name": "Comparative evaluation across source explainers",
            "evaluation_method_description": "LIME provides local feature weights; CEM (Contrastive Explanation Method) provides pertinent positives/negatives; MC-LIME finds minimal feature sets that flip predictions. These outputs are passed to LLMs which are then evaluated by the rubric, readability metrics, user preferences, and actionability simulations to assess how source explainer affects final explanation quality and impact.",
            "evaluation_criteria": "Instruction-following scores, presentation/readability metrics, user preference ratings, and simulated actionability improvements.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Explainable AI in education",
            "theory_type": "Feature-attribution and counterfactual explanations",
            "human_comparison": false,
            "evaluation_results": "Choice of source explainer had limited impact on instruction-following/readability/user preference overall (overlapping CIs), but MC-LIME produced larger simulated actionability gains (20.7% average improvement vs 13.5% LIME, 14.2% CEM).",
            "automated_vs_human_evaluation": "Evaluated via hybrid pipeline: LLM-generated explanations derived from each explainer were annotated and judged by GPT-4o/humans and rated by users.",
            "validation_method": "Standard settings for each explainer aligned with prior work, consistent extraction parameters (e.g., LIME num_samples=5000, top-20 features) and replication scripts described in appendix.",
            "limitations_challenges": "Post-hoc explainers are not fully coherent with each other and may produce differing feature lists; reliance on post-hoc explainers means explanation reliability depends on explainer faithfulness to model internals.",
            "benchmark_dataset": "Explainer outputs derived from BiLSTM models trained on three MOOCs' clickstream features for 315 students.",
            "uuid": "e4463.8",
            "source_info": {
                "paper_title": "From Explanations to Action: A Zero-Shot, Theory-Driven LLM Framework for Student Performance Feedback",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Presentation protocol",
            "name_full": "Explanation Presentation Protocol (Hattie & Timperley + Grice)",
            "brief_description": "A presentation prompting scheme that condenses selected explanations into concise actionable student feedback using Hattie & Timperley's 'Where am I going? How am I doing? Where to next?' and Grice's conversational maxims.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Presentation quality evaluation using feedback and conversational maxims",
            "evaluation_method_description": "Prompts instruct the LLM to produce feedback structured into the three Hattie & Timperley steps; Grice's maxims (Quality, Quantity, Relation, Manner) further constrain truthfulness, relevance, conciseness, and clarity. Output JSON-format feedback is then assessed using decomposed presentation questions and readability metrics.",
            "evaluation_criteria": "Presentation-specific decomposed questions (e.g., conciseness/readability within 5 minutes), readability scores, human preference (actionability/trustworthiness/usefulness).",
            "model_name": "GPT-4o, Gemma2-9B, Llama3-70B",
            "model_size": "GPT-4o: unspecified; Gemma2-9B: 9B; Llama3-70B: 70B",
            "scientific_domain": "Educational feedback communication",
            "theory_type": "Communicative presentation of explanations",
            "human_comparison": false,
            "evaluation_results": "Presentation-level rubric scores averaged &gt;0.82; Llama3 achieved highest presentation-stage rubric scores while GPT-4o achieved best readability metrics; no significant differences across courses or source explainers for presentation quality.",
            "automated_vs_human_evaluation": "Hybrid: presentation outputs annotated by GPT-4o (validated earlier) and rated by human participants in the user study.",
            "validation_method": "Presentation rubric validated via human annotation and inter-rater checks; readability metrics applied for objective measures.",
            "limitations_challenges": "Adherence to maxims may conflict with pedagogical detail needs; condensing verbose selection outputs may omit relevant nuance; measuring actual impact beyond perceived actionability requires longitudinal intervention testing.",
            "benchmark_dataset": "Presentation outputs for 21,915 generated explanations with focused readability evaluation on representative samples (300 students for presentation-stage scores).",
            "uuid": "e4463.9",
            "source_info": {
                "paper_title": "From Explanations to Action: A Zero-Shot, Theory-Driven LLM Framework for Student Performance Feedback",
                "publication_date_yy_mm": "2024-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "The Structure and Function of Explanations",
            "rating": 2,
            "sanitized_title": "the_structure_and_function_of_explanations"
        },
        {
            "paper_title": "Conversational Processes and Causal Explanation",
            "rating": 2,
            "sanitized_title": "conversational_processes_and_causal_explanation"
        },
        {
            "paper_title": "Causes and explanations: A structural-model approach. Part II: Explanations",
            "rating": 2,
            "sanitized_title": "causes_and_explanations_a_structuralmodel_approach_part_ii_explanations"
        },
        {
            "paper_title": "The Power of Feedback",
            "rating": 2,
            "sanitized_title": "the_power_of_feedback"
        },
        {
            "paper_title": "Contrastive Explanation",
            "rating": 2,
            "sanitized_title": "contrastive_explanation"
        },
        {
            "paper_title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
            "rating": 1,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "A New Readability Yardstick",
            "rating": 1,
            "sanitized_title": "a_new_readability_yardstick"
        },
        {
            "paper_title": "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?",
            "rating": 1,
            "sanitized_title": "on_the_dangers_of_stochastic_parrots_can_language_models_be_too_big"
        },
        {
            "paper_title": "Why Should I Trust You? (LIME)",
            "rating": 2,
            "sanitized_title": "why_should_i_trust_you_lime"
        },
        {
            "paper_title": "Explanations based on the Missing: Towards Contrastive Explanations with Pertinent Negatives",
            "rating": 2,
            "sanitized_title": "explanations_based_on_the_missing_towards_contrastive_explanations_with_pertinent_negatives"
        }
    ],
    "cost": 0.020514499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>From Explanations to Action: A Zero-Shot, Theory-Driven LLM Framework for Student Performance Feedback
12 Sep 2024</p>
<p>Vinitra Swamy 
Davide Romano 
Bhargav Srinivasa Desikan 
Oana-Maria Camburu 
Tanja Käser 
From Explanations to Action: A Zero-Shot, Theory-Driven LLM Framework for Student Performance Feedback
12 Sep 20244E5056A0A9A424BB9E1F6FBD7CA98D76arXiv:2409.08027v1[cs.CY]Causal Connection Explanation Selection Explanation Presentation F1F2F3F4… F42F43F44F45 GPT4O / Llama3 / Gemma2 40B 9B F1F2F3F4… F42F43F44F45 CEM MC-LIME Pass Fail why LIME F1F2F3F4… F42F43F44F45
Recent advances in eXplainable AI (XAI) for education have highlighted a critical challenge: ensuring that explanations for state-of-the-art AI models are understandable for non-technical users such as educators and students.In response, we introduce iLLuMinaTE, a zero-shot, chainof-prompts LLM-XAI pipeline inspired by Miller (2019)'s cognitive model of explanation.iLLuMinaTE is designed to deliver theory-driven, actionable feedback to students in online courses.iLLuMinaTE navigates three main stages -causal connection, explanation selection, and explanation presentation -with variations drawing from eight social science theories (e.g.Abnormal Conditions, Pearl's Model of Explanation, Necessity and Robustness Selection, Contrastive Explanation).We extensively evaluate 21,915 natural language explanations of iLLuMinaTE extracted from three LLMs (GPT-4o, Gemma2-9B, Llama3-70B), with three different underlying XAI methods (LIME, Counterfactuals, MC-LIME), across students from three diverse online courses.Our evaluation involves analyses of explanation alignment to the social science theory, understandability of the explanation, and a real-world user preference study with 114 university students containing a novel actionability simulation.We find that students prefer iLLuMinaTE explanations over traditional explainers 89.52% of the time.Our work provides a robust, ready-to-use framework for effectively communicating hybrid XAI-driven insights in education, with significant generalization potential for other human-centric fields.</p>
<p>Introduction</p>
<p>Over the last decade, AI has seen widespread application in education, encompassing both learner-centric modelssuch as intelligent tutoring systems (Mousavinasab et al. 2021), knowledge tracing (Piech et al. 2015), and automated feedback systems (Jacobsen and Weber 2023) -and teacher-centric models, including real-time classroom insights (Holstein et al. 2018) and automated question generation (Hang, Tan, and Yu 2024).To adopt these solutions in real-world classrooms, model explainability is essential.Nazaretsky et al. (2022) underscore the importance of transparency for fostering educators' trust in AI-based educational technologies, while Conati, Porayska-Pomsta, and Mavrikis (2018) stress the need for interpretable models in * These authors contributed equally to this work.contexts where students see decision outcomes without understanding the underlying reasoning.</p>
<p>Recent literature on eXplainable AI (XAI) in education can be categorized into three main motivations: (1) allowing educational stakeholders to audit model mistakes (Khosravi et al. 2022;Pinto and Paquette 2024), (2) building student and teacher trust in AI (Nazaretsky et al. 2024), and (3) designing personalized interventions for students (Hur et al. 2022;Asadi et al. 2023).The most popular approaches in XAI for education are post-hoc explainers, like LIME (Hasib et al. 2022;Vultureanu-Albis ¸i and Bȃdicȃ 2021;Scheers and De Laet 2021;Pei and Xing 2021) and SHAP (Baranyi, Nagy, and Molontay 2020;Mu, Jetten, and Brunskill 2020).These explainers treat the underlying model as a black-box, enabling explanations after model training.</p>
<p>Despite their rising popularity, XAI methods suffer from a major weakness: a lack of adequate understandability, especially for a non-technical audience.At a course level, STEM professors expressed difficulty in understanding explainer outputs, requesting "more concrete and granular insights" on the scale of individual students (Swamy et al. 2023).At the individual student level, Hur et al. (2022) designed XAI-based interventions, but found they were extensive to integrate and provided limited learning gains compared to expert feedback.With pervasive educational benchmarks like ASSISTments (Heffernan and Heffernan 2014) and MOOCRadar (Yu et al. 2023) having hundreds of features and temporal aspects, it becomes difficult for students and teachers to interpret feature importance.</p>
<p>LLMs can be useful in making XAI more humaninterpretable, especially towards building stakeholder trust in AI and designing personalized student interventions.A recent study (Kroeger et al. 2024) suggests that LLMs can act as post-hoc explainers for complex models, finding that LLMs could identify relevant features when given examples of input data and model outputs.Atanasova et al. (2022) integrate explanation generation directly into the LLM's training, optimizing over diagnostic properties like data consistency and confidence.However, in domains where explanations directly influence human decisions, the nature of LLMs as "stochastic parrots" can be accompanied by detrimental side effects (Bender et al. 2021;Sarkar 2024).LLMs, while demonstrating potential in areas such as knowledge tracing or student synthesis, have not yet matured enough to act Figure 1: iLLuMinaTE involves five steps: 1) modeling course data for student success prediction, 2) using XAI methods to extract feature importance scores, 3) selecting important aspects of the explanation through an LLM aligned with a given social science theory, 4) converting this report into concise and actionable suggestions through an LLM, 5) evaluating student preferences of different explanation strategies through a real-world user study.</p>
<p>as student models that can be accompanied by inherent explanations (Neshaei et al. 2024;Nguyen, Tschiatschek, and Singla 2023).We instead propose to use LLMs as communicators of explanations (Zytek, Pidò, and Veeramachaneni 2024) to present XAI outputs in aligned text and visual formats that are actionable for educational stakeholders.We therefore present iLLuMinaTE, an in-context, chainof-prompts, zero-shot LLM pipeline that is inspired by Miller's cognitive processes of explanation (Miller 2019).iLLuMinaTE follows three main stages: (1) causal connection, (2) explanation selection, and (3) explanation presentation.Our experiments range over eight prevalent social science theories of explanation (Hilton 1990;Hilton and Slugoski 1986;Halpern and Pearl 2005;Lombrozo 2010; Woodward and Ross 2021), with three underlying explainers (LIME, Counterfactuals, MC-LIME), data from three online courses, evaluated using three LLMs (GPT-4o, Gemma2 9b, Llama3 70b) and a real-world user study with 114 university students.Notably, we find that students preferred iLLuMinaTE's explanations over baseline hybrid (text and visual) explanations from post-hoc methods 89.52% of the time, and had a particular preference on actionability for abnormal, pearl, and contrastive explanations.With our study, we make the following main contributions: 1. iLLuMinaTE, a chain-of-prompts framework in the education context to extract theory-driven natural language explanations (NLE) for student feedback.2.An LLM-XAI efficacy analysis of 216 variations of iLLuMinaTE prompting strategies over explainers, LLMs, social science theories, and student populations.3. A real-world evaluation of LLM-XAI preferences conducted with 114 university students.4.An XAI actionability study simulating student performance gains based on actions they selected from generated explanations.We provide our modular implementation of iLLuMinaTE publicly with adaptations for LangChain, Groq Cloud, and Replicate 1 .Our work provides a theorydriven methodology to communicate results of XAI to 1 https://github.com/epfl-ml4ed/iLLuMinaTEstudents, with broad generalization potential of explanation theory instruction prompts for other human-centric fields (e.g., healthcare, welfare, product recommendation).</p>
<p>Methodology</p>
<p>Our iLLuMinaTE pipeline (see Fig. 1) consists of five stages towards communicating explanations in a humanunderstandable and actionable way through LLMs.</p>
<p>In the Student Modeling phase, we extract behavioral features from raw clickstreams of student interactions and use BiLSTMs (Graves and Schmidhuber 2005) to predict student success following prior work (Asadi et al. 2023;Swamy, Marras, and Käser 2022).We then employ post-hoc explainers to obtain feature importance scores, representing the XAI: Causal Connection step.With the results from an explainer, the course context, and the student's feature values, we prompt an LLM using Explanation Selection instructions specific to social science theories of explanations.We evaluate the obtained explanations using human expert and GPT-4o annotations.We then use Explanation Presentation prompts to summarize the often verbose explanation selection reports into concise and actionable feedback for a student, taking into account theory on effective feedback (Hattie and Timperley 2007)'s and maxims for communication (Grice 1975).We then evaluate the final explanations using expert annotations.In the final phase, we assess Student Preferences and the actionability of iLLuMinaTE explanations in comparison with the current state-of-the-art in XAI for education in a user study with 114 students.</p>
<p>Student Modeling</p>
<p>To create the models building the basis for the explanations, we use the same features and model architectures as prior research working with the same datasets (Swamy, Marras, and Käser 2022;Galici et al. 2023;Swamy et al. 2023Swamy et al. , 2024b)).Data Collection.Our experiments are based on data collected from three MOOCs (Digital Signal Processing (DSP), Villes Africaines (VA), and Elements de Geomatique (Geo)) offered by a European University to a global student audience.The courses were organized into weekly modules including video lectures and quizzes and required students to complete graded assignments to earn course certificates.Students interacted with learning objects (videos, quizzes) associated with specific course weeks, enabling the creation of course-specific learning indicators.We represent a student's interations as a time series I c s = i 1 , . . ., i K , where each interaction i is a tuple (t, a, o), including a timestamp t, an action a (e.g., video play, pause; quiz submission), and a learning object o.The binary success label (pass-fail) for student s in course c is denoted as y s,c .Data collection and analysis were approved by the university's ethics review board (Nr.Anonymous).</p>
<p>Feature Extraction.We use a broad set of 45 behavioral features h derived from the student interactions, incorporating features from four feature sets shown to be predictive for student performance in MOOCs (Marras, Vignoud, and Käser 2021).The key behavioral aspects include: • Regularity (3 features): Tracks consistent study habits (Boroujeni et al. 2016).Modeling.For a course c and student s, our objective is to build a model that predicts y s,c early, using the features h s from the first five weeks.Following prior work (Swamy, Marras, and Käser 2022), we employ a BiLSTM for this task.We provide all reproducibility details in Appendix 6.</p>
<p>XAI: Causal Connection</p>
<p>We use three popular post-hoc explainers (LIME, CEM, and MC-LIME) to extract local, instance-specific explanations from the student models.We chose these three methods based on their popularity, but any in-hoc or interpretableby-design model could be used.</p>
<p>LIME (Local Interpretable</p>
<p>Model-Agnostic Explanations) (Ribeiro, Singh, and Guestrin 2016) provides interpretable explanations for individual predictions by approximating the complex model locally with an interpretable model.This process results in a set of feature weights indicating the positive or negative influence of each feature on the prediction.</p>
<p>CEM (Contrastive Explanation Method) (Dhurandhar et al. 2018) is a counterfactual method that identifies the features that need to be present (pertinent positives) or absent (pertinent negatives) for a model to maintain its prediction for a given instance.</p>
<p>MC-LIME (Minimal Counterfactual LIME) (Cohausz 2022) finds the minimal set of LIME features that, if changed, would alter the outcome.It focuses on features that increase the likelihood of an event (e.g., student dropout).MC-LIME applies changes to individual features, then pairs, and so on, until a change flips the prediction.This approach combines explanation sparseness with the advantages of counterfactuals and LIME.</p>
<p>• Regularity (3): Tracks consistent study habits (Boroujeni et al. 2016).• Engagement (13): Overall course involvement (Chen and Cui 2020).• Control (22): Assesses detailed video usage (Lallé and Conati 2020).• Participation (7): Monitors attendance in scheduled activities (Marras, Vignoud, and Kaser 2021).</p>
<p>Modeling.For a given course c, our objective is to build a model that predicts early the success label y s for a student s, using the extracted behavioral features h s from the first five weeks.Following prior work (Swamy, Marras, and Käser 2022), we employ a BiLSTM (two BiLSTM layers of size 32, loopback 3, followed by a dense layer) for this task.We provide further experimental details in the Appendix.</p>
<p>XAI: Causal Connection</p>
<p>We use three popular post-hoc explainers (LIME, CEM, and MC-LIME) to extract local, instance-specific explanations from the student models.We chose these three methods based on their popularity, but any in-hoc or interpretableby-design model could be used.</p>
<p>LIME (Local Interpretable</p>
<p>Model-Agnostic Explanations) (Ribeiro, Singh, and Guestrin 2016) provides interpretable explanations for individual predictions by approximating the complex model locally with an interpretable model.This process results in a set of feature weights indicating the positive or negative influence of each feature on the prediction.</p>
<p>CEM (Contrastive Explanation Method) (Dhurandhar et al. 2018) is a counterfactual method that identifies the features that need to be present (pertinent positives) or absent (pertinent negatives) for a model to maintain its prediction for a given instance.</p>
<p>MC-LIME (Minimal Counterfactual LIME) (Cohausz 2022) finds the minimal set of LIME features that, if changed, would alter the outcome.It focuses on features that increase the likelihood of an event (e.g., student dropout).MC-LIME applies changes to individual features, then pairs, and so on, until a change flips the prediction.This approach combines explanation sparseness with the advantages of counterfactuals and LIME.Relevance Selection.Based on Hilton's conversational model of explanation (Hilton 1990), the goal of relevancebased selection theory is to "resolve a puzzle in the explainee's mind about why the event happened by closing a gap in his or her knowledge".The theory emphasizes that shared knowledge between the explainer and the explainee are presuppositions of the explanations, and the other factors are the causes that should be explained.In short, the explainer should not explain any causes they think the explainee already knows.Abnormal conditions.This theory, based upon Hilton and Slugoski's abnormal conditions model (Hilton and Slugoski 1986), suggests that explanations often rely on unusual and temporally proximal events.People do not solely count on statistical likelihood but highlight uncommon factors contributing to an event to explain that event.During a conversation, the explainer is relying on the perceived common prior knowledge to identify potential causes that are considered abnormal, with greater weight given to temporally proximal</p>
<p>Explanation Selection</p>
<p>Explanation Selection</p>
<p>Our iLLuMinaTE pipeline generates explanations aligned to social science theories.Specifically, we have translated eight theories into prompts: Relevance Selection (RS), Abnormal Conditions (AC), Pearl's Model (Pearl's), Necessity and Robustness (NR), two contrastive explanations (BC, Con), Statistical Relevance (SR), and Chain of Thought (CoT).Our prompt structure contains (1) descriptions of the model, features, XAI method, and course context, (2) explainer importance scores and relevant student feature values, and (3) social science theory instructions.We present the general prompt template and two theory-specific examples (RS, AC) in this section, all details are in Appendix 7.1.Relevance Selection.Based on Hilton's conversational model of explanation (Hilton 1990), relevance-based selection theory aims to "resolve a puzzle in the explainee's mind" by filling gaps in their knowledge.The theory emphasizes that shared knowledge between the explainer and the explainee are presuppositions of the explanations, and the other factors are the causes that should be explained.In short, the explainer should not explain any causes they think the explainee already knows.Abnormal conditions.This theory, based upon Hilton and Slugoski's abnormal conditions model (Hilton and Slugoski 1986), suggests that explanations often rely on unusual and temporally proximal events.People do not solely count on statistical likelihood but highlight uncommon factors contributing to an event to explain that event.During a conversation, the explainer is relying on the perceived common prior knowledge to identify potential causes that are considered abnormal, with greater weight given to temporally proximal events and factors that the explainee can control.This focus on controllable factors helps the explainee understand how to potentially avoid similar situations in the future.Pearl's Model of explanation.Halpern and Pearl (2005) present a formal framework for selecting explanations based on epistemic relevance and structural causal models.The model distinguishes between exogenous variables, whose values are determined by external factors, and endogenous variables, whose values are influenced by relationships with other variables.Within a context (a specific assignment of values to variables), the model defines an actual cause as a minimal set of events that must occur for an event to happen.Necessity &amp; Robustness selection.Two key criteria for selecting strong explanatory causes are necessity and robustness (Lipton 1990).Necessity refers to whether a cause is essential for the effect to occur.Robustness considers how generally a cause applies (Lombrozo 2010).This idea aligns with the concept of simplicity, where broader explanations with fewer specific requirements are favored.Contrastive Explanation.This theory suggests explanations are not simply cause and effect statements, but rather comparisons between what happened (the target event) and what could have happened (a counterfactual contrasting event) (Hilton 1990).One possible way to make the identification of the counterfactual event (foil) successful is to ask the module to reformulate the question.This technique is known as Rephrase and Respond (RaR) (Deng et al. 2024).Statistical Relevance.This method is based on the SR model based on scientific causal reasoning (Woodward and Ross 2021).The SR model explanations can be defined in simple terms as "statistically relevant properties are explanatory and statistically irrelevant properties are not".It follows this structure: "Based on empirical data, factors A, B and C contribute to the probability of Y by the amount of X".Chain-of-Thought (baseline).Chain-of-Thought (CoT) prompting (Wei et al. 2023) guides an LLM through sequential reasoning that mimics human thought processes.Evaluation.To assess whether iLLuMinaTE responses align with post-hoc explanations, student feature values, and course context, we developed an annotation rubric based on decomposed questions as a basis for human and LLM annotation.Recent studies (Wang et al. 2023) demonstrated that LLMs can match human annotators, especially when instructions are decomposed into simple criteria phrased as binary ("yes"/"no") questions (Qin et al. 2024).We created four general decomposed questions applicable across all theories and additional theory-specific questions (1 − 6, de-the explainer and the explainee are presuppositions of the explanations, and the other factors are the causes that should be explained.In short, the explainer should not explain any causes they think the explainee already knows.</p>
<p>Relevance-based selection prompt [1] Select the causes that are most relevant to the question, context and user [2] Select the causes that include information that is not already shared with the student oth [2] Se by •</p>
<p>•</p>
<p>Necessity lecting st ness (Lip</p>
<p>Abnormal conditions model prompt 1. Select potential causes using these criteria:</p>
<p>• Abnormality: Tend to prefer abnormal causes.</p>
<p>• Temporality: Recent events are more relevant for the user and considered more mutable.• Controllability: focus on the features that the student can control.</p>
<ol>
<li>Select one explanation that follows all of the criteria above (Abnormality, Temporality, Controllability).</li>
</ol>
<p>Pearl's Model of explanation.Halpern and Pearl (2005) present a formal framework for selecting explanations based on epistemic relevance and structural causal models.The model distinguishes between exogenous variables, whose values are determined by external factors, and endogenous variables, whose values are influenced by relationships with other variables.Within a context (a specific assignment of values to variables), the model defines an actual cause as a minimal set of events that must occur for an event to happen.</p>
<p>Necessity &amp; Robustness selection.Two key criteria for selecting strong explanatory causes are necessity and robustness (Lipton 1990).Necessity refers to whether a cause is essential for the effect to occur.Robustness considers how generally a cause applies (Lombrozo 2010).This idea aligns with the concept of simplicity, where broader explanations with fewer specific requirements are favored.</p>
<p>Contrastive Explanation.This theory suggests explanations are not simply cause and effect statements, but rather comparisons between what happened (the target event) and what could have happened (a counterfactual contrasting event) (Hilton 1990).One possible way to make the identification of the counterfactual event (foil) successful is to ask the module to reformulate the question.This technique is known as Rephrase and Respond (RaR) (Deng et al. 2024).</p>
<p>Statistical Relevance.This method is based on the SR model based on scientific causal reasoning (Woodward and Ross 2021).The SR model explanations can be defined in simple terms as "statistically relevant properties are explanatory and statistically irrelevant properties are not".It follows this structure: "Based on empirical data, factors A, B and C contribute to the probability of Y by the amount of X".Chain-of-Thought (baseline).Chain-of-Thought (CoT) prompting (Wei et al. 2023) guides an LLM through sequential reasoning that mimics human thought processes.Evaluation.To assess whether iLLuMinaTE responses align with post-hoc explanations, student feature values, and course context, we developed an annotation rubric based on decomposed questions as a basis for human and LLM annotation.Recent studies (Wang et al. 2023) demonstrated that LLMs can match human annotators, especially when instructions are decomposed into simple criteria phrased as binary ("yes"/"no") questions (Qin et al. 2024).We created four general decomposed questions applicable across all theories and additional theory-specific questions (1 6, depending on the theory).These were validated by a compu-Releva tional relevan the exp ing a ga The the exp explana be expl causes
Rele [1] S t [2] S n
Explanation selection template You are an AI assistant that analyzes struggling students behavior to help them in their learning trajectories and facilitate learning in the best possible way.You have the following information to help you in your goal:</p>
<p>• A model prediction of student performance at the end of the course, in the form of "pass" or "fail".• A post-hoc explainable AI approach that identifies which features are important to this student's prediction.• Data in the form of student's features over 5 weeks that were used by the model.You will see always the most relevant features selected by {explainer}.</p>
<p>INSTRUCTIONS: {instructions}</p>
<p>QUESTION: Given the information above, follow the instructions precisely and write a small report on what you found.Only use the results from the explainable AI approach and the student's behavior data to justify your conclusions.</p>
<p>Relevance-based selection.Based on Hilton's conversational model of explanation (Hilton 1990), the goal of relevance-based selection theory is to "resolve a puzzle in the explainee's mind about why the event happened by closing a gap in his or her knowledge".</p>
<p>The theory emphasizes that shared knowledge between the explainer and the explainee are presuppositions of the explanations, and the other factors are the causes that should be explained.In short, the explainer should not explain any causes they think the explainee already knows.</p>
<p>Relevance-based selection prompt [1] Select the causes that are most relevant to the question, context and user [2] Select the causes that include information that is not already shared with the student pending on the theory).These were validated by a computational social scientist who was not involved in the prompt creation process.The general questions used items such as 'Is the generated text correctly using the model's predicted outcome?' or 'Is the generated text analysis based solely on the explainer results provided?'.An example of a theoryspecific questions is: 'Is the generated text selecting the causes that are most relevant to the user?' (RS).The complete rubric is available in Appendix Table ??.</p>
<p>We instructed three experts to annotate the same block of 42 responses (2 students, 3 explainers, 8 theories) across a set of up to eight decomposed questions for each setting, resulting in an inter-rater agreement of κ = 0.71 ± 0.13 (Cohen's Kappa).After that, each annotator proceeded with independently evaluating between 84 to 105 more responses, leading to a total of 315 human-annotated responses and in total over 2, 350 annotations.We then proceeded with instructing GPT-4o to annotate the same 42 responses using the same decomposed questions.We obtained an agreement of 94.68% ± 5.20 (percentage of answers with agreement) between human and GPT-4o annotation.The interrater agreement per theory is in Appendix Table 4.Given the high agreement, we annotated all explanations by GPT-4o.</p>
<p>Explanation Presentation</p>
<p>Inspired by Hilton's conversational model (Hilton 1990), we employ a presentation prompt to refine and condense the output of the explanation selection prompt to make it relevant to the explainee's future actions, current context and prior knowledge.</p>
<p>This prompt is formulated using Grice's maxims on communication (1975) and learning science literature on best practices in communicating feedback to students (Shute 2008;Hattie and Timperley 2007).Grice's maxims provide a framework for understanding cooperative conversation, emphasizing providing the right amount of relevant and clear information to achieve a shared goal.Grice further divides this principle into four maxims: Quality (truthfulness and evidence-based statements), Quantity (providing enough but not excessive information), Relation (relevance), and Manner (clear and concise communication).Hattie and Timperley (2007) provide a prominent educational feedback framework with three steps: "Where am I going?", "How am I doing?"2 , and "Where to next?"These steps require the feedback provider to clearly state the learning goal, provide a summary of relevant student performance, and suggest concrete actions for improvement in the near future.The full explanation presentation prompt is included in the Appendix 7.2.Evaluation.Similar to the explanation selection phase, we evaluated the final explanations using a rubric of decomposed 'yes'/'no' questions (the detailed set of questions is illustrated in Appendix Table 5).Given the high agreement between GPT-4o and human annotations in the explanation selection phase, we used GPT-4o as an expert.Additionally, we conducted a readability evaluation using the following metrics: Flesch-Kincaid Grade Level (Flesch 1948), Gunning Fog Index (Gunning 1952), SMOG index (Mc Laughlin 1969), and LanguageTool Grammar Issues (Mozgovoy 2011).These metrics evaluate the comprehensibility of the text in terms of sentence length, vocabulary complexity, grammatical correctness, and estimated years of schooling needed for understanding.</p>
<p>Student Preferences (User Study)</p>
<p>To evaluate students' explanation preferences as well as the actionability of iLLuMinaTE explanations, we conducted a user study comparing our explanations (text and visual) to post-hoc baselines.We recruited 114 students on Prolific3 , (see Appendix 9.1 for detailed information about the participants' demographics).Students were told that the explanations related to their own performance in three different online courses they were enrolled in.</p>
<p>For each course, we presented participants with eight explanations on their predicted success or failure in that course: four explanations at a time, with three randomly ordered iLLuMinaTE variations and one baseline approach.We selected to use the six iLLuMinaTE instructions with the highest instruction-following accuracy from the humanexpert evaluation for this experiment, and an equal mixture of passing and failing behavior at different model confidence levels.Each explanation was provided as a brief text accompanied by a graph illustrating the features and concepts used by the model.GPT-4o created all iLLuMinaTE visuals based on the first two responses (full prompt in Appendix 7.3).LIME visualizations were used from the package, and CEM and MC-LIME visualizations were expert-created and iterated upon with six pilot participants.Examples of the presentation format are included in our repository4 and Appendix 9.2.Participants were asked to choose their preferred explanation for each set of comparisons and explain their choice (open-ended question).They were then asked to compare the explanations based on five criteria (Frej et al. 2024):</p>
<ol>
<li>Usefulness: This explanation is useful to understand the prediction based on my learning behavior.2. Trustworthiness: This explanation lets me judge if I should trust the suggestions.3. Actionability: This explanation helps me make a decision on how to improve my learning behavior.4. Completeness: This explanation has sufficient detail to understand why the prediction was made based on my learning behavior.5. Conciseness: Every detail of this explanation is necessary.</li>
</ol>
<p>Finally, participants were asked to choose one of ten suggested actions for the next week, based on their preferred explanation.These actions were aligned with behavioral features from the model, allowing us to simulate the impact if the student acted according to them.We trained a BiLSTM to predict student success on six weeks of student data, and conducted inference on simulated students increasing the relevant features by 25% percentile.We also asked students which weeks of material they would focus on based on the explanation (between one to three weeks).</p>
<p>Results</p>
<p>We evaluated iLLuMinaTE explanations by assessing the LLM's instruction-following abilities during explanation selection (Exp 1) and using readability metrics and an automated analysis to measure explanation understandability at the presentation stage (Exp 2).We then analyzed student preferences in the user study (Exp 3) and simulated whether actions derived from the explanations improved student performance (Exp 4).Experimental Protocol.We optimized the BiLSTM models (one for each course) using a train-validation-test split of 80:10:10 and including a hyperparameter search.We achieve balanced accuracies of 90.8 (DSP), 80.3 (Afr), and 76.8 (Geo) respectively.These results for early student performance prediction at five weeks are in line with prior work (Swamy, Marras, and Käser 2022).Explainers (LIME, CEM) were extracted with the same settings as per related work to ensure a fair comparison (Swamy et al. 2022(Swamy et al. , 2023)).</p>
<p>Exp 1: iLLuMinaTE is aligned with social science theories of explanation</p>
<p>In the first analysis, we evaluated how well the generated explanations aligned with the instructions.We selected 105 representative students per course, distributed across six behavioral dimensions (regularity, effort, consistency, proactivity, control, and assessment) (Mejia-Domenzain et al. 2022).Table 1 shows the average (with standard deviation) scores for each general decomposed question (see Section 2.3 and Appendix 8), for the theory-specific questions, and overall.The general decomposed questions involve whether the response is using the provided data extensively (Q1), whether the analysis in the response is based solely on the results from the explainer (Q2), whether the response is correctly using the model's prediction (Q3), and whether the response is using the course content and structure.The scores represent the ratio of 'Yes' answers averaged across students and courses, with overall scores exceeding 0.82 for all explainers and theories.There were no significant differences between explainers or theories, as indicated by overlapping 95% CIs.There were generally also no differences in scores between questions.Only Q4 ("Using course content and structure") had lower average scores than the other questions, suggesting that generating explanations fully incorporating the course content is a challenge.Notably, inter-rater agreement for Q4 was also lower than the average (κ = 0.52 vs. κ = 0.71 overall), indicating lower reliability of annotations for this question.</p>
<p>In a second analysis, we compared different LLMs' abilities to generate explanations according to instructed theories.Table 2 (column Explanation Selection) shows the average scores for each model and explainer for the same representative students.Scores reflect the number of 'Yes' answers to the decomposed questions described in Section 2.4, with annotation done automatically using GPT-4o.GPT-4o iLLuMinaTE explanations scored the highest, closely followed by Gemma2 9b, and Llama3 70b.However, all of the 95% confidence overlap.Again, we found no differences between the different explainers.</p>
<p>Exp 2: iLLuMinaTE explanations are understandable</p>
<p>We assessed the presentation of iLLuMinaTE explanations using both LLM annotation and readability metrics.Table 2 (column Explanation Presentation) shows scores per LLM and explainer, averaged over 300 representative students (see Section 3.1).Scores represent the ratio of "Yes" answers to the set of decomposed questions described in Section 2.4, as annotated by GPT-4o.For this stage, Llama3 70b achieved the highest scores, followed closely by GPT-4o and Gemma2 9b.This finding is in contrast to the explanation selection stage, where GPT-4o reached the highest score, suggesting that depending on the use case (whether selection or presentation is more important), it is possible to use much smaller and open source models for the task at hand. Figure 2 illustrates the readability scores (Flesch Kincaid, SMOG Index, Gunning Fog, Grammar Issues) of the final iLLuMinaTE explanations after the explanation presentation prompt.Lower scores are better for all metrics.GPT-4o reached the best readability performance of the three LLMs (Fig 2,top), with no overlap in 95% confidence intervals.In contrast, Llama3 70b committed the least grammatical errors.We found no differences in readabilty or grammar between explanations for the different courses Fig 2,middle), demonstrating that our approach is generalizable to diverse educational contexts.Similarly, all explainers achieved similar scores across all four metrics Fig 2,bottom), showing that choice of source explainer does not have a strong impact on explanation understandability.</p>
<p>Exp 3: Students prefer iLLuMinaTE explanations</p>
<p>For each explanation in the user study, participants were asked to indicate their preferred explanation variation be-
CEM LIME MC-LIME DSP Geo VA Gemma2 GPT4O Llama3
Flesch We evaluated students' responses to five Likert-scale questions on the usefulness, trustworthiness, actionability, completeness, and conciseness of explanations.Figure 4 shows the Likert score distribution (1 -5) for two criteria per method; full results are in Appendix 9.4.Theory-based explanations received consistently high scores with no significant differences between theories.Base explanations were rated substantially lower (e.g., Usefulness contrastive = 4.18, Usefulness BASE = 3.59), highlighting the superior usefulness of iLLuMinaTE explanations.
Kincaid Gunning Fog SMOG Index Grammar Issues</p>
<p>Exp 4: iLLuMinaTE explanations can be effective at improving student performance</p>
<p>Participants were asked to choose an action for the next week based on their preferred explanation.Over all explainers and theories, students most frequently selected actions to improve regularity of learning (200 responses) and attempt more problems (147 responses), while the least chosen action was to speed up quiz solving (10 responses  1: Alignment of GPT-4o, Gemma2 9b, and Llama3 70B generated explanations with theory.Average (± std) of "Yes" answers, displayed separately for the first four general decomposed questions as well as averaged over the theory-specific questions and overall.Annotated by experts and GPT-4o.Scores over 95% (less than 65%) are highlighted in green (red).which correspond to the weeks directly after the intervention.These were followed by review in weeks 5 (222 responses) and 4 (198 responses).This observation indicates that timing and proximity to the intervention influenced their choices.We also conducted a simulation experiment, applying participants' actions chosen for week 6 to student behavior in that week.For participants preferring iLLuMinaTE responses, average performance improved significantly, in- dependent of the underlying explainer: 13.5% for LIME, 14.2% for CEM, and 20.7% for MC-LIME (full results in Appendix 10).MC-LIME's effectiveness may stem from its minimal counterfactual approach, which greedily searches for the smallest set of features that cause the prediction to flip, making it suited for a single-or few-action intervention.Across different theories, both Contrastive and Necessity Robustness explanations result in the most actionable interventions with 28.2% and 24.9% average performance improvement respectively (Appendix Fig. 22).</p>
<p>Model</p>
<p>work leveraging LLMs for generating NLEs through a chain of prompts, consisting of causal connection, explanation selection, and explanation presentation.</p>
<p>We tested our framework on 315 students (105 per course) with all combinations of three post-hoc explainers, three LLMs, and eight prompting strategies, resulting in 21, 915 generated NLEs.We evaluated the instruction-following abilities using GPT-4o and human expert annotation, with both decomposed questions and readability metrics.In a user study with 114 university students, we found that students significantly preferred iLLuMinaTE explanations over LIME, CEM, and MC-LIME and were able to derive actions that could improve their performance.</p>
<p>Several challenges remain, including the variability of results depending on the explainer and the difficulty of evaluating NLEs independently of prior beliefs and knowledge.In future work, we aim to explore the LLM advantage of interactive explanation conversations with students.Our study highlights the potential of leveraging LLMs and social science theories to improve predictive models in education, contributing to scalable, personalized student support.</p>
<p>All code to replicate our student models and explainability results are provided in our repository: https://github.com/epfl-ml4ed/iLLuMinaTE.Any details not covered in the following reproducibility notes can also be found directly in the repository.Student Model.We input 45 student features in line with the exact extraction procedure from (Marras, Vignoud, and Käser 2021; Swamy, Marras, and Käser 2022) into a neural network architecture consisting of a Masking layer (with a mask value of −1), followed by two Bidirectional LSTM (BiLSTM) layers with 64 and 32 units, respectively, and a loopback of 3. The final layer is a Dense layer with a Sigmoid activation function, which has a single output unit.This model predicts the probability that a student will fail the course.Swamy, Marras, and Käser (2022) find that a BiLSTM architecture outperforms both traditional machine learning models (e.g., Support Vector Machines, Logistic Regression, Random Forest) and other deep learning models (e.g., Dense Fully-Connected Networks, RNNs, LSTMs, CNNs, and BiLSTMs) in the context of student performance prediction for online courses.They report a 26.8% increase over traditional models (as measured by validation set for the DSP course).In our grid search for the best BiLSTM architecture, we tested various hidden size configurations, including {32, 64, 128, 256, 32-32, 64-32, 128-32, 128-64, 128-128, 256-128, 64-64-32, 128-64-32, 64-64-64-32-32}.The 64-32 configuration achieved the highest balanced accuracy for predicting outcomes in the DSP 1 course.We used the TensorFlow library (Abadi et al. 2016) to train our models.Model training took approximately 15 minutes per model on a c2-standard-8 machine on Google Cloud (Linux Debian OS, 8vCPUs, 32 GB of RAM, 150 GB of disk).Each model was trained for 20 epochs, and the best performing model checkpoint was saved.LIME.The explanation highlights up to the 20 most influential features.The mode is set to 'classification', indicating that the function is tailored for the binary student performance prediction task.discretize continuous is True, meaning continuous features will be discretized to make the explanations more interpretable.The num samples parameter is set to 5000, specifying the number of perturbed samples LIME will generate to approximate the local decision boundary of the model.The distance metric is set to 'euclidean', which is used to measure the distance between the original instance and the perturbed samples.Lastly, the sampling method is set to 'gaussian', indicating that the perturbed samples will be drawn from a Gaussian distribution.The LIME setting is the exact same as used in Swamy et al. (2022Swamy et al. ( , 2023)), but is expanded to select more features than the default (20 instead of 10) to not over-narrow the search space for MC-LIME.MC-LIME.The init processing function preprocesses the LIME results dataset by extracting feature names and reformatting the DataFrame.The calculate std dev step sizes function computes step sizes for feature modification based on a fraction of each feature's standard deviation, ensuring that changes applied to features during MC-LIME are proportionate.In the get lime results function, LIME important features and corresponding student-specific features are extracted, and step sizes are calculated to assess the sensitivity of model predictions.The MC LIME function iteratively modifies student features in small steps (guided by calculated step sizes) within specified bounds (0 to 1) to determine the minimal set of feature changes that could alter the model's prediction.The key hyperparameters in this procedure include the fraction of standard deviation used for step sizes, a prediction threshold (defaulting to 0.5), and the maximum group size for feature combinations (defaulting to 3), which controls the complexity and granularity of the search for influential features.The script ensures that feature modifications are meaningful and within realistic bounds, aiming to identify the minimal changes required to change a student's predicted outcome.CEM.The pn all function generates pertinent negative (PN) explanations for a set of instances using the Contrastive Explanation Method (CEM) applied to our underlying BiLSTM model.The function is configured with several key hyperparameters: kappa (set to 0.0) controls the minimum probability difference needed to minimize the first loss term, beta (0.1) weights the L1 loss term, and gamma (100) weights the optional auto-encoder loss term.The initial weight c init (1.0) influences the loss term that encourages a prediction different from the original class, with c steps (10) updates and max iterations (1000) iterations per value of c.The feature range is set based on the minimum and maximum values of the features, and gradient clipping is applied within the range clip (-1000.0,1000.0).The learning rate lr is initialized to 0.01, and no info val is set to -1.0, representing a non-informative value for prediction.The function iterates over the specified instances, reshaping the features and generating explanations through CEM, with results stored in changes, explanations, and final num instances.Errors during processing are handled and logged, ensuring robustness in explanation generation.The CEM setting is the exact same as used in Swamy et al. (2022Swamy et al. ( , 2023)).LLMs.We use three LLMs through these experiments: GPT-4o (gpt-4o-2024-05-13), Gemma2 9b (google/gemma-2-9b-it, July 2024), and Llama3 70B (meta/meta-llama-3-70b-instruct, April 18 2024 release).We use the instruct variations of each of these models, using the OpenAI API5 for GPT-4o and Groq Cloud6 for Gemma2 and Replicate7 for Llama3.The visualizations for the user study (Sec.2.5, Appendix 9.2) were generated through the ChatGPT user interface with underlying GPT-4o.It can also be generated through running LLM code snippets, but we chose to use the ChatGPT interface to remain closest to an actual teacher's workflow (using models as presented with a zero-shot chain-of-prompts, ensuring minimal post-processing steps).</p>
<p>Dataset.Our objective is to predict student success during the initial weeks of three massive open online courses (MOOCs) by analyzing students' clickstream data.The log data collected from students includes detailed interactions with videos (such as play, pause, fast forward, and seek actions) as well as quiz-related events (such as submissions).To ensure the protection of student privacy, all student data is fully anonymized and the dataset is kept confidential, as required by the ethical guidelines outlined in HREC 058-2020058- /10.09.2020058- and HREC 096-2020058- /09.04.2022. .Formally, given the interactions I s generated by students S up until course week w, we construct a matrix H ⊂ R |S|×w×f .This indicates that each feature in the feature set is computed for each student on a weekly basis, where f ∈ N represents the dimensionality of the feature set.We extract these features for each student s and concatenate the feature sets to produce the final combined behavioral feature vector h s for each student.The complete matrix of features is defined as
H ∈ R |S|×w×42 , with H = [H 1 • H 2 • H 3 • H 4 ]
(where • represents concatenation).Due to the varying scales of the features, we apply min-max normalization to each feature in H, scaling them between 0 and 1, considering all students and weeks for that feature.The appendix table in (Swamy, Marras, and Käser 2022) lists all the features used in this dataset.Detailed statistics for each course can be found in Table 3.</p>
<p>Title</p>
<p>Identifier</p>
<p>Prompting Framework</p>
<p>In this Appendix, we present the explanation selection prompts (Sec.2.3), explanation presentation prompts (Sec.2.4), and explanation visualization prompts (used for the hybrid explanation user study in Sec.2.5).We then discuss the development procedure of these prompts, and the exact frameworks and models used such that the results can be replicated.</p>
<p>Explanation Selection Prompts</p>
<p>The explanation selection template structure is highlighted in Section 2.3, and detailed again here.We provide an example of the model description, features description, explainer description (CEM), and course description (DSP 1).The details of the theory instructions are included below for each prompting strategy.</p>
<p>Explanation selection template You are an AI assistant that analyzes struggling students behavior to help them in their learning trajectories and facilitate learning in the best possible way.You have the following information to help you in your goal:</p>
<p>• A model prediction of student performance at the end of the course, in the form of "pass" or "fail".</p>
<p>• A post-hoc explainable AI approach that identifies which features are important to this student's prediction.</p>
<p>• Data in the form of student's features over 5 weeks that were used by the model.You will see always the most relevant features selected by {explainer} CEM.• Data in the form of student's features over 5 weeks that were used by the model.You will see always the most relevant features selected by {explainer} CEM.• The course {course name} Digital Signal Processing 1 content and structure.</p>
<p>• Detailed instructions on how to reason.</p>
<p>{model description} The model you are using is a recurrent neural network that is trained on predicting the student performance at the end of the course, in the form of "pass" or "fail".The features of that the model are using are derived from student behavior:
{explainer description}
We use CEM Counterfactuals as our explainable AI approach, which finds the smallest number of changes necessary to change a prediction from student failure to student success.The output is the minimal difference in the feature values that would change the prediction.</p>
<p>{course description}</p>
<p>The course the student is taking is Digital Signal Processing 1, which is a Master's level course over 10 weeks under the topic of Electrical Engineering and Computer Science.This is the course content:</p>
<p>Evaluation Prompts</p>
<p>LLM-as-a-judge prompt Based on the provided Input (if any) and Generated Text, answer the ensuing Questions with either a YES or NO choice.Your selection should be based on your judgment as well as the following rules:</p>
<p>• YES: Select YES if the generated text entirely fulfills the condition specified in the question.However, note that even minor inaccuracies exclude the text from receiving a YES rating.As an illustration, consider a question that asks, "Does each sentence in the generated text use a second person?"If even one sentence does not use the second person, the answer should NOT be 'YES'.To qualify for a YES rating, the generated text must be entirely accurate and relevant to the question.• NO: Opt for NO if the generated text fails to meet the question's requirements or provides no information that could be utilized to answer the question.For instance, if the question asks, "Is the second sentence in the generated text a compound sentence?"and the generated text only has one sentence, it offers no relevant information to answer the question.Consequently, the answer should be NO.2024), we use GPT-4o as a annotator.Table 4 compares the human expert annotation with GPT-4o annotation, finding an average agreement of 94.68%.The only type of explanation where the agreement is less that 90% is for the Base Contrastive explanations (82.22%, with a high standard deviation), where we found that GPT-4o is a lot more critical than human annotators.As the bias is on the side of GPT-4o grading more harshly (especially for the fourth question regarding context, which is the most difficult question to annotate) we found it appropriate to proceed.</p>
<p>QUESTIONS: {question}</p>
<p>Explanation Selection Stage</p>
<p>We detail the decomposed questions (DQs) used by GPT-4o and human experts to annotate the explanation responses.</p>
<p>Theory Explanation Selection DQs</p>
<p>Relevance Selection</p>
<p>• Is the generated text using the provided data extensively?</p>
<p>• Is the generated text analysis based largely on the explainer results provided?• Is the generated text correctly using the model's predicted outcome?• Is the generated text considering the context (course structure)?</p>
<p>• Is the generated text selecting the causes that are most relevant to the explainer results?• Is the generated text selecting the causes that are most relevant to the question?• Is the generated text selecting the causes that are most relevant to the user?• Is the generated text selecting some information that is not already shared with the student?</p>
<p>Abnormal Conditions • Is the generated text using the provided data extensively?</p>
<p>• Is the generated text analysis based largely on the explainer results provided?• Is the generated text correctly using the model's predicted outcome?</p>
<p>• Is the generated text considering the context (course structure)?</p>
<p>• Is the generated text selecting the causes based on the abnormality of the causes?• Is the generated text selecting the causes based on the temporality of the causes?• Is the generated text selecting the causes based on the controllability of the causes?• Is the selected explanation following the criteria of Abnormality, Temporality, Controllability?</p>
<p>Pearl Explanation</p>
<p>• Is the generated text using the provided data extensively?</p>
<p>• Is the generated text analysis based largely on the explainer results provided?• Is the generated text correctly using the model's predicted outcome?• Is the generated text considering the context (course structure)?</p>
<p>• Is the generated text identifying variables that are determined by factors external to the model?• Is the generated text identifying variables that are determined by relationships with other variables?• Is the explanation selected containing only a set of factors that is minimal?• Is the explanation considering variables that could potentially alter the model output?</p>
<p>Theory</p>
<p>Explanation Selection DQs Necessity Robustness Selection • Is the generated text using the provided data extensively?</p>
<p>• Is the generated text analysis based largely on the explainer results provided?• Is the generated text correctly using the model's predicted outcome?• Is the generated text considering the context (course structure)?</p>
<p>• Is the generated text selecting features that explain the model's output?• Is the generated text selecting a few of these sets of factors by considering the necessity criteria?• Is the generated text selecting the most robust explanation?</p>
<p>Contrastive Explanation</p>
<p>• Is the generated text using the provided data extensively?</p>
<p>• Is the generated text analysis based largely on the explainer results provided?• Is the generated text correctly using the model's predicted outcome?</p>
<p>• Is the generated text considering the context (course structure)?</p>
<p>• Is the generated text reformulating the question in a "Why P rather than Q?" format?• Is the generated text selecting the event P as the target event?</p>
<p>• Is the generated text selecting the event Q as the counterfactual event that did not occur?• Is the generated text selecting the foil based on abnormality to the context, question, and user?• Is the generated text selecting the foil based on relevance to the context, question, and user?</p>
<p>Base Contrastive Explanation</p>
<p>• Is the generated text using the provided data extensively?</p>
<p>• Is the generated text based only on the explainer results provided?</p>
<p>• Is the generated text correctly using the model's predicted outcome?</p>
<p>• Is the generated text considering the context (course structure)?</p>
<p>• Is the generated text reformulating the question in a "Why P rather than Q?" format?• Is the generated text determining the causes that explain the difference between P and not-Q?• Is the generated text reasoning about which causes are a cause of P? • Is the generated text reasoning about causes that aren't present in the causal chain of Q?</p>
<p>Statistical Relevance</p>
<p>• Is the generated text using the provided data extensively?</p>
<p>• Is the generated text analysis based largely on the explainer results provided?• Is the generated text correctly using the model's predicted outcome?• Is the generated text considering the context (course structure)?</p>
<p>• Is the generated text selecting the explanation based on likelihood?</p>
<p>Theory Explanation Selection DQs</p>
<p>Chain of Thought</p>
<p>• Is the generated text using the provided data extensively?</p>
<p>• Is the generated text analysis based largely on the explainer results provided?• Is the generated text correctly using the model's predicted outcome?• Is the generated text considering the context (course structure)?</p>
<p>• Is the generated text selecting the explanation by step-by-step tasks?</p>
<p>Explanation Evaluation</p>
<p>Across both explanation selection and presentation, the decomposed questions were generated and revised by two authors (who are well familiar with LLM prompting and explainability theories), then individually vetted by a computational social scientist.For explanation selection, the question "Is the generated text considering the context (course structure)?" was iterated upon several times to find non-subjective wording.For explanation presentation, the question "Is the generated text concise (readable within 5 minutes)?"was similarly iterated upon.Both of these questions are educational context dependent, and the phrasing inside the parentheses can be adapted to relate to tasks from different domains.</p>
<p>Student Preference Study</p>
<p>Background of Participants</p>
<p>We recruit 114 participants using Prolific,8 , selecting the ones who identified their current status as a student and are over 18 years old.As our target participants have expertise with being a student after high school, they would be well-suited to understand both the educational context of the study and individually evaluate what explanations are most useful for them.During the study, we ask the participants whether they have ever taken an online course (MOOC), their level of education, and their level of ease with coursework.Detailed demographics distribution can be found in Figure 11.The sample of participants is gender-balanced, and about half of them have taken or participated in creating a MOOC.The median completion time is 41 minutes, and the average reward per hour is £9.00.</p>
<p>Study Materials</p>
<p>In this section, we provide a detailed overview of the user study introduced in Section 2.5, covering its design, content, and further analyses of the results.The study was developed over two pilot rounds, involving 6 participants from diverse backgrounds, with continuous and iterative revisions made based on their feedback.The survey received approval from the Human Research Ethics Committee (HREC) under application number HREC 065-2022/27.09.2022.At the beginning of the study, the participants are shown the following welcome message and disclaimer: Dear participant, Thank you for participating in our study on model explanations.We are very grateful for your participation and your invaluable insight.Please read this Explanatory Statement in full before moving forward.If you would like further information regarding any aspect of this project, please contact us via the email address provided below.</p>
<p>We are a group of researchers from the ML4ED Laboratory at EPFL dedicated to improving education through technology.The goal of this study is to evaluate different explanations to help a student improve their performance in an online course.</p>
<p>-This survey has been approved by the EPFL Human Research Ethics Committee (HREC) under application number HREC 065-2022/27.09.2022.HREC reviews research proposals involving human participants to ensure that they are ethically acceptable.</p>
<p>-All the personal information will be kept confidential and anonymized.Only demographic information is being recorded and will only be reported as aggregate in a way that prevents identification of any individual participant.You can freely withdraw at any time and any collected data you provided so far will be destroyed.</p>
<p>-All data will be collected and stored safely and reported in an anonymous form, in accordance with the Swiss Federal law on data protection ("Loi fédérale sur la protection des données" -RS 235.1).</p>
<p>-Only anonymized or aggregated data may be used in follow-up research (subject to ethics approval), and made available to other researchers for further analysis and for verification of the conclusions reached by the research team.</p>
<p>-Only the principal investigator and the aforementioned researchers have access to the original data under strict confidentiality.Results from the project may be published in conference papers and/or journal articles.In any case, no personal data will be published (only aggregated, anonymous and/or anonymized data will be published).</p>
<p>-Personal data of participants will be stored for 5 years from the date of collection.During this time, participants have the right to access their data and request information about the processing of their personal data.In order to exercise this right, you need to contact the Principal Investigator.</p>
<p>By participating in this survey, you agree that your data can be used for scientific purposes.</p>
<p>In the following study, you will be asked to compare explanations for approximately 20 minutes.Please ensure that you have enough time to finish the study correctly.Unfinished or only partially answered studies will not be considered as taken part.</p>
<p>We ask you to approach the questions and exercises with seriousness and to complete them to the best of your ability.We will subsequently check questionnaires for seriousness and will have to discard questionnaires that do not meet this requirement.</p>
<p>Thank you for your help.If you encounter any problem with the survey, or if you want to give extra feedback, or receive additional information, feel free to contact us (vinitra.swamy@epfl.ch).</p>
<p>Content of the study</p>
<p>First, we explain the setting of the study to the participants with the following introductory message: You are a student taking three online courses (MOOCs): Digital Signal Processing, African Cities, and Elements of Geometry.Since the courses are difficult, often with low passing rates, the teaching team wants to help students who are not doing well to perform better in the course by giving them personalized assistance, and encourage students who are already performing well to continue.</p>
<p>To do this, we have a very good model (over 90% accurate) to predict students' success or failure using various weekly behavior features (such as number of video clicks or how accurately questions are answered on the weekly quizzes).We predict student performance early in the course (before the half-way point) as passing or failing behavior.We use the explanation of the prediction to give students additional, personalized feedback to help pass the course.</p>
<p>We want to compare these personalized feedback explanations according to several criteria:</p>
<p>• Usefulness: This explanation is useful to understand the prediction based on my learning behavior.</p>
<p>• Trustworthiness: This explanation lets me judge if I should trust the suggestions.</p>
<p>• Actionability: This explanation helps me make a decision on how to improve my learning behavior.</p>
<p>• Completeness: This explanation has sufficient detail to understand why the prediction was made based on my learning behavior.• Conciseness: Every detail of this explanation is necessary.</p>
<p>We randomly sample 6 students' explanation responses, 2 from each course, and one passing and failing student for each course.For each student, we predict their success or failure with each model and generate an explanation with LIME, MC-LIME, and CEM, then feed it through the iLLuMinaTE pipeline for explanation selection, presentation and visualization.We provide them to the participants.The ground truth (student's performance) and the models' exact performance are not provided to the participants so that we do not bias their assessment.</p>
<p>The content of the explanation obtained by each method differs greatly.We simplify the explanations and render them in textual and graph format to make them as easy to understand as possible to a human.For baseline explanations (LIME, CEM, MC-LIME), we craft a text template in line with Swamy et al. (2024a) and iterate on the wording with 2 learning scientists.We provide the description and list of features found important by the model and suggest that improving on these attributes can improve the student performance.For LIME, we show the default graphic that comes with the explanation (Fig. 18).For CEM, we visualize all features found important across the 5 weeks of student progress in a heatmap (Fig. 20), and describe the five most important features in the textual explanation.For MC-LIME, we present only the most concise counterfactual set found important based on the LIME scores and show how much those features need to change to impact the prediction (Fig. 19).The remaining figures for all iLLuMinaTE explanations are generated directly by GPT-4o, often choosing a line graph, a heatmap, or a bar plot that is directly related to the importance scores or feature values offered in the explanation.Several examples of iLLuMinaTE plots are included in Fig. 12,13,14,15,16,17.Consider that the way we chose to present the explanations may influence participants' perceptions.Ideally, we would offer a comprehensive breakdown of how each model utilizes features and how the explanations are generated, allowing participants to fairly evaluate the explanation's quality.However, in a practical setting, the user encountering the explanation may lack the time or background knowledge to fully grasp these details.To address the trade-off between thoroughness and accessibility, we decided on a balanced approach, presenting each explanation with a simple graph and a concise explanatory text.</p>
<p>For each student sample, participants were presented with four model explanations in a random order.They were then required to evaluate these explanations using Likert scales based on five predefined criteria.To familiarize participants with the evaluation process and to screen for inattentive responses, a practice question was included.Data from three participants who did not correctly answer the practice question were excluded from the final analysis.A screenshot of the answer section can be seen in Figure 21.</p>
<p>All study materials can be found in the iLLuMinaTE repository: https://github.com/epfl-ml4ed/iLLuMinaTE/study-materials/, with the exact versions of the survey shown to students.There are six versions of the survey: two variations with 3 randomly chosen students each (one from each course) and three different source explainer versions (LIME, MC-LIME, CEM) for each student variation.</p>
<p>Extended Results</p>
<p>The extended results of student preferences over five criteria (two of which are included in Fig. 4) is explored in Fig. 23.Violin plots are chosen to reflect the distributions of student rankings of the criteria.Students also ranked the criteria by what  they found most important (lower ranks are more important), as reflected in Fig. 24.We note that there is a clear ordering from the student perspective over explainability criteria: usefulness, actionability, trustworthiness and then at a similar level completeness and conciseness.</p>
<p>Actionability Experiments</p>
<p>The following experiments indicate the results of the actionability simulation (with standard deviations) over explainers and theories.We note that MC-LIME is the most performant explainer, and contrastive explanations, closely followed by necessity-</p>
<p>Figure 2 :Figure 3 :
23
Figure 2: Readability metrics.Fleach Kincaid, Gunning Fog, SMOG Index, Grammar Issues across LLM (blue, top), course (green, middle), and explainer (orange, bottom).Lower scores are better.</p>
<p>Table 2 :
2
± 0.302 0.997 ± 0.042 MC-LIME 0.881 ± 0.258 0.998 ± 0.037 Explanation quality by LLM.Degree of instruction-following for explanations generated by GPT-4o, Gemma2 9b, and Llama3 70b.Average (with standard deviation) of "Yes" answers, annoted by GPT-4o.</p>
<p>Figure 4 :
4
Figure 4: Comparison of student preferences across two dimensions.Distribution of Likert scores for Usefulness and Actionability, averaged over all participants and explainers.</p>
<p>to the DSP course, Introduction to signal processing WEEK 2 SKILLS: Digital Signals TOPICS: Discrete time signals, The complex exponential, The Karplus-Strong Algorithm WEEK 3 SKILLS: Hilbert (Linear Alg.) TOPICS: Motivation and Examples, From Euclid to Hilbert, Hilbert Space, properties and bases, Hilbert Space and approximation WEEK 4 SKILLS: DFT, DTFT DFS, FFT TOPICS: Exploration via a change of basis, The Discrete Fourier Transform (DFT), DFT Examples, DFT, DFS, DTFT, DTFT formalism, Relationship between transforms, Sinusoidal modulation, FFT: history and algorithms WEEK 5 SKILLS: Ideal Filters, Filter Design TOPICS: Linear Filters, Filtering by example, Filter stability, Frequency response, Ideal filters, Filter design -Part 1: Approximation of ideal filters, Realizable filters, Implementation of digital filters, Filter design -Part 2: Intuitive filters, Filter design -Part 3: Design from specs, Real-time processing, Dereverberation and echo cancelation WEEK 6 SKILLS: Modulation, Interpolation &amp; Sampling TOPICS: Introduction to continuous-time paradigm, Interpolation, The space of bandlimited signals, Sampling and aliasing: Introduction, Sampling and aliasing, Discrete-time processing and continuous-time signals, Another example of sampled acquisition WEEK 7 SKILLS: Multirate TOPICS: Stochastic signal processing, Quantization, A/D and D/A conversion WEEK 8 SKILLS: DFT, DTFT DFS, Ideal Filters TOPICS: (Revisiting the topics of week 4 with additional context) Image processing, Image manipulations, Frequency analysis, Image filtering, Image compression, The JPEG compression algorithm WEEK 9 SKILLS: Modulation, Quantization TOPICS: Digital communication systems, Controlling the bandwidth, Controlling the power, Modulation and demodulation, Receiver design, ADSL WEEK 10 SKILLS: Applications TOPICS: The end, Startups and DSP, Acknowledgements, Conclusion video</p>
<p>Figure 5 :
5
Figure 5: A iLLuMinaTE GPT-4o visual explanation for a sample student with the LIME explainer and the abnormal conditions instruction.</p>
<p>Figure 6 :
6
Figure 6: A iLLuMinaTE GPT-4o visual explanation for a sample student with the LIME explainer and the chain-ofthought instruction.</p>
<p>Figure 7 :
7
Figure 7: A iLLuMinaTE GPT-4o visual explanation for a sample student with the CEM explainer and the contrastive explanation instruction.</p>
<p>Figure 8 :
8
Figure 8: An iLLuMinaTE example GPT-4o visual explanation with the CEM explainer and the Pearl's explanation instruction.</p>
<p>Figure 9: A iLLuMinaTE GPT-4o visual explanation for a sample student with the MC-LIME explainer and the Necessity Robustness instruction.Figure 10: An example iLLuMinaTE GPT-4o visual explanation with the MC-LIME explainer and the Relevant Selection instruction.</p>
<p>Figure 11 :
11
Figure 11: Demographics of students (age, gender, self-identified struggling in courses, and MOOC Experience) that participated in the user study.</p>
<p>Figure 12 :
12
Figure 12: Example CEM iLLuMinaTE explanation for Relevance Selection.</p>
<p>Figure 13 :
13
Figure 13: Example CEM iLLuMinaTE explanation for Abnormal Conditions.</p>
<p>Figure 14 :
14
Figure 14: Example CEM iLLuMinaTE explanation for Pearl Explanation.</p>
<p>Figure 15 :
15
Figure 15: Example CEM iLLuMinaTE explanation for Necessity Robustness.</p>
<p>Figure 16 :
16
Figure 16: Example CEM iLLuMinaTE explanation for Contrastive Explanation.</p>
<p>Figure 17 :
17
Figure 17: Example chain of thought (CoT) iLLuMinaTE explanation.</p>
<p>Figure 18 :
18
Figure 18: Example LIME (BASE) explanation.Figure19: Example MC-LIME (BASE) explanation.</p>
<p>Figure 19 :
19
Figure 18: Example LIME (BASE) explanation.Figure19: Example MC-LIME (BASE) explanation.</p>
<p>Figure 20 :
20
Figure 20: Example CEM (BASE) explanation.</p>
<p>Figure 21 :
21
Figure 21: Grading criteria across four explanations presented at a time (one base explanation and three iLLuMinaTE explanations).</p>
<p>Figure 22 :
22
Figure 22: Actionability decision made by the students.</p>
<p>Figure 23 :
23
Figure 23: Results of student evaluation of different theories of explanations (Base, Relevance Selection, Abnormal Conditions, Necessity Robustness, CoT, Pearl, RaR + Contrastive) on all five axes of explanation.</p>
<p>Figure 24 :
24
Figure 24: Results of student ranking of different explanation criteria.Lower ranking is better (more important).</p>
<p>Figure 25: (Left) Actionability simulation across source explainers (LIME, CEM, MC-LIME), averaged over courses and theories.(Right) Actionability simulation across six theories (Relevance Selection, Abnormal Conditions, Pearl's Model, Necessity and Robustness, Contrastive explanations, and Chain of Thought), averaged over courses and source explainer.</p>
<p>• Data in the form of student's features over 5 weeks that were used by the model.You will see always the most relevant features selected by {explainer}.•Thecourse {course name} content and structure.•Detailed instructions on how to reason.
{model description} {features description} {explainer description} {course description}Take into consideration this data: {explainer importance scores} {student feature values}</p>
<p>Table 3 :
3
Topic Level Language No. Weeks No. Students Passing Rate [%] Course Details and Statistics.
Digital Signal Processing DSP Éléments de Géomatique Geo Villes Africaines VACS Math SSMSc BSc BScEnglish French En/Fr10 11 124,012 452 5,64323.1 45.1 9.9
Topic abbreviations: Math: Mathematics; NS: Natural Science; CS: Computer Science; SS: Social Science; Arch: Architecture; Bus: Economics and Business.</p>
<p>Table 4 :
4
Agreement between Human and GPT-4o annotation.Average percentage of answers with agreement between human and GPT-4o.Standard deviations were computed between the different explainers.
TheoryAgreementAbnormal Conditions 98.89 ± 4.72 Base Contrastive 82.22 ± 9.01 Chain of Thought 95.11 ± 6.13 RaR + Contrastive 98.27 ± 3.39 Necessity Robustness 97.78 ± 2.62 Pearl Explanation 95.83 ± 4.41 Relevance Selection 97.78 ± 3.58 Statistical Relevance 91.56 ± 6.99Overall94.68 ± 5.20
We adaptedHattie and Timperley (2007)'s question "How am I going?" to "How am I doing?" for ease of understanding.
 www.prolific.com/ <br />
https://anonymous.4open.science/r/illuminate/study.pdf
Summary and OutlookIn this work, we addressed the critical need for humanunderstandable explanations of complex models in education. We introduced iLLuMinaTE, a theory-driven frame-
https://platform.openai.com/
https://console.groq.com/
https://replicate.com/
 www.prolific.com/ <br />
AcknowledgmentsWe kindly thank the Swiss State Secretariat for Education, Research and Innovation (SERI) for supporting this project.Oana-Maria Camburu was supported by a Leverhulme Early Career Fellowship.Take into consideration this data: {explainer importance scores} MODEL PREDICTION: pass, with 74.538538% of confidence.FEATURE IMPORTANCESThese are the features found important by CEM: number sessions InWeek5 -0.466320 regularity peak dayhour InWeek5 -0.458857 number sessions InWeek4 -0.390114 regularity peak dayhour InWeek4 -0.386096 number sessions InWeek3 -0.307134 frequency action Video.Load InWeek5 -0.279141 number sessions InWeek2 -0.271637 time sessions mean InWeek1 -0.240877 time sessions mean InWeek2 -0.237525 number sessions InWeek1 -0.199099 {student feature values}The relevant feature values found by {explainer} CEM for the student are included below:'number sessions InWeek5': 0.0908182726364544, 'regularity peak dayhour InWeek5': 0.0073937247167162, 'number sessions InWeek4': 0.0, 'regularity peak dayhour InWeek4': 0.0088844825597196, 'number sessions InWeek3': 0.0, 'frequency action Video.Load InWeek5': 0.0, 'number sessions InWeek2': 0.0, 'time sessions mean InWeek1': 0.0, 'time sessions mean InWeek2': 0.0, 'number sessions InWeek1': 0.0 INSTRUCTIONS: {theory instructions} Use the social science theory "Relevance-Based Selection" to derive key insights from the model prediction, explainer, and student feature values.1. Select the causes that are most relevant to the question, context and user 2. Select the causes that include information that is not already shared with the student QUESTION: Given the information above, follow the instructions precisely and write a small report on what you found.Only use the results from the explainable AI approach and the student's behavior data to justify your conclusions.Other examples of {explainer instructions} include: LIME: We use LIME as our explainable AI approach, which gives importance scores for 20 features that contributed the most to the prediction of the model.Positive scores positively contribute to the model's decision to reach the predicted outcome.Negative scores would push the prediction towards the opposite class.The magnitude of the score indicates the strength of the feature's contribution.MC LIME: We use Minimal Counterfactual LIME as our explainable AI approach, which finds the smallest number of changes necessary to change a prediction from student failure to student success.It uses LIME scores to select the features to change.The output are the sets of features with the new values that would change the prediction.We have included the social science instructions from each of the eight explanation theories (Relevance Selection, Abnormal Conditions, Pearl's Model of explanation, Necessity and Robustness selection, Base Contrastive Explanation, RaR + Contrastive Explanation, Statistical Relevance, and Chain-of-Thought).Relevance-based selection prompt 1. Select the causes that are most relevant to the question, context and user 2. Select the causes that include information that is not already shared with the student Abnormal conditions model prompt 1. Select potential causes using these criteria:• Abnormality: Tend to prefer abnormal causes.• Temporality: Recent events are more relevant for the user and considered more mutable.• Controllability: focus on the features that the student can control.2. Select one explanation that follows all of the criteria above (Abnormality, Temporality, Controllability).Pearl's model of explanation prompt 1. Identify variables that are determined by factors external to the model (exogenous variables) and variables that are determined by relationships with other variables (endogenous variables).2. Select a set of factors representing the explanation by following the criteria:• The set of factors is minimal -there are no irrelevant causes in the set.• A change in these factors' values would result in a reversed prediction.Necessity &amp; Robustness prompt 1. Define sets of factors that would explain the model's output given the feature scores and the explainer scores.2. Select a few of these sets of factors that are the most necessary to cause the event.3. Robustness is defined as the extent to which the model's output would still have occurred if conditions were somewhat different.Select an explanation that is the most robust.Base Contrastive Explanation prompt 1. Reformulate the question as "Why P rather than Q?" 2. Using the explainable AI importance scores, determine the causes that explain the difference between P and not-Q.Which causes are a cause of P, but are not present in the causal chain of Q?RaR + Contrastive Explanation prompt 1.An explanation is always of the form "Why P rather than Q?", in which P is the target event and Q is a counterfactual contrast case that did not occur.P is the fact (the event that did occur) and Q the foil (event that did not).2. Select one foil based on two criterias: abnormality and relevance given the question, context, and user.3. Using the explainable AI importance scores, determine the causes that explains the difference between P and not-Q.Which causes are a cause of P, but are not present in the causal chain of Q?SR prompt 1. 1. Select the most likely explanations for this model output."Based on the data, factors A, B and C contribute to the probability of Y". 2. Complete this phrase with the corresponding factors of our data.CoT prompt Let's think step-by-step.iLLuMinaTE Explanation Selection Examples We present an example for the same sample student from the DSP 1 course using the CEM explainer and the abnormal conditions explanation strategy with Gemma-2 9B, GPT-4o, and Llama-3 70B.Introduction: This report analyzes the performance prediction of a student in the Digital Signal Processing 1 course using an explainable AI approach and student behavior data.The model predicts a "pass" with 99.74% confidence.GemmaApplying the "Abnormal Conditions" Theory: The "Abnormal Conditions" theory proposes that deviations from typical behavior patterns can indicate underlying issues.By applying this theory to the student's data, we aim to identify potential causes for their predicted success.Identifying Potential Causes: The explainable AI approach (CEM) highlights several features as crucial for the prediction:• Number of sessions in Week 5: Significantly low (0.09) compared to the average.• Time sessions standard deviation in Week 5: Zero, indicating a very consistent session length, which could be unusual.• Regularity peak day hour in Week 4: Low (0.07) suggesting a lack of a strong peak time for activity.• Total clicks on videos in Week 5: Low (0.04) compared to other features.Selected Explanation: The most compelling explanation based on the "Abnormal Conditions" theory is the low number of sessions in Week 5 (0.09).This deviates significantly from the average and suggests a potential issue with engagement during this crucial week.Justification:• Abnormality: The low number of sessions is significantly below the expected range.• Temporality: Week 5 is a recent period, making its impact more immediate and relevant to the current prediction.• Controllability: The student can directly control the number of sessions they participate in.Conclusion: While the model predicts a high probability of success, the low number of sessions in Week 5 raises a concern.This could indicate a temporary dip in engagement or a potential learning barrier encountered during this week.Further investigation into the specific content and activities of Week 5 might reveal the underlying cause and allow for targeted interventions to ensure the student's continued success.GPT-4o, DSP 1, CEM, Abnormal Conditions -iLLuMinaTE Selection Prompt Student Performance Analysis ReportModel Prediction: The model predicts that the student will pass the course with a confidence of 99.74%.-Potential Causes of the Result:1. Abnormality: The student's number of sessions in Week 5 (0.090818) is significantly low compared to the importance placed on this feature.Additionally, the total clicks on videos in Week 5 (0.041874) are notably low.2. Temporality: Recent weeks (Weeks 4 and 5) show more relevance for the student's current and future performance.Features from Week 5 are particularly critical as they are the most recent data points available.3. Controllability: The student has control over their number of sessions and video clicks, making these features actionable for improving their performance.-Explanation: Based on the criteria of Abnormality, Temporality, and Controllability, the most significant insight is the student's low engagement in Week 5, particularly in terms of the number of sessions (0.090818) and total clicks on videos (0.041874), which are abnormally low.Conclusion:The student's low engagement in Week 5 is a controllable and recent abnormality that significantly affects their predicted success.To improve their learning trajectory and maintain their predicted pass, the student should:• Increase the number of sessions: Aim to participate in more online sessions.• Increase video engagement: Watch and interact with more video content.By addressing these specific areas, the student can enhance their engagement, solidify their understanding of the course material, and increase their likelihood of success.Llama-3 70B, DSP 1, CEM, Abnormal Conditions -iLLuMinaTE Selection Prompt Report: Based on the model prediction, explainer, and student feature values, I applied the social science theory Abnormal Conditions to derive key insights.Potential Causes of the Result:1. Abnormality: The feature time sessions std InWeek5 has a value of 0.0, which is an abnormal condition, indicating that the student's time spent in sessions during Week 5 has no standard deviation.This could be a cause of the model's prediction of "pass" with high confidence.Temporality:The features number sessions InWeek5, total clicks Video InWeek5, total clicks video InWeek5, and time in video sum InWeek5 are all related to the student's behavior during Week 5, which is a recent event.These features are more relevant and mutable than earlier events.3. Controllability: The features number sessions InWeek5, total clicks Video InWeek5, and total clicks video InWeek5 are all related to the student's engagement with the course content, which is within their control.Selected Explanation: Based on the criteria above, I select the explanation that the student's high engagement with the course content during Week 5, as indicated by the abnormal feature values, is a key cause of the model's prediction of "pass" with high confidence.Specifically, the student's high number of sessions (number sessions InWeek5) and clicks on videos (total clicks Video InWeek5 and total clicks video InWeek5) during Week 5 suggest that they are actively participating in the course and making an effort to learn the material.This explanation meets all the criteria: it is an abnormal condition (high engagement), it is a recent event (Week 5), and it is within the student's control (engagement with course content).Extended examples across different instruction strategies and base explainers can be found in our code repository.Explanation Presentation PromptsWe use the framework of Hattie et al.(Hattie and Timperley 2007)to present the explanation in the form of student feedback:1.Where Am I Going?-A brief description of the student's performance and explicitly state the learning goal 2. How Am I Doing?-A brief description of the explanation findings 3.Where to Next? -Two recommended actions that the student can take that are specific to weeks of the course (make connections between previous weeks and upcoming weeks)We also include Grice's Maxims to guide the communication of the explanation(Grice 1975).Grice's maxims provide a framework for understanding cooperative conversation.They propose that communication is a collaborative effort, with participants adhering to the "cooperative principle".This principle emphasizes providing the right amount of relevant and clear information to achieve a shared goal.Grice further divides this principle into four maxims:• Quality (truthfulness and evidence-based statements) • Quantity (providing enough but not excessive information) • Relation (staying relevant) • Manner (clear and concise communication) Notably, Grice acknowledges that maxims can be strategically ignored to express meaning indirectly, as seen in irony or metaphors.We have expressed Grice's maxims in the prompt as follows:1. do not say things that you believe to be false 2. do not say things for which you do not have sufficient evidence.3. do not add information that is not relevant 4. only say what is relevant 5. be orderly Explanation Presentation Structure Given this report, I want you to write a shorter version using the theory of feedback from Hattie et al.:Where Am I Going?-A brief description of the student's performance and explicitly state the learning goal How Am I Doing?-A brief description of the explanation findings Where to Next? -Two recommended actions that the student can take that are specific to weeks of the course (make connections between previous weeks and upcoming weeks)The student who is going to interact with you is the same student that the data is about, so use the tone of a professor who is talking to a student in a comforting and personable manner.To communicate this intervention most effectively, use Grice's maxims of conversation.• do not say things that you believe to be false • do not say things for which you do not have sufficient evidence.• do not add information that is not relevant • only say what is relevant • be orderlyYour goal is to explain to the student what they should do to improve their performance in the course in the best way possible.Follow the instructions above.We use the following format template to guide the LLM's response into an easily post-processable format.Format Template{format instructions} Return a JSON file with a string with your feedback to the student.{conversation template}Current conversation: {chat history} {input} AI Assistant:For the {presentation instruction} portion of the prompt, we have the following instructions.Relevance-Based Selection Presentation Prompt Relevant Causes: Say to the student which causes you selected as relevant based on the question, context, and your background.Be explicit.New Information: Say to the student which information you thought they knew already, such as "Assuming that you know. . ." and highlight aspects that haven't been previously communicated to the student.Be clear and honest.Finally: Say to the student that you focused on the most relevant causes that provide new insights.Specify which causes you selected and why they are important.Abnormal Conditions Presentation Prompt Abnormal Causes: Tell the student which causes you selected as abnormal and explain why these are important based on the recent events and their uniqueness.Recent Events: Highlight the recent events that are relevant to the result.Emphasize why these events are important for the student to consider.Controllable Factors: Point out the factors that the student has control over.Explain how focusing on these factors can help them improve their outcomes.Be clear and direct.Pearl's Explanation Presentation Prompt External Factors: Identify the external factors (exogenous variables) that influence the result.Explain why these factors are important and how they impact the outcome.Internal Relationships: Point out the variables determined by their relationships with other variables (endogenous variables).Describe these relationships and their significance.Focus on Improvement: Highlight any features that negatively impact the student's success.Offer clear and actionable advice on how to improve these areas.Necessity &amp; Robustness Presentation Prompt Necessary Causes: Identify the most necessary factors that explain the result.Explain why these factors are essential for understanding the outcome.Robust Explanation: Describe the explanation that would still hold true even if conditions were slightly different.Highlight why this robust explanation is reliable and important.Base Contrastive Presentation Prompt Target Event: Explain the event that did occur (P).Describe this event clearly and why it is significant.Contrast Event: Identify the contrasting event that did not occur (Q).Explain why this event is relevant and why it was chosen as a contrast.Key Differences: Highlight the main differences between the target event and the contrast event.Explain why these differences are important for understanding the result.RaR + Contrastive Presentation Prompt Target Event (P): Describe the event that did occur (P).Explain why this event is important.Contrast Event (Q): Describe the event that did not occur (Q).Explain why this event was chosen as a contrast.Key Causes: Identify the causes that explain the difference between P and Q. Highlight which causes led to P and were not present in Q. Explain why these causes are significant for understanding the outcome.Statistical Relevance Presentation Prompt Likely Causes: List the factors (A, B, and C) that most likely contribute to the result (Y) based on the feature scores.Explain why these factors are important.Explanation: Provide a concise explanation of how these factors lead to the result.Be clear and straightforward, avoiding complex terms.Chain of Thought Presentation Prompt Let's process the explanation step by step.iLLuMinaTE Presentation ExamplesWe present examples from the presentation prompt for a sample student in DSP 1, with explainer CEM, and the abnormal conditions strategy across three language models.Gemma-2 9B, DSP 1, CEM, Abnormal Conditions -iLLuMinaTE Presentation PromptYou're doing great in the Digital Signal Processing course!Let's take a look at your progress so far and see what we can do to help you continue on this successful path.How Am I Doing?It seems like you've been engaging well with the course material.However, I noticed that in Week 5, you had a lower number of sessions compared to other weeks.This is a bit unusual because Week 5 covers important concepts like ideal filters and filter design, which build upon the knowledge from earlier weeks.Remember, consistent engagement is key to mastering these complex topics.Where to Next?• Connect the Dots: Take some time to review the material from Week 4, especially the concepts related to the Discrete Fourier Transform (DFT) and its applications.This will help solidify your understanding of the foundational principles needed for Week 5's content.• Active Learning: In Week 6, you'll delve into modulation, interpolation, and sampling.To make the most of these concepts, try actively applying them to real-world examples.Think about how these techniques are used in everyday technologies like audio and video compression.GPT-4o, DSP 1, CEM, Abnormal Conditions -iLLuMinaTE Presentation PromptFeedback on Your Course ProgressWhere Am I Going?You're progressing well in Digital Signal Processing 1.Your goal is to master the course content and successfully pass the course.How Am I Doing?• Abnormal Causes: Your engagement in Week 5 is notably low, with very few online sessions and minimal interaction with video content.These are critical because they deviate from the norm and significantly impact your learning.• Recent Events: Weeks 4 and 5 are crucial as they cover essential topics like Filter Design and FFT.Your low engagement during these weeks is concerning.• Controllable Factors: You can control the number of online sessions you participate in and your interaction with video materials.Increasing these activities will enhance your understanding and performance.Where to Next?1. Week 6: Focus on Modulation and Interpolation.Engage more in online sessions and interact with videos to catch up. 2. Week 7: As you move into Multirate processing, ensure you maintain high engagement.Build on the foundation from previous weeks to better understand complex concepts.Llama-3 70B, DSP 1, CEM, Abnormal Conditions -iLLuMinaTE Presentation PromptWhere Am I Going?You're doing great so far, and your goal is to master the skills in Digital Signal Processing 1. You're currently on track to pass the course, but let's work together to ensure you continue to excel.How Am I Doing?• Abnormal Causes: Your high engagement with the course content during Week 5 is unusual and important.This suggests you're actively participating in the course and making an effort to learn the material.• Recent Events: Your behavior during Week 5, such as having a high number of sessions and clicks on videos, is crucial to your success.This recent engagement is a key factor in your predicted performance.Extended examples across different instruction strategies and base explainers can be found in our code repository.Visualization PromptWe use this prompt with the ChatGPT-4o interface to directly extract the visualizations from the native chat environment.It is also possible to ask language models to generate the code and run it separately.We did not evaluate this case as it would necessitate additional post-processing steps, causing friction to the non-technical educator's workflow in a real-world setting.Given this detailed report:Realist and Pragmatic TheoriesLargely, explanation theories can be divided into realist and pragmatic theories (Danks 2022).Realist theories emerge from the realms of the philosophy of science and logic, while pragmatic theories of explanation are rooted in studies of psychology and social sciences.The difference stands mostly on the consideration of the context as important factor in the definition of an explanation.Realist theories define an explanation a relation between theory and fact, while pragmatic theories a relation between theory, fact, and context.This difference can be boiled down to whether radically false (i.e., not even approximately true) statements can be part of an explanation: realists say "no" while pragmatists say "yes".In this work, we focus mostly on pragmatic theories of explanation.Explanation as a process or as a product?Explanation can be seen as one or the other, or both(Lombrozo 2006).The theories that define explanation as a process could be categorized in three main groups (Miller 2019):• Explanation as a causal process -Explanation is the process of inferring the real causes of an event.• Explanation as a cognitive process -Explanation is a process that takes place in our cognition and it is closely related to abductive reasoning.• Explanation as a social process -Explanation is a process of transferring knowledge between explainer (the agent who explains a decision or event) and explainee (the agent receiving the explanation).Theories that see explanation as a product define it as an answer to a "why-question" (Miller 2019; Molnar 2020).Explanation as a causal processFor most of the philosophical theories, explanation is rooted in causality, starting from Hume's regularity theory(Hume 2016)of causation that argues "there is a cause between two types of events if events of the first type are always followed by events of the second".In broad terms, theories defined upon causality have important assumptions in common:• "Everyday" explanations don't differ from scientific explanations• Explanation is a causal relationship• Radically false statements can't be part of an explanation Various philosophical theories emerged from causality such as the Deductive-Nomological Model (DN Model)(Woodward and Ross 2021) which can be summarized as "Whenever phenomenon X is observed to occur in the setting of conditions C, Y will be observed"(Cambria et al. 2023).An extension of these theories were the probabilistic theories, such as the Statistical Relevance model (SR model) that in general state that"event C is a cause of event E if and only if the occurrence of C increases the probability of E occurring" (Miller 2019).Other models have been defined but all the theories in this category have strong limitations when applied to social or human realms.The main problem is that "the explanations that successfully follow these criteria seem to vary greatly in the extent to which they are explanatory deep or illuminating in the eye of the person receiving the explanation" (Miller 2019).They miss relevance and bring little impact as in most cases there are no possible explanations that can enter in the realm of certainty and causality.Thus, from this group of theories we will keep only the SR model, as it mainly relies on likelihood and probability.Explanation as a cognitive processIn general the cognitive process that we follow when we search for an explanation follows a general structure:When we encounter something confusing, we use abductive reasoning to make sense of it and come up with an explanation.This process includes the mental simulation of hypothetical scenarios where the event didn't happen (counterfactuals) that lead us to the identification of potential causes.From these possibilities, we infer and select an explanation.Abductive Reasoning Abductive reasoning, also known as abduction, is a logical process in which an individual derives a hypothesis to explain an observed phenomenon(Peirce 1997).It is often used to generate the best possible explanation for surprising or unexpected facts in a context of uncertainty.A common form of abductive inference follows this structure:The surprising fact, C, is observed.But if A were true, C would be a matter of course.Hence, there is reason to suspect that A is true.(Peirce 1997)Alternatively, it can be expressed as:D is a collection of data(facts, observations, givens). H explains D (would, if true, explain D).No other hypothesis can explain D as well as H does. Therefore, H is probably true.(Josephson and Josephson 1996)The process could be summarized in these four steps:1. Observation: Observe some (presumably unexpected or surprising) events.2. Hypothesis Generation: Generate one or more hypotheses about these events.3. Plausibility Judgment: Judge the plausibility of the hypotheses.4. Hypothesis Selection: Select the 'best' hypothesis as the explanation.Counterfactuals and Contrastive explanations Counterfactuals are very relevant to abductive reasoning as studies suggest that people don't simply explain why things happen, but rather why something happened one way (event P) instead of another way (event Q) (Molnar 2020).They were firstly described in Hume's work(Hume 2016) as the counterfactual case:"the cause should be understood relative to an imagined, counterfactual case: event C is said to have caused event E if, under some hypothetical counterfactual case the event C did not occur, E would not have occurred."In other words, explanations are always contrastive.We explain why P occurred by contrasting it with a situation where P didn't happen (Q), even if that situation is purely counterfactual.The Problem of Implicit Foils.Lipton (1990)acknowledges a challenge.While explanations often highlight the target event (P), they rarely explicitly mention the foil (Q).For example, the question "Why did Elizabeth open the door?" could have countless foils: leaving it closed, opening the window, or someone else opening it.The unspecified foil, "not(Elizabeth open the door)," creates a vast pool of possibilities.This theory (2002) further proposes a classification system for explanations based on the type of contrast:• Plain-Fact: Answers basic curiosity about an object's property (e.g., "Why does a magnet attract metal?").These questions lack a foil.• P-Contrast: Compares properties within an object (e.g., "Why is the sky blue rather than red?").• O-Contrast: Compares properties between objects (e.g., "Why is a car red, while a bus is yellow?").• T-Contrast: Compares properties of the same object across time (e.g., "Why was the water liquid yesterday but solid today?").Hesslow (1988)argues that the explanandum, the event to be explained, should be viewed not as a simple property, but as a difference between the target object and a reference class (foils) with respect to a particular property.He emphasizes that the selection and weighting of causes depend on their explanatory relevance.Lipton (1990)'s difference condition further clarifies that explaining "why P rather than Q" requires identifying a causal difference.This difference consists of a cause for P that is absent in the counterfactual scenario (not-Q).Contrastive theory suggests that when selecting an explanation from multiple possible causes, people focus on the differences between the target event (P) and the counterfactual foil (Q).Explanation as a social processFurthermore, explanation can also be considered finally as a social process that is inherently part of a conversation between an explainer and the explainee.Usually the explainer has enough knowledge or understanding of the causes of an event to be able to explain it to others.The most relevant work in this category is Hilton's conversational model of explanation(Hilton 1990).Why people ask for an explanation?The primary function of explanation goes beyond simply acquiring new information, it is about learning and refining our understanding of the world.Explanations can ultimately shape our causal beliefs and our prior knowledge.Malle argues that explanation helps us resolve contradictions or inconsistencies in our existing understanding and create a sense of shared meaning within a group of individuals(Malle 2006).Finally, explanations can serve as a tool for persuasion, influencing how others perceive the world.Therefore, an explanation can have different goals in the setting of explainable AI. it is up to those designing the AI system to decide what's most important to explain in each situation.For example, an AI could be designed to build trust in the user, thus persuasion and other forms of manipulation could be more relevant than the actual truthfulness of the answer.11.8 Structure of explanation Overton's scientific structure of explanation Overton's model(Overton 2012) of scientific explanation defines five categories of properties or objects that are explained in science.These are:• Theories: These are sets of principles that form the building blocks for models.• Models: An abstraction of a theory that represents the relationships between kinds and their attributes.• Kinds: An abstract universal class that supports counterfactual reasoning.For example, an "arthropod" is a kind.• Entities: An instantiation of a kind.For example, a specific spider is an entity.• Data: Statements about activities, such as measurements and observations.For example, the fact that a particular spider has eight legs is data.Overton argues that explanations of phenomena at one level must be relative to and refer to at least one other level, and that explanations between two such levels must refer to all intermediate levels.Malle's theoryMalle (2006)proposes a theory that breaks down the cognitive processes of explanation into two main groups:1. Information Processes -These involve the creation and assembly of explanations.2. Impression Management Processes -These concern the social dynamics of providing explanations.Malle further divides these dimensions into two additional aspects (Figure26), the tools and methods in the hands of the explainer for constructing and delivering explanations and the explainer's perspective or prior knowledge.By considering these two dimensions, we have four items:1. Information Requirements -These are the necessary elements needed for a proper explanation.2. Information Access -This refers to the information available to the explainer.Sometimes, critical information may be missing.3. Pragmatic Goals -This involves the purpose of the explanation, such as imparting knowledge to the listener, portraying someone in a certain light, or building trust.4. Functional Capacities -These are the inherent limitations and capabilities of the explanatory tools, which determine what goals can be achieved with them.In more simple terms these four elements could be described as what information is needed, what is available, the intentional goals of the explainer and the constraints of the context and tools.Malle's framework is fundamental to determine the Context Definition (Sec.7.1) where we define these four elements for our LLMs to give an effective explanation.Hilton's conversational model of explanation Hilton's conversational model(Hilton 1990) emphasizes the social nature of explanation, contrasting it with simple causal attribution.He argues that explanations are conversations aimed at resolving a "puzzle" in the explainee's mind.This model consist of a two-stage process, which involves:
{TensorFlow}: A System for {Large-Scale} Machine Learning. M Abadi, P Barham, J Chen, Z Chen, A Davis, J Dean, M Devin, S Ghemawat, G Irving, M Isard, 12th USENIX symposium on operating systems design and implementation. 2016</p>
<p>M Asadi, V Swamy, J Frej, J Vignoud, M Marras, T Käser, arXiv:2212.01133RIPPLE: Concept-Based Interpretation for Raw Time Series Models in Education. 2023</p>
<p>Diagnostics-guided explanation generation. P Atanasova, J G Simonsen, C Lioma, I Augenstein, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202236</p>
<p>Interpretable deep learning for university dropout prediction. M Baranyi, M Nagy, R Molontay, 21st Annual Conference on Information Technology Education. 2020</p>
<p>On the dangers of stochastic parrots: Can language models be too big?. E M Bender, T Gebru, A Mcmillan-Major, S Shmitchell, Proceedings of the 2021 ACM conference on fairness, accountability, and transparency. the 2021 ACM conference on fairness, accountability, and transparency2021</p>
<p>How to quantify student's regularity?. M S Boroujeni, K Sharma, Ł Kidziński, L Lucignano, P Dillenbourg, European conference on technology enhanced learning. Springer2016</p>
<p>Utilizing Student Time Series Behaviour in Learning Management Systems for Early Prediction of Course Performance. E Cambria, L Malandri, F Mercorio, M Mezzanzanica, N Nobani, F Chen, Y Cui, Information Processing &amp; Management. 6012023. 2020Journal of Learning Analytics</p>
<p>Towards Real Interpretability of Student Success Prediction Combining Methods of XAI and Social Science. L Cohausz, C Conati, K Porayska-Pomsta, M Mavrikis, arXiv:1807.00154AI in Education needs interpretable machine learning: Lessons from Open Learner Modelling. A Mitrovic, N Bosch, Durham, United Kingdom; Danks, DInternational Educational Data Mining Society2022. 2018. 2022Governance via explainability. In The Oxford Handbook of AI Governance</p>
<p>Y Deng, W Zhang, Z Chen, Q Gu, A Dhurandhar, P.-Y Chen, R Luss, C.-C Tu, P Ting, K Shanmugam, P Das, arXiv:2311.04205arXiv:1802.07623Explanations based on the Missing: Towards Contrastive Explanations with Pertinent Negatives. 2024. 201832221Flesch, R. 1948. A new readability yardstick</p>
<p>Finding Paths for Explainable MOOC Recommendation: A Learner Perspective. J Frej, N Shah, M Knezevic, T Nazaretsky, T Käser, Proceedings of the 14th Learning Analytics and Knowledge Conference. the 14th Learning Analytics and Knowledge Conference2024</p>
<p>Do not trust a model because it is confident: Uncovering and characterizing unknown unknowns to student success predictors in online-based learning. R Galici, T Käser, G Fenu, M Marras, LAK23: 13th International learning analytics and knowledge conference. 2023</p>
<p>Framewise phoneme classification with bidirectional LSTM networks. A Graves, J Schmidhuber, Proceedings. 2005 IEEE International Joint Conference on Neural Networks. 2005 IEEE International Joint Conference on Neural NetworksIEEE2005. 20054</p>
<p>Gunning, R. 1952. The technique of clear writing. H P Grice, Speech acts. Brill1975Logic and conversation. No Title</p>
<p>Causes and explanations: A structural-model approach. Part II: Explanations. The British journal for the philosophy of science. J Y Halpern, J Pearl, 2005</p>
<p>A Machine Learning and Explainable AIApproach for Predicting Secondary School Student Performance. C N Hang, C W Tan, P.-D Yu, F Rahman, R Hasnat, M G R Alam, IEEE 12th Annual Computing and Communication Workshop and Conference. 2024. 2022MCQGen: A Large Language Model-Driven MCQ Generator for Personalized Learning</p>
<p>The power of feedback. J Hattie, H Timperley, Review of educational research. 7712007</p>
<p>The ASSISTments ecosystem: Building a platform that brings scientists and teachers together for minimally invasive research on human learning and teaching. N T Heffernan, C L Heffernan, International Journal of Artificial Intelligence in Education. 242014</p>
<p>The problem of causal selection. Contemporary science and natural explanation: Commonsense conceptions of causality. G Hesslow, 1988</p>
<p>Conversational processes and causal explanation. D J Hilton, Psychological Bulletin. 1071651990</p>
<p>Knowledge-based causal attribution: The abnormal conditions focus model. D J Hilton, B R Slugoski, Psychological review. 931751986</p>
<p>The classroom as a dashboard: Codesigning wearable cognitive augmentation for K-12 teachers. K Holstein, G Hong, M Tegene, B M Mclaren, V Aleven, Proceedings of the 8th international conference on learning Analytics and knowledge. the 8th international conference on learning Analytics and knowledge2018</p>
<p>Using Machine Learning Explainability Methods to Personalize Interventions for Students. D Hume, P Hur, H Lee, S Bhat, N Bosch, Seven masterpieces of philosophy. International Educational Data Mining Society2016. 2022An enquiry concerning human understanding</p>
<p>The promises and pitfalls of ChatGPT as a feedback provider in higher education: An exploratory study of prompt engineering and the quality of AI-driven feedback. L J Jacobsen, K E Weber, 2023</p>
<p>J R Josephson, S G Josephson, Abductive inference: Computation, philosophy, technology. Cambridge University Press1996</p>
<p>Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav). H Khosravi, S B Shum, G Chen, C Conati, Y.-S Tsai, J Kay, S Knight, R Martinez-Maldonado, S Sadiq, D Gašević, M Wattenberg, J Gilmer, C Cai, J Wexler, F Viegas, International conference on machine learning. Kim, BPMLR2022. 20183Explainable artificial intelligence in education</p>
<p>Concept bottleneck models. P W Koh, T Nguyen, Y S Tang, S Mussmann, E Pierson, B Kim, P Liang, N Kroeger, D Ley, S Krishna, C Agarwal, H Lakkaraju, arXiv:2310.05797Are Large Language Models Post Hoc Explainers. 2020. 2024International conference on machine learning</p>
<p>A data-driven student model to provide adaptive support during video watching across MOOCs. S Lallé, C Conati, International Conference on Artificial Intelligence in Education. Springer2020</p>
<p>Contrastive explanation. Royal Institute of Philosophy Supplements. P Lipton, 199027</p>
<p>The structure and function of explanations. T Lombrozo, Trends in cognitive sciences. 10102006</p>
<p>Causal-explanatory pluralism: How intentions, functions, and mechanisms influence causal ascriptions. T Lombrozo, Cognitive psychology. 6142010</p>
<p>Can Feature Predictive Power Generalize? Benchmarking Early Predictors of Student Success across Flipped and Online Courses. B F Malle, M Marras, J T T Vignoud, T Käser, 14th International Conference on Educational Data Mining. MIT press2006. 2021How the mind explains behavior: Folk explanations, meaning, and social interaction</p>
<p>SMOG grading-a new readability formula. G H Mc Laughlin, Journal of reading. 1281969</p>
<p>Identifying and comparing multi-dimensional student profiles across flipped classrooms. P Mejia-Domenzain, M Marras, C Giang, T Käser, International Conference on Artificial Intelligence in Education. 2022</p>
<p>Explanation in artificial intelligence: Insights from the social sciences. Springer, T Miller, C Molnar, E Zarifsanaiey, N ; R. Niakan, S Kalhori, M Rakhshan, L Keikha, Ghazi Saeedi, M , telligent tutoring systems: a systematic review of characteristics, applications, and evaluation methods. 2019. 2020. 2021267Lulu. com. Mousavinasab,</p>
<p>Teachers' trust in AI-powered educational technology and a professional development program to improve it. M Mozgovoy, Ieee, T Mu, A Jetten, E Brunskill, T Nazaretsky, M Ariely, M Cukurova, G Alexandron, Towards Suggesting Actionable Interventions for Wheel-Spinning Students. International Educational Data Mining Society. 2011. 2020. 202253Dependency-based rules for grammar checking with LanguageTool</p>
<p>AI or Human? Evaluating Student Feedback Perceptions in Higher Education. T Nazaretsky, P Mejia-Domenzain, V Swamy, J Frej, T Käser, S P Neshaei, R L Davis, A Hazimeh, B Lazarevski, P Dillenbourg, T Käser, arXiv:2403.14661European Conference on Technology Enhanced Learning, EC-TEL 2024. 2024. 2024arXiv preprintTowards modeling learner performance with large language models</p>
<p>Large Language Models for In-Context Student Modeling: Synthesizing Student's Behavior in Visual Programming from One-Shot Observation. M H Nguyen, S Tschiatschek, A Singla, 2023EDM</p>
<p>Explanation in Science. J A Overton, 2012The University of Western Ontario (Canada)</p>
<p>An Interpretable Pipeline for Identifying At-Risk Students. B Pei, W Xing, Journal of Educational Computing Research. 073563312110381682021</p>
<p>Pragmatism as a principle and method of right thinking: The 1903 Harvard lectures on pragmatism. C S ; Peirce, J Bassen, J Huang, S Ganguli, M Sahami, L J Guibas, J Sohl-Dickstein, Deep Knowledge Tracing. Advances in Neural Information Processing Systems. Piech, CSuny Press1997. 201528</p>
<p>Towards a Unified Framework for Evaluating Explanations. J D Pinto, L Paquette, Y Qin, K Song, Y Hu, W Yao, S Cho, X Wang, X Wu, F Liu, P Liu, D Yu, arXiv:2401.03601InFoBench: Evaluating Instruction Following Ability in Large Language Models. EDM (HEXED Workshop2024. 2024</p>
<p>Why Should I Trust You?. M T Ribeiro, S Singh, C Guestrin, arXiv:1602.04938Explaining the Predictions of Any Classifier. 2016</p>
<p>Large Language Models Cannot Explain Themselves. A Sarkar, arXiv:2405.043822024arXiv preprint</p>
<p>Interactive and Explainable Advising Dashboard Opens the Black Box of Student Success Prediction. H Scheers, T De Laet, European Conference on Technology Enhanced Learning. Springer2021</p>
<p>Focus on formative feedback. V J Shute, Review of educational research. 7812008</p>
<p>Vultureanu-Albis ¸i, A.; and Bȃdicȃ, C. 2021. Improving Students' Performance by Interpretable Explanations using Ensemble Tree-Based Approaches. V Swamy, S Du, M Marras, T Käser, V Swamy, M Marras, T Käser, V Swamy, S Montariol, J Blackwell, J A Frej, M Jaggi, T Käser, V Swamy, B Radmehr, N Krco, M Marras, T Käser, V Satayeva, M Frej, J Bossy, T Vogels, T Jaggi, M Käser, T Hartley, M.-A Van Bouwel, J Weber, E Liang, Y Meng, F Sun, Z Shi, H Li, Z Xu, J Qu, J Zhou, J , arXiv:2303.04048Trusting the Explainers: Teacher Validation of Explainable Artificial Intelligence for Course Design. Learning Analytics and Knowledge. 2023. 2022. 2024a. 2022. 2024b. 2002. 202336arXiv preprintIEEE 15th International Symposium on Applied Computational Intelligence and Informatics. Is chatgpt a good nlg evaluator? a preliminary study</p>
<p>J Wei, X Wang, D Schuurmans, M Bosma, B Ichter, F Xia, E Chi, Q Le, D Zhou, arXiv:2201.11903Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. 2023</p>
<p>Scientific Explanation. J Woodward, L Ross, The Stanford Encyclopedia of Philosophy. E N Zalta, 2021Metaphysics Research Lab, Stanford UniversitySummer 2021 edition</p>
<p>Moocradar: A finegrained and multi-aspect knowledge repository for improving cognitive student modeling in moocs. J Yu, M Lu, Q Zhong, Z Yao, S Tu, Z Liao, X Li, M Li, L Hou, H.-T Zheng, Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval2023</p>
<p>A Zytek, S Pidò, K Veeramachaneni, arXiv:2405.06064LLMs for XAI: Future Directions for Explaining Explanations. 2024arXiv preprint</p>
<p>What is a good explanation? To address this question, it is important to discuss what an explanation is, generally. Explanation can be seen through different lenses: it can be seen as causal or non-causal, as a process or a product of it. In this section, we will first look at different ideas about what makes a good explanation. Then, we will explore how humans understand and create explanations themselves. Finally. we will discuss how explanations are best communicated</p>
<p>Diagnosis: exploration of the possible causes of an event. </p>
<p>Explanation presentation: presenting the selected explanation in a socially relevant way Crucially, good explanations must be relevant to the explainee's specific question, not just provide any identified cause. Hilton emphasizes that explanations follow the principles of cooperative conversation, including Grice's maxims (Grice 1975) (see Appendix 7.2) for quality, quantity, relation, and manner. In essence, explanations should be truthful, provide the right amount of detail, stay focused on the question, and be clear and respectful. It highlights also that explanation should be tailored based on the explainee's existing knowledge, focusing on causes they may not already understand</p>
<p>However, as we seen from (Swamy et al. 2023), different explainers are not coherent to each other. Thus, our work comparing LIME, CEM and MC-LIME begins to answer the question "are the results different depending on the post hoc explainer used?" -we mostly identify that the choice of explainer does not lead to an overall difference in which theories students prefer (Sec. 2.5). However, we do identify a significant increase of actionability for the MC-LIME source explainer over the other approaches (Sec. 3.4 and Appendix 10). A longer exploration with more source explainers is required to make generalizable statements (beyond the education use case) regarding how choice of source explainer impacts LLM communication of the explanation. iLLuMinaTE could use interpretable-by-design models (i.e. Concept Bottleneck Models (Koh et al. 2020), InterpretCC (Swamy et al. 2024a)). Hasib et al. 2022; Cohausz 2022Reliance on post-hoc explainers In our study we decided to use post-hoc explainers for the causal connection step and we have an instruction rule in the prompt to explicitly use the explainer's results. or in-hoc explainability methods (i.e. TCAV (Kim et al. 2018)) as a causal connection step</p>            </div>
        </div>

    </div>
</body>
</html>
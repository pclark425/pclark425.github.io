<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-325 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-325</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-325</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-14.html">extraction-schema-14</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <p><strong>Paper ID:</strong> paper-9b45af10429681249fafb07c3b6012ea4ce63ffe</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/9b45af10429681249fafb07c3b6012ea4ce63ffe" target="_blank">A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> This work proposes a novel framework, which pins down the causal effect of various factors in the input, e.g., the surface form of the problem text, the operands, and math operators on the output solution, and applies this framework on a test bed of math word problems.</p>
                <p><strong>Paper Abstract:</strong> We have recently witnessed a number of impressive results on hard mathematical reasoning problems with language models. At the same time, the robustness of these models has also been called into question; recent works have shown that models can rely on shallow patterns in the problem description when generating a solution.Building on the idea of behavioral testing, we propose a novel framework, which pins down the causal effect of various factors in the input, e.g., the surface form of the problem text, the operands, and math operators on the output solution.By grounding the behavioral analysis in a causal graph describing an intuitive reasoning process, we study the behavior of language models in terms of robustness and sensitivity to direct interventions in the input space. We apply our framework on a test bed of math word problems.Our analysis shows that robustness does not appear to continuously improve as a function of size, but the GPT-3 Davinci models (175B) achieve a dramatic improvement in both robustness and sensitivity compared to all other GPT variants.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e325.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e325.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-2 family</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-2 (distilled, small, medium, large, XL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Autoregressive language models from the GPT-2 family evaluated on math word problems using statement-form prompts; serve as smaller-scale baselines in the causal robustness analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>distilled / small / medium / large / XL</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>addition, subtraction, multiplication, division (two- and some three-operand MWPs)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>integers in {1..300} (tokens constrained to be single-token numbers), primarily two-operand problems and some three-operand problems</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>standard prompting (converted question→statement), causal do-interventions on operands and textual templates (result-altering and result-preserving swaps); probability distributions over numeric tokens measured</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Small models (Distil and GPT-2 small) exhibit low sensitivity (low TCE) and low robustness improvements; generally low probability mass on correct results and poor consistency across operand permutations (no single numerical performance % reported in main text).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Tends to show low sensitivity to changes in ground-truth result; exhibits diagonal / periodic assignment of higher probability to certain familiar results, suggesting reliance on surface distributional patterns rather than robust arithmetic reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Larger GPT-2 variants show increased sensitivity relative to distilled/small but still limited robustness; overall trend within GPT-2 family: some improvement with size but not monotonic robust arithmetic mastery.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Assigns higher probability to a small set of frequent results (patterned diagonals in heatmaps), inconsistent predictions across different operand sets leading to brittleness under result-preserving interventions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared across sizes within GPT-2 family and against larger models (GPT-J, NeoX, GPT-3 Davinci variants).</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>GPT-2 models are weak at robust arithmetic: increasing size within this family yields only modest sensitivity gains and persistent brittleness to surface/textual and operand perturbations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e325.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e325.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-Neo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-Neo (1.3B, 2.7B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source autoregressive language models (EleutherAI GPT-Neo variants) evaluated as mid-scale language model baselines on math word problems under causal interventions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-Neo</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>1.3B / 2.7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>addition, subtraction, multiplication, division (two- and some three-operand MWPs)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>integers in {1..300}, primarily two-operand problems</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>standard prompting (statement-form) and causal do-interventions on operands and templates; measured changes in predicted result and probability assigned to ground-truth</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Larger GPT-Neo variants show increased sensitivity compared to small GPT-2 but still show similar sensitivity to result-altering and result-preserving interventions (i.e., limited robustness); no single accuracy number in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Behaviors indicate models pick up spurious correlations between numerical tokens and outputs rather than reliably mediating via correct result G; TCE and DCE patterns consistent with partial reliance on operand and template surface cues.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Some improvement with model scale (1.3B→2.7B) in sensitivity but not in robust correct responsiveness; no emergent robust arithmetic ability reported.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Responds similarly to result-altering and result-preserving operand changes (poor robustness); susceptible to surface-form variations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against GPT-2 family, GPT-J, NeoX, and GPT-3 variants.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>GPT-Neo models gain sensitivity with size but do not reliably separate correct sensitivity to ground-truth from spurious direct effects, i.e., not robust arithmetic solvers.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e325.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e325.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-J-6B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-J (6B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 6-billion-parameter autoregressive language model evaluated on math word problems; shows increased sensitivity to ground-truth results but also substantial direct effects in some metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-J</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>6B</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>addition, subtraction, multiplication, division (two-operand focus in main analyses)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>integers in {1..300}, heatmaps shown for 0..50 for visualization; two-operand problems primarily</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>statement-form prompts; do-interventions on operands (result-altering and result-preserving) and templates; probability heatmaps over resulting numeric token</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Shows high TCE relative to DCE for operands: reported TCE approximately 30x larger than DCE (sensitivity much larger than undesired direct effect) — but this did not fully translate into change-of-prediction metric (δ_cp) improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Assigns higher overall probability mass to correct results compared to small models, but inconsistent across different operand pairs for same ground-truth; suggests partial learning of correct mapping but remaining reliance on operand-specific or surface associations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Higher sensitivity than smaller models; improvement with size up to 6B apparent but not equivalent to behavior of very large instruction-tuned models.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Despite higher probability mass for correct answer, predictions vary when operands are permuted even if result same (high DCE on δ_cp), indicating brittle reliance on operand patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to smaller GPT-2 variants, GPT-Neo, NeoX-20B, and GPT-3 Davinci models.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>GPT-J-6B demonstrates substantially greater sensitivity to correct results (TCE ≫ DCE) but remains brittle in prediction changes across result-preserving operand perturbations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e325.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e325.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-NeoX-20B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-NeoX (20B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 20-billion-parameter open-source autoregressive language model evaluated for causal robustness on MWPs; exhibits strong sensitivity (TCE) increases but mixed robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-NeoX</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>20B</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>addition, subtraction, multiplication, division (two-operand problems primarily)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>integers in {1..300}, two-operand problems</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>statement-form prompts; operand and template do-interventions; probability distributions over numeric tokens analyzed</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Reported TCE for operands much larger than DCE in some analyses (NeoX TCE reported ≈1000× DCE in a cited comparison), indicating very high sensitivity in relative terms, though change-of-prediction metric improvements were limited.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Like GPT-J, assigns increased probability mass to correct answers but still exhibits inconsistency across operand permutations, indicating some mixture of algorithmic sensitivity and memorized/spurious associations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Improvement in sensitivity as size increases to 20B; however, not necessarily monotonic improvement in robustness across all metrics vs. instruction-tuned 175B GPT-3.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Similar brittleness: susceptible to result-preserving perturbations producing different predictions (high DCE in δ_cp for more complex settings), and sensitivity sometimes not reflected in predicted-answer changes.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared with GPT-J-6B, GPT-2 family, and GPT-3 Davinci models.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>GPT-NeoX-20B achieves very large relative sensitivity (TCE≫DCE) in some measures but still suffers from brittle prediction changes under result-preserving interventions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e325.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e325.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3 (Davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3 Davinci-002 (instruction-tuned variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An instruction-tuned GPT-3 variant evaluated on arithmetic MWPs using causal interventions; represents an instruction-tuned large model family member.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (Davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>≈175B</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>addition, subtraction, multiplication, division; two-operand and some three-operand MWPs</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>integers in {1..300}, two-operand primary; limited three-operand experiments on Davinci variants</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>statement-form prompts via OpenAI API; do-interventions on operands and templates; probability approximations due to API top-k limitations (k up to 100 in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Instruction-tuned GPT-3 Davinci variants show dramatic improvement in both sensitivity and robustness compared to non-instruction-tuned models; specific Davinci-002 numbers not reported separately in main text, but heatmap in Figure 4 shows much higher probability mass on correct results than smaller models.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Appears to better follow the desired mediator path (O,N → G → R), showing larger changes in confidence for true result when ground-truth changes, suggesting more human-like mediation through the computed G and less reliance on spurious surface cues.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Instruction tuning and massive scale (175B) correlated with emergent robust behavior relative to smaller models; differences between Davinci variants indicate training procedure matters.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Still susceptible to complexity increases (three-operand tasks degrade performance) and to some textual framing variations, but substantially more robust than smaller models.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to non-instruction-tuned GPT variants (GPT-2, GPT-Neo, GPT-J, NeoX) and other GPT-3 variants (Davinci-003, Curie, Instruct).</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Instruction-tuned GPT-3 Davinci variants allocate higher probability to correct results and exhibit a marked improvement in both sensitivity and robustness versus non-instruction-tuned models, suggesting instruction tuning + scale fosters more robust arithmetic behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e325.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e325.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3 (Davinci-003)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3 Davinci-003 (PPO / instruction-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A PPO-trained / instruction-tuned 175B GPT-3 variant that shows the strongest gains in causal sensitivity and robustness on MWPs among evaluated models, but degrades on more complex (three-operand) tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (Davinci-003)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>≈175B</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>addition, subtraction, multiplication, division; evaluated on two- and three-operand MWPs</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>integers in {1..300}; two-operand dominated analyses, three-operand experiments show degradation</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>statement-form prompting via OpenAI API; do-interventions on operands and templates; relative confidence computed with API top-k approximation (k up to 100 when available)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Davinci-003 shows dramatic improvement: e.g., an 84% difference reported between direct and total effect (Section 5.1) and a 76% difference for prediction-change metric on textual framing (Section 5.2). On three-operand problems, DCE (δ_cp) for Davinci-003 increases from 0.17 to 0.87, indicating large brittleness when moving to more complex problems.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Compared to smaller models, Davinci-003 appears to rely more on the correct causal mediation via G (ground-truth) — i.e., when g changes the model's confidence changes appropriately — suggesting instruction tuning and RLHF help align the model to algorithmic or mediator-based reasoning rather than surface correlations. Still, complexity taxes this mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Emergent robust sensitivity at 175B with instruction/RL fine-tuning; however, increased problem complexity (three operands) causes large degradation in robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Substantial brittleness on three-operand problems (predicts different results 87% of the time under result-preserving interventions); sensitive to textual framing variations in some cases despite overall robustness gains.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Directly compared to Davinci-002, Curie, Instruct, and many non-instruction-tuned GPT variants (GPT-2 family, GPT-J, NeoX).</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Davinci-003 (175B, instruction/RLHF-tuned) shows the strongest combination of sensitivity and robustness on two-operand MWPs, but this advantage collapses substantially on three-operand problems, revealing limits to emergent arithmetic robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e325.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e325.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3 (Curie / Instruct variants)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3 instruction-tuned family (Curie, Instruct / davinci-instruct-beta)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Other instruction-tuned GPT-3 variants included for comparison; show intermediate improvements over non-instruction-tuned models but below Davinci-003 on causal sensitivity/robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (Curie / Instruct / davinci-instruct-beta)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>varies (Curie ~6.7B; Instruct is tuned on base GPT-3 sizes)</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>addition, subtraction, multiplication, division (two-operand problems primarily)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>integers in {1..300}, two-operand primarily</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>statement-form prompts via OpenAI API; do-interventions on operands and template variants; probability estimates via top-k API constraints</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Instruction-tuned Curie/Instruct variants show improved sensitivity and robustness relative to non-instruction-tuned models but not to the same degree as Davinci-003; specific numeric breakdowns not provided for each variant.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Instruction tuning shifts model behavior toward greater responsiveness to ground-truth changes (higher TCE) and in some cases increased susceptibility to textual framing (S→R) effects, indicating tuning can change which causal paths the model relies on.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Training procedure (instruction tuning) matters: instruction-tuned variants outperform many non-tuned models of comparable size.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Still exhibit brittleness and sensitivity to template wording in some setups; do not match Davinci-003's robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared across GPT-3 variants and non-instruction-tuned GPT families.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Instruction tuning improves arithmetic sensitivity/robustness compared to non-instruction-tuned models, but the scale and type of instruction/RLHF are important for the magnitude of improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e325.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e325.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA family</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA (7B, 13B, 30B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open models (LLaMA family) evaluated to probe the role of size vs. instruction tuning; tokenization differences required separate handling in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B / 13B / 30B</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>addition, subtraction, multiplication, division (two-operand MWPs); three-operand not broadly evaluated for LLaMA in main text</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>integers in {1..300}; note: LLaMA tokenizer tokenizes each digit individually, complicating multi-digit probability normalization</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>statement-form prompting; do-interventions on operands and templates; probability analysis adapted due to tokenizer differences (couldn't compute RCC metric reliably)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Larger LLaMA sizes show increased difference between TCE and DCE (sensitivity improves with size); LLaMA 7B had ~9.0% TCE_cp–DCE_cp gap whereas larger sizes increased this gap (exact numbers for 13B/30B shown in Figure 6 but not transcribed numerically in main text).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Scaling within LLaMA increases sensitivity-to-ground-truth relative to undesired direct effects, consistent with size-driven emergence of arithmetic-related behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Clearer size-driven improvement within LLaMA family: larger parameter counts correlate with larger TCE–DCE gaps (better sensitivity relative to brittleness).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Tokenizer makes numeric-probability interpretation difficult; still exhibits brittleness and reduced robustness on more complex problems compared to largest instruction-tuned GPT-3 variants.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared across LLaMA sizes and to Alpaca (instruction-tuned LLaMA 7B) and other GPT-family models.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Within LLaMA family, increasing model size correlates with larger sensitivity gains (TCE vs DCE), implicating parameter scale as an important driver of arithmetic sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e325.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e325.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Alpaca (LLaMA-7B tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Alpaca — instruction-tuned LLaMA 7B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A small instruction-tuned model (Stanford Alpaca) based on LLaMA-7B evaluated to investigate the role of instruction tuning focused on user-oriented tasks (not math-focused) on arithmetic robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Alpaca (instruction-tuned LLaMA-7B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>addition, subtraction, multiplication, division (two-operand MWPs)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>integers in {1..300}; two-operand problems</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>instruction tuning evaluation via same prompting and do-intervention framework as LLaMA; RCC metric not computed reliably due to tokenizer</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Instruction tuning via Alpaca decreased both TCE and DCE relative to base LLaMA-7B (i.e., improved robustness at expense of sensitivity), but slightly increased the TCE–DCE gap (9.5% vs 9.0% reported for base LLaMA 7B). No absolute accuracy numbers provided in main sections.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Generic instruction tuning (user-oriented instructions not focused on reasoning) can increase robustness (reduce direct unwanted effects) but may also reduce sensitivity to ground-truth result changes, suggesting instruction data composition matters for arithmetic capability.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Instruction tuning effect depends on content: Alpaca's user-focused instructions produced modest robustness gains but did not improve arithmetic sensitivity like the GPT-3 instruction/RLHF regime.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Reduced sensitivity leads to lower correct responsiveness to result-altering interventions; does not provide the emergent arithmetic gains seen in large GPT-3 Davinci variants.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against base LLaMA-7B and LLaMA larger sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Alpaca's instruction tuning (with general user-oriented instructions) slightly improves robustness but lowers sensitivity, indicating instruction tuning must be targeted to reasoning tasks to boost arithmetic ability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Are NLP models really able to solve simple math word problems? <em>(Rating: 2)</em></li>
                <li>Training language models to follow instructions with human feedback <em>(Rating: 2)</em></li>
                <li>GPT-J-6B: A 6 billion parameter autoregressive language model <em>(Rating: 1)</em></li>
                <li>GPT-NeoX-20B: An open-source autoregressive language model <em>(Rating: 1)</em></li>
                <li>LLaMA: Open and efficient foundation language models <em>(Rating: 2)</em></li>
                <li>Emergent abilities of large language models <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-325",
    "paper_id": "paper-9b45af10429681249fafb07c3b6012ea4ce63ffe",
    "extraction_schema_id": "extraction-schema-14",
    "extracted_data": [
        {
            "name_short": "GPT-2 family",
            "name_full": "GPT-2 (distilled, small, medium, large, XL)",
            "brief_description": "Autoregressive language models from the GPT-2 family evaluated on math word problems using statement-form prompts; serve as smaller-scale baselines in the causal robustness analysis.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-2",
            "model_size": "distilled / small / medium / large / XL",
            "model_architecture": null,
            "arithmetic_operation_type": "addition, subtraction, multiplication, division (two- and some three-operand MWPs)",
            "number_range_or_complexity": "integers in {1..300} (tokens constrained to be single-token numbers), primarily two-operand problems and some three-operand problems",
            "method_or_intervention": "standard prompting (converted question→statement), causal do-interventions on operands and textual templates (result-altering and result-preserving swaps); probability distributions over numeric tokens measured",
            "performance_result": "Small models (Distil and GPT-2 small) exhibit low sensitivity (low TCE) and low robustness improvements; generally low probability mass on correct results and poor consistency across operand permutations (no single numerical performance % reported in main text).",
            "mechanistic_insight": "Tends to show low sensitivity to changes in ground-truth result; exhibits diagonal / periodic assignment of higher probability to certain familiar results, suggesting reliance on surface distributional patterns rather than robust arithmetic reasoning.",
            "performance_scaling": "Larger GPT-2 variants show increased sensitivity relative to distilled/small but still limited robustness; overall trend within GPT-2 family: some improvement with size but not monotonic robust arithmetic mastery.",
            "failure_modes": "Assigns higher probability to a small set of frequent results (patterned diagonals in heatmaps), inconsistent predictions across different operand sets leading to brittleness under result-preserving interventions.",
            "comparison_baseline": "Compared across sizes within GPT-2 family and against larger models (GPT-J, NeoX, GPT-3 Davinci variants).",
            "key_finding": "GPT-2 models are weak at robust arithmetic: increasing size within this family yields only modest sensitivity gains and persistent brittleness to surface/textual and operand perturbations.",
            "uuid": "e325.0",
            "source_info": {
                "paper_title": "A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "GPT-Neo",
            "name_full": "GPT-Neo (1.3B, 2.7B)",
            "brief_description": "Open-source autoregressive language models (EleutherAI GPT-Neo variants) evaluated as mid-scale language model baselines on math word problems under causal interventions.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-Neo",
            "model_size": "1.3B / 2.7B",
            "model_architecture": null,
            "arithmetic_operation_type": "addition, subtraction, multiplication, division (two- and some three-operand MWPs)",
            "number_range_or_complexity": "integers in {1..300}, primarily two-operand problems",
            "method_or_intervention": "standard prompting (statement-form) and causal do-interventions on operands and templates; measured changes in predicted result and probability assigned to ground-truth",
            "performance_result": "Larger GPT-Neo variants show increased sensitivity compared to small GPT-2 but still show similar sensitivity to result-altering and result-preserving interventions (i.e., limited robustness); no single accuracy number in main text.",
            "mechanistic_insight": "Behaviors indicate models pick up spurious correlations between numerical tokens and outputs rather than reliably mediating via correct result G; TCE and DCE patterns consistent with partial reliance on operand and template surface cues.",
            "performance_scaling": "Some improvement with model scale (1.3B→2.7B) in sensitivity but not in robust correct responsiveness; no emergent robust arithmetic ability reported.",
            "failure_modes": "Responds similarly to result-altering and result-preserving operand changes (poor robustness); susceptible to surface-form variations.",
            "comparison_baseline": "Compared against GPT-2 family, GPT-J, NeoX, and GPT-3 variants.",
            "key_finding": "GPT-Neo models gain sensitivity with size but do not reliably separate correct sensitivity to ground-truth from spurious direct effects, i.e., not robust arithmetic solvers.",
            "uuid": "e325.1",
            "source_info": {
                "paper_title": "A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "GPT-J-6B",
            "name_full": "GPT-J (6B)",
            "brief_description": "A 6-billion-parameter autoregressive language model evaluated on math word problems; shows increased sensitivity to ground-truth results but also substantial direct effects in some metrics.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-J",
            "model_size": "6B",
            "model_architecture": null,
            "arithmetic_operation_type": "addition, subtraction, multiplication, division (two-operand focus in main analyses)",
            "number_range_or_complexity": "integers in {1..300}, heatmaps shown for 0..50 for visualization; two-operand problems primarily",
            "method_or_intervention": "statement-form prompts; do-interventions on operands (result-altering and result-preserving) and templates; probability heatmaps over resulting numeric token",
            "performance_result": "Shows high TCE relative to DCE for operands: reported TCE approximately 30x larger than DCE (sensitivity much larger than undesired direct effect) — but this did not fully translate into change-of-prediction metric (δ_cp) improvements.",
            "mechanistic_insight": "Assigns higher overall probability mass to correct results compared to small models, but inconsistent across different operand pairs for same ground-truth; suggests partial learning of correct mapping but remaining reliance on operand-specific or surface associations.",
            "performance_scaling": "Higher sensitivity than smaller models; improvement with size up to 6B apparent but not equivalent to behavior of very large instruction-tuned models.",
            "failure_modes": "Despite higher probability mass for correct answer, predictions vary when operands are permuted even if result same (high DCE on δ_cp), indicating brittle reliance on operand patterns.",
            "comparison_baseline": "Compared to smaller GPT-2 variants, GPT-Neo, NeoX-20B, and GPT-3 Davinci models.",
            "key_finding": "GPT-J-6B demonstrates substantially greater sensitivity to correct results (TCE ≫ DCE) but remains brittle in prediction changes across result-preserving operand perturbations.",
            "uuid": "e325.2",
            "source_info": {
                "paper_title": "A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "GPT-NeoX-20B",
            "name_full": "GPT-NeoX (20B)",
            "brief_description": "A 20-billion-parameter open-source autoregressive language model evaluated for causal robustness on MWPs; exhibits strong sensitivity (TCE) increases but mixed robustness.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-NeoX",
            "model_size": "20B",
            "model_architecture": null,
            "arithmetic_operation_type": "addition, subtraction, multiplication, division (two-operand problems primarily)",
            "number_range_or_complexity": "integers in {1..300}, two-operand problems",
            "method_or_intervention": "statement-form prompts; operand and template do-interventions; probability distributions over numeric tokens analyzed",
            "performance_result": "Reported TCE for operands much larger than DCE in some analyses (NeoX TCE reported ≈1000× DCE in a cited comparison), indicating very high sensitivity in relative terms, though change-of-prediction metric improvements were limited.",
            "mechanistic_insight": "Like GPT-J, assigns increased probability mass to correct answers but still exhibits inconsistency across operand permutations, indicating some mixture of algorithmic sensitivity and memorized/spurious associations.",
            "performance_scaling": "Improvement in sensitivity as size increases to 20B; however, not necessarily monotonic improvement in robustness across all metrics vs. instruction-tuned 175B GPT-3.",
            "failure_modes": "Similar brittleness: susceptible to result-preserving perturbations producing different predictions (high DCE in δ_cp for more complex settings), and sensitivity sometimes not reflected in predicted-answer changes.",
            "comparison_baseline": "Compared with GPT-J-6B, GPT-2 family, and GPT-3 Davinci models.",
            "key_finding": "GPT-NeoX-20B achieves very large relative sensitivity (TCE≫DCE) in some measures but still suffers from brittle prediction changes under result-preserving interventions.",
            "uuid": "e325.3",
            "source_info": {
                "paper_title": "A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "GPT-3 (Davinci-002)",
            "name_full": "GPT-3 Davinci-002 (instruction-tuned variant)",
            "brief_description": "An instruction-tuned GPT-3 variant evaluated on arithmetic MWPs using causal interventions; represents an instruction-tuned large model family member.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3 (Davinci-002)",
            "model_size": "≈175B",
            "model_architecture": null,
            "arithmetic_operation_type": "addition, subtraction, multiplication, division; two-operand and some three-operand MWPs",
            "number_range_or_complexity": "integers in {1..300}, two-operand primary; limited three-operand experiments on Davinci variants",
            "method_or_intervention": "statement-form prompts via OpenAI API; do-interventions on operands and templates; probability approximations due to API top-k limitations (k up to 100 in experiments)",
            "performance_result": "Instruction-tuned GPT-3 Davinci variants show dramatic improvement in both sensitivity and robustness compared to non-instruction-tuned models; specific Davinci-002 numbers not reported separately in main text, but heatmap in Figure 4 shows much higher probability mass on correct results than smaller models.",
            "mechanistic_insight": "Appears to better follow the desired mediator path (O,N → G → R), showing larger changes in confidence for true result when ground-truth changes, suggesting more human-like mediation through the computed G and less reliance on spurious surface cues.",
            "performance_scaling": "Instruction tuning and massive scale (175B) correlated with emergent robust behavior relative to smaller models; differences between Davinci variants indicate training procedure matters.",
            "failure_modes": "Still susceptible to complexity increases (three-operand tasks degrade performance) and to some textual framing variations, but substantially more robust than smaller models.",
            "comparison_baseline": "Compared to non-instruction-tuned GPT variants (GPT-2, GPT-Neo, GPT-J, NeoX) and other GPT-3 variants (Davinci-003, Curie, Instruct).",
            "key_finding": "Instruction-tuned GPT-3 Davinci variants allocate higher probability to correct results and exhibit a marked improvement in both sensitivity and robustness versus non-instruction-tuned models, suggesting instruction tuning + scale fosters more robust arithmetic behavior.",
            "uuid": "e325.4",
            "source_info": {
                "paper_title": "A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "GPT-3 (Davinci-003)",
            "name_full": "GPT-3 Davinci-003 (PPO / instruction-tuned)",
            "brief_description": "A PPO-trained / instruction-tuned 175B GPT-3 variant that shows the strongest gains in causal sensitivity and robustness on MWPs among evaluated models, but degrades on more complex (three-operand) tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3 (Davinci-003)",
            "model_size": "≈175B",
            "model_architecture": null,
            "arithmetic_operation_type": "addition, subtraction, multiplication, division; evaluated on two- and three-operand MWPs",
            "number_range_or_complexity": "integers in {1..300}; two-operand dominated analyses, three-operand experiments show degradation",
            "method_or_intervention": "statement-form prompting via OpenAI API; do-interventions on operands and templates; relative confidence computed with API top-k approximation (k up to 100 when available)",
            "performance_result": "Davinci-003 shows dramatic improvement: e.g., an 84% difference reported between direct and total effect (Section 5.1) and a 76% difference for prediction-change metric on textual framing (Section 5.2). On three-operand problems, DCE (δ_cp) for Davinci-003 increases from 0.17 to 0.87, indicating large brittleness when moving to more complex problems.",
            "mechanistic_insight": "Compared to smaller models, Davinci-003 appears to rely more on the correct causal mediation via G (ground-truth) — i.e., when g changes the model's confidence changes appropriately — suggesting instruction tuning and RLHF help align the model to algorithmic or mediator-based reasoning rather than surface correlations. Still, complexity taxes this mechanism.",
            "performance_scaling": "Emergent robust sensitivity at 175B with instruction/RL fine-tuning; however, increased problem complexity (three operands) causes large degradation in robustness.",
            "failure_modes": "Substantial brittleness on three-operand problems (predicts different results 87% of the time under result-preserving interventions); sensitive to textual framing variations in some cases despite overall robustness gains.",
            "comparison_baseline": "Directly compared to Davinci-002, Curie, Instruct, and many non-instruction-tuned GPT variants (GPT-2 family, GPT-J, NeoX).",
            "key_finding": "Davinci-003 (175B, instruction/RLHF-tuned) shows the strongest combination of sensitivity and robustness on two-operand MWPs, but this advantage collapses substantially on three-operand problems, revealing limits to emergent arithmetic robustness.",
            "uuid": "e325.5",
            "source_info": {
                "paper_title": "A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "GPT-3 (Curie / Instruct variants)",
            "name_full": "GPT-3 instruction-tuned family (Curie, Instruct / davinci-instruct-beta)",
            "brief_description": "Other instruction-tuned GPT-3 variants included for comparison; show intermediate improvements over non-instruction-tuned models but below Davinci-003 on causal sensitivity/robustness.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3 (Curie / Instruct / davinci-instruct-beta)",
            "model_size": "varies (Curie ~6.7B; Instruct is tuned on base GPT-3 sizes)",
            "model_architecture": null,
            "arithmetic_operation_type": "addition, subtraction, multiplication, division (two-operand problems primarily)",
            "number_range_or_complexity": "integers in {1..300}, two-operand primarily",
            "method_or_intervention": "statement-form prompts via OpenAI API; do-interventions on operands and template variants; probability estimates via top-k API constraints",
            "performance_result": "Instruction-tuned Curie/Instruct variants show improved sensitivity and robustness relative to non-instruction-tuned models but not to the same degree as Davinci-003; specific numeric breakdowns not provided for each variant.",
            "mechanistic_insight": "Instruction tuning shifts model behavior toward greater responsiveness to ground-truth changes (higher TCE) and in some cases increased susceptibility to textual framing (S→R) effects, indicating tuning can change which causal paths the model relies on.",
            "performance_scaling": "Training procedure (instruction tuning) matters: instruction-tuned variants outperform many non-tuned models of comparable size.",
            "failure_modes": "Still exhibit brittleness and sensitivity to template wording in some setups; do not match Davinci-003's robustness.",
            "comparison_baseline": "Compared across GPT-3 variants and non-instruction-tuned GPT families.",
            "key_finding": "Instruction tuning improves arithmetic sensitivity/robustness compared to non-instruction-tuned models, but the scale and type of instruction/RLHF are important for the magnitude of improvement.",
            "uuid": "e325.6",
            "source_info": {
                "paper_title": "A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "LLaMA family",
            "name_full": "LLaMA (7B, 13B, 30B)",
            "brief_description": "Open models (LLaMA family) evaluated to probe the role of size vs. instruction tuning; tokenization differences required separate handling in experiments.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA",
            "model_size": "7B / 13B / 30B",
            "model_architecture": null,
            "arithmetic_operation_type": "addition, subtraction, multiplication, division (two-operand MWPs); three-operand not broadly evaluated for LLaMA in main text",
            "number_range_or_complexity": "integers in {1..300}; note: LLaMA tokenizer tokenizes each digit individually, complicating multi-digit probability normalization",
            "method_or_intervention": "statement-form prompting; do-interventions on operands and templates; probability analysis adapted due to tokenizer differences (couldn't compute RCC metric reliably)",
            "performance_result": "Larger LLaMA sizes show increased difference between TCE and DCE (sensitivity improves with size); LLaMA 7B had ~9.0% TCE_cp–DCE_cp gap whereas larger sizes increased this gap (exact numbers for 13B/30B shown in Figure 6 but not transcribed numerically in main text).",
            "mechanistic_insight": "Scaling within LLaMA increases sensitivity-to-ground-truth relative to undesired direct effects, consistent with size-driven emergence of arithmetic-related behavior.",
            "performance_scaling": "Clearer size-driven improvement within LLaMA family: larger parameter counts correlate with larger TCE–DCE gaps (better sensitivity relative to brittleness).",
            "failure_modes": "Tokenizer makes numeric-probability interpretation difficult; still exhibits brittleness and reduced robustness on more complex problems compared to largest instruction-tuned GPT-3 variants.",
            "comparison_baseline": "Compared across LLaMA sizes and to Alpaca (instruction-tuned LLaMA 7B) and other GPT-family models.",
            "key_finding": "Within LLaMA family, increasing model size correlates with larger sensitivity gains (TCE vs DCE), implicating parameter scale as an important driver of arithmetic sensitivity.",
            "uuid": "e325.7",
            "source_info": {
                "paper_title": "A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "Alpaca (LLaMA-7B tuned)",
            "name_full": "Alpaca — instruction-tuned LLaMA 7B",
            "brief_description": "A small instruction-tuned model (Stanford Alpaca) based on LLaMA-7B evaluated to investigate the role of instruction tuning focused on user-oriented tasks (not math-focused) on arithmetic robustness.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Alpaca (instruction-tuned LLaMA-7B)",
            "model_size": "7B",
            "model_architecture": null,
            "arithmetic_operation_type": "addition, subtraction, multiplication, division (two-operand MWPs)",
            "number_range_or_complexity": "integers in {1..300}; two-operand problems",
            "method_or_intervention": "instruction tuning evaluation via same prompting and do-intervention framework as LLaMA; RCC metric not computed reliably due to tokenizer",
            "performance_result": "Instruction tuning via Alpaca decreased both TCE and DCE relative to base LLaMA-7B (i.e., improved robustness at expense of sensitivity), but slightly increased the TCE–DCE gap (9.5% vs 9.0% reported for base LLaMA 7B). No absolute accuracy numbers provided in main sections.",
            "mechanistic_insight": "Generic instruction tuning (user-oriented instructions not focused on reasoning) can increase robustness (reduce direct unwanted effects) but may also reduce sensitivity to ground-truth result changes, suggesting instruction data composition matters for arithmetic capability.",
            "performance_scaling": "Instruction tuning effect depends on content: Alpaca's user-focused instructions produced modest robustness gains but did not improve arithmetic sensitivity like the GPT-3 instruction/RLHF regime.",
            "failure_modes": "Reduced sensitivity leads to lower correct responsiveness to result-altering interventions; does not provide the emergent arithmetic gains seen in large GPT-3 Davinci variants.",
            "comparison_baseline": "Compared against base LLaMA-7B and LLaMA larger sizes.",
            "key_finding": "Alpaca's instruction tuning (with general user-oriented instructions) slightly improves robustness but lowers sensitivity, indicating instruction tuning must be targeted to reasoning tasks to boost arithmetic ability.",
            "uuid": "e325.8",
            "source_info": {
                "paper_title": "A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models",
                "publication_date_yy_mm": "2022-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Are NLP models really able to solve simple math word problems?",
            "rating": 2
        },
        {
            "paper_title": "Training language models to follow instructions with human feedback",
            "rating": 2
        },
        {
            "paper_title": "GPT-J-6B: A 6 billion parameter autoregressive language model",
            "rating": 1
        },
        {
            "paper_title": "GPT-NeoX-20B: An open-source autoregressive language model",
            "rating": 1
        },
        {
            "paper_title": "LLaMA: Open and efficient foundation language models",
            "rating": 2
        },
        {
            "paper_title": "Emergent abilities of large language models",
            "rating": 2
        }
    ],
    "cost": 0.017596749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models</h1>
<p>Alessandro Stolfo*<br>ETH Zürich<br>stolfoa@ethz.ch</p>
<p>Kumar Shridhar<br>ETH Zürich<br>shkumar@ethz.ch</p>
<p>Bernhard Schölkopf
MPI \&amp; ETH Zürich
bs@tue.mpg.de</p>
<h2>Zhijing Jin*</h2>
<p>MPI \&amp; ETH Zürich
jinzhi@ethz.ch</p>
<h2>Mrinmaya Sachan</h2>
<p>ETH Zürich
msachan@ethz.ch</p>
<h2>Abstract</h2>
<p>We have recently witnessed a number of impressive results on hard mathematical reasoning problems with language models. At the same time, the robustness of these models has also been called into question; recent works have shown that models can rely on shallow patterns in the problem description when generating a solution. Building on the idea of behavioral testing, we propose a novel framework, which pins down the causal effect of various factors in the input, e.g., the surface form of the problem text, the operands, and math operators on the output solution. By grounding the behavioral analysis in a causal graph describing an intuitive reasoning process, we study the behavior of language models in terms of robustness and sensitivity to direct interventions in the input space. We apply our framework on a test bed of math word problems. Our analysis shows that robustness does not appear to continuously improve as a function of size, but the GPT-3 Davinci models (175B) achieve a dramatic improvement in both robustness and sensitivity compared to all other GPT variants. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>Many natural language understanding situations, such as understanding the financial news, require reasoning with text that includes numbers. However, such mathematical reasoning is challenging for NLP models (Cobbe et al., 2021; Mishra et al., 2022b). Mathematical reasoning for text has been an active area of research for a while (Seo et al., 2015; Sachan and Xing, 2017; Sachan et al., 2017, 2018, inter alia), and has also emerged as a key task to track the capabilities of large language models (LLMs) in recent years (Brown et al., 2020; Ouyang et al., 2022; Wei et al., 2022a, inter alia).</p>
<p>However, despite the impressive performance of LLMs on various math reasoning benchmarks (e.g.,</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Through our framework, we conduct dointerventions on the input and evaluate the change in the distribution $\mathbb{P}(R)$ of the prediction $R$ by LLMs, in this figure, GPT-J. This allows us to measure the causal effect of each factor in the input on the model's response.</p>
<p>Ouyang et al., 2022; Chowdhery et al., 2022), it remains unclear whether these models have learned mere artifacts in the data or have truly mastered the mathematical concepts needed to consistently solve all variations of the same problem (Patel et al., 2021; Razeghi et al., 2022; Welleck et al., 2022). In sharp contrast with a large number of papers on improving the performance of LLMs on various types of math-based problems, there has been little effort on behavioral analysis of LLMs for these tasks. Existing methods for understanding the robustness of these models (Patel et al., 2021) rely on manually constructing variations of math problems, and we do not yet have a principled, comprehensive framework for quantifying such robustness.</p>
<p>Thus, in this work, we propose a formal framework based on causal inference, to quantify the robustness of NLP models' math reasoning abilities. Specifically, we describe a causal graph formulation of math reasoning, where the graph allows us to measure the difference in the structural causal</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Causal graph of model predictions on math questions. We highlight the difference between a cognitively-inspired correct reasoning path (G<sup>L</sup>) and the undesired effects that some factors might have on the model's prediction (red arrows). By performing controlled interventions of the numerical values (N) and on the textual framing of the problem (T, S), we are able to quantify the causal effects of each factor.</p>
<p>models of human reasoning and model judgment. We consider various causal factors such as the textual framing of the question, numerical operands, and operation types. Then, we identify a set of interventions in the context of math word problems (an example of which is illustrated in Figure 1), and provide a causal inference framework to obtain causal effects of each factor via direct do-interventions (Pearl, 1995) and causal mediation analysis (Pearl, 2001). While our approach is reminiscent of recent studies using causal analysis for LLMs (Finlayson et al., 2021; Vig et al., 2020; Meng et al., 2022), in this work, we provide a new theoretical analysis framework specifically suitable for math reasoning. Using our framework, we disentangle factors affecting the model's predictions and measure their influences. This way, we are able to provide insights into the model's reasoning in terms of <em>robustness</em> and <em>sensitivity</em> with respect to changes in these factors.</p>
<p>We apply our framework to study a set of thirteen GPT models with various sizes and training procedures (i.e., instruction-tuned and non-instruction-tuned). We observe that, among non-instruction-tuned language models, the larger ones tend to be more sensitive to changes in the ground-truth result of a math word problem, but not necessarily more robust. However, we observe a different behavior in the instruction-tuned GPT-3 models (Ouyang et al., 2022), which show a remarkable improvement in both sensitivity and robustness, although the robustness reduces when problems get more complicated. We additionally investigate the role of size and instruction tuning on the model's performance with three models of the LLaMA family (Touvron et al., 2023) and Stanford Alpaca (Taori et al., 2023).</p>
<h2>2 Problem Setup</h2>
<p>We consider a dataset D of math word problems (MWPs), where each MWP is denoted as a question Q. Q is a list (T, N) consisting of a question template T and an ordered list of operands N = (N<sub>1</sub>, N<sub>2</sub>, . . . , N<sub>m</sub>). Each question template T := (O, S) further contains two types of information: a set of arithmetic operations O implicitly expressed in the question, and the text surface form S irrelevant to the arithmetic operations. O incorporates the information relative to the operations as a collection of tuples {(O<sub>1</sub>, i<sub>1</sub>, j<sub>1</sub>), (O<sub>2</sub>, i<sub>2</sub>, j<sub>2</sub>), . . . }, where O<sub>k</sub> ∈ {+, −, ×, 亠} (k ∈ ℕ) and i<sub>k</sub>, j<sub>k</sub> ∈ ℕ represent the indices of the operands to which operator O<sub>k</sub> should be applied to.<sup>2</sup> The ground-truth result G = f<sub>O</sub>(N) is calculated by computing the function f<sub>O</sub>, which represents the application of all the operators in O to the respective operands. We illustrate the factors in Q and their inter-dependency in the causal graph in Figure 2. A two-operand instance q of Q in this form from Patel et al. (2021) is:</p>
<ul>
<li>Template t: Mark has n<sub>1</sub> trees in his backyard. If he plants n<sub>2</sub> more, how many trees will he have?</li>
<li>Operands n: (n<sub>1</sub> = 12, n<sub>2</sub> = 13)</li>
<li>Operations o: {(“+”, 1, 2)}</li>
<li>Result: g = f<sub>o</sub>(n) = n<sub>1</sub> + n<sub>2</sub> = 25</li>
</ul>
<p><sup>2</sup>The intermediate result of operation O<sub>l</sub> is indicated by i<sub>k</sub> = m + l.</p>
<p>Our goal is to quantify the robustness of a model $\mathcal{M}$ on the set of problems $\boldsymbol{q} \in \mathcal{D}$. Ideally, $\mathcal{D}$ should be a dataset not seen by the model during training. We assume that a model takes $\boldsymbol{q}$ as input and predicts a probability distribution of the result $R: \mathbb{P}(R \mid \boldsymbol{t}, \boldsymbol{n})$. Our formulation below will be easier to understand using this finite discrete set and can be generalized to any kind of data pairing a natural language template with a function that maps a set of operands to a result (e.g., a Python program; Mishra et al. 2022a).</p>
<h2>3 A Causal Framework</h2>
<p>In this section, we describe our framework in three steps. First, we define the idea of model robustness on MWPs. Then, we identify possible dointerventions (Pearl, 1995) that we can perform. Finally, we describe the causal effects that we measure to quantify the robustness of various models.</p>
<h3>3.1 Step 1. Question Reformulation</h3>
<p>We address the research question "Is a model reasoning robustly on MWPs?" by comparing the causal mechanisms of the model's decisions to a hypothesized human reasoning mechanism. Note that we do not claim to know how humans reason about these problems. We simply propose a reasonable and intuitive way to judge model robustness given a reasonable and intuitive human reasoning mechanism inspired by findings regarding the independence of language and mathematical reasoning in humans (Brannon, 2005; Monti et al., 2012).</p>
<p>Human Reasoning Mechanisms. The causal mechanisms of how humans might solve $\boldsymbol{q}$ include</p>
<p>$$
\begin{aligned}
\boldsymbol{o} &amp; =f_{\text {abstract }}(\boldsymbol{q}) \
g &amp; =f_{\boldsymbol{o}}(\boldsymbol{n})
\end{aligned}
$$</p>
<p>where they first abstract the arithmetic operations $\boldsymbol{o}$ from the problem $\boldsymbol{q}$ by some cognitive process $f_{\text {abstract }}$, and then apply the operation to the operands to obtain the result $g$. We show these mechanisms in the green subgraph $\mathcal{G}_{h}$ of Figure 2.</p>
<p>Model Reasoning Mechanisms. In contrast, the causal mechanisms of how a model might solve $\boldsymbol{q}$ are as follows:</p>
<p>$$
r=f_{\text {blackBox }}(\boldsymbol{t}, \boldsymbol{n})
$$</p>
<p>where we are unsure about (1) what part(s) of $\boldsymbol{t}$ the model takes into account, and (2) how it operates over the relevant variables.</p>
<p>Thus, we draw all possible causal mechanisms that might take place in the black-box model $f_{\text {blackBox }}$ in the complete causal graph in Figure 2. Some possible fine-grained causal mechanisms are</p>
<ol>
<li>The model might attend over the question template $\boldsymbol{t}$ in two ways: paying attention to the text surface form $s$ via the causal path $\boldsymbol{T} \rightarrow S \rightarrow R$, or text relevant to the math operations $\boldsymbol{o}$ via the causal path $\boldsymbol{T} \rightarrow \boldsymbol{O} \rightarrow R$.</li>
<li>The model might also attend to the operands $\boldsymbol{n}:=\left(n_{1}, n_{2}, \ldots\right)$ via a causal path $\boldsymbol{N} \rightarrow R$.</li>
<li>If the model learns the correct causal mechanisms as in the human cognitive process, it should capture how the operator and the operands matter to the ground-truth result $g$ (via $\boldsymbol{O} \rightarrow G$ and $\boldsymbol{N} \rightarrow G$ ) and then the model prediction should be sensitive to any changes in the ground truth, namely $G \rightarrow R$. No spurious correlations can directly affect $R$ without going through the mediator $G$.
Hence, to answer the question "How robust is the mathematical reasoning of a model on MWPs?" we can answer the following subquestions:</li>
<li>How does $R$ change in response to $G$ ? By quantifying this, we assess the sensitivity (correct responsiveness) of the model to changes in the problem. In other words, does the model correctly adjust its prediction in response to a change in the correct solution of the problem?</li>
<li>What is the (unwanted) direct causal effect size of $S \rightarrow R$, and $\boldsymbol{N} \rightarrow R$ ? We see the quantities as a measure of the brittleness (i.e., wrong responsiveness) of the model to resultpreserving changes in the input. The lower the direct causal effect of $S$ and $\boldsymbol{N}$, the more robust the model is.</li>
</ol>
<h3>3.2 Step 2. Causal Intervention List</h3>
<p>After formulating the cognitively-inspired subgraph $\mathcal{G}_{h}$ and defining the undesired causal paths in Figure 2, we list all feasible limited actions that allow us to perform our causal analysis. In the context of MWPs, we use the following interventions:</p>
<ol>
<li>Direct intervention on all possible $n_{1}, n_{2}, \ldots$;</li>
<li>Partially controllable interventions on $\boldsymbol{T}$. We can replace the template $\boldsymbol{T}$ in two ways:</li>
</ol>
<p>(a) both $S$ and $\boldsymbol{O}$ are affected, or
(b) $S$ is affected but $\boldsymbol{O}$ is not affected.</p>
<h3>3.3 Step 3. Turning Limited Actions into Causal Effect Sizes</h3>
<p>Next, we explain how we can obtain the causal effect sizes we want (listed in Step 1) from the limited set of interventions we can do (listed in Step 2). Specifically, we first start from all the feasible interventions, and for variables that we cannot directly intervene on, we apply deductions from do-calculus (Pearl, 1995) to obtain or approximate the direct causal effect sizes. In the following, we describe a list of causal effect sizes that we need.</p>
<p>General Formulation. Let us consider an intervention $\operatorname{do}\left(X: x \rightarrow x^{\prime}\right)$, where $X \in{\boldsymbol{T}, S, \boldsymbol{N}}$ and a problem $\boldsymbol{Q}={\boldsymbol{T}, \boldsymbol{N}}$. The support of the numerical values $N_{i}$ 's and $R$ is $\mathcal{I} \subseteq \mathbb{N}$, and we consider $\boldsymbol{N}$ to be distributed uniformly over the set $\left{\boldsymbol{n} \in \mathcal{I}^{2} \mid f_{\boldsymbol{O}}(\boldsymbol{n}) \in \mathcal{I}\right}$. We denote the distribution before the intervention $\mathbb{P}(R \mid \boldsymbol{T}, \boldsymbol{N})$ as $P$ and the distribution after the intervention as $P^{\prime}$.</p>
<p>Following the distributional definition of causal effect by Pearl (1995), we quantify the effect of factor $X$ in our causal graph using a distance metric $\delta$ between the distributions $P$ and $P^{\prime}$. That is,</p>
<p>$$
\mathrm{CE}=\delta\left(P, P^{\prime}\right)
$$</p>
<p>where CE can refer to the total causal effect (TCE, i.e., the joint effect through all the directed causal paths from a variable to another), or the direct causal effect (DCE, i.e., the effect from the directed causal path from a variable to another that does not go through any intermediate variables) (Pearl, 2001). We describe our choices for $\delta$ in Section 3.4.</p>
<p>Causal Effects of the Operands. When intervening on the operands $\boldsymbol{N}:=\left(N_{1}, N_{2}, \ldots\right)$, we can obtain the size of the total causal effect of $\boldsymbol{N}$ on $R$, namely</p>
<p>$$
\begin{aligned}
&amp; \operatorname{TCE}(\boldsymbol{N} \text { on } R):=\mathbb{E}_{\boldsymbol{n}^{\prime} \sim \mathbb{P}(\boldsymbol{N})}\left[\delta\left(P, P^{\prime}\right)\right] \
&amp; \text { where } P^{\prime}=\mathbb{P}\left(R \mid \boldsymbol{T}, \operatorname{do}\left(\boldsymbol{N}=\boldsymbol{n}^{\prime}\right)\right)
\end{aligned}
$$</p>
<p>Note that this TCE is not the exact desired quantity, because we want to separate two different paths of how $\boldsymbol{N}$ affects $R$ : (1) the path $\boldsymbol{N} \rightarrow G \rightarrow R$, which is the correct decision path that we want the model to pick up (where the model reacts to
the change in the ground-truth answer), and (2) the path $\boldsymbol{N} \rightarrow R$, which is the spurious correlation that the model might have learned (where the model relies on some spurious correlations with certain numerical values, which could be traced to perhaps their frequencies in the training corpus).</p>
<p>We can quantify the direct causal effect (DCE, i.e., the effect from the directed causal path from a variable to another that does not go through any intermediate variables) (Pearl, 2001) of $\boldsymbol{N}$ on $R$, namely the strength of the direct causal path $\boldsymbol{N} \rightarrow$ $R$, by controlling for $G$ to be fixed every time we intervene on $\boldsymbol{N}$ :</p>
<p>$$
\begin{aligned}
&amp; \operatorname{DCE}(\boldsymbol{N} \rightarrow R):=\mathbb{E}_{\boldsymbol{n}^{\prime} \sim \mathbb{P}(\boldsymbol{N} \mid G)}\left[\delta\left(P, P^{\prime}\right)\right] \
&amp; \text { where } P^{\prime}=\mathbb{P}\left(R \mid \boldsymbol{T}, \operatorname{do}\left(\boldsymbol{N}=\boldsymbol{n}^{\prime}\right)\right)
\end{aligned}
$$</p>
<p>For example, if we observe a model doing $100+$ $100=200$ correctly, we want to separate the math ability here into (1) the model's sensitivity towards the ground-truth answer, and (2) the model's decisions based on its familiarity with just the operand 100. Here, the overall effect is the calculable $\operatorname{TCE}(\boldsymbol{N}$ on $R)$ by Eq. 5, and one of the subeffects is the calculable $\operatorname{DCE}(\boldsymbol{N} \rightarrow R)$ by Eq. 7.</p>
<p>Causal Effects of the Text Surface Form. As for the operands, we can compute both the direct and indirect effects of the surface form representing the math problem. In particular, intervening on $\boldsymbol{T}$ without controlling for $\boldsymbol{O}$ (intervention 2a in Sec. 3.2), we can compute the total effect, i.e.,</p>
<p>$$
\begin{aligned}
&amp; \operatorname{TCE}(\boldsymbol{T} \text { on } R):=\mathbb{E}_{\boldsymbol{t}^{\prime} \sim \mathbb{P}(\boldsymbol{T})}\left[\delta\left(P, P^{\prime}\right)\right] \
&amp; \text { where } P^{\prime}=\mathbb{P}\left(R \mid \boldsymbol{N}, \operatorname{do}\left(\boldsymbol{T}=\boldsymbol{t}^{\prime}\right)\right)
\end{aligned}
$$</p>
<p>Controlling for the operations $\boldsymbol{O}$ (intervention 2 b in Sec. 3.2) will instead allow us to obtain the direct causal effect of the surface text:</p>
<p>$$
\begin{aligned}
&amp; \operatorname{DCE}(S \rightarrow R):=\mathbb{E}_{\boldsymbol{t}^{\prime} \sim \mathbb{P}(\boldsymbol{T} \mid O)}\left[\delta\left(P, P^{\prime}\right)\right] \
&amp; \text { where } P^{\prime}=\mathbb{P}\left(R \mid \boldsymbol{N}, \operatorname{do}\left(\boldsymbol{T}=\boldsymbol{t}^{\prime}\right)\right)
\end{aligned}
$$</p>
<p>Note that since there is no mediator between $S$ and $R$, the $\operatorname{DCE}(S \rightarrow R)$ is also TCE of $S$ on $R$. The only adaptation that we need to make with regard to the MWPs is that it is not feasible to enumerate all possible perturbations of $S$. Therefore, the practical results that researchers can achieve are over a certain subset of $S$. In practice, we obtain this by intervening on $\boldsymbol{T}$ without affecting $\boldsymbol{O}$.</p>
<p>Causal Effects of the Operators. The ideal way to obtain the TCE of $\boldsymbol{O}$ on $R$ is through some careful human annotation that minimally changes the templates as Kaushik et al. (2020) do for sentiment classification. The challenge for MWPs in our case is that with all our possible interventions, we cannot only intervene on $\boldsymbol{O}$ without introducing changes to the irrelevant surface form. However, we might get some information about $\operatorname{TCE}(\boldsymbol{O}$ on $R)$ because, on the causal graph, the total causal influence of $\boldsymbol{T}$ on $R$ actually flows into two directed paths, one through $S$ to $R$ (which is the $\operatorname{DCE}(S \rightarrow R)$ ), and the other from $\boldsymbol{O}$ to $R$, which is our interested quantity $\operatorname{TCE}(\boldsymbol{O}$ on $R)$. Therefore, we compare the two quantities we know, $\operatorname{TCE}(\boldsymbol{T} \rightarrow R)$ and $\operatorname{DCE}(S \rightarrow R)$, to get a sense of the causal influence of $\boldsymbol{O}$ on $R$ that we cannot obtain in any other way.</p>
<h3>3.4 Step 4. Quantifying the Causal Influence</h3>
<p>Consider a realization of problem $\boldsymbol{Q}$ with operands $\boldsymbol{n}$ and ground-truth result $g=f_{\boldsymbol{o}}(\boldsymbol{n})$, and denote by $g^{\prime}$ the result after the intervention do $(X: x \rightarrow$ $\left.x^{\prime}\right)$. We quantify the causal effect of factor $X$ on the model's prediction $R$ in two ways: by assessing the change in the predicted result, and by measuring the change in the probability assigned by the model to the correct result $g$ (or $g^{\prime}$ ).
Change in the Prediction. To account for the inability of LMs to capture the continuous property of numbers (Jin et al., 2021a), we measure the change in the model's prediction using an indicator of the "change result" event:</p>
<p>$$
\delta_{\mathrm{cp}}\left(P, P^{\prime}\right):=\mathbb{1}\left(r \neq r^{\prime}\right)
$$</p>
<p>where $r=\arg \max <em _in="\in" _mathcal_I="\mathcal{I" x="x">{x \in \mathcal{I}} P(x)$, and $r^{\prime}=$ $\arg \max </em>(x)$.
Relative Change in Confidence. Inspired by Finlayson et al. (2021), we also highlight the change in terms of the relative difference in the probability assigned to $g$ and $g^{\prime}$. We formulate two types of relative change, one quantifying the relative change in the confidence of $g$, and the other quantifying the relative change in the confidence of $g^{\prime}$ :}} P^{\prime</p>
<p>$$
\begin{aligned}
&amp; \Delta_{\text {rel }}=\frac{P(g)-P^{\prime}(g)}{P^{\prime}(g)} \
&amp; \Delta_{\text {rel }}^{\prime}=\frac{P^{\prime}\left(g^{\prime}\right)-P\left(g^{\prime}\right)}{P\left(g^{\prime}\right)}
\end{aligned}
$$</p>
<p>We quantify the overall relative change in confidence (RCC) as the average of the two relative
changes above:</p>
<p>$$
\delta_{\mathrm{rcc}}\left(P, P^{\prime}\right)=\frac{1}{2}\left(\Delta_{\mathrm{rel}}+\Delta_{\mathrm{rel}}^{\prime}\right)
$$</p>
<p>A Unified Form. We are interested in the average causal effect of the intervention across all problems in $\mathcal{D}$. Thus, we measure the average of the effects over all instances $\boldsymbol{q} \in \mathcal{D}$. We denote by the subscripts $\mathrm{TCE}<em _mathrm_cp="\mathrm{cp">{\mathrm{cp}} / \mathrm{DCE}</em>}}$ and $\mathrm{TCE<em _mathrm_rec="\mathrm{rec">{\mathrm{rec}} / \mathrm{DCE}</em>$ in Section 4.2.}}$ the causal effects computed using the change in prediction metric and the relative change in confidence, respectively. We describe how we construct the dataset $\mathcal{D</p>
<h2>4 Experimental Setup</h2>
<p>In this section, we describe the data used to perform the interventions and to measure the causal effects.</p>
<h3>4.1 Datasets</h3>
<p>For our analyses, we use instances of math word problems from three popular datasets: ASDiv-A (Miao et al., 2020), MAWPS (Koncel-Kedziorski et al., 2016), and SVAMP (Patel et al., 2021). The examples contained in these collections are pairs $(\boldsymbol{t}, \boldsymbol{o})$ consisting of a question template $\boldsymbol{t}$ with its annotated operations $\boldsymbol{o}$. Each of these pairs can be instantiated multiple times into problems $\boldsymbol{q}=(\boldsymbol{t}, \boldsymbol{n})$ by filling the template with numerical values $\left(n_{1}, n_{2}, \ldots\right)$ and computing the groundtruth result $g=f_{\boldsymbol{o}}(\boldsymbol{n})$ (most problems involve two to three operands, i.e., $|\boldsymbol{n}| \in{2,3})$. We select a set of 437 two-operand and 307 three-operand template-expression pairs that we use to generate pairs of prompts representing an intervention. More details about the prompt generation procedure are in Appendix A. We use $(\boldsymbol{t}, \boldsymbol{n})$ to refer to an instantiated template that we use as a prompt.</p>
<h3>4.2 Intervention Data</h3>
<p>Given an MWP $\boldsymbol{q}=(\boldsymbol{t}, \boldsymbol{n})$ and its solution $g$, we generate a second problem-solution instance $\left(\boldsymbol{q}^{\prime}, g^{\prime}\right)$ depending on the type of causal effect CE we want to measure and on the considered variable. When intervening on the operands of the problem, the text of the problem is kept unaltered and a set of new operands $\boldsymbol{n}$ is sampled in such a way that the result $g$ is affected or not depending on the effect that is being measured. When changing the textual description of the problem, we change $\boldsymbol{t}$ such that either $\boldsymbol{o}^{\prime}=\boldsymbol{o}$, or $\boldsymbol{o}^{\prime} \neq \boldsymbol{o}$. In the former case, we sample a different template $\boldsymbol{t}^{\prime}=\left(s^{\prime}, \boldsymbol{o}\right)$ from the</p>
<p>set of templates describing the same operations $\boldsymbol{o}$, in the latter case we sample a new $\boldsymbol{t}^{\prime}$ describing a different operation. In Appendix B. 1 we report some examples of $\left(\boldsymbol{q}, \boldsymbol{q}^{\prime}\right)$ pairs representing the different types of interventions.</p>
<p>Given a model, we use the question pair $\left(\boldsymbol{q}, \boldsymbol{q}^{\prime}\right)$ to obtain a pair of answer distributions $\mathbb{P}(R \mid \boldsymbol{t}, \boldsymbol{n})$ and $\mathbb{P}\left(R \mid \boldsymbol{t}^{\prime}, \boldsymbol{n}^{\prime}\right)$, which we use to measure the causal effect of the intervention. We consider the space for the numerical values to be $\mathcal{I}=$ ${1,2, \ldots, C}$ consisting of integer values, following the setup of several existing MWP datasets (Miao et al., 2020; Koncel-Kedziorski et al., 2016; Patel et al., 2021). To control our experimental costs and make sure the models keep the number as one token, we set $C=300$. From all the tokens in a model's vocabulary, we focus on the probability assigned to the numbers in our numerical space $\mathcal{I}$, and thus we use $\mathbb{P}(R=r)$ to denote the normalized probability $\mathbb{P}<em r="1">{\text {raw }}(R=r) / Z$, where $Z=\sum</em>}^{C} \mathbb{P<em _raw="{raw" _text="\text">{\text {raw }}(R=r)$, and $\mathbb{P}</em>\right)$ pairs. Unless otherwise specified, for our experiments we generate 500 intervention pairs for each template, and results are averaged over three seeds.}}(x)$ is the raw probability score assigned to the vocabulary token $x$. For each intervention type, we generate a dataset $\mathcal{D}$ consisting of $\left(\boldsymbol{q}, \boldsymbol{q}^{\prime</p>
<h3>4.3 Models to Evaluate</h3>
<p>We use our framework to assess the robustness of reasoning in thirteen pre-trained language models. We consider five sizes of the GPT-2 model (Radford et al., 2019): distilled (Sanh et al., 2019), small, medium, large, and XL. We evaluate four models from EleutherAI that were pre-trained on the Pile (Gao et al., 2020): GPT-Neo 1.3B and 2.7B (Black et al., 2021), GPT-J-6B (Wang and Komatsuzaki, 2021), and GPT-NeoX-20B (Black et al., 2022). We use HuggingFace Transformers (Wolf et al., 2019) to access the models. Additionally, we experiment with a set of instruction-tuned versions of GPT-3 (Brown et al., 2020): Instruct (Ouyang et al., 2022), Curie, Davinci-002, and Davinci-003. ${ }^{3}$ Experiments with GPT-3 are carried out under the constraints set by the OpenAI APIS ${ }^{4}$, which prevent us from computing the causal effect using the same procedure as for the other models. We report the details about how the metrics were computed</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Comparison of $\operatorname{DCE}(\boldsymbol{N} \rightarrow R)$ and $\operatorname{TCE}(\boldsymbol{N}$ on $R) .{ }^{<em>}$ approx values, see Appendix C.
for GPT-3 in Appendix C. In the reported results, we indicate with an asterisk (</em>) the metrics that were influenced by this limitation.</p>
<h2>5 Results</h2>
<p>Our analyses focus primarily on two-operand problems (Sections 5.1 and 5.2) and later extend to more complex problems that involve three operands (Section 5.5) for the models that perform best on the two-operand test bed. We compare the direct causal effect DCE and the total causal effect TCE of $\boldsymbol{N}$ and $\boldsymbol{T}$ on $R$. DCE represents the undesired effect for a model to being mistakenly responsive to a change in $\boldsymbol{N}$ or $\boldsymbol{T}$ not leading to a change in the result $g$ (low robustness), whereas higher values of TCE indicate a higher ability of the model to correctly adjust the probability weight assigned to the new solution $g^{\prime}$ after the intervention (high sensitivity).</p>
<h3>5.1 Effect of $N$ on $R$</h3>
<p>From the results in Figure 3, we notice that larger models exhibit a larger $\mathrm{TCE}<em _rec="{rec" _text="\text">{\text {rec }} / \mathrm{DCE}</em>\right)$, for which the models show to be affected by resultpreserving changes almost as equally as by resultaltering interventions. This behavior changes sig-}}$ ratio. In particular, in GPT-J-6B and NeoX, the TCE is, respectively, 30x and 1000x larger than the DCE. However, this improvement in sensitivity is not manifested in terms of change of prediction $\left(\delta_{\mathrm{cp}</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Heatmaps displaying $P(g)$ for Distil-GPT-2 (left), GPT-J-6B (center), and GPT-3 Davinci-002 (right). $g$ is the ground-truth result $g=n_{1}+n_{2}$ ( $n_{1}$ and $n_{2}$ are represented by the x and y axes, respectively. The probability values for each combination of $\left(\left(n_{1}, n_{2}\right), g\right)$ are averaged over 20 different templates. Probability values over 0.2 are displayed with the darkest color.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Comparison of $\operatorname{DCE}(S \rightarrow R)$ and $\operatorname{TCE}(\boldsymbol{T}$ on $R)$. We use * to denote approximated values, explained in Appendix C.
nificantly in instruction-tuned models. In particular, for the 175B-parameter GPT-3, performance varies depending on the type of supervision, with the PPOtrained Davinci-003 exhibiting an $84 \%$ difference between direct and total effect.</p>
<p>In Figure 4, we present a different visualization of the direct causal effect of $\boldsymbol{N}$ on the model's prediction. We report the heatmaps showing the probability assigned by the model to the result $g$ of a problem $\left(\boldsymbol{t},\left(n_{1}, n_{2}\right), g\right) \mid g=n_{1}+n_{2}, \forall g \in$ ${0,1, \ldots, 50}, \forall\left(n_{1}, n_{2}\right) \in{0,1, \ldots, 50}^{2}$. For Distil-GPT-2 we observe low overall probability assigned to $g$ and diagonal patterns indicating consistency in assigning higher probability to specific results (e.g., 10, 20, 30, 40, 50). For the two larger models we notice a higher probability mass as-
signed to the problem's result, but less consistency on the prediction of the same result with different sets of operands (this is true for GPT-J in particular). This result is consistent with the observed higher DCE and TCE in larger models: $P(g)$ might vary more considerably when intervening on $\boldsymbol{N}$ without affecting $g$, but overall the model assigns higher probability weight to the correct result, which correlates with higher sensitivity.</p>
<h3>5.2 Effect of $T$ on $R$</h3>
<p>In Figure 5, we report the total causal effect of the textual framing $\boldsymbol{T}$ and the direct causal effect of the irrelevant text elements $S$ on the model's prediction. For the instruction-tuned models, the improvement in terms of prediction change ( $\delta_{\mathrm{cp}}$ ) follows a similar trend as for $\boldsymbol{N}$, with GPT-3 Davinci-003 showing a $76 \%$ difference between direct and total effect. An interesting observation is that the irrelevant textual information $S$ appears to have a lower direct effect than $\boldsymbol{N}$ for all non-instruction-tuned models. However, in the GPT-3 Davinci-00x models, we observe the opposite (i.e., $\operatorname{DCE}(\boldsymbol{N} \rightarrow R) \leq \operatorname{DCE}(S \rightarrow R)$ ). This suggests that large instruction-based models tend to be more susceptible to variation in the textual framing of a problem, while smaller models are more responsive to changes in the numerical values (though not necessarily correctly).</p>
<h3>5.3 Overall Insights</h3>
<p>In comparison to other models, GPT-3 Davinci shows the highest $\mathrm{DCE}<em _mathrm_cp="\mathrm{cp">{\text {rcc }}$, but low $\mathrm{DCE}</em>(g)$ might vary significantly.}}$. This discrepancy is related to the quantities that the two metrics consider. $\delta_{\text {rcc }}$ takes into account the probability assigned to $g$, while $\delta_{\mathrm{cp}}$ does not consider the ground truth solution. One interpretation of this result is that GPT-3 Davinci consistently predicts the same answer $r=r^{\prime}$ when $g=g^{\prime}$, but the probabilities $P(g)$ and $P^{\prime</p>
<p>The results observed for the two kinds of intervention $\operatorname{do}\left(\boldsymbol{T}: \boldsymbol{t} \rightarrow \boldsymbol{t}^{\prime}\right)$ and $\operatorname{do}\left(\boldsymbol{N}:\left(n_{1}, n_{2}\right) \rightarrow\right.$ $\left.\left(n_{1}^{\prime}, n_{2}^{\prime}\right)\right)$ show similar trends. Small models (Distilled and Small GPT-2) exhibit low sensitivity to interventions. Larger models (from GPT-2 Medium to GPT-Neo) appear to be more influenced by changes in both $\boldsymbol{N}$ and $\boldsymbol{T}$. However, they display similar sensitivity to both result-altering and resultpreserving interventions. An improvement in sensitivity is noticeable in GPT-J and NeoX, though not accompanied by an improvement in robustness. Remarkably different behavior is instead shown by the GPT-3 Davinci models, which demonstrate substantially higher sensitivity to result-altering interventions (high TCE), and higher robustness (in terms of prediction change). In Appendix B.2, we report the accuracy of the models on the generated instances of MWPs, which exhibits a similar trend as the robustness/sensitivity changes we observed.</p>
<p>Possible explanations for the improved robustness and sensitivity demonstrated by the large GPT3 models might be the dramatic size increase and extension/enhancement of the training procedure involving instructions. The former idea is aligned with the emergent abilities hypothesis (Wei et al., 2022a), which postulates the existence of skills that are displayed by large-scale models but are not present in smaller-scale models. However, our observations show different performances in versions of GPT-3 Davinci that differ in the training procedure. ${ }^{5}$ This raises the question of whether the capability of LLMs to reason about math problems benefits from instruction-based tuning. We address this question in the following section.</p>
<h3>5.4 Extending to LLaMA-Based Models</h3>
<p>To further investigate the roles played by size and training method in the model's performance, we carry out our experimental procedure on three versions with different sizes (7B, 13B, and 30B) of the LLaMA model (Touvron et al., 2023), and on Stanford Alpaca (which applies instruction tuning on LLaMA 7B) (Taori et al., 2023). We present these results separately, as the LLaMA tokenization makes the prediction setup different from the one used from the other models, and prevents us from computing the relative change in confidence</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Comparison of direct and total effects of $N$ on $R$ for LLaMA and Alpaca.
$\left(\delta_{\text {rcc }}\right) .{ }^{6}$
From the results (Figure 6), two notable observations emerge. Firstly, the increased difference between TCE and DCE observed with the increasing size of the LLaMA models suggests that a larger number of parameters can be a significant driver behind robustness/sensitivity improvement. However, this is not necessarily the case across different models: GPT-NeoX-20B shows a smaller $\mathrm{TCE}<em _mathrm_cp="\mathrm{cp">{\mathrm{cp}}-\mathrm{DCE}</em>$ gap compared to LLaMA 7B ( $5.2 \%$ vs $9.0 \%$ ). Secondly, the instruction tuning procedure of Alpaca does not seem to help significantly with mathematical computation: the decrease in both TCE and DCE shows that robustness improves at the expense of sensitivity. Nonetheless, overall, when comparing Alpaca compared to its base model, LLaMA 7B, we observe an increase in the gap between TCE and DCE, although this difference is minimal ( $9.5 \%$ vs $9.0 \%$ ).}</p>
<p>The limited improvement of Alpaca might be attributed to its instruction tuning procedure consisting of "a list of user-oriented instructions including email writing, social media, and productivity tools" (Taori et al., 2023), which differs from reasoningintensive tasks. We suggest future work to examine different types of instruction tuning (e.g., focused on reasoning procedures or reinforcement learning from human feedback), which might help the model answer more complex types of questions in a step-by-step manner and more accurately. We hypothesize that the different performances in versions of GPT-3 Davinci might be produced by the specific type of instructions used for training, by the reinforcement learning component (Ouyang et al., 2022), or simply by an extension of the language modeling pre-training. It is challenging to</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Comparison of direct and total effects of $\boldsymbol{N}$ on $R$ for three-operand problems.
pinpoint the exact factor in the training procedure that contributes to this improvement, as specific methodological details are not available.</p>
<h3>5.5 Moving to Three-Operand Problems</h3>
<p>We extend our evaluation to consider the threeoperand problems in the dataset. In these experiments, we consider only the GPT-3 175Bparameter models, as they are the only models performing well on the simpler bivariate problems. The results regarding the effects of $\boldsymbol{N}$ are reported in Figure 7. We notice that the large difference between the desired (TCE) and undesired (DCE) effects observed on simpler problems shrinks significantly for both metrics. In particular, for Davinci003, the direct effect of $\boldsymbol{N}$ (measured as $\delta_{\mathrm{cp}}$ ) grows from 0.17 to 0.87 . That is, GPT-3 Davinci-003 predicts a different result $87 \%$ of the time after an intervention that does not affect the ground-truth solution. The increase in direct effect indicates a performance degradation in terms of brittleness: even the models that show good performance on two-operand problems, now display an unstable behavior after result-preserving interventions.</p>
<h2>6 Related Work</h2>
<p>Causal NLP. Causal inference aims to study the cause and effect from observational and interventional data (Pearl, 2009; Peters et al., 2017). Traditionally, researchers usually apply causal techniques to phenomena in nature and human society. With the rise of powerful models in NLP, recent research has started to explore the intersection of causal inference and NLP, forming the study of Causal NLP (Jin et al., 2022; Feder et al., 2021a).</p>
<p>There are several formulations for Causal NLP: the causality for NLP thread involves using the causal framework for data collection and task for-
mulation (Jin et al., 2021c), inspecting the (pathspecific) causal effect of certain neurons on predictions (Vig et al., 2020; Meng et al., 2022), understanding the causal effect of data and learning paradigm for model performance ( Ni et al., 2022), and as a way to frame prompts (Lyu et al., 2023); and NLP for causality involves testing the pure causal inference skills of LLMs (Jin et al., 2023a,b), and use text as a variable for causal effect estimation (Roberts et al., 2020; Veitch et al., 2020; Jin et al., 2021b, 2023c).</p>
<p>The most similar line of research to our work is the application of causal effect estimation on interpreting models' behavior, such as how models understand syntactic agreement (Finlayson et al., 2021), and how interventions in the representations and weights affect the model prediction (Feder et al., 2021b). To the best of our knowledge, our work is the first to formulate a causal framework for robustness behavioral tests, and also we are the first to introduce the idea to quantify the differences in the causal mechanisms of human reasoning and model decisions.</p>
<p>Math Reasoning in NLP. A growing body of work tries to improve the math reasoning capability in NLP models (Zhang et al., 2020; Geva et al., 2020; Spokoyny et al., 2021), and prompting techniques for LLMs (Cobbe et al., 2021; Shen et al., 2021; Kojima et al., 2022; Wei et al., 2022b; Chowdhery et al., 2022). For analysis, significant attention has been given to models' ability to understand numerical quantities (Wallace et al., 2019; Thawani et al., 2021) and numerical operations (Pal and Baral, 2021; Berg-Kirkpatrick and Spokoyny, 2020; Piękos et al., 2021; Razeghi et al., 2022).</p>
<h2>7 Conclusion</h2>
<p>We developed a framework to disentangle and separately measure the effect of different factors influencing the predictions of LLMs for math reasoning. Our results indicate that a drastic increase in both robustness and sensitivity emerges in the GPT-3 Davinci models. Additionally, we study the contribution of size and instruction tuning in the models of the LLaMA family, observing that the Alpaca instruction tuning, while increasing the model's robustness, does not significantly improve the overall performance. Our framework provides a formalized theory of behavioral testing for math reasoning models and opens new future directions to design behavioral tests of models in a principled way.</p>
<h2>Ethical Considerations</h2>
<p>As for the ethical practice in this work, the data involved are from existing MWP datasets with no private user information, and available under the MIT license. As for the ethical impact of the use of this work, the study is about providing a metric and analyzing existing models' robustness, so there is less concern over harmful usage. Rather, it is more about putting checks on existing AI models and helping humans understand them better before use. Potential stakeholders that could benefit from this research include NLP researchers working on math models, practitioners working on various applications involving mathematical reasoning with text, and e-learning design.</p>
<h2>Limitations</h2>
<p>A key limitation in our work is that LLMs might have seen these math problems. Our work theoretically assumes this is not the case. Another limitation is that for the sake of simplicity, our work makes some assumptions. For example, we assume all numbers in the range of integers 0 to $C=300$. This would not cover every MWP out there. And future work is needed to generalize our framework to other forms of MWPs. In this work, we are also constrained by the limitations of the OpenAI policy on the GPT-3 API. This limits the number of perturbations we consider in this work as well as the accuracy with which we can estimate our causal distributions. Finally, our work is restricted to English, and extending it to other languages will require us to create an MWP dataset in that language.</p>
<h2>Acknowledgments</h2>
<p>This material is based in part upon works supported by the German Federal Ministry of Education and Research (BMBF): Tübingen AI Center, FKZ: 01IS18039B; by the Machine Learning Cluster of Excellence, EXC number 2064/1 - Project number 390727645; by the John Templeton Foundation (grant #61156); by a Responsible AI grant by the Haslerstiftung; and an ETH Grant (ETH-19 21-1). Alessandro Stolfo is supported by armasuisse Science and Technology through a CYD Doctoral Fellowship. Zhijing Jin is supported by PhD fellowships from the Future of Life Institute and Open Philanthropy, as well as the travel support from ELISE (GA no 951847) for the ELLIS pro-
gram. We also thank OpenAI Researcher Access Program for granting our team credits to their API.</p>
<h2>References</h2>
<p>Taylor Berg-Kirkpatrick and Daniel Spokoyny. 2020. An empirical investigation of contextualized number prediction. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4754-4764, Online. Association for Computational Linguistics. 9</p>
<p>Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, et al. 2022. GPT-NeoX-20B: An open-source autoregressive language model. arXiv preprint arXiv:2204.06745. 6</p>
<p>Sid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. 2021. GPT-Neo: Large scale autoregressive language modeling with mesh-tensorflow. If you use this software, please cite it using these metadata. 6</p>
<p>Elizabeth M. Brannon. 2005. The independence of language and mathematical reasoning. Proceedings of the National Academy of Sciences, 102(9):31773178. 3</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877-1901. Curran Associates, Inc. 1,6</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311. 1, 9</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168. 1, 9</p>
<p>Amir Feder, Katherine A. Keith, Emaad Manzoor, Reid Pryzant, Dhanya Sridhar, Zach Wood-Doughty, Jacob Eisenstein, Justin Grimmer, Roi Reichart, Margaret E. Roberts, Brando n M. Stewart, Victor Veitch, and Diyi Yang. 2021a. Causal inference in natural language processing: Estimation, prediction, interpretation and beyond. CoRR, abs/2109.00725. 9</p>
<p>Amir Feder, Nadav Oved, Uri Shalit, and Roi Reichart. 2021b. CausaLM: Causal model explanation through counterfactual language models. Computational Linguistics, 47(2):333-386. 9</p>
<p>Matthew Finlayson, Aaron Mueller, Sebastian Gehrmann, Stuart Shieber, Tal Linzen, and Yonatan Belinkov. 2021. Causal analysis of syntactic agreement mechanisms in neural language models. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1828-1843, Online. Association for Computational Linguistics. 2, 5, 9</p>
<p>Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. 2020. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027. 6</p>
<p>Mor Geva, Ankit Gupta, and Jonathan Berant. 2020. Injecting numerical reasoning skills into language models. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 946-958, Online. Association for Computational Linguistics. 9</p>
<p>Zhihua Jin, Xin Jiang, Xingbo Wang, Qun Liu, Yong Wang, Xiaozhe Ren, and Huamin Qu. 2021a. Numgpt: Improving numeracy ability of generative pre-trained models. arXiv preprint arXiv:2109.03137. 5</p>
<p>Zhijing Jin, Yuen Chen, Felix Leeb, Luigi Gresele, Ojasv Kamal, Zhiheng Lyu, Kevin Blin, Fernando Gonzalez Adauto, Max Kleiman-Weiner, Mrinmaya Sachan, and Bernhard Schoelkopf. 2023a. Cladder: Assessing causal reasoning in language models. 9</p>
<p>Zhijing Jin, Amir Feder, and Kun Zhang. 2022. CausaNLP tutorial: An introduction to causality for natural language processing. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: Tutorial Abstracts, pages 1722, Abu Dubai, UAE. Association for Computational Linguistics. 9</p>
<p>Zhijing Jin, Jiarui Liu, Zhiheng Lyu, Spencer Poff, Mrinmaya Sachan, Rada Mihalcea, Mona Diab, and Bernhard Schoelkopf. 2023b. Can large language models infer causation from correlation? 9</p>
<p>Zhijing Jin, Zhiheng Lyu, Yiwen Ding, Mrinmaya Sachan, Kun Zhang, Rada Mihalcea, and Bernhard Schoelkopf. 2023c. AI Scholars: A dataset for NLPinvolved causal inference. 9</p>
<p>Zhijing Jin, Zeyu Peng, Tejas Vaidhya, Bernhard Schoelkopf, and Rada Mihalcea. 2021b. Mining the cause of political decision-making from social media: A case study of COVID-19 policies across the US
states. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 288-301, Punta Cana, Dominican Republic. Association for Computational Linguistics. 9</p>
<p>Zhijing Jin, Julius von Kügelgen, Jingwei Ni, Tejas Vaidhya, Ayush Kaushal, Mrinmaya Sachan, and Bernhard Schoelkopf. 2021c. Causal direction of data collection matters: Implications of causal and anticausal learning for NLP. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 9499-9513, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. 9</p>
<p>Divyansh Kaushik, Eduard H. Hovy, and Zachary Chase Lipton. 2020. Learning the difference that makes A difference with counterfactually-augmented data. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net. 5</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916. 9</p>
<p>Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi. 2016. MAWPS: A math word problem repository. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1152-1157, San Diego, California. Association for Computational Linguistics. 5, 6</p>
<p>Zhiheng Lyu, Zhijing Jin, Justus Mattern, Rada Mihalcea, Mrinmaya Sachan, and Bernhard Schölkopf. 2023. Psychologically-inspired causal prompts. CoRR, abs/2305.01764. 9</p>
<p>Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. 2022. Locating and editing factual associations in GPT. Advances in Neural Information Processing Systems, 35:17359-17372. 2, 9</p>
<p>Shen-yun Miao, Chao-Chun Liang, and Keh-Yih Su. 2020. A diverse corpus for evaluating and developing English math word problem solvers. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 975-984, Online. Association for Computational Linguistics. 5, 6</p>
<p>Swaroop Mishra, Matthew Finlayson, Pan Lu, Leonard Tang, Sean Welleck, Chitta Baral, Tanmay Rajpurohit, Oyvind Tafjord, Ashish Sabharwal, Peter Clark, et al. 2022a. Lila: A unified benchmark for mathematical reasoning. arXiv preprint arXiv:2210.17517. 3</p>
<p>Swaroop Mishra, Arindam Mitra, Neeraj Varshney, Bhavdeep Sachdeva, Peter Clark, Chitta Baral, and Ashwin Kalyan. 2022b. NumGLUE: A suite of fundamental yet challenging mathematical reasoning tasks. In Proceedings of the 60th Annual Meeting of</p>
<p>the Association for Computational Linguistics (Volume 1: Long Papers), pages 3505-3523, Dublin, Ireland. Association for Computational Linguistics. 1</p>
<p>Martin M Monti, Lawrence M Parsons, and Daniel N Osherson. 2012. Thought beyond language: Neural dissociation of algebra and natural language. Psychological science, 23(8):914-922. 3</p>
<p>Jingwei Ni, Zhijing Jin, Markus Freitag, Mrinmaya Sachan, and Bernhard Schölkopf. 2022. Original or translated? A causal analysis of the impact of translationese on machine translation performance. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5303-5320, Seattle, United States. Association for Computational Linguistics. 9</p>
<p>Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. CoRR, abs/2203.02155. 1, 2, 6, 8</p>
<p>Kuntal Kumar Pal and Chitta Baral. 2021. Investigating numeracy learning ability of a text-to-text transfer model. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 3095-3101, Punta Cana, Dominican Republic. Association for Computational Linguistics. 9</p>
<p>Arkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021. Are NLP models really able to solve simple math word problems? In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2080-2094, Online. Association for Computational Linguistics. 1, 2, 5, 6, 14</p>
<p>Judea Pearl. 1995. Causal diagrams for empirical research. Biometrika, 82(4):669-688. 2, 3, 4</p>
<p>Judea Pearl. 2001. Direct and indirect effects. In UAI '01: Proceedings of the 17th Conference in Uncertainty in Artificial Intelligence, University of Washington, Seattle, Washington, USA, August 2-5, 2001, pages 411-420. Morgan Kaufmann. 2, 4</p>
<p>Judea Pearl. 2009. Causality. Cambridge University Press. 9</p>
<p>Jonas Peters, Dominik Janzing, and Bernhard Schölkopf. 2017. Elements of causal inference: Foundations and learning algorithms. The MIT Press. 9</p>
<p>Piotr Piękos, Mateusz Malinowski, and Henryk Michalewski. 2021. Measuring and improving BERT's mathematical abilities by predicting the order of reasoning. In Proceedings of the 59th Annual Meeting of the Association for Computational</p>
<p>Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pages 383-394, Online. Association for Computational Linguistics. 9</p>
<p>Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. 6</p>
<p>Yasaman Razeghi, Robert L Logan IV, Matt Gardner, and Sameer Singh. 2022. Impact of pretraining term frequencies on few-shot reasoning. arXiv preprint arXiv:2202.07206. 1, 9</p>
<p>Margaret E Roberts, Brandon M Stewart, and Richard A Nielsen. 2020. Adjusting for confounding with text matching. American Journal of Political Science, 64(4):887-903. 9</p>
<p>Mrinmaya Sachan, Kumar Dubey, and Eric Xing. 2017. From textbooks to knowledge: A case study in harvesting axiomatic knowledge from textbooks to solve geometry problems. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 773-784. 1</p>
<p>Mrinmaya Sachan, Kumar Avinava Dubey, Tom M Mitchell, Dan Roth, and Eric P Xing. 2018. Learning pipelines with limited data and domain knowledge: A study in parsing physics problems. Advances in Neural Information Processing Systems, 31. 1</p>
<p>Mrinmaya Sachan and Eric Xing. 2017. Learning to solve geometry problems from natural language demonstrations in textbooks. In Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), pages 251-261. 1</p>
<p>Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. DistilBERT, a distilled version of BERT: Smaller, faster, cheaper and lighter. In NeurIPS EMC ${ }^{2}$ Workshop. 6</p>
<p>Minjoon Seo, Hannaneh Hajishirzi, Ali Farhadi, Oren Etzioni, and Clint Malcolm. 2015. Solving geometry problems: Combining text and diagram interpretation. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1466-1476, Lisbon, Portugal. Association for Computational Linguistics. 1</p>
<p>Jianhao Shen, Yichun Yin, Lin Li, Lifeng Shang, Xin Jiang, Ming Zhang, and Qun Liu. 2021. Generate \&amp; rank: A multi-task framework for math word problems. arXiv preprint arXiv:2109.03034. 9</p>
<p>Daniel Spokoyny, Ivan Lee, Zhao Jin, and Taylor BergKirkpatrick. 2021. Masked measurement prediction: Learning to jointly predict quantities and units from textual context. arXiv preprint arXiv:2112.08616. 9</p>
<p>Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/ stanford_alpaca. 2, 8</p>
<p>Avijit Thawani, Jay Pujara, Filip Ilievski, and Pedro Szekely. 2021. Representing numbers in NLP: a survey and a vision. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 644-656, Online. Association for Computational Linguistics. 9</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971. 2, 8</p>
<p>Victor Veitch, Dhanya Sridhar, and David M. Blei. 2020. Adapting text embeddings for causal inference. In Proceedings of the Thirty-Sixth Conference on Uncertainty in Artificial Intelligence, UAI 2020, virtual online, August 3-6, 2020, volume 124 of Proceedings of Machine Learning Research, pages 919-928. AUAI Press. 9</p>
<p>Jesse Vig, Sebastian Gehrmann, Yonatan Belinkov, Sharon Qian, Daniel Nevo, Yaron Singer, and Stuart Shieber. 2020. Investigating gender bias in language models using causal mediation analysis. Advances in Neural Information Processing Systems, 33:1238812401. 2, 9</p>
<p>Eric Wallace, Yizhong Wang, Sujian Li, Sameer Singh, and Matt Gardner. 2019. Do NLP models know numbers? probing numeracy in embeddings. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 5307-5315, Hong Kong, China. Association for Computational Linguistics. 9</p>
<p>Ben Wang and Aran Komatsuzaki. 2021. GPT-J-6B: A 6 billion parameter autoregressive language model. 6</p>
<p>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. 2022a. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682. 1, 8</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed H. Chi, Quoc Le, and Denny Zhou. 2022b. Chain of thought prompting elicits reasoning in large language models. CoRR, abs/2201.11903. 9</p>
<p>Sean Welleck, Peter West, Jize Cao, and Yejin Choi. 2022. Symbolic brittleness in sequence models: on systematic generalization in symbolic mathematics. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 8629-8637. 1</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R'emi Louf, Morgan Funtowicz, and Jamie Brew. 2019. HuggingFace's transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771. 6</p>
<p>Xikun Zhang, Deepak Ramachandran, Ian Tenney, Yanai Elazar, and Dan Roth. 2020. Do language embeddings capture scales? In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4889-4896, Online. Association for Computational Linguistics. 9</p>
<h2>A Creation of the Prompts</h2>
<p>We consider MWP examples from the union of the three datasets SVAMP, ASDiv-A, and MAWPS. The textual template $\boldsymbol{t}$ of a problem consists of a context (describing a real-world state and/or actions) and a question. In order to obtain suitable prompts for the models, we convert the problems' questions into statements where the result of the problem is expected to be the first token after the prompt. E.g., in the example in section 2, how many trees will he have? is converted into the number of trees that he will have is ... From the MWP templates of the SVAMP/ASDiv-A/MAWPS collection (we consider all splits), we filter out the templates whose questions do not start with How many..., and we use spaCy $^{7}$ to identify the subject, the object and the verbs in the sentence. This allows us to convert the last sentence of the template from The number of... is. This way, we obtain 437 statement-based MWP templates for two-operand problems and 307 for three-operand problems. We manually checked a subset of the templates to identify possible mistakes in the conversion procedure.</p>
<h2>B Frequently Asked Questions</h2>
<h2>B. 1 How do the intervention data look like?</h2>
<p>In Table 1 we report examples of MWP pairs representing different types of intervention.</p>
<h2>B. 2 What is the accuracy of the evaluated models on the generated problems?</h2>
<p>We report the accuracy of the models considered for evaluation in terms of accuracy at 1 and accuracy at 10. Results are displayed in Figure 8.</p>
<h2>B. 3 What is the relation between accuracy and the RCC metric?</h2>
<p>We examine the relationship between performance and robustness, computing the Pearson correlation coefficient between accuracy (accuracy@10) and the relative confidence change (RCC) metric. On a per-template basis ( 500 instances for each template), we found accuracy to be positively</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">$\operatorname{TCE}(\boldsymbol{N} \rightarrow R)$</th>
<th style="text-align: center;">Ruby has 87 candies. If she shares the candies among 29 friends, the number of candies that each friend gets is</th>
<th style="text-align: center;">$g=87 / 29=3$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Ruby has 35 candies. If she shares the candies among 5 friends, the number of candies that each friend gets is</td>
<td style="text-align: center;">$g=35 / 5=7$</td>
</tr>
<tr>
<td style="text-align: center;">$\operatorname{DCE}(\boldsymbol{N} \rightarrow R)$</td>
<td style="text-align: center;">The school is composed of 13 buildings each having 10 classrooms. The number of classrooms that the school has is</td>
<td style="text-align: center;">$g=10 \times 13=130$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">The school is composed of 65 buildings each having 2 classrooms. The number of classrooms that the school has is</td>
<td style="text-align: center;">$g=65 \times 2=130$</td>
</tr>
<tr>
<td style="text-align: center;">$\operatorname{DCE}(S \rightarrow R)$</td>
<td style="text-align: center;">The razorback t-shirt shop ordered 6 cases of t-shirts. If each case contains 17 t-shirts the number of t-shirts that they ordered is</td>
<td style="text-align: center;">$g=17 \times 6=102$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">The roller coaster at the state fair costs 6 tickets per ride. If 17 friends were going to ride the roller coaster the number of tickets that they would need is</td>
<td style="text-align: center;">$g=17 \times 6=102$</td>
</tr>
<tr>
<td style="text-align: center;">$\operatorname{TCE}(\boldsymbol{T} \rightarrow R)$</td>
<td style="text-align: center;">Sean has 23 whistles. He has 6 more whistles than Charles. The number of whistles that Charles has is</td>
<td style="text-align: center;">$g=23-6=17$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Jovana filled her bucket with 23 pounds of shells. If she adds 6 more pounds of shell to fill her bucket, the number of pounds that she has is</td>
<td style="text-align: center;">$g=23+6=29$</td>
</tr>
</tbody>
</table>
<p>Table 1: For each of the causal effects measured (left column), we report a pair of MWPs illustrating the intervention performed (center), along with their respective ground-truth result (left column).
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Average accuracy of the models on the generated instances of MWPs. Results are averaged over two sets consisting of 500 problem instances generated for each template. The lower figure shows a zoomed-in visualization of the accuracy at 1 .
correlated with $\operatorname{TCE}(\boldsymbol{N}$ on $R)$ and $\operatorname{TCE}(T$ on $R)$ ( 0.24 and 0.49 , respectively) and negatively correlated with $\operatorname{DCE}(\boldsymbol{N} \rightarrow R)$ and $\operatorname{DCE}(S \rightarrow R)$ $(-0.26$ and -0.36 , respectively). We see these results as a quantitative validation of the intuition behind our framework: the better the model's performance, the more the model tends to correctly adjust its prediction after a result-altering intervention (higher sensitivity) and to correctly not change its prediction after a result-preserving intervention (higher robustness).</p>
<p>Moreover, we conduct an additional sanity check as in Patel et al. (2021): removing the question from the MWP templates, we observe a sensitivityrobustness degradation to random guessing (i.e., $\mathrm{TCE} \simeq \mathrm{DCE}$ ). This indicates that the measurement of the causal effects within our framework is not affected by patterns in the templates that might have been picked up or memorized by large models.</p>
<h2>C Computation of Causal Effects for GPT-3</h2>
<p>We access GPT-3 through the OpenAI APIs, which allow a user to prompt the model and obtain the probabilities assigned by the model to the $k$-th most likely vocabulary entries, for each token generated. To overcome this limitation, we approximate the</p>
<p>relative probability change $\delta_{\text {rcc }}$ as follows, depending on the kind of effect measured.</p>
<p>The limit for $k$ is set by OpenAI to 5. However, for our main set of experiments (i.e., computing the causal effects of $\boldsymbol{N}, S$, and $\boldsymbol{T}$ ) we were granted an increased limit of $k$ to 100 . This allowed us to obtain reasonable estimates for the causal effects, as the number of cases in which $P(g)$ is not defined is less than $10 \%$ of the number of examples that we consider.</p>
<div class="codehilite"><pre><span></span><code>Algorithm 1 Computation of \(\delta_{\text {rcc }}\) for GPT-3
\(\overline{\boldsymbol{Q}}=(\boldsymbol{t}, \boldsymbol{n}, \bar{g})\)
\(Q^{\prime}=\left(t^{\prime}, \boldsymbol{n}^{\prime}, g^{\prime}\right)\)
if \(P(g)\) is defined then
    if \(P^{\prime}(g)\) is defined then
        \(\Delta=\frac{P(g)-P^{\prime}(g)}{P^{\prime}(g)}\)
    else
        \(\hat{P}^{\prime} \leftarrow P^{\prime}(k\)-th most likely token)
        \(\Delta=\frac{P(g)-\hat{P}^{\prime}}{\hat{P}^{\prime}}\)
    end
else
    \(\Delta=0\)
end
if \(P^{\prime}\left(g^{\prime}\right)\) is defined then
    if \(P\left(g^{\prime}\right)\) is defined then
        \(\Delta^{\prime}=\frac{P^{\prime}\left(g^{\prime}\right)-P\left(g^{\prime}\right)}{P\left(g^{\prime}\right)}\)
    else
        \(\hat{P} \leftarrow P(k\)-th most likely token)
        \(\Delta^{\prime}=\frac{P^{\prime}\left(g^{\prime}\right)-\hat{P}}{\hat{P}}\)
    end
else
    \(\Delta^{\prime}=0\)
end
\(\delta_{\mathrm{rcc}}=\frac{1}{2}\left(\Delta+\Delta^{\prime}\right)\)
</code></pre></div>

<h2>C. $1 \operatorname{TCE}(\boldsymbol{N}$ on $R)$ and $\operatorname{TCE}(\boldsymbol{T}$ on $R)$</h2>
<p>In cases when $P(g)$ is defined (i.e. when $g$ appears in the top $k$ token predictions) and $P^{\prime}(g)$ is not defined, we compute a lower bound on the relative change using the upper bound on $P^{\prime}(g)$ given by the probability of the $k$-th most likely token. This gives us a conservative estimate of $\Delta$. For cases in which $P(g)$ is not defined, we cannot say anything about the relative change, and we set $\Delta=0$. The same applies when swapping $P$ and $P^{\prime}$. This procedure is illustrated by Algorithm 1.
C. $2 \operatorname{DCE}(\boldsymbol{N} \rightarrow R)$ and $\operatorname{DCE}(S \rightarrow R)$</p>
<p>In this case, we simply discard the examples for which $P(g)$ is not defined or $P^{\prime}(g)$ are not defined. In that is not the case, then we compute $\delta_{\text {rcc }}$ as in Section 3.4.</p>
<h2>C. 3 Heatmap Illustration</h2>
<p>The heatmap for GPT-3 displayed in Figure 4 was computed by taking the raw probability score produced by the model over the whole vocabulary, as the limit on the available top predicted tokens makes it impossible to normalize it over the set ${0, \ldots, 300}$, as done for the other models. The probability was set to 0 when $g$ did not appear in the model's top 5 predictions for the next token after the prompt.</p>
<h2>D Computing Infrastructure \&amp; Inference Details</h2>
<p>To run our experiments, we used a single NVIDIA TITANRTX with 24GB of memory for all the versions of GPT-2 and GPT-Neo. We used a single NVIDIA A100 with 40GB of memory for GPT-J6B and a single NVIDIA A100 with 80GB of memory for GPT-NeoX and the LLaMA models (two for the 30B version). We accessed GPT-3 using the OpenAI APIs. The longest run (GPT-J) on the four kinds of experiments corresponding to the four kinds of effects measured took $\sim 12$ hours, using 500 MWP instances for each of the 437 templates. Due to budget and resource constraints, the experiments on GPT-3, GPT-NeoX, and LLaMA were carried out using 20 examples generated for each template and took $\sim 7$ hours. Experiment tracking was carried out using Weights \&amp; Biases ${ }^{8}$.</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>A For every submission:</h1>
<p>A1. Did you describe the limitations of your work?
Section "Limitations".
$\checkmark$ A2. Did you discuss any potential risks of your work?
Section "Ethical Considerations".
$\checkmark$ A3. Do the abstract and introduction summarize the paper's main claims?
Section 1: Introduction.
$\mathscr{H}$ A4. Have you used AI writing assistants when working on this paper?
Left blank.</p>
<h2>B Did you use or create scientific artifacts?</h2>
<h2>Section 4</h2>
<p>$\checkmark$ B1. Did you cite the creators of artifacts you used?
Section 4.1
$\checkmark$ B2. Did you discuss the license or terms for use and / or distribution of any artifacts?
Section "Limitations"
$\checkmark$ B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)?
Section "Limitations"
$\checkmark$ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect / anonymize it?
Section "Limitations"
$\checkmark$ B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.?
Section "Limitations"
$\checkmark$ B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits, etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the number of examples in train / validation / test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be.
Section 4.1 and Appendix A</p>
<h2>C Did you run computational experiments?</h2>
<p>Sections 4 and 5
$\checkmark$ C1. Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used?</p>
<p>Sections 4.3 and Appendix D
The Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing assistance.</p>
<p>C2. Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values?
Sections 4.3
C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run?
Sections 4.1, 4.2, and 5
C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE, etc.)?
Section 4.3 and Appendix A
D Did you use human annotators (e.g., crowdworkers) or research with human participants? Left blank.</p>
<p>D1. Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators, etc.?
No response.
D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants' demographic (e.g., country of residence)?
No response.
D3. Did you discuss whether and how consent was obtained from people whose data you're using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used?
No response.
D4. Was the data collection protocol approved (or determined exempt) by an ethics review board? No response.</p>
<p>D5. Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data?
No response.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{8}$ http://wandb.ai/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{6}$ The LLaMA tokenizer considers each digit as an independent token in the vocabulary. This makes it problematic to compare the probability value assigned by the model to multi-digit numbers.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
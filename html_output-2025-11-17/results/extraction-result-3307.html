<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3307 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3307</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3307</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-76.html">extraction-schema-76</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-d48b29889241551e1ee6622fa78c3fa4159255dd</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/d48b29889241551e1ee6622fa78c3fa4159255dd" target="_blank">Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> A Selection-Inference (SI) framework is proposed that exploits pre-trained LLMs as general processing modules, and alternates between selection and inference to generate a series of interpretable, casual reasoning steps leading to the final answer.</p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have been shown to be capable of impressive few-shot generalisation to new tasks. However, they still tend to perform poorly on multi-step logical reasoning problems. Here we carry out a comprehensive evaluation of LLMs on 50 tasks that probe different aspects of logical reasoning. We show that language models tend to perform fairly well at single step inference or entailment tasks, but struggle to chain together multiple reasoning steps to solve more complex problems. In light of this, we propose a Selection-Inference (SI) framework that exploits pre-trained LLMs as general processing modules, and alternates between selection and inference to generate a series of interpretable, casual reasoning steps leading to the final answer. We show that a 7B parameter LLM used within the SI framework in a 5-shot generalisation setting, with no fine-tuning, yields a performance improvement of over 100% compared to an equivalent vanilla baseline on a suite of 10 logical reasoning tasks. The same model in the same setting even outperforms a significantly larger 280B parameter baseline on the same suite of tasks. Moreover, answers produced by the SI framework are accompanied by a causal natural-language-based reasoning trace, which has important implications for the safety and trustworthiness of the system.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3307.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3307.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SI (prompt-engineered, 7B)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Selection-Inference framework (prompt-engineered) using a 7B Gopher LLM</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative, modular reasoning pipeline that alternates a Selection step (select relevant facts from context) and an Inference step (infer a new fact from the selected facts), implemented via few-shot prompt engineering on a frozen 7B Gopher model; produces causal, human-interpretable multi-step reasoning traces.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gopher family LLM (used within SI prompt-engineered)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pre-trained Transformer-based language model from the Gopher family (Rae et al., 2021). In these experiments the model is frozen and used in 5-shot prompt-engineered Selection and Inference modules (no gradient updates). Selection is implemented by scoring candidate facts (log-likelihood) and picking the highest-scoring facts; Inference is prompted to generate a single new fact from the selection.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Selection-Inference (iterative modular)', 'Chain-of-Thought (COT) (compared baseline)', 'Vanilla few-shot prompting (compared baseline)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Selection-Inference: each reasoning step decomposed into (a) Selection: model is prompted to choose relevant facts from the context (implemented by scoring candidate facts via LLM log-likelihood and joining top selections) and (b) Inference: a separate prompt takes the selection (but not the original question) and generates a single inferred fact to be appended to the context; steps are iterated for a fixed number of iterations. Chain-of-Thought: k-shot prompts include example reasoning traces and the model is asked to generate a reasoning trace and answer in one generative pass. Vanilla: k-shot prompts show only [context, question, answer] examples and the model is asked to generate the answer directly (implicit reasoning).</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Uses a distinct, modular two-stage reasoning method (Selection + Inference) that is different in style from single-pass chain-of-thought or vanilla prompting; the paper explicitly contrasts this modular iterative style (diverse/different) with the similar single-style COT and vanilla approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Subset of logical reasoning tasks (bAbI Tasks 1-3, 15-16; ProofWriter depths 0,1,2,3,5; plus evaluation across other logic-oriented tasks drawn from AAC, 2WikiMultiHop, BigBench subsets, StrategyQA, Jeopardy)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Natural-language logical reasoning benchmarks probing single-step entailment, deductive and inductive multi-step reasoning, multi-hop retrieval-style inference, and tasks that include distractor facts or require chaining multiple supporting facts (examples: bAbI temporal/fact-chaining tasks, ProofWriter logical proof depths, 2WikiMultiHop multi-hop QA).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Generative evaluation (5-shot): SI (7B) = 58.75% average accuracy across 11 selected datasets; by comparison vanilla 7B generative = 2.94%, COT 7B generative = 41.32%; SI (7B) also outperformed 280B baselines (see comparisons). For specific tasks: SI achieved 100% on bAbI15 (deduction) and significantly outperformed baselines on ProofWriter depth 0 and depth 1; per-task breakdown in Fig.4b of the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Direct head-to-head comparisons in the paper show prompt-engineered SI (7B) > COT (7B) > vanilla (7B) in generative evaluation (all comparisons statistically significant, p<0.01). SI (7B prompt-engineered) also outperformed a 280B model run in vanilla and COT modes on the same suite (examples and p-values reported). Multi-choice evaluation differences are noted: vanilla 7B multi-choice can be high on some tasks (57.31%) but generative comparisons were used for fairness with SI.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Decomposing reasoning into the diverse modular Selection + Inference style substantially improves multi-step logical reasoning vs similar single-pass styles (COT) or vanilla prompting, even allowing a 7B model in SI mode to outperform much larger 280B models run naively or with COT on the evaluated logical tasks; SI produces causal, inspectable traces.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>SI prompt-engineered gains diminish as reasoning depth increases when Selection is imperfect (prompt-engineered Selection is a bottleneck). Vanilla and COT still compete on easier single-step entailment tasks where LLMs are already strong; in multi-choice settings vanilla 7B can outperform a larger 280B model on some tasks (suggesting priors and evaluation format affect outcomes).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3307.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3307.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SI (fine-tuned, 7B)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Selection-Inference framework (fine-tuned) using a 7B Gopher LLM</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The SI pipeline with Selection and Inference modules implemented by fine-tuning two separate 7B LLMs on single-step reasoning data (ProofWriter), yielding substantially higher final reasoning accuracy and more faithful reasoning traces.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gopher family LLM (fine-tuned for Selection and Inference)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Two copies of a 7B LLM fine-tuned separately: (a) Selection LLM trained to output sentence identifiers (preventing fabrications) that identify which context sentences should be used for a single inference step; (b) Inference LLM fine-tuned to generate the entailed single-step conclusion given selection. Fine-tuning performed on ProofWriter single-step data.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Selection-Inference (iterative modular, fine-tuned)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Same Selection + Inference decomposition as prompt-engineered SI, but Selection and Inference models are fine-tuned on single-step ground-truth traces (Selection outputs labels like 'sent 2', Inference learns entailment mapping). Inference converged rapidly (>99% single-step accuracy after ~300 fine-tuning steps); Selection trained ~40k steps and achieved >80% exact-match on many depths though early selections for deep problems remain hard (planning).</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Uses the same diverse two-stage modular approach; fine-tuning reinforces the distinct behaviors of the two modules (selection constrained to context labels, inference constrained to only see selection), increasing the effective diversity of reasoning styles compared to single-pass approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ProofWriter OWA depths (0,1,2,3,5) and subset evaluation reported in Fig.4 and Fig.5</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>ProofWriter: language-formalized logical reasoning tasks with ground-truth proof traces; task depth equals number of inference steps needed to prove or disprove a statement under an Open World Assumption (Unknowns removed in used subset).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Generative evaluation (fine-tuned SI 7B) = 78.95% final reasoning accuracy (on the evaluated ProofWriter subset), compared to prompt-engineered SI = 57.93%, and other prompt-engineered baselines: vanilla/COT generative 7B = 0.34% / 15.73% and 280B = 31.58% / 21.12% (numbers reported in the paper). Inference module single-step test accuracy >99%; Selection module >80% exact-match across most depths after fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Fine-tuning Selection and Inference significantly improves end-task reasoning accuracy vs prompt-engineered SI and vs single-model fine-tuning that predicts all steps at once. The fine-tuned SI produced reasoning traces with higher Jaccard similarity (intersection-over-union) with ground-truth proof steps than a baseline 7B model fine-tuned to predict entire proofs in one pass; baseline proofs often contain extraneous or misordered steps.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Fine-tuning the modular SI components on single-step data yields large gains: fine-tuned SI (7B) substantially outperforms both prompt-engineered SI and baselines, and yields more faithful, ordered reasoning traces that match ground-truth proof steps better than monolithic fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Selection still struggles on deeper problems that require planning which facts to pick early; predicting early selections for deeper reasoning tasks remains a limitation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3307.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3307.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>COT (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting (COT) baseline with Gopher LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Single-pass prompting approach that inserts example multi-step reasoning traces into k-shot prompts so the model generates an explicit chain-of-thought (reasoning trace) and an answer in one generation; used here as a primary baseline to compare to SI.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain of thought prompting elicits reasoning in large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gopher family LLMs (7B and 280B) used with Chain-of-Thought prompts</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pre-trained Gopher models prompted in 5-shot with examples that include explicit reasoning traces and answers; model is asked to generate [reasoning, answer] in a single generative step (no iterative selection/inference).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B and 280B (evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Chain-of-Thought (single-pass explicit reasoning)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Prompt examples include concatenated [context, question, reasoning, answer] × k; the model is then prompted on a new [context, question] and asked to generate reasoning and final answer together in one pass. This encourages the model to expose intermediate steps but does not force causal dependence between answer and reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Single-style (similar) reasoning approach across steps; COT uses a similar one-pass generative style for all steps rather than modularly decomposing selection and inference.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Same selected logical reasoning tasks as SI experiments (bAbI, ProofWriter depths, and others in the 11-dataset subset used for Fig.4)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Benchmarks probing single- and multi-step logical reasoning across natural-language contexts; used here to test whether providing example multi-step traces alone is sufficient to induce causal, correct multi-step reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Generative evaluation (5-shot): COT (7B) = 41.32% average accuracy; COT (280B) = 44.03% average accuracy on the evaluated subset. These are higher than vanilla 7B generative (2.94%) but lower than SI (prompt 7B = 58.75%) and SI (fine-tuned 7B = 78.95%).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>COT improves over vanilla prompting but underperforms the modular SI approach in both prompt-engineered and fine-tuned settings. The paper also reports qualitative findings that COT-produced traces can be incorrect even when final answers are correct, showing weak causal coupling between reasoning trace and model answer.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Providing reasoning traces in a single-pass (COT) helps but is insufficient to ensure causal, correct multi-step reasoning; decomposing into Selection and Inference (SI) produces more accurate final answers and more faithful causal traces.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>COT traces frequently contain unrelated or incorrect steps while still producing a correct final answer; thus COT traces are not reliably causal or auditably faithful. COT improvements vary with model scale but did not match SI's gains on evaluated logical tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3307.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3307.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Vanilla prompting baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vanilla few-shot prompting baseline (direct answer generation / scoring)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard few-shot prompting where the model is given concatenated examples of [context, question, answer] and asked to generate the answer directly (implicit reasoning); also evaluated in multi-choice by scoring each answer option with the LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gopher family LLMs (7B and 280B) used with vanilla prompts</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pre-trained Gopher models prompted in 5-shot with examples that contain only [context, question, answer] (no explicit reasoning examples). For multi-choice evaluation the LLM scores each candidate answer by likelihood; for generative evaluation the model must produce the exact answer string.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B and 280B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Vanilla few-shot prompting (implicit reasoning)', 'Multi-choice scoring (evaluation style)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Vanilla: model directly maps context + question to answer in one generation; Multi-choice evaluation uses LLM log-likelihood scores to choose among candidate answers (higher chance-level baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Single-style implicit reasoning, no explicit step decomposition; similar across tasks and scales.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Same logical reasoning benchmarks evaluated in the paper (various subsets including 50-task evaluation and the 11-dataset subset used for SI comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Benchmarks probing entailment and multi-step logical reasoning; multi-choice and generative evaluation settings used to measure differences in performance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Generative (5-shot): vanilla 7B = 2.94% average accuracy (on 11-dataset subset), vanilla 280B = 31.19%. Multi-choice evaluation: vanilla 7B = 57.31% vs vanilla 280B = 51.45% (surprisingly the smaller model outperformed the larger in multi-choice on the reported subset). Other numbers reported in Sec.6 for prompt-engineered baselines: vanilla/COT generative 7B = 0.34% / 15.73% and 280B = 31.58% / 21.12% (context-specific comparisons in Fig.4/6).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Vanilla prompting is weakest in generative multi-step settings. Multi-choice evaluation gives vanilla 7B an advantage in some cases, indicating evaluation format and model priors materially affect measured performance. Overall, SI (both prompt and fine-tuned) outperforms vanilla prompting on multi-step reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Implicit one-shot or k-shot prompting without explicit stepwise decomposition yields poor multi-step logical reasoning in generative settings; multi-choice scoring can mask weaknesses because the task is easier when candidate answers are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Vanilla 280B sometimes outperforms smaller models in generative tasks but not in SI comparisons; vanilla 7B surprisingly outperformed 280B in multi-choice on the reported subset, suggesting large models may carry priors that interfere with task-specific logical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3307.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3307.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gopher (model family)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gopher: Scaling language models — Methods, analysis & insights from training Gopher</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The Gopher family of pre-trained Transformer language models used across experiments as the base LLM (various sizes including 7B and 280B); provided the pre-trained backbone for prompt-engineered and fine-tuned Selection and Inference modules.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Scaling language models: Methods, analysis & insights from training gopher</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gopher</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based large language models trained and evaluated in Rae et al., 2021. The paper uses 7B and 280B parameter variants of Gopher as the LLMs for all experiments; models are either frozen and prompt-engineered or fine-tuned for Selection/Inferences tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B and 280B (evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Prompt-engineered Selection-Inference', 'Fine-tuned Selection-Inference', 'Chain-of-Thought prompting', 'Vanilla few-shot prompting / multi-choice scoring']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Gopher models were used as the underlying LLM in all above styles: frozen for prompt-engineered SI, COT, and vanilla experiments; fine-tuned for the SI Selection and Inference modules in some experiments (ProofWriter).</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>The same base model family was used to instantiate multiple reasoning styles; the paper compares how the same base architecture behaves when induced to use different reasoning methods.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>50-task evaluation suite covering many logical reasoning aspects (source corpora include bAbI, BigBench subsets, AAC, ProofWriter, 2WikiMultiHop, Jeopardy, StrategyQA)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>A broad set of tasks probing single-step entailment, multi-step deduction/induction/abduction, multi-hop retrieval, and tasks with distractors/negation; used to characterize LLM capabilities and scaling behavior on logic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>See aggregated numbers above: on 38 multi-choice logic tasks scaling is poorer than on other natural language tasks; even the largest 280B model achieved only +13.6% above chance on average across the 38 multi-choice logic tasks in the family evaluation. Specific experiment numbers: SI 7B prompt-engineered 58.75% (generative, 11-dataset subset), SI 7B fine-tuned 78.95%, vanilla/COT/280B numbers as reported earlier.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Paper demonstrates that varying the reasoning method (modular SI vs single-pass COT vs vanilla) has larger effects on logical reasoning performance than just scaling model size; e.g., a 7B model in SI can outperform a 280B model in vanilla/COT on the evaluated logic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Model architecture/scale alone is insufficient to solve multi-step logical reasoning; inducing modular, causal reasoning (Selection-Inference) yields much larger improvements than straightforward scaling in the evaluated tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Even the 280B Gopher model gives limited gains on the broader set of logic tasks (average only modestly above chance), indicating scaling delivers diminishing returns for logical reasoning without structural method changes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Scaling language models: Methods, analysis & insights from training gopher <em>(Rating: 2)</em></li>
                <li>ProofWriter: Generating implications, proofs, and abductive statements over natural language <em>(Rating: 2)</em></li>
                <li>Show your work: Scratchpads for intermediate computation with language models <em>(Rating: 1)</em></li>
                <li>Explaining answers with entailment trees <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3307",
    "paper_id": "paper-d48b29889241551e1ee6622fa78c3fa4159255dd",
    "extraction_schema_id": "extraction-schema-76",
    "extracted_data": [
        {
            "name_short": "SI (prompt-engineered, 7B)",
            "name_full": "Selection-Inference framework (prompt-engineered) using a 7B Gopher LLM",
            "brief_description": "An iterative, modular reasoning pipeline that alternates a Selection step (select relevant facts from context) and an Inference step (infer a new fact from the selected facts), implemented via few-shot prompt engineering on a frozen 7B Gopher model; produces causal, human-interpretable multi-step reasoning traces.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Gopher family LLM (used within SI prompt-engineered)",
            "model_description": "Pre-trained Transformer-based language model from the Gopher family (Rae et al., 2021). In these experiments the model is frozen and used in 5-shot prompt-engineered Selection and Inference modules (no gradient updates). Selection is implemented by scoring candidate facts (log-likelihood) and picking the highest-scoring facts; Inference is prompted to generate a single new fact from the selection.",
            "model_size": "7B",
            "reasoning_methods": [
                "Selection-Inference (iterative modular)",
                "Chain-of-Thought (COT) (compared baseline)",
                "Vanilla few-shot prompting (compared baseline)"
            ],
            "reasoning_methods_description": "Selection-Inference: each reasoning step decomposed into (a) Selection: model is prompted to choose relevant facts from the context (implemented by scoring candidate facts via LLM log-likelihood and joining top selections) and (b) Inference: a separate prompt takes the selection (but not the original question) and generates a single inferred fact to be appended to the context; steps are iterated for a fixed number of iterations. Chain-of-Thought: k-shot prompts include example reasoning traces and the model is asked to generate a reasoning trace and answer in one generative pass. Vanilla: k-shot prompts show only [context, question, answer] examples and the model is asked to generate the answer directly (implicit reasoning).",
            "diversity_of_methods": "Uses a distinct, modular two-stage reasoning method (Selection + Inference) that is different in style from single-pass chain-of-thought or vanilla prompting; the paper explicitly contrasts this modular iterative style (diverse/different) with the similar single-style COT and vanilla approaches.",
            "reasoning_task_name": "Subset of logical reasoning tasks (bAbI Tasks 1-3, 15-16; ProofWriter depths 0,1,2,3,5; plus evaluation across other logic-oriented tasks drawn from AAC, 2WikiMultiHop, BigBench subsets, StrategyQA, Jeopardy)",
            "reasoning_task_description": "Natural-language logical reasoning benchmarks probing single-step entailment, deductive and inductive multi-step reasoning, multi-hop retrieval-style inference, and tasks that include distractor facts or require chaining multiple supporting facts (examples: bAbI temporal/fact-chaining tasks, ProofWriter logical proof depths, 2WikiMultiHop multi-hop QA).",
            "performance_by_method": "Generative evaluation (5-shot): SI (7B) = 58.75% average accuracy across 11 selected datasets; by comparison vanilla 7B generative = 2.94%, COT 7B generative = 41.32%; SI (7B) also outperformed 280B baselines (see comparisons). For specific tasks: SI achieved 100% on bAbI15 (deduction) and significantly outperformed baselines on ProofWriter depth 0 and depth 1; per-task breakdown in Fig.4b of the paper.",
            "comparison_of_methods": "Direct head-to-head comparisons in the paper show prompt-engineered SI (7B) &gt; COT (7B) &gt; vanilla (7B) in generative evaluation (all comparisons statistically significant, p&lt;0.01). SI (7B prompt-engineered) also outperformed a 280B model run in vanilla and COT modes on the same suite (examples and p-values reported). Multi-choice evaluation differences are noted: vanilla 7B multi-choice can be high on some tasks (57.31%) but generative comparisons were used for fairness with SI.",
            "key_findings": "Decomposing reasoning into the diverse modular Selection + Inference style substantially improves multi-step logical reasoning vs similar single-pass styles (COT) or vanilla prompting, even allowing a 7B model in SI mode to outperform much larger 280B models run naively or with COT on the evaluated logical tasks; SI produces causal, inspectable traces.",
            "counter_examples_or_negative_results": "SI prompt-engineered gains diminish as reasoning depth increases when Selection is imperfect (prompt-engineered Selection is a bottleneck). Vanilla and COT still compete on easier single-step entailment tasks where LLMs are already strong; in multi-choice settings vanilla 7B can outperform a larger 280B model on some tasks (suggesting priors and evaluation format affect outcomes).",
            "uuid": "e3307.0",
            "source_info": {
                "paper_title": "Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "SI (fine-tuned, 7B)",
            "name_full": "Selection-Inference framework (fine-tuned) using a 7B Gopher LLM",
            "brief_description": "The SI pipeline with Selection and Inference modules implemented by fine-tuning two separate 7B LLMs on single-step reasoning data (ProofWriter), yielding substantially higher final reasoning accuracy and more faithful reasoning traces.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Gopher family LLM (fine-tuned for Selection and Inference)",
            "model_description": "Two copies of a 7B LLM fine-tuned separately: (a) Selection LLM trained to output sentence identifiers (preventing fabrications) that identify which context sentences should be used for a single inference step; (b) Inference LLM fine-tuned to generate the entailed single-step conclusion given selection. Fine-tuning performed on ProofWriter single-step data.",
            "model_size": "7B",
            "reasoning_methods": [
                "Selection-Inference (iterative modular, fine-tuned)"
            ],
            "reasoning_methods_description": "Same Selection + Inference decomposition as prompt-engineered SI, but Selection and Inference models are fine-tuned on single-step ground-truth traces (Selection outputs labels like 'sent 2', Inference learns entailment mapping). Inference converged rapidly (&gt;99% single-step accuracy after ~300 fine-tuning steps); Selection trained ~40k steps and achieved &gt;80% exact-match on many depths though early selections for deep problems remain hard (planning).",
            "diversity_of_methods": "Uses the same diverse two-stage modular approach; fine-tuning reinforces the distinct behaviors of the two modules (selection constrained to context labels, inference constrained to only see selection), increasing the effective diversity of reasoning styles compared to single-pass approaches.",
            "reasoning_task_name": "ProofWriter OWA depths (0,1,2,3,5) and subset evaluation reported in Fig.4 and Fig.5",
            "reasoning_task_description": "ProofWriter: language-formalized logical reasoning tasks with ground-truth proof traces; task depth equals number of inference steps needed to prove or disprove a statement under an Open World Assumption (Unknowns removed in used subset).",
            "performance_by_method": "Generative evaluation (fine-tuned SI 7B) = 78.95% final reasoning accuracy (on the evaluated ProofWriter subset), compared to prompt-engineered SI = 57.93%, and other prompt-engineered baselines: vanilla/COT generative 7B = 0.34% / 15.73% and 280B = 31.58% / 21.12% (numbers reported in the paper). Inference module single-step test accuracy &gt;99%; Selection module &gt;80% exact-match across most depths after fine-tuning.",
            "comparison_of_methods": "Fine-tuning Selection and Inference significantly improves end-task reasoning accuracy vs prompt-engineered SI and vs single-model fine-tuning that predicts all steps at once. The fine-tuned SI produced reasoning traces with higher Jaccard similarity (intersection-over-union) with ground-truth proof steps than a baseline 7B model fine-tuned to predict entire proofs in one pass; baseline proofs often contain extraneous or misordered steps.",
            "key_findings": "Fine-tuning the modular SI components on single-step data yields large gains: fine-tuned SI (7B) substantially outperforms both prompt-engineered SI and baselines, and yields more faithful, ordered reasoning traces that match ground-truth proof steps better than monolithic fine-tuning.",
            "counter_examples_or_negative_results": "Selection still struggles on deeper problems that require planning which facts to pick early; predicting early selections for deeper reasoning tasks remains a limitation.",
            "uuid": "e3307.1",
            "source_info": {
                "paper_title": "Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "COT (baseline)",
            "name_full": "Chain-of-Thought prompting (COT) baseline with Gopher LLMs",
            "brief_description": "Single-pass prompting approach that inserts example multi-step reasoning traces into k-shot prompts so the model generates an explicit chain-of-thought (reasoning trace) and an answer in one generation; used here as a primary baseline to compare to SI.",
            "citation_title": "Chain of thought prompting elicits reasoning in large language models",
            "mention_or_use": "use",
            "model_name": "Gopher family LLMs (7B and 280B) used with Chain-of-Thought prompts",
            "model_description": "Pre-trained Gopher models prompted in 5-shot with examples that include explicit reasoning traces and answers; model is asked to generate [reasoning, answer] in a single generative step (no iterative selection/inference).",
            "model_size": "7B and 280B (evaluated)",
            "reasoning_methods": [
                "Chain-of-Thought (single-pass explicit reasoning)"
            ],
            "reasoning_methods_description": "Prompt examples include concatenated [context, question, reasoning, answer] × k; the model is then prompted on a new [context, question] and asked to generate reasoning and final answer together in one pass. This encourages the model to expose intermediate steps but does not force causal dependence between answer and reasoning.",
            "diversity_of_methods": "Single-style (similar) reasoning approach across steps; COT uses a similar one-pass generative style for all steps rather than modularly decomposing selection and inference.",
            "reasoning_task_name": "Same selected logical reasoning tasks as SI experiments (bAbI, ProofWriter depths, and others in the 11-dataset subset used for Fig.4)",
            "reasoning_task_description": "Benchmarks probing single- and multi-step logical reasoning across natural-language contexts; used here to test whether providing example multi-step traces alone is sufficient to induce causal, correct multi-step reasoning.",
            "performance_by_method": "Generative evaluation (5-shot): COT (7B) = 41.32% average accuracy; COT (280B) = 44.03% average accuracy on the evaluated subset. These are higher than vanilla 7B generative (2.94%) but lower than SI (prompt 7B = 58.75%) and SI (fine-tuned 7B = 78.95%).",
            "comparison_of_methods": "COT improves over vanilla prompting but underperforms the modular SI approach in both prompt-engineered and fine-tuned settings. The paper also reports qualitative findings that COT-produced traces can be incorrect even when final answers are correct, showing weak causal coupling between reasoning trace and model answer.",
            "key_findings": "Providing reasoning traces in a single-pass (COT) helps but is insufficient to ensure causal, correct multi-step reasoning; decomposing into Selection and Inference (SI) produces more accurate final answers and more faithful causal traces.",
            "counter_examples_or_negative_results": "COT traces frequently contain unrelated or incorrect steps while still producing a correct final answer; thus COT traces are not reliably causal or auditably faithful. COT improvements vary with model scale but did not match SI's gains on evaluated logical tasks.",
            "uuid": "e3307.2",
            "source_info": {
                "paper_title": "Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "Vanilla prompting baseline",
            "name_full": "Vanilla few-shot prompting baseline (direct answer generation / scoring)",
            "brief_description": "Standard few-shot prompting where the model is given concatenated examples of [context, question, answer] and asked to generate the answer directly (implicit reasoning); also evaluated in multi-choice by scoring each answer option with the LLM.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Gopher family LLMs (7B and 280B) used with vanilla prompts",
            "model_description": "Pre-trained Gopher models prompted in 5-shot with examples that contain only [context, question, answer] (no explicit reasoning examples). For multi-choice evaluation the LLM scores each candidate answer by likelihood; for generative evaluation the model must produce the exact answer string.",
            "model_size": "7B and 280B",
            "reasoning_methods": [
                "Vanilla few-shot prompting (implicit reasoning)",
                "Multi-choice scoring (evaluation style)"
            ],
            "reasoning_methods_description": "Vanilla: model directly maps context + question to answer in one generation; Multi-choice evaluation uses LLM log-likelihood scores to choose among candidate answers (higher chance-level baseline).",
            "diversity_of_methods": "Single-style implicit reasoning, no explicit step decomposition; similar across tasks and scales.",
            "reasoning_task_name": "Same logical reasoning benchmarks evaluated in the paper (various subsets including 50-task evaluation and the 11-dataset subset used for SI comparisons)",
            "reasoning_task_description": "Benchmarks probing entailment and multi-step logical reasoning; multi-choice and generative evaluation settings used to measure differences in performance.",
            "performance_by_method": "Generative (5-shot): vanilla 7B = 2.94% average accuracy (on 11-dataset subset), vanilla 280B = 31.19%. Multi-choice evaluation: vanilla 7B = 57.31% vs vanilla 280B = 51.45% (surprisingly the smaller model outperformed the larger in multi-choice on the reported subset). Other numbers reported in Sec.6 for prompt-engineered baselines: vanilla/COT generative 7B = 0.34% / 15.73% and 280B = 31.58% / 21.12% (context-specific comparisons in Fig.4/6).",
            "comparison_of_methods": "Vanilla prompting is weakest in generative multi-step settings. Multi-choice evaluation gives vanilla 7B an advantage in some cases, indicating evaluation format and model priors materially affect measured performance. Overall, SI (both prompt and fine-tuned) outperforms vanilla prompting on multi-step reasoning.",
            "key_findings": "Implicit one-shot or k-shot prompting without explicit stepwise decomposition yields poor multi-step logical reasoning in generative settings; multi-choice scoring can mask weaknesses because the task is easier when candidate answers are provided.",
            "counter_examples_or_negative_results": "Vanilla 280B sometimes outperforms smaller models in generative tasks but not in SI comparisons; vanilla 7B surprisingly outperformed 280B in multi-choice on the reported subset, suggesting large models may carry priors that interfere with task-specific logical reasoning.",
            "uuid": "e3307.3",
            "source_info": {
                "paper_title": "Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "Gopher (model family)",
            "name_full": "Gopher: Scaling language models — Methods, analysis & insights from training Gopher",
            "brief_description": "The Gopher family of pre-trained Transformer language models used across experiments as the base LLM (various sizes including 7B and 280B); provided the pre-trained backbone for prompt-engineered and fine-tuned Selection and Inference modules.",
            "citation_title": "Scaling language models: Methods, analysis & insights from training gopher",
            "mention_or_use": "use",
            "model_name": "Gopher",
            "model_description": "Transformer-based large language models trained and evaluated in Rae et al., 2021. The paper uses 7B and 280B parameter variants of Gopher as the LLMs for all experiments; models are either frozen and prompt-engineered or fine-tuned for Selection/Inferences tasks.",
            "model_size": "7B and 280B (evaluated)",
            "reasoning_methods": [
                "Prompt-engineered Selection-Inference",
                "Fine-tuned Selection-Inference",
                "Chain-of-Thought prompting",
                "Vanilla few-shot prompting / multi-choice scoring"
            ],
            "reasoning_methods_description": "Gopher models were used as the underlying LLM in all above styles: frozen for prompt-engineered SI, COT, and vanilla experiments; fine-tuned for the SI Selection and Inference modules in some experiments (ProofWriter).",
            "diversity_of_methods": "The same base model family was used to instantiate multiple reasoning styles; the paper compares how the same base architecture behaves when induced to use different reasoning methods.",
            "reasoning_task_name": "50-task evaluation suite covering many logical reasoning aspects (source corpora include bAbI, BigBench subsets, AAC, ProofWriter, 2WikiMultiHop, Jeopardy, StrategyQA)",
            "reasoning_task_description": "A broad set of tasks probing single-step entailment, multi-step deduction/induction/abduction, multi-hop retrieval, and tasks with distractors/negation; used to characterize LLM capabilities and scaling behavior on logic tasks.",
            "performance_by_method": "See aggregated numbers above: on 38 multi-choice logic tasks scaling is poorer than on other natural language tasks; even the largest 280B model achieved only +13.6% above chance on average across the 38 multi-choice logic tasks in the family evaluation. Specific experiment numbers: SI 7B prompt-engineered 58.75% (generative, 11-dataset subset), SI 7B fine-tuned 78.95%, vanilla/COT/280B numbers as reported earlier.",
            "comparison_of_methods": "Paper demonstrates that varying the reasoning method (modular SI vs single-pass COT vs vanilla) has larger effects on logical reasoning performance than just scaling model size; e.g., a 7B model in SI can outperform a 280B model in vanilla/COT on the evaluated logic tasks.",
            "key_findings": "Model architecture/scale alone is insufficient to solve multi-step logical reasoning; inducing modular, causal reasoning (Selection-Inference) yields much larger improvements than straightforward scaling in the evaluated tasks.",
            "counter_examples_or_negative_results": "Even the 280B Gopher model gives limited gains on the broader set of logic tasks (average only modestly above chance), indicating scaling delivers diminishing returns for logical reasoning without structural method changes.",
            "uuid": "e3307.4",
            "source_info": {
                "paper_title": "Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning",
                "publication_date_yy_mm": "2022-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2
        },
        {
            "paper_title": "Scaling language models: Methods, analysis & insights from training gopher",
            "rating": 2
        },
        {
            "paper_title": "ProofWriter: Generating implications, proofs, and abductive statements over natural language",
            "rating": 2
        },
        {
            "paper_title": "Show your work: Scratchpads for intermediate computation with language models",
            "rating": 1
        },
        {
            "paper_title": "Explaining answers with entailment trees",
            "rating": 1
        }
    ],
    "cost": 0.015899999999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning</h1>
<p>Antonia Creswell ${ }^{1}$, Murray Shanahan ${ }^{1}$ and Irina Higgins ${ }^{1}$<br>${ }^{1}$ DeepMind</p>
<h4>Abstract</h4>
<p>Large language models (LLMs) have been shown to be capable of impressive few-shot generalisation to new tasks. However, they still tend to perform poorly on multi-step logical reasoning problems. Here we carry out a comprehensive evaluation of LLMs on 50 tasks that probe different aspects of logical reasoning. We show that language models tend to perform fairly well at single step inference or entailment tasks, but struggle to chain together multiple reasoning steps to solve more complex problems. In light of this, we propose a Selection-Inference (SI) framework that exploits pre-trained LLMs as general processing modules, and alternates between selection and inference to generate a series of interpretable, casual reasoning steps leading to the final answer. We show that a 7B parameter LLM used within the SI framework in a 5 -shot generalisation setting, with no fine-tuning, yields a performance improvement of over $100 \%$ compared to an equivalent vanilla baseline on a suite of 10 logical reasoning tasks. The same model in the same setting even outperforms a significantly larger 280B parameter baseline on the same suite of tasks. Moreover, answers produced by the SI framework are accompanied by a causal natural-language-based reasoning trace, which has important implications for the safety and trustworthiness of the system.</p>
<h2>1. Introduction</h2>
<p>Large language models (LLMs) are powerful few-shot learners (Bommasani et al., 2021; Brown et al., 2020; Lu et al., 2021). However, one area where they tend to perform poorly is logical reasoning (Rae et al., 2021). Yet the ability to perform multi-step, logically valid reasoning is fundamental for the discovery of new knowledge and explainability. It underpins many advancements that have been made in science, medicine, maths and philosophy. It is also one of the most valued strengths of classical, symbolic AI over contemporary deep learning methods (Bengio et al., 2021; Marcus, 2020; Marcus and Davis, 2019), prompting the recent increase in the use of neurosymbolic approaches to bridge this gap (Garcez and Lamb, 2020; Garnelo and Shanahan, 2019). Here we propose a Selection-Inference (SI) framework that takes inspiration from the neurosymbolic literature to improve the ability of LLMs to do logically valid reasoning.</p>
<p>There are many flavours of neurosymbolic models (Garcez and Lamb, 2020). Those from which we draw inspiration tend to have a modular structure, where each module is specialised for one type of operation (Andreas et al., 2016; Mao et al., 2019). For example, such modules may be neural networks or hand-crafted functions designed to attend to a single object, or to compare the location or size of two inputs (Andreas et al., 2016; Yi et al., 2018). Neurosymbolic models can produce an answer to a complex query by chaining these operations together, passing inputs from one module to another. This has the benefit of producing an interpretable trace of intermediate computations, in contrast to the "black-box" computations common to end-to-end deep learning approaches. Importantly, the modularity of neurosymbolic methods allows them to generalise to significantly harder problems that require long chains of reasoning (Hudson and Manning, 2019). However, the hand-crafted and specialised nature of the modules often makes the resulting systems brittle, hard to optimise, and</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1 | Selection-Inference (SI) framework (c) in comparison with the vanilla baseline (a) and Chain-of-Thought, COT, (b) approach (Wei et al., 2022). (a): The vanilla large language model baseline takes in concatenated [context, question, answer] $\times k$ for k -shot prompting, followed by [context, question] and is asked to generate the answer. All reasoning is done implicitly; (b): COT (Wei et al., 2022) inspired baseline takes in concatenated [context, question, reasoning, answer] $\times k$ for k-shot prompting, followed by [context, question] and is asked to generate the [reasoning, answer]; (c): SI framework consists of two steps. The selection step takes in concatenated [context, question, selection] $\times k$ for k-shot prompting, followed by [context, question] and is asked to select a subset of facts from the context to support a single step of reasoning. The inference step takes in [selection, inference] $\times k$ for k -shot prompting, followed by the selection produced by the Selection module to produce a new fact (the inference) to be added to the context. Each combination of [selection + inference + add fact to context] makes up one step of reasoning. These can be chained together to answer harder problems. The final inference is taken to be the answer.
difficult to extend to new domains (Yi et al., 2018).
Our SI framework, drawing on best practice from neurosymbolic approaches, decomposes logical reasoning into two modular stages: 1) selection, which involves choosing a subset of relevant information sufficient to make a single step of inference, and 2) inference, which only sees the limited information provided by the selection module, and uses it to infer a new intermediate piece of evidence on the way to the final answer (see Fig. 1c). We implement both stages using pre-trained LLMs which, thanks to their powerful few-shot generalisation capabilities, serve as more general alternatives to the hand-crafted, specialised modules typically used in neurosymbolic approaches. In the SI framework, multiple steps of selection and inference are chained together to produce a sequence of reasoning steps. As well as underpinning better performance on reasoning problems, this yields an interpretable trace that justifies the final answer.</p>
<p>Furthermore, the reasoning trace produced by our system is causal, in the sense that each step follows from, and depends on, the previous step. Each inference step is made in isolation, based solely on the limited information provided by the Selection module, without direct access to the question or to previous steps of reasoning. This contrasts with the more common approach of obtaining post-hoc rationalisation, where the answer produced by the model has no direct dependence on the explanation, since the explanation is produced either in parallel to the answer or after the fact (Cobbe et al., 2021; Lampinen et al., 2021; Saha et al., 2020). A notable example that sits in the grey area between post-hoc rationalisation approaches and the more causal explanation approaches is Chain-Of-Thought (COT) (Wei et al., 2022) (see Fig. 1b). In this approach LLMs are encouraged to produce a reasoning trace before the answer. However the dependence of the answer on the reasoning is not explicitly</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" />
(a) Correct reasoning on bAbI deduction (left) and induction (right) tasks.</p>
<p>Figure 2 | Qualitative results from the Selection-Inference (SI) model on bAbI tasks.
encouraged to be causal (as defined above). Indeed, the authors show that while the COT explanations help boost the final answer accuracy, the reasoning traces produced by the model are often wrong even when the final answer is correct (see the Supplementary Materials of (Wei et al., 2022) for examples). Developing a system that can demonstrate how it reaches its answers using a causal reasoning trace has important benefits in terms of safety, explainability, interpretability, debugging, and trust. In this paper we make the following contributions:</p>
<ol>
<li>We provide a comprehensive evaluation of LLMs on a set of 50 tasks probing different aspects of logical reasoning, and show that LLMs are good at simpler single step logical inference in 5 -shot generalisation settings, but struggle with harder problems (Sec. 3)</li>
<li>We introduce the Selection-Inference (SI) framework, a modular, iterative approach to solving reasoning problems (Sec. 4).</li>
<li>We demonstrate the utility of the SI framework by evaluating a 7B parameter LLM from the Gopher family (Rae et al., 2021) on 10 logical reasoning tasks, showing overall that it almost triples the performance of the same model used naively and almost doubles the performance of the same model used in the COT framework. Moreover, it often outperforms a 40x larger 280B LLM baseline used both naively and in the COT framework.</li>
<li>We illustrate further benefits of the SI framework in terms of the causal and interpretable reasoning traces produced (Sec. F.1). These traces can help humans understand how the model reached its final answer, which is useful for debugging and opens the system's decisions to human critique.</li>
</ol>
<h2>2. Related Work</h2>
<p>Our Selection-Inference framework sits at the intersection of classical, symbolic AI and deep learning. A typical symbolic AI system might consist of a knowledge base, which is typically hand-curated by experts, and an inference engine that allows the system to perform logic-based reasoning over its knowledge base. For example, such a system could apply step-by-step reasoning to answer a complex question such as "What are the apothecary's incentives and disincentives for selling poison to Romeo in Romeo and Juliet?" (Lenat, 2019) - something that even the best contemporary deep learning based systems struggle to do.</p>
<p>One of the primary benefits of symbolic AI systems over deep learning models is their interpretabil-</p>
<p>ity; we can look at the reasoning steps such a system has taken to see how the final conclusion was reached. However, unlike deep learning approaches, symbolic AI systems require knowledge to be hand-crafted and are in general hard to scale. Although some approaches have attempted to bridge the gap between deep learning and symbolic AI by converting problems into formal logic (Nye et al., 2021b) and using existing solvers to help produce an answer, this process can be brittle and tends not to scale well. Another attempt to bridge the gap comes from the neurosymbolic perspective (Gupta et al., 2019; Hudson and Manning, 2019; Mao et al., 2019; Yi et al., 2018). These models combine the best parts of deep learning - learning knowledge from data - and symbolic AI - producing an interpretable reasoning trace. However, they are typically quite brittle due to the hand-crafted (Gupta et al., 2019), specialised nature (Mao et al., 2019) of the modules and optimisation difficulties.</p>
<p>On the deep learning side, recent work has attempted to adapt large pre-trained language models, LLMs, to the task of logical reasoning. At a high level these can be split into three groups: 1) approaches that try to fine-tune LLMs to produce the final answer directly, keeping reasoning implicit (Betz et al., 2020; Clark et al., 2020) (e.g. Fig. 1a); 2) approaches that encourage LLMs to produce reasoning explicitly, but all reasoning steps are produced in one generative step (Cobbe et al., 2021; Dalvi et al., 2021; Jhamtani and Clark, 2020; Nye et al., 2021a; Wei et al., 2022; Zelikman et al., 2022) (e.g. Fig. 1B); and 3) approaches that use LLMs to produce each reasoning step one at a time (Tafjord et al., 2020). The latter is where our Selection-Inference framework sits (Fig. 1C). In general it was found that the approaches that incorporate explicit reasoning work better than those that only try to predict the final answer. However, although explicit reasoning helps improve the accuracy of the models, encouraging the models to produce multiple steps of reasoning in a single generative pass is not enough to make the models use reasoning in a causal manner. The authors found that the generated reasoning traces often contain unrelated or incorrect steps while still resulting in the correct answer (see examples in the appendices of (Wei et al., 2022; Zelikman et al., 2022)). Encouraging LLMs to generate each reasoning step one at a time (Tafjord et al., 2020) is currently the most promising direction for achieving causal reasoning, and it is the approach we take in our paper. While the model proposed by Tafjord et al. (2020) is very impressive, it only works for "Prove this statement to be True/False" style questions, since it relies on enumerating all possible implications and checking whether the question statement or its negation are present in the inferred facts, which is also computationally expensive.</p>
<h1>3. How Well Do Large Language Models Reason?</h1>
<p>Past work has shown that LLMs are poor at logical reasoning (Rae et al., 2021), however the evaluation was done on a relatively small set of tasks, and was not systematic. In particular, here we are interested in 1) how LLMs perform on simple entailment tasks compared to multi-step reasoning problems and 2) how scaling laws - suggested by Rae et al. (2021) - apply to logical reasoning. To this end, we evaluated LLMs from the Gopher family in a 5 -shot ${ }^{1}$ setting on a larger set of 50 tasks that touch on different aspects of logical reasoning and vary in terms of the number of reasoning steps required, presence or absence of negation, whether the relevant context information was provided, and whether the model is required to evaluate the accuracy of multiple choices or generate the answer among others. The additional tasks were collected from six sources: bAbI (Weston et al., 2015), BigBench (Ghazal et al., 2017), AAC (Betz et al., 2020), Jeopardy (Tunguz, 2019), Proof Writer (Tafjord et al., 2020) and 2WikiMultiHop (Welbl et al., 2018) (see Fig. S5a in Supplementary Information for raw results).</p>
<p>Our analysis found that LLMs are good at some aspects of logical reasoning (Fig. 3b). In particular,</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" />
(a) Scaling laws for natural language tasks (bigbench, dark purple line, squares, 56 tasks) and tasks involving logical reasoning (logic, light purple line, circles, 38 tasks). All accuracy results are calculated relative to the random baseline ( $0 \%$ accuracy means chance level). Only multi-choice tasks are used.
<img alt="img-3.jpeg" src="img-3.jpeg" />
(b) Language models perform well for simple entailment tasks (AAC tasks, Entailed Polarity), their performance starts to get worse on single step inference problems (bAbI task 1, Proof Writer tasks 0-1), and they struggle with more complex multi-step reasoning problems (2WikiMultiHop tasks, bAbI tasks 2-3, Proof Writer tasks 2-5, StrategyQA).</p>
<p>Figure 3 | Vanilla language models perform poorly on multi-step logical reasoning tasks.
they appear to be good at simple entailment and implication tasks (e.g. see AAC tasks and Entailed Polarity in Fig. S5a). This appears to hold when negation is present (AAC Split Extended tasks), and both in generative (AAC Split) and multiple-choice scoring settings (AAC Split Extended tasks, Entailed Polarity). However, the performance of vanilla language models tends to decrease when they get presented with irrelevant facts alongside the ones relevant for reasoning (e.g. see 2WikiMultiHop With Context tasks, bAbI tasks 2-3 or Proof Writer tasks), when they have to infer the relevant facts from memory (e.g. 2WikiMultiHop or StrategyQA tasks), and as the questions start to require more steps of reasoning (e.g. see the performance drop between bAbI tasks 1-3 or Proof Writer Tasks).</p>
<p>In line with Rae et al.'s findings, our results confirmed that LLMs of larger sizes do perform better than the smaller models. However, we found that even the largest 280B model performed only 13.6\% above chance level on average across the 38 available multi-choice logic tasks (see Figs. S5a-S5b and Sec. H in Supplementary Information for more details). Furthermore, we found that logical reasoning tasks were qualitatively different from other natural language tasks. The scaling law for logic-based tasks within the Gopher family models was significantly worse than for other language tasks measured here as the average performance on the subset of BigBench tasks from (Rae et al., 2021) with the logic tasks used in this paper removed (see Fig. 3a).</p>
<h1>4. The Selection-Inference (SI) Framework</h1>
<p>We are interested in solving logical reasoning problems expressed in natural language. In this work we assume that each question is accompanied by context information (see Figs. 1 and 2a), which contains all the information necessary to solve the problem, as well as potentially irrelevant distractors. In the future this assumption can be relaxed, for example by extracting the necessary information</p>
<p>through search (Lazaridou et al., 2022; Menick et al., 2022). We also assume that all questions are well posed and definitively answerable given the context.</p>
<p>Logical reasoning problems require using existing information to infer new relevant knowledge necessary to answer the question. This can be done through deduction, induction or abduction, although the datasets we use here contain mostly deductive and a small number of inductive problems ${ }^{2}$. Some problems require multiple steps of inference, where later steps use the knowledge inferred in the earlier steps. Hence, we use an iterative framework where at each step the SI uses information in the existing context, $C_{t}$, to infer a new fact, $f_{t}$, which is appended back to the context to create new context, $C_{t+1}=C_{t} \cup f_{t}$. This process can then iterate until the solution to the question is found. In the current implementation of the SI framework, we repeat the process for a fixed number of steps and take the final inference to be the answer. Addressing the issue of halting is left for future work.</p>
<p>Inspired by neurosymbolic methods, we additionally split each step of reasoning into further two components: 1) Selection, which selects a subset of the information present in the context, $s_{t}$, given the context and the question, $C^{t} \cup q$. This selection, $s^{t}$ is fed to the next step, 2) inference, which produces the new fact, $f_{t}$, based on the information passed to it by the selection step. Examples of selection and inference are shown in Fig. 2a. This splitting of each step of reasoning into selection and inference is the main contribution of our paper, and is important for several reasons. First, and most importantly, it makes the resulting reasoning causal, since both steps have limited capabilities by design, and are interdependent. The selection step is constrained to only use the information available in the context, an the inference step only sees the subset of facts provided by the selection without access to the question. Hence, the model is unlikely to make up information to answer the question, and it cannot ignore reasoning when producing the final answer. The other benefit of this approach is that each step of reasoning is broken down into even smaller sub-tasks, which are easier for LLMs to adapt to, and which helps make the reasoning more generalisable to harder problems.</p>
<p>In this paper we use pre-trained, frozen language models from the Gopher family (Rae et al., 2021) in a 5 -shot generalisation setting using prompt engineering to implement the Selection and Inference modules. We settled on prompt engineering to evaluate the base utility of the SI framework, however it can also be used in the fine-tuning setting which we explore briefly in Sec. 6. We next describe the Selection and Inference modules in more detail.</p>
<h1>4.1. Selection Module</h1>
<p>We use prompt engineering to encourage the model to output the correct selection, $s^{t}$. The n-shot prompt is a string of the the following form:</p>
<div class="codehilite"><pre><span></span><code>&quot;&quot;&quot;
<span class="gh">#</span> n-shot prompt
<span class="gh">#</span> First example.
&lt;context 1&gt;
&lt;question 1&gt;
<span class="gh">#</span> Example selection
&lt;fact&gt;. We know that &lt;fact&gt;[ and &lt;fact&gt;]*. Therefore,
...
<span class="gh">#</span> Problem to solve.
&lt;context&gt;
&lt;question&gt;
</code></pre></div>

<p>"" "</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>where <fact>s are copied directly from the context, and [ and <fact>]* means that the module is allowed to select more than one fact for each step of inference, where the total number of facts is a hyper-parameter.</p>
<p>The simplest option to implement the selection is to feed this prompt directly to a pre-trained LLM and take the output generated by the language model. However, this unconstrained approach may allow the model to make up facts, thus removing the causal aspect of the reasoning trace. Indeed during experimentation this is what we often found. So instead we use the pre-trained LLM to score each of the facts in the context, and select the one with the highest log-likelihood. We can then repeat this process by appending each new fact to the end of the previous prompt until the full selection is constructed. Note that for now we halt after a fixed number of steps. See Algorithm 2 for more details.</p>
<h1>4.2. Inference module</h1>
<p>The n-shot prompt for the Inference module has the following form (shown below, also see Fig. 1):</p>
<div class="codehilite"><pre><span></span><code>&quot;&quot;&quot;
<span class="gh">#</span>n-shot inference prompt
<span class="gh">#</span> First example.
&lt;fact&gt;. We know that &lt;fact&gt;[ and &lt;fact&gt;]*. Therefore, &lt;new fact
<span class="k">    &gt;</span>
<span class="ge">. . .</span>
<span class="gh">#</span> Problem to solve.
&lt;output of the Selection module&gt;. Therefore,
&quot;&quot; &quot;
</code></pre></div>

<p>The n-shot prompt and the output of the Selection module, are fed to the pre-trained LLM serving as the Inference module. The first generated sentence (extracted from the generated text as per BigBench evaluation (Ghazal et al., 2017) pipeline, see Supplementary Materials for details) is taken to be the newly inferred fact. This fact is added to the context, which concludes one reasoning step of the SI framework. For now, we halt after a fixed number of steps. See Algorithm 1 for more details.</p>
<div class="codehilite"><pre><span></span><code>Algorithm 1 Selection-Inference
Require: An n-shot selection prompt, \(p_{\text {select }}\).
Require: An n-shot inference prompt, \(p_{\text {in } f e r}\).
Require: Initial Context, \(C^{0}\), made up of facts (and rules).
Require: The question, \(q\).
Require: Language model, LLM.
Require: A halting function, halt, determines if the answer has been
    reached.
    \(t=0 \quad \triangleright\) Start at step 0 .
    while not halt() do
        \(s^{t} \leftarrow\) Selection_Module \(\left(p_{\text {select }}, C^{t}, q, \mathrm{LLM}\right) \quad \triangleright\) Do selection.
        \(i^{t} \leftarrow\) Inference_Module \(\left(p_{\text {in } f e r}, s^{t}\right) \quad \triangleright\) Do inference.
        \(C^{t+1} \leftarrow C^{t} \cup i^{t} \quad \triangleright\) Add the newly inferred fact to the context.
        \(t \leftarrow t+1 \quad \triangleright\) Move onto the next step of reasoning
    end while
    return \(s^{t}\)
</code></pre></div>

<h1>5. Experiments and Results</h1>
<p>We evaluate our SI framework on a subset of $10 / 50$ logical reasoning tasks introduced in Sec. 3. These tasks were chosen based on whether they include context information necessary to answer the question, whether the questions have a definitive answer, and to ensure that they cover different kinds of reasoning abilities. The tasks include bAbI (Weston et al., 2015) Tasks 1-3, which require the model to use 1-3 supporting time-ordered facts respectively to answer a question, and Tasks 15-16, which test deductive and inductive reasoning respectively. We also evaluate our model on the Proof Writer OWA datasets (Tafjord et al., 2020) of depth 0, 1, 2, 3 and 5 (there is no depth 4 task). These are language-based logical reasoning problems, where the depth is the number of reasoning steps required to answer the question.</p>
<p>To baseline the performance of the SI framework, we consider a 7B (same size as the LLM used in the SI framework) and a 40x larger 280B parameter LLM evaluated in a 5-shot setting. There are two types of evaluation for these vanilla baselines that we consider: multi-choice and generative evaluation. In generative evaluation, we measure the exact string match (first sentence in lower case and ignoring any non-alphabetic characters) between the output generated by the LLM and the ground truth answer. This is appropriate, since most of the tasks that we consider require either a single word answer, or the dataset is such that the answers are highly structured. In multi-choice evaluation the LLM is used to score each of the answer choices, as in Li et al. (Li et al., 2021). In general LLMs perform significantly better in a multi-choice vs generative evaluation setting, since the chance level in the multi-choice setting is significantly higher.</p>
<p>We also consider a chain-of-thoughts (COT) (Wei et al., 2022) inspired baseline, where the k-shot prompts to the 7B and 280B models include reasoning traces for the same examples that we use to prompt the SI framework (although with selection and inference combined, see Supplementary Information for example prompts). This tests whether providing the reasoning examples alone is sufficient to improve performance, or whether the further breakdown into Selection and Inference sub-steps improves accuracy. Note that among all of the approaches outlined only the SI framework is explicitly set up to generate causal reasoning traces.</p>
<p>Fig. 4a demonstrates that overall when evaluated generatively, the 7B parameter LLM within the SI framework performs better ( $58.75 \%$ ) than the same model evaluated naively ( $2.94 \%$ ) or in the COT framework ( $41.32 \%$ ) (all $p&lt;0.01$, see Supplementary Information for details of the calculations). Not only that, the 7B parameter LLM within the SI framework also outperforms on average the 40x larger 280B parameter LLM in both vanilla ( $31.19 \%$ ) and COT framework ( $44.03 \%$ ) (all $p&lt;0.01$ ). When evaluated in the easier multi-choice setting, we find that surprisingly ${ }^{3}$ the vanilla 7B parameter LLM outperforms the 280B parameter LLM ( $57.31 \%$ vs $51.45 \%$ ), while still performing significantly worse than the 7B SI model ( $p=0.012$ ). Note that the latter is evaluated in the harder generative setting. Per task breakdown shown in Fig. 4b demonstrates that the SI framework solves the bAbI 15 deduction task, the only model to achieve $100 \%$ accuracy (significant difference from the other models, $p&lt;0.01$ ). Furthermore, it does so having seen only five examples in the prompt. The 7B SI model also significantly outperforms all other models on Proof Writer Depth 0 ( $p&lt;0.01$ ), Proof Writer Depth $1(p=0.034)$.</p>
<p>As well as improving upon most baselines quantitatively, the SI framework also has additional qualitative benefits: 1) it produces a causal, human interpretable reasoning trace that shows how the model reached its answer and 2) it is able to recover from errors. We will now discuss each of these</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" />
(a) Average accuracy over 11 datasets comparing like-for-like generative performance of the 7B and 280B parameter language models used in a 5 -shot generalisation setting to predict the answer directly (vanilla), in the Chain-Of-Thought framework, COT, and in the SI framework.
<img alt="img-5.jpeg" src="img-5.jpeg" />
(b) Per task breakdown of the performance of LLMs used naively, and within the COT and SI frameworks.</p>
<p>Figure 4 | Quantitative results for the Selection-Inference (SI) framework.
in turn.
Since the Selection module is only allowed to pick facts from the context and is separate from the Inference module, and since the latter does not have access to the question, the model has to use what is selected and cannot bypass the selection to compute its answer, thus creating a causal reasoning trace. Since the reasoning trace is in natural language and is causal, it can be audited and inspected by humans, which has significant implications for safety and interpretability.</p>
<p>Example reasoning traces produced by the SI model are shown in Fig. 2a. In the bAbI 16 example shown on the right the model is solving an inference problem, which requires the model to infer the colour of an animal given facts about the colours of other animals. In this example, the model is asked "What colour is greg", and told that "greg is a lion". This means first the model needs to use induction to infer a rule about the colour of lions. On the first step, we see that the model induces a rule, "lions are white", based on the fact that "brian is a lion" and "brian is white"; we can see exactly what data underlies the model's decision to form a new rule. On the second step, we see that the model applies this newly inferred rule to the fact that "greg is a lion" to reach the final conclusion that "greg is white" using deduction. Note that the ability of the SI framework to produce inductions relies on its ability to deal with uncertainty and understand what is "reasonable" - something that LLMs are naturally capable of, while also being a known struggle point for symbolic AI.</p>
<p>Since the reasoning traces are output in natural language, they are easy for humans to interpret and potentially intervene. Consider a scenario where there may be both a white lion and a green lion mentioned in the context, in which case we could see which information the model used to make its final decision and decide whether we want to trust it (example in Fig. 2b). We could also imagine examples where the model puts together two unrelated facts to come up with an incorrect inference, and this could also be easily be spotted by a human and rectified by replacing the wrongly inferred fact with a correct one, and re-running the consequent reasoning steps.</p>
<p>Aside from inspecting reasoning traces and using them to debug when something goes wrong, the additive nature of our model - it accumulates new knowledge with each reasoning step, means that it also has the ability to recover from errors. Fig. 2b demonstrates such an example. In the first step</p>
<p>the model inferred that "swans are often gray", using the facts that "julius is a swan" and "julius is gray". While this is correct, this new information is not useful for answering the question, which asks about lions. However, it is still possible for the model to make the more useful inference that "lions are often white" in a later step and recover from its original misstep.</p>
<h1>6. Fine-tuning Language Models for Selection and Inference</h1>
<p>In Sec. 5 we have demonstrated significant improvements in logical reasoning accuracy when using prompt-engineering to specialise LLMs for Selection and Inference in the SI framework. Promptengineering has the additional benefit of not requiring large amounts of step-by-step reasoning data, which may be hard to obtain. In this section we investigate whether the accuracy of the SI framework can be further improved by fine-tuning the LLMs for Selection and Inference. We use the Proof Writer dataset for which ground truth reasoning traces exist.</p>
<p>The Selection LLM is fine-tuned to select a subset of sentences (including facts and rules) from the context by generating a string of the form "sent 2. We know that sent 4 [and sent 7] "." given the context and the question. We ask the Selection LLM to generate sentence labels (e.g. "sent 2" or "sent 4") instead of the sentences themselves, because this prevents the Selection LLM from cheating by making up facts to answer the question quicker. This preserves the dependency of the selection and therefore subsequent reasoning steps on the context. The Inference LLM is fine-tuned to compute an entailment given the selection. Both models are fine-tuned on single steps of reasoning only. Example training data are shown in Fig. S2.</p>
<p>The Inference LLM converged very quickly to $&gt;99 \%$ test accuracy after only 300 fine-tuning steps with a batch size of 16 , which is to be expected as we found that pre-trained LLMs are good at single step entailment out of the box as shown in Fig. 3b. Examples of inferences made by the model are shown in Fig. S3. The Selection LLM was trained for $4 \times 10^{4}$ steps (with batch size 16 for 50 hours on a TPU) with the exact string match accuracy reported in Fig. 5a. Although we notice that the model is much better at predicting selections for problems that require fewer steps of inference than those that require more, ultimately the model still achieves high ( $&gt;80 \%$ ) accuracy across most of the reasoning depths. Predicting early selections for deeper reasoning problems is hard, because it requires planning. It is an important problem to address in future work.</p>
<p>Fig. 4 b shows that fine-tuning LLMs on single steps of reasoning within the SI framework leads to significant improvements in final reasoning accuracy ( $78.95 \%$ ) over the prompt-engineered version of the SI framework (57.93\%) and other prompt-engineered baselines (vanilla/COT generative 7B: $0.34 / 15.73 \%, 280 \mathrm{~B}: 31.58 / 21.12 \%)$. We also found that the fine-tuned 7B LLM used within the SI framework produces significantly more accurate reasoning traces compared to the same LLM fine-tuned to predict all reasoning steps in one go (Fig. 5b). We quantified this using the Jaccard Similarity, Jaccard Similarity $=(M \cap G T) /(M \cup G T)$, between the proof steps predicted by each model, $M$, and the ground-truth reasoning steps, $G T$, as shown in, calculated using exact string match over alphanumeric characters.</p>
<p>Qualitatively we observed that while the baseline model is good at predicting most of the reasoning steps, they often appear in the wrong order, there are additional reasoning steps that are not on the minimal reasoning path, and some steps get repeated a number of times.</p>
<h2>7. Conclusion</h2>
<p>We have presented the Selection-Inference framework for improving the ability of pre-trained language models to solve logical reasoning problems expressed in natural language. Our approach borrows</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" />
(a) Average test fine-tuning accuracy for the Selection module trained on single-step selection across all Proof Writer datasets (depth 1, 2, 3 and 5) and tested on problems of each depth separately.
<img alt="img-7.jpeg" src="img-7.jpeg" />
(b) Intersection over union between reasoning traces produced by a model and the ground truth reasoning steps. Baseline, 7B parameter LLM finetuned to predict all reasoning steps in one go; SI, using same 7B LLM fine-tuned for single step Selection and Inference.</p>
<p>Figure 5 | Fine-tuning the SI framework on the Proof Writer dataset.
from the best practices of neurosymbolic approaches to break down logical reasoning into a modular recursive pipeline that not only significantly improves the reasoning accuracy, but also produces a causal interpretable reasoning trace. We have demonstrated that prompt-engineered LLMs used in the SI framework significantly outperform both the vanilla and COT baselines evaluated in equivalent settings, and even 40x larger baselines. The performance of the SI framework can be further improved through fine-tuning if step-by-step reasoning data is available.</p>
<p>A model capable of casual, interpretable and logically valid multi-step reasoning has potential applications in law, medicine, science, maths, economics, and other areas where trustworthy and verifiable logical inference is important. At the same time we recognise that special care will need to be taken to evaluate such a model before deployment in any of these settings. Further work is also needed, for example, to improve the Selection module (e.g. by allowing the model search over and evaluate different reasoning traces); to address the halting issue (both in terms of when to stop the selection and when to stop the overall reasoning); to incorporate verifiers that would help avoid false inferences being added to the context; to enable the system to source its own relevant context rather than relying on it being provided in the dataset; and to extend the ability of the system to deal with ambiguous or unanswerable questions.</p>
<h1>References</h1>
<p>J. Andreas, M. Rohrbach, T. Darrell, and D. Klein. Neural module networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 39-48, 2016.
Y. Bengio, Y. Lecun, and G. Hinton. Deep learning for ai. Communications of the ACM, 64(7):58-65, 2021.
G. Betz, C. Voigt, and K. Richardson. Critical thinking for language models. arXiv preprint arXiv:2009.07185, 2020.
R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein, J. Bohg,</p>
<p>A. Bosselut, E. Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.
T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.
P. Clark, O. Tafjord, and K. Richardson. Transformers as soft reasoners over language. arXiv preprint arXiv:2002.05867, 2020.
K. Cobbe, V. Kosaraju, M. Bavarian, J. Hilton, R. Nakano, C. Hesse, and J. Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.
B. Dalvi, P. A. Jansen, O. Tafjord, Z. Xie, H. Smith, L. Pipatanangkura, and P. Clark. Explaining answers with entailment trees. ArXiv, abs/2104.08661, 2021.
A. d. Garcez and L. C. Lamb. Neurosymbolic ai: the 3rd wave. arXiv preprint arXiv:2012.05876, 2020.
M. Garnelo and M. Shanahan. Reconciling deep learning with symbolic artificial intelligence: representing objects and relations. Current Opinion in Behavioral Sciences, 29:17-23, 2019.
A. Ghazal, T. Ivanov, P. Kostamaa, A. Crolotte, R. Voong, M. Al-Kateb, W. Ghazal, and R. V. Zicari. Bigbench v2: The new and improved bigbench. In 2017 IEEE 33rd International Conference on Data Engineering (ICDE), pages 1225-1236, 2017. doi: 10.1109/ICDE.2017.167.
N. Gupta, K. Lin, D. Roth, S. Singh, and M. Gardner. Neural module networks for reasoning over text. arXiv preprint arXiv:1912.04971, 2019.
D. A. Hudson and C. D. Manning. Learning by abstraction: The neural state machine. arXiv preprint arXiv:1907.03950, 2019.
H. Jhamtani and P. Clark. Learning to explain: Datasets and models for identifying valid reasoning chains in multihop question-answering. arXiv preprint arXiv:2010.03274, 2020.
A. K. Lampinen, N. A. Roy, I. Dasgupta, S. C. Chan, A. C. Tam, J. L. McClelland, C. Yan, A. Santoro, N. C. Rabinowitz, J. X. Wang, et al. Tell me why!-explanations support learning of relational and causal structure. arXiv preprint arXiv:2112.03753, 2021.
A. Lazaridou, E. Gribovskaya, W. Stokowiec, and N. Grigorev. Internet-augmented language models through few-shot prompting for open-domain question answering. arXiv preprint arXiv:2203.05115, 2022.
D. Lenat. What ai can learn from romeo \&amp; juliet, Jul 2019. URL https://www.forbes.com/ sites/cognitiveworld/2019/07/03/what-ai-can-learn-from-romeo--juliet.
X. L. Li, A. Kuncoro, C. de Masson d'Autume, P. Blunsom, and A. Nematzadeh. A systematic investigation of commonsense understanding in large language models. arXiv e-prints, pages arXiv-2111, 2021.
B. Y. Lin, S. Lee, R. Khanna, and X. Ren. Birds have four legs?! numersense: Probing numerical commonsense knowledge of pre-trained language models. arXiv preprint arXiv:2005.00683, 2020.
K. Lu, A. Grover, P. Abbeel, and I. Mordatch. Pretrained transformers as universal computation engines. arXiv preprint arXiv:2103.05247, 2021.</p>
<p>J. Mao, C. Gan, P. Kohli, J. B. Tenenbaum, and J. Wu. The neuro-symbolic concept learner: Interpreting scenes, words, and sentences from natural supervision. arXiv preprint arXiv:1904.12584, 2019.
G. Marcus. The next decade in ai: four steps towards robust artificial intelligence. arXiv preprint arXiv:2002.06177, 2020.
G. Marcus and E. Davis. Rebooting AI: Building Artificial Intelligence We Can Trust. Ballantine Books Inc., 2019.
J. Menick, M. Trebacz, V. Mikulik, J. Aslanides, F. Song, M. Chadwick, M. Glaese, S. Young, L. CampbellGillingham, G. Irving, et al. Teaching language models to support answers with verified quotes. arXiv preprint arXiv:2203.11147, 2022.
S. Min, X. Lyu, A. Holtzman, M. Artetxe, M. Lewis, H. Hajishirzi, and L. Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work? arXiv preprint arXiv:2202.12837, 2022.
R. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim, C. Hesse, S. Jain, V. Kosaraju, W. Saunders, et al. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021.
M. Nye, A. J. Andreassen, G. Gur-Ari, H. Michalewski, J. Austin, D. Bieber, D. Dohan, A. Lewkowycz, M. Bosma, D. Luan, et al. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114, 2021a.
M. Nye, M. Tessler, J. Tenenbaum, and B. M. Lake. Improving coherence and consistency in neural sequence models with dual-system, neuro-symbolic reasoning. Advances in Neural Information Processing Systems, 34, 2021b.
J. W. Rae, S. Borgeaud, T. Cai, K. Millican, J. Hoffmann, F. Song, J. Aslanides, S. Henderson, R. Ring, S. Young, et al. Scaling language models: Methods, analysis \&amp; insights from training gopher. arXiv preprint arXiv:2112.11446, 2021.
S. Saha, S. Ghosh, S. Srivastava, and M. Bansal. Prover: Proof generation for interpretable reasoning over rules. arXiv preprint arXiv:2010.02830, 2020.
O. Tafjord, B. D. Mishra, and P. Clark. Proofwriter: Generating implications, proofs, and abductive statements over natural language. arXiv preprint arXiv:2012.13048, 2020.
B. Tunguz. 200,000+ jeopardy! questions, Nov 2019. URL https://www.kaggle.com/datasets/ tunguz/200000-jeopardy-questions.
J. Wei, X. Wang, D. Schuurmans, M. Bosma, E. Chi, Q. Le, and D. Zhou. Chain of thought prompting elicits reasoning in large language models, 2022.
J. Welbl, P. Stenetorp, and S. Riedel. Constructing datasets for multi-hop reading comprehension across documents. Transactions of the Association for Computational Linguistics, 6:287-302, 2018.
J. Weston, A. Bordes, S. Chopra, A. M. Rush, B. Van Merriënboer, A. Joulin, and T. Mikolov. Towards ai-complete question answering: A set of prerequisite toy tasks. arXiv preprint arXiv:1502.05698, 2015.
K. Yi, J. Wu, C. Gan, A. Torralba, P. Kohli, and J. B. Tenenbaum. Neural-symbolic vqa: Disentangling reasoning from vision and language understanding. arXiv preprint arXiv:1810.02338, 2018.
E. Zelikman, Y. Wu, and N. D. Goodman. Star: Bootstrapping reasoning with reasoning. arXiv preprint arXiv:2203.14465, 2022.</p>
<h1>Supplementary Information</h1>
<h2>A. Example prompts for vanilla baselines</h2>
<h2>A.1. ProofWriter</h2>
<div class="codehilite"><pre><span></span><code><span class="s2">&quot;&quot;</span><span class="err">&quot; </span>
<span class="err">Here are some statements that describe a situation:</span>
<span class="err">Bob is cold.</span>
<span class="err">Charlie is quiet.</span>
<span class="err">Gary is cold.</span>
<span class="err">Harry is quiet.</span>
<span class="err">Big things are cold.</span>
<span class="err">All blue things are not cold.</span>
<span class="err">If something is quiet and blue then it is not cold.</span>
<span class="err">All quiet things are cold.</span>
<span class="err">If something is big and rough then it is round.</span>
<span class="err">If something is cold and not rough then it is blue.</span>
<span class="err">If something is quiet and not furry then it is not blue.</span>
<span class="err">Round things are big.</span>
<span class="nv">Based</span><span class="w"> </span><span class="nv">on</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">above</span>,<span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">statement</span><span class="w"> </span><span class="s2">&quot;Charlie is cold&quot;</span><span class="w"> </span><span class="nv">is</span><span class="w"> </span><span class="nv">true</span>.
</code></pre></div>

<p>Here are some statements that describe a situation:
Erin is not cold.
Erin is kind.
Erin is red.
Erin is smart.
Erin is not white.
Erin is young.
Gary is cold.
Gary is not furry.
Gary is kind.
Gary is red.
Gary is not smart.
Gary is young.
All cold, smart things are not furry.
Young, cold things are not furry.
If something is white and smart then it is furry.
If Gary is white then Gary is not furry.
If Erin is young then Erin is furry.
If Gary is not young then Gary is smart.
If Erin is cold then Erin is young.
Red things are kind.
Based on the above, the statement "Erin is not furry" is
"""</p>
<h2>A.2. bAbI 1</h2>
<p>"" "
Context: daniel went to the bedroom
daniel journeyed to the office
daniel travelled to the bathroom
mary went to the office</p>
<div class="codehilite"><pre><span></span><code>john journeyed to the bedroom
daniel went back to the kitchen
john went to the garden
daniel travelled to the office
Question: where is john?
Choice: garden
Choice: bathroom
Choice: office
Choice: kitchen
Choice: bedroom
Choice: hallway
Answer: garden
</code></pre></div>

<p>.
Context: sandra went to the kitchen
sandra went to the office
sandra travelled to the hallway
sandra went back to the kitchen
mary travelled to the hallway
sandra went to the bedroom
john went to the garden
sandra travelled to the office
Question: where is sandra?
Choice: garden
Choice: bedroom
Choice: kitchen
Choice: bathroom
Choice: hallway
Choice: office
Answer:
"""</p>
<h1>A.3. 2WikiMultiHop</h1>
<p>New lines are added between facts to fit on the page.
"""
Q: When did Michael Baden-Powell's father die?
Here are some relationships to help answer this question.
Michael Baden-Powell::father::Peter Baden-Powell, 2nd Baron BadenPowell,
Peter Baden-Powell, 2nd Baron Baden-Powell::date of death::9
December 1962
A: 9 December 1962
・.
Q: Where does Ekaterina Rybolovleva's father work at?
Here are some relationships to help answer this question.
Ekaterina Dmitrievna Rybolovleva::father::Dmitry Rybolovlev,
Dmitry Rybolovlev::employer::Uralkali
A:
""</p>
<h1>B. Example prompts for COT baselines</h1>
<h2>B.1. Proof Writer 3</h2>
<div class="codehilite"><pre><span></span><code><span class="s">&quot;&quot;&quot;</span>
<span class="s">Given a set of rules and facts, you have to reason whether a</span>
<span class="s">    statement is true or false.</span>
<span class="s">Here are some facts and rules:</span>
<span class="s">If someone is red then they are nice.</span>
<span class="s">If someone is kind and red then they are white.</span>
<span class="s">If someone is nice then they are kind.</span>
<span class="s">Fiona is red.</span>
<span class="s">Does it imply that the statement &quot;</span><span class="nx">Fiona</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="nx">white</span><span class="err">&quot;</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">True</span><span class="p">?</span>
<span class="nx">Reasoning</span><span class="p">:</span><span class="w"> </span><span class="nx">If</span><span class="w"> </span><span class="nx">someone</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">red</span><span class="w"> </span><span class="k">then</span><span class="w"> </span><span class="nx">they</span><span class="w"> </span><span class="nx">are</span><span class="w"> </span><span class="nx">nice</span><span class="p">.</span><span class="w"> </span><span class="nx">We</span><span class="w"> </span><span class="nx">know</span><span class="w"> </span><span class="nx">that</span><span class="w"> </span><span class="nx">Fiona</span>
<span class="w">    </span><span class="k">is</span><span class="w"> </span><span class="nx">red</span><span class="p">.</span><span class="w"> </span><span class="nx">Therefore</span><span class="p">,</span><span class="w"> </span><span class="nx">Fiona</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">nice</span><span class="p">.</span>
<span class="nx">If</span><span class="w"> </span><span class="nx">someone</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">nice</span><span class="w"> </span><span class="k">then</span><span class="w"> </span><span class="nx">they</span><span class="w"> </span><span class="nx">are</span><span class="w"> </span><span class="nx">kind</span><span class="p">.</span><span class="w"> </span><span class="nx">We</span><span class="w"> </span><span class="nx">know</span><span class="w"> </span><span class="nx">that</span><span class="w"> </span><span class="nx">Fiona</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">nice</span><span class="p">.</span>
<span class="w">    </span><span class="nx">Therefore</span><span class="p">,</span><span class="w"> </span><span class="nx">Fiona</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">kind</span><span class="p">.</span>
<span class="nx">If</span><span class="w"> </span><span class="nx">someone</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">kind</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">red</span><span class="w"> </span><span class="k">then</span><span class="w"> </span><span class="nx">they</span><span class="w"> </span><span class="nx">are</span><span class="w"> </span><span class="nx">white</span><span class="p">.</span><span class="w"> </span><span class="nx">We</span><span class="w"> </span><span class="nx">know</span><span class="w"> </span><span class="nx">that</span><span class="w"> </span><span class="nx">Fiona</span>
<span class="w">    </span><span class="k">is</span><span class="w"> </span><span class="nx">kind</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">Fiona</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">red</span><span class="p">.</span><span class="w"> </span><span class="nx">Therefore</span><span class="p">,</span><span class="w"> </span><span class="nx">Fiona</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">white</span><span class="p">.</span>
</code></pre></div>

<p>Here are some facts and rules:
If someone chases the cow then they eat the cow.
If someone is big then they chase the cow.
If someone needs the bald eagle then the bald eagle is big.
If the bear is nice and the bear needs the cow then the bear eats the lion.
If someone needs the lion and they eat the bald eagle then they are blue.
If someone eats the bear and they do not chase the cow then the cow is young.
the bald eagle eats the lion.
the bear is round.
the lion eats the bald eagle.
the bald eagle needs the cow.
the bear is young.
the cow is not nice.
the cow does not chase the bald eagle.
the bear does not eat the bald eagle.
the bear needs the bald eagle.
the bald eagle chases the bear.
Does it imply that the statement "The bald eagle does not eat the cow" is True?
Reasoning: If someone needs the bald eagle then the bald eagle is big. We know that the bear needs the bald eagle. Therefore, the bald eagle is big.
If someone is big then they chase the cow. We know that the bald eagle is big. Therefore, the bald eagle chases the cow.
If someone chases the cow then they eat the cow. We know that the bald eagle chases the cow. Therefore, the bald eagle eats the cow
"" "</p>
<h1>B.2. bAbI 2</h1>
<p>"" "
Below are some stories about people moving objects between rooms.
After each story you have to answer a question about where a particular object is.
Story:
at $t=0$ mary grabbed the football there
at $t=1$ daniel got the apple there
at $t=2$ mary went to the kitchen
at $t=3$ daniel journeyed to the office
at $t=4$ daniel went to the bedroom
at $t=5$ mary moved to the garden
Question: where is the apple?
Reason: at $t=1$ daniel got the apple there. We know that at $t=4$ daniel went to the bedroom. Therefore, the apple is in the bedroom</p>
<p>Story:
at $t=0$ sandra went to the office
at $t=1$ john took the milk there
at $t=2$ sandra got the milk there
at $t=3$ john dropped the milk
Question: where is the milk?
Reason: at $t=2$ sandra got the milk there. We know that at $t=0$ sandra went to the office. Therefore, the milk is in the office
" " "</p>
<h2>C. Example prompts for Selection-Inference</h2>
<h2>C.1. bAbI 2</h2>
<p>The selection prompt:
"" "
Here are a collection of stories about people carrying objects from one room to another. You will be asked where any object is. To answer this question you need to figure out who last had the object and which room they have the object in by the end of the story. Here are some examples:</p>
<p>Story:
at $t=0$ mary grabbed the football there
at $t=1$ daniel got the apple there
at $t=2$ mary went to the kitchen
at $t=3$ daniel journeyed to the office
at $t=4$ daniel went to the bedroom
at $t=5$ mary moved to the garden
Question: where is the apple?
Reason: at $t=1$ daniel got the apple there. We know that at $t=4$ daniel went to the bedroom</p>
<p>at $t=0$ john moved to the bathroom
at $t=1$ john travelled to the office
at $t=2$ john picked up the football there
at $t=3$ john journeyed to the bathroom
Question: where is the football?
Reason:" " "
The inference prompt:</p>
<div class="codehilite"><pre><span></span><code>&quot;&quot;&quot;
at t=1 daniel got the apple there. We know that at t=4 daniel went
    to the bedroom. Therefore, the apple is in the bedroom.
at t=2 john picked up the football there. We know at t=0 john moved
    to the bathroom. Therefore&quot;&quot;&quot;
</code></pre></div>

<h1>C.2. Proof Writer</h1>
<p>Below is an example selection prompt. Note that this is for a depth-2 problem and so we show examples of the first reasoning step where the conclusion would not directly prove or disprove the statement and the last reasoning step, where the conclusion would directly prove or disprove the statement.</p>
<div class="codehilite"><pre><span></span><code><span class="s">&quot;&quot;&quot;</span>
<span class="s">Given a set of rules and facts, you have to reason whether a</span>
<span class="s">    statement is true or false.</span>
<span class="s">Here are some facts and rules:</span>
<span class="s">Nice people are quiet.</span>
<span class="s">If Dave is smart then Dave is nice.</span>
<span class="s">All white people are smart.</span>
<span class="s">Dave is smart.</span>
<span class="s">Harry is cold.</span>
<span class="s">Does it imply that the statement &quot;</span><span class="nx">Dave</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="nx">quiet</span><span class="err">&quot;</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="kc">true</span><span class="p">?</span>
<span class="nx">Reasoning</span><span class="p">:</span><span class="w"> </span><span class="nx">If</span><span class="w"> </span><span class="nx">Dave</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">smart</span><span class="w"> </span><span class="k">then</span><span class="w"> </span><span class="nx">Dave</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">nice</span><span class="p">.</span><span class="w"> </span><span class="nx">We</span><span class="w"> </span><span class="nx">know</span><span class="w"> </span><span class="nx">that</span><span class="w"> </span><span class="nx">Dave</span><span class="w"> </span><span class="k">is</span>
<span class="w">    </span><span class="nx">smart</span><span class="p">.</span><span class="w"> </span><span class="nx">Therefore</span><span class="p">,</span>
</code></pre></div>

<p>Here are some facts and rules:
Blue things are green.
All blue things are white.
If Anne is not big then Anne is blue.
Big things are white.
All kind things are round.
If something is white and big then it is not kind.
If something is big and not rough then it is green.
If something is white and blue then it is not green.
Erin is not white.
Anne is big.
Bob is rough.
Anne is white</p>
<div class="codehilite"><pre><span></span><code><span class="nx">Does</span><span class="w"> </span><span class="nx">it</span><span class="w"> </span><span class="nx">imply</span><span class="w"> </span><span class="nx">that</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">statement</span><span class="w"> </span><span class="s">&quot;Anne is kind&quot;</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">True</span><span class="p">?</span>
<span class="nx">Reasoning</span><span class="p">:</span><span class="w"> </span><span class="nx">If</span><span class="w"> </span><span class="nx">something</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">white</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">big</span><span class="w"> </span><span class="k">then</span><span class="w"> </span><span class="nx">it</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="nx">kind</span><span class="p">.</span><span class="w"> </span><span class="nx">We</span>
<span class="w">    </span><span class="nx">know</span><span class="w"> </span><span class="nx">that</span><span class="w"> </span><span class="nx">Anne</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">white</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">Anne</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">big</span><span class="p">.</span><span class="w"> </span><span class="nx">Therefore</span><span class="p">,</span>
</code></pre></div>

<p>. .</p>
<p>Here are some facts and rules:
If something likes the squirrel and it is not young then it chases the lion.
If something likes the squirrel then it is rough.
If something chases the rabbit and the rabbit is not young then it chases the lion.
If something eats the lion then it is young.
If something likes the rabbit then it chases the rabbit.
All rough things are nice.
the rabbit is young.
the squirrel likes the rabbit.
the lion likes the squirrel.
Does it imply that the statement "The lion is not nice" is True?
Reasoning: " " "
Example inference prompt:
" " "
Nice people are quiet. We know that Dave is nice. Therefore, Dave is quiet.</p>
<p>If the cow chases the bald eagle then the cow eats the bald eagle. We know that the cow chases the bald eagle. Therefore""</p>
<h1>D. Selection-Inference evaluation details</h1>
<h2>D.1. Selection module</h2>
<p>The algorithm for the Scoring Selection module is shown in Algorithm 2.
Algorithm 2 Scoring Selection_Module
Require: An n-shot prompt, $p$.
Require: Initial Context, $C^{0}$, made up of facts (and rules).
Require: The question, $q$.
Require: Language model, LLM.
Require: A halting function, halt, determines if the selection is complete. $s^{t} \leftarrow$ empty string
while not halt() do
$s_{\text {temp }} \leftarrow \arg \max <em _text="\text" _token="{token">{\text {rule_or_fact } \in C} \sum</em>\right)\right.$ $\triangleright$ Choose the rule or fact with the maximum log-likelihood under the LLM model.
$s^{t} \leftarrow \operatorname{join}\left(s^{t}, s_{\text {temp }}\right) \quad$ Join the selected fact or rule to the selection string. end whilereturn $s^{t}$} \in \text { rule_or_fact }} \operatorname{LLM}\left(\operatorname{token}\left|p, C^{t}, q, s^{t</p>
<h1>D.2. Inference module</h1>
<p>To extract the new fact to be added to the context we filter out the first sentence of the text generated by the LLM using the following regular expression: $r^{\prime}\left[{ }^{\wedge} . ?!\left\lfloor\right.\right.$ : $\left.\left.\backslash \mathrm{n}\right]+\right.$ '.</p>
<h2>D.3. bAbI</h2>
<p>For all bAbI tasks, the answer is a single word. For example, in bAbI 1-3 the answer is one of ["hallway", "bathroom", "bedroom", "garden", "kitchen", "office"]; for bAbI 15 the answer is one of ["sheep", "cat", "mouse", "wolf"] and for bAbI 16 the answer is one of ["yellow", "gray", "green", "white"]. However, our model outputs a complete sentence, for example "emily is afraid of mice". Therefore, we take the answer to be the final word output by the inference model on its last step.</p>
<p>To obtain the results for bAbI tasks 1-3, 15 and 16 shown in Fig. 4b we prompted the language model to solve the problem in a single step of reasoning. An example of such a prompt is shown in Sec. C.1.</p>
<p>We run the SI model for only a single step of reasoning too. Additional steps may increase the chance of the model reaching the correct answer, however, we do not yet have a mechanism for halting reasoning when the answer is reached.</p>
<p>BAbI 16 is an inductive reasoning task that could be solved in two steps (rather than one). The first step requires a rule to be inferred and the second step requires the inferred rule to be applied to another fact. For this reason, we also apply SI for two steps to solve the bAbI 16 problems, first inferring a rule from a number of facts and then applying the rule to the correct fact. An example of this is shown in Fig. 2. Using this two step approach, we can see exactly which facts contributed to the formation of a new rule.</p>
<h2>D.4. Proof Writer</h2>
<p>We use a subset of the Proof Writer Open World Assumption, OWA, dataset. In the Close World Assumption dataset, CWA, everything that can be proven is True otherwise it is False. This means things are either True or False. This also means that reasoning traces are only provided when a statement is True, but not when a statement is False. To "show" something is False one has to enumerate all possible facts (possibly up to a certain depth) and then if a statement is not shown to be True it is assumed to be False. It is therefore not simple to generate meaningful reasoning traces for these types of problems.</p>
<p>On the other hand, in the OWA data if it is not possible to prove something is True or False, then it is Unknown. This means that for True and False examples, where one may want to show $p(x)$, reasoning traces are available that terminate in $\mathrm{p}(\mathrm{x})$ (for True) or not $\mathrm{p}(\mathrm{x})$ (for False). If one cannot show $p(x)$ or not $p(x)$ then the answer is Unknown, and again there is not a clear reasoning trace for this; it is necessary to enumerate all possible facts (possibly up to a certain depth) and then if one has not shown $p(x)$ or not $p(x)$ it's considered Unknown. Note that here $p$ is a predicate and $x$ is a variable.</p>
<p>It is for this reason that we used the Proof Writer OWA dataset and removed the Unknowns; this gives us a dataset with reasoning traces concluding in either True or False. If we used CWA we would only have traces that could conclude True.</p>
<p>We evaluate the SI on 5 tasks from the Proof Writer dataset, each requiring varying numbers of reasoning steps ( $1,2,3$ and 5 ). This requires the model to learn to compute intermediate conclusions</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure S1 | Proof Writer: effect of additional reasoning steps. With additional iterations of selection and inference the probability of the model producing the correct answer increases.
that may not directly lead to the final answer, but may be needed to reach the final answer. While, this can be very hard to achieve using prompt engineering alone, we endeavour to do so, by demonstrating examples of intermediate and final steps of reasoning (for problems of depth $&gt;1$ ). This means that the language model sees (1) examples that both encourage the model to select rules and facts that may not answer the problem in one-step but may help the model to obtain an intermediate output that can be used in a future step and (2) examples of the final step, which takes the model to the final answer. See Sec. C. 2 for an example prompt.</p>
<p>The Proof Writer tasks involve predicting if a given statement, for example "Bob is nice.", is True or False given the context of facts and rules. Our SI model attempts to derive the statement "Bob is nice." or the negation of the statement "Bob is not nice." from the context. To assign a label True or False we follow the procedure proposed in the original Proof Writer paper (Tafjord et al., 2020) and test if any of the implications matches the given statement. If there is a match, the statement is considered to be True, otherwise False.</p>
<p>Proof Writer results in Fig. 4b show that the Selection-Inference model outperforms the baselines for problems of depth zero and one, however, with increasing depth, the gap between SI and the baselines diminishes. This is in part because prompt engineering is not sufficient to obtain an optimal Selection module.</p>
<p>Another challenge with the Proof Writer dataset is deciding how many arguments should be selected for each rule. In the Proof Writer dataset, some rules take one argument, others take two. We experimented with various different ways to encourage the model to stop selecting arguments. For example, we append ". Therefore, " as a choice to the context that the model can select. If the language model selects ". Therefore, " then the selection step ends. We allowed a maximum of two facts to be selected.</p>
<p>To obtain results in Fig. 4b we run SI model for the minimum number of steps needed to solve the problem; a Depth $d$ problem is run for $d$ steps, with the exception of the depth-0 problem which is run for 1 step. However, models may perform better when allowed to run for additional steps, in the case where the model makes a mistake on one step, but later recovers. Fig. S1 shows how the number of SI steps can lead to improved performance. There is greater improvement to performance</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ This could suggest that the 280B LLM has stronger priors, than the 7B LLM, which it favours over logical reasoning. For example, favouring sheep are afraid of wolves despite a context to the contrary (Min et al., 2022). However this requires further investigation.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
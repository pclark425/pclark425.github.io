<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3584 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3584</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3584</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-79.html">extraction-schema-79</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate or design novel chemicals for specific applications, including details of the model, the application, the generation method, evaluation metrics, results, and any reported limitations or challenges.</div>
                <p><strong>Paper ID:</strong> paper-316a011bf461d3a96965fb9f69398888da19bd9f</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/316a011bf461d3a96965fb9f69398888da19bd9f" target="_blank">SynerGPT: In-Context Learning for Personalized Drug Synergy Prediction and Drug Design</a></p>
                <p><strong>Paper Venue:</strong> bioRxiv</p>
                <p><strong>Paper TL;DR:</strong> A novel setting and models for in-context drug synergy learning are proposed, inspired by recent work that pre-trains a GPT language model (LM) to “in-context learn” common function classes, and novel pre-training schemes that enable a G PT model to in- context learn “drug synergy functions” are devised.</p>
                <p><strong>Paper Abstract:</strong> Predicting synergistic drug combinations can help accelerate discovery of cancer treatments, particularly therapies personalized to a patient’s specific tumor via biopsied cells. In this paper, we propose a novel setting and models for in-context drug synergy learning. We are given a small “personalized dataset” of 10-20 drug synergy relationships in the context of specific cancer cell targets. Our goal is to predict additional drug synergy relationships in that context. Inspired by recent work that pre-trains a GPT language model (LM) to “in-context learn” common function classes, we devise novel pre-training schemes that enable a GPT model to in-context learn “drug synergy functions”. Our model—which does not use any textual corpora, molecular fingerprints, protein interaction or any other domain-specific knowledge— is able to achieve competitive results. We further integrate our in-context approach with a genetic algorithm to optimize model prompts and select synergy candidates to test after conducting a patient biopsy. Finally, we explore a novel task of inverse drug design which can potentially enable the design of drugs that synergize specifically to target a given patient’s “personalized dataset”. Our findings can potentially have an important impact on precision cancer medicine, and also raise intriguing questions on non-textual pre-training for LMs.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3584.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3584.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate or design novel chemicals for specific applications, including details of the model, the application, the generation method, evaluation metrics, results, and any reported limitations or challenges.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SynerGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SynerGPT (in-context GPT-2 style model for synergy prediction and inverse design)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A decoder-only GPT-style model trained from scratch to perform in-context few-shot drug synergy prediction and to retrieve candidate molecular structures that match a desired synergy context for personalized cancer therapy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SynerGPT (GPT-2 family, decoder-only)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only GPT-2 family model trained from scratch for this work. Architecture used in experiments: 256-dimensional embeddings, 12 transformer layers, 4 attention heads; trained with batch size 64 and learning rate 2e-5 for the few-shot/ICL tasks. For inverse-design/retrieval the model's output for the [UNKNOWN] token is mapped via a linear head to a molecular representation space and trained with a contrastive minibatch loss.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Representation prediction + retrieval: SynerGPT predicts a continuous molecular representation (vector) for an unknown drug conditioned on in-context drug–drug–cell synergy tuples; retrieval of candidate molecules is done by cosine similarity between the predicted vector and pretrained molecular embeddings (MegaMolBARTv2) from a candidate pool. The paper also notes it would be trivial to swap retrieval for direct generation using a pretrained molecular generative model (Jin et al., 2020), but the implemented approach is retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Personalized drug discovery / inverse design of drugs predicted to be synergistic with a patient's drug/cell-context (cancer drug synergy inference and candidate retrieval).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>For retrieval: mean rank of the correct drug in the retrieved list as context size increases (reported decrease from ~1,500 to ~400 for 'seen' drugs as context increases). For synergy-context optimization (which improves downstream retrieval/usefulness) reported ROC-AUC for synergy prediction (context-optimized model: up to 81.5% ROC-AUC via genetic algorithm on unknown-drug split). Qualitative case studies (e.g., retrieved ground-truth molecule after 12 context examples) are also reported.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Implemented retrieval-based inverse design showed that as SynerGPT observes more contextual synergy examples, it increasingly retrieves structures closer to the ground-truth drug in multiple cases; mean rank for seen drugs fell from ~1,500 to ~400 as context increased. Qualitative examples include retrieving the exact ground-truth molecule (Adavosertib) after 12 optimized context examples. Retrieval performance is considerably stronger for drugs seen during training; for unknown drugs performance is better-than-random but remains low in absolute terms.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>No direct standard molecular-generation baselines were evaluated for the inverse-design retrieval task. For the related synergy-prediction tasks, SynerGPT's in-context approach outperforms or is competitive with established baselines (e.g., DeepDDS, DeepSynergy) in few-shot settings; however, those comparisons are for synergy prediction, not for molecule generation/retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Inverse-design performance is limited for unknown drugs (low absolute retrieval performance), retrieval is constrained to a fixed candidate pool (not full de novo generation), and synthesizability / practical chemical viability of retrieved candidates is not assessed. The method is a black-box deep model and retrieval only partially aids explainability. The authors note possible information limits in using only synergy tuples for molecule design and emphasize that retrieval doesn't fully address explainability or guarantee synthesizable/valid novel chemistry.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SynerGPT: In-Context Learning for Personalized Drug Synergy Prediction and Drug Design', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3584.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3584.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate or design novel chemicals for specific applications, including details of the model, the application, the generation method, evaluation metrics, results, and any reported limitations or challenges.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MegaMolBARTv2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MegaMolBARTv2 (NVIDIA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large pretrained molecular encoder trained on SMILES strings to produce molecular latent representations; used in this work as the target embedding space for retrieval-based inverse design.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Megamolbart v0.2</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MegaMolBARTv2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained molecular representation model (MegaMolBARTv2) trained on ~1.45 billion SMILES strings (cited as NVIDIA Corporation, 2022). Used as a fixed, high-capacity embedding space representing molecules.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Not used for generation here; used as a target embedding space for retrieval. SynerGPT predicts vectors which are matched to MegaMolBARTv2 embeddings of candidate molecules via cosine similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Molecular representation for retrieval-based inverse design of drugs in the context of drug synergy prediction (personalized cancer therapy candidate retrieval).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Cosine-similarity based retrieval ranking; contrastive minibatch loss during training (loss defined between predicted vectors and ground-truth MegaMolBARTv2 vectors). Downstream retrieval metric reported as mean rank of the true molecule in the candidate list as context is increased.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Using MegaMolBARTv2 as the retrieval target enabled SynerGPT to retrieve molecules closer to the ground truth as more synergy-context examples were provided; mean rank improvements and qualitative successful retrieval examples are reported. The pretrained embedding enabled practical nearest-neighbor retrieval over a large candidate pool.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>No explicit baseline molecular embedding models were compared in the inverse-design experiments reported in this paper; MegaMolBARTv2 was chosen because of coverage from large-scale SMILES pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Reliance on the coverage and quality of MegaMolBARTv2's latent space (if a true target molecule or class is not represented well in the pool or embedding space retrieval will fail). Using a fixed embedding pool limits discovery to molecules present in the pool; synthesizability or chemical validity beyond embedding similarity is not evaluated in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SynerGPT: In-Context Learning for Personalized Drug Synergy Prediction and Drug Design', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3584.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3584.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate or design novel chemicals for specific applications, including details of the model, the application, the generation method, evaluation metrics, results, and any reported limitations or challenges.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Jin2020 model (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hierarchical generation of molecular graphs using structural motifs (Jin et al., 2020)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hierarchical molecular graph generative model (cited as Jin et al., 2020) that can generate novel molecular graphs; cited in this paper as an example of a pretrained generative model that could be used in place of retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Hierarchical generation of molecular graphs using structural motifs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Hierarchical molecular-graph generative model (Jin et al., 2020)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A hierarchical generative model for molecular graphs (original paper introduces structural-motif based hierarchical generation). In the present paper it is only mentioned as a candidate generative model that could be substituted for retrieval; it was not trained or evaluated by the authors.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Mentioned as a pretrained generative model alternative for direct structure generation (hierarchical graph generation), but not applied in experiments in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Potential de novo molecular generation for drug design (mentioned in context of replacing retrieval with generative output for inverse design).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SynerGPT: In-Context Learning for Personalized Drug Synergy Prediction and Drug Design', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Hierarchical generation of molecular graphs using structural motifs <em>(Rating: 2)</em></li>
                <li>Megamolbart v0.2 <em>(Rating: 2)</em></li>
                <li>Chemcrow: Augmenting large-language models with chemistry tools <em>(Rating: 2)</em></li>
                <li>Is gpt-3 all you need for low-data discovery in chemistry? <em>(Rating: 2)</em></li>
                <li>Bayesian optimization of catalysts with in-context learning <em>(Rating: 2)</em></li>
                <li>Emergent autonomous scientific research capabilities of large language models <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3584",
    "paper_id": "paper-316a011bf461d3a96965fb9f69398888da19bd9f",
    "extraction_schema_id": "extraction-schema-79",
    "extracted_data": [
        {
            "name_short": "SynerGPT",
            "name_full": "SynerGPT (in-context GPT-2 style model for synergy prediction and inverse design)",
            "brief_description": "A decoder-only GPT-style model trained from scratch to perform in-context few-shot drug synergy prediction and to retrieve candidate molecular structures that match a desired synergy context for personalized cancer therapy.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "SynerGPT (GPT-2 family, decoder-only)",
            "model_description": "Decoder-only GPT-2 family model trained from scratch for this work. Architecture used in experiments: 256-dimensional embeddings, 12 transformer layers, 4 attention heads; trained with batch size 64 and learning rate 2e-5 for the few-shot/ICL tasks. For inverse-design/retrieval the model's output for the [UNKNOWN] token is mapped via a linear head to a molecular representation space and trained with a contrastive minibatch loss.",
            "generation_method": "Representation prediction + retrieval: SynerGPT predicts a continuous molecular representation (vector) for an unknown drug conditioned on in-context drug–drug–cell synergy tuples; retrieval of candidate molecules is done by cosine similarity between the predicted vector and pretrained molecular embeddings (MegaMolBARTv2) from a candidate pool. The paper also notes it would be trivial to swap retrieval for direct generation using a pretrained molecular generative model (Jin et al., 2020), but the implemented approach is retrieval.",
            "application_domain": "Personalized drug discovery / inverse design of drugs predicted to be synergistic with a patient's drug/cell-context (cancer drug synergy inference and candidate retrieval).",
            "evaluation_metrics": "For retrieval: mean rank of the correct drug in the retrieved list as context size increases (reported decrease from ~1,500 to ~400 for 'seen' drugs as context increases). For synergy-context optimization (which improves downstream retrieval/usefulness) reported ROC-AUC for synergy prediction (context-optimized model: up to 81.5% ROC-AUC via genetic algorithm on unknown-drug split). Qualitative case studies (e.g., retrieved ground-truth molecule after 12 context examples) are also reported.",
            "results_summary": "Implemented retrieval-based inverse design showed that as SynerGPT observes more contextual synergy examples, it increasingly retrieves structures closer to the ground-truth drug in multiple cases; mean rank for seen drugs fell from ~1,500 to ~400 as context increased. Qualitative examples include retrieving the exact ground-truth molecule (Adavosertib) after 12 optimized context examples. Retrieval performance is considerably stronger for drugs seen during training; for unknown drugs performance is better-than-random but remains low in absolute terms.",
            "comparison_to_baselines": "No direct standard molecular-generation baselines were evaluated for the inverse-design retrieval task. For the related synergy-prediction tasks, SynerGPT's in-context approach outperforms or is competitive with established baselines (e.g., DeepDDS, DeepSynergy) in few-shot settings; however, those comparisons are for synergy prediction, not for molecule generation/retrieval.",
            "limitations_challenges": "Inverse-design performance is limited for unknown drugs (low absolute retrieval performance), retrieval is constrained to a fixed candidate pool (not full de novo generation), and synthesizability / practical chemical viability of retrieved candidates is not assessed. The method is a black-box deep model and retrieval only partially aids explainability. The authors note possible information limits in using only synergy tuples for molecule design and emphasize that retrieval doesn't fully address explainability or guarantee synthesizable/valid novel chemistry.",
            "uuid": "e3584.0",
            "source_info": {
                "paper_title": "SynerGPT: In-Context Learning for Personalized Drug Synergy Prediction and Drug Design",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "MegaMolBARTv2",
            "name_full": "MegaMolBARTv2 (NVIDIA)",
            "brief_description": "A large pretrained molecular encoder trained on SMILES strings to produce molecular latent representations; used in this work as the target embedding space for retrieval-based inverse design.",
            "citation_title": "Megamolbart v0.2",
            "mention_or_use": "use",
            "model_name": "MegaMolBARTv2",
            "model_description": "Pretrained molecular representation model (MegaMolBARTv2) trained on ~1.45 billion SMILES strings (cited as NVIDIA Corporation, 2022). Used as a fixed, high-capacity embedding space representing molecules.",
            "generation_method": "Not used for generation here; used as a target embedding space for retrieval. SynerGPT predicts vectors which are matched to MegaMolBARTv2 embeddings of candidate molecules via cosine similarity.",
            "application_domain": "Molecular representation for retrieval-based inverse design of drugs in the context of drug synergy prediction (personalized cancer therapy candidate retrieval).",
            "evaluation_metrics": "Cosine-similarity based retrieval ranking; contrastive minibatch loss during training (loss defined between predicted vectors and ground-truth MegaMolBARTv2 vectors). Downstream retrieval metric reported as mean rank of the true molecule in the candidate list as context is increased.",
            "results_summary": "Using MegaMolBARTv2 as the retrieval target enabled SynerGPT to retrieve molecules closer to the ground truth as more synergy-context examples were provided; mean rank improvements and qualitative successful retrieval examples are reported. The pretrained embedding enabled practical nearest-neighbor retrieval over a large candidate pool.",
            "comparison_to_baselines": "No explicit baseline molecular embedding models were compared in the inverse-design experiments reported in this paper; MegaMolBARTv2 was chosen because of coverage from large-scale SMILES pretraining.",
            "limitations_challenges": "Reliance on the coverage and quality of MegaMolBARTv2's latent space (if a true target molecule or class is not represented well in the pool or embedding space retrieval will fail). Using a fixed embedding pool limits discovery to molecules present in the pool; synthesizability or chemical validity beyond embedding similarity is not evaluated in this work.",
            "uuid": "e3584.1",
            "source_info": {
                "paper_title": "SynerGPT: In-Context Learning for Personalized Drug Synergy Prediction and Drug Design",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Jin2020 model (mentioned)",
            "name_full": "Hierarchical generation of molecular graphs using structural motifs (Jin et al., 2020)",
            "brief_description": "A hierarchical molecular graph generative model (cited as Jin et al., 2020) that can generate novel molecular graphs; cited in this paper as an example of a pretrained generative model that could be used in place of retrieval.",
            "citation_title": "Hierarchical generation of molecular graphs using structural motifs",
            "mention_or_use": "mention",
            "model_name": "Hierarchical molecular-graph generative model (Jin et al., 2020)",
            "model_description": "A hierarchical generative model for molecular graphs (original paper introduces structural-motif based hierarchical generation). In the present paper it is only mentioned as a candidate generative model that could be substituted for retrieval; it was not trained or evaluated by the authors.",
            "generation_method": "Mentioned as a pretrained generative model alternative for direct structure generation (hierarchical graph generation), but not applied in experiments in this paper.",
            "application_domain": "Potential de novo molecular generation for drug design (mentioned in context of replacing retrieval with generative output for inverse design).",
            "evaluation_metrics": null,
            "results_summary": null,
            "comparison_to_baselines": null,
            "limitations_challenges": null,
            "uuid": "e3584.2",
            "source_info": {
                "paper_title": "SynerGPT: In-Context Learning for Personalized Drug Synergy Prediction and Drug Design",
                "publication_date_yy_mm": "2023-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Hierarchical generation of molecular graphs using structural motifs",
            "rating": 2
        },
        {
            "paper_title": "Megamolbart v0.2",
            "rating": 2
        },
        {
            "paper_title": "Chemcrow: Augmenting large-language models with chemistry tools",
            "rating": 2
        },
        {
            "paper_title": "Is gpt-3 all you need for low-data discovery in chemistry?",
            "rating": 2
        },
        {
            "paper_title": "Bayesian optimization of catalysts with in-context learning",
            "rating": 2
        },
        {
            "paper_title": "Emergent autonomous scientific research capabilities of large language models",
            "rating": 2
        }
    ],
    "cost": 0.014379,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>SynerGPT: In-Context Learning for Personalized Drug Synergy Prediction and Drug Design</h1>
<p>Carl Edwards<br>Department of Computer Science<br>University of Illinois Urbana-Champaign<br>cne2@illinois.edu</p>
<p>Tushar Khot<br>Allen Institute for Artificial Intelligence<br>tushark@allenai.org</p>
<p>Heng Ji<br>Department of Computer Science<br>University of Illinois Urbana-Champaign<br>hengji@illinois.edu</p>
<p>Aakanksha Naik<br>Allen Institute for Artificial Intelligence<br>aakankshan@allenai.org<br>Martin D. Burke<br>Department of Chemistry<br>University of Illinois Urbana-Champaign<br>mdburke@illinois.edu<br>Tom Hope<br>The Hebrew University of Jerusalem<br>Allen Institute for Artificial Intelligence<br>tomh@allenai.org</p>
<h4>Abstract</h4>
<p>Predicting synergistic drug combinations can help accelerate discovery of cancer treatments, particularly therapies personalized to a patient's specific tumor via biopsied cells. In this paper, we propose a novel setting and models for in-context drug synergy learning. We are given a small "personalized dataset" of 10-20 drug synergy relationships in the context of specific cancer cell targets. Our goal is to predict additional drug synergy relationships in that context. Inspired by recent work that pre-trains a GPT language model (LM) to "in-context learn" common function classes, we devise novel pre-training schemes that enable a GPT model to in-context learn "drug synergy functions". Our model-which does not use any textual corpora, molecular fingerprints, protein interaction or any other domainspecific knowledge- is able to achieve competitive results. We further integrate our in-context approach with a genetic algorithm to optimize model prompts and select synergy candidates to test after conducting a patient biopsy. Finally, we explore a novel task of inverse drug design which can potentially enable the design of drugs that synergize specifically to target a given patient's "personalized dataset". Our findings can potentially have an important impact on precision cancer medicine, and also raise intriguing questions on non-textual pre-training for LMs. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>Drug combination therapy is a standard practice for diseases including cancer (Mokhtari et al., 2017) and HIV. It is based on identifying multiple single agent therapies that, when used together, lead to synergistic effects. Predicting such combinatorial synergies is challenging, especially given the wide range of multiple different mutations as well as different genetic backgrounds typically found in different patients' cancer cells (Mroz \&amp; Rocco, 2017). Many drug combinations can also cause increased toxicity (Zapata et al., 2020; Juurlink et al., 2004) in a manner that may depend on specific patient backgrounds (O’Donnell \&amp; Dolan, 2009), adding further complexity to the problem. To enable the safest and most effective implementation of combination therapy in cancer care, it is thus important to personalize the prediction of drug synergies.</p>
<p>Since the number of drug combinations scales exponentially, differentiating between synergistic and antagonistic pairings is very expensive to test in large quantities in laboratory conditions. Thus,</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>considerable interest has recently grown in using machine learning for predicting synergistic and antagonistic effects between pairs of drugs in silico (Liu et al., 2020; Preuer et al., 2018; Rozemberczki et al., 2022a). These approaches are typically not evaluated in the few-shot setting, where only a few training examples are given. This is particularly relevant in the personalized setting described above, and more generally for cancer tissue types for which there is limited training data for synergy learning models. Additionally, these efforts use a variety of features to categorize the drugs, from molecular fingerprints (Preuer et al., 2018) to protein interactions (Yang et al., 2021). Obtaining these features often requires integrating external knowledge sources (e.g., from drug databases), which often results in findings being restricted to the limited subsets of drugs for which this information is available and also requires specialized engineering in model design. Finally, it is unclear if these external sources are actually needed for current models.</p>
<p>In this work, we address these limitations by exploring the ability of transformer language models (LMs) to learn drug synergy relations. We devise approaches that leverage transformers (1) without any external knowledge required to be integrated into the model (i.e., no protein interaction networks or patient cell line features); (2) in the few-shot setting with an in-context learning approach that can generalize to novel unseen drugs and patient cell lines; and (3) for designing novel synergistic drug structures in the context of a specific patient's data.</p>
<p>Transformer LMs are Strong Drug Synergy Learners—Even Without Textual Representations First, we consider drug synergy prediction using transformer language models without enriching drugs/cells with information from external knowledge bases. We find these "feature-less" models are able to achieve results that are better or competitive in comparison to knowledge-enhanced state-ofart drug synergy models (e.g., BERT models achieve $84.1 \%$ ROC-AUC to GraphSynergy's $83.4 \%$ ) Furthermore, in contrast to recent work that uses language models pre-trained on scientific corpora (Nadkarni et al., 2021), we discover an intriguing finding: using randomized (i.e. uninformative) tokens instead of drug/cell names is able to rival models that use textual names of those entities. This suggests that external information coming from pre-training on scientific corpora (e.g., as in SciBERT (Beltagy et al., 2019)) or the web (e.g., Wikipedia) has negligible impact on fine tuned models in this setting. These findings motivate us to explore the power of transformer models without external information, and to study generalization beyond memorization capacity by evaluating on novel drugs/cells that were unseen during training.</p>
<p>SynerGPT: A New In-Context Drug Synergy Setting \&amp; Model We take inspiration from recent work (Garg et al., 2022) that showed how a GPT model architecture can be trained to "in-context learn" function classes such as linear functions (e.g., linear regression/classification) and neural networks. We pre-train a GPT model from scratch on known drug synergies-using no textual corpora-and explore its ability to generalize in the few-shot setting to drugs and patient cell lines unseen during training. We find that our model, dubbed SynerGPT, is able to achieve strong competitive results without any external knowledge sources. In particular, we introduce a new setting of In-Context Learning for Drug Synergy (ICL-DS). In-Context Learning (ICL) (Dong et al., 2022) has emerged as a powerful paradigm for few-shot learning (Brown et al., 2020). In ICL, trained model parameters are never explicitly updated after pre-training, and adaptation to each task is done on the fly given contextual examples. This is particularly appealing in settings where it is prohibitively costly to perform parameter updates for each incoming new task and context (e.g., for each new patient in a hospital setting). We devise novel pre-training approaches for ICL-DS, including strategies for optimizing the language model prompt selection with a genetic algorithm. Prompts comprise specific combinations of drugs tested for synergy on specific patient cell lines; optimizing prompt selection in this setting has potential implications for the design of a standardized assay panel of drugs and cells to be tested for a patient's particular tumor. While specific patient data at this level is not readily available, we re-purpose existing drug combination data to lay the foundations for formalizing and studying our approaches from a machine learning perspective.</p>
<p>Designing New Molecules to be Synergistic in the Context of a Specific Patient Finally, in our third major contribution we propose an additional new task of Inverse Synergistic Drug Structure Design (ISDSD): using a GPT transformer model for generating or retrieving drug molecules that are synergistic in the context of a specific cancer patient's information (i.e., molecules that are synergistic with other drugs administered to a patient with specific cancer cells). This approach may in the future provide a new methodology for personalized drug candidate discovery.</p>
<h1>2 Background and Problem Setting</h1>
<p>In the last few decades, combination therapy has emerged as an effective method to target genetically unstable diseases (Mokhtari et al., 2017), with dramatic success in treating HIV (Moore \&amp; Chaisson, 1999) and more recently HCV(Liang \&amp; Ghany, 2013). Unlike HIV and HCV which encode only 10-15 proteins (Frankel \&amp; Young, 1998; Dubuisson, 2007), cancer is radically more complex. Since cancer has an unstable genome, combination therapy is often considered necessary (Mokhtari et al., 2017) and is commonly used in practice, with varying degrees of success.</p>
<p>Generally, drugs work by affecting cellular pathways-chain interactions of molecules which lead to changes in a cell. In drug synergy prediction, our goal is to predict whether combining drugs will have positive or negative outcomes in the complex system of these interacting pathways. Generally, synergy lab experiments are conducted in cell lines, which are a population of cells from a multi-cellular organism (for example, human lung cancer cells). In this work, we also investigate inverse design of drug molecules. Traditionally, the idea behind inverse design of molecules is to predict or retrieve a molecular structure which has some desired chemical property or protein target (Sanchez-Lengeling \&amp; Aspuru-Guzik, 2018). In our work, we seek to explore inverse design at a higher level- the "interactome" of drug interactions in complex cellular pathways.</p>
<p>General Problem Formulation Given $k$ input drugs $d^{1}, d^{2}, \ldots, d^{k} \in \mathcal{D}$ along with a cell line $c \in \mathcal{C}$, the goal of drug synergy prediction is to predict a synergy value $y$ for the interactions between the drugs in the given cell line. In existing datasets, only the pairwise $k=2$ setting is considered. Thus, we focus our experiments on pairwise drug synergy, the most commonly researched setting, but our methods can naturally be extended to n-ary synergies. This problem can be considered as either a regression $(y \in \mathbb{R})$ or a binary classification problem (synergistic (True) or not (False); $y \in[0,1]$ ). Synergy data comes from a dataset of tuples $\left(d^{1}, d^{2}, c, y\right) \in \mathfrak{D}$.</p>
<p>Few-Shot In-Context Setting We also consider the few-shot setting in our formulation, which has applications for predicting synergies when there is scarce training data such as in tumor-specific synergy prediction, uncommon cancer tissues, or newly introduced single-agent therapies. In the few-shot setting, we assume there are $n$ synergy tuples available which contain an unknown entity $h$ (unknown cell line $c^{h}$ or unknown drug $d^{h}$ ). Define these tuples as $x_{i}:=\left(d^{1}, d^{2}, c, y\right)$; for $i \in[1 . . n]$ where one of $d^{1}, d^{2}$, or $c$ is the unknown $h$. Each $x_{i}$ can then be used for training in addition to the existing training data. In our proposed method SynerGPT, we don't use these tuples $x_{i}$ in trainingrather, we use them as the prompt for in-context learning. Here, we are particularly interested in synergy prediction based on extremely small datasets (e.g. tested synergies from a patient's specific cancer cells), which makes traditional supervised approaches less effective. In section 3.2.3, we detail our training strategies for in-context learning with unknown $h$ from limited examples.</p>
<p>Inverse Drug Design from Drug Synergy Context We propose a new task where the goal is to predict the structure of a molecule given a context of drug synergy tuples (e.g., we might be given 20 synergy tuples). We train a model to predict the structure of some unknown drug $d^{h}$ from its synergy relations with other drugs. This has two important uses. First, this may enable scientists to predict new molecules which have desirable synergies or similar synergies to existing drugs, which is a novel way to consider drug discovery. This can potentially enable the design of drugs that synergize specifically to target a given patient's unique cancer cells. Secondly, this can support explainability of the synergy prediction model as a function of the context it is fed, by "visualizing" SynerGPT's understanding of the unknown drug given the context. Section 4.3 shows that we can observe the structure of the drug evolving towards the ground truth as more context examples are given. As this is a novel and difficult problem; we initially frame it as a retrieval task, effectively constraining the output space, though from an implementation perspective it is trivial to instead predict structures by using a pretrained generative model for molecules (Jin et al., 2020) with no architectural differences, as both the retrieval and generation of drug structures requires generating a latent vector.</p>
<h2>3 Methodology</h2>
<p>In this section, we will consider the four components of our paper. First, we detail how drug synergy tuples are input to encoder-only language models (§ 3.1). Next, we extend this idea to the few-shot</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Example of our prompt selection strategy (steps shown in black box). After training (step 0 ), we are given as input (step 1) a combination between three entities: a known drug Allopurinol, unknown drug Vandetanib, and a known patient cell line 786-0. We are given a small synergy graph $\mathcal{G}$ (bottom right). Nodes represent (drug, drug, cell line) tuples with synergy labels (from previous experiments). Edges represent shared entities; edge color indicates which entities (e.g. red, for sharing Mitotane ). Using different strategies, we adaptively select contextual examples (step 2) for in-context learning (step 3).</p>
<p>ICL setting and propose training methodologies to do so (§ 3.2). We then discuss optimization of the "prompt" used for ICL (§ 3.2.3). Finally, we extend our methodology to inverse drug design (§ 3.3).</p>
<h1>3.1 Input for EnCODer-only Language MODELs</h1>
<p>Initially, we explore the efficacy of BERT-style language models (Devlin et al., 2019; Beltagy et al., 2019; Yasunaga et al., 2022) for drug synergy prediction. We modify the task input to be in natural language using a simple formulation:</p>
<p>$$
\left[\text { CLS] } d^{1}[\mathrm{SEP}] d^{2}[\mathrm{SEP}] c[\mathrm{SEP}]\right.
$$</p>
<p>where $d^{1}$ and $d^{2}$ are drug names (e.g., imatinib, $5-F U$ ), and $c$ is the name of a cell line (e.g., $M C F 2$, Ishikawa). The model is then trained to predict the output value $y$ from the [CLS] token representation.</p>
<p>We also investigate to what extent pretraining knowledge is responsible for the model's performance. To do so, we evaluate the impact on performance when the drug and cell names are replaced with 'random' tokens. Given the ordered (by frequency) vocabulary $\mathcal{V}$ of the LM, we select the tokens $\left{v_{i} \in \mathcal{V} \mid i \in\left[k_{-}\left(k+|\mathcal{C}|+|\mathcal{D}|\right)\right]\right}$ to represent our drug and cell lines. Note we start at a threshold $k$ to avoid the most common tokens which might have specialized representations in the language model's latent space. We uniquely map each cell line and drug to a token in this set, which we use as input to the BERT LM. Essentially, this experiment is used to determine whether knowledge from pretraining or the transformer architecture itself is responsible for performance on the drug synergy task. An example input from this strategy is: [CLS] rabbit [SEP] fish [SEP] book [SEP].</p>
<h3>3.2 SynerGPT: In-Context Learning for Few-Shot Synergy Prediction</h3>
<h3>3.2.1 In-Context Learning for Function Classes: Background</h3>
<p>Recent work trained transformer models to "in-context learn" function classes (Garg et al., 2022). A function class is a set of functions that satisfy specific properties, such as linear functions or neural networks. In-context learning of a function class $\mathcal{F}$ is defined as being able to approximate $f\left(x_{\text {query }}\right)$ for "most" functions $f \in \mathcal{F}$ given a new query $x_{\text {query }}$ when conditioned on a prompt sequence $\left(x_{1}, f\left(x_{1}\right), \ldots, x_{n}, f\left(x_{n}\right), x_{\text {query }}\right)$. We define a prompt prefix $P^{n}:=\left(x_{1}, f\left(x_{1}\right), \ldots, x_{n}, f\left(x_{n}\right), x_{n+1}\right)$ as the</p>
<p>first $n$ in-context examples followed by the $n+1$ th input. A model $M_{\theta}$ parameterized by $\theta$ is trained to minimize the loss averaged over all prefixes</p>
<p>$$
\min <em i="0">{\theta} \mathbb{E}\left[\frac{1}{n+1} \sum</em>\right)\right)\right]
$$}^{n} w_{i} \ell\left(M_{\theta}\left(P^{i}\right), f\left(x_{i+1</p>
<p>given some appropriate loss function $\ell$. Weights $w_{n}:=1$ unless otherwise noted.</p>
<h1>3.2.2 Predicting Drug Synergy In-Context</h1>
<p>For in-context prediction of drug synergy, we redefine</p>
<p>$$
P^{n}=\left(d_{1}^{1}, d_{1}^{2}, c_{1}, y_{1}, \ldots, d_{n}^{1}, d_{n}^{2}, c_{n}, y_{n}, d_{n+1}^{1}, d_{n+1}^{2}, c_{n+1}\right)
$$</p>
<p>as the prompt prefix (as discussed in Section 2, we refer to this as the "context" or "input context"). Here, $y$ can be considered the output of a function measuring synergy on $\left(d^{1}, d^{2}, c\right)$. As in (Garg et al., 2022), we consider a GPT-2 family (Radford et al., 2019) decoder-only model, which we call SynerGPT. Here, the prediction of the synergy value $y_{j}$ is made using a linear transformation of the contextualized output representation of $c_{j}$ (note that this includes $d_{j}^{1}$ and $d_{j}^{2}$ due to self-attention). Model inputs-drugs $d$, cell lines $c$, and labels $y$-are initialized using a learnable embedding layer (i.e. no external features). To evaluate the model's ability to predict synergies of unknown entities, we hold out either $m$ drugs or $m$ cell lines and remove their synergy relations from the training set (see Section 4). We use a subset of the held out tuples as a pool of context examples. We now turn to selecting the context (prompt prefix) from this pool in a manner that increases predictive performance.</p>
<h3>3.2.3 How to sample the context?</h3>
<p>A central question about using language models without external features-including textual namesis how to teach the model to understand unknown drugs or cell lines. We propose using a masking strategy—every unknown drug $d^{h}$ or cell $c^{h}$ is represented by [UNKNOWN] and the model must use in-context learning to understand it based on contextually-related known drugs and cell lines. In this setting, we assume that we are given a set of synergy tuples to sample from to construct a prompt. During training, it's simply the training set. During evaluation, we consider a special held-out "context" set $\mathfrak{D}^{c} \subset \mathfrak{D}$ (thus named because we sample the context/prompt $P^{n}$ from this set). To sample from this context set, we propose a context-selection strategy based on constructing a graph $\mathcal{G}$ on this $\mathfrak{D}^{c}$. Specifically, we construct $\mathcal{G}$ by creating a node for every synergy tuple $x:=\left(d^{1}, d^{2}, c, y\right) \in \mathfrak{D}^{c}$. We construct a drug edge $e^{d}$ between two nodes $x_{1}$ and $x_{2}$ if they share drug $d$ (i.e. $d \in x_{1} \wedge d \in x_{2}$ ). Similarly, we construct a cell line edge $e^{c}$ if they share cell line $c$. See Figure 1 for an example and Appendix Figure 8 for more details. We employ the following context selection strategies to sample a context with $n$ examples given some node $x$ containing unknown $h$ which is either drug $d^{h}$ or cell $c^{h}$ :</p>
<ol>
<li>Random: Uniformly select $n$ context examples from $\mathfrak{D}^{c}$.</li>
<li>Graph: Uniformly select examples from the nodes adjacent to $x$ in $\mathcal{G}$.</li>
<li>Unknown-First: Uniformly select nodes adjacent to $x$ which share an edge of type $e^{h}$, i.e. prioritizing selection of nodes that contain the masked unknown $h$.</li>
</ol>
<p>Note that these strategies are hierarchical-Unknown-First falls back to Graph when there aren't enough examples which falls back to Random. Examples from Random are put earlier in the context than Graph which is again put before Unknown-First. In order to train the model to correctly use the [UNKNOWN] token, we need to artificially create unknown drugs or cells during training. Given training example $x$, we uniformly select $d^{1} \in x$ or $d^{2} \in x$ to be the hidden drug $d^{h}$. For the unknown cell line setting, $c \in x$ is always set to $c^{h}$ because there is just one cell line per example. We replace all occurrences of $h$ in the prompt with [UNKNOWN]. We note that our sampling strategy is related to retrieval augmented models (Mialon et al., 2023). Here, however, we note that the model is also in-context learning synergy functions for unknown drugs based on Definition 1 in (Garg et al., 2022).</p>
<h3>3.2.4 Optimizing the Context</h3>
<p>We further study whether the context can be optimized to best enable predictions for some unknown drug or cell line $h$ (see Figure 7 for an example). The purpose of these experiments is to enable the</p>
<p>eventual development of a standardized assay for drug synergy prediction. Thus, as output, these optimization algorithms produce a set of context tuples for each $h$. To do this optimization, we assume that we have four splits of data, which are constructed as follows. Given a set of $p$ "unknown" drugs/cells $H$, all synergy tuples not containing any $h \in H$ are put into a training set $\mathfrak{D}^{T r}$. The remaining tuples are randomly partitioned into three equal sized sets: a context bank $\mathfrak{D}^{c}$, a validation set $\mathfrak{D}^{v}$, and a test set $\mathfrak{D}^{T e}$. We first train a model on $\mathfrak{D}^{T r}$ following the Unknown-First strategy (where contexts are sampled from $\mathfrak{D}^{T r}$ itself). Following this, for each unknown entity $h_{i}$, we select $n$ context examples from $\mathfrak{D}^{c}$ which maximize the model's score on the validation set $\mathfrak{D}^{v}$. This is a combinatorial optimization problem which can be considered related to the best subset selection problem (Bertsimas et al., 2015; Miller, 2002). We consider a genetic algorithm (Gad, 2021): a metaheuristic method which is useful for black box optimization of systems containing complex interacting parts (Mitchell et al., 2007), which is suitable for the complex interactions between cellular pathways required for drug synergy prediction. As output, we get a set of context tuples for each $h$. Optimization algorithm details are given in Appendix B.</p>
<h1>3.3 In-Context Learning for Inverse Design</h1>
<p>To train the model to retrieve relevant drug structures in-context, we use the same architecture as for synergy prediction (§ 3.2.2), so that we can use the same data split and optimized contexts from Section 3.2.4 to understand how the model interprets them. For effective retrieval, we need a strong base molecular representation that makes it possible to effectively distinguish molecules. So, we choose to use MegaMolBARTv2 (NVIDIA Corporation, 2022) representations, which were trained on 1.45 billion molecular SMILES strings and thus have a relatively comprehensive (in terms of drug classes) latent space. We train a SynerGPT model from scratch to predict representations using a linear transformation on the output [UNKNOWN] representation. We use this final representation to retrieve the desired drug using cosine similarity with the MegaMolBARTv2 representations of the drugs in our synergy dataset. The training context is selected using the Unknown-First strategy. Finally, we train the model using a minibatch contrastive loss (Radford et al., 2021; Edwards et al., 2021) between the L2-normalized ground truth representations $D^{g}$ (here MegaMolBartv2) and predicted representations $D^{p}$ (output from our model's prediction head):</p>
<p>$$
\ell\left(D^{g}, D^{p}\right)=C E\left(e^{\tau} D^{g} D^{p T}, I_{b}\right)+C E\left(e^{\tau} D^{p} D^{g T}, I_{b}\right)
$$</p>
<p>where $C E$ is categorical cross-entropy loss, $b$ is the mini-batch size, $I_{b}$ is the identity matrix, and $\tau$ is a learnable temperature parameter. We use this loss for $\ell$ in equation 1.</p>
<h2>4 Results</h2>
<p>BERT can do Drug Synergy? In this section, we experiment with finetuning BERT on drug synergy data where all drugs and cell lines are seen during training (data splits detailed in Appendix A.1). As discussed earlier, there has been recent work using external network datasets capturing interactions between drugs, proteins and cell lines (Yang et al., 2021) for synergy prediction.</p>
<p>To evaluate the impact of these external datasets, we compare against a strong and recent model, Graphsynergy (Yang et al., 2021) that uses over a dozen different network datasets and achieves state-of-the-art on its subset of DrugCombDB.
We train four BERT-based (Devlin et al., 2019) language models (Beltagy et al., 2019; Yasunaga et al., 2022) and find that they outperform GraphSynergy in both name and random token settings. BioLinkBERT with random tokens, for example, achieves a</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">KB</th>
<th style="text-align: center;">Name</th>
<th style="text-align: center;">ROC-AUC</th>
<th style="text-align: center;">PR-AUC</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">DeepSynergy</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">84.3</td>
<td style="text-align: center;">70.4</td>
</tr>
<tr>
<td style="text-align: left;">MR-GNN</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">77.9</td>
<td style="text-align: center;">62.6</td>
</tr>
<tr>
<td style="text-align: left;">SSI-DDI</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">63.3</td>
<td style="text-align: center;">41.4</td>
</tr>
<tr>
<td style="text-align: left;">DeepDDS</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">87.2</td>
<td style="text-align: center;">77.0</td>
</tr>
<tr>
<td style="text-align: left;">SciBERT (random)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">86.9</td>
<td style="text-align: center;">76.3</td>
</tr>
<tr>
<td style="text-align: left;">BioLinkBERT (names)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">86.4</td>
<td style="text-align: center;">75.9</td>
</tr>
</tbody>
</table>
<p>Table 1: Classification results for four selected ChemicalX (Rozemberczki et al., 2022b) baselines and BERT on DrugCombDB (Liu et al., 2020). SciBERT and BioLinkBERT take random token and names as input, respectively. Values are average of five runs. Notably, SciBERT (random) outperforms four of the other five baselines. KB means external knowledge is used.</p>
<p>ROC-AUC score of $84.1 \%$ compared to GraphSynergy's $83.4 \%$ (p $&lt;0.05$ using paired $t$-test). In comparison, BioLinkBERT with drug names as input achieves $83.6 \%$. We checked multiple BERT configurations, and details on other BERT models are shown in Appendix A. 1 Table 4.</p>
<p>A natural question here is whether the model has learnt the required knowledge during pre-training. Surprisingly, replacing drug and cell names with random tokens (§ 3.1) resulted in no drop in performance. This suggests that the transformer architecture may be the dominant factor explaining BERT's performance on the task. However, if we use a randomly-initialized BERT model without any pre-training, we find the performance is worse (by 3 ROC-AUC pts).</p>
<p>To verify our findings, we consider the ChemicalX framework (Rozemberczki et al., 2022b), which implements several baselines and provides a standardized subset of DrugCombDB (Liu et al., 2020) with drug and cell line features. This standardization allows us to compare different baseline methodologies on the same dataset.The ChemicalX DrugCombDB dataset has 2,956 drugs, 112 cell lines, and 191,391 synergy tuples. We compare against baselines DeepSynergy (Preuer et al., 2018), MR-GNN (Xu et al., 2019), SSI-DDI (Nyamabo et al., 2021), and DeepDDS (Wang et al., 2022), which we train using default hyperparameters from the original papers for 50 epochs as in (Rozemberczki et al., 2022b). These baselines (details in Appendix A.2) represent the most popular approaches to drug synergy prediction and allow us to compare against transformer architecture performance. Remarkably, SciBERT with random tokens outperforms all baselines except DeepDDS in this setting (Table 1). We see similar results on the DrugComb dataset (this database is larger but is continuously modified by volunteers; see Appendix J). We note that, while this performance is surprising in this domain, it follows from results from other domains. For example, language models are able to learn complex grammar and interactions just by observing how words co-occur. We conjecture this may be related to the observation that pre-training on a nonsense corpus (Krishna et al., 2021) can provide good weight initializiations for downstream tasks. We further discuss related work in Section 5.</p>
<h1>4.1 In-Context Learning for Few-Shot Drug Synergy</h1>
<p>We now evaluate models on the few-shot and zero-shot setting, i.e, when a new drug or cell line is introduced with limited or no interaction data. We use the same architecture used in Garg et al. (2022): a GPT-2 (Radford et al., 2019) model with 256-dimensional embeddings, 12 layers, 4 attention heads, and batch size of 64 . We use a learning rate of $2 \mathrm{e}-5$. Model weights are initialized from scratch. To enable efficient experimentation in the few-shot setting, we construct a dataset split which contains multiple unknowns (i.e. $m$ held-out drugs or cells: $H:=\left{h_{i} \mid i \in[1 . . m]\right}$ ). To construct our split, we remove all synergy tuples containing $h \in H$ from the dataset $\mathbb{D}$ so that the remaining dataset only contains tuples with known drugs/cells (this is our training set $\mathbb{D}^{T r}$ ). Then, for each $h$, we select $n$ synergy tuples randomly to form the "context" bank/split $\mathbb{D}^{\prime}$. All other "unknown" synergy tuples are put into $\mathbb{D}^{T r}$.</p>
<p>For comparison, we use the same baselines trained in zero-shot and few-shot settings. We also test SetFit (Tunstall et al., 2022) (a few-shot LM approach), k-nearest neighbors, off-the-shelf pre-trained GPT-2 (using entity names as input, similar to CancerGPT (Li et al., 2023a)), and MAML with DeepDDS (details in Appendix A.2). In the few-shot setting, the context bank $\mathbb{D}^{\prime}$ is considered part of the training set, and in the zero-shot setting it is not used. Our model, SynerGPT, however, is not trained on the context bank but uses it as context (prompt) examples for evaluation. Examples are selected using the Random, Graph, or Unknown-First strategies. We separately investigate the setting where drugs are unknown and where cell lines are unknown.</p>
<p>Unknown Drugs To construct the dataset split, we set $m=50$ unknown, i.e.,"held-out" drugs and context $n=20$ synergy tuples. Hence, our context bank contains $50 \times 20=1,000$ tuples. Overall, we find that our SynerGPT can perform better in the few-shot setting than existing baselines on on this task, as shown in Table 2. Full results are in Appendix Table 6. SynerGPT is trained in the zero-shot setting, which means it can be evaluated both with context examples (few-shot) and without any examples (zero-shot). Each strategy performs roughly the same zero-shot (although since strategies are used in training there are small differences), but the performance with sampled context examples is much different. Without examples, SynerGPT performs worse than DeepDDS few-shot, but the same SynerGPT model outperforms DeepDDS when given the few-shot context. Overall, we outperform all prior models in the few-shot setting and zero-shot setting. In particular, Unknown-First is able to increase performance by $3.8 \%$ absolute ROC-AUC with context, whereas</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: center;">Unknown Drug</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Unknown Cell Line</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Mode</td>
<td style="text-align: left;">Model</td>
<td style="text-align: center;">ROC-AUC</td>
<td style="text-align: center;">PR-AUC</td>
<td style="text-align: center;">ROC-AUC</td>
<td style="text-align: center;">PR-AUC</td>
</tr>
<tr>
<td style="text-align: left;">Zero-Shot</td>
<td style="text-align: left;">DeepSynergy</td>
<td style="text-align: center;">67.5</td>
<td style="text-align: center;">47.7</td>
<td style="text-align: center;">78.6</td>
<td style="text-align: center;">63.6</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">DeepDDS</td>
<td style="text-align: center;">72.1</td>
<td style="text-align: center;">53.2</td>
<td style="text-align: center;">74.5</td>
<td style="text-align: center;">59.8</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">SciBERT (random)</td>
<td style="text-align: center;">67.7</td>
<td style="text-align: center;">47.4</td>
<td style="text-align: center;">79.1</td>
<td style="text-align: center;">64.4</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">MAML-DeepDDS</td>
<td style="text-align: center;">68.76</td>
<td style="text-align: center;">50.05</td>
<td style="text-align: center;">71.6</td>
<td style="text-align: center;">54.6</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">kNN-Features</td>
<td style="text-align: center;">65.4</td>
<td style="text-align: center;">45.9</td>
<td style="text-align: center;">82.0</td>
<td style="text-align: center;">70.3</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">SynerGPT* (ours)</td>
<td style="text-align: center;">$\mathbf{7 4 . 0}$</td>
<td style="text-align: center;">$\mathbf{5 7 . 3}$</td>
<td style="text-align: center;">$\mathbf{8 3 . 5}$</td>
<td style="text-align: center;">$\mathbf{7 2 . 1}$</td>
</tr>
<tr>
<td style="text-align: left;">Few-Shot</td>
<td style="text-align: left;">DeepSynergy</td>
<td style="text-align: center;">71.6</td>
<td style="text-align: center;">53.9</td>
<td style="text-align: center;">82.0</td>
<td style="text-align: center;">68.7</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">DeepDDS</td>
<td style="text-align: center;">75.5</td>
<td style="text-align: center;">57.4</td>
<td style="text-align: center;">74.2</td>
<td style="text-align: center;">60.4</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">SciBERT (random)</td>
<td style="text-align: center;">73.8</td>
<td style="text-align: center;">56.9</td>
<td style="text-align: center;">80.5</td>
<td style="text-align: center;">66.4</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">MAML-DeepDDS</td>
<td style="text-align: center;">68.79</td>
<td style="text-align: center;">50.00</td>
<td style="text-align: center;">71.4</td>
<td style="text-align: center;">54.6</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">kNN-Features</td>
<td style="text-align: center;">66.9</td>
<td style="text-align: center;">47.7</td>
<td style="text-align: center;">82.1</td>
<td style="text-align: center;">70.5</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">SetFit-S2</td>
<td style="text-align: center;">58.8</td>
<td style="text-align: center;">39.4</td>
<td style="text-align: center;">63.3</td>
<td style="text-align: center;">44.6</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">GPT-2</td>
<td style="text-align: center;">74.2</td>
<td style="text-align: center;">56.8</td>
<td style="text-align: center;">80.3</td>
<td style="text-align: center;">66.6</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">SynerGPT* (ours)</td>
<td style="text-align: center;">$\mathbf{7 7 . 7}$</td>
<td style="text-align: center;">$\mathbf{6 1 . 5}$</td>
<td style="text-align: center;">$\mathbf{8 3 . 8}$</td>
<td style="text-align: center;">$\mathbf{7 2 . 8}$</td>
</tr>
</tbody>
</table>
<p>Table 2: Few-shot and zero-shot results on ChemicalX DrugCombDB with 50 unknown drugs / 20 unknown cell lines. Our in-context methods perform better than baselines trained in the few-shot setting. Results are averaged over 5 runs. Zero-shot SynerGPT is evaluated without context. BERT models use random tokens. The difference between SynerGPT with and without context has $\mathrm{p}&lt;$ 0.05 for both unknown drugs and cell lines based on a paired $t$-test. Similarly, both are statistically significant from the best baseline. *For simplicity, we report the best selection strategy (UnknownFirst for unknown drug and Interpolate for unknown cell line). Full results are in Appendix A.2.</p>
<p>DeepDDS only increases $1.3 \%$ from zero- to few-shot. Our approach is able to leverage the few given examples more effectively as shown by this higher increase in ROC-AUC. It is also notable that Unknown-First outperforms Graph since the context contains more examples with the unknown drug which the model is able to utilize to produce better predictions.</p>
<p>For example, the tuple (Vismodegib, Mithramycin A, NCI-H226) with unknown Vismodegib is True. Without examples, this is predicted as 0.46 . For Graph with examples, it is predicted as 0.65 -closer to the ground truth. For Unknown-First, the prediction further increases to 0.79 . In this example, Graph only sees 15 examples containing the unknown but Unknown-First sees a full 20. Few-shot DeepDDS predicts 0.47 for this example, which is quite similar to our method without examples. As another example, (Chlorambucil, Cylocide, SK-OV-3) consists of two unknown drugs and has label False. Without examples, it is predicted as 0.62 . Graph improves this to 0.35 and Unknown-First improves to 0.23 . Interestingly, few-shot DeepDDS exhibits high uncertainty and predicts 0.50 .</p>
<p>Unknown Cell Lines Since there are only 112 cell lines, we set $m=20$ as unknown and use $n=10$ context examples. Interestingly, we find that models perform worse with context examples. We believe this is caused by the relatively small number of patient cell lines in the data vs. 2,956 drugs, making it harder to learn higher-level types of drug-cell line interaction. In other words, we are trying to learn a complex function class (drug synergy in an unknown cell line) without a significant number of example functions $f \in \mathcal{F}$. To alleviate this issue, we use 6 layers, batch size of 128, and only 30 epochs. Nonetheless, the issue still exists-performance decreases for baselines DeepDDS and MR-GNN and our strategies Unknown-First and Graph. We experiment with interpolating between training initially with Random to Unknown-First at the end (see Appendix A.2.2), which helps in the unknown cell line case. We believe this creates an exploration-exploitation effect.</p>
<h1>4.2 Context Optimization</h1>
<p>As we have shown in the previous section that the context selection strategy is very important for SynerGPT performance, the natural next question is to what extent the context can affect model performance. To test this, we conduct a different split. Like before, we select 50 unknown drugs and 20 cell lines; with their respective tuples, we create three uniform splits:</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Unknown Drug</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Unknown Cell Line</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Strategy</td>
<td style="text-align: center;">ROC-AUC</td>
<td style="text-align: center;">PR-AUC</td>
<td style="text-align: center;">ROC-AUC</td>
<td style="text-align: center;">PR-AUC</td>
</tr>
<tr>
<td style="text-align: left;">Mean UF</td>
<td style="text-align: center;">79.2</td>
<td style="text-align: center;">63.8</td>
<td style="text-align: center;">85.2</td>
<td style="text-align: center;">74.9</td>
</tr>
<tr>
<td style="text-align: left;">Best UF</td>
<td style="text-align: center;">80.8</td>
<td style="text-align: center;">66.4</td>
<td style="text-align: center;">85.6</td>
<td style="text-align: center;">75.7</td>
</tr>
<tr>
<td style="text-align: left;">GA</td>
<td style="text-align: center;">$\mathbf{8 1 . 5}$</td>
<td style="text-align: center;">$\mathbf{6 6 . 9}$</td>
<td style="text-align: center;">$\mathbf{8 6 . 1}$</td>
<td style="text-align: center;">$\mathbf{7 6 . 5}$</td>
</tr>
</tbody>
</table>
<p>Table 3: Test-set context optimization results ( $p&lt;$ 0.0001 ). Model parameters are fixed, only context is changed. UF indicates Unknown-First strategy.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: This figure shows the model's understanding of an unknown drug (Adavosertib) by retrieving candidates from the pool of held-out drugs. As the model sees more context synergy tuples $n$ (shown at the bottom; selected by the GA), it retrieves structures closer to the target molecule until finding the ground truth after 12 context examples. Repeated structures are skipped for brevity.
context, validation, and test. We train a Syn-
erGPT Unknown-First model using hyperparameters as in our above experiments.
In context optimization, our goal is to select examples from the context and train splits which maximize some metric on the validation split. For our experiments, we maximize ROC-AUC for our trained model using the validation set. Overall, we consider two strategies: Unknown-First, and a genetic algorithm (GA). For the genetic algorithm, we use the implementation and hyperparameters from PyGAD (Gad, 2021) with a population of 8 for 50 epochs. Here, we consider each example in the context split to be a potential gene. For comparison, we also select the context at random according to the Unknown-First strategy. To ensure comparability, we evaluate Unknown-First the same number of times as the genetic algorithm and select the best context. Our results (Table 3) show that the genetic algorithm optimizes the context from a starting average AUC of $79.2 \%$ up to $81.5 \%$ for unknown drugs and from $85.2 \%$ to $86.1 \%$ for unknown cells. Appendix C visualizes this and shows error bars. We further analyze the results by different tissue types (Appendix E). For example, we find that for unknown drugs, synergy prediction in ovarian cancer is effective, but for both unknown drugs and cell lines predictive performance on bone cell lines is low.</p>
<h1>4.3 Inverse drug design from synergy examples for discovery and explainability</h1>
<p>Explainability is one of the most challenging problems in deep learning. With transformer language models, the contrast between remarkable performance gain and lack of explainability becomes even more striking. Here we propose a novel drug design task to better understand the model's "thought process." As shown in Figure 2, essentially, we look at SynerGPT's prediction as it gains more information via synergy tuples. While this is a useful step, we do recognize that retrieval doesn't fully address explainability and hope to inspire further work. We refer to Limitations (§ 6) for more discussion.</p>
<p>In this novel task, we evaluate SynerGPT's ability to retrieve the structure of an unknown drug. We use the same splits as before but replace the classification head with a vector output trained using the loss in Equation 2. Using the same splits allows us to visualize the optimized context from the genetic algorithm. Experimentally, we achieve the best performance with the weight value from equation 1 set to $w_{i}:=i / k$. Two examples of the model retrieving drugs which match the context synergy pairs are shown in Figures 2 and 5. These show the retrieved drug after $i$ context examples have been observed by the model. Additionally, we show overall retrieval performance as the number of context examples shown to the model increases in Appendix D. Figure 4. For the weighted strategy, mean rank for seen drugs decreases from $\sim 1,500$ to $\sim 400$ as context increases. Qualitatively, we find that we can retrieve the relevant drug or a similar structure from synergy relationships in multiple cases. This is considerably more effective for drugs observed during training, but performance is also better than random for unknown drugs. This ability to visualize the model's understanding is helpful for explaining what the model predicts from observing a given context. Second, it enables retrieving drugs which have a desired set of synergies, which can help inform drug candidate discovery, including patient-specific scenarios. We note that we worked off a broad</p>
<p>definition of drug design as discovering new candidate medications. While retrieval is currently a challenging version of this, future work can expand the search space with generative models.</p>
<h1>5 Related Work</h1>
<h3>5.1 Molecular Language Models</h3>
<p>In recent years, advances in machine learning and NLP have been applied to molecule representations. Several efforts (Fabian et al., 2020; Chithrananda et al., 2020; Vaucher et al., 2021; Schwaller et al., 2021; NVIDIA Corporation, 2022; Tysinger et al., 2023) show excellent results training on string representations of molecules (Weininger, 1988; Weininger et al., 1989; Krenn et al., 2020; Cheng et al., 2023). Interest has also grown in multi-modal models (Edwards et al., 2022; Zeng et al., 2022) and multi-encoder models (Edwards et al., 2021; Vall et al., 2021; Xu \&amp; Wang, 2022; Su et al., 2022; Liu et al., 2022; Seidl et al., 2023; Xu et al., 2023b; Zhao et al., 2023) with applications to chemistry and biology. Existing work (Edwards et al., 2022; Su et al., 2022; Xu et al., 2023a; Christofidellis et al., 2023) also builds on this to "translate" between these modalities, such as MolT5 (Edwards et al., 2022), which translates between molecules and language.</p>
<h3>5.2 In-Context Learning</h3>
<p>With the success of models such as GPT-3 (Brown et al., 2020) and GPT-4 (OpenAI, 2023), interest has grown in the theoretical properties of in-context learning. (Garg et al., 2022), which we follow in this work, investigates the ability of transformers to learn function classes. (Olsson et al., 2022) investigates whether in-context learning is related to specific "induction heads". (von Oswald et al., 2022) shows that transformers do in-context learning by gradient descent. (Li et al., 2023b) frames in-context learning as algorithm learning to investigate generalization on unseen tasks.</p>
<h3>5.3 Language Models for Chemistry and Knowledge Graph Completion</h3>
<p>Very recently, considerable interest has grown in using language models, particularly GPT-4 (OpenAI, 2023), for uncovering chemical knowledge and molecular discovery (Hocky \&amp; White, 2022; White et al., 2022; Bran et al., 2023; Boiko et al., 2023; White et al., 2023; Castro Nascimento \&amp; Pimentel, 2023), including work in the few-shot setting (Ramos et al., 2023; Jablonka et al., 2023). CancerGPT (Li et al., 2023a), a related contemporaneous preprint, was recently released which explores a similar few-shot approach to drug-drug synergy prediction. It explores training literature-aware text-based GPT models on drug synergy data. The use of GPT models pretrained on massive textual corpora from the web also makes rigorous evaluation and comparison difficult. We believe our work is complementary, since we largely explore the transformer architecture without language and we consider in-context learning which they do not. We also consider extensions such as inverse design and context optimization. Due to the recency of (Li et al., 2023a), we leave additional comparisons beyond our real GPT2 baseline to future work. Applying language models to knowledge graphs has been investigated in the general (Yao et al., 2019; Kim et al., 2020; Youn \&amp; Tagkopoulos, 2022) and scientific domains (Nadkarni et al., 2021; Safavi et al., 2022). They can be considered similar to our tests of BERT language models applied to a drug synergy hypergraph (§ 4).</p>
<h3>5.4 Drug Synergy Prediction</h3>
<p>As discussed above, there are several approaches (Preuer et al., 2018; Xu et al., 2019; Nyamabo et al., 2021; Wang et al., 2022; Kuru et al., 2021; Sun et al., 2020; Rozemberczki et al., 2022b) which can predict synergy scores given cell line and drug features. There has also been interest in learning representations for these settings (Scherer et al., 2022). Recently, work (Yang et al., 2021; Rozemberczki et al., 2022a; Lin et al., 2022) has begun to incorporate additional data sources such as drug-protein interactions. This can help improve results, but it often requires creating a subset of the original synergy dataset which can bias results towards the proposed method. (Yang et al., 2023) extracts additional training data from the literature to improve synergy prediction results, which may relate to our results in Appendix F. Research also investigates the application of few-shot (Ma et al., 2021) and zero-shot (Huang et al., 2023) machine learning to drug response prediction-we extend this idea to drug synergy prediction. (Yang et al., 2020) and (Kuenzi et al., 2020) are related but</p>
<p>have different focuses compared to our paper; neither compare against any other synergy baselines or do large-scale evaluation. (Yang et al., 2020) focuses on a mechanistic understanding of (drug, tumor) activity-a different task. They use this understanding to rank subsystems and predict a limited number of drug combinations to evaluate. (Kuenzi et al., 2020) does database and experimental testing with small numbers of cell tissues and drugs.</p>
<h1>6 Conclusions and Future Work</h1>
<p>As demonstrated by HIV, HCV, and now cancer, combination therapy is a critical option for disease treatment. Yet, difficulties arise in regards to understanding drug-drug interactions and patient-specific genetic differences. To tackle this, we show that encoder-only language models are effective for drug synergy prediction. We then build on these results by proposing SynerGPT, a decoder model with a novel training strategy for in-context learning which can produce strong results for few-shot drug synergy prediction. We additionally show that the model context can be optimized using non-linear black-box approaches, which has exciting implications for the design of a standardized drug synergy testing panel for creating patient-specific synergy datasets. Finally, we explore a novel task of inverse design using desired drug synergy tuples. Performance on this challenging task is low for unknown drugs; nonetheless, it shows promise for future work that may enable personalized drug discovery.</p>
<h2>Limitations</h2>
<p>While we are able to achieve strong performance without additional cellular or drug data, our approach is very much a black box akin to most deep learning methods. To address this, we propose the task of inverse design from drug synergy examples, which allows the visualization of the model's structural understanding as it gains more information. While this is a useful step, we do recognize that further research on mechanistic explainability would be valuable. We hope our contribution on synergy-based inverse design can inspire further work on explainability and that SynerGPT's predictions can be useful inspiration for clinical researchers. We would also like to note that regardless of using deep learning models, pharmaceutical researchers are in many cases unable to explain the mechanisms of many important drugs on their own (e.g., Modafinil, Metformin, general anesthetics) (Stahl, 2020; Rena et al., 2013; Brown et al., 2011) — let alone explain their interactions with each other. These drugs are prescribed to hundreds of millions of patients. Recent studies (Lin et al., 2019) suggest that many purported protein drug targets may not be the actual target at all. Important progress with life-saving modern drugs can be made with limited visibility into underlying mechanisms, yet certainly improved mechanistic understanding would be highly useful.
While we show that strong performance is possible without features, future work will still likely want to integrate external database features into drug synergy prediction; however, they will likely need to be integrated in a more thoughtful manner in order to ensure an actual benefit.
It would also likely be interesting for future work to investigate the internal connections language models are learning and what it might mean for understanding the fundamental biology of how cellular pathways interact. It is also worth noting that designing molecules using drug synergy tuples is a somewhat atypical task, so there may exist a wall in terms of the information content inherent in the context. While we do analysis by separating model performance into different tissue types in this work (as done in multiple prior studies), we note that for future research it is likely too limiting and simplistic to separate cell lines into tissues types.</p>
<h1>References</h1>
<p>Walid Ahmad, Elana Simon, Seyone Chithrananda, Gabriel Grand, and Bharath Ramsundar. Chemberta-2: Towards chemical foundation models. arXiv preprint arXiv:2209.01712, 2022.</p>
<p>Iz Beltagy, Kyle Lo, and Arman Cohan. Scibert: A pretrained language model for scientific text. arXiv preprint arXiv:1903.10676, 2019.</p>
<p>Dimitris Bertsimas, Angela King, and Rahul Mazumder. Best subset selection via a modern optimization lens. arXiv: Methodology, 2015.</p>
<p>James RM Black and Nicholas McGranahan. Genetic and non-genetic clonal diversity in cancer evolution. Nature Reviews Cancer, 21(6):379-392, 2021.</p>
<p>Daniil A Boiko, Robert MacKnight, and Gabe Gomes. Emergent autonomous scientific research capabilities of large language models. arXiv preprint arXiv:2304.05332, 2023.</p>
<p>Andrew P Bradley. The use of the area under the roc curve in the evaluation of machine learning algorithms. Pattern recognition, 30(7):1145-1159, 1997.</p>
<p>Andres M Bran, Sam Cox, Andrew D White, and Philippe Schwaller. Chemcrow: Augmenting large-language models with chemistry tools. arXiv preprint arXiv:2304.05376, 2023.</p>
<p>Emery N Brown, Patrick L Purdon, and Christa J Van Dort. General anesthesia and altered states of arousal: a systems neuroscience analysis. Annual review of neuroscience, 34:601-628, 2011.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.</p>
<p>Cayque Monteiro Castro Nascimento and André Silva Pimentel. Do large language models understand chemistry? a conversation with chatgpt. Journal of Chemical Information and Modeling, 63(6): $1649-1655,2023$.</p>
<p>Austin H Cheng, Andy Cai, Santiago Miret, Gustavo Malkomes, Mariano Phielipp, and Alán AspuruGuzik. Group selfies: a robust fragment-based molecular string representation. Digital Discovery, 2023.</p>
<p>Seyone Chithrananda, Gabe Grand, and Bharath Ramsundar. Chemberta: Large-scale self-supervised pretraining for molecular property prediction. arXiv preprint arXiv:2010.09885, 2020.</p>
<p>Dimitrios Christofidellis, Giorgio Giannone, Jannis Born, Ole Winther, Teodoro Laino, and Matteo Manica. Unifying molecular and textual representations via multi-task language modelling. arXiv preprint arXiv:2301.12586, 2023.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171-4186, 2019.</p>
<p>Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. A survey for in-context learning. arXiv preprint arXiv:2301.00234, 2022.</p>
<p>Jean Dubuisson. Hepatitis c virus proteins. World journal of gastroenterology: WJG, 13(17):2406, 2007.</p>
<p>Carl Edwards, ChengXiang Zhai, and Heng Ji. Text2Mol: Cross-modal molecule retrieval with natural language queries. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 595-607, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. URL https://aclanthology.org/2021. emnlp-main. 47 .</p>
<p>Carl Edwards, Tuan Lai, Kevin Ros, Garrett Honke, Kyunghyun Cho, and Heng Ji. Translation between molecules and natural language. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 375-413, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. URL https://aclanthology. org/2022.emnlp-main. 26.</p>
<p>Benedek Fabian, Thomas Edlich, Héléna Gaspar, Marwin Segler, Joshua Meyers, Marco Fiscato, and Mohamed Ahmed. Molecular representation learning with language models and domain-relevant auxiliary tasks. arXiv preprint arXiv:2011.13230, 2020.</p>
<p>Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In International conference on machine learning, pp. 1126-1135. PMLR, 2017.</p>
<p>Alan D Frankel and John AT Young. Hiv-1: fifteen proteins and an rna. Annual review of biochemistry, 67(1):1-25, 1998.</p>
<p>Ahmed Fawzy Gad. Pygad: An intuitive genetic algorithm python library, 2021.
Shivam Garg, Dimitris Tsipras, Percy S Liang, and Gregory Valiant. What can transformers learn in-context? a case study of simple function classes. Advances in Neural Information Processing Systems, 35:30583-30598, 2022.</p>
<p>Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8): 1735-1780, 1997.</p>
<p>Glen M Hocky and Andrew D White. Natural language processing models that automate programming will transform chemistry research and teaching. Digital discovery, 1(2):79-83, 2022.</p>
<p>Kexin Huang, Tianfan Fu, Wenhao Gao, Yue Zhao, Yusuf Roohani, Jure Leskovec, Connor W Coley, Cao Xiao, Jimeng Sun, and Marinka Zitnik. Therapeutics data commons: Machine learning datasets and tasks for drug discovery and development. arXiv preprint arXiv:2102.09548, 2021.</p>
<p>Kexin Huang, Payal Chandak, Qianwen Wang, Shreyas Havaldar, Akhil Vaid, Jure Leskovec, Girish Nadkarni, Benjamin S Glicksberg, Nils Gehlenborg, and Marinka Zitnik. Zero-shot prediction of therapeutic use with geometric deep learning and clinician centered design. medRxiv, pp. 2023-03, 2023.</p>
<p>Kevin Maik Jablonka, Philippe Schwaller, Andres Ortega-Guerrero, and Berend Smit. Is gpt-3 all you need for low-data discovery in chemistry? ChemRxiv preprint, 2023.</p>
<p>Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Hierarchical generation of molecular graphs using structural motifs. In International conference on machine learning, pp. 4839-4848. PMLR, 2020.</p>
<p>David N Juurlink, Muhammad M Mamdani, Douglas S Lee, Alexander Kopp, Peter C Austin, Andreas Laupacis, and Donald A Redelmeier. Rates of hyperkalemia after publication of the randomized aldactone evaluation study. New England Journal of Medicine, 351(6):543-551, 2004.</p>
<p>Bosung Kim, Taesuk Hong, Youngjoong Ko, and Jungyun Seo. Multi-task learning for knowledge graph completion with pre-trained language models. In Proceedings of the 28th International Conference on Computational Linguistics, pp. 1737-1743, 2020.</p>
<p>Sunghwan Kim, Jie Chen, Tiejun Cheng, Asta Gindulyte, Jia He, Siqian He, Qingliang Li, Benjamin A Shoemaker, Paul A Thiessen, Bo Yu, et al. Pubchem 2023 update. Nucleic Acids Research, 51 (D1):D1373-D1380, 2023.</p>
<p>Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016.</p>
<p>Onno Kranenburg. The kras oncogene: past, present, and future. Biochimica et biophysica acta, 1756 (2):81-82, 2005.</p>
<p>Mario Krenn, Florian Häse, AkshatKumar Nigam, Pascal Friederich, and Alan Aspuru-Guzik. Selfreferencing embedded strings (selfies): A 100\% robust molecular string representation. Machine Learning: Science and Technology, 1(4):045024, 2020.</p>
<p>Kundan Krishna, Jeffrey Bigham, and Zachary C Lipton. Does pretraining for summarization require knowledge transfer? arXiv preprint arXiv:2109.04953, 2021.</p>
<p>Brent M Kuenzi, Jisoo Park, Samson H Fong, Kyle S Sanchez, John Lee, Jason F Kreisberg, Jianzhu Ma, and Trey Ideker. Predicting drug response and synergy using a deep learning model of human cancer cells. Cancer cell, 38(5):672-684, 2020.</p>
<p>Halil Ibrahim Kuru, Oznur Tastan, and A Ercument Cicek. Matchmaker: a deep learning framework for drug synergy prediction. IEEE/ACM Transactions on Computational Biology and Bioinformatics, 19(4):2334-2344, 2021.</p>
<p>Tianhao Li, Sandesh Shetty, Advaith Kamath, Ajay Jaiswal, Xianqian Jiang, Ying Ding, and Yejin Kim. Cancergpt: Few-shot drug pair synergy prediction using large pre-trained language models. arXiv preprint arXiv:2304.10946, 2023a.</p>
<p>Yingcong Li, M. Emrullah Ildiz, Dimitris Papailiopoulos, and Samet Oymak. Transformers as algorithms: Generalization and stability in in-context learning, 2023b.</p>
<p>T Jake Liang and Marc G Ghany. Current and future therapies for hepatitis c virus infection. New England Journal of Medicine, 368(20):1907-1917, 2013.</p>
<p>Ann Lin, Christopher J Giuliano, Ann Palladino, Kristen M John, Connor Abramowicz, Monet Lou Yuan, Erin L Sausville, Devon A Lukow, Luwei Liu, Alexander R Chait, et al. Off-target toxicity is a common mechanism of action of cancer drugs undergoing clinical trials. Science translational medicine, 11(509):eaaw8412, 2019.</p>
<p>Jiacheng Lin, Hanwen Xu, Addie Woicik, Jianzhu Ma, and Sheng Wang. Pisces: A combo-wise contrastive learning approach to synergistic drug combination prediction. bioRxiv, pp. 2022-11, 2022.</p>
<p>Hui Liu, Wenhao Zhang, Bo Zou, Jinxian Wang, Yuanyuan Deng, and Lei Deng. Drugcombdb: a comprehensive database of drug combinations toward the discovery of combinatorial therapy. Nucleic acids research, 48(D1):D871-D881, 2020.</p>
<p>Shengchao Liu, Weili Nie, Chengpeng Wang, Jiarui Lu, Zhuoran Qiao, Ling Liu, Jian Tang, Chaowei Xiao, and Anima Anandkumar. Multi-modal molecule structure-text model for text-based retrieval and editing. arXiv preprint arXiv:2212.10789, 2022.</p>
<p>Jianzhu Ma, Samson H Fong, Yunan Luo, Christopher J Bakkenist, John Paul Shen, Soufiane Mourragui, Lodewyk FA Wessels, Marc Hafner, Roded Sharan, Jian Peng, et al. Few-shot learning creates predictive models of drug response that translate from high-throughput screens to individual patients. Nature Cancer, 2(2):233-244, 2021.</p>
<p>Grégoire Mialon, Roberto Dessì, Maria Lomeli, Christoforos Nalmpantis, Ram Pasunuru, Roberta Raileanu, Baptiste Rozière, Timo Schick, Jane Dwivedi-Yu, Asli Celikyilmaz, et al. Augmented language models: a survey. arXiv preprint arXiv:2302.07842, 2023.</p>
<p>Alan Miller. Subset selection in regression. CRC Press, 2002.
Tom Michael Mitchell et al. Machine learning, volume 1. McGraw-hill New York, 2007.
Reza Bayat Mokhtari, Tina S Homayouni, Narges Baluch, Evgeniya Morgatskaya, Sushil Kumar, Bikul Das, and Herman Yeger. Combination therapy in combating cancer. Oncotarget, 8(23): 38022, 2017.</p>
<p>Richard D Moore and Richard E Chaisson. Natural history of hiv infection in the_era of combination antiretroviral therapy. Aids, 13(14):1933-1942, 1999.</p>
<p>Edmund A Mroz and James W Rocco. The challenges of tumor genetic diversity. Cancer, 123(6): 917-927, 2017.</p>
<p>Rahul Nadkarni, David Wadden, Iz Beltagy, Noah Smith, Hannaneh Hajishirzi, and Tom Hope. Scientific language models for biomedical knowledge base completion: An empirical study. In 3rd Conference on Automated Knowledge Base Construction, 2021.</p>
<p>NVIDIA Corporation. Megamolbart v0.2, 2022. URL https://catalog.ngc.nvidia.com/ orgs/nvidia/teams/clara/models/megamolbart_0_2.</p>
<p>Arnold K Nyamabo, Hui Yu, and Jian-Yu Shi. Ssi-ddi: substructure-substructure interactions for drug-drug interaction prediction. Briefings in Bioinformatics, 22(6):bbab133, 2021.</p>
<p>Peter H O’Donnell and M Eileen Dolan. Cancer pharmacoethnicity: Ethnic differences in susceptibility to the effects of chemotherapycancer pharmacoethnicity. Clinical Cancer Research, 15(15): $4806-4814,2009$.</p>
<p>Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et al. In-context learning and induction heads. arXiv preprint arXiv:2209.11895, 2022.</p>
<p>OpenAI. Gpt-4 technical report, 2023.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825-2830, 2011.</p>
<p>Kristina Preuer, Richard PI Lewis, Sepp Hochreiter, Andreas Bender, Krishna C Bulusu, and Günter Klambauer. Deepsynergy: predicting anti-cancer drug synergy with deep learning. Bioinformatics, 34(9):1538-1546, 2018.</p>
<p>Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.</p>
<p>Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pp. 8748-8763. PMLR, 2021. URL http://proceedings.mlr.press/v139/radford21a.html.</p>
<p>Mayk Caldas Ramos, Shane S Michtavy, Marc D Porosoff, and Andrew D White. Bayesian optimization of catalysts with in-context learning. arXiv preprint arXiv:2304.05341, 2023.</p>
<p>Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprint arXiv:1908.10084, 2019.</p>
<p>Graham Rena, Ewan R Pearson, and Kei Sakamoto. Molecular mechanism of action of metformin: old or new insights? Diabetologia, 56:1898-1906, 2013.</p>
<p>Benedek Rozemberczki, Anna Gogleva, Sebastian Nilsson, Gavin Edwards, Andriy Nikolov, and Eliseo Papa. Moomin: Deep molecular omics network for anti-cancer drug combination therapy. In Proceedings of the 31st ACM International Conference on Information \&amp; Knowledge Management, pp. 3472-3483, 2022a.</p>
<p>Benedek Rozemberczki, Charles Tapley Hoyt, Anna Gogleva, Piotr Grabowski, Klas Karis, Andrej Lamov, Andriy Nikolov, Sebastian Nilsson, Michael Ughetto, Yu Wang, et al. Chemicalx: A deep learning library for drug pair scoring. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pp. 3819-3828, 2022b.</p>
<p>Tara Safavi, Doug Downey, and Tom Hope. Cascader: Cross-modal cascading for knowledge graph link prediction. arXiv preprint arXiv:2205.08012, 2022.</p>
<p>Benjamin Sanchez-Lengeling and Alán Aspuru-Guzik. Inverse molecular design using machine learning: Generative models for matter engineering. Science, 361(6400):360-365, 2018.</p>
<p>Paul Scherer, Pietro Liò, and Mateja Jamnik. Distributed representations of graphs for drug pair scoring. arXiv preprint arXiv:2209.09383, 2022.</p>
<p>Philippe Schwaller, Daniel Probst, Alain C Vaucher, Vishnu H Nair, David Kreutter, Teodoro Laino, and Jean-Louis Reymond. Mapping the space of chemical reactions using attention-based neural networks. Nature Machine Intelligence, 3(2):144-152, 2021.</p>
<p>Philipp Seidl, Andreu Vall, Sepp Hochreiter, and Günter Klambauer. Enhancing activity prediction models in drug discovery with the ability to understand human language. arXiv preprint arXiv:2303.03363, 2023.</p>
<p>Amanpreet Singh, Mike D'Arcy, Arman Cohan, Doug Downey, and Sergey Feldman. Scirepeval: A multi-format benchmark for scientific document representations. arXiv preprint arXiv:2211.13308, 2022.</p>
<p>Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. Advances in neural information processing systems, 30, 2017.</p>
<p>Stephen M Stahl. Prescriber's guide: Stahl's essential psychopharmacology. Cambridge University Press, 2020.</p>
<p>Vaidotas Stankevicius, Gintautas Vasauskas, Rimante Noreikiene, Karolina Kuodyte, Mindaugas Valius, and Kestutis Suziedelis. Extracellular matrix-dependent pathways in colorectal cancer cell lines reveal potential targets for anticancer therapies. Anticancer Research, 36(9):4559-4567, 2016.</p>
<p>Bing Su, Dazhao Du, Zhao Yang, Yujie Zhou, Jiangmeng Li, Anyi Rao, Hao Sun, Zhiwu Lu, and Ji-Rong Wen. A molecular multimodal foundation model associating molecule graphs with natural language. arXiv preprint arXiv:2209.05481, 2022.</p>
<p>Mengying Sun, Fei Wang, Olivier Elemento, and Jiayu Zhou. Structure-based drug-drug interaction detection via expressive graph convolutional networks and deep sets (student abstract). In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp. 13927-13928, 2020.</p>
<p>John G Tate, Sally Bamford, Harry C Jubb, Zbyslaw Sondka, David M Beare, Nidhi Bindal, Harry Boutselakis, Charlotte G Cole, Celestino Creatore, Elisabeth Dawson, et al. Cosmic: the catalogue of somatic mutations in cancer. Nucleic acids research, 47(D1):D941-D947, 2019.</p>
<p>Lewis Tunstall, Nils Reimers, Unso Eun Seo Jo, Luke Bates, Daniel Korat, Moshe Wasserblat, and Oren Pereg. Efficient few-shot learning without prompts. arXiv preprint arXiv:2209.11055, 2022.</p>
<p>Emma P Tysinger, Brajesh K Rai, and Anton V Sinitskiy. Can we quickly learn to "translate" bioactive molecules with transformer models? Journal of Chemical Information and Modeling, 63 (6):1734-1744, 2023.</p>
<p>Andreu Vall, Sepp Hochreiter, and Günter Klambauer. Bioassayclr: Prediction of biological activity for novel bioassays based on rich textual descriptions. In ELLIS ML4Molecules workshop, 2021.</p>
<p>Alain C Vaucher, Philippe Schwaller, Joppe Geluykens, Vishnu H Nair, Anna Iuliano, and Teodoro Laino. Inferring experimental procedures from text-based representations of chemical reactions. Nature communications, 12(1):2573, 2021.</p>
<p>Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017.</p>
<p>Johannes von Oswald, Eyvind Niklasson, Ettore Randazzo, João Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent. arXiv preprint arXiv:2212.07677, 2022.</p>
<p>Jinxian Wang, Xuejun Liu, Siyuan Shen, Lei Deng, and Hui Liu. Deepdds: deep graph neural network with attention mechanism to predict synergistic drug combinations. Briefings in Bioinformatics, 23 (1):bbab390, 2022.</p>
<p>David Weininger. Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules. Journal of chemical information and computer sciences, 28(1):31-36, 1988 .</p>
<p>David Weininger, Arthur Weininger, and Joseph L Weininger. Smiles. 2. algorithm for generation of unique smiles notation. Journal of chemical information and computer sciences, 29(2):97-101, 1989 .</p>
<p>Andrew D White, Glen M Hocky, Heta A Gandhi, Mehrad Ansari, Sam Cox, Geemi P Wellawatte, Subarna Sasmal, Ziyue Yang, Kangxin Liu, Yuvraj Singh, et al. Do large language models know chemistry? ChemRxiv preprint, 2022.</p>
<p>Andrew D White, Glen M Hocky, Heta A Gandhi, Mehrad Ansari, Sam Cox, Geemi P Wellawatte, Subarna Sasmal, Ziyue Yang, Kangxin Liu, Yuvraj Singh, et al. Assessment of chemistry knowledge in large language models that generate code. Digital Discovery, 2(2):368-376, 2023.</p>
<p>Hanwen Xu and Sheng Wang. Protranslator: zero-shot protein function prediction using textual description. In Research in Computational Molecular Biology: 26th Annual International Conference, RECOMB 2022, San Diego, CA, USA, May 22-25, 2022, Proceedings, pp. 279-294. Springer, 2022.</p>
<p>Hanwen Xu, Addie Woicik, Hoifung Poon, Russ B Altman, and Sheng Wang. Multilingual translation for zero-shot biomedical classification using biotranslator. Nature Communications, 14(1):738, 2023a.</p>
<p>Minghao Xu, Xinyu Yuan, Santiago Miret, and Jian Tang. Protst: Multi-modality learning of protein sequences and biomedical texts. arXiv preprint arXiv:2301.12040, 2023b.</p>
<p>Nuo Xu, Pinghui Wang, Long Chen, Jing Tao, and Junzhou Zhao. Mr-gnn: Multi-resolution and dual graph neural network for predicting structured entity interactions. arXiv preprint arXiv:1905.09558, 2019 .</p>
<p>Cai Yang, Addie Woicik, Hoifung Poon, and Sheng Wang. Bliam: Literature-based data synthesis for synergistic drug combination prediction. arXiv preprint arXiv:2302.06860, 2023.</p>
<p>Jiannan Yang, Zhongzhi Xu, William Ka Kei Wu, Qian Chu, and Qingpeng Zhang. Graphsynergy: a network-inspired deep learning model for anticancer drug combination prediction. Journal of the American Medical Informatics Association, 28(11):2336-2345, 2021.</p>
<p>Mi Yang, Patricia Jaaks, Jonathan Dry, Mathew Garnett, Michael P Menden, and Julio Saez-Rodriguez. Stratification and prediction of drug synergy based on target functional similarity. npj Systems Biology and Applications, 6(1):16, 2020.</p>
<p>Liang Yao, Chengsheng Mao, and Yuan Luo. Kg-bert: Bert for knowledge graph completion. arXiv preprint arXiv:1909.03193, 2019.</p>
<p>Michihiro Yasunaga, Jure Leskovec, and Percy Liang. Linkbert: Pretraining language models with document links. arXiv preprint arXiv:2203.15827, 2022.</p>
<p>Jason Youn and Ilias Tagkopoulos. Kglm: Integrating knowledge graph structure in language models for link prediction. arXiv preprint arXiv:2211.02744, 2022.</p>
<p>Bulat Zagidullin, Jehad Aldahdooh, Shuyu Zheng, Wenyu Wang, Yinyin Wang, Joseph Saad, Alina Malyutina, Mohieddin Jafari, Ziaurrehman Tanoli, Alberto Pessia, et al. Drugcomb: an integrative cancer drug combination data portal. Nucleic acids research, 47(W1):W43-W51, 2019.</p>
<p>Lorenzo Villa Zapata, Philip D Hansten, Jennifer Panic, John R Horn, Richard D Boyce, Sheila Gephart, Vignesh Subbian, Andrew Romero, and Daniel C Malone. Risk of bleeding with exposure to warfarin and nonsteroidal anti-inflammatory drugs: a systematic review and meta-analysis. Thrombosis and haemostasis, 120(07):1066-1074, 2020.</p>
<p>Zheni Zeng, Yuan Yao, Zhiyuan Liu, and Maosong Sun. A deep-learning system bridging molecule structure and biomedical text with comprehension comparable to human professionals. Nature communications, 13(1):862, 2022.</p>
<p>Wenyu Zhao, Dong Zhou, Buqing Cao, Kai Zhang, and Jinjun Chen. Adversarial modality alignment network for cross-modal molecule retrieval. IEEE Transactions on Artificial Intelligence, 2023.</p>
<p>Shuyu Zheng, Jehad Aldahdooh, Tolou Shadbahr, Yinyin Wang, Dalal Aldahdooh, Jie Bao, Wenyu Wang, and Jing Tang. Drugcomb update: a more comprehensive drug sensitivity data repository and analysis portal. Nucleic acids research, 49(W1):W174-W184, 2021.</p>
<h1>A Full Results Tables</h1>
<h2>A. 1 GraphSynergy Full Results</h2>
<p>Full results for the BERT input method and GraphSynergy tests are in Table 4. We compare on the specific subset of DrugCombDB Liu et al. (2020) which was selected to match Graphsynergy's network data (i.e. selecting the subset of DrugCombDB with drugs/cells that can be matched with external protein-protein interaction, drug-protein association, and cell-protein association networks) and a 7:1:2 train:validation:test split. This data subset also contains useful surface names (the common natural language name of the drug; e.g. dasatinib), which allows us to compare the effect that drug names have on language model synergy prediction performance.</p>
<p>We consider three BERT training variations: the original BERT Devlin et al. (2019), SciBERT Beltagy et al. (2019), and BioLinkBERT Yasunaga et al. (2022). SciBERT was trained on a corpus of scientific documents which would be considerably more focused on drugs than a general corpus. BioLinkBERT is a biomedical BERT model additionally trained using document relation prediction (e.g. citation links). We would like to reiterate the rather remarkable finding that pre-training on scientific literature does not necessarily help the model perform drug synergy prediction any better. Overall, these results indicate that models using external data may not be behaving how we think they are.</p>
<p>ChemicalX Results We report full results on the subset of DrugCombDB Liu et al. (2020) used by ChemicalX Rozemberczki et al. (2022b) in Table 5. Previous work tested on different subsets of existing datasets (due to filtering for external features).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Input</th>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">ROC-AUC</th>
<th style="text-align: center;">F1</th>
<th style="text-align: center;">Precision</th>
<th style="text-align: center;">Recall</th>
<th style="text-align: center;">Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GraphSynergy</td>
<td style="text-align: center;">83.4</td>
<td style="text-align: center;">72.7</td>
<td style="text-align: center;">73.5</td>
<td style="text-align: center;">71.9</td>
<td style="text-align: center;">75.5</td>
</tr>
<tr>
<td style="text-align: center;">Name</td>
<td style="text-align: center;">Unpretrained BERT-base</td>
<td style="text-align: center;">80.6</td>
<td style="text-align: center;">71.0</td>
<td style="text-align: center;">71.7</td>
<td style="text-align: center;">70.3</td>
<td style="text-align: center;">74.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BioLinkBERT-base</td>
<td style="text-align: center;">83.6</td>
<td style="text-align: center;">73.1</td>
<td style="text-align: center;">73.4</td>
<td style="text-align: center;">72.8</td>
<td style="text-align: center;">75.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SciBERT-base</td>
<td style="text-align: center;">83.8</td>
<td style="text-align: center;">73.8</td>
<td style="text-align: center;">73.3</td>
<td style="text-align: center;">74.3</td>
<td style="text-align: center;">75.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BERT-base</td>
<td style="text-align: center;">83.8</td>
<td style="text-align: center;">73.3</td>
<td style="text-align: center;">74.2</td>
<td style="text-align: center;">72.4</td>
<td style="text-align: center;">76.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BioLinkBERT-large</td>
<td style="text-align: center;">84.7</td>
<td style="text-align: center;">73.9</td>
<td style="text-align: center;">74.7</td>
<td style="text-align: center;">73.1</td>
<td style="text-align: center;">76.7</td>
</tr>
<tr>
<td style="text-align: center;">Random <br> Token</td>
<td style="text-align: center;">BioLinkBERT-base</td>
<td style="text-align: center;">84.1</td>
<td style="text-align: center;">73.7</td>
<td style="text-align: center;">73.6</td>
<td style="text-align: center;">73.8</td>
<td style="text-align: center;">76.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SciBERT-base</td>
<td style="text-align: center;">83.8</td>
<td style="text-align: center;">73.3</td>
<td style="text-align: center;">74.2</td>
<td style="text-align: center;">72.4</td>
<td style="text-align: center;">76.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BERT-base</td>
<td style="text-align: center;">84.0</td>
<td style="text-align: center;">73.4</td>
<td style="text-align: center;">74.1</td>
<td style="text-align: center;">72.7</td>
<td style="text-align: center;">76.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BioLinkBERT-large</td>
<td style="text-align: center;">84.1</td>
<td style="text-align: center;">73.8</td>
<td style="text-align: center;">73.4</td>
<td style="text-align: center;">74.2</td>
<td style="text-align: center;">76.1</td>
</tr>
</tbody>
</table>
<p>Table 4: Performance of BERT models with names and random tokens and GraphSynergy on the custom subset of DrugCombDB Liu et al. (2020). Results are average of 5 runs. Name indicates that the common name of the drug is used as input, while Random Token uses the strategy described in Section 3.1.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">KB Info</th>
<th style="text-align: center;">Name Info</th>
<th style="text-align: center;">ROC-AUC</th>
<th style="text-align: center;">PR-AUC</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">DeepSynergy</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">84.3</td>
<td style="text-align: center;">70.4</td>
</tr>
<tr>
<td style="text-align: center;">MR-GNN</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">77.9</td>
<td style="text-align: center;">62.6</td>
</tr>
<tr>
<td style="text-align: center;">SSI-DDI</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">63.3</td>
<td style="text-align: center;">41.4</td>
</tr>
<tr>
<td style="text-align: center;">DeepDDS</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">87.2</td>
<td style="text-align: center;">77.0</td>
</tr>
<tr>
<td style="text-align: center;">SciBERT (random)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">86.9</td>
<td style="text-align: center;">76.3</td>
</tr>
<tr>
<td style="text-align: center;">BioLinkBERT (random)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">86.8</td>
<td style="text-align: center;">76.4</td>
</tr>
<tr>
<td style="text-align: center;">BioLinkBERT (name)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">86.4</td>
<td style="text-align: center;">75.9</td>
</tr>
</tbody>
</table>
<p>Table 5: Classification results for four selected ChemicalX Rozemberczki et al. (2022b) baselines and two BERT-base models on DrugCombDB Liu et al. (2020). First two BERT models use random token inputs and last model uses drug names as input. Values are average of five runs.</p>
<h1>A. 2 Few-Shot Full Results</h1>
<h2>A.2.1 Baseline Descriptions</h2>
<p>DeepSynergy is a popular feedforward model which uses cell line features and drug fingerprints. MRGNN is a graph convolutional network (GCN) Kipf \&amp; Welling (2016) fed into an LSTM Hochreiter \&amp; Schmidhuber (1997) which takes the drug structure into account. SSI-DDI uses a graph attention network (GAT) Veličković et al. (2017) with a final co-attention layer. DeepDDS uses both a GAT and GCN, which are fed into a fully connected feed forward network.</p>
<p>Real GPT-2 We train a GPT-2 model ${ }^{2}$ in the few-shot setting (as opposed to SynerGPT's zero-shot) using random context and the same hyperparameters to mimic SynerGPT's training settings as much as possible. We use names of the drugs obtained from linking to PubChem Kim et al. (2023) as input in the form "Are drugs [DRUG1] and [DRUG2] synergistic in cell line [CELL]?"</p>
<p>SetFit Furthermore, we test finetuning a few-shot language-model baseline, SetFit Tunstall et al. (2022), on our few-shot data. We follow the original paper in using batch size $16, R=20$ text pairs generated for contrastive learning, and 1 epoch. Inputs to the model follow the same format as BERT in Section 3.1. We test using four models.</p>
<ol>
<li>SetFit-SBERT: paraphrase-multilingual-mpnet-base-v2 from Reimers \&amp; Gurevych (2019) with names as input. This model was trained to create semantic embeddings via Siamese networks.</li>
<li>SetFit-C: recobo/chemical-bert-uncased-simcse from Recobo.ai ${ }^{3}$ with names as input. This model was trained using SimCSE on chemistry text.</li>
<li>SetFit-S2: allenai/specter2 from Singh et al. (2022) with names as input. This model was trained on multiple scientific classification and regression tasks, such as MeSH descriptors classification.</li>
<li>SetFit-SMILES: DeepChem/ChemBERTa-77M-MTR from Ahmad et al. (2022) with SMILES strings as input. This model was pretrained by predicting 200 molecular properties for a molecule given its SMILES string.</li>
</ol>
<p>Model-Agnostic Meta-Learning We also consider a meta-learning formulation of our problem setting. We use MAML Finn et al. (2017) to train a DeepDDS model. Since MAML ${ }^{4}$ does few-shot classification using episodes sampled from different learning tasks, we reframe our problem to match this. We consider predicting synergy for each drug to be a task. Then, we sample an episode for training from a random task for each mini-batch. We aggregate rare drugs without enough samples to form an episode into the same task until there are enough samples for an episode. Additionally, since we are dealing with binary classification here, we use $N=2$-way. We sample the "validation" portion of each episode from our training set like in SynerGPT. We use the same context bank (and context size) for "adaptation" during evaluation. The same learning rate $(1 e-3)$, batch size (512), and number of steps/epochs as DeepDDS is used. We report few-shot (first-order) and zero-shot (no adaptation) versions. Overall, we find that the MAML training procedure produces poor results, and adaptation produces insignificant performance increases. We attribute this to the episode-based sampling strategy neglecting important information in training.</p>
<p>Protonets As another meta-learning baseline, we consider Protonets Snell et al. (2017). We use the same meta-learning framework as for MAML. Because we don't have drug task meta-data, we only consider the few-shot setting.
k-Nearest Neighbors We also consider a k-Nearest Neighbors baseline using scikit-learn Pedregosa et al. (2011) similar to Nadkarni et al. (2021). We construct embeddings for each synergy pair by concatenating (Drug1, Drug2, Cell) embeddings. In the training set, we also include (Drug2, Drug1, Cell). We consider two embedding sources. For the first, kNN-Features, we consider the drug and</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>cell fingerprint features from ChemicalX. For the second, kNN-S2, we use name embeddings from the Specter2 model. We report both zero-shot and few-shot versions. In the few-shot setting, the context bank is added to the training data. We set $k$ equal to the context number ( 20 and 10 for drugs and cell lines, respectively). We find performance on cell lines to be surprisingly effective, although still less than SynerGPT.</p>
<h1>A.2.2 Interpolate Details</h1>
<p>In the Unknown cell line setting, we observe that Random has an interesting effect where it performs better after examples (although still worse than Unknown-First (no-ex)), so we consider a fourth strategy: interpolating between Random Unknown-First. Essentially, for each data mini-batch in epoch $e$ of $E$ total epochs, we select either the Random strategy with probability $\max \left(0.25,1-\frac{e}{E}\right)$ otherwise we use the Unknown-First Strategy. This is analogous to an exploration-exploitation approach where we are pretraining with Random and transitioning to Unknown-First. We use a threshold of $25 \%$ to ensure the benefits of Random are kept until the end of training. We find that this interpolation strategy is effective (with $\mathrm{p}&lt;0.05$, see Table 6 ) in dealing with the unknown cell line case.</p>
<h2>A.2.3 In-Context Implementation Details</h2>
<p>In the unknown drug setting, to allow for tuples with multiple unknown drugs, we use both a [UNKNOWN] and [UNKNOWN2] token (e.g. a tuple containing two unknown drugs would be ([UNKNOWN], [UNKNOWN2], $c$ )).</p>
<p>For the inverse design experiments, in some cases, context examples do not contain the unknown entity $h$ and therefore no [UNKNOWN] tokens, so we use $\overrightarrow{0}$ as a replacement for the ground truth representation when calculating our loss function. We use the same model, splits, and training hyperparameters as in the context optimization setting.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>2 "gpt2" from HuggingFace.
${ }^{3}$ www.recobo.ai
${ }^{4}$ We use the implementation from https://github.com/cnguyen10/few_shot_meta_learning&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
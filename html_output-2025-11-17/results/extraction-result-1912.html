<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1912 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1912</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1912</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-39.html">extraction-schema-39</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <p><strong>Paper ID:</strong> paper-276937820</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2503.08950v1.pdf" target="_blank">FP3: A 3D Foundation Policy for Robotic Manipulation</a></p>
                <p><strong>Paper Abstract:</strong> Following its success in natural language processing and computer vision, foundation models that are pre-trained on large-scale multi-task datasets have also shown great potential in robotics. However, most existing robot foundation models rely solely on 2D image observations, ignoring 3D geometric information, which is essential for robots to perceive and reason about the 3D world. In this paper, we introduce FP3, a first large-scale 3D foundation policy model for robotic manipulation. FP3 builds on a scalable diffusion transformer architecture and is pre-trained on 60k trajectories with point cloud observations. With the model design and diverse pre-training data, FP3 can be efficiently fine-tuned for downstream tasks while exhibiting strong generalization capabilities. Experiments on real robots demonstrate that with only 80 demonstrations, FP3 is able to learn a new task with over 90% success rates in novel environments with unseen objects, significantly surpassing existing robot foundation models.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1912.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1912.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FP3</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>3D Foundation Policy (FP3)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 1.3B-parameter diffusion-transformer language-visuomotor foundation policy that ingests 3D point clouds (two camera views), CLIP language embeddings, and proprioception; pre-trained on 60k robotic manipulation demonstrations and fine-tuned with LoRA on small task-specific datasets for sample-efficient real-robot manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>FP3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder-decoder diffusion transformer policy. Inputs: two-view 3D point clouds encoded by a 300M-parameter Uni3D ViT (fine-tuned), frozen CLIP language embedding as instruction, and robot proprioception; Transformer encoder fuses multimodal tokens, Transformer decoder denoises action chunks using adaLN conditioning; predicts continuous Cartesian actions (action chunks) via DDPM/DDIM denoising.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>imitation/demonstration pretraining on multi-task robotic data (3D point cloud observations) with a pre-trained 3D encoder aligned to image-text space</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Pre-trained on 60k demonstrations (subset of DROID: originally 76k demos) across 86 tasks and 564 scenes (depth + RGB used to recover point clouds). Data contains multi-task manipulation trajectories (actions), objects and scene variations, but not explicit large-scale natural-language instruction supervision beyond CLIP alignment used in Uni3D encoder pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Real-world robotic manipulation (multi-task language-conditioned visuomotor control)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Four downstream real-robot tasks evaluated: Fold Towel, Clean Table (pick and drop a small item into bin), Stand up Cup (upright a lying cup), Pour Water (grasp bottle, pour into cup, place bottle). Continuous Cartesian action space (absolute Cartesian-space control) recorded at 15 Hz; objects vary in appearance and size; evaluation on real Franka Emika Panda robot with wrist and third-person ZED cameras; both in-domain (seen env/objects) and in-the-wild (novel environments/objects) real-world tests.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Partially discussed: Uni3D encoder used was pre-trained to align 3D point-cloud features with image-text aligned features (explicitly stated), providing some semantic alignment between 3D perception and language; CLIP embeddings supply language-to-visual alignment for instructions, but paper notes this is a simple conditioning and may be insufficient for complex/dynamic language grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Post-training FP3 (pretrained + 80 fine-tune demos) achieves very high success: per-paper claims 'learn a new task with over 90% success rates in novel environments with unseen objects' and average in-the-wild success 'over 80%'; Clean Table ablation table reports FP3 (ours) = 100% in-domain, 95% in-the-wild (success rate).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>FP3-Scratch (same architecture trained from scratch on the small post-training data) performs poorly: e.g., Clean Table FP3-Scratch = 35% in-domain, 0% in-the-wild (success rate). General baselines without large pretraining (DP/DP3/OpenVLA) have much lower success (often <50% in-domain and near-zero in-the-wild for many tasks); paper also notes classical small policies typically need ~200 expert episodes to learn a new task.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Explicitly reported: FP3 requires only 80 post-training demonstrations (10 per environment × 8 envs) to fine-tune to new tasks and reach >90% success in novel environments; baseline small models typically trained from scratch require ~200 demonstrations (cited as typical) and failed to generalize in the 80-demo regime. FP3 fine-tuning time: ~2 hours on single A800 GPU.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>No explicit attention-map or transformer attention analyses are presented. The paper describes architecture choices (adaLN conditioning, encoder-decoder fusion) but provides no visualization or quantitative analysis of attention focusing on semantic regions or affordances.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Partial: the paper states Uni3D was pre-trained to align 3D point-cloud features with image-text aligned features (semantic alignment claim), but provides no empirical embedding-space analyses (e.g., clustering, t-SNE) or quantitative probes in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Behavioral evidence: language-conditioned policy can follow different language instructions in the same initial state (instruction-following evaluation) and execute the corresponding tasks; FP3's superior success and qualitative observations (more precise and smoother actions, reasonable recovery attempts after failure) support language-to-action grounding, but no direct mechanistic probes (e.g., causal interventions, grounding attribution) are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>No explicit multi-level feature (low-level vs high-level) analysis is given. Ablations show that 3D geometric features (point cloud input) confer robustness where image-only variants drop substantially in the wild, suggesting higher-level geometric representations help transfer, but no layerwise feature hierarchy study is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Transfer success is influenced by observation modality (3D point clouds help), pretraining scale (model + data scaling improves generalization), and calibration (consistent world-frame point clouds improve view robustness). Transfer fails when pretraining is absent or data diversity is limited; image-only variants and small models generalize poorly to camera-view, lighting, and novel-object shifts.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Evaluated: FP3 was fine-tuned on 8 environments/objects (80 demos) and tested zero-shot on 4 unseen environments/objects; FP3 retained high performance (e.g., averaged >80% in-the-wild; Clean Table 95% in-the-wild), whereas baselines and FP3-Scratch largely failed on unseen objects (near-zero).</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Few-shot: FP3 demonstrates few-shot fine-tuning (80 demonstrations) that yields strong zero-shot deployment to novel objects/environments; paper claims 'capable of zero-shot generalization to both novel objects and environments with about 90% success rate' after the 80-demo post-training. Pure zero-shot from base model (no post-training) is limited per paper.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Some component/scale ablations performed: model-size ablation (FP3-Base: 365M vs FP3 1.3B) and pretraining-data-size ablation (FP3-Base-30k vs FP3-Base 60k). They also report image-only variant (FP3-Base-Image) to test observation modality. There is no fine-grained per-layer importance analysis or systematic freezing/probing beyond these ablations. Uni3D encoder is fine-tuned during pre-training (they state freezing harms performance based on prior works).</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>No direct evidence that language pretraining hurt downstream performance is reported. The paper does note limited zero-shot capability of the base model and that simple CLIP conditioning may be insufficient for complex instructions, but not negative transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Yes: explicit comparisons. Image-only variant (FP3-Base-Image) performs similarly in-domain but much worse in-the-wild (example Clean Table: FP3-Base-Image = 90% in-domain, 55% in-the-wild vs FP3-Base = 95% in-domain, 90% in-the-wild), demonstrating 3D point cloud inputs outperform image-only under cross-domain shifts.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>No explicit analysis of representation or performance dynamics over fine-tuning steps beyond reporting total pre-training (3M steps) and fine-tuning times; no staged learning curves presented.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>No analysis of representational dimensionality (PCA/intrinsic dimension) is reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1912.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1912.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Uni3D</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Uni3D ViT (large)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pre-trained large-scale 3D point-cloud encoder (Uni3D ViT) used to extract semantic + geometric embeddings from 4000-point point clouds per view; reported to have been pre-trained to align 3D point features with image-text aligned features.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Uni3d: Exploring unified 3d representation at scale</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Uni3D ViT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A Vision Transformer-style encoder for 3D point clouds (used here as 300M-parameter encoder). Pre-trained to align point-cloud features with image-text aligned feature space, producing embeddings that the FP3 policy fine-tunes during pre-training.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>3D representation pretraining with alignment to image-text (vision-language aligned feature space)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not fully detailed in this paper; described as large-scale 3D pretraining that aligns point-cloud features with image-text aligned features (implies use of paired 3D and image-text supervision or distillation to image-text models).</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Point-cloud encoding for robotic manipulation policy</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Encodes two-view point clouds (wrist and third-person) for downstream manipulation control tasks in real robot environment; used with temporal stacking (history length 2) and color channels preserved for experiments conditioned on color.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Explicitly stated that Uni3D was pre-trained to align 3D point-cloud features with image-text aligned features — this is used by FP3 to provide semantically rich 3D features.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Not independently reported for Uni3D in this paper; its inclusion in FP3 correlates with improved sample-efficiency and cross-domain generalization (FP3 results).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not reported in isolation; the paper mentions fine-tuning the Uni3D weights during policy pretraining rather than freezing them (citing prior works where freezing harms performance).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not separately quantified for Uni3D alone; it's a component contributing to the FP3 overall sample efficiency (80-demo fine-tune).</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>No per-encoder attention or feature-visualization analysis in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Claimed alignment to image-text feature space but no analysis presented here.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Indirect: Uni3D provides semantically aligned 3D features that support language-conditioned action prediction in FP3, but no direct mechanistic grounding experiments isolating Uni3D are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>None reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Uni3D's 3D geometric encoding is highlighted as improving robustness to camera viewpoint, lighting, and background changes when compared to image-only representations.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>No isolated evaluation of Uni3D alone; FP3 using Uni3D generalizes well to novel objects.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not applicable in isolation.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>No layerwise probing of Uni3D in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>None reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1912.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1912.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CLIP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CLIP (Contrastive Language–Image Pretraining)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A frozen CLIP model is used to embed natural language instructions into a language embedding that conditions the FP3 policy; the CLIP embedding is used without further fine-tuning in FP3.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning transferable visual models from natural language supervision</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CLIP</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A vision-language contrastive model that produces aligned image and text embeddings; FP3 uses the frozen CLIP text encoder to embed language instructions as conditioning tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>vision-language pretraining on large image-text pairs (contrastive)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Large-scale internet image-text pairs containing object labels and scene descriptions; not robot-action-specific and not containing robot trajectories or fine-grained motor affordance supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Language-conditioned manipulation (as instruction encoder)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Embeds natural language task instructions which are provided to the FP3 policy at inference/fine-tuning time to condition continuous action prediction for manipulation tasks in real-world robot setups.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>CLIP provides a semantic alignment between instruction tokens and visual concepts; FP3 uses this embedding but the paper notes CLIP conditioning is a simple approach and may be insufficient for complex/dynamic instruction grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>FP3 with frozen CLIP embeddings supports instruction following and high downstream success after fine-tuning (FP3 results), but no ablation isolating CLIP vs alternative language encoders is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not reported. The paper does not test variants without CLIP conditioning for language instructions (though it notes CLIP is frozen).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not reported specifically for CLIP conditioning.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>No analysis of attention on CLIP-derived tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>No internal analysis of CLIP embedding interactions with Uni3D embeddings in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Behavioral instruction-following results indicate CLIP embeddings suffice for the evaluated instruction types given FP3 pretraining + fine-tuning, but authors caution CLIP may be insufficient for complex/dynamic instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>None reported.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Paper notes combining stronger VLMs with diffusion-based policies (e.g., π0) is a promising direction; suggests CLIP conditioning alone is limited.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not specifically analyzed for CLIP encoding alone.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>CLIP used as frozen encoder; FP3 demonstrates few-shot fine-tuning; CLIP's role in pure zero-shot is not isolated.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>No.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Authors mention CLIP embedding is insufficiently expressive for complicated/dynamic language, but no evidence of active harm.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1912.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1912.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DP (Diffusion Policy)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Diffusion Policy</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A classic diffusion-based visuomotor imitation-learning policy operating on 2D image observations used as a baseline; in this paper, augmented with language-conditioning to match FP3's inputs for fair comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Diffusion policy: Visuomotor policy learning via action diffusion</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DP (image-based diffusion policy)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Conditional action diffusion model that denoises action trajectories conditioned on past observations (here: 2D images) and proprioception; in the paper used as a small 2D baseline (with added language-conditioning module).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>no large-scale pretraining reported in this work (trained from scratch or on small task data for baseline experiments); original DP is an imitation-learning method.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Baseline training used only the small post-training dataset (80 demos) in experiments — no large diverse pretraining as with FP3.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Real-world robotic manipulation (same four downstream tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Same continuous-action real-robot tasks; uses 2D image-only observations which are more sensitive to lighting, background, and camera-view shifts.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>No vision-language pretraining; language-conditioning module was added for fairness but no pretrained language-vision alignment leveraged.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Not applicable (no vision-language pretraining). Using only 80 demonstrations, DP obtains low success rates: typically below 50% in-domain for easier tasks and nearly zero in-the-wild for novel objects/environments per paper descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Baseline DP (no large pretraining) is the primary baseline; poor in-the-wild performance (near-zero) and sub-50% in-domain on most tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>DP trained from 80 demonstrations performs poorly; paper contrasts that FP3 achieves strong performance with same 80 demos, indicating large pretraining yields substantial sample efficiency gains (qualitative and aggregated quantitative comparisons provided).</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>None provided for DP in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>None provided for DP in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Behaviorally DP often fails to recover or make precise interactions; no evidence of robust language-action grounding reported.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>None.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Image-only observation leads to sensitivity to lighting, viewpoint and background changes, causing transfer failures.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>DP often fails on novel objects/environments (near-zero success in-the-wild).</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Does not demonstrate successful zero-shot; few-shot (80 demos) insufficient.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>No.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>No explicit negative transfer, but clear failure to generalize.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1912.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1912.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DP3</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DP3 (3D Diffusion Policy / 3D point-cloud diffusion policy)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A diffusion-based imitation policy that uses 3D point-cloud observations and a lightweight point-cloud encoder; used as a baseline to compare to FP3's large Uni3D-based 3D approach.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>3d diffusion policy: Generalizable visuomotor policy learning via simple 3d representations</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DP3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Diffusion policy architecture similar to DP but with 3D point-cloud inputs encoded by a lightweight point-cloud encoder (smaller than Uni3D); predicts continuous actions via denoising.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>No large-scale pretraining in this paper's experiments; evaluated as a small 3D baseline trained on the 80 demonstration post-training dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Only small post-training datasets used in baseline experiments (no large diverse pretraining).</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Real-world robotic manipulation (same four downstream tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Same tasks as FP3; uses point-cloud observations (no large pretraining) with smaller encoder; shows improved robustness relative to image-only DP on some axes (lighting/color) but limited by in-domain performance.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>No vision-language pretraining; language-conditioning module added similar to FP3 for fairness but no pre-aligned semantic features leveraged.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Not applicable (no VLM pretraining used). As a small 3D baseline trained on 80 demos, DP3 has modest in-domain performance (often <50% on harder tasks) and near-zero in-the-wild for many tasks; DP3 shows stability to lighting/color but limited overall generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>This describes DP3's actual evaluation mode in the paper; limited success in-domain and poor in-the-wild.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>DP3 trained on 80 demos underperforms compared to FP3 which uses large pretraining, indicating FP3 requires fewer post-training demos for high performance.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>None.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>None.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>No explicit grounding analysis; DP3 failures often due to imprecise interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>None.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>3D point cloud input helps robustness to lighting and color changes compared to image-only DP, but transfer is still limited without large-scale pretraining and model capacity.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Per paper, DP3 fails on novel objects/environments when trained only with small post-training data.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>No successful zero-shot reported; few-shot (80 demos) insufficient for generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>None.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>None beyond overall poor generalization.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1912.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1912.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OpenVLA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenVLA (Open Vision-Language-Action)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large image-based Vision-Language-Action model (VLA) used as a baseline; in this paper, OpenVLA performed poorly when fine-tuned with the small post-training dataset and struggled with action chunking and precise interaction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Openvla: An open-source vision-language-action model</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenVLA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained vision-language model adapted to action prediction (VLA). In this work used as a large 2D foundational policy baseline (image-only observation, pretraining from VLMs), but apparently lacking action chunking and history in the tested variant.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Vision-language pretraining on large image-text corpora (original OpenVLA work); in this paper OpenVLA is used as a pre-trained baseline fine-tuned to tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Large-scale image-text pretraining (web-scale image-caption pairs) and potentially foundation policy pretraining in original OpenVLA work; not robot-trajectory/action-specific.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Real-world robotic manipulation (same four downstream tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Applied to continuous robotic manipulation actions using only third-person image observation (no wrist-view/historical observation), leading to limited field of view in tasks that require close-up interaction.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>OpenVLA brings image-text alignment by pretraining, but paper reports its architecture lacked action chunking and observation history which limits control performance despite VLM knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>In this paper OpenVLA struggled on tasks with 80 demos and often failed to complete tasks, exhibiting failure modes like getting stuck and inability to precisely interact; specific aggregated numbers reported as very low success (near-zero in many cases).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not applicable — OpenVLA was used as a pre-trained VLA baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>OpenVLA with 80 demos performed poorly compared to FP3, suggesting that image-based VLA pretraining alone (without 3D geometric inputs and diffusion architecture tuned for action chunking) did not yield sample-efficient manipulation in these tests.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>No.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>No.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Behavioral: OpenVLA tended to get stuck and fail to interact precisely, suggesting insufficient grounding in motor/affordance space for the tested tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>No.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Lacking wrist-view and action chunking harmed OpenVLA's performance; single third-person image observation made close-range manipulations challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>OpenVLA performed poorly both in-domain and especially in-the-wild (near-zero on novel objects per qualitative description).</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not effective in few-shot (80 demos) regime in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>No.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>No explicit negative transfer quantified, but empirical failures indicate pretraining did not translate to robust manipulation without the right observation/modalities and action representation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1912.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1912.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RDT / RDT-1b</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RDT (RDT-1b: diffusion foundation model for bimanual manipulation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mentioned related work: a diffusion-transformer foundation model for manipulation (bimanual) that scales up diffusion transformers; compared architecturally to FP3 but uses 2D image observations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Rdt-1b: a diffusion foundation model for bimanual manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RDT / RDT-1b</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large diffusion-transformer policy for bimanual manipulation (related work). Uses diffusion transformers with cross-attention conditioning (per referenced description) and 2D image observations.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Large-scale imitation/pretraining on multi-task robotic datasets (2D image observations) in the referenced work.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Large multi-task robot demonstration datasets with image observations (details in original RDT paper, not in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Robotic manipulation (bimanual) in referenced work</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Bimanual manipulation tasks in real-world/benchmarked datasets (details in original paper).</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Mentioned as an architecture baseline — no detailed semantic alignment discussion in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>This paper notes RDT employs cross-attention conditioning while FP3 uses adaLN blocks, but no empirical attention analysis is provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Mentioned that RDT (image-only) may have limited generalizability compared to 3D-based FP3.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1912.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e1912.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RT-2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RT-2</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mentioned related work: a vision-language-action model that transfers web-scale VLM knowledge to robotic control (2D image-based VLA).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Rt-2: Vision-languageaction models transfer web knowledge to robotic control</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RT-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Vision-language-action model that leverages pre-trained VLMs to inform robotic policies (image-based VLA); referenced in related work as part of the VLA family.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Vision-language pretraining on large image-text corpora (external work).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Large-scale image-text (web) corpora; not robot-action specific.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Robotic control / manipulation (referenced work)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Image-conditioned instruction-following robotic tasks (referenced work).</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>RT-2 leverages VLM alignment to transfer semantic knowledge; in this paper it is referenced but not evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1912.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e1912.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>π0 (pi-zero)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>π0 (pi-zero) vision-language-action flow model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mentioned related work: a model that combines a pre-trained Vision-Language Model backbone with diffusion/flow-matching models for robot control (VLA + diffusion).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>0 : A visionlanguage-action flow model for general robot control</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>π0 (pi-zero)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Combines VLM backbone with diffusion/flow-matching generative control models to produce actions conditioned on language and vision (referenced as a promising architecture hybrid).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Vision-language pretraining for backbone plus generative control modeling (in referenced work).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Large image-text and robotic demonstration datasets (details in referenced work).</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>General robot control / manipulation (referenced work)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Language-conditioned action generation using diffusion/flow models and VLM features (referenced work).</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Mentioned as an approach to more tightly integrate VLMs with generative policy models; paper suggests combining FP3 with VLMs like π0 as future work.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Droid: A large-scale in-the-wild robot manipulation dataset <em>(Rating: 2)</em></li>
                <li>Diffusion policy: Visuomotor policy learning via action diffusion <em>(Rating: 2)</em></li>
                <li>3d diffusion policy: Generalizable visuomotor policy learning via simple 3d representations <em>(Rating: 2)</em></li>
                <li>Openvla: An open-source vision-language-action model <em>(Rating: 2)</em></li>
                <li>Uni3d: Exploring unified 3d representation at scale <em>(Rating: 2)</em></li>
                <li>Rdt-1b: a diffusion foundation model for bimanual manipulation <em>(Rating: 2)</em></li>
                <li>Rt-2: Vision-languageaction models transfer web knowledge to robotic control <em>(Rating: 1)</em></li>
                <li>0 : A visionlanguage-action flow model for general robot control <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1912",
    "paper_id": "paper-276937820",
    "extraction_schema_id": "extraction-schema-39",
    "extracted_data": [
        {
            "name_short": "FP3",
            "name_full": "3D Foundation Policy (FP3)",
            "brief_description": "A 1.3B-parameter diffusion-transformer language-visuomotor foundation policy that ingests 3D point clouds (two camera views), CLIP language embeddings, and proprioception; pre-trained on 60k robotic manipulation demonstrations and fine-tuned with LoRA on small task-specific datasets for sample-efficient real-robot manipulation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "FP3",
            "model_description": "Encoder-decoder diffusion transformer policy. Inputs: two-view 3D point clouds encoded by a 300M-parameter Uni3D ViT (fine-tuned), frozen CLIP language embedding as instruction, and robot proprioception; Transformer encoder fuses multimodal tokens, Transformer decoder denoises action chunks using adaLN conditioning; predicts continuous Cartesian actions (action chunks) via DDPM/DDIM denoising.",
            "pretraining_type": "imitation/demonstration pretraining on multi-task robotic data (3D point cloud observations) with a pre-trained 3D encoder aligned to image-text space",
            "pretraining_data_description": "Pre-trained on 60k demonstrations (subset of DROID: originally 76k demos) across 86 tasks and 564 scenes (depth + RGB used to recover point clouds). Data contains multi-task manipulation trajectories (actions), objects and scene variations, but not explicit large-scale natural-language instruction supervision beyond CLIP alignment used in Uni3D encoder pretraining.",
            "target_task_name": "Real-world robotic manipulation (multi-task language-conditioned visuomotor control)",
            "target_task_description": "Four downstream real-robot tasks evaluated: Fold Towel, Clean Table (pick and drop a small item into bin), Stand up Cup (upright a lying cup), Pour Water (grasp bottle, pour into cup, place bottle). Continuous Cartesian action space (absolute Cartesian-space control) recorded at 15 Hz; objects vary in appearance and size; evaluation on real Franka Emika Panda robot with wrist and third-person ZED cameras; both in-domain (seen env/objects) and in-the-wild (novel environments/objects) real-world tests.",
            "semantic_alignment": "Partially discussed: Uni3D encoder used was pre-trained to align 3D point-cloud features with image-text aligned features (explicitly stated), providing some semantic alignment between 3D perception and language; CLIP embeddings supply language-to-visual alignment for instructions, but paper notes this is a simple conditioning and may be insufficient for complex/dynamic language grounding.",
            "performance_with_language_pretraining": "Post-training FP3 (pretrained + 80 fine-tune demos) achieves very high success: per-paper claims 'learn a new task with over 90% success rates in novel environments with unseen objects' and average in-the-wild success 'over 80%'; Clean Table ablation table reports FP3 (ours) = 100% in-domain, 95% in-the-wild (success rate).",
            "performance_without_language_pretraining": "FP3-Scratch (same architecture trained from scratch on the small post-training data) performs poorly: e.g., Clean Table FP3-Scratch = 35% in-domain, 0% in-the-wild (success rate). General baselines without large pretraining (DP/DP3/OpenVLA) have much lower success (often &lt;50% in-domain and near-zero in-the-wild for many tasks); paper also notes classical small policies typically need ~200 expert episodes to learn a new task.",
            "sample_efficiency_comparison": "Explicitly reported: FP3 requires only 80 post-training demonstrations (10 per environment × 8 envs) to fine-tune to new tasks and reach &gt;90% success in novel environments; baseline small models typically trained from scratch require ~200 demonstrations (cited as typical) and failed to generalize in the 80-demo regime. FP3 fine-tuning time: ~2 hours on single A800 GPU.",
            "has_sample_efficiency_data": true,
            "attention_analysis": "No explicit attention-map or transformer attention analyses are presented. The paper describes architecture choices (adaLN conditioning, encoder-decoder fusion) but provides no visualization or quantitative analysis of attention focusing on semantic regions or affordances.",
            "embedding_space_analysis": "Partial: the paper states Uni3D was pre-trained to align 3D point-cloud features with image-text aligned features (semantic alignment claim), but provides no empirical embedding-space analyses (e.g., clustering, t-SNE) or quantitative probes in this work.",
            "action_grounding_evidence": "Behavioral evidence: language-conditioned policy can follow different language instructions in the same initial state (instruction-following evaluation) and execute the corresponding tasks; FP3's superior success and qualitative observations (more precise and smoother actions, reasonable recovery attempts after failure) support language-to-action grounding, but no direct mechanistic probes (e.g., causal interventions, grounding attribution) are provided.",
            "hierarchical_features_evidence": "No explicit multi-level feature (low-level vs high-level) analysis is given. Ablations show that 3D geometric features (point cloud input) confer robustness where image-only variants drop substantially in the wild, suggesting higher-level geometric representations help transfer, but no layerwise feature hierarchy study is reported.",
            "transfer_conditions": "Transfer success is influenced by observation modality (3D point clouds help), pretraining scale (model + data scaling improves generalization), and calibration (consistent world-frame point clouds improve view robustness). Transfer fails when pretraining is absent or data diversity is limited; image-only variants and small models generalize poorly to camera-view, lighting, and novel-object shifts.",
            "novel_vs_familiar_objects": "Evaluated: FP3 was fine-tuned on 8 environments/objects (80 demos) and tested zero-shot on 4 unseen environments/objects; FP3 retained high performance (e.g., averaged &gt;80% in-the-wild; Clean Table 95% in-the-wild), whereas baselines and FP3-Scratch largely failed on unseen objects (near-zero).",
            "zero_shot_or_few_shot": "Few-shot: FP3 demonstrates few-shot fine-tuning (80 demonstrations) that yields strong zero-shot deployment to novel objects/environments; paper claims 'capable of zero-shot generalization to both novel objects and environments with about 90% success rate' after the 80-demo post-training. Pure zero-shot from base model (no post-training) is limited per paper.",
            "layer_analysis": "Some component/scale ablations performed: model-size ablation (FP3-Base: 365M vs FP3 1.3B) and pretraining-data-size ablation (FP3-Base-30k vs FP3-Base 60k). They also report image-only variant (FP3-Base-Image) to test observation modality. There is no fine-grained per-layer importance analysis or systematic freezing/probing beyond these ablations. Uni3D encoder is fine-tuned during pre-training (they state freezing harms performance based on prior works).",
            "negative_transfer_evidence": "No direct evidence that language pretraining hurt downstream performance is reported. The paper does note limited zero-shot capability of the base model and that simple CLIP conditioning may be insufficient for complex instructions, but not negative transfer.",
            "comparison_to_vision_only": "Yes: explicit comparisons. Image-only variant (FP3-Base-Image) performs similarly in-domain but much worse in-the-wild (example Clean Table: FP3-Base-Image = 90% in-domain, 55% in-the-wild vs FP3-Base = 95% in-domain, 90% in-the-wild), demonstrating 3D point cloud inputs outperform image-only under cross-domain shifts.",
            "temporal_dynamics": "No explicit analysis of representation or performance dynamics over fine-tuning steps beyond reporting total pre-training (3M steps) and fine-tuning times; no staged learning curves presented.",
            "dimensionality_analysis": "No analysis of representational dimensionality (PCA/intrinsic dimension) is reported.",
            "uuid": "e1912.0"
        },
        {
            "name_short": "Uni3D",
            "name_full": "Uni3D ViT (large)",
            "brief_description": "A pre-trained large-scale 3D point-cloud encoder (Uni3D ViT) used to extract semantic + geometric embeddings from 4000-point point clouds per view; reported to have been pre-trained to align 3D point features with image-text aligned features.",
            "citation_title": "Uni3d: Exploring unified 3d representation at scale",
            "mention_or_use": "use",
            "model_name": "Uni3D ViT",
            "model_description": "A Vision Transformer-style encoder for 3D point clouds (used here as 300M-parameter encoder). Pre-trained to align point-cloud features with image-text aligned feature space, producing embeddings that the FP3 policy fine-tunes during pre-training.",
            "pretraining_type": "3D representation pretraining with alignment to image-text (vision-language aligned feature space)",
            "pretraining_data_description": "Not fully detailed in this paper; described as large-scale 3D pretraining that aligns point-cloud features with image-text aligned features (implies use of paired 3D and image-text supervision or distillation to image-text models).",
            "target_task_name": "Point-cloud encoding for robotic manipulation policy",
            "target_task_description": "Encodes two-view point clouds (wrist and third-person) for downstream manipulation control tasks in real robot environment; used with temporal stacking (history length 2) and color channels preserved for experiments conditioned on color.",
            "semantic_alignment": "Explicitly stated that Uni3D was pre-trained to align 3D point-cloud features with image-text aligned features — this is used by FP3 to provide semantically rich 3D features.",
            "performance_with_language_pretraining": "Not independently reported for Uni3D in this paper; its inclusion in FP3 correlates with improved sample-efficiency and cross-domain generalization (FP3 results).",
            "performance_without_language_pretraining": "Not reported in isolation; the paper mentions fine-tuning the Uni3D weights during policy pretraining rather than freezing them (citing prior works where freezing harms performance).",
            "sample_efficiency_comparison": "Not separately quantified for Uni3D alone; it's a component contributing to the FP3 overall sample efficiency (80-demo fine-tune).",
            "has_sample_efficiency_data": false,
            "attention_analysis": "No per-encoder attention or feature-visualization analysis in this paper.",
            "embedding_space_analysis": "Claimed alignment to image-text feature space but no analysis presented here.",
            "action_grounding_evidence": "Indirect: Uni3D provides semantically aligned 3D features that support language-conditioned action prediction in FP3, but no direct mechanistic grounding experiments isolating Uni3D are reported.",
            "hierarchical_features_evidence": "None reported in this paper.",
            "transfer_conditions": "Uni3D's 3D geometric encoding is highlighted as improving robustness to camera viewpoint, lighting, and background changes when compared to image-only representations.",
            "novel_vs_familiar_objects": "No isolated evaluation of Uni3D alone; FP3 using Uni3D generalizes well to novel objects.",
            "zero_shot_or_few_shot": "Not applicable in isolation.",
            "layer_analysis": "No layerwise probing of Uni3D in this paper.",
            "negative_transfer_evidence": "None reported.",
            "uuid": "e1912.1"
        },
        {
            "name_short": "CLIP",
            "name_full": "CLIP (Contrastive Language–Image Pretraining)",
            "brief_description": "A frozen CLIP model is used to embed natural language instructions into a language embedding that conditions the FP3 policy; the CLIP embedding is used without further fine-tuning in FP3.",
            "citation_title": "Learning transferable visual models from natural language supervision",
            "mention_or_use": "use",
            "model_name": "CLIP",
            "model_description": "A vision-language contrastive model that produces aligned image and text embeddings; FP3 uses the frozen CLIP text encoder to embed language instructions as conditioning tokens.",
            "pretraining_type": "vision-language pretraining on large image-text pairs (contrastive)",
            "pretraining_data_description": "Large-scale internet image-text pairs containing object labels and scene descriptions; not robot-action-specific and not containing robot trajectories or fine-grained motor affordance supervision.",
            "target_task_name": "Language-conditioned manipulation (as instruction encoder)",
            "target_task_description": "Embeds natural language task instructions which are provided to the FP3 policy at inference/fine-tuning time to condition continuous action prediction for manipulation tasks in real-world robot setups.",
            "semantic_alignment": "CLIP provides a semantic alignment between instruction tokens and visual concepts; FP3 uses this embedding but the paper notes CLIP conditioning is a simple approach and may be insufficient for complex/dynamic instruction grounding.",
            "performance_with_language_pretraining": "FP3 with frozen CLIP embeddings supports instruction following and high downstream success after fine-tuning (FP3 results), but no ablation isolating CLIP vs alternative language encoders is reported.",
            "performance_without_language_pretraining": "Not reported. The paper does not test variants without CLIP conditioning for language instructions (though it notes CLIP is frozen).",
            "sample_efficiency_comparison": "Not reported specifically for CLIP conditioning.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "No analysis of attention on CLIP-derived tokens.",
            "embedding_space_analysis": "No internal analysis of CLIP embedding interactions with Uni3D embeddings in this paper.",
            "action_grounding_evidence": "Behavioral instruction-following results indicate CLIP embeddings suffice for the evaluated instruction types given FP3 pretraining + fine-tuning, but authors caution CLIP may be insufficient for complex/dynamic instructions.",
            "hierarchical_features_evidence": "None reported.",
            "transfer_conditions": "Paper notes combining stronger VLMs with diffusion-based policies (e.g., π0) is a promising direction; suggests CLIP conditioning alone is limited.",
            "novel_vs_familiar_objects": "Not specifically analyzed for CLIP encoding alone.",
            "zero_shot_or_few_shot": "CLIP used as frozen encoder; FP3 demonstrates few-shot fine-tuning; CLIP's role in pure zero-shot is not isolated.",
            "layer_analysis": "No.",
            "negative_transfer_evidence": "Authors mention CLIP embedding is insufficiently expressive for complicated/dynamic language, but no evidence of active harm.",
            "uuid": "e1912.2"
        },
        {
            "name_short": "DP (Diffusion Policy)",
            "name_full": "Diffusion Policy",
            "brief_description": "A classic diffusion-based visuomotor imitation-learning policy operating on 2D image observations used as a baseline; in this paper, augmented with language-conditioning to match FP3's inputs for fair comparison.",
            "citation_title": "Diffusion policy: Visuomotor policy learning via action diffusion",
            "mention_or_use": "use",
            "model_name": "DP (image-based diffusion policy)",
            "model_description": "Conditional action diffusion model that denoises action trajectories conditioned on past observations (here: 2D images) and proprioception; in the paper used as a small 2D baseline (with added language-conditioning module).",
            "pretraining_type": "no large-scale pretraining reported in this work (trained from scratch or on small task data for baseline experiments); original DP is an imitation-learning method.",
            "pretraining_data_description": "Baseline training used only the small post-training dataset (80 demos) in experiments — no large diverse pretraining as with FP3.",
            "target_task_name": "Real-world robotic manipulation (same four downstream tasks)",
            "target_task_description": "Same continuous-action real-robot tasks; uses 2D image-only observations which are more sensitive to lighting, background, and camera-view shifts.",
            "semantic_alignment": "No vision-language pretraining; language-conditioning module was added for fairness but no pretrained language-vision alignment leveraged.",
            "performance_with_language_pretraining": "Not applicable (no vision-language pretraining). Using only 80 demonstrations, DP obtains low success rates: typically below 50% in-domain for easier tasks and nearly zero in-the-wild for novel objects/environments per paper descriptions.",
            "performance_without_language_pretraining": "Baseline DP (no large pretraining) is the primary baseline; poor in-the-wild performance (near-zero) and sub-50% in-domain on most tasks.",
            "sample_efficiency_comparison": "DP trained from 80 demonstrations performs poorly; paper contrasts that FP3 achieves strong performance with same 80 demos, indicating large pretraining yields substantial sample efficiency gains (qualitative and aggregated quantitative comparisons provided).",
            "has_sample_efficiency_data": true,
            "attention_analysis": "None provided for DP in this paper.",
            "embedding_space_analysis": "None provided for DP in this paper.",
            "action_grounding_evidence": "Behaviorally DP often fails to recover or make precise interactions; no evidence of robust language-action grounding reported.",
            "hierarchical_features_evidence": "None.",
            "transfer_conditions": "Image-only observation leads to sensitivity to lighting, viewpoint and background changes, causing transfer failures.",
            "novel_vs_familiar_objects": "DP often fails on novel objects/environments (near-zero success in-the-wild).",
            "zero_shot_or_few_shot": "Does not demonstrate successful zero-shot; few-shot (80 demos) insufficient.",
            "layer_analysis": "No.",
            "negative_transfer_evidence": "No explicit negative transfer, but clear failure to generalize.",
            "uuid": "e1912.3"
        },
        {
            "name_short": "DP3",
            "name_full": "DP3 (3D Diffusion Policy / 3D point-cloud diffusion policy)",
            "brief_description": "A diffusion-based imitation policy that uses 3D point-cloud observations and a lightweight point-cloud encoder; used as a baseline to compare to FP3's large Uni3D-based 3D approach.",
            "citation_title": "3d diffusion policy: Generalizable visuomotor policy learning via simple 3d representations",
            "mention_or_use": "use",
            "model_name": "DP3",
            "model_description": "Diffusion policy architecture similar to DP but with 3D point-cloud inputs encoded by a lightweight point-cloud encoder (smaller than Uni3D); predicts continuous actions via denoising.",
            "pretraining_type": "No large-scale pretraining in this paper's experiments; evaluated as a small 3D baseline trained on the 80 demonstration post-training dataset.",
            "pretraining_data_description": "Only small post-training datasets used in baseline experiments (no large diverse pretraining).",
            "target_task_name": "Real-world robotic manipulation (same four downstream tasks)",
            "target_task_description": "Same tasks as FP3; uses point-cloud observations (no large pretraining) with smaller encoder; shows improved robustness relative to image-only DP on some axes (lighting/color) but limited by in-domain performance.",
            "semantic_alignment": "No vision-language pretraining; language-conditioning module added similar to FP3 for fairness but no pre-aligned semantic features leveraged.",
            "performance_with_language_pretraining": "Not applicable (no VLM pretraining used). As a small 3D baseline trained on 80 demos, DP3 has modest in-domain performance (often &lt;50% on harder tasks) and near-zero in-the-wild for many tasks; DP3 shows stability to lighting/color but limited overall generalization.",
            "performance_without_language_pretraining": "This describes DP3's actual evaluation mode in the paper; limited success in-domain and poor in-the-wild.",
            "sample_efficiency_comparison": "DP3 trained on 80 demos underperforms compared to FP3 which uses large pretraining, indicating FP3 requires fewer post-training demos for high performance.",
            "has_sample_efficiency_data": true,
            "attention_analysis": "None.",
            "embedding_space_analysis": "None.",
            "action_grounding_evidence": "No explicit grounding analysis; DP3 failures often due to imprecise interactions.",
            "hierarchical_features_evidence": "None.",
            "transfer_conditions": "3D point cloud input helps robustness to lighting and color changes compared to image-only DP, but transfer is still limited without large-scale pretraining and model capacity.",
            "novel_vs_familiar_objects": "Per paper, DP3 fails on novel objects/environments when trained only with small post-training data.",
            "zero_shot_or_few_shot": "No successful zero-shot reported; few-shot (80 demos) insufficient for generalization.",
            "layer_analysis": "None.",
            "negative_transfer_evidence": "None beyond overall poor generalization.",
            "uuid": "e1912.4"
        },
        {
            "name_short": "OpenVLA",
            "name_full": "OpenVLA (Open Vision-Language-Action)",
            "brief_description": "A large image-based Vision-Language-Action model (VLA) used as a baseline; in this paper, OpenVLA performed poorly when fine-tuned with the small post-training dataset and struggled with action chunking and precise interaction.",
            "citation_title": "Openvla: An open-source vision-language-action model",
            "mention_or_use": "use",
            "model_name": "OpenVLA",
            "model_description": "Pretrained vision-language model adapted to action prediction (VLA). In this work used as a large 2D foundational policy baseline (image-only observation, pretraining from VLMs), but apparently lacking action chunking and history in the tested variant.",
            "pretraining_type": "Vision-language pretraining on large image-text corpora (original OpenVLA work); in this paper OpenVLA is used as a pre-trained baseline fine-tuned to tasks.",
            "pretraining_data_description": "Large-scale image-text pretraining (web-scale image-caption pairs) and potentially foundation policy pretraining in original OpenVLA work; not robot-trajectory/action-specific.",
            "target_task_name": "Real-world robotic manipulation (same four downstream tasks)",
            "target_task_description": "Applied to continuous robotic manipulation actions using only third-person image observation (no wrist-view/historical observation), leading to limited field of view in tasks that require close-up interaction.",
            "semantic_alignment": "OpenVLA brings image-text alignment by pretraining, but paper reports its architecture lacked action chunking and observation history which limits control performance despite VLM knowledge.",
            "performance_with_language_pretraining": "In this paper OpenVLA struggled on tasks with 80 demos and often failed to complete tasks, exhibiting failure modes like getting stuck and inability to precisely interact; specific aggregated numbers reported as very low success (near-zero in many cases).",
            "performance_without_language_pretraining": "Not applicable — OpenVLA was used as a pre-trained VLA baseline.",
            "sample_efficiency_comparison": "OpenVLA with 80 demos performed poorly compared to FP3, suggesting that image-based VLA pretraining alone (without 3D geometric inputs and diffusion architecture tuned for action chunking) did not yield sample-efficient manipulation in these tests.",
            "has_sample_efficiency_data": true,
            "attention_analysis": "No.",
            "embedding_space_analysis": "No.",
            "action_grounding_evidence": "Behavioral: OpenVLA tended to get stuck and fail to interact precisely, suggesting insufficient grounding in motor/affordance space for the tested tasks.",
            "hierarchical_features_evidence": "No.",
            "transfer_conditions": "Lacking wrist-view and action chunking harmed OpenVLA's performance; single third-person image observation made close-range manipulations challenging.",
            "novel_vs_familiar_objects": "OpenVLA performed poorly both in-domain and especially in-the-wild (near-zero on novel objects per qualitative description).",
            "zero_shot_or_few_shot": "Not effective in few-shot (80 demos) regime in this paper.",
            "layer_analysis": "No.",
            "negative_transfer_evidence": "No explicit negative transfer quantified, but empirical failures indicate pretraining did not translate to robust manipulation without the right observation/modalities and action representation.",
            "uuid": "e1912.5"
        },
        {
            "name_short": "RDT / RDT-1b",
            "name_full": "RDT (RDT-1b: diffusion foundation model for bimanual manipulation)",
            "brief_description": "Mentioned related work: a diffusion-transformer foundation model for manipulation (bimanual) that scales up diffusion transformers; compared architecturally to FP3 but uses 2D image observations.",
            "citation_title": "Rdt-1b: a diffusion foundation model for bimanual manipulation",
            "mention_or_use": "mention",
            "model_name": "RDT / RDT-1b",
            "model_description": "Large diffusion-transformer policy for bimanual manipulation (related work). Uses diffusion transformers with cross-attention conditioning (per referenced description) and 2D image observations.",
            "pretraining_type": "Large-scale imitation/pretraining on multi-task robotic datasets (2D image observations) in the referenced work.",
            "pretraining_data_description": "Large multi-task robot demonstration datasets with image observations (details in original RDT paper, not in this paper).",
            "target_task_name": "Robotic manipulation (bimanual) in referenced work",
            "target_task_description": "Bimanual manipulation tasks in real-world/benchmarked datasets (details in original paper).",
            "semantic_alignment": "Mentioned as an architecture baseline — no detailed semantic alignment discussion in this paper.",
            "performance_with_language_pretraining": null,
            "performance_without_language_pretraining": null,
            "sample_efficiency_comparison": null,
            "has_sample_efficiency_data": false,
            "attention_analysis": "This paper notes RDT employs cross-attention conditioning while FP3 uses adaLN blocks, but no empirical attention analysis is provided here.",
            "embedding_space_analysis": null,
            "action_grounding_evidence": null,
            "hierarchical_features_evidence": null,
            "transfer_conditions": "Mentioned that RDT (image-only) may have limited generalizability compared to 3D-based FP3.",
            "novel_vs_familiar_objects": null,
            "zero_shot_or_few_shot": null,
            "layer_analysis": null,
            "negative_transfer_evidence": null,
            "uuid": "e1912.6"
        },
        {
            "name_short": "RT-2",
            "name_full": "RT-2",
            "brief_description": "Mentioned related work: a vision-language-action model that transfers web-scale VLM knowledge to robotic control (2D image-based VLA).",
            "citation_title": "Rt-2: Vision-languageaction models transfer web knowledge to robotic control",
            "mention_or_use": "mention",
            "model_name": "RT-2",
            "model_description": "Vision-language-action model that leverages pre-trained VLMs to inform robotic policies (image-based VLA); referenced in related work as part of the VLA family.",
            "pretraining_type": "Vision-language pretraining on large image-text corpora (external work).",
            "pretraining_data_description": "Large-scale image-text (web) corpora; not robot-action specific.",
            "target_task_name": "Robotic control / manipulation (referenced work)",
            "target_task_description": "Image-conditioned instruction-following robotic tasks (referenced work).",
            "semantic_alignment": "RT-2 leverages VLM alignment to transfer semantic knowledge; in this paper it is referenced but not evaluated.",
            "performance_with_language_pretraining": null,
            "performance_without_language_pretraining": null,
            "sample_efficiency_comparison": null,
            "has_sample_efficiency_data": false,
            "attention_analysis": null,
            "embedding_space_analysis": null,
            "action_grounding_evidence": null,
            "hierarchical_features_evidence": null,
            "transfer_conditions": null,
            "novel_vs_familiar_objects": null,
            "zero_shot_or_few_shot": null,
            "layer_analysis": null,
            "negative_transfer_evidence": null,
            "uuid": "e1912.7"
        },
        {
            "name_short": "π0 (pi-zero)",
            "name_full": "π0 (pi-zero) vision-language-action flow model",
            "brief_description": "Mentioned related work: a model that combines a pre-trained Vision-Language Model backbone with diffusion/flow-matching models for robot control (VLA + diffusion).",
            "citation_title": "0 : A visionlanguage-action flow model for general robot control",
            "mention_or_use": "mention",
            "model_name": "π0 (pi-zero)",
            "model_description": "Combines VLM backbone with diffusion/flow-matching generative control models to produce actions conditioned on language and vision (referenced as a promising architecture hybrid).",
            "pretraining_type": "Vision-language pretraining for backbone plus generative control modeling (in referenced work).",
            "pretraining_data_description": "Large image-text and robotic demonstration datasets (details in referenced work).",
            "target_task_name": "General robot control / manipulation (referenced work)",
            "target_task_description": "Language-conditioned action generation using diffusion/flow models and VLM features (referenced work).",
            "semantic_alignment": "Mentioned as an approach to more tightly integrate VLMs with generative policy models; paper suggests combining FP3 with VLMs like π0 as future work.",
            "performance_with_language_pretraining": null,
            "performance_without_language_pretraining": null,
            "sample_efficiency_comparison": null,
            "has_sample_efficiency_data": false,
            "attention_analysis": null,
            "embedding_space_analysis": null,
            "action_grounding_evidence": null,
            "hierarchical_features_evidence": null,
            "transfer_conditions": null,
            "novel_vs_familiar_objects": null,
            "zero_shot_or_few_shot": null,
            "layer_analysis": null,
            "negative_transfer_evidence": null,
            "uuid": "e1912.8"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Droid: A large-scale in-the-wild robot manipulation dataset",
            "rating": 2
        },
        {
            "paper_title": "Diffusion policy: Visuomotor policy learning via action diffusion",
            "rating": 2
        },
        {
            "paper_title": "3d diffusion policy: Generalizable visuomotor policy learning via simple 3d representations",
            "rating": 2
        },
        {
            "paper_title": "Openvla: An open-source vision-language-action model",
            "rating": 2
        },
        {
            "paper_title": "Uni3d: Exploring unified 3d representation at scale",
            "rating": 2
        },
        {
            "paper_title": "Rdt-1b: a diffusion foundation model for bimanual manipulation",
            "rating": 2
        },
        {
            "paper_title": "Rt-2: Vision-languageaction models transfer web knowledge to robotic control",
            "rating": 1
        },
        {
            "paper_title": "0 : A visionlanguage-action flow model for general robot control",
            "rating": 1
        }
    ],
    "cost": 0.02167975,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>11 Mar 2025</p>
<p>Shanghai AI Laboratory
11 Mar 20256933B720DFC8BE9E45EFD72CE9F1F231arXiv:2503.08950v1[cs.RO]
large-scale 3D foundation policy model for robotic manipulation.FP3 builds on a scalable diffusion transformer architecture and is pre-trained on 60k trajectories with point cloud observations.With the model design and diverse pre-training data, FP3 can be efficiently fine-tuned for downstream tasks while exhibiting strong generalization capabilities.Experiments on real robots demonstrate that with only 80 demonstrations, FP3 is able to learn a new task with over 90% success rates in novel environments with unseen objects, significantly surpassing existing robot foundation models.Visualizations and code are available at: FP3.</p>
<p>Fig. 1: Overview of 3D Foundation Policy (FP3), a 1.3B 3D point cloud-based language-visuomotor policy pre-trained on 60k episodes from the DROID dataset [35].FP3 supports data-efficient fine-tuning for downstream tasks, while demonstrating superior generalizability to unseen environments and novel objects.</p>
<p>Abstract-Following its success in natural language processing and computer vision, foundation models that are pre-trained on large-scale multi-task datasets have also shown great potential in robotics.However, most existing robot foundation models rely solely on 2D image observations, ignoring 3D geometric information, which is essential for robots to perceive and reason about the 3D world.In this paper, we introduce FP3, a first</p>
<p>I. INTRODUCTION</p>
<p>Learning-based policies have shown great effectiveness in robotic manipulation [6,80,12,75,36,3].However, these learned policies often show limited or even zero generalization capability to unseen scenarios, new objects, and distractors [66].Additionally, most current methods are trained on single or few tasks [12,75], requiring a relatively large amount of expert demonstrations (usually about 200 episodes) to learn a new task.In contrast, natural language processing (NLP) and computer vision (CV) have achieved remarkable success in developing foundation models that are trained on largescale data and diverse tasks, enabling them to generalize to arbitrary scenarios in the wild.Therefore, building a similar foundation model in robotic manipulation that can generalize to novel objects, scenes, and tasks becomes a promising topic [36,39,3].</p>
<p>Towards this goal of policy foundation models, there have been some initial attempts at vision-language-action (VLA) models [80,36,3], which build upon the vision-language models (VLM) trained on internet-scale data of vision and language to inherit the commonsense knowledge and finetune the VLMs on large-scale robotics datasets [46,35].In the meanwhile, works such as RDT [39] try to scale up diffusion models to build foundation policies.Despite significant progress, their generalizability remains limited when confronted with novel tasks, objects, scenes, and camera views, etc.</p>
<p>One potential limitation of current policy foundation models is their exclusive reliance on 2D image observations, lacking 3D observation inputs.However, 3D geometric information is significant for perceiving 3D environments and reasoning about spatial relationships [78,72,70,63].There have been works showing that 3D representations can improve the sample efficiency and generalizability of robotic manipulation policies [54,70,33,72].Among all the 3D representations such as RGB-D images, point clouds, voxels, and 3D Gaussians [34], point clouds are found to be the most effective [70].</p>
<p>In this work, we introduce 3D Foundation Policy (FP3), the first 3D point cloud-based language-visuomotor policy foundation model for robotic manipulation that exhibits strong generalizability and sample efficiency.To extract rich semantic and geometric representation from 3D point cloud observation, FP3 adopts a pre-trained large-scale point cloud encoder Uni3D [77].We further leverage an encoder-decoder Diffusion Transformer (DiT) architecture to integrate the point cloud representations, language embedding, and proprioception for denoising the actions.</p>
<p>With the proposed policy architecture, we employ a pre-training&amp;post-training recipe for FP3, mirroring the common practice in large language models (LLMs) [1,57] where the model is pre-trained on large-scale diverse corpus and finetuned on curated task-specific data to adapt for downstream tasks.We first pre-train FP3 on the large-scale robotic manipulation dataset DROID [35] which contains 76k demonstration trajectories or 350h of interaction data from 564 scenes and 86 tasks.We then collect a small amount of high-quality teleoperation data for several tasks and fine-tune FP3.The result indicates that our model can efficiently master a new task with only 80 post-training trajectories and is capable of zeroshot generalization to both novel objects and environments with about 90% success rate.In contrast, strong baselines like DP3 [70] and OpenVLA [36] almost completely fail in this setting.Due to the advantages of 3D representation, FP3 is also robust to background variations, lighting conditions, camera angles, and distractors.Finally, we conduct ablation studies to demonstrate that the 3D representation, data scaling, and model scaling all contribute to the model's superior performance.</p>
<p>We summarize our main contributions as follows:</p>
<p>1) We propose a novel diffusion-based 3D robot policy architecture, FP3. 2) We pre-train FP3 on large-scale robotic manipulation data with 3D observation, establishing a 1B-parameter 3D policy foundation model.3) We collect data on several new tasks and demonstrate the efficient and generalizable fine-tuning of FP3, achieving around 60% in-domain and 80% in-the-wild performance improvement on average over strong baselines with only 2-hour and single-GPU finetuning.</p>
<p>II. RELATED WORK</p>
<p>A. Foundation Models in Robotics</p>
<p>Similar to the cases in natural language processing and computer vision, foundation models have already been widely used in multiple aspects of robotics, including representation learning [44,42,64,67], high-level task planning [7,24], training-free robotic manipulation [28,26,27], etc.In this work, we concentrate on policy foundation models in robotics, which also often refer to multi-task "generalist" robot policies [6,80,36,58,39,3] trained on large-scale robot datasets [46,60,35,17].A significant subset of policy foundation models consists of autoregressive vision-language-action (VLA) models like RT-2 [80] and OpenVLA [36], which directly fine-tune pre-trained large vision-language models to predict actions by treating discretized actions as language tokens.Recently, RDT [39] scales up a diffusion transformer [47] to build a foundation model for bimanual manipulation.Another recent work π 0 [3] investigates the combination of a pre-trained VLM backbone with diffusion (flow matching [38]) models for robot control.Our work is most similar to RDT in terms of architecture, as both build upon diffusion transformers, but there are key differences, such as conditioning blocks.RDT employs cross attention blocks, while we opt for adaLN blocks [47,15] for stabilizing training.</p>
<p>In addition to architecture, a key difference between these approaches and FP3 is the observation modality.Unlike these works, which all take 2D image observation as input, our work utilizes 3D point cloud observations to enhance the perception of 3D geometric information and reasoning about spatial relationships, introducing the first policy foundation model with 3D representations to our knowledge.</p>
<p>B. Robotic Manipulation with 3D representations</p>
<p>Compared to 2D images, 3D representations such as RGB-D images, point clouds, and voxels contain richer geometric information and have thus been widely used in robotic manipulation [54,70,19].Kite [56] directly leverages the RGB-D observation for semantic manipulation.Other works [11,68,70,61] reconstruct point clouds from RGB-D images and process them using a point cloud encoder for manipulation.Voxelizing the point clouds for perception is also a viable solution [30,54,25].Another set of works [18,65,33,72,73] lifts 2D image features to 3D space to benefit from both semantic and geometric information.There have also been attempts in combining implicit or explicit 3D reconstruction (NeRF [43], 3D gaussians [34]) with robotic manipulation [69,29,14,76,41].In this work, we choose the point cloud as the 3D representation as it is found to be more effective than other representations in DP3 [70].</p>
<p>Despite the differences in representation, all the aforementioned methods consist of small networks trained on a limited number of tasks.FP3 differs from these works in several ways: it is a foundation model trained on a large multi-task dataset, it scales up the point cloud encoder to 300M parameters and the whole network to 1.3B parameters, and it supports efficient and generalizable fine-tuning for new tasks.</p>
<p>C. Diffusion models in Robotics</p>
<p>Diffusion models have achieved great success in image generation [21,55,53] and video generation [22,4,8,20] by modeling complex high-dimensional continuous distributions through progressive denoising.Due to their remarkable expressiveness, diffusion models have also been applied across various fields in robotics, including reinforcement learning [62,2,10], imitation learning [12,70,52,39,58,3], and motion planning [31,59,9].Our work focuses on "Diffusion Policies" [12], which refers to methods that directly employ conditional diffusion models as visuomotor policy models for imitation learning.Most closely related to our work is RDT [39], a diffusion foundation model for manipulation.A key distinction between our work and RDT is that our FP3 model leverages 3D point cloud representations to achieve improved data efficiency and generalizability.</p>
<p>III. METHOD</p>
<p>We introduce the 3D Foundation Policy (FP3) model for generalist robotic manipulation, achieving high data efficiency and generalization capability.FP3 is a 1.3B encoder-decoder transformer network following a two-stage pre-training and post-training recipe.We first provide the detailed architecture and key design decisions of FP3 in Section III-A.Then we describe the pre-training and post-training procedures in Section III-B and Section III-C, respectively.</p>
<p>A. FP3 model</p>
<p>At its core, FP3 is a diffusion-based policy model similar to [12,39].It takes the 3D point cloud observation, language, and robot proprioceptive state as input and predicts action chunks of future actions.Formally, we formalize the problem of language-conditioned visuomotor control as modeling the distribution p(A t |o t ), where o t = [P 1 t , ..., P n t , ℓ t , q t ] is the observation at time t including point cloud observation P i t from the i th camera (including historical observation), language instruction ℓ t and proprioceptive information q t and A t = [a t , a t+1 , ..., a t+H−1 ] denotes the predicted action chunk.We train a denoising diffusion probabilistic model (DDPM) [21] to approximate the conditional distribution and use the denoising diffusion implicit model (DDIM) [55] method to accelerate inference.</p>
<p>Now we describe the detailed structure of FP3 model, including the encoding of multi-modal inputs and the transformer-based encoder-decoder architecture.</p>
<p>Encoding of multi-modal inputs.To process the multimodal input, we encode the input signals into a unified token space with the same dimensions as follows:</p>
<p>• Point cloud observations contain rich semantic and geometric information and are found to be more suitable for policy learning compared to other 3D representations [70].Therefore, we consider using point cloud as the 3D representation in FP3.Current point cloud-based robot policies [70,61,73] typically use sparse point cloud and small networks such as PointNet++ [49] and PointNeXt [50] to encode the points into embeddings.However, pre-trained large-scale foundation vision encoders have demonstrated a performance advantage over small encoders in image-based policies [13,37].Consequently, we increase the number of input points to 4000 for each view and employ a 300M-parameter point cloud encoder Uni3D ViT [77] that is pre-trained to align the 3D point cloud features with the image-text aligned features to obtain the point cloud embeddings.For the third-personview and the wrist-view point clouds, we use separate encoders since their point distributions might be greatly different.Following [37], we choose to fine-tune the weights of Uni3D ViTs during policy training.[47,16] and policy learning [79,15], we adopt the transformer architecture and scale it up for FP3.To better fuse the point cloud, language, and proprioceptive state embeddings, we utilize a Transformer Encoder-Decoder architecture similar to [75,74,52,15].Specifically, FP3 first feeds all the embeddings into  [77] encoder.The language instruction ℓ t is embedded with a frozen CLIP [51] model.The Transformer encoder fuses multi-modal input embeddings to latent tokens, while the Transformer decoder takes in the noise actions and leverages adaLN [47,5,32] blocks to integrate the latent tokens generated by the encoder, predicting denoised action chunks.a transformer encoder, producing a sequence of informative latent tokens.</p>
<p>The diffusion denoiser of FP3 is a Transformer decoder that denoises the action chunks from noise with temporal causal masking following [79].To infuse the latent tokens with multimodal information into the denoiser, FP3 adopts the adaptive Layer-Norm (adaLN) module for conditioning, which has been found to be essential for implementing diffusion training in image generation [47,16] and policy learning [79,15].</p>
<p>B. Pre-training</p>
<p>Pre-training data.To build a 3D policy foundation model, we need to train our model on large-scale 3D robotic manipulation datasets.However, most existing large-scale robot datasets such as the Open X-Embodiment dataset (OXE, [46]) are mainly 2D-only.Thus, in this work, we pre-train FP3 with the DROID dataset [35], which includes 86 tasks and 76k demonstrations and provides depth observation data.We finally use 60k demonstrations from DROID to pre-train FP3.</p>
<p>Data pre-processing.While DROID uses three cameras for data collection, we only use two of them including one thirdview camera and one wrist-view camera in FP3 for convenience.We use the RGB image and the depth map to recover the 3D point cloud for each camera and transform the two point clouds to the same world frame.As we only care about the operated object, we cropped the points outside a 1-meter box to remove redundant points.Further, we downsample each point cloud by farthest point sampling (FPS, [48]) to 4000 points to facilitate model training while retaining sufficient information.We preserve the color channels of each point to enable further experiments conditioned on colors.</p>
<p>Pre-training details.Following prior works [36,37] which found that freezing pre-trained vision encoders may harm the policy performance, we fine-tune the Uni3D ViT encoder during pre-training.We also randomly drop some points during training for augmentation, and the dropout rate is randomly selected from 0 to 0.8.</p>
<p>We use the AdamW optimizer [40] with a cosine learning rate schedule.The weight decay is set to 0.1, and gradient clipping is set to 1.0.The FP3 base model is pre-trained for 3M steps with a batch size of 128 using 8 NVIDIA A800 GPUs, which takes about 48 hours.Fine-tuning the same model on a single NVIDIA A800 GPU takes approximately 2 hours and can be further sped up with multi-GPU training.</p>
<p>To handle the partial observation, we stack 2 frames as input, including 1 step observation history, to compensate for the missing dynamic information of the robot.</p>
<p>C. Post-training</p>
<p>After obtaining the pre-trained base model, we further employ a post-training process using a small amount of highquality data to adapt the model to certain tasks, which aligns with most modern LLM practice [1,57].Different from the fine-tuning settings adopted in most existing robot foundation models in which they focus on either fine-tuning the model to adapt to new robot setups [36,58] or learning new tasks in a fixed environment [39,3], our goal is to fine-tune our model to solve specific tasks on any object, in any environment.</p>
<p>To achieve this goal, we further collect data for each downstream task in our robot setup.Taking the lesson from Lin et al. [37], we aim to enhance the diversity of environments and objects rather than merely increasing the number of demonstrations in the same scenario.Specifically, for each task, we collect 10 teleoperation demonstrations in each of 8 environments using 8 unique objects, i.e., 80 demonstrations in total.We then fine-tune the base model on this data using the parameter-efficient fine-tuning strategy LoRA [23].Thanks to the effective initialization from pre-training, this small amount of fine-tuning data enables zero-shot deployment to novel environments and objects.We will discuss the tasks and results in detail in Section IV.</p>
<p>IV. EXPERIMENTS</p>
<p>We conduct experiments on real robots for four downstream tasks to investigate the following questions:</p>
<p>1) Can FP3 be efficiently fine-tuned for new tasks?2) How well does the fine-tuned FP3 generalize to unseen objects and scenes compared to the existing imitation learning (foundation) policies?3) How robust is FP3 to the environment perturbations, such as lighting, camera views, distractors, etc.? 4) Can FP3 correctly execute the corresponding tasks following the language instruction?</p>
<p>A. Experimental Setups</p>
<p>Real robot setup.As we pre-train our FP3 model on the DROID dataset, we also build a real robot setup similar to DROID for evaluating downstream tasks.This setup features a Franka Emika Panda robot arm equipped with a Robotiq gripper, mounted on a movable desk.For point cloud observation, we utilize one ZED mini camera in wrist view and one ZED 2 camera in third-person view.To collect data, we employ a Meta Quest 2 VR headset to teleoperate the robot.We record the absolute Cartesian Space Control as the action space for policy training and deployment.More details are provided in the Appendix.</p>
<p>Pour Water Clean Table Fold Towel</p>
<p>Stand up Cup Fig. 3: Task illustrations.We evaluate our model on four downstream tasks: Fold Towel, Clean Tasks.We choose four downstream tasks to evaluate our model and the baselines:</p>
<p>• Fold Towel: Fold a long towel from right to left on a flat surface.• Pour Water: Pick up a water bottle, pour the water from the water bottle into a cup, then place the water bottle on a coaster.Figure 3 illustrates the process of these four tasks.Further details are provided in the Appendix.Baselines.In order to comprehensively evaluate FP3, we carefully select three baselines:</p>
<p>• Diffusion Policy (DP) [12]: a classic diffusion-based imitation learning policy with 2D image observation.</p>
<p>• DP3 [70]: an alternative version of DP which changes the 2D image observation to 3D point cloud and designs a lightweight encoder to encode the point cloud.</p>
<p>• OpenVLA [36]: a most widely-used image-based Vision-Language-Action (VLA) model.These three baselines each represent a small 2D policy, a small 3D policy, and a large 2D foundational policy.For DP and DP3, we add a language-conditioning module in the same manner as FP3 to fuse language instructions.Metrics.We report the success rate as our metric.Results in Table I are averaged over 20 evaluation trials.</p>
<p>B. Efficient and generalizable fine-tuning for new tasks</p>
<p>We first evaluate FP3's capability to learn new tasks efficiently.We collect only 10 demonstrations per environmentobject pair for 8 pairs to obtain 80 demonstrations in total for each task.For DP and DP3, we use the demonstrations to train the policies, while for OpenVLA and our FP3, we follow the pre-training and post-training recipe to fine-tune the base models for each task.Additionally, we train FP3 from scratch to validate the necessity of pre-training.</p>
<p>For each task, we not only evaluate all the policies in four in-domain environments with seen objects, but also deploy them zero-shot in four out-of-domain environments with unseen objects, which is a huge challenge for the model's generalizability.</p>
<p>In-domain Performance.Results in Table I show that in in-domain experiments, with only 10 demonstrations per scene, DP and DP3 can somewhat handle easier tasks, even though the success rate is below 50% in most cases; however, they almost completely fail in the more difficult task Pour Water.And OpenVLA struggles to perform any task, likely due to the lack of action chunking.In contrast, thanks to pre-training and 3D representation, FP3 efficiently learns all tasks with a success rate exceeding 90%.Qualitatively, we find that the failures of all baseline algorithms are mainly due to issues in the details, such as not being precise enough when attempting to grasp objects, which causes the object to be pushed away, or the bottle opening being off-center when pouring water, etc.In contrast, due to its large number of parameters and extensive pre-training, our FP3 policy can better predict complex target actions.The actions predicted by the FP3 policy are significantly smoother and more precise, leading to a notably higher success rate compared to the strong baselines.</p>
<p>In-the-Wild Performance.We further move the robot arm to novel environments and evaluate the policies with unseen objects.In this challenging setting, we observe that all baseline policies without pre-training, including FP3-Scratch, often fail to recognize the target objects, resulting in near-zero performance as shown in Figure 4.In contrast, FP3 rarely encounters such situations and consistently performs well in all scenarios and tasks, achieving an average success rate of over 80%, which devastates all the baselines.We attribute the superior performance to our large-scale pre-training, as the pre-training data encompasses a wide variety of scenes TABLE I: Post-training Evaluation.We fine-tune FP3 and baseline methods on 80 demonstrations from 8 environments and evaluate them on four in-domain environments with seen objects and four in-the-wild environments with unseen objects, conducting 5 trials for each.FP3 significantly outperforms other policies both in domain and in the wild.</p>
<p>Fold Towel</p>
<p>Clean Table Stand  and objects, greatly enhancing the robustness of the policy.Furthermore, the point cloud observation is also a crucial factor, enabling better capture of geometric information, which is essential for cross-domain generalization.</p>
<p>Failure Analysis of Baselines.It is worth noting that in all cases, OpenVLA performs poorly, with the primary failure modes being its tendency to get stuck at specific positions and its inability to interact accurately with objects.The issue of getting stuck may stem from OpenVLA's lack of action chunking and observation history.The failure to interact precisely might be because OpenVLA uses only third-personview observation, which offers a restricted field of view.</p>
<p>Another interesting issue is the policy's response after an initial failure attempt.Sometimes, the policy fails in its first attempt, as seen in the Clean Table task where it fails to grasp the crumpled paper and ends up grabbing nothing.In such cases, we observe that only OpenVLA and FP3 make reasonable subsequent attempts, while DP, DP3, and FP3-Scratch tend to get stuck and simply wobble around the area after failure, unable to have a try again.This phenomenon happens probably because the fine-tuning data is limited, thus the policies without pre-training can fall into an out-of-distribution state after the first failure, and hence fail to output reasonable behaviors.Conversely, Large-scale pre-training with diverse tasks and objects in FP3 addresses this issue.</p>
<p>C. More experiments on generalization</p>
<p>Having demonstrated the efficient adaptation to new tasks and remarkable generalizability to novel objects and environments of FP3, we conduct more comprehensive experiments on FP3's generalizability to different environments and robot setups using the Clean Table task.Figure 5 demonstrates the results and visualizations.</p>
<p>Generalization to different object appearances, backgrounds, and lighting conditions.Conventional image-based policy networks are sensitive to visual variations.Therefore, we systematically alter one aspect of the in-domain environment: object appearance, background texture, or lighting condition, to evaluate the policies.The results indicate that the image-based DP experiences a performance decline compared to the in-domain results, particularly with modified color and background.In contrast, DP3's performance in lighting and object color generalization remains stable, as it eliminates the color channels and relies solely on point positions.However, it is still limited by in-domain performance.With the benefits of pre-training initialization and 3D geometry understanding, FP3 surpasses the baselines.Qualitatively, DP and DP3 occasionally struggle to accurately recognize objects or estimate their positions, while FP3 consistently performs well.</p>
<p>Generalization to new camera views.Camera view variations have also been a major challenge for image-based policies.Therefore, we adjust the camera view by approximately 30 degrees from the training data to evaluate the robustness of the policies.Once again, DP completely fails in this scenario, and DP3 is constrained by its in-domain performance, while FP3 maintains its high performance since the point cloud is consistently converted to the same coordinates as long as the camera is properly calibrated.</p>
<p>Generalization to distractors.We also try putting random distractors around the target object to assess the robustness of these policies.In such settings, we find that the policies may attempt to grasp interfering objects.This issue occurs in all methods, including FP3, yet FP3's performance remains the highest and most stable.</p>
<p>D. Instruction following</p>
<p>Since FP3 is a language-conditioned visuomotor policy, it's also important to evaluate its capability to execute tasks following the language command.Hence we fine-tune FP3 and the baselines in a multi-task setting using data from all tasks and evaluate them by providing different language instructions in the same initial state.Figure 6 demonstrates that FP3 can execute tasks according to different language commands within the same starting context, while the baseline methods either fail to complete the task or are disrupted by the target objects of other tasks.</p>
<p>E. Ablations</p>
<p>We finally do ablation studies on the observation choice, model size, and pre-training data size.We consider the following variants of FP3:</p>
<p>• FP3-Base reduces the transformer size of both the encoder and decoder in FP3 from ViT-Large to ViT-Base, decreasing the total parameters from 1.3B to 365M.• FP3-Base-30k further reduces the pre-training data of FP3-Base from 60k to 30k demonstrations.Table II presents the results of each variant on the Clean Table task.FP3-Scratch exhibits poor performance both in domain and in the wild, indicating the importance of pretraining.FP3-Base-Image's performance aligns with FP3-Base in domain, but there is a huge performance drop in the wild, highlighting the effectiveness of 3D point cloud representation.FP3-Base and FP3-Base-30k demonstrate similar performance, both lower than our final FP3.More challenging tasks and additional data points are likely required to draw more definitive conclusions about the precise scaling law of pre-training.V. LIMITATIONS While FP3 shows strong performance as a policy foundation model, it still has several limitations.One limitation is that although FP3 enables efficient and generalizable downstream fine-tuning, the base model exhibits limited zero-shot performance.One possible reason is that the pre-training dataset DROID is still not large enough compared to other 2D robotics datasets like OXE.Future work can consider collecting larger 3D robotics datasets for pre-training.Another limitation is that FP3 incorporates language conditioning through a simple CLIP embedding, which is insufficient to represent complicated and dynamic information.Combining diffusion-based FP3 with VLM to build a VLA model like π 0 [3] seems a promising future direction.Additionally, FP3 does not leverage the robust pre-trained 2D vision encoders like DINOV2 [45] and SigLIP [71].There is huge potential in merging 3D point cloud features with 2D image features or lifting the 2D features to 3D space.We leave these explorations for future work.</p>
<p>Training Scenes</p>
<p>FP3 (Ours) DP
Fold Towel Clean Table Stand Up Cup Pour Water Scene Illustration</p>
<p>VI. CONCLUSION</p>
<p>In this work, we present the 3D Foundation Policy (FP3), a large-scale Diffusion Transformer-based policy with 3D point cloud input.We pre-train FP3 on 60k episodes of robotic manipulation data and subsequently fine-tune it for downstream tasks.Through extensive experiments, we demonstrate that FP3 serves as an outstanding policy initialization for dataefficient and generalizable fine-tuning for new tasks.With only 80 demonstrations, FP3 can learn a new task with over 90% success rates in novel environments with unseen objects, significantly outperforming existing robot policies.We hope that our work will pave the way for more exciting advancements in robot foundation models utilizing 3D representations.</p>
<p>VII. ACKNOWLEDGMENT</p>
<p>Fig. 2 :
2
Fig.2: FP3 architecture.Each camera view's point cloud observation P i t (with history length of two) is encoded with a Uni3D ViT-L[77] encoder.The language instruction ℓ t is embedded with a frozen CLIP[51] model.The Transformer encoder fuses multi-modal input embeddings to latent tokens, while the Transformer decoder takes in the noise actions and leverages adaLN[47,5,32] blocks to integrate the latent tokens generated by the encoder, predicting denoised action chunks.</p>
<p>Fig. 4 :
4
Fig. 4: Visualizations of post-training environments and in-the-wild evaluations.The green boxes represent successful steps, while the red boxes represent failed ones.FP3 generalize well to all unseen environments and new objects, while Diffusion Policy often fails to recognize the target object or misses the target position.</p>
<p>Fig. 5 :
5
Fig.5: Generalization evaluation.We evaluate FP3 and baseline policies on a diverse set of tasks, covering different axes of generalization, including lighting, camera view, distractor, object and background.FP3 achieves outstanding performance in all generalization evaluation settings.</p>
<p>Fig. 6 :
6
Fig.6: Instruction following evaluation.We evaluate FP3 and baseline policies in the same initial state with different language instructions.FP3 can perfectly follow the instructions to execute the correct tasks rather than simply memorize the training distribution.</p>
<p>Pick up a crumpled piece of paper and put it in the bucket.</p>
<p>• Clean Table: • Stand up Cup: Stand a cup lying on a flat surface upright.</p>
<p>up Cup Pour Water Average In-domain In-the-wild In-domain In-the-wild In-domain In-the-wild In-domain In-the-wild In-domain In-the-wild
DP2007554505036.251.25DP320025104500022.502.50OpenVLA501551510007.503.75FP3-Scratch35535015035030.001.25FP3 (ours)9085100959575957595.0082.50</p>
<p>TABLE II :
II
Ablation study.We achieve the best performance when using 3D point cloud input, a larger model, and largerscale pre-training data.
Clean TableIn-domainIn-the-wildFP3-Scratch350FP3-Base-Image9055FP3-Base9590FP3-Base-30k9590FP3 (ours)10095
This work is supported by the National Natural Science Foundation of China (62176135), National Key R&amp;D Program of China (2022ZD0161700), Shanghai Qi Zhi Institute Innovation Program SQZ202306 and the Tsinghua University Dushi Program.We would like to express our gratitude to Tong Zhang and Yingdong Hu for their help with the robot hardware setup..We also appreciate the insightful discussions and feedback from Jiacheng You, Shengjie Wang and Chengbo Yuan.APPENDIXA. Environment VisualizationFor all four tasks, we collect post-training data in 8 environments and evaluate the policies in 4 in-domain environments and 4 unseen environments.We visualize all the scenes in Figure7.B. Object VisualizationSimilar to the scenes, we have 8 training objects and 4 unseen objects for each task.All the objects are visualized in Figure8.We list the training hyperparameters for pre-training and fine-tuning in TableIIIand TableIV.We train all models on 8 NVIDIA A800 GPUs.• Fold Towel: The robot is required to complete two steps: first, grasping the right edge of the towel, and second, folding the towel to the left.The towel's center position is randomly varied, and its orientation is randomly rotated within ±30 degrees.The towels have different colors, textures, and materials, but they are prefolded into approximately rectangular shapes of similar size.• Clean Table:The robot is required to complete two steps: first, grasping the spitball on the surface, and second, moving it just above the trash bin and dropping it into the bin.The positions of the spitball and the trash bin are randomly placed.The trash bins have different colors and textures, and their sizes vary to some extent.• Stand up Cup: The robot is required to complete two steps: first, it inserts the gripper into the cup's opening and grasps the cup, and second, it lifts and stands the cup upright.The cup's placement is random, with its orientation varying within the 180-degree range where the opening faces the robotic arm.• Pour Water: The robot performs three sequential actions: initially, it grasps a drink bottle; subsequently, it pours water into a mug; and finally, it places the bottle on a coaster.The position of the mug is randomly varied.The relative positions are also randomized, but the water bottle is always approximately to the left of the cup, while the coaster is approximately to the right of the cup.The water bottle, cup, and coaster vary in color, material, and size.As Figure9shows, we basically setup the robot following the DROID.We use a Franka Emika Panda robot arm equipped with a Robotiq gripper, mounted on a movable lifting desk.For point cloud observation, we use one ZED mini camera and one ZED 2 camera.The ZED 2 provides third-person perspective images, fixed on a movable lifting desk, while the ZED mini provides wrist perspective images, fixed on the camera's end effector.For data collection, we use a Meta Quest2 VR glass to teleoperate the robot with a control frequency of 15Hz.All policy evaluations are performed on an RTX 3090 GPU (24GB VRAM).And everything is powered by a mobile power supply (EcoFlow DELTA 2 Max).
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. Gpt-4 technical report. 2023arXiv preprint</p>
<p>Is conditional generative modeling all you need for decision making?. Anurag Ajay, Yilun Du, Abhi Gupta, Joshua B Tenenbaum, Tommi S Jaakkola, Pulkit Agrawal, The Eleventh International Conference on Learning Representations. </p>
<p>0 : A visionlanguage-action flow model for general robot control. Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, arXiv:2410.241642024arXiv preprint</p>
<p>Stable video diffusion: Scaling latent video diffusion models to large datasets. Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, arXiv:2311.151272023arXiv preprint</p>
<p>Large scale gan training for high fidelity natural image synthesis. Andrew Brock, Jeff Donahue, Karen Simonyan, International Conference on Learning Representations. 2018</p>
<p>Rt-1: Robotics transformer for real-world control at scale. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, arXiv:2212.068172022arXiv preprint</p>
<p>Do as i can, not as i say: Grounding language in robotic affordances. Anthony Brohan, Yevgen Chebotar, Chelsea Finn, Karol Hausman, Alexander Herzog, Daniel Ho, Julian Ibarz, Alex Irpan, Eric Jang, Ryan Julian, Conference on robot learning. PMLR2023</p>
<p>Video generation models as world simulators. Tim Brooks, Bill Peebles, Connor Holmes, Will Depue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, Aditya Ramesh, 2024</p>
<p>Motion planning diffusion: Learning and planning of robot motions with diffusion models. J Carvalho, A T Le, M Baierl, D Koert, J Peters, IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). 2023</p>
<p>Offline reinforcement learning via highfidelity generative behavior modeling. Huayu Chen, Cheng Lu, Chengyang Ying, Hang Su, Jun Zhu, The Eleventh International Conference on Learning Representations. </p>
<p>Polarnet: 3d point clouds for languageguided robotic manipulation. Shizhe Chen, Ricardo Garcia Pinel, Cordelia Schmid, Ivan Laptev, Conference on Robot Learning. PMLR2023</p>
<p>Diffusion policy: Visuomotor policy learning via action diffusion. Cheng Chi, Zhenjia Xu, Siyuan Feng, Eric Cousineau, Yilun Du, Benjamin Burchfiel, Russ Tedrake, Shuran Song, The International Journal of Robotics Research. 027836492412736682023</p>
<p>Cheng Chi, Zhenjia Xu, Chuer Pan, Eric Cousineau, Benjamin Burchfiel, Siyuan Feng, Russ Tedrake, Shuran Song, arXiv:2402.10329Universal manipulation interface: In-thewild robot teaching without in-the-wild robots. 2024arXiv preprint</p>
<p>Graspnerf: Multiview-based 6-dof grasp detection for transparent and specular objects using generalizable nerf. Qiyu Dai, Yan Zhu, Yiran Geng, Ciyu Ruan, Jiazhao Zhang, He Wang, 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE2023</p>
<p>The ingredients for robotic diffusion transformers. Sudeep Dasari, Oier Mees, Sebastian Zhao, Mohan Kumar Srirama, Sergey Levine, arXiv:2410.100882024arXiv preprint</p>
<p>Scaling rectified flow transformers for high-resolution image synthesis. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Forty-first International Conference on Machine Learning. 2024</p>
<p>Rh20t: A robotic dataset for learning diverse skills in one-shot. Hongjie Hao-Shu Fang, Zhenyu Fang, Jirong Tang, Junbo Liu, Haoyi Wang, Cewu Zhu, Lu, RSS 2023 Workshop on Learning for Task and Motion Planning. 2023</p>
<p>Act3d: 3d feature field transformers for multi-task robotic manipulation. Theophile Gervet, Nikolaos Zhou Xian, Katerina Gkanatsios, Fragkiadaki, 7th Annual Conference on Robot Learning. 2023</p>
<p>Rvt: Robotic view transformer for 3d object manipulation. Ankit Goyal, Jie Xu, Yijie Guo, Valts Blukis, Yu-Wei Chao, Dieter Fox, Conference on Robot Learning. PMLR2023</p>
<p>Seer: Language instructed video prediction with latent diffusion models. Xianfan Gu, Chuan Wen, Weirui Ye, Jiaming Song, Yang Gao, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Denoising diffusion probabilistic models. Jonathan Ho, Ajay Jain, Pieter Abbeel, Advances in neural information processing systems. 202033</p>
<p>Video diffusion models. Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, David J Fleet, Advances in Neural Information Processing Systems. 202235</p>
<p>Low-rank adaptation of large language models. J Edward, Phillip Hu, Zeyuan Wallis, Yuanzhi Allen-Zhu, Shean Li, Lu Wang, Weizhu Wang, Chen, International Conference on Learning Representations. </p>
<p>Look before you leap: Unveiling the power of gpt-4v in robotic vision-language planning. Yingdong Hu, Fanqi Lin, Tong Zhang, Li Yi, Yang Gao, First Workshop on Vision-Language Models for Navigation and Manipulation at ICRA 2024. </p>
<p>Haojie Huang, Owen Howell, Xupeng Zhu, Dian Wang, Robin Walters, Robert Platt, arXiv:2401.12046Fourier transporter: Biequivariant robotic manipulation in 3d. 2024arXiv preprint</p>
<p>Copa: General robotic manipulation through spatial constraints of parts with foundation models. Haoxu Huang, Fanqi Lin, Yingdong Hu, Shengjie Wang, Yang Gao, First Workshop on Vision-Language Models for Navigation and Manipulation at ICRA 2024. </p>
<p>Rekep: Spatio-temporal reasoning of relational keypoint constraints for robotic manipulation. Wenlong Huang, Chen Wang, Yunzhu Li, Ruohan Zhang, Li Fei-Fei, 8th Annual Conference on Robot Learning. </p>
<p>Voxposer: Composable 3d value maps for robotic manipulation with language models. Wenlong Huang, Chen Wang, Ruohan Zhang, Yunzhu Li, Jiajun Wu, Li Fei-Fei, Conference on Robot Learning. PMLR2023</p>
<p>Dex-nerf: Using a neural radiance field to grasp transparent objects. Jeffrey Ichnowski, Yahav Avigal, Justin Kerr, Ken Goldberg, 5th Annual Conference on Robot Learning. </p>
<p>Coarse-to-fine q-attention: Efficient learning for visual robotic manipulation via discretisation. Stephen James, Kentaro Wada, Tristan Laidlow, Andrew J Davison, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>
<p>Planning with diffusion for flexible behavior synthesis. Michael Janner, Yilun Du, Joshua Tenenbaum, Sergey Levine, International Conference on Machine Learning. PMLR2022</p>
<p>A stylebased generator architecture for generative adversarial networks. Tero Karras, Samuli Laine, Timo Aila, IEEE Transactions on Pattern Analysis and Machine Intelligence. 43122021</p>
<p>3d diffuser actor: Policy diffusion with 3d scene representations. Tsung-Wei Ke, Nikolaos Gkanatsios, Katerina Fragkiadaki, First Workshop on Vision-Language Models for Navigation and Manipulation at ICRA 2024. </p>
<p>3d gaussian splatting for real-time radiance field rendering. Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler, George Drettakis, ACM Trans. Graph. 4242023</p>
<p>Droid: A large-scale in-the-wild robot manipulation dataset. Alexander Khazatsky, Karl Pertsch, Suraj Nair, Ashwin Balakrishna, Sudeep Dasari, Siddharth Karamcheti, Soroush Nasiriany, Mohan Kumar Srirama, Lawrence Yunliang Chen, Kirsty Ellis, 2024CoRR</p>
<p>Openvla: An open-source vision-language-action model. Jin Moo, Karl Kim, Siddharth Pertsch, Ted Karamcheti, Ashwin Xiao, Suraj Balakrishna, Rafael Nair, Ethan Rafailov, Grace Foster, Pannag Lam, Sanketi, arXiv:2406.092462024arXiv preprint</p>
<p>Data scaling laws in imitation learning for robotic manipulation. Fanqi Lin, Yingdong Hu, Pingyue Sheng, Chuan Wen, Jiacheng You, Yang Gao, arXiv:2410.186472024arXiv preprint</p>
<p>Flow matching for generative modeling. Yaron Lipman, Ricky Tq Chen, Heli Ben-Hamu, Maximilian Nickel, Matthew Le, The Eleventh International Conference on Learning Representations. </p>
<p>Rdt-1b: a diffusion foundation model for bimanual manipulation. Songming Liu, Lingxuan Wu, Bangguo Li, Hengkai Tan, Huayu Chen, Zhengyi Wang, Ke Xu, Hang Su, Jun Zhu, arXiv:2410.078642024arXiv preprint</p>
<p>Decoupled weight decay regularization. Ilya Loshchilov, Frank Hutter, International Conference on Learning Representations. </p>
<p>Manigaussian: Dynamic gaussian splatting for multi-task robotic manipulation. Guanxing Lu, Shiyi Zhang, Ziwei Wang, Changliu Liu, Jiwen Lu, Yansong Tang, European Conference on Computer Vision. Springer2024</p>
<p>Vip: Towards universal visual reward and representation via value-implicit pre-training. Jason Yecheng, Shagun Ma, Dinesh Sodhani, Osbert Jayaraman, Vikash Bastani, Amy Kumar, Zhang, The Eleventh International Conference on Learning Representations. </p>
<p>Nerf: Representing scenes as neural radiance fields for view synthesis. Ben Mildenhall, Matthew Pratul P Srinivasan, Jonathan T Tancik, Ravi Barron, Ren Ramamoorthi, Ng, Communications of the ACM. 6512021</p>
<p>R3m: A universal visual representation for robot manipulation. Suraj Nair, Aravind Rajeswaran, Vikash Kumar, Chelsea Finn, Abhinav Gupta, 6th Annual Conference on Robot Learning. </p>
<p>Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. Maxime Oquab, Timothée Darcet, Théo Moutakanni, V Huy, Marc Vo, Vasil Szafraniec, Pierre Khalidov, Fernandez, Haziza Daniel, Francisco Massa, Transactions on Machine Learning Research. </p>
<p>Open x-embodiment: Robotic learning datasets and rt-x models: Open x-embodiment collaboration 0. Abby O' Neill, Abdul Rehman, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE2024</p>
<p>Scalable diffusion models with transformers. William Peebles, Saining Xie, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023</p>
<p>Pointnet: Deep learning on point sets for 3d classification and segmentation. Hao Charles R Qi, Kaichun Su, Leonidas J Mo, Guibas, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2017</p>
<p>Pointnet++: Deep hierarchical feature learning on point sets in a metric space. Charles Ruizhongtai, Qi , Li Yi, Hao Su, Leonidas J Guibas, Advances in neural information processing systems. 302017</p>
<p>Pointnext: Revisiting pointnet++ with improved training and scaling strategies. Guocheng Qian, Yuchen Li, Houwen Peng, Jinjie Mai, Hasan Hammoud, Mohamed Elhoseiny, Bernard Ghanem, Advances in neural information processing systems. 352022</p>
<p>Learning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, International conference on machine learning. PMLR2021</p>
<p>Multimodal diffusion transformer: Learning versatile behavior from multimodal goals. Moritz Reuss, Ömer Erdinc ¸yagmurlu, Fabian Wenzel, Rudolf Lioutikov, First Workshop on Vision-Language Models for Navigation and Manipulation at ICRA 2024. 2024</p>
<p>High-resolution image synthesis with latent diffusion models. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn Ommer, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2022</p>
<p>Perceiver-actor: A multi-task transformer for robotic manipulation. Mohit Shridhar, Lucas Manuelli, Dieter Fox, Conference on Robot Learning. PMLR2023</p>
<p>Denoising diffusion implicit models. Jiaming Song, Chenlin Meng, Stefano Ermon, International Conference on Learning Representations. </p>
<p>Kite: Keypoint-conditioned policies for semantic manipulation. Priya Sundaresan, Suneel Belkhale, Dorsa Sadigh, Jeannette Bohg, Conference on Robot Learning. PMLR2023</p>
<p>Gemini: a family of highly capable multimodal models. Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, arXiv:2312.118052023arXiv preprint</p>
<p>Octo Model Team, Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey Hejna, Tobias Kreiman, Charles Xu, arXiv:2405.12213An open-source generalist robot policy. 2024arXiv preprint</p>
<p>)-diffusionfields: Learning smooth cost functions for joint grasp and motion optimization through diffusion. Julen Urain, Niklas Funk, Jan Peters, Georgia Chalvatzaki, Se, 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE2023</p>
<p>Bridgedata v2: A dataset for robot learning at scale. Homer Rich Walke, Kevin Black, Tony Z Zhao, Quan Vuong, Chongyi Zheng, Philippe Hansen-Estruch, Andre Wang He, Vivek Myers, Jin Moo, Max Kim, Du, Conference on Robot Learning. PMLR2023</p>
<p>Rise: 3d perception makes real-world robot imitation simple and effective. Chenxi Wang, Hongjie Fang, Hao-Shu, Cewu Fang, Lu, ICRA 2024 Workshop on 3D Visual Representations for Robot Manipulation. </p>
<p>Diffusion policies as an expressive policy class for offline reinforcement learning. Zhendong Wang, Jonathan J Hunt, Mingyuan Zhou, The Eleventh International Conference on Learning Representations. </p>
<p>Can transformers capture spatial relations between objects?. Chuan Wen, Dinesh Jayaraman, Yang Gao, The Twelfth International Conference on Learning Representations. 2023</p>
<p>Any-point Trajectory Modeling for Policy Learning. Chuan Wen, Xingyu Lin, John Ian, Reyes So, Kai Chen, Qi Dou, Yang Gao, Pieter Abbeel, 10.15607/RSS.2024.XX.092Proceedings of Robotics: Science and Systems. Robotics: Science and SystemsDelft, NetherlandsJuly 2024</p>
<p>Chaineddiffuser: Unifying trajectory diffusion and keypose prediction for robotic manipulation. Nikolaos Zhou Xian, Theophile Gkanatsios, Tsung-Wei Gervet, Katerina Ke, Fragkiadaki, 7th Annual Conference on Robot Learning. 2023</p>
<p>Decomposing the generalization gap in imitation learning for visual robotic manipulation. Annie Xie, Lisa Lee, Ted Xiao, Chelsea Finn, 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE2024</p>
<p>General flow as foundation affordance for scalable robot learning. Chengbo Yuan, Chuan Wen, Tong Zhang, Yang Gao, 8th Annual Conference on Robot Learning. 2024</p>
<p>M2t2: Multi-task masked transformer for object-centric pick and place. Wentao Yuan, Adithyavairavan Murali, Arsalan Mousavian, Dieter Fox, 7th Annual Conference on Robot Learning. </p>
<p>Gnfactor: Multi-task real robot learning with generalizable neural feature fields. Yanjie Ze, Ge Yan, Yueh-Hua Wu, Annabella Macaluso, Yuying Ge, Jianglong Ye, Nicklas Hansen, Li Erran Li, Xiaolong Wang, Conference on Robot Learning. PMLR2023</p>
<p>3d diffusion policy: Generalizable visuomotor policy learning via simple 3d representations. Yanjie Ze, Gu Zhang, Kangning Zhang, Chenyuan Hu, Muhan Wang, Huazhe Xu, ICRA 2024 Workshop on 3D Visual Representations for Robot Manipulation. 2024</p>
<p>Sigmoid loss for language image pretraining. Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, Lucas Beyer, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023</p>
<p>A universal semantic-geometric representation for robotic manipulation. Tong Zhang, Yingdong Hu, Hanchen Cui, Hang Zhao, Yang Gao, Conference on Robot Learning. PMLR2023</p>
<p>Tong Zhang, Yingdong Hu, Jiacheng You, Yang Gao, arXiv:2406.10615Leveraging locality to boost sample efficiency in robotic manipulation. 2024arXiv preprint</p>
<p>Aloha unleashed: A simple recipe for robot dexterity. Tony Z Zhao, Jonathan Tompson, Danny Driess, Pete Florence, Seyed Kamyar, Seyed Ghasemipour, Chelsea Finn, Ayzaan Wahid, 8th Annual Conference on Robot Learning. </p>
<p>Learning fine-grained bimanual manipulation with low-cost hardware. Tony Z Zhao, Vikash Kumar, Sergey Levine, Chelsea Finn, arXiv:2304.137052023arXiv preprint</p>
<p>Gaussiangrasper: 3d language gaussian splatting for open-vocabulary robotic grasping. Yuhang Zheng, Xiangyu Chen, Yupeng Zheng, Songen Gu, Runyi Yang, Bu Jin, Pengfei Li, Chengliang Zhong, Zengmao Wang, Lina Liu, arXiv:2403.096372024arXiv preprint</p>
<p>Uni3d: Exploring unified 3d representation at scale. Junsheng Zhou, Jinsheng Wang, Baorui Ma, Yu-Shen Liu, Tiejun Huang, Xinlong Wang, The Twelfth International Conference on Learning Representations. </p>
<p>Point cloud matters: Rethinking the impact of different observation spaces on robot learning. Haoyi Zhu, Yating Wang, Di Huang, Weicai Ye, Wanli Ouyang, Tong He, arXiv:2402.025002024arXiv preprint</p>
<p>Scaling diffusion policy in transformer to 1 billion parameters for robotic manipulation. Minjie Zhu, Yichen Zhu, Jinming Li, Junjie Wen, Zhiyuan Xu, Ning Liu, Ran Cheng, Chaomin Shen, Yaxin Peng, Feifei Feng, arXiv:2409.144112024arXiv preprint</p>
<p>Rt-2: Vision-languageaction models transfer web knowledge to robotic control. Brianna Zitkovich, Tianhe Yu, Sichun Xu, Peng Xu, Ted Xiao, Fei Xia, Jialin Wu, Paul Wohlhart, Stefan Welker, Ayzaan Wahid, Conference on Robot Learning. PMLR2023</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4737 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4737</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4737</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-102.html">extraction-schema-102</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <p><strong>Paper ID:</strong> paper-097dc73d5d422b3c09286e72d16b2561ae5fb395</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/097dc73d5d422b3c09286e72d16b2561ae5fb395" target="_blank">Complementary Explanations for Effective In-Context Learning</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> This work proposes a maximal marginal relevance-based exemplar selection approach for constructing exemplar sets that are both relevant as well as complementary, which successfully improves the in-context learning performance across three real-world tasks on multiple LLMs.</p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have exhibited remarkable capabilities in learning from explanations in prompts, but there has been limited understanding of exactly how these explanations function or why they are effective. This work aims to better understand the mechanisms by which explanations are used for in-context learning. We first study the impact of two different factors on the performance of prompts with explanations: the computation trace (the way the solution is decomposed) and the natural language used to express the prompt. By perturbing explanations on three controlled tasks, we show that both factors contribute to the effectiveness of explanations. We further study how to form maximally effective sets of explanations for solving a given test query. We find that LLMs can benefit from the complementarity of the explanation set: diverse reasoning skills shown by different exemplars can lead to better performance. Therefore, we propose a maximal marginal relevance-based exemplar selection approach for constructing exemplar sets that are both relevant as well as complementary, which successfully improves the in-context learning performance across three real-world tasks on multiple LLMs.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4737.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4737.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OPT-175B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OPT-175B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large pretrained transformer language model from the OPT family used in experiments; in this paper it represents a vanilla language-model-trained LLM baseline for arithmetic and symbolic reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OPT-175B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A large transformer language model from the OPT family trained with a vanilla language modeling objective (used here as a non-instruction-tuned baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Grade-school arithmetic (GSM) multi-step problems involving addition and multiplication; also symbolic tasks like Letter Concatenation and CoinFlip (simple state updates).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>LLMs utilize presented computation traces (intermediate steps) together with natural-language framing; OPT appears to rely on explicit traces and NL to produce correct arithmetic, suggesting pattern-following of decomposed computation in the prompt rather than robust internal algorithmic arithmetic.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Perturbation experiments: masking or substituting intermediate states in exemplar explanations reduces OPT accuracy substantially (GSM: Gold 32.5% -> Mask1 19.0% / Mask2 10.0% / Incorrect 18.5% / No NL 8.0% / Standard(no explanations) 5.5%), indicating reliance on explicit traces and NL. Mixture-of-exemplars experiments show improved performance when exemplars cover complementary steps (e.g., mixture outperforms masked-only exemplars on LetCat and CoinFlip).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>On some simple tasks (e.g., LETCAT, COINFLIP), completely random or partially incorrect explanations can still improve performance over standard prompting, suggesting some template-triggered/slot-filling behavior in addition to faithful following.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>GSM: Standard 5.5%, Gold explanations 32.5%, Mask1 19.0%, Mask2 10.0%, Random explanations 3.0%, Incorrect 18.5%, No NL 8.0%. LETCAT: Standard 8.5%, Gold 50.0%, Mask1 11.0%, Mask2 32.5%, Random 10.0%, Incorrect 40.0%, No NL 29.0%. COINFLIP: Standard 51.5%, Gold 94.0%, Mask1 71.0%, Mask2 84.0%, Random 52.5%, Incorrect 60.5%, No NL 59.5%.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>Interventions: (1) Masking intermediate states (empty-string mask) degrades accuracy; (2) Substituting incorrect intermediate state values reduces performance; (3) Removing natural-language wrapping of traces reduces accuracy; (4) Mixing exemplars that each show different sub-steps improves ability to solve composed arithmetic (mixture > individual masked sets).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>Highly sensitive to missing or incorrect intermediate steps and to removal of natural-language explanation; often fails on multi-step arithmetic when traces are incomplete or corrupted; small improvements from random/incorrect explanations highlight non-robust heuristics and trigger-based behavior rather than stable algorithmic arithmetic.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Per-table comparisons show OPT performs worse than instruction-tuned models (text-davinci-001/002) and Codex variants; OPT benefits from gold explanations but less so than instruction-tuned models; more sensitive to perturbations than stronger models like text-davinci-002.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Complementary Explanations for Effective In-Context Learning', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4737.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4737.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3 (davinci)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3 (davinci)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large pretrained GPT-3 family model (davinci variant) trained with a vanilla language modeling objective; used as a baseline to test sensitivity to explanations on arithmetic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (davinci)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A large transformer language model (GPT-3 family, davinci variant) trained with a vanilla language modeling objective (non-instruction-tuned in this study).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Grade-school arithmetic (GSM) multi-step problems (addition, multiplication, mixtures), and symbolic tasks (LET CAT, CoinFlip).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>davinci relies on explicit computation traces and NL scaffolding provided in exemplars; it appears to follow the decomposed steps rather than robustly computing internally when such traces are corrupted or missing.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Perturbations reduce performance: GSM Gold 26.0% -> Mask1 21.0% / Mask2 16.0% / Incorrect 17.0% / No NL 19.5% / Standard 7.5%. LETCAT Gold 59.0% -> Mask1 16.0% / Mask2 49.5% / Incorrect 53.0% / No NL 15.0% / Standard 8.5%. CoinFlip Gold 89.5% -> Mask1 88.0% / Mask2 91.5% / Incorrect 86.0% / No NL 86.0% / Standard 83.0%, showing sensitivity but somewhat higher baseline competence on CoinFlip.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>On some tasks random/external explanations can still help compared to no explanations (e.g., LETCAT random 25.0% improves over standard prompting), indicating templates or other cues can trigger partial correct behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>GSM: Standard 7.5%, Gold 26.0%, Mask1 21.0%, Mask2 16.0%, Random 3.0%, Incorrect 17.0%, No NL 19.5%. LETCAT: Standard 8.5%, Gold 59.0%, Mask1 16.0%, Mask2 49.5%, Random 25.0%, Incorrect 53.0%, No NL 15.0%. COINFLIP: Standard 83.0%, Gold 89.5%, Mask1 88.0%, Mask2 91.5%, Random 54.5%, Incorrect 86.0%, No NL 86.0%.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>Same interventions as OPT: masking and incorrect substitutions reduce accuracy; removing NL harms performance; mixing exemplar types helps composition; sensitivity extents vary by task (CoinFlip less affected than LETCAT/GSM).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>Fails on many GSM multi-step problems even with gold explanations (modest absolute accuracy), vulnerable to corrupted traces and missing NL; exhibits non-robust gains from irrelevant exemplars indicating reliance on pattern/trigger behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>davinci generally outperforms OPT but underperforms instruction-tuned models (text-davinci-001/002) and code-davinci variants on GSM and exemplar-selection tasks; it gains from gold explanations but to a lesser extent than instruction-tuned LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Complementary Explanations for Effective In-Context Learning', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4737.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4737.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>text-davinci-001</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>InstructGPT (text-davinci-001)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An instruction-tuned GPT model (InstructGPT family) aligned with human expectations via instruction finetuning; used to evaluate robustness of arithmetic reasoning with chain-of-thought style explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-davinci-001 (InstructGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A GPT-family model finetuned via instruction-following objective (alignment with human instructions) to better follow prompts; used here to contrast with vanilla LM models.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Grade-school math (GSM) multi-step arithmetic (addition/multiplication), LETCAT, COINFLIP.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>Instruction tuning increases the model's tendency to follow provided chain-of-thought style explanations; the model uses the combination of computation trace tokens and natural-language glue to produce arithmetic solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Perturbations reduce accuracy (GSM Gold 25.0% -> Mask1 12.5% / Mask2 11.5% / Incorrect 10.0% / No NL 14.5% / Standard 11.0%). LETCAT Gold 85.0% -> Mask1 21.5% / Mask2 68.0% / Incorrect 67.5% / No NL 46.5% / Standard 10.5%. CoinFlip Gold 100% -> many perturbed forms still high (Mask1 61.5% / Mask2 99.0% / No NL 99.0% / Standard 68.0%), showing both reliance and robustness depending on task complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>For CoinFlip and some simple tasks, removal of NL or partial perturbation sometimes does not catastrophically reduce performance (text-davinci-001 often resilient), indicating both stronger pattern generalization and the possibility of shortcut strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>GSM: Standard 11.0%, Gold 25.0%, Mask1 12.5%, Mask2 11.5%, Random 1.0%, Incorrect 10.0%, No NL 14.5%. LETCAT: Standard 10.5%, Gold 85.0%, Mask1 21.5%, Mask2 68.0%, Random 28.0%, Incorrect 67.5%, No NL 46.5%. COINFLIP: Standard 68.0%, Gold 100%, Mask1 61.5%, Mask2 99.0%, Random 67.0%, Incorrect 52.0%, No NL 99.0%.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>Masking intermediate computation steps and substituting incorrect states degrades performance substantially on more complex tasks; mixture-of-exemplars experiments show composition ability (mixture prompts better than single-step masked sets).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>Although instruction-tuning improves following of explanations, the model still fails frequently on GSM and is sensitive to corrupted traces; random or irrelevant explanations can sometimes still impact outputs, reflecting partial reliance on prompt patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>text-davinci-001 typically outperforms vanilla LM models (OPT/davinci) on many tasks, but generally underperforms the stronger text-davinci-002; Codex variants show complementary strengths for exemplar selection tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Complementary Explanations for Effective In-Context Learning', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4737.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4737.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>text-davinci-002</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>text-davinci-002</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A highly capable instruction-tuned LLM (an InstructGPT/DaVinci-derivative) used in experiments; shows the strongest arithmetic performance and the greatest robustness to certain perturbations among models tested.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-davinci-002</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An instruction-tuned large language model (an improved InstructGPT/DaVinci variant) aligned to human expectations; used as the strongest model in the paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Grade-school arithmetic (GSM) multi-step problems (addition & multiplication and compositions), LETCAT, COINFLIP.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>text-davinci-002 appears able to both follow explicit computation traces and in some simple cases 'shortcut' or infer missing steps, implying a mixture of pattern-based decoding and stronger internalized multi-step reasoning when model capacity and tuning allow.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>High robustness in many settings: LETCAT Gold 100% and remains 100% under many perturbations; COINFLIP Gold 100% and remains 100% for many perturbed forms; GSM Gold 57.5% but still degraded by perturbations (Mask1 29.5%, Mask2 27.5%, Incorrect 16.5%, No NL 45.5%), showing that for simple/formulaic traces this model can 'shortcut' but for more challenging arithmetic it still benefits from accurate traces and NL.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>On GSM, despite high capacity, the model still shows substantial drops when traces are masked/incorrect, indicating it is not purely performing stable algorithmic arithmetic internally and still depends on exemplars/explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>GSM: Standard 26.5%, Gold 57.5%, Mask1 29.5%, Mask2 27.5%, Random 34.5%, Incorrect 16.5%, No NL 45.5%. LETCAT: Standard 16.0%, Gold 100%, Mask1 100%, Mask2 100%, Random 13.0%, Incorrect 99.5%, No NL 100%. COINFLIP: Standard 99.0%, Gold 100%, Mask1 100%, Mask2 100%, Random 69.0%, Incorrect 100%, No NL 100%. In exemplar-selection experiments (GSM), code-davinci-derived similarity used: text-davinci-002 Random 48.8% (in some settings) and LM-based neighbor selection improved to 52.0% (see paper tables).</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>Perturbation experiments show text-davinci-002 is uniquely robust on simple/formulaic tasks (LET CAT, COINFLIP) to masked or removed NL traces, but remains sensitive for harder GSM arithmetic; mixture-of-exemplars improves GSM performance modestly (mixing Add & Mul exemplars improves over single-operator exemplars).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>Although strong, it still fails on many GSM problems; sensitive to corrupted or incorrect intermediate values for harder arithmetic; can rely on prompt patterns and therefore may produce errors if exemplars are inconsistent or wrong; exemplar-selection and complementarity remain important for best performance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Outperforms OPT, davinci, and text-davinci-001 on most tasks, especially simple symbolic ones; comparable or outperforms Codex-derived models on many benchmarks but some Codex variants are strong in exemplar-selection experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Complementary Explanations for Effective In-Context Learning', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4737.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4737.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>code-davinci</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>code-davinci-001 / code-davinci-002 (Codex variants)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Codex-family models finetuned on code data (two variants tested) that show strong performance in exemplar-selection settings and competitive arithmetic reasoning when prompted with explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>code-davinci-001 and code-davinci-002</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-style models finetuned on large code datasets (Codex family); used here for exemplar selection and reasoning comparisons, noted to be strong on textual reasoning despite code focus.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Grade-school arithmetic (GSM) multi-step problems (addition, multiplication and compositions); used primarily for testing exemplar-selection strategies on GSM, ECQA, E-SNLI.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>Codex variants benefit from instruction-like exemplars and can leverage exemplar relevance and diversity; mechanism aligns with following computation traces and NL but these models appear particularly strong at extracting similarity signals for exemplar selection and benefiting from complementary exemplars.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Exemplar-selection experiments (Table 3/4): code-davinci-002 Random GSM performance 64.6%; LLM-based neighbor selection yields 65.8% (NN) and MMR selection improves to 67.0% (LM MMR) or 68.2% (BERTScore MMR), demonstrating strong gains from relevance and complementarity when solving GSM with explanation-infused prompts. This supports the idea that these models exploit exemplars and explanations effectively.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>While strong, Codex variants still rely on high-quality exemplars; random exemplars or poor selection reduce performance and they are not immune to incorrect/masked traces on harder tasks (paper focuses exemplar-selection rather than detailed perturbation per Codex).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>In exemplar-selection experiments across GSM/ECQA/E-SNLI: code-davinci-001 Random Avg 39.0% (GSM 16.3%), code-davinci-002 Random Avg 71.3% (GSM 64.6%); LM-based neighbor selection and BERTScore improve results (code-002 LM NN GSM 65.8% -> LM MMR 67.0%; BERTScore NN GSM 66.7% -> BERTScore MMR 68.2%).</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>Paper uses these models to compute LM-based similarity scores and to evaluate MMR exemplar selection; results show MMR (relevance + diversity) improves arithmetic performance, indicating models exploit complementary exemplars to compose reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>Dependence on exemplar quality and selection; performance degrades with random or irrelevant exemplars; not completely robust to incorrect traces for hard arithmetic tasks (although the paper emphasizes exemplar-selection gains rather than detailed perturbation analyses for Codex).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>code-davinci-002 is among the best-performing models in exemplar-selection experiments and often outperforms text-davinci-002 on some selection metrics (e.g., used for LM-based similarity scoring), while instruction-tuned text-davinci-002 remains very strong on raw reasoning tasks in other settings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Complementary Explanations for Effective In-Context Learning', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4737.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4737.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mechanism (explanations-based)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Computation-trace + Natural-Language explanation mechanism</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An analysis-level entity: the paper's core claim that arithmetic performance in LLMs is largely enabled by in-context computation traces (intermediate steps) presented with natural-language scaffolding, which LLMs follow to varying extents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>N/A (mechanism observed across evaluated LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Not a single model but the operational mechanism identified in this paper: an exemplar explanation comprises a computation trace (sequence of intermediate states) transformed by natural-language framing; LLMs use both components to produce arithmetic outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Applies to multi-step arithmetic (GSM) and symbolic tasks used in the paper (LET CAT, CoinFlip).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>LLMs perform arithmetic in ICL primarily by following explicit computation traces and the natural-language mapping that glues trace tokens into coherent reasoning steps; exemplar complementarity enables composition of sub-reasoning steps across examples.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Systematic perturbation experiments: masking intermediate states, substituting incorrect intermediate values, and stripping natural-language wrapper all reduce performance across models; mixture-of-exemplars experiments demonstrate LLMs can compose reasoning from different exemplars (mixture > single-part exemplars); exemplar-selection (MMR) experiments show that diverse, relevant exemplars yield better arithmetic performance.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Some high-capacity/instruction-tuned models (notably text-davinci-002) can sometimes 'shortcut' simple traces or infer missing intermediate states without full traces, and random/incorrect explanations occasionally improve performance on simple tasks, indicating additional pattern-based or memorization components beyond faithful step-following.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Aggregate behaviors: gold explanations substantially boost accuracy vs standard prompting across models (e.g., GSM Gold ranges ~25-57.5% vs Standard ~5.5-26.5% depending on model); corrupting traces or removing NL causes notable drops (detailed per-model numbers appear in the paper's Tables 1-4).</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>Primary interventions: (1) Masking intermediate steps (empty string mask) tests recovery of intermediate computation; (2) Replacing intermediate states with incorrect values tests robustness to corrupted traces; (3) Removing NL wrapper tests importance of natural-language mapping; (4) Mixing exemplars tests compositional fusion of sub-reasoning. All provide consistent evidence that both trace and NL matter.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>Mechanism is not wholly algorithmic â€” models are brittle: performance is sensitive to completeness and correctness of traces and to NL wording; exemplar similarity and diversity matter; LLMs sometimes use spurious template triggers; not all models generalize to composed arithmetic without exemplar coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Instruction-tuned and higher-capacity models rely less strictly on perfect traces for simple tasks but still benefit from high-quality explanations; vanilla-LM models suffer more from perturbations. Exemplar-selection strategies (MMR) improve performance across model types, but gains vary by model capacity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Complementary Explanations for Effective In-Context Learning', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Show your work: Scratchpads for intermediate computation with language models <em>(Rating: 2)</em></li>
                <li>Training verifiers to solve math word problems <em>(Rating: 2)</em></li>
                <li>Rethinking the role of demonstrations: What makes in-context learning work? <em>(Rating: 2)</em></li>
                <li>Teaching algorithmic reasoning via in-context learning <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4737",
    "paper_id": "paper-097dc73d5d422b3c09286e72d16b2561ae5fb395",
    "extraction_schema_id": "extraction-schema-102",
    "extracted_data": [
        {
            "name_short": "OPT-175B",
            "name_full": "OPT-175B",
            "brief_description": "A large pretrained transformer language model from the OPT family used in experiments; in this paper it represents a vanilla language-model-trained LLM baseline for arithmetic and symbolic reasoning tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "OPT-175B",
            "model_description": "A large transformer language model from the OPT family trained with a vanilla language modeling objective (used here as a non-instruction-tuned baseline).",
            "arithmetic_task_type": "Grade-school arithmetic (GSM) multi-step problems involving addition and multiplication; also symbolic tasks like Letter Concatenation and CoinFlip (simple state updates).",
            "mechanism_hypothesis": "LLMs utilize presented computation traces (intermediate steps) together with natural-language framing; OPT appears to rely on explicit traces and NL to produce correct arithmetic, suggesting pattern-following of decomposed computation in the prompt rather than robust internal algorithmic arithmetic.",
            "evidence_for_mechanism": "Perturbation experiments: masking or substituting intermediate states in exemplar explanations reduces OPT accuracy substantially (GSM: Gold 32.5% -&gt; Mask1 19.0% / Mask2 10.0% / Incorrect 18.5% / No NL 8.0% / Standard(no explanations) 5.5%), indicating reliance on explicit traces and NL. Mixture-of-exemplars experiments show improved performance when exemplars cover complementary steps (e.g., mixture outperforms masked-only exemplars on LetCat and CoinFlip).",
            "evidence_against_mechanism": "On some simple tasks (e.g., LETCAT, COINFLIP), completely random or partially incorrect explanations can still improve performance over standard prompting, suggesting some template-triggered/slot-filling behavior in addition to faithful following.",
            "performance_metrics": "GSM: Standard 5.5%, Gold explanations 32.5%, Mask1 19.0%, Mask2 10.0%, Random explanations 3.0%, Incorrect 18.5%, No NL 8.0%. LETCAT: Standard 8.5%, Gold 50.0%, Mask1 11.0%, Mask2 32.5%, Random 10.0%, Incorrect 40.0%, No NL 29.0%. COINFLIP: Standard 51.5%, Gold 94.0%, Mask1 71.0%, Mask2 84.0%, Random 52.5%, Incorrect 60.5%, No NL 59.5%.",
            "probing_or_intervention_results": "Interventions: (1) Masking intermediate states (empty-string mask) degrades accuracy; (2) Substituting incorrect intermediate state values reduces performance; (3) Removing natural-language wrapping of traces reduces accuracy; (4) Mixing exemplars that each show different sub-steps improves ability to solve composed arithmetic (mixture &gt; individual masked sets).",
            "limitations_and_failure_modes": "Highly sensitive to missing or incorrect intermediate steps and to removal of natural-language explanation; often fails on multi-step arithmetic when traces are incomplete or corrupted; small improvements from random/incorrect explanations highlight non-robust heuristics and trigger-based behavior rather than stable algorithmic arithmetic.",
            "comparison_to_other_models": "Per-table comparisons show OPT performs worse than instruction-tuned models (text-davinci-001/002) and Codex variants; OPT benefits from gold explanations but less so than instruction-tuned models; more sensitive to perturbations than stronger models like text-davinci-002.",
            "uuid": "e4737.0",
            "source_info": {
                "paper_title": "Complementary Explanations for Effective In-Context Learning",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "GPT-3 (davinci)",
            "name_full": "GPT-3 (davinci)",
            "brief_description": "A large pretrained GPT-3 family model (davinci variant) trained with a vanilla language modeling objective; used as a baseline to test sensitivity to explanations on arithmetic tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3 (davinci)",
            "model_description": "A large transformer language model (GPT-3 family, davinci variant) trained with a vanilla language modeling objective (non-instruction-tuned in this study).",
            "arithmetic_task_type": "Grade-school arithmetic (GSM) multi-step problems (addition, multiplication, mixtures), and symbolic tasks (LET CAT, CoinFlip).",
            "mechanism_hypothesis": "davinci relies on explicit computation traces and NL scaffolding provided in exemplars; it appears to follow the decomposed steps rather than robustly computing internally when such traces are corrupted or missing.",
            "evidence_for_mechanism": "Perturbations reduce performance: GSM Gold 26.0% -&gt; Mask1 21.0% / Mask2 16.0% / Incorrect 17.0% / No NL 19.5% / Standard 7.5%. LETCAT Gold 59.0% -&gt; Mask1 16.0% / Mask2 49.5% / Incorrect 53.0% / No NL 15.0% / Standard 8.5%. CoinFlip Gold 89.5% -&gt; Mask1 88.0% / Mask2 91.5% / Incorrect 86.0% / No NL 86.0% / Standard 83.0%, showing sensitivity but somewhat higher baseline competence on CoinFlip.",
            "evidence_against_mechanism": "On some tasks random/external explanations can still help compared to no explanations (e.g., LETCAT random 25.0% improves over standard prompting), indicating templates or other cues can trigger partial correct behavior.",
            "performance_metrics": "GSM: Standard 7.5%, Gold 26.0%, Mask1 21.0%, Mask2 16.0%, Random 3.0%, Incorrect 17.0%, No NL 19.5%. LETCAT: Standard 8.5%, Gold 59.0%, Mask1 16.0%, Mask2 49.5%, Random 25.0%, Incorrect 53.0%, No NL 15.0%. COINFLIP: Standard 83.0%, Gold 89.5%, Mask1 88.0%, Mask2 91.5%, Random 54.5%, Incorrect 86.0%, No NL 86.0%.",
            "probing_or_intervention_results": "Same interventions as OPT: masking and incorrect substitutions reduce accuracy; removing NL harms performance; mixing exemplar types helps composition; sensitivity extents vary by task (CoinFlip less affected than LETCAT/GSM).",
            "limitations_and_failure_modes": "Fails on many GSM multi-step problems even with gold explanations (modest absolute accuracy), vulnerable to corrupted traces and missing NL; exhibits non-robust gains from irrelevant exemplars indicating reliance on pattern/trigger behavior.",
            "comparison_to_other_models": "davinci generally outperforms OPT but underperforms instruction-tuned models (text-davinci-001/002) and code-davinci variants on GSM and exemplar-selection tasks; it gains from gold explanations but to a lesser extent than instruction-tuned LLMs.",
            "uuid": "e4737.1",
            "source_info": {
                "paper_title": "Complementary Explanations for Effective In-Context Learning",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "text-davinci-001",
            "name_full": "InstructGPT (text-davinci-001)",
            "brief_description": "An instruction-tuned GPT model (InstructGPT family) aligned with human expectations via instruction finetuning; used to evaluate robustness of arithmetic reasoning with chain-of-thought style explanations.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "text-davinci-001 (InstructGPT)",
            "model_description": "A GPT-family model finetuned via instruction-following objective (alignment with human instructions) to better follow prompts; used here to contrast with vanilla LM models.",
            "arithmetic_task_type": "Grade-school math (GSM) multi-step arithmetic (addition/multiplication), LETCAT, COINFLIP.",
            "mechanism_hypothesis": "Instruction tuning increases the model's tendency to follow provided chain-of-thought style explanations; the model uses the combination of computation trace tokens and natural-language glue to produce arithmetic solutions.",
            "evidence_for_mechanism": "Perturbations reduce accuracy (GSM Gold 25.0% -&gt; Mask1 12.5% / Mask2 11.5% / Incorrect 10.0% / No NL 14.5% / Standard 11.0%). LETCAT Gold 85.0% -&gt; Mask1 21.5% / Mask2 68.0% / Incorrect 67.5% / No NL 46.5% / Standard 10.5%. CoinFlip Gold 100% -&gt; many perturbed forms still high (Mask1 61.5% / Mask2 99.0% / No NL 99.0% / Standard 68.0%), showing both reliance and robustness depending on task complexity.",
            "evidence_against_mechanism": "For CoinFlip and some simple tasks, removal of NL or partial perturbation sometimes does not catastrophically reduce performance (text-davinci-001 often resilient), indicating both stronger pattern generalization and the possibility of shortcut strategies.",
            "performance_metrics": "GSM: Standard 11.0%, Gold 25.0%, Mask1 12.5%, Mask2 11.5%, Random 1.0%, Incorrect 10.0%, No NL 14.5%. LETCAT: Standard 10.5%, Gold 85.0%, Mask1 21.5%, Mask2 68.0%, Random 28.0%, Incorrect 67.5%, No NL 46.5%. COINFLIP: Standard 68.0%, Gold 100%, Mask1 61.5%, Mask2 99.0%, Random 67.0%, Incorrect 52.0%, No NL 99.0%.",
            "probing_or_intervention_results": "Masking intermediate computation steps and substituting incorrect states degrades performance substantially on more complex tasks; mixture-of-exemplars experiments show composition ability (mixture prompts better than single-step masked sets).",
            "limitations_and_failure_modes": "Although instruction-tuning improves following of explanations, the model still fails frequently on GSM and is sensitive to corrupted traces; random or irrelevant explanations can sometimes still impact outputs, reflecting partial reliance on prompt patterns.",
            "comparison_to_other_models": "text-davinci-001 typically outperforms vanilla LM models (OPT/davinci) on many tasks, but generally underperforms the stronger text-davinci-002; Codex variants show complementary strengths for exemplar selection tasks.",
            "uuid": "e4737.2",
            "source_info": {
                "paper_title": "Complementary Explanations for Effective In-Context Learning",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "text-davinci-002",
            "name_full": "text-davinci-002",
            "brief_description": "A highly capable instruction-tuned LLM (an InstructGPT/DaVinci-derivative) used in experiments; shows the strongest arithmetic performance and the greatest robustness to certain perturbations among models tested.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "text-davinci-002",
            "model_description": "An instruction-tuned large language model (an improved InstructGPT/DaVinci variant) aligned to human expectations; used as the strongest model in the paper's experiments.",
            "arithmetic_task_type": "Grade-school arithmetic (GSM) multi-step problems (addition & multiplication and compositions), LETCAT, COINFLIP.",
            "mechanism_hypothesis": "text-davinci-002 appears able to both follow explicit computation traces and in some simple cases 'shortcut' or infer missing steps, implying a mixture of pattern-based decoding and stronger internalized multi-step reasoning when model capacity and tuning allow.",
            "evidence_for_mechanism": "High robustness in many settings: LETCAT Gold 100% and remains 100% under many perturbations; COINFLIP Gold 100% and remains 100% for many perturbed forms; GSM Gold 57.5% but still degraded by perturbations (Mask1 29.5%, Mask2 27.5%, Incorrect 16.5%, No NL 45.5%), showing that for simple/formulaic traces this model can 'shortcut' but for more challenging arithmetic it still benefits from accurate traces and NL.",
            "evidence_against_mechanism": "On GSM, despite high capacity, the model still shows substantial drops when traces are masked/incorrect, indicating it is not purely performing stable algorithmic arithmetic internally and still depends on exemplars/explanations.",
            "performance_metrics": "GSM: Standard 26.5%, Gold 57.5%, Mask1 29.5%, Mask2 27.5%, Random 34.5%, Incorrect 16.5%, No NL 45.5%. LETCAT: Standard 16.0%, Gold 100%, Mask1 100%, Mask2 100%, Random 13.0%, Incorrect 99.5%, No NL 100%. COINFLIP: Standard 99.0%, Gold 100%, Mask1 100%, Mask2 100%, Random 69.0%, Incorrect 100%, No NL 100%. In exemplar-selection experiments (GSM), code-davinci-derived similarity used: text-davinci-002 Random 48.8% (in some settings) and LM-based neighbor selection improved to 52.0% (see paper tables).",
            "probing_or_intervention_results": "Perturbation experiments show text-davinci-002 is uniquely robust on simple/formulaic tasks (LET CAT, COINFLIP) to masked or removed NL traces, but remains sensitive for harder GSM arithmetic; mixture-of-exemplars improves GSM performance modestly (mixing Add & Mul exemplars improves over single-operator exemplars).",
            "limitations_and_failure_modes": "Although strong, it still fails on many GSM problems; sensitive to corrupted or incorrect intermediate values for harder arithmetic; can rely on prompt patterns and therefore may produce errors if exemplars are inconsistent or wrong; exemplar-selection and complementarity remain important for best performance.",
            "comparison_to_other_models": "Outperforms OPT, davinci, and text-davinci-001 on most tasks, especially simple symbolic ones; comparable or outperforms Codex-derived models on many benchmarks but some Codex variants are strong in exemplar-selection experiments.",
            "uuid": "e4737.3",
            "source_info": {
                "paper_title": "Complementary Explanations for Effective In-Context Learning",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "code-davinci",
            "name_full": "code-davinci-001 / code-davinci-002 (Codex variants)",
            "brief_description": "Codex-family models finetuned on code data (two variants tested) that show strong performance in exemplar-selection settings and competitive arithmetic reasoning when prompted with explanations.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "code-davinci-001 and code-davinci-002",
            "model_description": "GPT-style models finetuned on large code datasets (Codex family); used here for exemplar selection and reasoning comparisons, noted to be strong on textual reasoning despite code focus.",
            "arithmetic_task_type": "Grade-school arithmetic (GSM) multi-step problems (addition, multiplication and compositions); used primarily for testing exemplar-selection strategies on GSM, ECQA, E-SNLI.",
            "mechanism_hypothesis": "Codex variants benefit from instruction-like exemplars and can leverage exemplar relevance and diversity; mechanism aligns with following computation traces and NL but these models appear particularly strong at extracting similarity signals for exemplar selection and benefiting from complementary exemplars.",
            "evidence_for_mechanism": "Exemplar-selection experiments (Table 3/4): code-davinci-002 Random GSM performance 64.6%; LLM-based neighbor selection yields 65.8% (NN) and MMR selection improves to 67.0% (LM MMR) or 68.2% (BERTScore MMR), demonstrating strong gains from relevance and complementarity when solving GSM with explanation-infused prompts. This supports the idea that these models exploit exemplars and explanations effectively.",
            "evidence_against_mechanism": "While strong, Codex variants still rely on high-quality exemplars; random exemplars or poor selection reduce performance and they are not immune to incorrect/masked traces on harder tasks (paper focuses exemplar-selection rather than detailed perturbation per Codex).",
            "performance_metrics": "In exemplar-selection experiments across GSM/ECQA/E-SNLI: code-davinci-001 Random Avg 39.0% (GSM 16.3%), code-davinci-002 Random Avg 71.3% (GSM 64.6%); LM-based neighbor selection and BERTScore improve results (code-002 LM NN GSM 65.8% -&gt; LM MMR 67.0%; BERTScore NN GSM 66.7% -&gt; BERTScore MMR 68.2%).",
            "probing_or_intervention_results": "Paper uses these models to compute LM-based similarity scores and to evaluate MMR exemplar selection; results show MMR (relevance + diversity) improves arithmetic performance, indicating models exploit complementary exemplars to compose reasoning.",
            "limitations_and_failure_modes": "Dependence on exemplar quality and selection; performance degrades with random or irrelevant exemplars; not completely robust to incorrect traces for hard arithmetic tasks (although the paper emphasizes exemplar-selection gains rather than detailed perturbation analyses for Codex).",
            "comparison_to_other_models": "code-davinci-002 is among the best-performing models in exemplar-selection experiments and often outperforms text-davinci-002 on some selection metrics (e.g., used for LM-based similarity scoring), while instruction-tuned text-davinci-002 remains very strong on raw reasoning tasks in other settings.",
            "uuid": "e4737.4",
            "source_info": {
                "paper_title": "Complementary Explanations for Effective In-Context Learning",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "Mechanism (explanations-based)",
            "name_full": "Computation-trace + Natural-Language explanation mechanism",
            "brief_description": "An analysis-level entity: the paper's core claim that arithmetic performance in LLMs is largely enabled by in-context computation traces (intermediate steps) presented with natural-language scaffolding, which LLMs follow to varying extents.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "N/A (mechanism observed across evaluated LLMs)",
            "model_description": "Not a single model but the operational mechanism identified in this paper: an exemplar explanation comprises a computation trace (sequence of intermediate states) transformed by natural-language framing; LLMs use both components to produce arithmetic outputs.",
            "arithmetic_task_type": "Applies to multi-step arithmetic (GSM) and symbolic tasks used in the paper (LET CAT, CoinFlip).",
            "mechanism_hypothesis": "LLMs perform arithmetic in ICL primarily by following explicit computation traces and the natural-language mapping that glues trace tokens into coherent reasoning steps; exemplar complementarity enables composition of sub-reasoning steps across examples.",
            "evidence_for_mechanism": "Systematic perturbation experiments: masking intermediate states, substituting incorrect intermediate values, and stripping natural-language wrapper all reduce performance across models; mixture-of-exemplars experiments demonstrate LLMs can compose reasoning from different exemplars (mixture &gt; single-part exemplars); exemplar-selection (MMR) experiments show that diverse, relevant exemplars yield better arithmetic performance.",
            "evidence_against_mechanism": "Some high-capacity/instruction-tuned models (notably text-davinci-002) can sometimes 'shortcut' simple traces or infer missing intermediate states without full traces, and random/incorrect explanations occasionally improve performance on simple tasks, indicating additional pattern-based or memorization components beyond faithful step-following.",
            "performance_metrics": "Aggregate behaviors: gold explanations substantially boost accuracy vs standard prompting across models (e.g., GSM Gold ranges ~25-57.5% vs Standard ~5.5-26.5% depending on model); corrupting traces or removing NL causes notable drops (detailed per-model numbers appear in the paper's Tables 1-4).",
            "probing_or_intervention_results": "Primary interventions: (1) Masking intermediate steps (empty string mask) tests recovery of intermediate computation; (2) Replacing intermediate states with incorrect values tests robustness to corrupted traces; (3) Removing NL wrapper tests importance of natural-language mapping; (4) Mixing exemplars tests compositional fusion of sub-reasoning. All provide consistent evidence that both trace and NL matter.",
            "limitations_and_failure_modes": "Mechanism is not wholly algorithmic â€” models are brittle: performance is sensitive to completeness and correctness of traces and to NL wording; exemplar similarity and diversity matter; LLMs sometimes use spurious template triggers; not all models generalize to composed arithmetic without exemplar coverage.",
            "comparison_to_other_models": "Instruction-tuned and higher-capacity models rely less strictly on perfect traces for simple tasks but still benefit from high-quality explanations; vanilla-LM models suffer more from perturbations. Exemplar-selection strategies (MMR) improve performance across model types, but gains vary by model capacity.",
            "uuid": "e4737.5",
            "source_info": {
                "paper_title": "Complementary Explanations for Effective In-Context Learning",
                "publication_date_yy_mm": "2022-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2
        },
        {
            "paper_title": "Show your work: Scratchpads for intermediate computation with language models",
            "rating": 2
        },
        {
            "paper_title": "Training verifiers to solve math word problems",
            "rating": 2
        },
        {
            "paper_title": "Rethinking the role of demonstrations: What makes in-context learning work?",
            "rating": 2
        },
        {
            "paper_title": "Teaching algorithmic reasoning via in-context learning",
            "rating": 2
        }
    ],
    "cost": 0.018282,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Complementary Explanations for Effective In-Context Learning</h1>
<p>Xi Ye ${ }^{\text {S }}$ Srinivasan Iyer ${ }^{\text {A }}$ Asli Celikyilmaz ${ }^{\text {A }} \quad$ Ves Stoyanov ${ }^{\text {A }}$<br>Greg Durrett ${ }^{\text {Â® }}$ Ramakanth Pasunuru ${ }^{\text {A }}$<br>${ }^{\text {Â® }}$ The University of Texas at Austin ${ }^{\text {A }}$ Meta AI<br>${ }^{\circ}$ {xiye,gdurrett}@cs.utexas.edu<br>${ }^{\text {Â® }}$ {sviyer,ves,aslic,rpasunuru}@meta.com</p>
<h4>Abstract</h4>
<p>Large language models (LLMs) have exhibited remarkable capabilities in learning from explanations in prompts, but there has been limited understanding of exactly how these explanations function or why they are effective. This work aims to better understand the mechanisms by which explanations are used for in-context learning. We first study the impact of two different factors on the performance of prompts with explanations: the computation trace (the way the solution is decomposed) and the natural language used to express the prompt. By perturbing explanations on three controlled tasks, we show that both factors contribute to the effectiveness of explanations. We further study how to form maximally effective sets of explanations for solving a given test query. We find that LLMs can benefit from the complementarity of the explanation set: diverse reasoning skills shown by different exemplars can lead to better performance. Therefore, we propose a maximal marginal relevance-based exemplar selection approach for constructing exemplar sets that are both relevant as well as complementary, which successfully improves the incontext learning performance across three realworld tasks on multiple LLMs.</p>
<h2>1 Introduction</h2>
<p>Large language models (LLMs) have achieved promising progress in learning from only a few exemplars in prompts via in-context learning (ICL) (Brown et al., 2020; Chowdhery et al., 2022). To scale to complex tasks, recent work in the past year has shown that LLMs can benefit from explanations in prompts, particularly for tasks involving multistep reasoning (Nye et al., 2021; Wei et al., 2022; Wang et al., 2022b; Madaan et al., 2022; Jung et al., 2022). However, while including explanations in prompts has been demonstrated to be useful, little has been shown regarding what particular features make them effective and how they function in ICL.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Prompting OPT (Zhang et al., 2022) with explanations where we perturb the computation traces or natural language. Perturbing either traces or natural language will lead to performance degradation.</p>
<p>Our work aims to better understand the mechanisms by which explanations are used for ICL. As shown in the example for a multi-step reasoning task in Figure 1, we view an explanation as a combination of a computation trace and natural language which glues together the states in the trace. We design a series of probing experiments that perturb the explanations (as shown in Figure 2) and test LLMs' performance to understand the sensitivity of LLMs on these two factors. The results suggest that both factors contribute to making effective explanations, as LLMs see substantial performance degradation when prompted with defective explanations. Nonetheless, incomplete explanations are still beneficial compared to no explanations at all (Figure 1). This suggests that LLMs "faithfully" follow the reasoning process specified by the explanations to some extent, as opposed to naively following the template patterns while disregarding critical information (Min et al., 2022c).</p>
<p>The observations from our probing experiments lead us to focus next on understanding what makes</p>
<p>an effective set of exemplars with explanations for solving a given test query. We primarily focus on two aspects, the exemplar-exemplar interplay (how exemplars work together) and the query-exemplar interplay. On the former, we find that the complementarity of the exemplar set is beneficial, as LLMs can fuse different reasoning processes exhibited by individual exemplars in-context. For these studies, we probe LLMs with a mixture of two types of exemplars; each type only specifies a part of the reasoning process (see Figure 3 for detailed instances). We also test how the relevance between the query and the exemplars impacts the performance. Choosing the nearest neighbors (NN) of the query to prompt LLMs has been shown to be effective in the standard prompting setting <em>Liu et al. (2021)</em>. Our experiments covering three similarity metrics show that this is also applicable in the setting that prompts LLMs with explanations.</p>
<p>Our analyses inspire us to rethink the exemplar selection process of using explanations for ICL. The prominent NN-based paradigm only considers relevance <em>Shin et al. (2021); Liu et al. (2021); Rubin et al. (2022)</em>, which could result in selecting mostly similar exemplars. We argue that complementarity should also be considered when constructing explanation-infused prompts. Therefore, we propose an exemplar selection strategy based on the maximum marginal relevance (MMR) <em>Carbonell and Goldstein (1998)</em> approach which selects exemplars that are both relevant as well as diverse. The underlying rationale is that a diverse set of exemplars is more likely to showcase complementary reasoning types that are required to illustrate the reasoning required in the query. We test our MMR-based strategy on three real-world datasets spanning multiple reasoning tasks. On a powerful LLM, text-davinci-002, our MMR-based strategy is able to improve the accuracy over the baseline of using random exemplars by 4.0%, 3.9%, and 8.5% on GSM, ECQA, and E-SNLI, respectively.</p>
<p>In summary, our main findings are: (1) We show that both the computation trace and natural language contribute to making effective explanations for ICL. (2) We show that LLMs can benefit from exemplar sets that exhibit both complementarity and relevance to a given test query. (3) We propose an MMR-based exemplar selection strategy considering both complementarity and relevance and demonstrate that it is more effective than solely choosing the nearest neighbors.</p>
<h2>2 Background</h2>
<p>In-Context Learning Our study is focused on the usage of explanations in in-context learning (ICL). Let $q$ be the test query to solve. The standard ICL prompts a language model, $M$, with a set of exemplar input-output pairs, $\left{\left(q_{1},a_{1}\right) \ldots\left(q_{m}, a_{m}\right)\right}$, and predict an answer $\hat{a}$ for the query:</p>
<p>$$
\hat{a}=\underset{a}{\arg \max } p_{M}\left(a \mid q,\left{\left(q_{1},a_{1}\right) \ldots\left(q_{m}, a_{m}\right)\right}\right)
$$</p>
<p>In addition to just input-output pairs, we can also include explanations (in the style of Scratchpad <em>Nye et al. (2021)</em> or chain-of-thought <em>Wei et al. (2022)</em>) in prompts, which leads the LLM to generate explanations for its predictions as well:</p>
<p>$$
\hat{a}=\underset{a}{\arg \max } \sum_{e} p_{M}(a, e \mid q, C)
$$</p>
<p>where $C=\left{\left(q_{1}, e_{1}, a_{1}\right) \ldots\left(q_{m}, e_{m}, a_{m}\right)\right}$ is the set of input-explanation-output triplets in prompts. Ideally, inference in this requires marginalizing out the explanation $e$, which is impractical, especially with LLMs. Following <em>Wei et al. (2022); Ye and Durrett (2022)</em>, we employ greedy decoding to make an approximate prediction during inference.</p>
<p>The end task performance of ICL is sensitive to the selected exemplars <em>Liu et al. (2021)</em>. While much prior work uses a fixed set of manually selected exemplars <em>Wei et al. (2022); Wang et al. (2022b)</em>, there is also work devoted to studying how to select more effective exemplars from a pool of exemplars. Given a test query $q$, the task is to select a set of $m$ exemplars from a pool of $n$ exemplars $D=\left{\left(q_{1}, e_{1}, a_{1}\right) \ldots\left(q_{n}, e_{n}, a_{n}\right)\right}$ to construct a prompt for solving $q$. We note that this yields varying exemplar sets for different queries.</p>
<p>Datasets \&amp; Large Language Models Our analysis is based on model performance on various reasoning datasets. For probing experiments, we mainly use symbolic reasoning datasets, including 1) Letter Concatenation (Let Cat) <em>Wei et al. (2022)</em> which requires extracting the last letters of two words and then concatenating them, 2) Coin Flips <em>Wei et al. (2022)</em> which reasons about the states of a coin after two steps of operations (flipping or not flipping), and 3) Grade School</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>MATH (GSM) <em>Cobbe et al. (2021)</em> which focuses on grade school-level arithmetic reasoning problems expressed in natural language.</p>
<p>To investigate the effectiveness of different exemplar selection strategies, we use two more textual reasoning datasets, namely ECQA <em>Aggarwal et al. (2021)</em> and E-SNLI <em>Camburu et al. (2018)</em>, in addition to GSM. The task of E-SNLI is to decide whether a premise entails a hypothesis. ECQA asks multiple-choice commonsense questions. These three tasks include human-annotated explanations and cover diverse reasoning abilities.</p>
<p>Our experiments cover an array of LLMs, including OPT-175B <em>Zhang et al. (2022)</em>, GPT-3 (davinci) <em>Brown et al. (2020)</em>, InstructGPT (text-davinci-001) <em>Ouyang et al. (2022)</em>, and text-davinci-002. In addition, we also use GPT-3 Codex models <em>Chen et al. (2021)</em> that are finetuned on a large scale of code snippets in our exemplar selection experiments, namely code-davinci-001 and code-davinci-002. Though codex models primarily target code-related applications, we find that they are strong in textual reasoning tasks as well.</p>
<h2>3 Do LLMs Follow Explanations?</h2>
<p>We first investigate what makes explanations effective for LLMs to learn from. We view an explanation as a computation trace $(T)$ that is transformed by a natural language function $(L)$ which maps the trace to a complete natural language explanation. A computation trace $T$ is the chain of intermediate steps (instantiated as tokens in explanations), $s_{1}, \ldots, s_{n}$, that are used to derive the final answer. For instance, the trace for LetCat is the two last letters of the two words and the concatenated two-letter tokens; the traces for GSM are the intermediate equations. These computation traces are wrapped by natural language function $L$ to form the final explanation $L\left(s_{1}, \ldots, s_{n}\right)$, which presumably makes the generation of these traces more "natural" with respect to language modeling.</p>
<p>Setup We choose the three symbolic reasoning datasets mentioned in Section 2 for our probing experiments for two reasons. First, LLMs see substantial benefits from including explanations in prompts for these tasks. Second, we can easily manipulate the traces in their explanations. The gold explanations are directly taken from or adapted from Wei et al. <em>Wei et al. (2022)</em>. More details on the gold explanations used for these tasks can be found in Appendix A.</p>
<p>Question: Take the last letters of the words in "Bill Gates" and concatenate them.</p>
<p>Gold: The last letter of Bill is 1. The last letter of Gates is x . Concatenating 1 and $x$ is Is. So the answer is Is.
Mask1: The last letter of Bill is . The last letter of Gates is . Concatenating 1 and $x$ is Is. So the answer is Is.
Mask2: The last letter of Bill is 1. The last letter of Gates is s. Concatenating $\ldots$ and $\ldots$ is . So the answer is Is.
Incorrect: The last letter of Bill is y. The last letter of Gates is e. Concatenating $y$ and $e$ is ye. So the answer is ye.
No NL: Bill, 1. Gates, s. 1, s, Is. So the answer is Is.
Question: A coin is heads up. Ka does not flip the coin. Sal flips the coin. Is the coin still heads up?</p>
<p>Gold: The coin started heads up. Ka does not flip the coin, so it becomes heads up. Sal flips the coin, so it becomes tails up. So the answer is no. Mask1: The coin started heads up. Ka does not flip the coin, so it becomes . . up. Sal flips the coin, so it becomes tails up. So the answer is no. Mask2: The coin started heads up. Ka does not flip the coin, so it becomes heads up. Sal flips the coin, so it becomes . up. So the answer is no. Incorrect: The coin started heads up. Ka does not flip the coin, so it becomes tails up. Sal flips the coin, so it becomes heads up. So the answer is yes.
No NL: heads, heads, tails. So the answer is no.
Question: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?</p>
<p>Gold: Leah had 32 chocolates and Leah's sister had 42. That means there were originally $32+42=74$ chocolates. 35 have been eaten. So in total they still have $74-35=39$ chocolates. The answer is 39.
Mask1: Leah had 32 chocolates and Leah's sister had 42. That means there were originally $32+42=$, chocolates. 35 have been eaten. So in total they have $\ldots 35=39$ chocolates. The answer is 39.
Mask2: Leah had 32 chocolates and Leah's sister had 42. That means there were originally $\ldots$ chocolates. 35 have been eaten. So in total they have $\ldots$ chocolates. The answer is 39.
Incorrect: Leah had 32 chocolates and Leah's sister had 42. That means there were originally $32+42=62$ chocolates. 35 have been eaten. So in total they have $62-35=27$ chocolates. The answer is 27.
No NL: $32+42=74,74-35=39$. The answer is 39.</p>
<p>Figure 2: Examples of gold explanations and perturbed explanations. We perturb the trace in gold explanations (colored) by masking intermediate states or substituting them with incorrect values.</p>
<p>We experiment on both LLMs trained with vanilla language modeling objectives (OPT, davinci) and LLMs that are aligned with human expectations via different forms of instruction tuning (text-davinci-001, text-davinci-002).</p>
<h3>3.1 Explanations or Triggers?</h3>
<p>We start by investigating whether actual computation traces matter. If the correctness of the incontext demonstration is unimportant, then that serves as evidence that explanations act as triggers that induce LLMs to follow certain patterns and perform slot-filling. To study this, we prompt LLMs with perturbed explanations, i.e., by perturbing computational traces and measuring the impact on performance in an ICL setting.</p>
<p>Figure 2 shows concrete examples of how we perturb the gold explanations. We experiment with two ways of perturbing the inputs. The first way is</p>
<table>
<thead>
<tr>
<th></th>
<th>LETCAT</th>
<th></th>
<th></th>
<th></th>
<th>COINFLIP</th>
<th></th>
<th></th>
<th></th>
<th>GSM</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>OPT</td>
<td>davinci</td>
<td>txt-01</td>
<td>txt-02</td>
<td>OPT</td>
<td>davinci</td>
<td>txt-01</td>
<td>txt-02</td>
<td>OPT</td>
<td>davinci</td>
<td>txt-01</td>
<td>txt-02</td>
</tr>
<tr>
<td>Standard</td>
<td>8.5</td>
<td>8.5</td>
<td>10.5</td>
<td>16.0</td>
<td>51.5</td>
<td>83.0</td>
<td>68.0</td>
<td>99.0</td>
<td>5.5</td>
<td>7.5</td>
<td>11.0</td>
<td>26.5</td>
</tr>
<tr>
<td>Gold</td>
<td>50.0</td>
<td>59.0</td>
<td>85.0</td>
<td>100.</td>
<td>94.0</td>
<td>89.5</td>
<td>100.</td>
<td>100.</td>
<td>32.5</td>
<td>26.0</td>
<td>25.0</td>
<td>57.5</td>
</tr>
<tr>
<td>Mask1</td>
<td>11.0</td>
<td>16.0</td>
<td>21.5</td>
<td>100.</td>
<td>71.0</td>
<td>88.0</td>
<td>61.5</td>
<td>100.</td>
<td>19.0</td>
<td>21.0</td>
<td>12.5</td>
<td>29.5</td>
</tr>
<tr>
<td>Mask2</td>
<td>32.5</td>
<td>49.5</td>
<td>68.0</td>
<td>100.</td>
<td>84.0</td>
<td>91.5</td>
<td>99.0</td>
<td>100.</td>
<td>10.0</td>
<td>16.0</td>
<td>11.5</td>
<td>27.5</td>
</tr>
<tr>
<td>Random</td>
<td>10.0</td>
<td>25.0</td>
<td>28.0</td>
<td>13.0</td>
<td>52.5</td>
<td>54.5</td>
<td>67.0</td>
<td>69.0</td>
<td>3.0</td>
<td>3.0</td>
<td>1.0</td>
<td>34.5</td>
</tr>
<tr>
<td>Incorrect</td>
<td>40.0</td>
<td>53.0</td>
<td>67.5</td>
<td>99.5</td>
<td>60.5</td>
<td>86.0</td>
<td>52.0</td>
<td>100.</td>
<td>18.5</td>
<td>17.0</td>
<td>10.0</td>
<td>16.5</td>
</tr>
<tr>
<td>No NL</td>
<td>29.0</td>
<td>15.0</td>
<td>46.5</td>
<td>100.</td>
<td>59.5</td>
<td>86.0</td>
<td>99.0</td>
<td>100.</td>
<td>8.0</td>
<td>19.5</td>
<td>14.5</td>
<td>45.5</td>
</tr>
</tbody>
</table>
<p>Table 1: In-context performance obtained using various perturbed explanations on three datasets. Perturbed explanations achieve inferior performance than complete ones, but many of the perturbed explanations still grant performance gains over standard prompting. masking out the intermediate states by replacing a state $s_{i}$ (or several states) in $s_{1}, \ldots, s_{n}$ with a mask token (e.g., empty string), which tests whether LLMs can implicitly infer the intermediate states. The second way is to replace a state $s_{i}$ with an incorrect one, which tests whether LLMs can recover the correct computation from corrupted traces.</p>
<p>Construction of Perturbed Explanations We carefully design the way to mask out the intermediate states. We experiment with various choices of masks in our preliminary experiments. We do not observe large variance caused by different masks, and we choose to use empty string which leads to less performance degradation in general, as our goal here is to probe LLMs' maximum capabilities in recovering the reasoning process from perturbed explanations. More details on the preliminary experiments on choosing masks can be found in Appendix B. When constructing incorrect explanations, we also experiment with different sets of random values used to substitute the correct ones. Furthermore, we also include complete random explanations (taken from other exemplars), which replace the whole gold traces with incorrect ones.</p>
<p>Results Table 1 shows the results obtained using prompts with various perturbations on the three tasks. First, LMs are indeed relying on the actual computation traces. Perturbing the traces of the explanations will lead to performance degradation in various degrees on all these three tasks for OPT, davinci, and text-davinci-001. text-davinci-002 does not exhibit performance degradation when being prompted with incomplete explanations on the simple tasks, LETCAT and COINFLIP: when the trace is straightforward, a powerful enough LLM is able to "shortcut" some particular steps. While for the more challenging tasks, i.e., GSM, text-002 is also affected by perturbations in explanations. Nonetheless, LLMs can still benefit from partially complete or partially correct explanations and outperform standard prompting without using explanations. In particular, on the LETCAT task, even completely irrelevant random explanations can be beneficial, although they lag gold explanations. Overall, incorrect and incomplete or even totally irrelevant explanations are able to elicit reasoning, but LLMs do rely on gold explanations to work well.</p>
<h3>3.2 Is Natural Language Necessary?</h3>
<p>Next, we question whether the natural language (NL) is really necessary and test whether LLMs can infer the reasoning steps from the computation traces alone. We perturb gold explanations by not wrapping computation traces with natural language transformation $L$, as shown in the examples from Figure 2, and only retain the traces.</p>
<p>Results We show the performance obtained by using these prompts in Table 1. Natural language also plays an essential role in making effective explanations. Removing the NL leads to substantially worse performance. On LETCAT, the accuracy of OPT, and davinci drops by more than 20, respectively, compared to using gold explanations. On GSM, removing NL consistently leads to performance degradation. Meanwhile, including intermediate states without NL can still improve the performance compared to not using any explanations.</p>
<h3>3.3 Discussion</h3>
<p>As suggested in the experimental results in Section 3.1 and Section 3.2, LLMs do generally follow the explanations in the prompts. Both concrete computation traces and natural language contribute to making effective explanations for ICL. Perturbing certain parts of the explanations will accordingly result in performance degradation, but partial explanations are still beneficial to LMs.</p>
<p>By contrast, recent work shows LLMs are not sensitive to perturbations on the ground-truth inputlabel mapping in the standard prompting paradigm that does not use explanations <em>Min et al. (2022c)</em>. Our work shows that LLMs are sensitive to perturbations in the input-explanation mapping and other more subtle perturbations in the explanations. Using explanations in prompts is a promising way to guide LLMs in learning a new task via ICL.</p>
<h2>4 What Makes A Good Exemplar Set?</h2>
<p>Our probing experiments have established how the general factors, computation trace and natural language, impact explanationsâ€™ effectiveness in ICL. We now study how a set of exemplars, as a whole, functions together in solving a particular test query. We study this problem from two angles, the interplay between exemplars and the interplay between the query and the exemplars.</p>
<h3>4.1 Exemplar-Exemplar Interplay</h3>
<p>As in Section 3, LLMs can learn to follow the reasoning processes as specified in exemplars. As reasoning processes can be composed, we hypothesize that LLMs might also be able to fuse the reasoning processes of different exemplars together to solve a test query. We design a set of probing experiments that successfully verify this hypothesis.</p>
<h4>Experiment Design</h4>
<p>At the abstract level, we compare the performance of LLMs when being prompted with three sets of exemplars. The first and second set of exemplars each focuses on a particular part of the reasoning process, and these two parts are disjoint. That is, for a computation trace $s_{1}, \ldots, s_{n}$, the first and second set contain exemplars where $s_{i}$ and $s_{j}$ are perturbed, and $i \neq j$. The third set of exemplars includes the mixture from the first and second sets. We test the ICL performance of the prompts constructed from these three types of exemplar sets on the test set that requires combining two types of reasoning. If the third type gives superior performance than the first two types, that means LLMs can pick up the disjoint reasoning and fuse them in-context.</p>
<p>To better illustrate such a hypothesis, we give a concrete example as follows. We have introduced two different types of masked explanations in Figure 2 for LetCat, where the first type masks the last letter extraction part, and the second type masks the letter concatenation part. These two different masked explanations specify two steps.</p>
<table>
<thead>
<tr>
<th>1. Q: Marion received 20 more turtles than Mia at the animal rescue center.</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>2. If Mia received 40 turtles, how many turtles did they receive together?</td>
<td></td>
</tr>
<tr>
<td>3. A: Since Marion received 20 more turtles than Mia, she had $20+40=60$</td>
<td></td>
</tr>
<tr>
<td>4. turtles. The two received $60+40=100$ turtles. The answer is 100 .</td>
<td></td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th>5. Q: Super Clean Car Wash Company cleans 80 cars per day. They make</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>6. $5 per car washed. How much money will they make in 5 days?</td>
<td></td>
</tr>
<tr>
<td>7. A: Each day they will make $80 * 5 = $400. They will make $400 * 5 =$</td>
<td></td>
</tr>
<tr>
<td>8. $2000 in 5 days. The answer is 2000.</td>
<td></td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th>9. Q: Peter purchased 20 popsicles at $0.25 each. He also purchased 4 ice</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>10. cream bars at $0.50 each. How much did he pay in total in dollars?</td>
<td></td>
</tr>
<tr>
<td>11. A: The popsicles cost $0.25 * 20=5$ dollars. The ice cream bars cost $0.5 *$</td>
<td></td>
</tr>
<tr>
<td>12. $4=2$ dollars. He paid $5+2=7$ dollars. The answer is 7.</td>
<td></td>
</tr>
</tbody>
</table>
<p>Figure 3: Examples of GSM data points that involve only addition operators, only multiplication operators, and both of them at the same time. We note that Add \&amp; Mul are only used at the test time. of the reasoning process needed; combining these two steps will yield the complete reasoning steps needed for solving this task. We test whether LLMs can combine these two reasoning steps in-context if being prompted with a mixture of these two corresponding types of masked prompts.</p>
<p>For GSM, we use a more organic way to partition the reasoning process. We separate the reasoning skills needed for a test query based on the operators (addition and multiplication) that are used in the steps. Concretely, we filter the GSM dataset by looking at the provided explanations paired with examples, and obtain disjoint sets that 1) only involves addition operators in the explanation 2) only involves multiplication operators in the explanation (See Figure 3 for examples). Next, we test the performance on a test set consisting of examples that require both operators at the same time (Add and Mul in Figure 3). This forms a test-bed for investigating whether LLMs can better learn to solve problems where both operators are present at the same time while being prompted with the mixture of these two operators, even if no explicit combinations are shown in the prompts.</p>
<p>Setup We experiment on the same three datasets as used in Section 3. On LetCat and CoinFlip, we test whether LLMs can combine the reasoning steps specified in two different types of masked explanations; on GSM, we test whether LLMs can compose addition and multiplication. The mixture type prompts include half of the exemplars from the first type and second type which bear different reasoning. For each setting of GSM, we experiment with 4 different sets of randomly drawn exemplars and report the average. More details about the setting can be found in Appendix C.</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th>OPT</th>
<th>davinci</th>
<th>txt-01</th>
<th>txt-02</th>
</tr>
</thead>
<tbody>
<tr>
<td>1-4</td>
<td>Mask1</td>
<td>11.0</td>
<td>16.0</td>
<td>21.5</td>
<td>100.</td>
</tr>
<tr>
<td>1-4</td>
<td>Mask2</td>
<td>32.5</td>
<td>49.5</td>
<td>68.0</td>
<td>100.</td>
</tr>
<tr>
<td>1-4</td>
<td>Mixture</td>
<td>37.0</td>
<td>56.5</td>
<td>82.0</td>
<td>100.</td>
</tr>
<tr>
<td>1-4</td>
<td>Mask1</td>
<td>71.0</td>
<td>88.0</td>
<td>61.5</td>
<td>100.</td>
</tr>
<tr>
<td>1-4</td>
<td>Mask2</td>
<td>84.0</td>
<td>91.5</td>
<td>99.0</td>
<td>100.</td>
</tr>
<tr>
<td>1-4</td>
<td>Mixture</td>
<td>93.5</td>
<td>91.0</td>
<td>100.</td>
<td>100.</td>
</tr>
<tr>
<td>1-4</td>
<td>AddOnly</td>
<td>6.8</td>
<td>13.5</td>
<td>14.1</td>
<td>50.3</td>
</tr>
<tr>
<td>1-4</td>
<td>MalOnly</td>
<td>4.7</td>
<td>17.2</td>
<td>16.7</td>
<td>50.1</td>
</tr>
<tr>
<td>1-4</td>
<td>Mixture</td>
<td>7.0</td>
<td>18.9</td>
<td>18.2</td>
<td>52.0</td>
</tr>
</tbody>
</table>
<p>Table 2: The accuracy of prompting LLMs with exemplars focusing on single parts of the reasoning or a mixture of them. LLMs achieve better performance when being prompted with exemplars covering multiple aspects of the reasoning process.</p>
<p>Results As in Table 2, on LetCat, the prompts with mixed explanations largely surpass Mask1, and outperform Mask2 by 6.0, 7.0, and 12.0 on OPT, davinci, text-davinci-001, respectively. Particularly, the mixture prompts is able to perform roughly on par with the complete prompt (GoldExpl in Table 1) for LetCat on davinci, and text-001. On CoinFlip, using mixture prompts also leads to improvements on OPT.</p>
<p>On the realistic GSM dataset, prompting text-davinci-002 with only addition or multiplication exemplars leads to a performance of 50.3 and 50.1, respectively, whereas prompting with a mixture of these two types of exemplars achieves a better performance of 52.0. On davinci, and text-001, addition exemplars give worse performance than multiplication exemplars. Nevertheless, including these inferior addition exemplars in prompts together with multiplication exemplars still leads to better performance, as they can complement the reasoning. In general, results on three datasets suggest LLMs are able to fuse the reasoning process that is spread over different exemplars. Therefore, we can expect the exemplars to be able to complement each other and collaborate together to solve the reasoning needed in the test query.</p>
<h3>4.2 Query-Exemplar Interplay</h3>
<p>Next, we explore how the interplay between the query and exemplars impacts the ICL performance. Recent work has studied how to make good incontext exemplar sets for a given query in the standard prompting setting: choosing nearest neighbors that are more similar to the query leads to better performance <em>Liu et al. (2021); Shin et al. (2021)</em>. Our work investigates how choosing relevant exem- plars impact the performance in the setting when using explanations in prompts. We compare the performance obtained by constructing prompts using nearest neighbors against using randomly selected exemplars. The results verify that choosing the nearest neighbors is also beneficial in this setting.</p>
<p>Similarity Measurements We test three different ways to measure the similarity $\mathcal{S}\left(q, q_{i}\right)$ between a test query $q$ and an exemplar $q_{i}$. 1) CLSbased: Liu et al. <em>Liu et al. (2021)</em> use smaller LMs (e.g., BERT <em>Devlin et al. (2019)</em>) to extract the CLS embedding of the input $q$ and $q_{i}$ and then use cosine similarity to score the embedding pairs, i.e., $\cos \left(\operatorname{CLS}(q), \operatorname{CLS}\left(q_{i}\right)\right)$. 2) LM-based: the similarity is given as the probability of generating the query when the language model is conditioned on the exemplar, i.e., $L M\left(q \mid q_{i}\right)$ <em>Shin et al. (2021); Rubin et al. (2022)</em>. 3) BERTScore: we also experiment with using BERTScore <em>Zhang et al. (2020)</em> as the similarity score, in addition to the two approaches that are commonly used in prior work.</p>
<p>It is worthwhile to note that measuring similarity using large LLMs is expensive. As it requires querying a large number of query-exemplar pairs.</p>
<p>Setup We experiment on three realistic datasets, GSM, ECQA, and E-SNLI. leaving out synthetic tasks which feature formulaic explanations that are all similar to each other. We set the number of exemplars to be 8 for all three test datasets. We compare the performance of selecting nearest exemplars against that of selecting random exemplars.</p>
<p>Given the intensive cost of querying LLMs, we set the train exemplars pool size to be 512, and allocate computational resources to experimenting over 4 sets of randomly selected 512 exemplar pools to alleviate the influence of randomness. We focus on more capable LLMs, including code-davinci-001, and code-davinci-002, and text-davinci-002, leaving out OPT and davinci which have inferior performance. We note that we do not use text-002 to measure similarity, owing to its high cost. Rather, we take the similarity scores computed by code-002 and use those for text-002. So the performance of text-002 when</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th></th>
<th>code-davinci-001</th>
<th></th>
<th></th>
<th></th>
<th>code-davinci-002</th>
<th></th>
<th></th>
<th></th>
<th>text-davinci-002</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>GSM</td>
<td>ECQA</td>
<td>E-SNLI</td>
<td>Avg</td>
<td>GSM</td>
<td>ECQA</td>
<td>E-SNLI</td>
<td>Avg</td>
<td>GSM</td>
<td>ECQA</td>
<td>E-SNLI</td>
<td>Avg</td>
</tr>
<tr>
<td>Random</td>
<td>16.3</td>
<td>53.6</td>
<td>47.2</td>
<td>39.0</td>
<td>64.6</td>
<td>74.7</td>
<td>74.9</td>
<td>71.3</td>
<td>48.8</td>
<td>71.9</td>
<td>75.1</td>
<td>65.3</td>
</tr>
<tr>
<td>CLS</td>
<td>16.5</td>
<td>55.0</td>
<td>54.1</td>
<td>41.8</td>
<td>65.4</td>
<td>74.9</td>
<td>74.8</td>
<td>71.7</td>
<td>50.4</td>
<td>72.1</td>
<td>77.4</td>
<td>66.6</td>
</tr>
<tr>
<td>LLM</td>
<td>18.5</td>
<td>56.0</td>
<td>57.4</td>
<td>43.9</td>
<td>65.8</td>
<td>76.8</td>
<td>81.6</td>
<td>74.7</td>
<td>52.0^{âˆ—}</td>
<td>74.3^{âˆ—}</td>
<td>83.9^{âˆ—}</td>
<td>70.0</td>
</tr>
<tr>
<td>BERTScore</td>
<td>18.5</td>
<td>54.6</td>
<td>53.7</td>
<td>42.3</td>
<td>66.7</td>
<td>75.9</td>
<td>75.6</td>
<td>72.8</td>
<td>51.0</td>
<td>72.8</td>
<td>76.7</td>
<td>67.6</td>
</tr>
</tbody>
</table>
<p>Table 3: Comparison between the performance obtained by choosing relevant exemplars using CLS embedding, LM, or BERTScore. Avg denotes the average across the three datasets. Selecting relevant exemplars leads to performance improvements, especially when using LLMs themselves to measure the similarity. Using Bertscore also consistently improves the performance across all tasks, even surpassing LM-based scores on GSM. We note that the results on text-davinci-002 use the LM-based scores provided by code-davinci-002 (denoted by ^{âˆ—}).
using the LM-based measure might be suboptimal given the discrepancy.</p>
<p>Results As shown in Table 3, choosing relevant exemplars is also useful in the setting that includes explanations in prompts. Using the LMbased similarity measurements brings performance improvements across all three datasets, and has the most significant impacts on E-SNLI, though this is achieved with non-negligible computation cost. Using CLS-embeddings for selecting exemplars mildly improves the performance on GSM but does not result in any performance gains on ECQA. The limited improvements can be attributed to the size of the exemplar pools that we use. In our experiments, the size is 512, which is significantly smaller than that in <em>Liu et al. (2021)</em> (typically tens of thousands of exemplars). Nevertheless, this size is large enough for the LM-based method and BERTScore to take advantage of.</p>
<p>In addition, the results suggest that choosing relevant exemplars using BERTScore is also able to improve the performance across all datasets. Specifically, BERTScore-based exemplar selection achieves an accuracy of 66.7 on GSM using code-002, which even surpasses the performance of LM-based exemplar selection. While using BERTScore lags the LM-based on ECQA and E-SNLI, it still outperforms choosing random exemplars or CLS-based exemplar selection. Overall, using BERTScore to select the closest exemplars can lead to credible performance improvements while does not require heavy overheads caused by using LLMs to score query-exemplar pairs.</p>
<h2>5 MMR for Exemplar Selection</h2>
<p>We have established that emplar-exemplar interplay together with the query-exemplar interplay impacts the performance of using explanations in ICL. This leads us to rethink how to select good exemplars for a given query. Based on our prior analysis on</p>
<p>Algorithm 1 MMR-Based Exemplar Selection
1:procedure MMRSELECT(<em>D</em>, <em>q</em>, <em>k</em>, <em>S</em>)
input: exemplar pool $D=\left{q_{1}...q_{n}\right}$, test query $q$, number of shots $m$ and similarity measurement $S$
output: selected exemplars $T=\left{q_{1}...q_{m}\right}$
2: $S:=\left[\left[\mathcal{S}\left(q_{i},q_{j}\right)\right]\right]<em i="i">{q</em> ; \triangleright$ the pairwise similarity between exemplars in $D$
3: $\mathbb{Q}:=\left[\mathcal{S}\left(q,q_{i}\right)\right]},q_{j}\in D<em i="i">{q</em> ; \triangleright$ the similarity between query and exemplars in $T$
4: $T:={ }$
5: while $|T|&lt;k$ do
6: $\hat{q}:=\operatorname{Equation}(1) ; \quad \triangleright$ get the next exemplar based on Eq (1)
7: $T . \operatorname{add}(\hat{q})$
8: return $T$;
the effects of complementarity and relevance in Section 4.1, we argue that a good set should consist of }\in D<em>relevant</em> exemplars that collaboratively cover the reasoning skills required for solving the query.</p>
<p>The prominent paradigm, i.e., NN-based exemplar selection strategy, only considers the relevance between the exemplars and the query. Yet, selecting nearest neighbors could result in mostly similar exemplar sets, which can possibly limit collaboration. We argue that complementarity should also be considered in the exemplar selection process, so that the selected set could have a higher chance to illustrate the required reasoning processes.</p>
<p>In practice, it is tricky to decide whether the reasoning underlying a set of exemplars is complementary categorically. We therefore use diversity as a proxy, since a set of less similar exemplars is arguably more likely to exhibit complementarity. To that end, we propose a maximal-marginalrelevance <em>Carbonell and Goldstein (1998)</em> (MMR) based exemplar selection strategy. The idea is to select exemplars that are relevant to the query while being diverse enough to be collaborative. Suppose for the given query $q$, we have already selected a set of exemplars $T=\left{q_{i}\right}$, then we will pick up the next exemplar according to:</p>
<table>
<thead>
<tr>
<th></th>
<th>code-davinci-001</th>
<th></th>
<th></th>
<th></th>
<th>code-davinci-002</th>
<th></th>
<th></th>
<th></th>
<th>text-davinci-002</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>GSM</td>
<td>ECQA</td>
<td>E-SNLI</td>
<td>Avg</td>
<td>GSM</td>
<td>ECQA</td>
<td>E-SNLI</td>
<td>Avg</td>
<td>GSM</td>
<td>ECQA</td>
<td>E-SNLI</td>
<td>Avg</td>
</tr>
<tr>
<td>LLM NN</td>
<td>18.5</td>
<td>56.0</td>
<td>57.4</td>
<td>43.9</td>
<td>65.8</td>
<td>76.8</td>
<td>81.6</td>
<td>74.7</td>
<td>52.0^{âˆ—}</td>
<td>74.3^{âˆ—}</td>
<td>83.9^{âˆ—}</td>
<td>70.0^{âˆ—}</td>
</tr>
<tr>
<td>LLM MMR</td>
<td>18.7</td>
<td>57.2</td>
<td>59.5</td>
<td>45.1</td>
<td>67.0</td>
<td>77.4</td>
<td>81.5</td>
<td>75.3</td>
<td>52.8^{âˆ—}</td>
<td>75.3^{âˆ—}</td>
<td>83.7^{âˆ—}</td>
<td>70.6^{âˆ—}</td>
</tr>
<tr>
<td>BERTScore NN</td>
<td>18.5</td>
<td>54.6</td>
<td>53.7</td>
<td>42.3</td>
<td>66.7</td>
<td>75.9</td>
<td>75.6</td>
<td>72.8</td>
<td>51.0</td>
<td>72.8</td>
<td>78.7</td>
<td>67.6</td>
</tr>
<tr>
<td>BERTScore MMR</td>
<td>19.4</td>
<td>56.3</td>
<td>53.9</td>
<td>43.2</td>
<td>68.2</td>
<td>78.1</td>
<td>77.8</td>
<td>74.7</td>
<td>52.0</td>
<td>73.7</td>
<td>78.2</td>
<td>68.0</td>
</tr>
</tbody>
</table>
<p>Table 4: Results of using MMR-based exemplar selection strategy on three datasets (Avg denotes the average). Using MMR generally selects better exemplars on all datasets, using either LM-based method or BERTScore. The results on text-davinci-002 use the LM-based scores provided by code-davinci-002 (denoted by ^{âˆ—}).</p>
<p>$\operatorname*{arg\,max}<em j="j">{q</em>)$}\in D/T}\lambda\mathcal{S}(q, q_{j})-(1-\lambda)max_{q_{i}\in T}\mathcal{S}(q_{j},q_{i</p>
<p>where $\mathcal{S}$ denotes similarity and $\lambda$ is a parameter that controls the balance between relevance and diversity. We rely on MMR to iteratively select exemplars from the exemplar pool, as shown in Algorithm (1). Note that this requires scoring all exemplar pairs within the pool. To run inference over $m$ queries using a pool of $n$ exemplars, MMR requires to score the similarity of $n n+m n$ pairs.</p>
<p>Results We apply the MMR strategy on top of LM-based method and BERTScore, leaving out the CLS-based approach which has inferior performance. The experimental setup largely follows Section 4.2, please refer to Appendix D for details.</p>
<p>We show the results in Table 3. In the setting that uses BERTScore, MMR-based selection successfully improves the performance for almost all LLMs for all datasets, compared to using nearest neighbors. On LM-based method, MMR is also able to improve the performance for GSM and ECQA across all LMs, and only marginally underperforms NN for E-SNLI.</p>
<p>In particular, using the MMR-based selection strategy achieves an accuracy of 68.2 and 78.1 on GSM and ECQA respectively, even outperforming LM-based method that requires a large number of queries to the LM. This suggests that BERTScore and MMR as a combination are able to construct effective explanation-infused prompts that approach that of actually querying LLMs. Furthermore, the fact that LLMs achieve better performance from the exemplars selected using our MMR-based method is congruent with our analysis in the previous section: LLMs can exploit complementary explanations.</p>
<h3>5.1 Analysis</h3>
<h4>Impacts of the Trade-off Between Relevance and Diversity</h4>
<p>We conduct an analysis to inves-</p>
<table>
<thead>
<tr>
<th>$\lambda$</th>
<th>GSM</th>
<th>ECQA</th>
<th>E-SNLI</th>
</tr>
</thead>
<tbody>
<tr>
<td>1.0</td>
<td>66.7</td>
<td>75.9</td>
<td>75.6</td>
</tr>
<tr>
<td>0.8</td>
<td>66.9</td>
<td>75.6</td>
<td>76.6</td>
</tr>
<tr>
<td>0.6</td>
<td>68.2</td>
<td>77.9</td>
<td>78.1</td>
</tr>
<tr>
<td>0.5</td>
<td>68.2</td>
<td>78.1</td>
<td>77.8</td>
</tr>
<tr>
<td>0.4</td>
<td>66.8</td>
<td>75.7</td>
<td>76.0</td>
</tr>
<tr>
<td>0.2</td>
<td>65.9</td>
<td>75.9</td>
<td>74.9</td>
</tr>
<tr>
<td>0.0</td>
<td>63.5</td>
<td>75.5</td>
<td>75.5</td>
</tr>
</tbody>
</table>
<p>Table 5: The performance of MMR exemplar selection strategy with varying $\lambda$.</p>
<table>
<thead>
<tr>
<th></th>
<th>GSM</th>
<th>ECQA</th>
<th>E-SNLI</th>
</tr>
</thead>
<tbody>
<tr>
<td>Random</td>
<td>65.4_{1.3}</td>
<td>74.1_{0.5}</td>
<td>74.0_{1.2}</td>
</tr>
<tr>
<td>NN</td>
<td>68.6_{0.7}</td>
<td>75.4_{0.5}</td>
<td>75.9_{1.1}</td>
</tr>
<tr>
<td>MMR</td>
<td>69.4_{1.0}</td>
<td>77.8_{0.7}</td>
<td>77.8_{0.9}</td>
</tr>
</tbody>
</table>
<p>Table 6: $\operatorname{Mean}_{\text {variance }}$ of the performance across 5 random order. Using better exemplars has more significant impact than varying exemplar order.
tigate how the trade-off between diversity and relevance impacts the performance. We test the performance under varying $\lambda$ on code-davinci-002 with BERTScore as the similarity metric. We note this is done on one pool of training exemplars. Generally, when $\lambda$ is large ( 0.8 ), the performance is similar to NN $(\lambda=1.0)$. MMR typically works well with a $\lambda$ of 0.6 or 0.5 (roughly balancing the two terms). The performance starts to degrade while decreasing $\lambda$ from 0.4 to 0 , as the selected exemplars are not relevant enough.</p>
<p>Sensitivity to Different Order We have shown choosing exemplar sets using MMR can lead to better ICL performance, which could be affected by other confounders such as the order of exemplars. We conduct experiments to show that using better exemplar sets has more impact than reordering exemplars. Specifically, we experiment with 5 random orders of the exemplar sets for each query and report averaged performance and variance of the accuracy. We note this is done on one pool of training exemplars for each dataset using</p>
<p>code-davinci-002 with BERTScore as the metric. As shown in Table 6, MMR is still substantially better than NN and Random under varying order.</p>
<h2>6 Related Work</h2>
<p>The growing scale of pretrained language models has granted them the ability to learn a new task from a few examples via in-context learning (Brown et al., 2020). Various approaches have been proposed to improve ICL in recent years, including meta-tuning LLMs (Min et al., 2022b; Chen et al., 2022), calibration of ICL (Zhao et al., 2021; Han et al., 2022), automatically determining the orders of exemplars (Lu et al., 2022), and alternative formulation of ICL based on PMI (Holtzman et al., 2021) or noisy-channel (Min et al., 2022a). More closely related to our work, prior research also contributes to better understanding ICL as Bayesian inference (Xie et al., 2022) or experiments that study what makes in-context learning works (Min et al., 2022c). Our work focuses on understanding the usage of explanations in ICL, as opposed to standard prompting where the LLMs are presented with only input-output pairs.</p>
<p>In particular, our work is connected to prior research on effective ways for selecting in-context exemplars (Shin et al., 2021; Liu et al., 2021; Rubin et al., 2022; Qiu et al., 2022; Su et al., 2022). While past work primarily focuses on the effectiveness of using relevant examples in the standard prompting paradigm, we examine the benefits of the complementary exemplars when prompting with explanations. We also propose an MMR-based strategy, which is more effective than the NN-based approach on various LLMs across three tasks.</p>
<p>Lastly, including textual explanations in prompts has exhibited remarkable benefits for LLMs to learn various reasoning tasks. Using Scratchpad (Nye et al., 2021) or Chain-of-Thought (Wei et al., 2022) significantly boosts ICL performance on multi-step reasoning tasks such as arithmetical reasoning and symbolic reasoning. Using free-text rationales is also helpful for more unstructured tasks like QA and NLI (Ye and Durrett, 2022; Wang et al., 2022a). While recent work largely aims to find better ways to prompt LLMs with explanations (Kojima et al., 2022; Zhou et al., 2022a; Press et al., 2022; Zhou et al., 2022b), we focus on analyzing the role of explanations in ICL and what makes effective explanations.</p>
<h2>7 Conclusion</h2>
<p>We have presented a series of studies on what makes effective explanations for in-context learning. We first investigated the impacts of computation traces and natural language in explanations. Through a set of probing experiments, we found that LLMs rely on both of them to effectively learn from explanations. We further examined the interplay among exemplars within prompts and the interplay between exemplars and the query. Our analysis uncovered the benefits of constructing prompts by selecting complementary explanations that are relevant to the query. Lastly, we proposed an MMRbased exemplar selection strategy, which successfully improved the end task performance across three important datasets.</p>
<h2>8 Limitations</h2>
<p>The models chosen in this work are selected to represent the state-of-the-art at the time the work was conducted, and in some cases omit weaker models. For example, our exemplar selection experiments do not cover those LLMs trained with vanilla language models objectives, namely OPT and davinci, as we find their performance substantially lags code-davinci-002 and text-davinci-002. For the same reason, we only consider the substantially large language models, omitting LLMs of smaller scales (e.g., text-curie-001). Running experiments using smaller LMs or vanilla LMs may provide insights into how scale or instruction finetuning impacts the ability of LMs in learning from explanations, but our investigation mainly focus on selecting exemplars to achieve the best in-context learning performance with state-of-the-art models.</p>
<p>In addition, certain aspects of our approach are computationally intensive, particularly using LMbased similarity scores. However, we think this is still feasible in practice: if practitioners are deploying a real-world system, investing more computation upfront to improve its performance is likely in reach for those deploying LLMs in practice.</p>
<p>Finally, our experiments consider a certain subset of NLP reasoning tasks written in English. While we believe the results here transfer to other tasks in this vein which have been frequently used to evaluate LLMs, it is unknown how well they handle other languages, dialects, or genres of text such as social media data.</p>
<h2>Acknowledgments</h2>
<p>Thanks to anonymous reviewers for their helpful feedback and colleagues at Meta AI for helpful discussions. This work was partially supported by NSF CAREER Award IIS-2145280 and the NSF AI Institute for Foundations of Machine Learning (IFML).</p>
<h2>References</h2>
<p>Shourya Aggarwal, Divyanshu Mandowara, Vishwajeet Agrawal, Dinesh Khandelwal, Parag Singla, and Dinesh Garg. 2021. Explanations for CommonsenseQA: New Dataset and Models. In Proceedings of the Annual Conference of the Association for Computational Linguistics (ACL).</p>
<p>Stephen H. Bach, Victor Sanh, Zheng-Xin Yong, Albert Webson, Colin Raffel, Nihal V. Nayak, Abheesht Sharma, Taewoon Kim, M Saiful Bari, Thibault Fevry, Zaid Alyafeai, Manan Dey, Andrea Santilli, Zhiqing Sun, Srulik Ben-David, Canwen Xu, Gunjan Chhablani, Han Wang, Jason Alan Fries, Maged S. Al-shaibani, Shanya Sharma, Urmish Thakker, Khalid Almubarak, Xiangru Tang, Xiangru Tang, Mike Tian-Jian Jiang, and Alexander M. Rush. 2022. PromptSource: An Integrated Development Environment and Repository for Natural Language Prompts. arXiv, abs/2202.01279.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Proceedings of the Conference on Advances in Neural Information Processing Systems (NeurIPS).</p>
<p>Oana-Maria Camburu, Tim RocktÃ¤schel, Thomas Lukasiewicz, and Phil Blunsom. 2018. e-snli: Natural language inference with natural language explanations. In Proceedings of the Conference on Advances in Neural Information Processing Systems (NeurIPS).</p>
<p>Jaime Carbonell and Jade Goldstein. 1998. The Use of MMR, Diversity-Based Reranking for Reordering Documents and Producing Summaries. In Proceedings of the Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR). Association for Computing Machinery.</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde, Jared Kaplan, Harrison Edwards, Yura Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael</p>
<p>Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, David W. Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William H. Guss, Alex Nichol, Igor Babuschkin, S. Arun Balaji, Shantanu Jain, Andrew Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew M. Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. Evaluating large language models trained on code. ArXiv, abs/2107.03374.</p>
<p>Yanda Chen, Ruiqi Zhong, Sheng Zha, George Karypis, and He He. 2022. Meta-learning via language model in-context tuning. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers).</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Baindoor Rao, Parker Barnes, Yi Tay, Noam M. Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Benton C. Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier GarcÃ­a, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Oliveira Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathleen S. Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022. Palm: Scaling language modeling with pathways. ArXiv, abs/2204.02311.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT).</p>
<p>Zhi Han, Yaru Hao, Li Dong, and Furu Wei. 2022. Prototypical calibration for few-shot learning of language models. ArXiv, abs/2205.10183.</p>
<p>Ari Holtzman, Peter West, Vered Shwartz, Yejin Choi, and Luke Zettlemoyer. 2021. Surface form competition: Why the highest probability answer isn't always right. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing.</p>
<p>Jaehun Jung, Lianhui Qin, Sean Welleck, Faeze Brahman, Chandra Bhagavatula, Ronan Le Bras, and Yejin Choi. 2022. Maieutic prompting: Logically consistent reasoning with recursive explanations. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. ArXiv, abs/2205.11916.</p>
<p>Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2021. What makes good in-context examples for gpt-3? ArXiv, abs/2101.06804.</p>
<p>Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2022. Fantastically ordered prompts and where to find them: Overcoming fewshot prompt order sensitivity. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers).</p>
<p>Aman Madaan, Shuyan Zhou, Uri Alon, Yiming Yang, and Graham Neubig. 2022. Language models of code are few-shot commonsense learners. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</p>
<p>Sewon Min, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022a. Noisy channel language model prompting for few-shot text classification. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers).</p>
<p>Sewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2022b. MetaICL: Learning to learn in context. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.</p>
<p>Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022c. Rethinking the role of demonstrations: What makes in-context learning work? In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</p>
<p>Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Augustus Odena. 2021. Show your work: Scratchpads for intermediate computation with language models. ArXiv, abs/2112.00114.</p>
<p>Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke E. Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Francis Christiano, Jan Leike, and Ryan J. Lowe. 2022. Training language models to follow instructions with human feedback. ArXiv, abs/2203.02155.</p>
<p>Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A. Smith, and Mike Lewis. 2022. Measuring and narrowing the compositionality gap in language models. ArXiv, abs/2210.03350.</p>
<p>Linlu Qiu, Peter Shaw, Panupong Pasupat, Tianze Shi, Jonathan Herzig, Emily Pitler, Fei Sha, and Kristina Toutanova. 2022. Evaluating the impact of model scale for compositional generalization in semantic parsing. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</p>
<p>Ohad Rubin, Jonathan Herzig, and Jonathan Berant. 2022. Learning to retrieve prompts for in-context learning. In Proceedings of the Annual Conference of the Association for Computational Linguistics (ACL).</p>
<p>Richard Shin, Christopher Lin, Sam Thomson, Charles Chen, Subhro Roy, Emmanouil Antonios Platanios, Adam Pauls, Dan Klein, Jason Eisner, and Benjamin Van Durme. 2021. Constrained language models yield few-shot semantic parsers. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</p>
<p>Hongjin Su, Jungo Kasai, Chen Henry Wu, Weijia Shi, Tianlu Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf, Luke Zettlemoyer, Noah A Smith, et al. 2022. Selective annotation makes language models better fewshot learners. arXiv preprint arXiv:2209.01975.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. 2022a. Rationaleaugmented ensembles in language models. ArXiv, abs/2207.00747.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. 2022b. Self-consistency improves chain of thought reasoning in language models. ArXiv, abs/2203.11171.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models. ArXiv, abs/2201.11903.</p>
<p>Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. 2022. An explanation of in-context learning as implicit bayesian inference. In International Conference on Learning Representations.</p>
<p>Xi Ye and Greg Durrett. 2022. The unreliability of explanations in few-shot prompting for textual reasoning. In Proceedings of the Conference on Advances in Neural Information Processing Systems (NeurIPS).</p>
<p>Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022. OPT: Open Pre-trained Transformer Language Models. ArXiv, abs/2205.01068.</p>
<p>Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. BERTScore: Evaluating Text Generation with BERT. In Proceedings of the International Conference on Learning Representations (ICLR).</p>
<p>Tony Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate before use: Improving few-shot performance of language models. In Proceedings of the International Conference on Learning Representations (ICLR).</p>
<p>Denny Zhou, Nathanael Scharli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, Quoc Le, and Ed Chi. 2022a. Least-to-most prompting enables complex reasoning in large language models. ArXiv, abs/2205.10625.</p>
<p>Hattie Zhou, Azade Nova, H. Larochelle, Aaron C. Courville, Behnam Neyshabur, and Hanie Sedghi. 2022b. Teaching algorithmic reasoning via incontext learning. ArXiv, abs/2211.09066.</p>
<table>
<thead>
<tr>
<th>A</th>
<th>Details of the Explanations Used for LETCAT and CoinFLIP on OPT</th>
</tr>
</thead>
<tbody>
<tr>
<td>$\begin{aligned} &amp; \frac{\pi}{2} \ &amp; \frac{\pi}{2} \end{aligned}$</td>
<td>Question: Take the last letters of the words in "Bill Gates" and concatenate them.</td>
</tr>
<tr>
<td></td>
<td>Ours: Add space to "Bill" and get "B i11", the last letter is l. Add space to "Gates" and get "G a t e s", the last letter is s. Concatenating l and s is ls. So the answer is ls.</td>
</tr>
<tr>
<td></td>
<td>Wei et al. (2022): The last letter of Bill is l. The last letter of Gates is s. Concatenating l and s is ls. So the answer is ls.</td>
</tr>
<tr>
<td>$\begin{aligned} &amp; \frac{\pi}{2} \ &amp; \frac{\pi}{2} \end{aligned}$</td>
<td>Question: A coin is heads up. Shaunda does not flip the coin. Shalonda flips the coin. Is the coin still heads up?</td>
</tr>
<tr>
<td></td>
<td>Ours: The coin started heads up. Shaunda does not flip the coin, so it becomes heads up. Shalonda flips the coin, so it becomes tails up. So the answer is no.</td>
</tr>
<tr>
<td></td>
<td>Wei et al. (2022): The coin was flipped by Shalonda. So the coin was flipped 1 time, which is an odd number. The coin started heads up, so after an odd number of flips, it will be tails up. So the answer is no.</td>
</tr>
</tbody>
</table>
<p>Figure 4: Examples of original chain-of-thoughts from Wei et al. (2022) and ours used for OPT.</p>
<table>
<thead>
<tr>
<th></th>
<th>LETCAT</th>
<th>COINFLIP</th>
</tr>
</thead>
<tbody>
<tr>
<td>Standard</td>
<td>8.5</td>
<td>51.5</td>
</tr>
<tr>
<td>(Wei et al., 2022)</td>
<td>29.5</td>
<td>61.0</td>
</tr>
<tr>
<td>Ours</td>
<td>50.0</td>
<td>94.0</td>
</tr>
</tbody>
</table>
<p>Table 7: Performance of original chain-of-thoughts and our explanations used for OPT.</p>
<p>For GSM, we directly use the gold explanations provided in Wei et al. (2022). For LETCAT and CoinFLIP, we take the original explanations from Wei et al. (2022) and manually engineered them, as the original ones are sub-optimal for OPT and do not lead to credible gains compared to standard prompting.</p>
<p>We show examples of the original explanations (chain-of-thoughts) used in Wei et al. (2022) and the explanation we adapted for OPT in Figure 4. For LETCAT, we add another step of tokenizing the two words. For CoinFLIP, we change the way of decomposing the problem. As shown in Table 7, our adapted explanations lead to more substantial performance improvements over standard prompting. We use engineered explanations for the probing experiments on OPT, which allows more distinguishable performance differences. We refer readers to the supplementary materials of Wei et al. (2022) for the complete set of exemplars and explanations.</p>
<h2>Appendix B Details of the Choice of Masks</h2>
<p>We conduct preliminary experiments on the LETCAT dataset using davinci to determine the choice</p>
<table>
<thead>
<tr>
<th></th>
<th>N/A</th>
<th>[mask]</th>
<th>?</th>
<th>_</th>
<th>Empty Str</th>
</tr>
</thead>
<tbody>
<tr>
<td>Standard</td>
<td>8.5</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Gold</td>
<td>59.0</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Mask1</td>
<td>14.0</td>
<td>14.0</td>
<td>15.0</td>
<td>13.5</td>
<td>16.0</td>
</tr>
<tr>
<td>Mask2</td>
<td>48.0</td>
<td>48.0</td>
<td>48.5</td>
<td>43.0</td>
<td>49.5</td>
</tr>
</tbody>
</table>
<p>Table 8: Results of using different mask tokens for LETCAT on OPT.
of masks. We tested masking with "N/A", "[mask]", "?", "_, and empty string. The results obtained using different masks are shown in Table 8. Whatever masks are used, LLMs see performance degradation compared to gold explanations, but can still learn from partially complete explanations. We use an empty string as the mask token across all datasets, which leads to the least performance degradation.</p>
<h2>Appendix C Details of the Setup for Exemplar-Exemplar Interplay Experiments</h2>
<p>For LETCAT, we experiment with 4 exemplars where the first steps are perturbed, 4 exemplars where the second steps are perturbed and a mixture of 2 from each of these explanations. For CoinFLIP, we use 8 exemplars and follow the same setting. For the mixture type of prompts, we experiment with 4 random combinations for mixing two types of masked exemplars.</p>
<p>For GSM, we use three types of prompts constructed by 1) 8 addition-only exemplars, 2) 8 multiplication-only exemplars, and 3) a mixture of 4 exemplars from each of the two types. We note that unlike what's in LETCAT and CoinFLIP which uses identical exemplars perturbed in different ways, the exemplars in the three sets, for GSM are drawn from different pools. We experiment with 4 different sets of randomly drawn examples and report the average in the setting. We note the test set for GSM that requires composing addition and multiplication contains 1,150 data points in total.</p>
<h2>Appendix D Details of the Setup for MMR-Based Exemplar Selection Experiments</h2>
<p>We evaluate the effectiveness of our MMR-based exemplar selection strategy on the three realistic datasets used in Section 4.2. Also, the experiments on text-davinci-002 using the LMbased method rely on similarity scores obtained</p>
<p>from code-davinci-002, given the prohibitive cost needed to run these experiments. We do not tune $\lambda$ in our experiments. $\lambda$ is set to roughly balance the variance among the two terms in Equation (1), which is 0.5 across all datasets and methods, except for using the LM-based method on code-davinci-001. For this particular setting, we set lambda to be $\frac{2}{3}$, as we observe higher variance among the diversity with exemplars.</p>
<h2>E Details of Prompts for Real-world Datasets</h2>
<p>We showcase how we format the prompts for GSM, ECQA, and E-SNLI in Figure 5, with one exemplar for each of the three datasets. We note the prompt format for E-SNLI is taken from PromptSource (Bach et al., 2022).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">GSM</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Q: Leah had 32 chocolates and her sister had 42. If they</td>
</tr>
<tr>
<td style="text-align: left;">ate 35, how many pieces do they have left in total?</td>
</tr>
<tr>
<td style="text-align: left;">A: Leah had 32 chocolates and Leah's sister had 42. That</td>
</tr>
<tr>
<td style="text-align: left;">means there were originally $32+42=74$ chocolates. 35</td>
</tr>
<tr>
<td style="text-align: left;">have been eaten. So in total they still have $74-35=39$</td>
</tr>
<tr>
<td style="text-align: left;">chocolates. The answer is 39.</td>
</tr>
<tr>
<td style="text-align: left;">ECQA</td>
</tr>
<tr>
<td style="text-align: left;">Q: Where can you get a bugle to take home with you?</td>
</tr>
<tr>
<td style="text-align: left;">Answer Choices:</td>
</tr>
<tr>
<td style="text-align: left;">(a) farmer's wife</td>
</tr>
<tr>
<td style="text-align: left;">(b) music store</td>
</tr>
<tr>
<td style="text-align: left;">(c) military base</td>
</tr>
<tr>
<td style="text-align: left;">(d) military band</td>
</tr>
<tr>
<td style="text-align: left;">(e) american army.</td>
</tr>
<tr>
<td style="text-align: left;">A: Bugle is a musical instrument. Musical instruments are</td>
</tr>
<tr>
<td style="text-align: left;">available in a music store. Music store is a building. So</td>
</tr>
<tr>
<td style="text-align: left;">the answer is (b).</td>
</tr>
<tr>
<td style="text-align: left;">E-SNLI</td>
</tr>
<tr>
<td style="text-align: left;">Premise:</td>
</tr>
<tr>
<td style="text-align: left;">"A man at a flea market browsing."</td>
</tr>
<tr>
<td style="text-align: left;">Based on this premise, can we conclude the hypothesis "A</td>
</tr>
<tr>
<td style="text-align: left;">man is sleeping at a flea market." is true?</td>
</tr>
<tr>
<td style="text-align: left;">OPTIONS:</td>
</tr>
<tr>
<td style="text-align: left;">- yes</td>
</tr>
<tr>
<td style="text-align: left;">- no</td>
</tr>
<tr>
<td style="text-align: left;">- not possible to tell</td>
</tr>
<tr>
<td style="text-align: left;">A: One cannot be sleeping and browsing at the same time.</td>
</tr>
<tr>
<td style="text-align: left;">The answer is no.</td>
</tr>
</tbody>
</table>
<p>Figure 5: Detailed examples of prompts for GSM, ECQA, and E-SNLI.</p>
<h2>F License of Datasets</h2>
<ul>
<li>GSM (Cobbe et al., 2021): MIT license.</li>
<li>E-SNLI (Camburu et al., 2018): MIT license.</li>
<li>ECQA (Aggarwal et al., 2021):Community Data License Agreement - Sharing - Version 1.0.</li>
<li>Letter Concatenation and CoinFlip (Wei et al., 2022): MIT license.</li>
</ul>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ The similarity is measured only based on the input part and excludes the explanations part, as we do not have access to the explanation of the query in the test phase.
${ }^{3}$ For instance, calculating the similarity between 500 queries and a pool of 500 exemplars for a dataset whose typical question token number is 50, would cost $500 using GPT-3 API (rate: $50.02/1000$ tokens).&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5808 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5808</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5808</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-116.html">extraction-schema-116</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <p><strong>Paper ID:</strong> paper-b0bac6aca93021105c8a4f165184a097a249fbce</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/b0bac6aca93021105c8a4f165184a097a249fbce" target="_blank">Evaluating the Zero-shot Robustness of Instruction-tuned Language Models</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> A simple method is proposed by introducing ``soft prompt'' embedding parameters and optimizing these to maximize the similarity between representations of semantically equivalent instructions, and it is shown that this method consistently improves the robustness of instruction-tuned models.</p>
                <p><strong>Paper Abstract:</strong> Instruction fine-tuning has recently emerged as a promising approach for improving the zero-shot capabilities of Large Language Models (LLMs) on new tasks. This technique has shown particular strength in improving the performance of modestly sized LLMs, sometimes inducing performance competitive with much larger model variants. In this paper we ask two questions: (1) How sensitive are instruction-tuned models to the particular phrasings of instructions, and, (2) How can we make them more robust to such natural language variation? To answer the former, we collect a set of 319 instructions manually written by NLP practitioners for over 80 unique tasks included in widely used benchmarks, and we evaluate the variance and average performance of these instructions as compared to instruction phrasings observed during instruction fine-tuning. We find that using novel (unobserved) but appropriate instruction phrasings consistently degrades model performance, sometimes substantially so. Further, such natural instructions yield a wide variance in downstream performance, despite their semantic equivalence. Put another way, instruction-tuned models are not especially robust to instruction re-phrasings. We propose a simple method to mitigate this issue by introducing ``soft prompt'' embedding parameters and optimizing these to maximize the similarity between representations of semantically equivalent instructions. We show that this method consistently improves the robustness of instruction-tuned models.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5808.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5808.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Observed_vs_Unobserved_Instructions</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of Observed (Training) vs Unobserved (Novel) Instruction Phrasings</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Comparison of zero-shot performance when instruction-tuned LLMs are given instructions seen during instruction fine-tuning (observed) versus novel but semantically appropriate instructions (unobserved). The paper measures accuracy changes across multiple models, datasets, and task types and finds consistent degradation with unobserved phrasings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple (Flan-T5 family, Alpaca, T0++)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (80M -- 11B; specific sizes include Flan-T5 small/base/large/XL/XXL, Alpaca-7B/13B, T0++ 11B)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MMLU and BIG-BENCH LITE (BBL) — QA, Binary Classification, Multi-class Classification</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>MMLU: 57 multiple-choice, expert-knowledge QA tasks; BBL (LITE): selected QA, binary, and multi-class tasks from BIG-BENCH.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Zero-shot instruction following. Two main prompt types: (a) 'Observed' instruction templates drawn from the instruction-tuning datasets (Flan/P3/Alpaca) used at evaluation, and (b) 'Unobserved' instructions manually written by NLP researchers (319 novel phrasings) that are semantically appropriate but were not present during model training. Prompts include prefixed task directives plus the example/instance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Observed (training-seen) instruction templates vs. Unobserved (novel) instruction phrasings</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Varies by model and dataset; examples from Table 3: Flan-T5-3B overall observed accuracy 57.3% (±1.7) vs unobserved 54.2% (±5.1); Alpaca-7B overall observed 44.1% (±2.3) vs unobserved 38.5% (±4.7); Flan-T5-11B overall observed 61.4% (±2.1) vs unobserved 59.2% (±5.2).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Paper reports a typical accuracy reduction when using unobserved instructions; authors state 'unobserved instructions reduce accuracy by over five points across models' (text). Table 3 shows per-model overall drops: Flan-T5-3B Δ -3.1 pp, Alpaca-7B Δ -5.6 pp, T0++11B Δ -2.8 pp, Flan-T5-11B Δ -2.2 pp, Alpaca-13B Δ -5.2 pp.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>reduced</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors hypothesize and provide evidence that instruction-tuned models implicitly rely on instruction phrasings observed during fine-tuning; semantic/representational divergence between novel and training instructions causes degradation. Classification tasks appear more sensitive. Models do not fully generalize to natural paraphrases even if semantically equivalent.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>MMLU exhibited much smaller degradation than BBL; for some settings (e.g., T0++ on MMLU) unobserved instructions produced equal or slightly higher accuracy (e.g., T0++ MMLU Δ +0.2 pp), suggesting that when unobserved instructions are similar to training templates (low semantic distance) effects can be minimal or absent.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating the Zero-shot Robustness of Instruction-tuned Language Models', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5808.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5808.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IncorrectObserved_vs_AppropriateUnobserved</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Using Incorrect but Observed Instructions vs Correct but Unobserved Instructions (Closer Look experiment)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Experiment comparing several instruction variants: 'Closest' (observed closest task instruction), 'Incorrect' (an observed instruction for a different/inappropriate task with same output format), 'Negated' (negated instruction), 'Task designer' (task author prefix), 'Collected' (novel instructions from researchers), and 'Nonsensical'.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Flan-T5-XXL (and aggregated across models for the 'closer look' analysis)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>XXL (11B) for the example; analysis aggregated across multiple models in that experiment</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Subset of BIG-BENCH tasks (7 datasets used in the 'closer look' experiment)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Seven representative classification/QA tasks from BIG-BENCH used to probe instruction sensitivity with multiple instruction variants.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Zero-shot prompt variants as listed above (Closest, Incorrect-observed, Collected-unobserved, Task-designer, Negated, Nonsensical). Each variant supplies a different instruction prefix while keeping instance content identical.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Multiple comparative instruction variants (observed-appropriate, observed-inappropriate, unobserved-appropriate, negated, nonsensical)</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Aggregated averages (Table 16): Closest (observed appropriate) overall ≈ 60.45%; Incorrect (observed but inappropriate) ≈ 57.4%; Collected (novel/unobserved appropriate) ≈ 56.8%; Task Designer ≈ 56.4%; Negated ≈ 30.0%; Nonsensical ≈ 39.3%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Incorrect-but-observed outperforms Collected (observed-inappropriate ≈ 57.4% vs collected-unobserved ≈ 56.8%) on average in this experiment; large drops for negated (~ -30 pp) and nonsensical (~ -20 pp relative to observed-closest).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>format can increase or decrease performance depending on whether the instruction was observed during training; observed-but-inappropriate can outperform semantically-correct-unobserved prompts (i.e., format seen in training helps even when semantically wrong).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors interpret this as evidence that models rely heavily on familiar instruction templates seen during fine-tuning; exposure to an observed instruction (even if semantically incorrect) can trigger learned behavior aligned with the seen template and thus sometimes outperform semantically correct but novel phrasings.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating the Zero-shot Robustness of Instruction-tuned Language Models', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5808.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5808.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Semantic_Distance_vs_Performance</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Representational (Semantic) Distance between Instructions and Performance</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Analysis linking representational similarity (penultimate-layer embeddings for first decoded token) between observed and unobserved instructions to performance degradation; includes t-SNE visualization and average L2 distances per dataset category.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Flan-T5-XXL (representational visualizations) and analyses reported across models</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>XXL (11B) for visualization; distances computed for other sizes as well</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MMLU and BBL (BIG-BENCH LITE)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same tasks as the main evaluation; representation extraction for instruction+example instances to compute distances between unobserved and nearest observed instruction representations.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Zero-shot prompts; representation for 'first decoded token' is extracted for instances under different instruction templates; distances and t-SNE used to examine distributional shift.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Observed instruction representations vs Unobserved instruction representations, measured via L2 distance and t-SNE clustering</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Table 4 reports average Δℓ2 distances and average Δ accuracy: MMLU Avg Δℓ2 = 19.8 with Avg ΔAcc ≈ -0.5 pp; BBL-QA Avg Δℓ2 = 37.9 with Avg ΔAcc ≈ -3.4 pp; BBL-BC Δℓ2=25.3 ΔAcc -2.0 pp; BBL-MC Δℓ2=26.1 ΔAcc -2.8 pp.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Quantitative relationship: higher representational distance correlates with larger performance drops. Linear fit reported with intercept ≈ -0.8 and slope ≈ -0.2 (p = 0.08) linking similarity of first decoded token embeddings to percent performance degradation (weak but negative relationship).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>Greater semantic/representational divergence between unobserved and observed instructions -> reduced performance</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>When unobserved instructions induce representations closer to observed training instructions (small semantic distance), models generalize better. MMLU unobserved instructions were closer to observed ones (smaller Δℓ2), explaining its relative robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>Relationship is present but relatively weak (slope -0.2, p=0.08); semantic distance does not fully predict performance in all cases.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating the Zero-shot Robustness of Instruction-tuned Language Models', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5808.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5808.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InContext_Learning_vs_ZeroShot</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of In-Context Learning (Few-shot examples) on Instruction Robustness</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Assessment of whether including few-shot examples in the prompt reduces sensitivity to instruction phrasing; replicates zero-shot experiments in a few-shot (one-shot) setting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Same instruction-tuned models (Flan-T5 variants, Alpaca, T0++ as tested in zero-shot experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (as above)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MMLU and BBL (selected tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>One-shot in-context learning: prompts include a single example (demonstration) plus the instruction and test instance.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>One-shot/few-shot prompts where demonstrations (examples) are added to the prompt (ICL) compared to zero-shot instruction-only prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Zero-shot (instruction only) vs one-shot in-context learning (instruction + example)</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative improvement: Figure 6 and text indicate that one-shot ICL 'slightly decreases the sensitivity' to unobserved instructions; performance differences between observed and unobserved instructions move closer to zero under one-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Described qualitatively as 'slight' reduction in performance gap; no single aggregate numeric effect size is provided in main text, but plots show convergence of observed/unobserved accuracy differences toward zero under one-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved (reduced sensitivity)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Examples in context likely implicitly demonstrate the desired task and label format, reducing reliance on the instruction phrasing and thereby making models more robust to instruction paraphrases.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>Sensitivity is reduced but not eliminated; some discrepancy between formats remains under ICL.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating the Zero-shot Robustness of Instruction-tuned Language Models', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5808.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5808.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Scaling_Effect</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Model Scale (Parameters) vs Instruction Robustness</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Investigation of whether increasing model size (Flan-T5 from 80M up to 11B) reduces sensitivity to instruction paraphrasing; includes per-size MMLU results.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Flan-T5 family (Small/Base/Large/XL/XXL)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>80M, 250M, 780M, 3B (XL), 11B (XXL)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MMLU (and other BBL subsets shown in scaling plots)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Zero-shot instruction-following evaluated across model sizes to see if larger models show emergent robustness to unobserved instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Zero-shot observed vs unobserved instruction prompts tested across model sizes</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Small->Large scale comparison of same prompt formats</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>MMLU (Table 9): Observed accuracies by size: Small 29.4%, Base 34.1%, Large 41.1%, XL 48.1%, XXL 53.2%. Unobserved: Small 29.6%, Base 33.8%, Large 40.7%, XL 47.5%, XXL 52.7%. Differences remain across sizes but do not vanish.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Difference observed vs unobserved across sizes is small and does not consistently shrink with scale up to 11B (e.g., XXL MMLU observed 53.2% vs unobserved 52.7% Δ -0.5 pp).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>no strong emergent robustness with scale (up to 11B); effect persists</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors note that larger models (beyond 11B, e.g., 175B) might offer greater robustness, but within evaluated mid-sized regimes scaling did not eliminate sensitivity to instruction wording.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>For some datasets and sizes the gap is negligible (e.g., small differences at XXL), and in a few cases unobserved performed slightly better (see T0++ MMLU), indicating no universal monotonic effect of scale within the evaluated range.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating the Zero-shot Robustness of Instruction-tuned Language Models', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5808.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5808.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SoftPrompt_Alignment_(PT+KL)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Soft Prompt Alignment: Prefix Tuning with KL Alignment Loss (PT+KL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A mitigation introduced by the paper: add trainable 'soft prompt' prefix embeddings and a KL-divergence loss between output distributions induced by reference vs paraphrased instructions; only prefix embeddings are optimized (prefix tuning) using automatically generated paraphrases.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Flan-T5-XL (3B) and Alpaca-7B in experiments; method applicable to other instruction-tuned models</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>3B (Flan-T5-XL) and 7B (Alpaca-7B) in reported experiments</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MMLU and BBL (same evaluation buckets as main experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Zero-shot evaluation; method fine-tunes only soft prompt parameters using an augmented dataset of automatically generated paraphrased instructions and a combined cross-entropy + KL loss.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Training-time modification: introduce n soft prefix tokens (R^{d x n}) prepended to instructions; generate paraphrased instructions (via GPT-4) to pair with reference instructions; objective includes standard CE (teacher-forcing) and KL divergence between output distributions of paraphrase and reference; at evaluation, the model receives test instructions (observed or unobserved) unchanged but with the learned soft prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Baseline instruction-tuned model (no adaptation) vs FT (fine-tune all params on paraphrases), FT+KL, PT (prefix tuning without KL), and PT+KL (full proposed method).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Table 5: PT+KL yields consistent improvements on unobserved instructions: Example highlights — Flan-T5-3B unobserved MMLU baseline 47.5% → PT+KL 47.9% (+0.4 pp); BBL unobserved 51.9% → PT+KL 53.7% (+1.8 pp). Alpaca-7B unobserved overall baseline 39.7% → PT+KL 41.8% (+2.1 pp); BBL unobserved 42.9% → PT+KL 46.6% (+3.7 pp).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>FT (fine-tuning all params on paraphrases) often degraded performance (e.g., Flan-T5-3B FT overall Δ -8.0 pp), FT+KL mitigated but still worse than PT+KL; PT alone gave small/no benefit, PT+KL gave the best and most consistent improvements on unobserved instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Improvements modest but consistent: PT+KL increased unobserved-instruction accuracy by up to ~3.7 percentage points in some settings (e.g., Alpaca-7B on BBL), with smaller gains in other datasets (e.g., ~0.4 pp on MMLU for Flan-T5-XL). Table 6 shows decreased representational distance accompanying gains (e.g., BBL-BC closest distance 30.1→27.9 with +4.2% acc improvement reported in table).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Encouraging model to produce similar output distributions (via KL) for paraphrased vs reference instructions makes the model's internal representations for semantically equivalent instructions closer, reducing sensitivity to surface wording; prefix tuning restricts updates to soft prompt parameters, avoiding catastrophic changes from full fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>Simple fine-tuning on paraphrases (FT) often harmed performance, indicating that naive augmentation without the KL alignment or prefix constraint can be detrimental.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating the Zero-shot Robustness of Instruction-tuned Language Models', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5808.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5808.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FineTuning_on_Paraphrases_(FT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Full Fine-Tuning on Automatically Generated Paraphrased Instructions (FT and FT+KL ablations)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Ablation where the entire model is fine-tuned on automatically generated paraphrase-augmented training data, tested with and without the KL alignment term.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Flan-T5-XL (3B) and Alpaca-7B (examples reported)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>3B (Flan-T5-XL) and 7B (Alpaca-7B)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MMLU and BBL (same benchmarks as main experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Models are fine-tuned (all parameters) on augmented datasets containing paraphrased instructions generated automatically (via GPT-4/generation procedures described); evaluated on observed and unobserved instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Training-time augmentation: full parameter fine-tuning (FT) on paraphrase-augmented instruction data; variant FT+KL adds the KL alignment objective.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Baseline vs FT vs FT+KL vs PT vs PT+KL</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Table 5: FT often harmed performance: e.g., Flan-T5-3B FT MMLU observed 39.4% (Δ -8.7) vs baseline 48.1%; FT unobserved 40.1% (Δ -7.4). FT+KL mitigated some loss but still underperformed PT+KL in many cases (Flan-T5-3B FT+KL MMLU unobs 43.6% vs PT+KL 47.9%). Alpaca-7B FT shifts were smaller but FT+KL still under PT+KL.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>FT sometimes results in very large negative changes (e.g., Flan-T5-3B MMLU Δ ≈ -8.0 pp overall), demonstrating that naive full fine-tuning on paraphrase data can degrade zero-shot capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>reduced (in many reported cases)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Full fine-tuning on paraphrases may overfit to augmented data distribution or disrupt original instruction-following behavior; constraining updates (prefix tuning) plus an explicit alignment objective (KL) avoids this harm and yields net improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>FT+KL sometimes recovers more performance than FT alone but still often underperforms PT+KL, showing that full fine-tuning is not a reliable remedy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating the Zero-shot Robustness of Instruction-tuned Language Models', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Do prompt-based models really understand the meaning of their prompts? <em>(Rating: 2)</em></li>
                <li>Can large language models truly understand prompts? a case study with negated prompts <em>(Rating: 2)</em></li>
                <li>Robustness of learning from task instructions <em>(Rating: 2)</em></li>
                <li>The flan collection: Designing data and methods for effective instruction tuning <em>(Rating: 1)</em></li>
                <li>Multitask prompted training enables zero-shot task generalization <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5808",
    "paper_id": "paper-b0bac6aca93021105c8a4f165184a097a249fbce",
    "extraction_schema_id": "extraction-schema-116",
    "extracted_data": [
        {
            "name_short": "Observed_vs_Unobserved_Instructions",
            "name_full": "Effect of Observed (Training) vs Unobserved (Novel) Instruction Phrasings",
            "brief_description": "Comparison of zero-shot performance when instruction-tuned LLMs are given instructions seen during instruction fine-tuning (observed) versus novel but semantically appropriate instructions (unobserved). The paper measures accuracy changes across multiple models, datasets, and task types and finds consistent degradation with unobserved phrasings.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Multiple (Flan-T5 family, Alpaca, T0++)",
            "model_size": "various (80M -- 11B; specific sizes include Flan-T5 small/base/large/XL/XXL, Alpaca-7B/13B, T0++ 11B)",
            "task_name": "MMLU and BIG-BENCH LITE (BBL) — QA, Binary Classification, Multi-class Classification",
            "task_description": "MMLU: 57 multiple-choice, expert-knowledge QA tasks; BBL (LITE): selected QA, binary, and multi-class tasks from BIG-BENCH.",
            "problem_format": "Zero-shot instruction following. Two main prompt types: (a) 'Observed' instruction templates drawn from the instruction-tuning datasets (Flan/P3/Alpaca) used at evaluation, and (b) 'Unobserved' instructions manually written by NLP researchers (319 novel phrasings) that are semantically appropriate but were not present during model training. Prompts include prefixed task directives plus the example/instance.",
            "comparison_format": "Observed (training-seen) instruction templates vs. Unobserved (novel) instruction phrasings",
            "performance": "Varies by model and dataset; examples from Table 3: Flan-T5-3B overall observed accuracy 57.3% (±1.7) vs unobserved 54.2% (±5.1); Alpaca-7B overall observed 44.1% (±2.3) vs unobserved 38.5% (±4.7); Flan-T5-11B overall observed 61.4% (±2.1) vs unobserved 59.2% (±5.2).",
            "performance_comparison": null,
            "format_effect_size": "Paper reports a typical accuracy reduction when using unobserved instructions; authors state 'unobserved instructions reduce accuracy by over five points across models' (text). Table 3 shows per-model overall drops: Flan-T5-3B Δ -3.1 pp, Alpaca-7B Δ -5.6 pp, T0++11B Δ -2.8 pp, Flan-T5-11B Δ -2.2 pp, Alpaca-13B Δ -5.2 pp.",
            "format_effect_direction": "reduced",
            "explanation_or_hypothesis": "Authors hypothesize and provide evidence that instruction-tuned models implicitly rely on instruction phrasings observed during fine-tuning; semantic/representational divergence between novel and training instructions causes degradation. Classification tasks appear more sensitive. Models do not fully generalize to natural paraphrases even if semantically equivalent.",
            "counterexample_or_null_result": "MMLU exhibited much smaller degradation than BBL; for some settings (e.g., T0++ on MMLU) unobserved instructions produced equal or slightly higher accuracy (e.g., T0++ MMLU Δ +0.2 pp), suggesting that when unobserved instructions are similar to training templates (low semantic distance) effects can be minimal or absent.",
            "uuid": "e5808.0",
            "source_info": {
                "paper_title": "Evaluating the Zero-shot Robustness of Instruction-tuned Language Models",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "IncorrectObserved_vs_AppropriateUnobserved",
            "name_full": "Using Incorrect but Observed Instructions vs Correct but Unobserved Instructions (Closer Look experiment)",
            "brief_description": "Experiment comparing several instruction variants: 'Closest' (observed closest task instruction), 'Incorrect' (an observed instruction for a different/inappropriate task with same output format), 'Negated' (negated instruction), 'Task designer' (task author prefix), 'Collected' (novel instructions from researchers), and 'Nonsensical'.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Flan-T5-XXL (and aggregated across models for the 'closer look' analysis)",
            "model_size": "XXL (11B) for the example; analysis aggregated across multiple models in that experiment",
            "task_name": "Subset of BIG-BENCH tasks (7 datasets used in the 'closer look' experiment)",
            "task_description": "Seven representative classification/QA tasks from BIG-BENCH used to probe instruction sensitivity with multiple instruction variants.",
            "problem_format": "Zero-shot prompt variants as listed above (Closest, Incorrect-observed, Collected-unobserved, Task-designer, Negated, Nonsensical). Each variant supplies a different instruction prefix while keeping instance content identical.",
            "comparison_format": "Multiple comparative instruction variants (observed-appropriate, observed-inappropriate, unobserved-appropriate, negated, nonsensical)",
            "performance": "Aggregated averages (Table 16): Closest (observed appropriate) overall ≈ 60.45%; Incorrect (observed but inappropriate) ≈ 57.4%; Collected (novel/unobserved appropriate) ≈ 56.8%; Task Designer ≈ 56.4%; Negated ≈ 30.0%; Nonsensical ≈ 39.3%.",
            "performance_comparison": null,
            "format_effect_size": "Incorrect-but-observed outperforms Collected (observed-inappropriate ≈ 57.4% vs collected-unobserved ≈ 56.8%) on average in this experiment; large drops for negated (~ -30 pp) and nonsensical (~ -20 pp relative to observed-closest).",
            "format_effect_direction": "format can increase or decrease performance depending on whether the instruction was observed during training; observed-but-inappropriate can outperform semantically-correct-unobserved prompts (i.e., format seen in training helps even when semantically wrong).",
            "explanation_or_hypothesis": "Authors interpret this as evidence that models rely heavily on familiar instruction templates seen during fine-tuning; exposure to an observed instruction (even if semantically incorrect) can trigger learned behavior aligned with the seen template and thus sometimes outperform semantically correct but novel phrasings.",
            "counterexample_or_null_result": null,
            "uuid": "e5808.1",
            "source_info": {
                "paper_title": "Evaluating the Zero-shot Robustness of Instruction-tuned Language Models",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Semantic_Distance_vs_Performance",
            "name_full": "Representational (Semantic) Distance between Instructions and Performance",
            "brief_description": "Analysis linking representational similarity (penultimate-layer embeddings for first decoded token) between observed and unobserved instructions to performance degradation; includes t-SNE visualization and average L2 distances per dataset category.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Flan-T5-XXL (representational visualizations) and analyses reported across models",
            "model_size": "XXL (11B) for visualization; distances computed for other sizes as well",
            "task_name": "MMLU and BBL (BIG-BENCH LITE)",
            "task_description": "Same tasks as the main evaluation; representation extraction for instruction+example instances to compute distances between unobserved and nearest observed instruction representations.",
            "problem_format": "Zero-shot prompts; representation for 'first decoded token' is extracted for instances under different instruction templates; distances and t-SNE used to examine distributional shift.",
            "comparison_format": "Observed instruction representations vs Unobserved instruction representations, measured via L2 distance and t-SNE clustering",
            "performance": "Table 4 reports average Δℓ2 distances and average Δ accuracy: MMLU Avg Δℓ2 = 19.8 with Avg ΔAcc ≈ -0.5 pp; BBL-QA Avg Δℓ2 = 37.9 with Avg ΔAcc ≈ -3.4 pp; BBL-BC Δℓ2=25.3 ΔAcc -2.0 pp; BBL-MC Δℓ2=26.1 ΔAcc -2.8 pp.",
            "performance_comparison": null,
            "format_effect_size": "Quantitative relationship: higher representational distance correlates with larger performance drops. Linear fit reported with intercept ≈ -0.8 and slope ≈ -0.2 (p = 0.08) linking similarity of first decoded token embeddings to percent performance degradation (weak but negative relationship).",
            "format_effect_direction": "Greater semantic/representational divergence between unobserved and observed instructions -&gt; reduced performance",
            "explanation_or_hypothesis": "When unobserved instructions induce representations closer to observed training instructions (small semantic distance), models generalize better. MMLU unobserved instructions were closer to observed ones (smaller Δℓ2), explaining its relative robustness.",
            "counterexample_or_null_result": "Relationship is present but relatively weak (slope -0.2, p=0.08); semantic distance does not fully predict performance in all cases.",
            "uuid": "e5808.2",
            "source_info": {
                "paper_title": "Evaluating the Zero-shot Robustness of Instruction-tuned Language Models",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "InContext_Learning_vs_ZeroShot",
            "name_full": "Effect of In-Context Learning (Few-shot examples) on Instruction Robustness",
            "brief_description": "Assessment of whether including few-shot examples in the prompt reduces sensitivity to instruction phrasing; replicates zero-shot experiments in a few-shot (one-shot) setting.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Same instruction-tuned models (Flan-T5 variants, Alpaca, T0++ as tested in zero-shot experiments)",
            "model_size": "various (as above)",
            "task_name": "MMLU and BBL (selected tasks)",
            "task_description": "One-shot in-context learning: prompts include a single example (demonstration) plus the instruction and test instance.",
            "problem_format": "One-shot/few-shot prompts where demonstrations (examples) are added to the prompt (ICL) compared to zero-shot instruction-only prompts.",
            "comparison_format": "Zero-shot (instruction only) vs one-shot in-context learning (instruction + example)",
            "performance": "Qualitative improvement: Figure 6 and text indicate that one-shot ICL 'slightly decreases the sensitivity' to unobserved instructions; performance differences between observed and unobserved instructions move closer to zero under one-shot.",
            "performance_comparison": null,
            "format_effect_size": "Described qualitatively as 'slight' reduction in performance gap; no single aggregate numeric effect size is provided in main text, but plots show convergence of observed/unobserved accuracy differences toward zero under one-shot.",
            "format_effect_direction": "improved (reduced sensitivity)",
            "explanation_or_hypothesis": "Examples in context likely implicitly demonstrate the desired task and label format, reducing reliance on the instruction phrasing and thereby making models more robust to instruction paraphrases.",
            "counterexample_or_null_result": "Sensitivity is reduced but not eliminated; some discrepancy between formats remains under ICL.",
            "uuid": "e5808.3",
            "source_info": {
                "paper_title": "Evaluating the Zero-shot Robustness of Instruction-tuned Language Models",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Scaling_Effect",
            "name_full": "Model Scale (Parameters) vs Instruction Robustness",
            "brief_description": "Investigation of whether increasing model size (Flan-T5 from 80M up to 11B) reduces sensitivity to instruction paraphrasing; includes per-size MMLU results.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Flan-T5 family (Small/Base/Large/XL/XXL)",
            "model_size": "80M, 250M, 780M, 3B (XL), 11B (XXL)",
            "task_name": "MMLU (and other BBL subsets shown in scaling plots)",
            "task_description": "Zero-shot instruction-following evaluated across model sizes to see if larger models show emergent robustness to unobserved instructions.",
            "problem_format": "Zero-shot observed vs unobserved instruction prompts tested across model sizes",
            "comparison_format": "Small-&gt;Large scale comparison of same prompt formats",
            "performance": "MMLU (Table 9): Observed accuracies by size: Small 29.4%, Base 34.1%, Large 41.1%, XL 48.1%, XXL 53.2%. Unobserved: Small 29.6%, Base 33.8%, Large 40.7%, XL 47.5%, XXL 52.7%. Differences remain across sizes but do not vanish.",
            "performance_comparison": null,
            "format_effect_size": "Difference observed vs unobserved across sizes is small and does not consistently shrink with scale up to 11B (e.g., XXL MMLU observed 53.2% vs unobserved 52.7% Δ -0.5 pp).",
            "format_effect_direction": "no strong emergent robustness with scale (up to 11B); effect persists",
            "explanation_or_hypothesis": "Authors note that larger models (beyond 11B, e.g., 175B) might offer greater robustness, but within evaluated mid-sized regimes scaling did not eliminate sensitivity to instruction wording.",
            "counterexample_or_null_result": "For some datasets and sizes the gap is negligible (e.g., small differences at XXL), and in a few cases unobserved performed slightly better (see T0++ MMLU), indicating no universal monotonic effect of scale within the evaluated range.",
            "uuid": "e5808.4",
            "source_info": {
                "paper_title": "Evaluating the Zero-shot Robustness of Instruction-tuned Language Models",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "SoftPrompt_Alignment_(PT+KL)",
            "name_full": "Soft Prompt Alignment: Prefix Tuning with KL Alignment Loss (PT+KL)",
            "brief_description": "A mitigation introduced by the paper: add trainable 'soft prompt' prefix embeddings and a KL-divergence loss between output distributions induced by reference vs paraphrased instructions; only prefix embeddings are optimized (prefix tuning) using automatically generated paraphrases.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Flan-T5-XL (3B) and Alpaca-7B in experiments; method applicable to other instruction-tuned models",
            "model_size": "3B (Flan-T5-XL) and 7B (Alpaca-7B) in reported experiments",
            "task_name": "MMLU and BBL (same evaluation buckets as main experiments)",
            "task_description": "Zero-shot evaluation; method fine-tunes only soft prompt parameters using an augmented dataset of automatically generated paraphrased instructions and a combined cross-entropy + KL loss.",
            "problem_format": "Training-time modification: introduce n soft prefix tokens (R^{d x n}) prepended to instructions; generate paraphrased instructions (via GPT-4) to pair with reference instructions; objective includes standard CE (teacher-forcing) and KL divergence between output distributions of paraphrase and reference; at evaluation, the model receives test instructions (observed or unobserved) unchanged but with the learned soft prompt.",
            "comparison_format": "Baseline instruction-tuned model (no adaptation) vs FT (fine-tune all params on paraphrases), FT+KL, PT (prefix tuning without KL), and PT+KL (full proposed method).",
            "performance": "Table 5: PT+KL yields consistent improvements on unobserved instructions: Example highlights — Flan-T5-3B unobserved MMLU baseline 47.5% → PT+KL 47.9% (+0.4 pp); BBL unobserved 51.9% → PT+KL 53.7% (+1.8 pp). Alpaca-7B unobserved overall baseline 39.7% → PT+KL 41.8% (+2.1 pp); BBL unobserved 42.9% → PT+KL 46.6% (+3.7 pp).",
            "performance_comparison": "FT (fine-tuning all params on paraphrases) often degraded performance (e.g., Flan-T5-3B FT overall Δ -8.0 pp), FT+KL mitigated but still worse than PT+KL; PT alone gave small/no benefit, PT+KL gave the best and most consistent improvements on unobserved instructions.",
            "format_effect_size": "Improvements modest but consistent: PT+KL increased unobserved-instruction accuracy by up to ~3.7 percentage points in some settings (e.g., Alpaca-7B on BBL), with smaller gains in other datasets (e.g., ~0.4 pp on MMLU for Flan-T5-XL). Table 6 shows decreased representational distance accompanying gains (e.g., BBL-BC closest distance 30.1→27.9 with +4.2% acc improvement reported in table).",
            "format_effect_direction": "improved",
            "explanation_or_hypothesis": "Encouraging model to produce similar output distributions (via KL) for paraphrased vs reference instructions makes the model's internal representations for semantically equivalent instructions closer, reducing sensitivity to surface wording; prefix tuning restricts updates to soft prompt parameters, avoiding catastrophic changes from full fine-tuning.",
            "counterexample_or_null_result": "Simple fine-tuning on paraphrases (FT) often harmed performance, indicating that naive augmentation without the KL alignment or prefix constraint can be detrimental.",
            "uuid": "e5808.5",
            "source_info": {
                "paper_title": "Evaluating the Zero-shot Robustness of Instruction-tuned Language Models",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "FineTuning_on_Paraphrases_(FT)",
            "name_full": "Full Fine-Tuning on Automatically Generated Paraphrased Instructions (FT and FT+KL ablations)",
            "brief_description": "Ablation where the entire model is fine-tuned on automatically generated paraphrase-augmented training data, tested with and without the KL alignment term.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Flan-T5-XL (3B) and Alpaca-7B (examples reported)",
            "model_size": "3B (Flan-T5-XL) and 7B (Alpaca-7B)",
            "task_name": "MMLU and BBL (same benchmarks as main experiments)",
            "task_description": "Models are fine-tuned (all parameters) on augmented datasets containing paraphrased instructions generated automatically (via GPT-4/generation procedures described); evaluated on observed and unobserved instructions.",
            "problem_format": "Training-time augmentation: full parameter fine-tuning (FT) on paraphrase-augmented instruction data; variant FT+KL adds the KL alignment objective.",
            "comparison_format": "Baseline vs FT vs FT+KL vs PT vs PT+KL",
            "performance": "Table 5: FT often harmed performance: e.g., Flan-T5-3B FT MMLU observed 39.4% (Δ -8.7) vs baseline 48.1%; FT unobserved 40.1% (Δ -7.4). FT+KL mitigated some loss but still underperformed PT+KL in many cases (Flan-T5-3B FT+KL MMLU unobs 43.6% vs PT+KL 47.9%). Alpaca-7B FT shifts were smaller but FT+KL still under PT+KL.",
            "performance_comparison": null,
            "format_effect_size": "FT sometimes results in very large negative changes (e.g., Flan-T5-3B MMLU Δ ≈ -8.0 pp overall), demonstrating that naive full fine-tuning on paraphrase data can degrade zero-shot capabilities.",
            "format_effect_direction": "reduced (in many reported cases)",
            "explanation_or_hypothesis": "Full fine-tuning on paraphrases may overfit to augmented data distribution or disrupt original instruction-following behavior; constraining updates (prefix tuning) plus an explicit alignment objective (KL) avoids this harm and yields net improvements.",
            "counterexample_or_null_result": "FT+KL sometimes recovers more performance than FT alone but still often underperforms PT+KL, showing that full fine-tuning is not a reliable remedy.",
            "uuid": "e5808.6",
            "source_info": {
                "paper_title": "Evaluating the Zero-shot Robustness of Instruction-tuned Language Models",
                "publication_date_yy_mm": "2023-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Do prompt-based models really understand the meaning of their prompts?",
            "rating": 2
        },
        {
            "paper_title": "Can large language models truly understand prompts? a case study with negated prompts",
            "rating": 2
        },
        {
            "paper_title": "Robustness of learning from task instructions",
            "rating": 2
        },
        {
            "paper_title": "The flan collection: Designing data and methods for effective instruction tuning",
            "rating": 1
        },
        {
            "paper_title": "Multitask prompted training enables zero-shot task generalization",
            "rating": 1
        }
    ],
    "cost": 0.018619749999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Evaluating the Zero-shot Robustness of Instruction-tuned Language Models</h1>
<p>Jiuding Sun<br>Khoury College of Computer Sciences<br>Northeastern University<br>sun.jiu@northeastern.edu</p>
<h2>Chantal Shaib</h2>
<p>Khoury College of Computer Sciences
Northeastern University
shaib.c@northeastern.edu</p>
<p>Byron C. Wallace<br>Khoury College of Computer Sciences<br>Northeastern University<br>b.wallace@northeastern.edu</p>
<h4>Abstract</h4>
<p>Instruction fine-tuning has recently emerged as a promising approach for improving the zero-shot capabilities of Large Language Models (LLMs) on new tasks. This technique has shown particular strength in improving the performance of modestly sized LLMs, sometimes inducing performance competitive with much larger model variants. In this paper we ask two questions: (1) How sensitive are instructiontuned models to the particular phrasings of instructions, and, (2) How can we make them more robust to such natural language variation? To answer the former, we collect a set of 319 instructions manually written by NLP practitioners for over 80 unique tasks included in widely used benchmarks, and we evaluate the variance and average performance of these instructions as compared to instruction phrasings observed during instruction fine-tuning. We find that using novel (unobserved) but appropriate instruction phrasings consistently degrades model performance, sometimes substantially so. Further, such natural instructions yield a wide variance in downstream performance, despite their semantic equivalence. Put another way, instruction-tuned models are not especially robust to instruction re-phrasings. We propose a simple method to mitigate this issue by introducing "soft prompt" embedding parameters and optimizing these to maximize the similarity between representations of semantically equivalent instructions. We show that this method consistently improves the robustness of instruction-tuned models. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>Large Language Models (LLMs) have come to dominate NLP, in part because they enable zeroand few-shot adaptation to new tasks via prompting [3; 4; 10; 37]. Recent work has demonstrated the promise of fine-tuning such models with natural language instructions. Such instruction-tuning improves LLM performance in zero- and few-shot settings, sometimes dramatically, especially for "mid-sized" models [5; 22]. For example, on some benchmarks the instruction-tuned Flan-T5-XL (3B parameters) [5] outperforms GPT-3 (175B), despite being dramatically smaller. Furthermore, LLaMa-7B [27]-after being fine-tuned on large-scale corpora on the Alpaca [26] instruction setoutperforms GPT-3 across a range of NLP benchmarks.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: How well do models trained on instruction-tuning datasets generalize to novel instructions (unobserved in training)? Our analysis suggests that they do not do so very well. Above we show a case where pairing an example with an observed instruction yields the correct output, while providing a distinct but semantically equivalent instruction produces an incorrect response. We propose and evaluate a simple method that improves this.</p>
<p>These empirical successes have motivated efforts to curate instruction-augmented task collections for meta-learning [31; 33; 33], and research into improving instruction-tuning [17; 34; 24]. In this work we investigate how robust instruction-tuned models are. More specifically, we ask: How sensitive are instruction-tuned LMs to shifts in instruction phrasings at test time? This is particularly important given that the primary motivation of instruction tuning is to facilitate zero-shot adaptation via natural language instruction: If models are overly sensitive to the particular phrasing of a task instruction it may greatly limit their utility in practice.</p>
<p>Prior work—reviewed at length in Section 2—has established that LLMs do not seem to intuitively "understand" prompts [32; 12; 38], but these efforts did not consider instruction-tuned models specifically. Recent, contemporaneous work to ours [8] investigated the robustness of instruction-tuned models, and found that instruction-tuned T5 [23] is robust to instruction perturbations in few-shot settings, but less so in zero-shot application. We contribute a more in-depth analysis of this phenomena across a much wider set of instruction-tuned models and benchmarks. We also introduce and evaluate a method for improving the robustness of such models, with promising results.</p>
<p>More specifically, we collect a relatively large set of task instructions manually composed by NLP researchers; these are valid instructions but distinct from those found in the Flan collection. We then assess the performance of LLMs fine-tuned on the Flan collection instruction set when given these novel instructions on two benchmarks: MMLU [9] and BBL [25]. We find that using novel instructions in zero-shot application degrades accuracy considerably (Figure 1 illustrates this). For example, comparing the performance of Flan-T5 XXL when using (a) instructions that were seen in training to (b) semantically equivalent but unobserved in training, we observe a 6.9 point drop in absolute performance on average across large benchmarks.</p>
<p>Our main contributions are summarized as follows. (1) We perform a comprehensive and in-depth analysis of the robustness of instruction-tuned LLMs across three "families" of such models (Flan-T5 [33], Alpaca [26], and T0 [24]) using large benchmarks [9; 25]. For this we collect a large set of new task instructions manually composed by researchers in NLP; we will release this dataset to facilitate additional work on instruction robustness. We observe substantial performance degradation when using "novel" (unseen in training) instructions. (2) We propose a simple method to improve robustness by imposing an objective encouraging LLMs to induce similar representations for semantically equivalent instructions. We find that this consistently improves the performance realized when using novel but appropriate task instructions.</p>
<h1>2 Related Work</h1>
<p>Multitask learning and instruction-tuning Training a single text-to-text model capable of providing responses to arbitrary queries has been an aspiration in NLP for at least half a decade. For example, prior to modern prompting and instructing strategies, there were efforts to unify disparate tasks by reframing them as instances of general question answering [18; 14; 13]. More recent efforts have focussed on compiling and fine-tuning LLMs on corpora comprising diverse tasks with associated natural language instructions [33; 20; 24]; we refer to this strategy as instruction-tuning. One example of this is Super-NaturalInstructions [31], which compiles over 1600 tasks and enriches these with both instructions and negative examples. Similarly, the recently released OPT-IML Bench [11] comprises 2000 NLP tasks. The Flan 2022 task collection [17] additionally features Chain-of-Thought (CoT) style "reasoning" chains in instruction templates; the authors show that including these (as well as zero-shot examples and "input inversions") during instruction fine-tuning yields improvements on held-out tasks.</p>
<p>These meta-resources-collections of instructions, tasks, and samples-have facilitated the training of instruction-tuned model families such as Flan-T5, Flan-PaLM [5], and OPT-IML [11]. ${ }^{2}$ Results have been encouraging; fine-tuning LLMs to follow instructions provides clear and consistent gains across models, and, perhaps most exciting, enables relatively "small" ( 10B) LLMs to achieve near SOTA performance comparable to massive ( $\sim 175 \mathrm{~B}$ ) models [26]. This has motivated interest in characterizing how instructions help models, and developing techniques to further improve instructiontuning; we review recent efforts related to these two research threads below.</p>
<p>Evaluating prompting and instruction capabilities Instructions may be seen as a special sort of model prompting, which a few recent efforts have critically evaluated. For example, Webson and Pavlick ask whether models meaningfully "understand" prompts [32], finding that they largely do not: Performance is often unaffected when irrelevant and misleading prompts are provided. In follow up work, Jang et al. [12] evaluates performance on negated prompts, observing an "inverse-scaling" phenomenon in which larger models perform worse in this case.</p>
<p>Other work has attempted to characterize how and when in-context learning (ICL)—i.e., including a few examples in prompts-works [19; 29; 6; 1; 36]. ICL is a form of prompting orthogonal to the present effort, as we are primarily interested in the zero-shot adaptability of instruction-tuned LLMs.</p>
<p>In work contemporaneous to ours, Gu et al. [8] investigated how robust instruction-tuned models are to instruction perturbations (e.g., dropping words) and paraphrasings. They found that models are relatively robust when given examples (i.e., in few-shot settings), but quite sensitive when used zero-shot; this is qualitatively in line with our findings. Our work differs in important way from this coincident research: (1) We provide a much more comprehensive analysis of robustness; Gu et al. considered only T5 instruction-tuned on a single instruction dataset, whereas we evaluate three LLMs (and different sizes of each) using five instruction tuning datasets, and we evaluate using over 80 test tasks in all (Gu et al. considered only 12). (2) We propose and evaluate a new approach to improving the robustness of instruction-tuned models; Gu et al. offered no mechanism to improve robustness.</p>
<p>Improving instruction-tuning Past work has also sought to improve instruction-tuning in various ways. One means to do so is to instruction tune based on human feedback [22; 7; 2; 21; 39]. This tends to improve open-ended model responses but degrade performance on downstream tasks. Another strategy is to leverage existing resources to automatically generate instruction-tuning datasets at scale. For example, Wang et al. [30] use LLMs to generate instructions, inputs, and outputs and use these to improve their own instruction-following capabilities. In a similarly meta vein, Zhou and colleagues [40] propose using LLMs to engineer prompts. Finally, Ye et al. [35] propose "flipping" the standard task by tasking LLMs with generating instructions, given an input and label.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>3 Instruction Datasets</h1>
<h3>3.1 Evaluation Benchmarks</h3>
<p>We evaluate a set of instruction-tuned models on two large benchmarks: MMLU [9] and BIGBENCH [25]. MMLU is a multiple-choice question-answering benchmark comprising 57 tasks that require expert knowledge. BIG-BENCH is a collaboratively built benchmark containing 204 diverse tasks from various domains; here consider the BIG-BENCH LITE subset, and we include only QA, multi-class, and binary classification tasks, yielding 18 tasks from in all.</p>
<h3>3.2 Collecting New Instructions from NLP Researchers</h3>
<p>We aim to evaluate instruction-tuned models when they are provided instructions which are semantically equivalent to, but superficially different from, those with which they were trained. To this end, we enlist NLP researchers (graduate students) to compose novel instructions for the tasks considered; these particular instruction phrasings were therefore unobserved during instruction fine-tuning.
More specifically, we recruited 36 NLP graduate students working in NLP. All had at least some experience with instruction-tuned models and the downstream tasks included in the evaluation benchmarks. For each of the 18 tasks in BBL and all tasks in MMLU, we asked 12 graduate students to write one (distinct) instruction they would use for zero-shot inference with an instruction-tuned model. We provide details on this instruction collection process in Appendix A. We will release all 319 instructions acquired for this work to ensure the reproducibility of this work and to facilitate further research on instruction-tuned model robustness.</p>
<h2>4 Evaluating the Robustness of Instruction-tuned LLMs</h2>
<h3>4.1 Models and Data</h3>
<p>We conduct experiments with model variants trained over three instruction collections (these provide observed task instructions): P3 [24], Flan-2022 [5], and Alpaca [26]. To facilitate our analyses, we manually identified all instructions that correspond to (a) multiple-choice question answering (QA), (b) binary classification (BC), or tasks that demand "yes" or "no" responses, and (c) multi-class classification (MC), which requires classifying inputs into a finite set of categories.
To evaluate model robustness with respect to instruction phrasings we use two benchmarks: MMLU [9] and BIG-BENCH LITE (BBL) [25] along with the acquired set of novel instructions described in Section 3.2. We include all 57 tasks from MMLU, and 14 of 24 tasks from BBL. From the latter we exclude two tasks that rely on generation metrics, four that use exact-match, and four that contain tokens unrecognized by the T5 and/or LLaMa tokenizer (e.g., inputs are emojis in one task).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">QA</th>
<th style="text-align: left;">In this task, you are given a multiple-choice question and you have to pick the <br> correct option. Answer with option indexes (i.e., "A", "B", "C", and "D"). <br> Q: {question} A. {choiceA} B. {choiceB} C. {choiceC} D. {choiceD}</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">MC</td>
<td style="text-align: left;">Pick one category for the following text. The options are - {options} {text}</td>
</tr>
<tr>
<td style="text-align: left;">BC</td>
<td style="text-align: left;">{paragraph} Choose your answer: According to the above paragraph, the <br> question "{question}" is "{response}"?</td>
</tr>
</tbody>
</table>
<p>Table 1: Examples of observed instructions we collected for three general types of tasks.</p>
<p>We use the same instructions for all tasks in the same category, taken from the published instruction tuning datasets associated with each model. These instructions are general, e.g., in the case of classification they request that the model consider an example with respect to categorization criteria and label space provided by the instance, and select an appropriate category (examples in Table 1). One can "mix-and-match" such instructions so long as they are appropriate for the task type.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Observed Instructions</th>
<th style="text-align: right;"></th>
<th style="text-align: right;"></th>
<th style="text-align: right;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Instruction Type</td>
<td style="text-align: right;">QA</td>
<td style="text-align: right;">MC</td>
<td style="text-align: right;">BC</td>
</tr>
<tr>
<td style="text-align: left;">Flan</td>
<td style="text-align: right;">50</td>
<td style="text-align: right;">35</td>
<td style="text-align: right;">18</td>
</tr>
<tr>
<td style="text-align: left;">Alpaca</td>
<td style="text-align: right;">20</td>
<td style="text-align: right;">20</td>
<td style="text-align: right;">11</td>
</tr>
<tr>
<td style="text-align: left;">P3</td>
<td style="text-align: right;">13</td>
<td style="text-align: right;">8</td>
<td style="text-align: right;">7</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: left;">Unobserved Instructions</th>
<th style="text-align: right;"></th>
<th style="text-align: right;"></th>
<th style="text-align: right;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Number of tasks</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">14</td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">Instructions per task</td>
<td style="text-align: right;">20</td>
<td style="text-align: right;">10</td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">Total instructions</td>
<td style="text-align: right;">20</td>
<td style="text-align: right;">140</td>
<td style="text-align: right;"></td>
</tr>
</tbody>
</table>
<p>Table 2: Counts of instruction phrasings (unobserved and observed) we use for evaluations.</p>
<h1>4.2 Results</h1>
<p>We present the main aggregated analysis results in Figure 2 and Table 3. The take-away here is that using instructions unobserved in training-but manually composed for the task at hand and so semantically appropriate-leads to considerable degradation in performance: On average, unobserved instructions reduce accuracy by over five points across models considered. Table 3 reports results disaggregated by task type; we observe that classification tasks are most harmed by use of novel instructions. We provide additional, more granular (dataset-level) results in the Appendix.
<img alt="img-1.jpeg" src="img-1.jpeg" />
(a) Average zero-shot performance over all tasks when using observed and unobserved instructions.
<img alt="img-2.jpeg" src="img-2.jpeg" />
(b) Performances of Flan-T5 using observed and unobserved instructions as a function of model size.</p>
<p>Figure 2: Using novel but valid instructions at test time (phrasings unobserved in training) consistently degrades the performance of instruction-tuned LLMs (a). Scale does not necessarily fix this (b).</p>
<h3>4.3 A Closer Look at Instruction Robustness</h3>
<p>Above we used general instructions requesting the model to perform tasks (Table 1). Here we delve further into the performance degradation observed when using novel instructions. We report a curious result highlighting the degree to which models rely on having previously observed instructions: Incorrect but observed instructions outperform appropriate but unobserved instructions (Figure 3).
We come to this observation by evaluating the performance of Flan-T5-XXL (11B) using six instruction types over seven datasets from Big-Bench. In particular, this includes (variants of) two instructions observed in training: Closest is the instruction from the most similar task in the instruction-tuning set; Incorrect is an observed instruction for a completely different and inappropriate task (but which has the same desired output format, e.g., classification)-intuitively these should not yield the desired behavior; Negated is the same as closest, but we negate the instruction to indicate that it should not perform the task.</p>
<p>For unobserved instructions, we consider: Task designer, the instruction (task prefix) provided by the author of the task in Big-Bench, and; Newly collected, or the novel instructions collected from NLP graduate students, described above. As a control for reference, we also consider Nonsensical, which is a random "instruction" completely irrelevant to any task.</p>
<p>Figure 3 reports average results for these variants. Consistent with our findings, using instructions unobserved in training degrades performance. Strikingly, here we also find that using an inappropriate but observed instruction outperforms using appropriate but unobserved instructions. This indicates that instruction-tuned models-or at least modestly sized ones we have evaluated here-may in some</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>MMLU</th>
<th>BBL-QA</th>
<th>BBL-BC</th>
<th>BBL-MC</th>
<th>Overall</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Avg. Std.</td>
<td>Avg. Std.</td>
<td>Avg. Std.</td>
<td>Avg. Std.</td>
<td>Avg. Std.</td>
</tr>
<tr>
<td>Flan-T5-3B</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Observed</td>
<td>48.1 ( $\pm 0.3$ )</td>
<td>59.0 ( $\pm 2.1$ )</td>
<td>66.5 ( $\pm 3.8$ )</td>
<td>55.6 ( $\pm 0.7$ )</td>
<td>57.3 ( $\pm 1.7$ )</td>
</tr>
<tr>
<td>UNOBSERVED</td>
<td>47.5 ( $\pm 0.9$ )</td>
<td>56.0 ( $\pm 7.3$ )</td>
<td>61.1 ( $\pm 6.9$ )</td>
<td>52.1 ( $\pm 5.4$ )</td>
<td>54.2 ( $\pm 5.1$ )</td>
</tr>
<tr>
<td>Performance $\Delta$</td>
<td>$\downarrow 0.6$</td>
<td>$\downarrow 3.0$</td>
<td>$\downarrow 5.5$</td>
<td>$\downarrow 3.5$</td>
<td>$\downarrow 3.1$</td>
</tr>
<tr>
<td>Alpaca-7B</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Observed</td>
<td>41.9 ( $\pm 0.6$ )</td>
<td>48.6 ( $\pm 2.8$ )</td>
<td>53.8 ( $\pm 3.4$ )</td>
<td>32.1 ( $\pm 2.2$ )</td>
<td>44.1 ( $\pm 2.3$ )</td>
</tr>
<tr>
<td>UNOBSERVED</td>
<td>39.7 ( $\pm 2.2$ )</td>
<td>45.3 ( $\pm 6.5$ )</td>
<td>52.4 ( $\pm 6.5$ )</td>
<td>16.4 ( $\pm 3.5$ )</td>
<td>38.5 ( $\pm 4.7$ )</td>
</tr>
<tr>
<td>Performance $\Delta$</td>
<td>$\downarrow 2.2$</td>
<td>$\downarrow 3.3$</td>
<td>$\downarrow 1.4$</td>
<td>$\downarrow 15.7$</td>
<td>$\downarrow 5.6$</td>
</tr>
<tr>
<td>T0++ 11B</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Observed</td>
<td>48.3 ( $\pm 0.9$ )</td>
<td>54.1 ( $\pm 4.1$ )</td>
<td>66.1 ( $\pm 2.1$ )</td>
<td>42.0 ( $\pm 2.1$ )</td>
<td>52.6 ( $\pm 2.3$ )</td>
</tr>
<tr>
<td>UNOBSERVED</td>
<td>48.5 ( $\pm 0.9$ )</td>
<td>54.7 ( $\pm 3.7$ )</td>
<td>54.7 ( $\pm 4.3$ )</td>
<td>41.4 ( $\pm 2.4$ )</td>
<td>49.8 ( $\pm 2.8$ )</td>
</tr>
<tr>
<td>Performance $\Delta$</td>
<td>$\uparrow 0.2$</td>
<td>$\uparrow 0.7$</td>
<td>$\downarrow 11.4$</td>
<td>$\downarrow 0.6$</td>
<td>$\downarrow 2.8$</td>
</tr>
<tr>
<td>Flan-T5-11B</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Observed</td>
<td>53.2 ( $\pm 0.2$ )</td>
<td>67.9 ( $\pm 1.8$ )</td>
<td>65.6 ( $\pm 6.0$ )</td>
<td>58.7 ( $\pm 0.5$ )</td>
<td>61.4 ( $\pm 2.1$ )</td>
</tr>
<tr>
<td>UNOBSERVED</td>
<td>52.7 ( $\pm 0.8$ )</td>
<td>64.6 ( $\pm 8.5$ )</td>
<td>63.6 ( $\pm 6.1$ )</td>
<td>55.9 ( $\pm 5.5$ )</td>
<td>59.2 ( $\pm 5.2$ )</td>
</tr>
<tr>
<td>Performance $\Delta$</td>
<td>$\downarrow 0.5$</td>
<td>$\downarrow 3.4$</td>
<td>$\downarrow 2.0$</td>
<td>$\downarrow 2.8$</td>
<td>$\downarrow 2.2$</td>
</tr>
<tr>
<td>Alpaca-13B</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Observed</td>
<td>47.8 ( $\pm 0.5$ )</td>
<td>53.9 ( $\pm 2.2$ )</td>
<td>57.9 ( $\pm 4.8$ )</td>
<td>36.7 ( $\pm 1.8$ )</td>
<td>49.1 ( $\pm 2.3$ )</td>
</tr>
<tr>
<td>UNOBSERVED</td>
<td>47.0 ( $\pm 0.8$ )</td>
<td>51.7 ( $\pm 5.7$ )</td>
<td>54.1 ( $\pm 5.6$ )</td>
<td>22.7 ( $\pm 7.5$ )</td>
<td>43.9 ( $\pm 14.0$ )</td>
</tr>
<tr>
<td>Performance $\Delta$</td>
<td>$\downarrow 0.9$</td>
<td>$\downarrow 2.2$</td>
<td>$\downarrow 3.8$</td>
<td>$\downarrow 14.0$</td>
<td>$\downarrow 5.2$</td>
</tr>
</tbody>
</table>
<p>Table 3: Results using observed and unobserved instructions across benchmark tasks (grouped by type). Performance degrades-sometimes by 10+ points-when one uses (UNOBSERVED) instructions, suggesting that instruction-tuned models are not particularly robust. BC, MC, and QA stand for binary classification, multi-class classification, and question answering, respectively.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 3: Incorrect but observed instructions perform better on average than correct but unobserved instructions. We report averages over benchmarks, but show example instructions on the right for a specific, illustrative task. We provide all instructions in the Appendix.
way overrely on having observed instructions in training, and do not generalize to new instructions and phrasings as we might hope. We provide all the instructions and results in the Appendix.</p>
<h1>4.4 Scaling</h1>
<p>Does instruction robustness begin to emerge as a function of scale? To attempt to answer this, we repeated all experiments from Table 3 with Flan-T5 model sizes ranging from small (80M parameters) to XXL (11B). We observe in Figure 2b that the disparity between results achieved with observed versus unobserved instructions does not seem to decrease with model scale, at least up to this point. That said, massive models (175B+) may offer greater robustness. However, we reiterate that much of the excitement about instruction tuning is the possibility that this technique appears to allow much smaller models to achieve results competitive with massive alternatives.</p>
<h1>4.5 Robustness with Semantic Distance</h1>
<p>One observation in 4.2 is that performance on MMLU is less affected by using unobserved instructions. MMLU is a benchmark with 57 QA tasks about different knowledge domains; these tasks all share a similar form of input-output (question, four choices $\rightarrow$ answer). During instruction collection, we treated all tasks in MMLU as a general QA task and asked NLP researchers to write general QA instructions. As a result, we hypothesize that these instructions are comparatively similar to the observed instructions, and this in turn explains the relative robustness in this case.
We empirically verify this in Figure 4 and Table 4. For each instance (instruction plus example), we extract the representation at the penultimate layer for the first decoded token. We use tSNE [28] to visualize these representations of observed and unobserved instructions over instances in MMLU and BBL. Figure 4 shows that in the case of MMLU the unobserved instructions we collected are quite similar to the observed, while there is a greater separation between unobserved and observed instructions in BBL. We also provide a numerical measurement of this phenomonen in Table 4. We report the average $/ 2$ distance between representations of unobserved instructions and those of their nearest observed counterparts. We see that MMLU unobserved instructions are, on average, closer to the nearest observed instruction; this correlates with the lower observed performance drop. These findings are in line with the hypothesis that the unobserved instructions for MMLU are more similar to the observed instructions for this dataset, and this likely explains the apparent robustness in this case.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 4: tSNE plots of representations for the first decoded tokens of 300 randomly sampled examples from MMLU and BBL with Flan-T5 (XXL). Embeddings of observed and unobserved instructions for MMLU are similar, while for BBL they are quite different. This result holds across most but not all models considered: See the D for visualizations over all models.</p>
<p>We plot mean performance degradation (as \%) as a function of average similarity between the similarity of the first decoded tokens (following unobserved instructions) and the same for the most similar observed instruction. The negative slope implies the intuitive relationship: Instructions that are dissimilar (in terms of model representations) tend to result in poorer performance. However, the relationship is relatively weak, yielding an intercept estimate of -0.8 and a slope of $-0.2(p=0.08)$.</p>
<h3>4.6 Robustness Under In-Context Learning (ICL)</h3>
<p>Previous study [8] has shown that the LLMs are less sensitive to prompt / instruction variation when few-shot examples are provided in context. While we are focused on zero-shot capabilities, for completeness, we re-ran all experiments in a few-shot setting. We report these results in the C. The main finding is that while some discrepancy remains, in general ICL slightly decreases the sensitivity of models to the use of unobserved instructions. This is intuitive, given that the examples themselves likely imply the desired task and may affect the distribution.</p>
<h2>5 Aligning Equivalent Instructions</h2>
<p>We now introduce a simple, lightweight, but effective method to improve the robustness of instructiontuned LLMs. The intuition is to introduce a term in the objective which explicitly encourages the model to yield similar predictions (and hence similar representations) for the same input when provided distinct but semantically equivalent instructions.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 5: Plots of average degradations in performance versus the semantic distance while using unobserved instructions.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: left;">Avg. $\Delta \ell 2$</th>
<th style="text-align: left;">Avg. $\Delta$ Acc.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">MMLU</td>
<td style="text-align: left;">$\mathbf{1 9 . 8}$</td>
<td style="text-align: left;">$\mathbf{- 0 . 5}$</td>
</tr>
<tr>
<td style="text-align: left;">BBL-QA</td>
<td style="text-align: left;">37.9</td>
<td style="text-align: left;">-3.4</td>
</tr>
<tr>
<td style="text-align: left;">BBL-BC</td>
<td style="text-align: left;">25.3</td>
<td style="text-align: left;">-2.0</td>
</tr>
<tr>
<td style="text-align: left;">BBL-MC</td>
<td style="text-align: left;">26.1</td>
<td style="text-align: left;">-2.8</td>
</tr>
</tbody>
</table>
<p>Table 4: Average degradations in performance for four categories. It could be seen that MMLU has minimal average distance, which indicates a smaller distribution shift, and hence leads to the smallest degradation
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 6: The performance degradation when using unobserved instruction at BBL and MMLU with Flan-T5-XXL. We plot the accuracy degradation of all the unobserved instructions compared with the average accuracy of the observed ones. It could be seen that under one-shot in-context learning, the model is slightly more robust as the performance difference converges closer to 0</p>
<p>More specifically, we aim to align semantically equivalent instructions in the space induced by the model. To this end we introduce soft embedding parameters with dimensions $\mathbb{R}^{d \times n}$; this is equivalent to adding $n$ novel tokens (with embedding dimension $d$ ) as prefixes to inputs (preceding instructions). The intuition is to push the representations for semantically equivalent tasks close together. To this end, we add additional term to the loss: The KL-divergence $\mathcal{L}<em i="i">{\text {KL }}$ of the output probabilities between a reference instruction for a given task and paraphrased (semantically equivalent) version of the same. We combine this with the standard cross-entropy loss, and fine-tune only the introduced soft prompt parameters under this objective (Figure 7). Here $\lambda$ is a loss-weighting hyper-parameter, $\hat{y}</em>$
Optimizing for the above objective requires paraphrased instructions $i$ for each task in the training data; we generate these automatically as follows. For instruction-tuning dataset, we sample a small amount of training data to use for alignment. We paraphrase these reference instructions using GPT-4. For the Alpaca collection, we randomly sampled 1000 tasks and paraphrased them with three prompts, and collected the top three candidates under temperature 0.5 . For the Flan collection, we randomly sampled 986 instances from the mixture with 3 prompts with greedy decoding.}^{(\jmath)}$ and $\hat{y}_{r}^{(\jmath)}$ are the distributions over the vocabulary $\mathcal{V}$ induced by the model with paraphrased instruction $i$ and the reference instruction $r$ at token position $j .{ }^{3</p>
<p>For fine-tuning, we then create instances for each example by pairing them with every distinct instruction available for the corresponding task. We then form batches by including one instance featuring</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 7: Schematic depiction of the proposed instruction alignment method (left) and associated loss terms (right). Dotted (red) lines indicate backpropagation; we update only the soft prompt parameters, which we show yields performance superior to fine-tuning all model parameters.
the original instruction and the rest comprising paraphrased instructions. For the implementation of the prefix, we follow the setting of [16], which freezes the model parameters and just trains the prefix embeddings with the MLP layers.</p>
<h1>6 Results</h1>
<p>We experiment with the proposed method using two representative instruction-tuned LLMs: Flan-XL (3B) and Alpaca (7B). We compare the canonical versions of these models trained in the usual way (the same evaluated in Table 3) to variants fine-tuned using our proposed approach. We ablate components of our method to tease out the contributions of data and objectives.</p>
<p>Specifically, we consider variants where we: Fine-tune all model parameters on the additional, automatically generated instruction paraphrases (FT); impose the new KL loss term (again fine-tuning all model parameters; FT+KL); introduce the additional soft prompt parameters and fine-tune on the paraphrase instances, but without KL (PT); and then the full proposed strategy, which introduces the soft prompt parameters and optimizes them for the loss augmented with the KL term (PT+KL).</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">MMLU</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">BBL</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Model</td>
<td style="text-align: center;">OBS.</td>
<td style="text-align: center;">UnOBS.</td>
<td style="text-align: center;">Avg.</td>
<td style="text-align: center;">OBS.</td>
<td style="text-align: center;">UNOBS.</td>
<td style="text-align: center;">Avg.</td>
</tr>
<tr>
<td style="text-align: left;">Flan-T5-3B</td>
<td style="text-align: center;">48.1</td>
<td style="text-align: center;">47.5</td>
<td style="text-align: center;">47.8</td>
<td style="text-align: center;">$\mathbf{5 6 . 1}$</td>
<td style="text-align: center;">51.9</td>
<td style="text-align: center;">54.0</td>
</tr>
<tr>
<td style="text-align: left;">FT</td>
<td style="text-align: center;">$39.4 \mathbf{( - 8 . 7 )}$</td>
<td style="text-align: center;">$40.1 \mathbf{( - 7 . 4 )}$</td>
<td style="text-align: center;">$39.8 \mathbf{( - 8 . 0 )}$</td>
<td style="text-align: center;">$48.2 \mathbf{( - 7 . 9 )}$</td>
<td style="text-align: center;">$42.3 \mathbf{( - 9 . 2 )}$</td>
<td style="text-align: center;">$45.3 \mathbf{( - 8 . 7 )}$</td>
</tr>
<tr>
<td style="text-align: left;">FT+KL</td>
<td style="text-align: center;">$41.8 \mathbf{( - 6 . 3 )}$</td>
<td style="text-align: center;">$43.6 \mathbf{( - 3 . 9 )}$</td>
<td style="text-align: center;">$45.9 \mathbf{( - 1 . 9 )}$</td>
<td style="text-align: center;">$47.7 \mathbf{( - 8 . 4 )}$</td>
<td style="text-align: center;">$43.1 \mathbf{( - 8 . 8 )}$</td>
<td style="text-align: center;">$45.4 \mathbf{( - 8 . 6 )}$</td>
</tr>
<tr>
<td style="text-align: left;">PT</td>
<td style="text-align: center;">$48.1 \mathbf{( + 0 . 0 )}$</td>
<td style="text-align: center;">$47.6 \mathbf{( + 0 . 1 )}$</td>
<td style="text-align: center;">$47.9 \mathbf{( + 0 . 1 )}$</td>
<td style="text-align: center;">$55.9 \mathbf{( - 0 . 2 )}$</td>
<td style="text-align: center;">$52.1 \mathbf{( + 0 . 2 )}$</td>
<td style="text-align: center;">$54.0 \mathbf{( + 0 . 0 )}$</td>
</tr>
<tr>
<td style="text-align: left;">PT+KL</td>
<td style="text-align: center;">$\mathbf{4 8 . 1}(\mathbf{+ 0 . 1 )}$</td>
<td style="text-align: center;">$\mathbf{4 7 . 9}(\mathbf{+ 0 . 4 )}$</td>
<td style="text-align: center;">$\mathbf{4 8 . 0}(\mathbf{+ 0 . 2 )}$</td>
<td style="text-align: center;">$55.9 \mathbf{( - 0 . 2 )}$</td>
<td style="text-align: center;">$\mathbf{5 3 . 7}(\mathbf{+ 1 . 8 )}$</td>
<td style="text-align: center;">$\mathbf{5 4 . 8}(\mathbf{+ 0 . 8 )}$</td>
</tr>
<tr>
<td style="text-align: left;">ALPACA-7B</td>
<td style="text-align: center;">41.9</td>
<td style="text-align: center;">39.7</td>
<td style="text-align: center;">40.8</td>
<td style="text-align: center;">47.6</td>
<td style="text-align: center;">42.9</td>
<td style="text-align: center;">45.3</td>
</tr>
<tr>
<td style="text-align: left;">FT</td>
<td style="text-align: center;">$40.3 \mathbf{( - 1 . 6 )}$</td>
<td style="text-align: center;">$39.1 \mathbf{( - 0 . 6 )}$</td>
<td style="text-align: center;">$39.7 \mathbf{( - 1 . 1 )}$</td>
<td style="text-align: center;">$44.4 \mathbf{( - 3 . 2 )}$</td>
<td style="text-align: center;">$42.1 \mathbf{( - 0 . 8 )}$</td>
<td style="text-align: center;">$43.4 \mathbf{( - 2 . 0 )}$</td>
</tr>
<tr>
<td style="text-align: left;">FT+KL</td>
<td style="text-align: center;">$39.7 \mathbf{( - 2 . 2 )}$</td>
<td style="text-align: center;">$40.2 \mathbf{( + 0 . 5 )}$</td>
<td style="text-align: center;">$40.0 \mathbf{( - 0 . 8 )}$</td>
<td style="text-align: center;">$45.6 \mathbf{( - 2 . 0 )}$</td>
<td style="text-align: center;">$42.8 \mathbf{( - 0 . 1 )}$</td>
<td style="text-align: center;">$44.2 \mathbf{( - 1 . 1 )}$</td>
</tr>
<tr>
<td style="text-align: left;">PT</td>
<td style="text-align: center;">$42.1 \mathbf{( + 0 . 2 )}$</td>
<td style="text-align: center;">$40.0 \mathbf{( + 0 . 3 )}$</td>
<td style="text-align: center;">$41.1 \mathbf{( + 0 . 3 )}$</td>
<td style="text-align: center;">$47.5 \mathbf{( - 0 . 1 )}$</td>
<td style="text-align: center;">$43.0 \mathbf{( + 0 . 1 )}$</td>
<td style="text-align: center;">$45.3 \mathbf{( + 0 . 0 )}$</td>
</tr>
<tr>
<td style="text-align: left;">PT+KL</td>
<td style="text-align: center;">$\mathbf{4 2 . 4}(\mathbf{+ 0 . 5 )}$</td>
<td style="text-align: center;">$\mathbf{4 1 . 8}(\mathbf{+ 2 . 1 )}$</td>
<td style="text-align: center;">$\mathbf{4 2 . 1}(\mathbf{+ 1 . 3 )}$</td>
<td style="text-align: center;">$\mathbf{4 7 . 9}(\mathbf{+ 0 . 3 )}$</td>
<td style="text-align: center;">$\mathbf{4 6 . 6}(\mathbf{+ 3 . 7 )}$</td>
<td style="text-align: center;">$\mathbf{4 7 . 3}(\mathbf{+ 2 . 0 )}$</td>
</tr>
</tbody>
</table>
<p>Table 5: Results and ablations of the proposed soft prompt alignment method. All ablated versions use the augmented set with automatically paraphrased instructions. FT refers to simply fine-tuning (with teacher-forcing) on this additional data; PT denotes prefix tuning (i.e., introducing soft prompt parameters); KL refers to the alignment objective that we proposed above. Using all of these components together yields the best performance, especially on unobserved instructions.</p>
<p>We report results in Table 5. Two observations: (1) The proposed soft prompt alignment strategy (PT+KL) yields consistent improvements across the tasks and models considered and especially improves performance on unobserved instructions, as anticipated. (2) The full benefit of the approach is realized only when all components-the additional automatically paraphrased training instructions, soft prompt parameters, and additional KL loss term—are in place.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: left;">Closest Distance Before</th>
<th style="text-align: left;">Closest Distance After</th>
<th style="text-align: left;">$\Delta$ Acc. Improvement (\%)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">MMLU</td>
<td style="text-align: left;">22.2</td>
<td style="text-align: left;">21.3</td>
<td style="text-align: left;">$+0.3 \%$</td>
</tr>
<tr>
<td style="text-align: left;">BBL QA</td>
<td style="text-align: left;">22.4</td>
<td style="text-align: left;">23.0</td>
<td style="text-align: left;">$+0.4 \%$</td>
</tr>
<tr>
<td style="text-align: left;">BBL BC</td>
<td style="text-align: left;">30.1</td>
<td style="text-align: left;">$\mathbf{2 7 . 9}$</td>
<td style="text-align: left;">$+\mathbf{4 . 2 \%}$</td>
</tr>
<tr>
<td style="text-align: left;">BBL MC</td>
<td style="text-align: left;">26.0</td>
<td style="text-align: left;">24.6</td>
<td style="text-align: left;">$+0.3 \%$</td>
</tr>
</tbody>
</table>
<p>Table 6: Average distances before and after soft prompt alignment with Flan-T5-XL.</p>
<p>Following our approach in 4.5, we take the average distance between observed and unobserved instructions before and after alignment. Table 6 shows that our method brings observed and unobserved instruction representations closer together. The similarity is most increased in the case of the biggest accuracy gain, further suggesting the mechanism of improvement provided by soft prompt alignment.</p>
<h1>7 Conclusions</h1>
<p>Instruction-tuned LLMs have emerged as a promising means of achieving zero-shot performance with smaller models that is competitive to, and sometimes even better than, that observed using much larger LLMs [17; 26]. In this work we empirically characterized the robustness of such models with respect to instruction rephrasings. In particular, we collected manually composed instructions from 36 graduate students in NLP across 75 tasks, and we evaluated different families of instruction-tuned LLMs (Flan, Alpaca, and T0) when provided observed and unobserved instructions (seen in training and not, respectively). We found that using the latter consistently degrades model performance, indicating that models are unduly sensitive to instruction phrasings.</p>
<p>We then proposed a simple mechanism intended to improve the robustness of instruction-tuned LLMs. This approach entails introducing an additional loss term that penalizes the model for inducing dissimilar distributions over output tokens when using (a) paraphrased instructions as opposed to (b) a reference instruction for the same task. We found that training under this objective consistently (though modestly) improves results, and in particular mitigates the degradation observed when previously unobserved instructions are used.</p>
<h2>8 Limitations</h2>
<p>This work has important limitations: For example we only evaluated "mid-sized" models (&lt;20B parameters), it is unclear if our findings would generalize to much larger instruction-tuned models. (However, we note that instruction tuning has been most promising for smaller models.) We also restricted our evaluation to three task types: QA and multi-class and binary classification.</p>
<p>Ethics This work does not have an explicit ethical dimension, but we acknowledge that all LLMs are likely to encode problematic biases; it is unclear how instruction-tuning might interact with these.</p>
<h2>9 Acknowledgments</h2>
<p>This work was supported by the National Science Foundation (NSF) grant 1901117.
We thank Jay DeYoung and Alberto Mario Ceballos Arroyo for their advice and feedback on the paper. We also thank Alberto Mario Ceballos Arroyo, Arnab Sen Sharma, Bowen Zhao, Eric Todd, Hanming Li, Hiba Ahsan, Hye Sun Yun, Shulin Cao, Jay DeYoung, Jered McInerney, Ji Qi, Jifan Yu, Jize Jiang, Kaisheng Zeng, Koyena Pal, Kundan Krishna, Linxiao Nie, Hailong Jin, Jinxin Matthew Liu, Millicent Li, Monica Munnangi, Nikhil Prakash, Pouya Pezeshpour, Sanjana Ramprasad, Sarthak Jain, Shangqing Tu, Somin Wadhwa, Tingjian Zhang, Hao Wesley Peng, Xiaozhi Wang, Xingyu Lu, Xin Lv, Zijun Yao for providing manually written instructions.</p>
<h2>References</h2>
<p>[1] Ekin Akyürek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning algorithm is in-context learning? investigations with linear models. arXiv preprint arXiv:2211.15661, 2022.</p>
<p>[2] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022.
[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.
[4] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.
[5] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022.
[6] Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Zhifang Sui, and Furu Wei. Why can gpt learn in-context? language models secretly perform gradient descent as meta optimizers. arXiv preprint arXiv:2212.10559, 2022.
[7] Amelia Glaese, Nat McAleese, Maja Trębacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, et al. Improving alignment of dialogue agents via targeted human judgements. arXiv preprint arXiv:2209.14375, 2022.
[8] Jiasheng Gu, Hanzi Xu, Liangyu Nie, and Wenpeng Yin. Robustness of learning from task instructions. 2023.
[9] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.
[10] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.
[11] Srinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru, Todor Mihaylov, Dániel Simig, Ping Yu, Kurt Shuster, Tianlu Wang, Qing Liu, Punit Singh Koura, et al. OPT-IML: Scaling language model instruction meta learning through the lens of generalization. arXiv preprint arXiv:2212.12017, 2022.
[12] Joel Jang, Seonghyeon Ye, and Minjoon Seo. Can large language models truly understand prompts? a case study with negated prompts. In Transfer Learning for Natural Language Processing Workshop, pages 52-62. PMLR, 2023.
[13] Nitish Shirish Keskar, Bryan McCann, Caiming Xiong, and Richard Socher. Unifying question answering, text classification, and regression via span extraction. arXiv preprint arXiv:1904.09286, 2019.
[14] Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, and Hannaneh Hajishirzi. Unifiedqa: Crossing format boundaries with a single qa system. arXiv preprint arXiv:2005.00700, 2020.
[15] Alexandre Lacoste, Alexandra Luccioni, Victor Schmidt, and Thomas Dandres. Quantifying the carbon emissions of machine learning. arXiv preprint arXiv:1910.09700, 2019.
[16] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. arXiv preprint arXiv:2101.00190, 2021.
[17] Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V Le, Barret Zoph, Jason Wei, et al. The flan collection: Designing data and methods for effective instruction tuning. arXiv preprint arXiv:2301.13688, 2023.</p>
<p>[18] Bryan McCann, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. The natural language decathlon: Multitask learning as question answering. arXiv preprint arXiv:1806.08730, 2018.
[19] Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work? arXiv preprint arXiv:2202.12837, 2022.
[20] Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. Cross-task generalization via natural language crowdsourcing instructions. arXiv preprint arXiv:2104.08773, 2021.
[21] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021.
[22] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155, 2022.
[23] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485-5551, 2020.
[24] Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. Multitask prompted training enables zero-shot task generalization. arXiv preprint arXiv:2110.08207, 2021.
[25] Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615, 2022.
[26] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Alpaca: A strong, replicable instruction-following model, 2023.
[27] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.
[28] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research, 9(11), 2008.
[29] Xinyi Wang, Wanrong Zhu, and William Yang Wang. Large language models are implicitly topic models: Explaining and finding good demonstrations for in-context learning. arXiv preprint arXiv:2301.11916, 2023.
[30] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions. arXiv preprint arXiv:2212.10560, 2022.
[31] Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, et al. Benchmarking generalization via in-context instructions on 1,600+ language tasks. arXiv preprint arXiv:2204.07705, 2022.
[32] Albert Webson and Ellie Pavlick. Do prompt-based models really understand the meaning of their prompts? In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2300-2344, Seattle, United States, July 2022. Association for Computational Linguistics.</p>
<p>[33] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021.
[34] Zhiyang Xu, Ying Shen, and Lifu Huang. Multiinstruct: Improving multi-modal zero-shot learning via instruction tuning. arXiv preprint arXiv:2212.10773, 2022.
[35] Seonghyeon Ye, Doyoung Kim, Joel Jang, Joongbo Shin, and Minjoon Seo. Guess the instruction! making language models stronger zero-shot learners. arXiv preprint arXiv:2210.02969, 2022.
[36] Ping Yu, Tianlu Wang, Olga Golovneva, Badr Alkhamissy, Gargi Ghosh, Mona Diab, and Asli Celikyilmaz. Alert: Adapting language models to reasoning tasks. arXiv preprint arXiv:2212.08286, 2022.
[37] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. Glm-130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414, 2022.
[38] Kai Zhang, Bernal Jiménez Gutiérrez, and Yu Su. Aligning instruction tasks unlocks large language models as zero-shot relation extractors. arXiv preprint arXiv:2305.11159, 2023.
[39] Tianjun Zhang, Fangchen Liu, Justin Wong, Pieter Abbeel, and Joseph E Gonzalez. The wisdom of hindsight makes language models better instruction followers. arXiv preprint arXiv:2302.05206, 2023.
[40] Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. Large language models are human-level prompt engineers. arXiv preprint arXiv:2211.01910, 2022.</p>
<h1>Appendix</h1>
<h2>Table of Contents</h2>
<p>A Experimental Setup Details ..... 15
A. 1 Evaluation Protocols ..... 15
A. 2 Hyperparameters ..... 15
B Disaggregated Results ..... 16
B. 1 Main Results and Scaling Results ..... 16
B. 2 "Closer Look" Experiment Results ..... 21
C Instruction Robustness with In-context Learning ..... 22
D Representational Similarity and Model Performance ..... 23
E Instruction Collection ..... 25
E. 1 Observed Instructions ..... 25
E. 2 Unobserved Instructions ..... 56
E. 3 Granular Experiment Instructions ..... 86
E. 4 Paraphrased Instructions ..... 121
F Procedures and Surveys ..... 123</p>
<h1>A Experimental Setup Details</h1>
<p>To ensure reproducibility, we provide all details regarding our evaluation of the robustness of instruction-tuned LLMs.</p>
<h2>A. 1 Evaluation Protocols</h2>
<p>LLMs sometimes generate outputs that are correct but different from a (natural language) target. Therefore, we predict answers according to "multiple-choice" grading suggested by BIG-BENCH, by which we take the logits score and argmax over all the possible choices to obtain the prediction. In most cases, this approach yields the same accuracy as using exact match for evaluation. Here are the configurations for all the models we evaluated.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Models</th>
<th style="text-align: left;">Node Type</th>
<th style="text-align: left;">Precision</th>
<th style="text-align: center;">Batch Size</th>
<th style="text-align: right;">Hours</th>
<th style="text-align: right;">$C O_{2}$ emission (KG)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Inference</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">Flan-T5-Small</td>
<td style="text-align: left;">V100-SXM2-32G</td>
<td style="text-align: left;">FP16</td>
<td style="text-align: center;">128</td>
<td style="text-align: right;">64</td>
<td style="text-align: right;">4.0</td>
</tr>
<tr>
<td style="text-align: left;">Flan-T5-Base</td>
<td style="text-align: left;">V100-SXM2-32G</td>
<td style="text-align: left;">FP16</td>
<td style="text-align: center;">128</td>
<td style="text-align: right;">128</td>
<td style="text-align: right;">8.1</td>
</tr>
<tr>
<td style="text-align: left;">Flan-T5-Large</td>
<td style="text-align: left;">V100-SXM2-32G</td>
<td style="text-align: left;">FP16</td>
<td style="text-align: center;">32</td>
<td style="text-align: right;">256</td>
<td style="text-align: right;">16.2</td>
</tr>
<tr>
<td style="text-align: left;">Flan-T5-XL</td>
<td style="text-align: left;">V100-SXM2-32G</td>
<td style="text-align: left;">FP16</td>
<td style="text-align: center;">32</td>
<td style="text-align: right;">512</td>
<td style="text-align: right;">32.3</td>
</tr>
<tr>
<td style="text-align: left;">Flan-T5-XXL</td>
<td style="text-align: left;">RTX-A6000-46G</td>
<td style="text-align: left;">BF16</td>
<td style="text-align: center;">8</td>
<td style="text-align: right;">600</td>
<td style="text-align: right;">37.8</td>
</tr>
<tr>
<td style="text-align: left;">T0++</td>
<td style="text-align: left;">RTX-A6000-46G</td>
<td style="text-align: left;">BF16</td>
<td style="text-align: center;">2</td>
<td style="text-align: right;">128</td>
<td style="text-align: right;">8.1</td>
</tr>
<tr>
<td style="text-align: left;">Alpaca-7B</td>
<td style="text-align: left;">A100-SXM4-80G</td>
<td style="text-align: left;">BF16</td>
<td style="text-align: center;">16</td>
<td style="text-align: right;">160</td>
<td style="text-align: right;">13.4</td>
</tr>
<tr>
<td style="text-align: left;">Alpaca-13B</td>
<td style="text-align: left;">A100-SXM4-80G</td>
<td style="text-align: left;">BF16</td>
<td style="text-align: center;">8</td>
<td style="text-align: right;">192</td>
<td style="text-align: right;">16.1</td>
</tr>
<tr>
<td style="text-align: left;">Training</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">Flan-T5-XL</td>
<td style="text-align: left;">A100-SXM4-80G</td>
<td style="text-align: left;">BF16</td>
<td style="text-align: center;">256</td>
<td style="text-align: right;">256</td>
<td style="text-align: right;">21.5</td>
</tr>
<tr>
<td style="text-align: left;">Alpaca-7B</td>
<td style="text-align: left;">A100-SXM4-80G</td>
<td style="text-align: left;">BF16</td>
<td style="text-align: center;">128</td>
<td style="text-align: right;">80</td>
<td style="text-align: right;">6.7</td>
</tr>
<tr>
<td style="text-align: left;">Estimated Total $C O_{2}$ Emission (KG)</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: right;">164.2</td>
<td style="text-align: right;"></td>
</tr>
</tbody>
</table>
<p>Table 7: The configurations for evaluating different instruction-tuned LMs. The $C O_{2}$ emission is estimated by [15]. The total emission is estimated to be equivalent to 679 Km driven by an average ICE car.</p>
<h2>A. 2 Hyperparameters</h2>
<p>We conduct all our training and ablation studies on 8 A100s with 80GB memory. We kept the KL-Loss weight to 0.8 . We train both Flan-T5-XL and Alpaca-7B with a batch size of 4. The weight decay is set to be $1 e-5$. The learning rate is $5 e-4$ for the experiment.</p>
<h1>B Disaggregated Results</h1>
<h2>B. 1 Main Results and Scaling Results</h2>
<p>In the main paper we reported aggregated results over benchmark corpora. Here we report results on individual datasets for BBL. For MMLU, we evaluate all 57 datasets together, because these are all QA tasks (and we would want a QA model to be capable of answering questions across a diverse set of domains). We report means and stadard deviations of the accuracies achieved over all instructions in Table 9. The numbers on the left of the setting suggest the number of instructions used. We also share even more granular results-reporting the performance for each instruction-in CSV files provided in the supplemental material.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">MMLU</th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Model</td>
<td style="text-align: left;">Flan-T5-XL</td>
<td style="text-align: left;">Flan-T5-XXL</td>
<td style="text-align: left;">T0pp-11B</td>
<td style="text-align: left;">Alpaca-7B</td>
<td style="text-align: left;">Alpaca-13B</td>
</tr>
<tr>
<td style="text-align: left;">MMLU</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">ObSERVED</td>
<td style="text-align: left;">$\mathbf{4 8 . 1 ( \pm 0 . 3 )}$</td>
<td style="text-align: left;">$\mathbf{5 3 . 2 ( \pm 0 . 2 )}$</td>
<td style="text-align: left;">$48.3( \pm 0.9)$</td>
<td style="text-align: left;">$\mathbf{4 1 . 9 ( \pm 0 . 6 )}$</td>
<td style="text-align: left;">$\mathbf{4 7 . 8 ( \pm 0 . 5 )}$</td>
</tr>
<tr>
<td style="text-align: left;">UNOBSERVED</td>
<td style="text-align: left;">$47.5( \pm 0.9)$</td>
<td style="text-align: left;">$52.7( \pm 0.8)$</td>
<td style="text-align: left;">$\mathbf{4 8 . 5 ( \pm 0 . 9 )}$</td>
<td style="text-align: left;">$39.7( \pm 2.2)$</td>
<td style="text-align: left;">$47.0( \pm 0.8)$</td>
</tr>
</tbody>
</table>
<p>Table 8: Granular results for Table 3 on each dataset of MMLU We treated all tasks in MMLU equally as general QA and computed the overall accuracy.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">MMLU</th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Size Variance</td>
<td style="text-align: left;">Small (80M)</td>
<td style="text-align: left;">Base (250M)</td>
<td style="text-align: left;">Large (780M)</td>
<td style="text-align: left;">XL (3B)</td>
<td style="text-align: left;">XXL (11B)</td>
</tr>
<tr>
<td style="text-align: left;">MMLU</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">ObSERVED</td>
<td style="text-align: left;">$29.4( \pm 1.0)$</td>
<td style="text-align: left;">$\mathbf{3 4 . 1 ( \pm 0 . 4 )}$</td>
<td style="text-align: left;">$\mathbf{4 1 . 1 ( \pm 0 . 2 )}$</td>
<td style="text-align: left;">$\mathbf{4 8 . 1 ( \pm 0 . 3 )}$</td>
<td style="text-align: left;">$\mathbf{5 3 . 2 ( \pm 0 . 2 )}$</td>
</tr>
<tr>
<td style="text-align: left;">UNOBSERVED</td>
<td style="text-align: left;">$\mathbf{2 9 . 6 ( \pm 0 . 9 )}$</td>
<td style="text-align: left;">$33.8( \pm 1.2)$</td>
<td style="text-align: left;">$40.7( \pm 0.7)$</td>
<td style="text-align: left;">$47.5( \pm 0.9)$</td>
<td style="text-align: left;">$52.7( \pm 0.8)$</td>
</tr>
</tbody>
</table>
<p>Table 9: Granular results for Figure 2b on each dataset of MMLU We treated all tasks in MMLU equally as general QA and computed the overall accuracy.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">BBL-QA</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Model</td>
<td style="text-align: center;">Flan-T5-XL</td>
<td style="text-align: center;">Flan-T5-XXL</td>
<td style="text-align: center;">T0pp-11B</td>
<td style="text-align: center;">Alpaca-7B</td>
<td style="text-align: center;">Alpaca-13B</td>
</tr>
<tr>
<td style="text-align: center;">BBQ Lite</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Observed</td>
<td style="text-align: center;">$66.5( \pm 1.5)$</td>
<td style="text-align: center;">77.4 ( $\pm$ 2.4)</td>
<td style="text-align: center;">$51.8( \pm 5.3)$</td>
<td style="text-align: center;">$32.6( \pm 1.0)$</td>
<td style="text-align: center;">$43.5( \pm 1.4)$</td>
</tr>
<tr>
<td style="text-align: center;">Unobserved</td>
<td style="text-align: center;">$67.0( \pm 7.0)$</td>
<td style="text-align: center;">$73.7( \pm 11.4)$</td>
<td style="text-align: center;">$51.6( \pm 3.0)$</td>
<td style="text-align: center;">$33.1( \pm 1.3)$</td>
<td style="text-align: center;">$45.5( \pm 2.9)$</td>
</tr>
<tr>
<td style="text-align: center;">Code Desc.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Observed</td>
<td style="text-align: center;">73.6 ( $\pm$ 3.4)</td>
<td style="text-align: center;">83.6 ( $\pm$ 1.7)</td>
<td style="text-align: center;">$70.3( \pm 3.0)$</td>
<td style="text-align: center;">70.2 ( $\pm$ 2.5)</td>
<td style="text-align: center;">85.2 ( $\pm$ 2.4)</td>
</tr>
<tr>
<td style="text-align: center;">Unobserved</td>
<td style="text-align: center;">$69.7( \pm 12.4)$</td>
<td style="text-align: center;">$72.9( \pm 22.2)$</td>
<td style="text-align: center;">70.5 ( $\pm$ 3.7)</td>
<td style="text-align: center;">$67.5( \pm 11.3)$</td>
<td style="text-align: center;">$82.2( \pm 8.5)$</td>
</tr>
<tr>
<td style="text-align: center;">Hindu Know.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Observed</td>
<td style="text-align: center;">52.4 ( $\pm$ 1.6)</td>
<td style="text-align: center;">$53.9( \pm 1.8)$</td>
<td style="text-align: center;">57.1 ( $\pm$ 2.5)</td>
<td style="text-align: center;">$50.9( \pm 2.1)$</td>
<td style="text-align: center;">$63.8( \pm 0.7)$</td>
</tr>
<tr>
<td style="text-align: center;">Unobserved</td>
<td style="text-align: center;">$47.1( \pm 5.4)$</td>
<td style="text-align: center;">56.5 ( $\pm$ 3.5)</td>
<td style="text-align: center;">$53.2( \pm 3.0)$</td>
<td style="text-align: center;">$49.8( \pm 5.1)$</td>
<td style="text-align: center;">$63.9( \pm 1.1)$</td>
</tr>
<tr>
<td style="text-align: center;">Known Unk.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Observed</td>
<td style="text-align: center;">79.3 ( $\pm$ 2.5)</td>
<td style="text-align: center;">84.7 ( $\pm$ 2.1)</td>
<td style="text-align: center;">$70.9( \pm 10.2)$</td>
<td style="text-align: center;">75.2 ( $\pm$ 4.7)</td>
<td style="text-align: center;">81.9 ( $\pm$ 4.3)</td>
</tr>
<tr>
<td style="text-align: center;">Unobserved</td>
<td style="text-align: center;">$69.0( \pm 6.7)$</td>
<td style="text-align: center;">$80.6( \pm 8.1)$</td>
<td style="text-align: center;">76.1 ( $\pm$ 5.9)</td>
<td style="text-align: center;">$60.9( \pm 11.2)$</td>
<td style="text-align: center;">$71.1( \pm 16.3)$</td>
</tr>
<tr>
<td style="text-align: center;">Logical Ded.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Observed</td>
<td style="text-align: center;">52.5 ( $\pm$ 1.0)</td>
<td style="text-align: center;">58.0 ( $\pm$ 0.7)</td>
<td style="text-align: center;">45.5 ( $\pm$ 0.8)</td>
<td style="text-align: center;">25.5 ( $\pm$ 1.1)</td>
<td style="text-align: center;">29.2 ( $\pm$ 1.3)</td>
</tr>
<tr>
<td style="text-align: center;">Unobserved</td>
<td style="text-align: center;">$52.1( \pm 1.1)$</td>
<td style="text-align: center;">$57.8( \pm 0.6)$</td>
<td style="text-align: center;">$45.3( \pm 1.2)$</td>
<td style="text-align: center;">$24.5( \pm 2.3)$</td>
<td style="text-align: center;">$28.0( \pm 1.6)$</td>
</tr>
<tr>
<td style="text-align: center;">Novel Conc.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Observed</td>
<td style="text-align: center;">29.8 ( $\pm$ 2.4)</td>
<td style="text-align: center;">50.1 ( $\pm$ 1.9)</td>
<td style="text-align: center;">$28.8( \pm 2.9)$</td>
<td style="text-align: center;">37.2 ( $\pm$ 5.2)</td>
<td style="text-align: center;">20.0 ( $\pm$ 3.1)</td>
</tr>
<tr>
<td style="text-align: center;">Unobserved</td>
<td style="text-align: center;">31.2 ( $\pm$ 5.0)</td>
<td style="text-align: center;">$46.0( \pm 5.4)$</td>
<td style="text-align: center;">31.5 ( $\pm$ 5.1)</td>
<td style="text-align: center;">$36.1( \pm 7.6)$</td>
<td style="text-align: center;">$19.6( \pm 4.0)$</td>
</tr>
<tr>
<td style="text-align: center;">Logic Grid</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Observed</td>
<td style="text-align: center;">41.8 ( $\pm$ 1.1)</td>
<td style="text-align: center;">43.2 ( $\pm$ 1.8)</td>
<td style="text-align: center;">37.6 ( $\pm$ 1.7)</td>
<td style="text-align: center;">$24.4( \pm 1.9)$</td>
<td style="text-align: center;">29.3 ( $\pm$ 0.8)</td>
</tr>
<tr>
<td style="text-align: center;">Unobserved</td>
<td style="text-align: center;">$38.6( \pm 5.4)$</td>
<td style="text-align: center;">$39.6( \pm 4.4)$</td>
<td style="text-align: center;">$36.4( \pm 3.6)$</td>
<td style="text-align: center;">25.4 ( $\pm$ 1.1)</td>
<td style="text-align: center;">$28.7( \pm 1.2)$</td>
</tr>
<tr>
<td style="text-align: center;">Conc. Com.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Observed</td>
<td style="text-align: center;">75.9 ( $\pm$ 1.8)</td>
<td style="text-align: center;">75.0 ( $\pm$ 2.6)</td>
<td style="text-align: center;">$73.3( \pm 2.3)$</td>
<td style="text-align: center;">58.7 ( $\pm$ 4.0)</td>
<td style="text-align: center;">63.0 ( $\pm$ 2.2)</td>
</tr>
<tr>
<td style="text-align: center;">Unobserved</td>
<td style="text-align: center;">$75.0( \pm 1.9)$</td>
<td style="text-align: center;">$73.6( \pm 4.8)$</td>
<td style="text-align: center;">74.2 ( $\pm$ 2.6)</td>
<td style="text-align: center;">$55.9( \pm 6.2)$</td>
<td style="text-align: center;">$61.1( \pm 3.3)$</td>
</tr>
</tbody>
</table>
<p>Table 10: Granular results for Table 3 on each dataset of category BBL-QA</p>
<table>
<thead>
<tr>
<th style="text-align: center;">BBL-QA</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Size Variance</td>
<td style="text-align: center;">Small (80M)</td>
<td style="text-align: center;">Base (250M)</td>
<td style="text-align: center;">Large (780M)</td>
<td style="text-align: center;">XL (3B)</td>
<td style="text-align: center;">XXL (11B)</td>
</tr>
<tr>
<td style="text-align: center;">BBQ Lite</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Observed</td>
<td style="text-align: center;">$28.3( \pm 1.3)$</td>
<td style="text-align: center;">51.5 ( $\pm$ 1.4)</td>
<td style="text-align: center;">$56.6( \pm 2.0)$</td>
<td style="text-align: center;">$66.5( \pm 1.5)$</td>
<td style="text-align: center;">77.4 ( $\pm$ 2.4)</td>
</tr>
<tr>
<td style="text-align: center;">UnOBSERVED</td>
<td style="text-align: center;">28.6 ( $\pm$ 4.3)</td>
<td style="text-align: center;">$50.5( \pm 4.1)$</td>
<td style="text-align: center;">56.7 ( $\pm$ 4.7)</td>
<td style="text-align: center;">67.0 ( $\pm$ 7.0)</td>
<td style="text-align: center;">$73.7( \pm 11.4)$</td>
</tr>
<tr>
<td style="text-align: center;">Code Desc.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Observed</td>
<td style="text-align: center;">$22.0( \pm 4.0)$</td>
<td style="text-align: center;">55.7 ( $\pm$ 3.3)</td>
<td style="text-align: center;">72.4 ( $\pm$ 3.2)</td>
<td style="text-align: center;">73.6 ( $\pm$ 3.4)</td>
<td style="text-align: center;">83.6 ( $\pm$ 1.7)</td>
</tr>
<tr>
<td style="text-align: center;">UnOBSERVED</td>
<td style="text-align: center;">32.1 ( $\pm$ 7.2)</td>
<td style="text-align: center;">$48.6( \pm 7.2)$</td>
<td style="text-align: center;">$63.3( \pm 14.2)$</td>
<td style="text-align: center;">$69.7( \pm 12.4)$</td>
<td style="text-align: center;">$72.9( \pm 22.2)$</td>
</tr>
<tr>
<td style="text-align: center;">Hindu Know.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Observed</td>
<td style="text-align: center;">$25.1( \pm 15.2)$</td>
<td style="text-align: center;">30.7 ( $\pm$ 2.6)</td>
<td style="text-align: center;">$34.9( \pm 0.9)$</td>
<td style="text-align: center;">52.4 ( $\pm$ 1.6)</td>
<td style="text-align: center;">$53.9( \pm 1.8)$</td>
</tr>
<tr>
<td style="text-align: center;">UnOBSERVED</td>
<td style="text-align: center;">31.6 ( $\pm$ 10.7)</td>
<td style="text-align: center;">$26.9( \pm 4.4)$</td>
<td style="text-align: center;">37.5 ( $\pm$ 7.0)</td>
<td style="text-align: center;">$47.1( \pm 5.4)$</td>
<td style="text-align: center;">56.5 ( $\pm$ 3.5)</td>
</tr>
<tr>
<td style="text-align: center;">Known Unk.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Observed</td>
<td style="text-align: center;">$49.9( \pm 1.9)$</td>
<td style="text-align: center;">66.9 ( $\pm$ 4.7)</td>
<td style="text-align: center;">76.2 ( $\pm$ 3.5)</td>
<td style="text-align: center;">79.3 ( $\pm$ 2.5)</td>
<td style="text-align: center;">84.7 ( $\pm$ 2.1)</td>
</tr>
<tr>
<td style="text-align: center;">UnOBSERVED</td>
<td style="text-align: center;">52.8 ( $\pm$ 5.2)</td>
<td style="text-align: center;">$63.8( \pm 7.3)$</td>
<td style="text-align: center;">$68.4( \pm 11.1)$</td>
<td style="text-align: center;">$69.0( \pm 6.7)$</td>
<td style="text-align: center;">$80.6( \pm 8.1)$</td>
</tr>
<tr>
<td style="text-align: center;">Logical Ded.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Observed</td>
<td style="text-align: center;">$19.8( \pm 0.7)$</td>
<td style="text-align: center;">$27.1( \pm 1.3)$</td>
<td style="text-align: center;">$45.9( \pm 1.1)$</td>
<td style="text-align: center;">52.5 ( $\pm$ 1.0)</td>
<td style="text-align: center;">58.0 ( $\pm$ 0.7)</td>
</tr>
<tr>
<td style="text-align: center;">UnOBSERVED</td>
<td style="text-align: center;">19.9 ( $\pm$ 0.4)</td>
<td style="text-align: center;">28.9 ( $\pm$ 2.8)</td>
<td style="text-align: center;">46.4 ( $\pm$ 2.6)</td>
<td style="text-align: center;">$52.1( \pm 1.1)$</td>
<td style="text-align: center;">$57.8( \pm 0.6)$</td>
</tr>
<tr>
<td style="text-align: center;">Novel Conc.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Observed</td>
<td style="text-align: center;">22.9 ( $\pm$ 9.2)</td>
<td style="text-align: center;">$15.9( \pm 5.4)$</td>
<td style="text-align: center;">31.0 ( $\pm$ 2.3)</td>
<td style="text-align: center;">$29.8( \pm 2.4)$</td>
<td style="text-align: center;">50.1 ( $\pm$ 1.9)</td>
</tr>
<tr>
<td style="text-align: center;">UnOBSERVED</td>
<td style="text-align: center;">$19.3( \pm 3.6)$</td>
<td style="text-align: center;">16.8 ( $\pm$ 4.0)</td>
<td style="text-align: center;">$28.4( \pm 6.9)$</td>
<td style="text-align: center;">31.2 ( $\pm$ 5.0)</td>
<td style="text-align: center;">$46.0( \pm 5.4)$</td>
</tr>
<tr>
<td style="text-align: center;">Logic Grid</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Observed</td>
<td style="text-align: center;">$22.3( \pm 4.0)$</td>
<td style="text-align: center;">31.7 ( $\pm$ 0.8)</td>
<td style="text-align: center;">$32.6( \pm 2.1)$</td>
<td style="text-align: center;">41.8 ( $\pm$ 1.1)</td>
<td style="text-align: center;">43.2 ( $\pm$ 1.8)</td>
</tr>
<tr>
<td style="text-align: center;">UnOBSERVED</td>
<td style="text-align: center;">28.8 ( $\pm$ 3.1)</td>
<td style="text-align: center;">$29.4( \pm 5.1)$</td>
<td style="text-align: center;">34.1 ( $\pm$ 2.8)</td>
<td style="text-align: center;">$38.6( \pm 5.4)$</td>
<td style="text-align: center;">$39.6( \pm 4.4)$</td>
</tr>
<tr>
<td style="text-align: center;">Conc. Com.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Observed</td>
<td style="text-align: center;">$30.4( \pm 10.6)$</td>
<td style="text-align: center;">55.6 ( $\pm$ 5.1)</td>
<td style="text-align: center;">64.2 ( $\pm$ 1.9)</td>
<td style="text-align: center;">75.9 ( $\pm$ 1.8)</td>
<td style="text-align: center;">75.0 ( $\pm$ 2.6)</td>
</tr>
<tr>
<td style="text-align: center;">UnOBSERVED</td>
<td style="text-align: center;">32.2 ( $\pm$ 16.7)</td>
<td style="text-align: center;">$54.5( \pm 9.4)$</td>
<td style="text-align: center;">$58.1( \pm 11.6)$</td>
<td style="text-align: center;">$75.0( \pm 1.9)$</td>
<td style="text-align: center;">$73.6( \pm 4.8)$</td>
</tr>
</tbody>
</table>
<p>Table 11: Granular results for Figure 2b on each dataset of category BBL-QA</p>
<table>
<thead>
<tr>
<th style="text-align: center;">BBL-BC</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Model</td>
<td style="text-align: center;">Flan-T5-XL</td>
<td style="text-align: center;">Flan-T5-XXL</td>
<td style="text-align: center;">T0pp-11B</td>
<td style="text-align: center;">Alpaca-7B</td>
<td style="text-align: center;">Alpaca-13B</td>
</tr>
<tr>
<td style="text-align: center;">Play Dialog</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Observed</td>
<td style="text-align: center;">61.6 ( $\pm$ 5.8)</td>
<td style="text-align: center;">51.8 ( $\pm 9.5$ )</td>
<td style="text-align: center;">62.7 ( $\pm$ 0.4)</td>
<td style="text-align: center;">45.0 ( $\pm$ 2.0)</td>
<td style="text-align: center;">53.4 ( $\pm$ 5.8)</td>
</tr>
<tr>
<td style="text-align: center;">UnOBSERVED</td>
<td style="text-align: center;">$53.0( \pm 6.9)$</td>
<td style="text-align: center;">58.1 ( $\pm$ 4.4)</td>
<td style="text-align: center;">$55.2( \pm 8.1)$</td>
<td style="text-align: center;">$42.9( \pm 7.9)$</td>
<td style="text-align: center;">$42.9( \pm 8.8)$</td>
</tr>
<tr>
<td style="text-align: center;">Strat. QA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Observed</td>
<td style="text-align: center;">58.7 ( $\pm 3.3$ )</td>
<td style="text-align: center;">64.2 ( $\pm$ 3.0)</td>
<td style="text-align: center;">51.0 ( $\pm 1.8$ )</td>
<td style="text-align: center;">53.0 ( $\pm 2.1$ )</td>
<td style="text-align: center;">56.7 ( $\pm 3.8$ )</td>
</tr>
<tr>
<td style="text-align: center;">UnOBSERVED</td>
<td style="text-align: center;">60.7 ( $\pm$ 7.5)</td>
<td style="text-align: center;">59.3 ( $\pm 6.1$ )</td>
<td style="text-align: center;">54.5 ( $\pm$ 0.9)</td>
<td style="text-align: center;">53.3 ( $\pm$ 4.1)</td>
<td style="text-align: center;">61.0 ( $\pm$ 1.9)</td>
</tr>
<tr>
<td style="text-align: center;">Strange St.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Observed</td>
<td style="text-align: center;">69.3 ( $\pm 4.4$ )</td>
<td style="text-align: center;">71.0 ( $\pm 7.3$ )</td>
<td style="text-align: center;">51.2 ( $\pm$ 5.1)</td>
<td style="text-align: center;">67.0 ( $\pm$ 4.7)</td>
<td style="text-align: center;">69.8 ( $\pm$ 5.0)</td>
</tr>
<tr>
<td style="text-align: center;">Unobserved</td>
<td style="text-align: center;">70.5 ( $\pm$ 7.0)</td>
<td style="text-align: center;">77.4 ( $\pm$ 6.1)</td>
<td style="text-align: center;">48.4 ( $\pm 3.1$ )</td>
<td style="text-align: center;">59.9 ( $\pm 9.4$ )</td>
<td style="text-align: center;">57.5 ( $\pm 5.6$ )</td>
</tr>
<tr>
<td style="text-align: center;">Winowhy</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Observed</td>
<td style="text-align: center;">76.5 ( $\pm$ 1.9)</td>
<td style="text-align: center;">75.6 ( $\pm$ 4.0)</td>
<td style="text-align: center;">99.6 ( $\pm$ 1.0)</td>
<td style="text-align: center;">50.1 ( $\pm 4.8$ )</td>
<td style="text-align: center;">51.9 ( $\pm 4.6$ )</td>
</tr>
<tr>
<td style="text-align: center;">UnOBSERVED</td>
<td style="text-align: center;">$60.2( \pm 6.2)$</td>
<td style="text-align: center;">59.7 ( $\pm 7.7$ )</td>
<td style="text-align: center;">$60.9( \pm 5.1)$</td>
<td style="text-align: center;">53.4 ( $\pm$ 4.6)</td>
<td style="text-align: center;">55.2 ( $\pm$ 6.0)</td>
</tr>
</tbody>
</table>
<p>Table 12: Granular results for Table 3 on each dataset of category BBL-BC</p>
<table>
<thead>
<tr>
<th style="text-align: center;">BBL-BC</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Size Variance</td>
<td style="text-align: center;">Small (80M)</td>
<td style="text-align: center;">Base (250M)</td>
<td style="text-align: center;">Large (780M)</td>
<td style="text-align: center;">XL (3B)</td>
<td style="text-align: center;">XXL (11B)</td>
</tr>
<tr>
<td style="text-align: center;">Play Dialog</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Observed</td>
<td style="text-align: center;">51.6 ( $\pm 13.3$ )</td>
<td style="text-align: center;">54.6 ( $\pm 10.7$ )</td>
<td style="text-align: center;">59.0 ( $\pm$ 6.7)</td>
<td style="text-align: center;">61.6 ( $\pm$ 5.8)</td>
<td style="text-align: center;">51.8 ( $\pm 9.5$ )</td>
</tr>
<tr>
<td style="text-align: center;">Unobserved</td>
<td style="text-align: center;">61.6 ( $\pm$ 4.6)</td>
<td style="text-align: center;">56.3 ( $\pm$ 10.9)</td>
<td style="text-align: center;">57.3 ( $\pm 7.8$ )</td>
<td style="text-align: center;">53.0 ( $\pm 6.9$ )</td>
<td style="text-align: center;">58.1 ( $\pm$ 4.4)</td>
</tr>
<tr>
<td style="text-align: center;">Strat. QA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Observed</td>
<td style="text-align: center;">52.3 ( $\pm$ 1.0)</td>
<td style="text-align: center;">48.9 ( $\pm 2.1$ )</td>
<td style="text-align: center;">60.9 ( $\pm$ 1.3)</td>
<td style="text-align: center;">58.7 ( $\pm 3.3$ )</td>
<td style="text-align: center;">64.2 ( $\pm$ 3.0)</td>
</tr>
<tr>
<td style="text-align: center;">Unobserved</td>
<td style="text-align: center;">51.5 ( $\pm 2.7$ )</td>
<td style="text-align: center;">52.9 ( $\pm$ 1.3)</td>
<td style="text-align: center;">53.9 ( $\pm 3.8$ )</td>
<td style="text-align: center;">60.7 ( $\pm$ 7.5)</td>
<td style="text-align: center;">59.3 ( $\pm 6.1$ )</td>
</tr>
<tr>
<td style="text-align: center;">Strange St.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Observed</td>
<td style="text-align: center;">41.3 ( $\pm 10.3$ )</td>
<td style="text-align: center;">43.1 ( $\pm$ 4.2)</td>
<td style="text-align: center;">54.4 ( $\pm 1.2$ )</td>
<td style="text-align: center;">69.3 ( $\pm 4.4$ )</td>
<td style="text-align: center;">71.0 ( $\pm 7.3$ )</td>
</tr>
<tr>
<td style="text-align: center;">Unobserved</td>
<td style="text-align: center;">55.9 ( $\pm$ 18.5)</td>
<td style="text-align: center;">42.0 ( $\pm 5.5$ )</td>
<td style="text-align: center;">67.9 ( $\pm$ 8.0)</td>
<td style="text-align: center;">70.5 ( $\pm$ 7.0)</td>
<td style="text-align: center;">77.4 ( $\pm$ 6.1)</td>
</tr>
<tr>
<td style="text-align: center;">Winowhy</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Observed</td>
<td style="text-align: center;">54.8 ( $\pm$ 1.6)</td>
<td style="text-align: center;">55.9 ( $\pm 7.6$ )</td>
<td style="text-align: center;">60.4 ( $\pm$ 9.8)</td>
<td style="text-align: center;">76.5 ( $\pm$ 1.9)</td>
<td style="text-align: center;">75.6 ( $\pm$ 4.0)</td>
</tr>
<tr>
<td style="text-align: center;">Unobserved</td>
<td style="text-align: center;">53.7 ( $\pm 5.1$ )</td>
<td style="text-align: center;">57.1 ( $\pm$ 6.7)</td>
<td style="text-align: center;">53.5 ( $\pm 4.9$ )</td>
<td style="text-align: center;">60.2 ( $\pm 6.2$ )</td>
<td style="text-align: center;">59.7 ( $\pm 7.7$ )</td>
</tr>
</tbody>
</table>
<p>Table 13: Granular results for Figure 2b on each dataset of category BBL-BC</p>
<table>
<thead>
<tr>
<th style="text-align: center;">BBL-MC</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Model <br> Language ID</td>
<td style="text-align: center;">Flan-T5-XL</td>
<td style="text-align: center;">Flan-T5-XXL</td>
<td style="text-align: center;">T0pp-11B</td>
<td style="text-align: center;">Alpaca-7B</td>
<td style="text-align: center;">Alpaca-13B</td>
</tr>
<tr>
<td style="text-align: center;">Observed</td>
<td style="text-align: center;">32.6 ( $\pm$ 0.2)</td>
<td style="text-align: center;">38.9 ( $\pm$ 0.3)</td>
<td style="text-align: center;">15.7 ( $\pm$ 3.0)</td>
<td style="text-align: center;">12.9 ( $\pm 0.7$ )</td>
<td style="text-align: center;">18.5 ( $\pm 0.7$ )</td>
</tr>
<tr>
<td style="text-align: center;">Unobserved</td>
<td style="text-align: center;">25.5 ( $\pm 7.3$ )</td>
<td style="text-align: center;">31.6 ( $\pm 9.4$ )</td>
<td style="text-align: center;">14.3 ( $\pm 2.4$ )</td>
<td style="text-align: center;">14.7 ( $\pm$ 1.7)</td>
<td style="text-align: center;">21.7 ( $\pm$ 0.7)</td>
</tr>
<tr>
<td style="text-align: center;">Vitamin C</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Observed</td>
<td style="text-align: center;">78.6 ( $\pm 1.1$ )</td>
<td style="text-align: center;">78.5 ( $\pm 0.7$ )</td>
<td style="text-align: center;">68.3 ( $\pm 1.1$ )</td>
<td style="text-align: center;">51.4 ( $\pm$ 3.6)</td>
<td style="text-align: center;">54.9 ( $\pm$ 2.9)</td>
</tr>
<tr>
<td style="text-align: center;">Unobserved</td>
<td style="text-align: center;">78.6 ( $\pm 3.6$ )</td>
<td style="text-align: center;">80.2 ( $\pm$ 1.6)</td>
<td style="text-align: center;">68.5 ( $\pm$ 2.4)</td>
<td style="text-align: center;">18.1 ( $\pm 5.3$ )</td>
<td style="text-align: center;">23.6 ( $\pm 14.2$ )</td>
</tr>
</tbody>
</table>
<p>Table 14: Granular results for Table 3 on each dataset of category BBL-MC</p>
<table>
<thead>
<tr>
<th style="text-align: center;">BBL-MC</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Size Variance <br> Language ID</td>
<td style="text-align: center;">Small (80M)</td>
<td style="text-align: center;">Base (250M)</td>
<td style="text-align: center;">Large (780M)</td>
<td style="text-align: center;">XL (3B)</td>
<td style="text-align: center;">XXL (11B)</td>
</tr>
<tr>
<td style="text-align: center;">Observed</td>
<td style="text-align: center;">$11.9( \pm 0.2)$</td>
<td style="text-align: center;">$17.0( \pm 0.3)$</td>
<td style="text-align: center;">$25.8( \pm 0.3)$</td>
<td style="text-align: center;">$32.6( \pm 0.2)$</td>
<td style="text-align: center;">$38.9( \pm 0.3)$</td>
</tr>
<tr>
<td style="text-align: center;">UnOBSERVED</td>
<td style="text-align: center;">$9.5( \pm 0.2)$</td>
<td style="text-align: center;">$12.4( \pm 1.5)$</td>
<td style="text-align: center;">$19.2( \pm 4.4)$</td>
<td style="text-align: center;">$25.5( \pm 7.3)$</td>
<td style="text-align: center;">$31.6( \pm 9.4)$</td>
</tr>
<tr>
<td style="text-align: center;">Vitamin C</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Observed</td>
<td style="text-align: center;">$46.6( \pm 4.0)$</td>
<td style="text-align: center;">$60.7( \pm 5.6)$</td>
<td style="text-align: center;">$72.6( \pm 1.5)$</td>
<td style="text-align: center;">$78.6( \pm 1.1)$</td>
<td style="text-align: center;">$78.5( \pm 0.7)$</td>
</tr>
<tr>
<td style="text-align: center;">UnOBSERVED</td>
<td style="text-align: center;">$40.8( \pm 4.2)$</td>
<td style="text-align: center;">$63.0( \pm 4.6)$</td>
<td style="text-align: center;">$36.4( \pm 0.8)$</td>
<td style="text-align: center;">$78.6( \pm 3.6)$</td>
<td style="text-align: center;">$80.2( \pm 1.6)$</td>
</tr>
</tbody>
</table>
<p>Table 15: Granular results for Figure 2b on each dataset of category BBL-MC</p>
<h1>B. 2 "Closer Look" Experiment Results</h1>
<p>Here, we provide the detailed results that we reported in 3</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Observed</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Unobserved</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Control</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Closest</td>
<td style="text-align: center;">Incorrect</td>
<td style="text-align: center;">Collected</td>
<td style="text-align: center;">Task Designer</td>
<td style="text-align: center;">Negated</td>
<td style="text-align: center;">Nonsensical</td>
</tr>
<tr>
<td style="text-align: center;">Intent</td>
<td style="text-align: center;">93.6</td>
<td style="text-align: center;">93.1</td>
<td style="text-align: center;">94.1</td>
<td style="text-align: center;">94.66</td>
<td style="text-align: center;">28.0</td>
<td style="text-align: center;">40.7</td>
</tr>
<tr>
<td style="text-align: center;">Recognition</td>
<td style="text-align: center;">$\pm 0.3$</td>
<td style="text-align: center;">$\pm 1.0$</td>
<td style="text-align: center;">$\pm 0.6$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\pm 6.5$</td>
<td style="text-align: center;">$\pm 7.2$</td>
</tr>
<tr>
<td style="text-align: center;">Empirical</td>
<td style="text-align: center;">39.2</td>
<td style="text-align: center;">41.62</td>
<td style="text-align: center;">37.6</td>
<td style="text-align: center;">37.4</td>
<td style="text-align: center;">28.1</td>
<td style="text-align: center;">30.9</td>
</tr>
<tr>
<td style="text-align: center;">Judgments</td>
<td style="text-align: center;">$\pm 0.8$</td>
<td style="text-align: center;">$\pm 6.3$</td>
<td style="text-align: center;">$\pm 1.7$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\pm 3.5$</td>
<td style="text-align: center;">$\pm 2.5$</td>
</tr>
<tr>
<td style="text-align: center;">Conceptual</td>
<td style="text-align: center;">78.0</td>
<td style="text-align: center;">78.92</td>
<td style="text-align: center;">75.3</td>
<td style="text-align: center;">58.3</td>
<td style="text-align: center;">11.2</td>
<td style="text-align: center;">63.7</td>
</tr>
<tr>
<td style="text-align: center;">Combinations</td>
<td style="text-align: center;">$\pm 1.6$</td>
<td style="text-align: center;">$\pm 0.5$</td>
<td style="text-align: center;">$\pm 3.3$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\pm 2.1$</td>
<td style="text-align: center;">$\pm 2.6$</td>
</tr>
<tr>
<td style="text-align: center;">Language</td>
<td style="text-align: center;">38.94</td>
<td style="text-align: center;">29.3</td>
<td style="text-align: center;">28.8</td>
<td style="text-align: center;">27.6</td>
<td style="text-align: center;">36.9</td>
<td style="text-align: center;">12.4</td>
</tr>
<tr>
<td style="text-align: center;">Identification</td>
<td style="text-align: center;">$\pm 0.3$</td>
<td style="text-align: center;">$\pm 5.0$</td>
<td style="text-align: center;">$\pm 5.8$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\pm 0.5$</td>
<td style="text-align: center;">$\pm 0.5$</td>
</tr>
<tr>
<td style="text-align: center;">Logical</td>
<td style="text-align: center;">56.92</td>
<td style="text-align: center;">49.4</td>
<td style="text-align: center;">52.8</td>
<td style="text-align: center;">53.8</td>
<td style="text-align: center;">11.8</td>
<td style="text-align: center;">34.4</td>
</tr>
<tr>
<td style="text-align: center;">Sequence</td>
<td style="text-align: center;">$\pm 6.6$</td>
<td style="text-align: center;">$\pm 6.1$</td>
<td style="text-align: center;">$\pm 5.3$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\pm 6.9$</td>
<td style="text-align: center;">$\pm 5.9$</td>
</tr>
<tr>
<td style="text-align: center;">Crash</td>
<td style="text-align: center;">53.6</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">50.5</td>
<td style="text-align: center;">63.16</td>
<td style="text-align: center;">28.6</td>
<td style="text-align: center;">43.7</td>
</tr>
<tr>
<td style="text-align: center;">Blossom</td>
<td style="text-align: center;">$\pm 2.8$</td>
<td style="text-align: center;">$\pm 5.3$</td>
<td style="text-align: center;">$\pm 2.2$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\pm 6.2$</td>
<td style="text-align: center;">$\pm 1.4$</td>
</tr>
<tr>
<td style="text-align: center;">Epistemic</td>
<td style="text-align: center;">62.8</td>
<td style="text-align: center;">59.3</td>
<td style="text-align: center;">58.1</td>
<td style="text-align: center;">60.2</td>
<td style="text-align: center;">65.49</td>
<td style="text-align: center;">49.5</td>
</tr>
<tr>
<td style="text-align: center;">Reasoning</td>
<td style="text-align: center;">$\pm 2.9$</td>
<td style="text-align: center;">$\pm 3.4$</td>
<td style="text-align: center;">$\pm 1.7$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\pm 4.6$</td>
<td style="text-align: center;">$\pm 1.3$</td>
</tr>
<tr>
<td style="text-align: center;">Overall</td>
<td style="text-align: center;">60.45</td>
<td style="text-align: center;">57.4</td>
<td style="text-align: center;">56.8</td>
<td style="text-align: center;">56.4</td>
<td style="text-align: center;">30.0</td>
<td style="text-align: center;">39.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\pm 2.1$</td>
<td style="text-align: center;">$\pm 2.2$</td>
<td style="text-align: center;">$\pm 1.8$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\pm 2.2$</td>
<td style="text-align: center;">$\pm 2.3$</td>
</tr>
</tbody>
</table>
<p>Table 16: The detailed results of "A Closer Look" experiment. We provide all the instructions picked and their sources in Section E.3. In could be seen that the "Incorrect" but observed instruction, in most cases, outperform the correct but unobserved instructions ("Collected" and "Task Designer").</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ We pad instances such that the lengths in a given batch are effectively equal; the sum is therefore from 1 to the length associated with the current batch, we omit this for simplicity.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
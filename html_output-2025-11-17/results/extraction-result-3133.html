<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3133 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3133</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3133</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-73.html">extraction-schema-73</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <p><strong>Paper ID:</strong> paper-b285c067f2da04bf5647beb8853bfddf6d9b4e1b</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/b285c067f2da04bf5647beb8853bfddf6d9b4e1b" target="_blank">A Multi-Type Multi-Span Network for Reading Comprehension that Requires Discrete Reasoning</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> The Multi-Type Multi-Span Network (MTMSN) is introduced, a neural reading comprehension model that combines a multi-type answer predictor designed to support various answer types with amulti-span extraction method for dynamically producing one or multiple text spans.</p>
                <p><strong>Paper Abstract:</strong> Rapid progress has been made in the field of reading comprehension and question answering, where several systems have achieved human parity in some simplified settings. However, the performance of these models degrades significantly when they are applied to more realistic scenarios, such as answers involve various types, multiple text strings are correct answers, or discrete reasoning abilities are required. In this paper, we introduce the Multi-Type Multi-Span Network (MTMSN), a neural reading comprehension model that combines a multi-type answer predictor designed to support various answer types (e.g., span, count, negation, and arithmetic expression) with a multi-span extraction method for dynamically producing one or multiple text spans. In addition, an arithmetic expression reranking mechanism is proposed to rank expression candidates for further confirming the prediction. Experiments show that our model achieves 79.9 F1 on the DROP hidden test set, creating new state-of-the-art results. Source code (https://github.com/huminghao16/MTMSN) is released to facilitate future work.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3133.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3133.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MTMSN Add/Sub sign-assignment</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MTMSN per-number sign-assignment arithmetic mechanism (addition/subtraction)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Arithmetic is performed by assigning a categorical sign (plus, minus, zero) to each number mentioned in the passage using a learned classifier over per-number contextual vectors; the signed numbers form an arithmetic expression that is executed symbolically to produce the numeric answer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT-based MTMSN (BERT_BASE / BERT_LARGE)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder: pre-trained BERT transformer blocks (BERT_BASE and BERT_LARGE variants) produce contextual token representations; task-specific heads operate on pooled vectors and per-number vectors U (concatenation of M2 and M3 representations) to predict signs and other answer types.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Passage-based addition/subtraction of numbers (signed sum), typically up to 3 numbers in annotation search; decoding allows up to M signed numbers (M default 4).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Per-number sign classification (3-way: + / - / 0) using FFN over per-number vector u_i concatenated with global vectors (question, passage, CLS); resulting signed-number expression is executed symbolically.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Ablation: removing Add/Sub component causes a large performance drop (MTMSN_LARGE EM 76.68 → 53.8, F1 80.54 → 58.0), demonstrating the component's necessity; annotation statistics show adding arithmetic expressions increases labeled coverage from 56.4% to 91.7%, indicating many answers require arithmetic.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Error analysis shows many incorrect arithmetic computations (38% of sampled EM errors), indicating the sign-assignment mechanism yields wrong expressions frequently; the paper notes sign predictions are initially independent and can produce obviously-wrong global results (e.g., all minus/zero).</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Beam search decoding (to enumerate high-probability sign assignments); expression reranking (separate intervention described in another entry); limiting maximum signed numbers M; weak supervision with marginal likelihood over possible annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Beam search + reranking (see reranking entry) improves final selection versus taking max-sign-probability expression; restricting M reduces distraction from long expressions and improves reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>On gold Number-type questions (by gold answer type): MTMSN_LARGE EM 80.9, F1 81.1. On predicted Add/Sub type (by model's predicted type): EM 78.1, F1 78.2. Ablation: w/o Add/Sub (LARGE) EM 53.8, F1 58.0 (drop ≈22.9 EM, 22.5 F1). Annotation coverage: arithmetic annotations raise labeled examples to 91.7%.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Incorrect sign assignments producing implausible/large negative answers; inability to reliably reason over expressions with many numbers (performance drops as max M increases); limitations to add/sub only (no multiplication/division implemented).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Expressions produced by sign-assignment are executed by a symbolic evaluator; overall model still lags human performance (Human F1 on DROP 95.98 vs MTMSN test F1 79.88). The method is hybrid (neural sign prediction + symbolic execution), not a pure algorithmic solver.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Multi-Type Multi-Span Network for Reading Comprehension that Requires Discrete Reasoning', 'publication_date_yy_mm': '2019-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3133.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3133.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MTMSN arithmetic reranking</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MTMSN arithmetic expression reranking mechanism</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Candidate arithmetic expressions (from beam search over per-number sign assignments) are re-scored by a neural reranker that composes per-number contextual vectors with learned sign embeddings to incorporate expression-level context before selecting the final expression.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT-based MTMSN (BERT_BASE / BERT_LARGE)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Reranker consumes number vectors V_i (from U) and learned sign embeddings C_i (embedding matrix E ∈ R^{3×2×D}), computes an attention-weighted expression vector h^V and applies an FFN together with global vectors (question, passage, CLS) to produce p^arith for each candidate expression.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Reranking of candidate addition/subtraction expressions over passage numbers (beam size typically 3); applied to expressions with up to M signed numbers (M default 4).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Expression-level contextual scoring: sum/attention over (number vector + sign embedding) pairs produces a fixed expression representation which is combined with global vectors and an FFN to yield a reranking score.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Ablation: removing reranking reduces performance (MTMSN_LARGE F1 drops by ~1.8%), indicating reranking filters incorrect high-probability sign assignments; beam-size experiments show beam=3 gives best performance, consistent with reranking benefiting from a small candidate set.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Reranking yields modest gains (∼1.8% F1) — indicates context-aware reranking helps but does not eliminate arithmetic errors; reranker effectiveness depends on beam containing correct candidate.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Neural reranking of beam-searched expression candidates; learned sign embeddings; attention pooling over signed-number tuples.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Improved final-expression selection: reranking produced a ≈1.8% absolute F1 increase for the LARGE model; prevents selecting obviously-wrong expressions with high independent sign-probability.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reranking adds ≈1.8 percentage points F1 on MTMSN_LARGE (reported as 'w/o Reranking' comparison: MTMSN_LARGE F1 80.54 → without reranking 78.7). Beam size tuned to 3 gives best reranking performance.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Reranker can only choose among beam candidates — if beam misses correct expression, reranking cannot recover; larger beam sizes can hurt reranking (more candidates to rank, more noise).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Reranking still relies on subsequent symbolic execution of selected expression; compared to human-level reasoning, reranking is a limited contextual validation step rather than a general algorithmic corrector.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Multi-Type Multi-Span Network for Reading Comprehension that Requires Discrete Reasoning', 'publication_date_yy_mm': '2019-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3133.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3133.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MTMSN counting & negation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MTMSN counting classifier and negation operation modeling</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Counting is modeled as a multi-class classification over small integer classes; logical negation on numbers is modeled as a per-number binary classification (apply 100 - number) to capture questions asking for complements or 'not' calculations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT-based MTMSN (BERT_BASE / BERT_LARGE)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Count head: softmax over a small set of count classes (default up to 10) built from an attention summary h^U over number vectors U plus global vectors, followed by an FFN. Negation head: per-number 2-way FFN classifier over u_i concatenated with global vectors (question, passage, CLS).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Counting entities (discrete small counts) and logical numeric negation (e.g., 'How many percent were not X?' → 100 - X).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Classification-based approach: counts are predicted via multiclass softmax over pre-defined count range; negation is detected by binary per-number classifier and applied as arithmetic 100 - number.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Ablation: removing Count reduces F1 (~4–5%); removing Negation reduces F1 significantly (~9% on both models), showing these heads materially improve performance. Table 5 reports predicted-type performance: Count EM/F1 70.4; Negation EM/F1 96.3 (MTMSN_LARGE by predicted type).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Negation detection relies on heuristic annotation (they search for 100 - number equals answer) for weak supervision, which may miss more complex negation cases; counts limited to a small class set (10 classes) — may not generalize to larger counts.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Add explicit answer-type heads for Count and Negation; weakly supervised annotation rules to identify negation instances; direct training with marginal likelihood over candidate annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Adding negation type yields ~9% F1 improvement; adding count head yields ≈5% F1 improvement. Explicit modeling increases labeled coverage (final coverage 97.9% when negation included).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>By predicted type (MTMSN_LARGE): Count EM 70.4, F1 70.4; Negation EM 96.3, F1 96.3. Ablation rows: w/o Count (LARGE) F1 75.6 vs full 80.5 (drop ~4.9); w/o Negation (LARGE) F1 70.9 vs full 80.5 (drop ~9.6).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Negation supervision is heuristic and may miss cases; counting limited to small integer classes (max 10) so higher counts mispredicted; miscounting accounts for ~8% of sampled EM errors.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Negation and counting are modeled neurally and yield strong predicted-type performance (negation especially high), but overall numeric reasoning still below human performance; negation handled as a discrete operation (100 - x) rather than general symbolic reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Multi-Type Multi-Span Network for Reading Comprehension that Requires Discrete Reasoning', 'publication_date_yy_mm': '2019-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3133.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3133.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Training / limitations</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Weak supervision, beam search constraints, and limitations in arithmetic handling</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Training uses weak supervision (marginal likelihood over candidate annotations) and restricts expression search (addition/subtraction of up to 3 numbers during annotation, max M signed numbers in decoding), which constrains the scope of arithmetic the model learns and exposes failure modes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT-based MTMSN (BERT_BASE / BERT_LARGE)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Weakly supervised training: enumerate possible annotations (matching spans, arithmetic expressions, counts, negation, number of spans); beam search labels candidate expressions (correct/wrong) used to supervise reranker; marginal likelihood objective sums probabilities over all annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Addition/subtraction limited during annotation search to combinations of up to 3 numbers; decoding allows M up to default 4 but performance degrades as M increases.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Weak supervision over enumerated arithmetic annotations plus beam search for candidate expression generation; marginal likelihood objective used to train multi-headed predictors.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Annotation statistics: including Add/Sub raises annotated coverage from 56.4% to 91.7%; beam-size experiments and M experiments show trade-offs: beam=3 best, and performance decreases as M increases (most expressions use 2–3 numbers).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Limiting search (≤3 in annotation) and small M prevents learning or evaluating longer/chained operations reliably; error analysis identifies incorrect arithmetic computations (38%) and other failure modes (sorting 18%, miscounting 8%).</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Heuristic search limits during annotation; beam search with tunable beam size; marginal-likelihood training; NMS for multi-span extraction (not arithmetic but part of answer extraction pipeline).</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Heuristic search and limits allowed tractable training and produced strong gains on DROP, but also left a substantial fraction of arithmetic errors; tuning beam and M is necessary to balance recall vs noise (beam=3, M small).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Overall MTMSN_LARGE Dev/Test: EM ≈ 76.68 / 75.85, F1 ≈ 80.54 / 79.88. Error analysis of 100 sampled EM errors: 38% incorrect arithmetic computations, 18% require sorting, 8% miscounting. Performance decreases as M (max signed numbers) increases; optimal M small (2–3 typical).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Search and modeling limited to add/sub and simple negation; complex chained arithmetic, sorting, multiplication/division not handled; many arithmetic errors remain; reranking/beam based methods depend on beam containing correct candidate.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Multi-Type Multi-Span Network for Reading Comprehension that Requires Discrete Reasoning', 'publication_date_yy_mm': '2019-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs <em>(Rating: 2)</em></li>
                <li>Analysing mathematical reasoning abilities of neural models <em>(Rating: 2)</em></li>
                <li>Neural Programmer: Inducing latent programs with gradient descent <em>(Rating: 2)</em></li>
                <li>Neural Turing Machines <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3133",
    "paper_id": "paper-b285c067f2da04bf5647beb8853bfddf6d9b4e1b",
    "extraction_schema_id": "extraction-schema-73",
    "extracted_data": [
        {
            "name_short": "MTMSN Add/Sub sign-assignment",
            "name_full": "MTMSN per-number sign-assignment arithmetic mechanism (addition/subtraction)",
            "brief_description": "Arithmetic is performed by assigning a categorical sign (plus, minus, zero) to each number mentioned in the passage using a learned classifier over per-number contextual vectors; the signed numbers form an arithmetic expression that is executed symbolically to produce the numeric answer.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "BERT-based MTMSN (BERT_BASE / BERT_LARGE)",
            "model_description": "Encoder: pre-trained BERT transformer blocks (BERT_BASE and BERT_LARGE variants) produce contextual token representations; task-specific heads operate on pooled vectors and per-number vectors U (concatenation of M2 and M3 representations) to predict signs and other answer types.",
            "arithmetic_task_type": "Passage-based addition/subtraction of numbers (signed sum), typically up to 3 numbers in annotation search; decoding allows up to M signed numbers (M default 4).",
            "reported_mechanism": "Per-number sign classification (3-way: + / - / 0) using FFN over per-number vector u_i concatenated with global vectors (question, passage, CLS); resulting signed-number expression is executed symbolically.",
            "evidence_for_mechanism": "Ablation: removing Add/Sub component causes a large performance drop (MTMSN_LARGE EM 76.68 → 53.8, F1 80.54 → 58.0), demonstrating the component's necessity; annotation statistics show adding arithmetic expressions increases labeled coverage from 56.4% to 91.7%, indicating many answers require arithmetic.",
            "evidence_against_mechanism": "Error analysis shows many incorrect arithmetic computations (38% of sampled EM errors), indicating the sign-assignment mechanism yields wrong expressions frequently; the paper notes sign predictions are initially independent and can produce obviously-wrong global results (e.g., all minus/zero).",
            "intervention_type": "Beam search decoding (to enumerate high-probability sign assignments); expression reranking (separate intervention described in another entry); limiting maximum signed numbers M; weak supervision with marginal likelihood over possible annotations.",
            "effect_of_intervention": "Beam search + reranking (see reranking entry) improves final selection versus taking max-sign-probability expression; restricting M reduces distraction from long expressions and improves reliability.",
            "performance_metrics": "On gold Number-type questions (by gold answer type): MTMSN_LARGE EM 80.9, F1 81.1. On predicted Add/Sub type (by model's predicted type): EM 78.1, F1 78.2. Ablation: w/o Add/Sub (LARGE) EM 53.8, F1 58.0 (drop ≈22.9 EM, 22.5 F1). Annotation coverage: arithmetic annotations raise labeled examples to 91.7%.",
            "notable_failure_modes": "Incorrect sign assignments producing implausible/large negative answers; inability to reliably reason over expressions with many numbers (performance drops as max M increases); limitations to add/sub only (no multiplication/division implemented).",
            "comparison_to_humans_or_symbolic": "Expressions produced by sign-assignment are executed by a symbolic evaluator; overall model still lags human performance (Human F1 on DROP 95.98 vs MTMSN test F1 79.88). The method is hybrid (neural sign prediction + symbolic execution), not a pure algorithmic solver.",
            "uuid": "e3133.0",
            "source_info": {
                "paper_title": "A Multi-Type Multi-Span Network for Reading Comprehension that Requires Discrete Reasoning",
                "publication_date_yy_mm": "2019-08"
            }
        },
        {
            "name_short": "MTMSN arithmetic reranking",
            "name_full": "MTMSN arithmetic expression reranking mechanism",
            "brief_description": "Candidate arithmetic expressions (from beam search over per-number sign assignments) are re-scored by a neural reranker that composes per-number contextual vectors with learned sign embeddings to incorporate expression-level context before selecting the final expression.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "BERT-based MTMSN (BERT_BASE / BERT_LARGE)",
            "model_description": "Reranker consumes number vectors V_i (from U) and learned sign embeddings C_i (embedding matrix E ∈ R^{3×2×D}), computes an attention-weighted expression vector h^V and applies an FFN together with global vectors (question, passage, CLS) to produce p^arith for each candidate expression.",
            "arithmetic_task_type": "Reranking of candidate addition/subtraction expressions over passage numbers (beam size typically 3); applied to expressions with up to M signed numbers (M default 4).",
            "reported_mechanism": "Expression-level contextual scoring: sum/attention over (number vector + sign embedding) pairs produces a fixed expression representation which is combined with global vectors and an FFN to yield a reranking score.",
            "evidence_for_mechanism": "Ablation: removing reranking reduces performance (MTMSN_LARGE F1 drops by ~1.8%), indicating reranking filters incorrect high-probability sign assignments; beam-size experiments show beam=3 gives best performance, consistent with reranking benefiting from a small candidate set.",
            "evidence_against_mechanism": "Reranking yields modest gains (∼1.8% F1) — indicates context-aware reranking helps but does not eliminate arithmetic errors; reranker effectiveness depends on beam containing correct candidate.",
            "intervention_type": "Neural reranking of beam-searched expression candidates; learned sign embeddings; attention pooling over signed-number tuples.",
            "effect_of_intervention": "Improved final-expression selection: reranking produced a ≈1.8% absolute F1 increase for the LARGE model; prevents selecting obviously-wrong expressions with high independent sign-probability.",
            "performance_metrics": "Reranking adds ≈1.8 percentage points F1 on MTMSN_LARGE (reported as 'w/o Reranking' comparison: MTMSN_LARGE F1 80.54 → without reranking 78.7). Beam size tuned to 3 gives best reranking performance.",
            "notable_failure_modes": "Reranker can only choose among beam candidates — if beam misses correct expression, reranking cannot recover; larger beam sizes can hurt reranking (more candidates to rank, more noise).",
            "comparison_to_humans_or_symbolic": "Reranking still relies on subsequent symbolic execution of selected expression; compared to human-level reasoning, reranking is a limited contextual validation step rather than a general algorithmic corrector.",
            "uuid": "e3133.1",
            "source_info": {
                "paper_title": "A Multi-Type Multi-Span Network for Reading Comprehension that Requires Discrete Reasoning",
                "publication_date_yy_mm": "2019-08"
            }
        },
        {
            "name_short": "MTMSN counting & negation",
            "name_full": "MTMSN counting classifier and negation operation modeling",
            "brief_description": "Counting is modeled as a multi-class classification over small integer classes; logical negation on numbers is modeled as a per-number binary classification (apply 100 - number) to capture questions asking for complements or 'not' calculations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "BERT-based MTMSN (BERT_BASE / BERT_LARGE)",
            "model_description": "Count head: softmax over a small set of count classes (default up to 10) built from an attention summary h^U over number vectors U plus global vectors, followed by an FFN. Negation head: per-number 2-way FFN classifier over u_i concatenated with global vectors (question, passage, CLS).",
            "arithmetic_task_type": "Counting entities (discrete small counts) and logical numeric negation (e.g., 'How many percent were not X?' → 100 - X).",
            "reported_mechanism": "Classification-based approach: counts are predicted via multiclass softmax over pre-defined count range; negation is detected by binary per-number classifier and applied as arithmetic 100 - number.",
            "evidence_for_mechanism": "Ablation: removing Count reduces F1 (~4–5%); removing Negation reduces F1 significantly (~9% on both models), showing these heads materially improve performance. Table 5 reports predicted-type performance: Count EM/F1 70.4; Negation EM/F1 96.3 (MTMSN_LARGE by predicted type).",
            "evidence_against_mechanism": "Negation detection relies on heuristic annotation (they search for 100 - number equals answer) for weak supervision, which may miss more complex negation cases; counts limited to a small class set (10 classes) — may not generalize to larger counts.",
            "intervention_type": "Add explicit answer-type heads for Count and Negation; weakly supervised annotation rules to identify negation instances; direct training with marginal likelihood over candidate annotations.",
            "effect_of_intervention": "Adding negation type yields ~9% F1 improvement; adding count head yields ≈5% F1 improvement. Explicit modeling increases labeled coverage (final coverage 97.9% when negation included).",
            "performance_metrics": "By predicted type (MTMSN_LARGE): Count EM 70.4, F1 70.4; Negation EM 96.3, F1 96.3. Ablation rows: w/o Count (LARGE) F1 75.6 vs full 80.5 (drop ~4.9); w/o Negation (LARGE) F1 70.9 vs full 80.5 (drop ~9.6).",
            "notable_failure_modes": "Negation supervision is heuristic and may miss cases; counting limited to small integer classes (max 10) so higher counts mispredicted; miscounting accounts for ~8% of sampled EM errors.",
            "comparison_to_humans_or_symbolic": "Negation and counting are modeled neurally and yield strong predicted-type performance (negation especially high), but overall numeric reasoning still below human performance; negation handled as a discrete operation (100 - x) rather than general symbolic reasoning.",
            "uuid": "e3133.2",
            "source_info": {
                "paper_title": "A Multi-Type Multi-Span Network for Reading Comprehension that Requires Discrete Reasoning",
                "publication_date_yy_mm": "2019-08"
            }
        },
        {
            "name_short": "Training / limitations",
            "name_full": "Weak supervision, beam search constraints, and limitations in arithmetic handling",
            "brief_description": "Training uses weak supervision (marginal likelihood over candidate annotations) and restricts expression search (addition/subtraction of up to 3 numbers during annotation, max M signed numbers in decoding), which constrains the scope of arithmetic the model learns and exposes failure modes.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "BERT-based MTMSN (BERT_BASE / BERT_LARGE)",
            "model_description": "Weakly supervised training: enumerate possible annotations (matching spans, arithmetic expressions, counts, negation, number of spans); beam search labels candidate expressions (correct/wrong) used to supervise reranker; marginal likelihood objective sums probabilities over all annotations.",
            "arithmetic_task_type": "Addition/subtraction limited during annotation search to combinations of up to 3 numbers; decoding allows M up to default 4 but performance degrades as M increases.",
            "reported_mechanism": "Weak supervision over enumerated arithmetic annotations plus beam search for candidate expression generation; marginal likelihood objective used to train multi-headed predictors.",
            "evidence_for_mechanism": "Annotation statistics: including Add/Sub raises annotated coverage from 56.4% to 91.7%; beam-size experiments and M experiments show trade-offs: beam=3 best, and performance decreases as M increases (most expressions use 2–3 numbers).",
            "evidence_against_mechanism": "Limiting search (≤3 in annotation) and small M prevents learning or evaluating longer/chained operations reliably; error analysis identifies incorrect arithmetic computations (38%) and other failure modes (sorting 18%, miscounting 8%).",
            "intervention_type": "Heuristic search limits during annotation; beam search with tunable beam size; marginal-likelihood training; NMS for multi-span extraction (not arithmetic but part of answer extraction pipeline).",
            "effect_of_intervention": "Heuristic search and limits allowed tractable training and produced strong gains on DROP, but also left a substantial fraction of arithmetic errors; tuning beam and M is necessary to balance recall vs noise (beam=3, M small).",
            "performance_metrics": "Overall MTMSN_LARGE Dev/Test: EM ≈ 76.68 / 75.85, F1 ≈ 80.54 / 79.88. Error analysis of 100 sampled EM errors: 38% incorrect arithmetic computations, 18% require sorting, 8% miscounting. Performance decreases as M (max signed numbers) increases; optimal M small (2–3 typical).",
            "notable_failure_modes": "Search and modeling limited to add/sub and simple negation; complex chained arithmetic, sorting, multiplication/division not handled; many arithmetic errors remain; reranking/beam based methods depend on beam containing correct candidate.",
            "uuid": "e3133.3",
            "source_info": {
                "paper_title": "A Multi-Type Multi-Span Network for Reading Comprehension that Requires Discrete Reasoning",
                "publication_date_yy_mm": "2019-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs",
            "rating": 2
        },
        {
            "paper_title": "Analysing mathematical reasoning abilities of neural models",
            "rating": 2
        },
        {
            "paper_title": "Neural Programmer: Inducing latent programs with gradient descent",
            "rating": 2
        },
        {
            "paper_title": "Neural Turing Machines",
            "rating": 1
        }
    ],
    "cost": 0.0134805,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>A Multi-Type Multi-Span Network for Reading Comprehension that Requires Discrete Reasoning</h1>
<p>Minghao Hu, Yuxing Peng, Zhen Huang, Dongsheng Li<br>National University of Defense Technology, Changsha, China<br>{huminghao09, pengyuxing, huangzhen, dsli}@nudt.edu.cn</p>
<h4>Abstract</h4>
<p>Rapid progress has been made in the field of reading comprehension and question answering, where several systems have achieved human parity in some simplified settings. However, the performance of these models degrades significantly when they are applied to more realistic scenarios, where answers are involved with various types, multiple text strings are correct answers, or discrete reasoning abilities are required. In this paper, we introduce the Multi-Type Multi-Span Network (MTMSN), a neural reading comprehension model that combines a multi-type answer predictor designed to support various answer types (e.g., span, count, negation, and arithmetic expression) with a multi-span extraction method for dynamically producing one or multiple text spans. In addition, an arithmetic expression reranking mechanism is proposed to rank expression candidates for further confirming the prediction. Experiments show that our model achieves 79.9 F1 on the DROP hidden test set, creating new state-of-the-art results. Source code ${ }^{1}$ is released to facilitate future work.</p>
<h2>1 Introduction</h2>
<p>This paper considers the reading comprehension task in which some discrete-reasoning abilities are needed to correctly answer questions. Specifically, we focus on a new reading comprehension dataset called DROP (Dua et al., 2019), which requires Discrete Reasoning Over the content of Paragraphs to obtain the final answer. Unlike previous benchmarks such as CNN/DM (Hermann et al., 2015) and SQuAD (Rajpurkar et al., 2016) that have been well solved (Chen et al., 2016; Devlin et al., 2019), DROP is substantially more challenging in three ways. First, the answers to</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>the questions involve a wide range of types such as numbers, dates, or text strings. Therefore, various kinds of prediction strategies are required to successfully find the answers. Second, rather than restricting the answer to be a span of text, DROP loosens the constraint so that answers may be a set of multiple text strings. Third, for questions that require discrete reasoning, a system must have a more comprehensive understanding of the context and be able to perform numerical operations such as addition, counting, or sorting.</p>
<p>Existing approaches, when applied to this more realistic scenario, have three problems. First, to produce various answer types, Dua et al. (2019) extend previous one-type answer prediction (Seo et al., 2017) to multi-type prediction that supports span extraction, counting, and addition/subtraction. However, they have not fully considered all potential types. Take the question "What percent are not non-families?" and the passage snippet " $39.9 \%$ were non-families" as an example, a negation operation is required to infer the answer. Second, previous reading comprehension models (Wang et al., 2017; Yu et al., 2018; Hu et al., 2018) are designed to produce one single span as the answer. But for some questions such as "Which ancestral groups are smaller than 11\%?", there may exist several spans as correct answers (e.g., "Italian", "English", and "Polish"), which can not be well handled by these works. Third, to support numerical reasoning, prior work (Dua et al., 2019) learns to predict signed numbers for obtaining an arithmetic expression that can be executed by a symbolic system. Nevertheless, the prediction of each signed number is isolated, and the expression's context information has not been considered. As a result, obviously-wrong expressions, such as all predicted signs are either minus or zero, are likely produced.</p>
<p>To address the above issues, we introduce</p>
<p>the Multi-Type Multi-Span Network (MTMSN), a neural reading comprehension model for predicting various types of answers as well as dynamically extracting one or multiple spans. MTMSN utilizes a series of pre-trained Transformer blocks (Devlin et al., 2019) to obtain a deep bidirectional context representation. On top of it, a multi-type answer predictor is proposed to not only support previous prediction strategies such as span, count number, and arithmetic expression, but also add a new type of logical negation. This results in a wider range of coverage of answer types, which turns out to be crucial to performance. Besides, rather than always producing one single span, we present a multi-span extraction method to produce multiple answers. The model first predicts the number of answers, and then extracts non-overlapped spans to the specific amount. In this way, the model can learn to dynamically extract one or multiple spans, thus being beneficial for multi-answer cases. In addition, we propose an arithmetic expression reranking mechanism to rank expression candidates that are decoded by beam search, so that their context information can be considered during reranking to further confirm the prediction.</p>
<p>Our MTMSN model outperforms all existing approaches on the DROP hidden test set by achieving 79.9 F1 score, a $32.9 \%$ absolute gain over prior best work at the time of submission. To make a fair comparison, we also construct a baseline that uses the same BERT-based encoder. Again, MTMSN surpasses it by obtaining a 13.2 F1 increase on the development set. We also provide an in-depth ablation study to show the effectiveness of our proposed methods, analyze performance breakdown by different answer types, and give some qualitative examples as well as error analysis.</p>
<h2>2 Task Description</h2>
<p>In the reading comprehension task that requires discrete reasoning, a passage and a question are given. The goal is to predict an answer to the question by reading and understanding the passage. Unlike previous dataset such as SQuAD (Rajpurkar et al., 2016) where the answer is limited to be a single span of text, DROP loosens the constraint so that the answer involves various types such as number, date, or span of text (Figure 1). Moreover, the answer can be multiple text strings instead of single continuous span $\left(\mathbf{A}_{2}\right)$. To suc-</p>
<p>Passage: As of the census of 2000, there were 218,590 people, 79,667 households, ... $22.5 \%$ were of German people, $13.1 \%$ Irish people, $9.8 \%$ Italian people, ... $\mathbf{Q}<em 1="1">{1}$ : Which group from the census is larger: German or Irish?
$\mathbf{A}</em>$ : German
$\mathbf{Q}<em 3="3">{2}$ : Which ancestral groups are at least $10 \%$ ?
$\mathbf{A}</em>$ : German, Irish
$\mathbf{Q}<em 3="3">{3}$ : How many more people are there than households?
$\mathbf{A}</em>: 138,923$
$\mathbf{Q}<em 4="4">{4}$ : How many percent were not German?
$\mathbf{A}</em>: 77.5$
Figure 1: Question-answer pairs along with a passage from the DROP dataset.
cessfully find the answer, some discrete reasoning abilities, such as sorting $\left(\mathbf{A}<em 3="3">{1}\right)$, subtraction $\left(\mathbf{A}</em>\right)$, are required.}\right)$, and negation $\left(\mathbf{A}_{4</p>
<h2>3 Our Approach</h2>
<p>Figure 2 gives an overview of our model that aims to combine neural reading comprehension with numerical reasoning. Our model uses BERT (Devlin et al., 2019) as encoder: we map word embeddings into contextualized representations using pre-trained Transformer blocks (Vaswani et al., 2017) (§3.1). Based on the representations, we employ a multi-type answer predictor that is able to produce four answer types: (1) span from the text; (2) arithmetic expression; (3) count number; (4) negation on numbers (§3.2). Following Dua et al. (2019), we first predict the answer type of a given passage-question pair, and then adopt individual prediction strategies. To support multispan extraction (§3.3), the model explicitly predicts the number of answer spans. It then outputs non-overlapped spans until the specific amount is reached. Moreover, we do not directly use the arithmetic expression that possesses the maximum probability, but instead re-rank several expression candidates that are decoded by beam search to further confirm the prediction (§3.4). Finally, the model is trained under weakly-supervised signals to maximize the marginal likelihood over all possible annotations (§3.5).</p>
<h3>3.1 BERT-Based Encoder</h3>
<p>To obtain a universal representation for both the question and the passage, we utilize BERT (Devlin et al., 2019), a pre-trained deep bidirectional Transformer model that achieves state-of-the-art performance across various tasks, as the encoder.</p>
<p>Specifically, we first tokenize the question and</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 2: An illustration of MTMSN architecture. The multi-type answer predictor supports four kinds of answer types including span, addition/subtraction, count, and negation. A multi-span extraction method is proposed to dynamically produce one or several spans. The arithmetic expression reranking mechanism aims to rank expression candidates that are decoded by beam search for further validating the prediction.
the passage using the WordPiece vocabulary (Wu et al., 2016), and then generate the input sequence by concatenating a [CLS] token, the tokenized question, a [SEP] token, the tokenized passage, and a final [SEP] token. For each token in the sequence, its input representation is the elementwise addition of WordPiece embeddings, positional embeddings, and segment embeddings (Devlin et al., 2019). As a result, a list of input embeddings $\mathbf{H}<em i="i">{0} \in \mathbb{R}^{T \times D}$ can be obtained, where $D$ is the hidden size and $T$ is the sequence length. A series of $L$ pre-trained Transformer blocks are then used to project the input embeddings into contextualized representations $\mathbf{H}</em>$ as:</p>
<p>$$
\mathbf{H}<em i-1="i-1">{i}=\operatorname{TransformerBlock}\left(\mathbf{H}</em>\right), \forall i \in[1, L]
$$</p>
<p>Here, we omit a detailed introduction of the block architecture and refer readers to Vaswani et al. (2017) for more details.</p>
<h3>3.2 Multi-Type Answer Predictor</h3>
<p>Rather than restricting the answer to always be a span of text, the discrete-reasoning reading comprehension task involves different answer types (e.g., number, date, span of text). Following Dua et al. (2019), we design a multi-type answer predictor to selectively produce different kinds of answers such as span, count number, and arithmetic expression. To further increase answer coverage, we propose adding a new answer type to support logical negation. Moreover, unlike prior work that separately predicts passage spans and question spans, our approach directly extracts spans from the input sequence.</p>
<p>Answer type prediction Inspired by the Augmented QANet model (Dua et al., 2019), we use the contextualized token representations from the last four blocks $\left(\mathbf{H}<em L="L">{L-3}, \ldots, \mathbf{H}</em>}\right)$ as the inputs to our answer predictor, which are denoted as $\mathbf{M<em 1="1">{0}$, $\mathbf{M}</em>}, \mathbf{M<em 3="3">{2}, \mathbf{M}</em>}$, respectively. To predict the answer type, we first split the representation $\mathbf{M<em 2="2">{2}$ into a question representation $\mathbf{Q}</em>}$ and a passage representation $\mathbf{P<em 2="2">{2}$ according to the index of intermediate [SEP] token. Then the model computes two vectors $\mathbf{h}^{\mathbf{Q}</em>$ that summarize the question and passage information respectively:}}$ and $\mathbf{h}^{\mathbf{P}_{2}</p>
<p>$$
\boldsymbol{\alpha}^{Q}=\operatorname{softmax}\left(\mathbf{W}^{Q} \mathbf{Q}<em 2="2">{2}\right), \quad \mathbf{h}^{\mathbf{Q}</em>
$$}}=\boldsymbol{\alpha}^{Q} \mathbf{Q}_{2</p>
<p>where $\mathbf{h}^{\mathbf{P}<em 2="2">{2}}$ is computed in a similar way over $\mathbf{P}</em>$.
Next, we calculate a probability distribution to represent the choices of different answer types as:</p>
<p>$$
\mathbf{p}^{\text {type }}=\operatorname{softmax}\left(\operatorname{FFN}\left(\left[\mathbf{h}^{\mathbf{Q}<em 2="2">{2}} ; \mathbf{h}^{\mathbf{P}</em>\right]\right)\right)
$$}} ; \mathbf{h}^{\mathrm{CLS}</p>
<p>Here, $\mathbf{h}^{\mathrm{CLS}}$ is the first vector in the final contextualized representation $\mathbf{M}_{3}$, and FFN denotes a feed-forward network consisting of two linear projections with a GeLU activation (Hendrycks and Gimpel, 2016) followed by a layer normalization (Lei Ba et al., 2016) in between.</p>
<p>Span To extract the answer either from the passage or from the question, we combine the gating mechanism of Wang et al. (2017) with the standard decoding strategy of Seo et al. (2017) to predict the starting and ending positions across the entire sequence. Specifically, we first compute three vectors, namely $\mathbf{g}^{\mathbf{Q}<em 1="1">{0}}, \mathbf{g}^{\mathbf{Q}</em>$, that summarize}}, \mathbf{g}^{\mathbf{Q}_{2}</p>
<p>the question information among different levels of question representations:</p>
<p>$$
\beta^{Q}=\operatorname{softmax}\left(\operatorname{FFN}\left(\mathbf{Q}<em 2="2">{2}\right), \quad \mathbf{g}^{\mathbf{Q}</em>\right.
$$}}=\beta^{Q} \mathbf{Q}_{2</p>
<p>where $\mathbf{g}^{\mathbf{Q}<em 1="1">{0}}$ and $\mathbf{g}^{\mathbf{Q}</em>}}$ are computed over $\mathbf{Q<em 1="1">{0}$ and $\mathbf{Q}</em>$ respectively, in a similar way as described above.</p>
<p>Then we compute the probabilities of the starting and ending indices of the answer span from the input sequence as:</p>
<p>$$
\begin{aligned}
\tilde{\mathbf{M}}^{\text {start }} &amp; =\left[\mathbf{M}<em 0="0">{2} ; \mathbf{M}</em>} ; \mathbf{g}^{\mathbf{Q<em 2="2">{2}} \otimes \mathbf{M}</em>} ; \mathbf{g}^{\mathbf{Q<em 0="0">{0}} \otimes \mathbf{M}</em>\right] \
\tilde{\mathbf{M}}^{\text {end }} &amp; =\left[\mathbf{M}<em 1="1">{2} ; \mathbf{M}</em>} ; \mathbf{g}^{\mathbf{Q<em 2="2">{2}} \otimes \mathbf{M}</em>} ; \mathbf{g}^{\mathbf{Q<em 1="1">{1}} \otimes \mathbf{M}</em>\right] \
\mathbf{p}^{\text {start }} &amp; =\operatorname{softmax}\left(\mathbf{W}^{S} \tilde{\mathbf{M}}^{\text {start }}\right) \
\mathbf{p}^{\text {end }} &amp; =\operatorname{softmax}\left(\mathbf{W}^{E} \tilde{\mathbf{M}}^{\text {end }}\right)
\end{aligned}
$$</p>
<p>where $\otimes$ denotes the outer product between the vector $\mathbf{g}$ and each token representation in $\mathbf{M}$.</p>
<p>Arithmetic expression In order to model the process of performing addition or subtraction among multiple numbers mentioned in the passage, we assign a three-way categorical variable (plus, minus, or zero) for each number to indicate its sign, similar to Dua et al. (2019). As a result, an arithmetic expression that has a number as the final answer can be obtained and easily evaluated.</p>
<p>Specifically, for each number mentioned in the passage, we gather its corresponding representation from the concatenation of $\mathbf{M}<em 3="3">{2}$ and $\mathbf{M}</em>}$, eventually yielding $\mathbf{U}=\left(\mathbf{u<em N="N">{1}, \ldots, \mathbf{u}</em>$ where $N$ numbers exist. Then the probabilities of the $i$-th number being assigned a plus, minus or zero is computed as:}\right) \in \mathbb{R}^{N \times 2 * D</p>
<p>$$
\mathbf{p}<em i="i">{i}^{\text {sign }}=\operatorname{softmax}\left(\operatorname{FFN}\left(\left[\mathbf{u}</em>} ; \mathbf{h}^{\mathbf{Q<em 2="2">{2}} ; \mathbf{h}^{\mathbf{P}</em>\right]\right)\right)
$$}} ; \mathbf{h}^{\mathrm{CLS}</p>
<p>Count We consider the ability of counting entities and model it as a multi-class classification problem. To achieve this, the model first produces a vector $\mathbf{h}^{\mathbf{U}}$ that summarizes the important information among all mentioned numbers, and then computes a counting probability distribution as:</p>
<p>$$
\begin{aligned}
\boldsymbol{\alpha}^{U} &amp; =\operatorname{softmax}\left(\mathbf{W}^{U} \mathbf{U}\right), \quad \mathbf{h}^{\mathbf{U}}=\boldsymbol{\alpha}^{U} \mathbf{U} \
\mathbf{p}^{\text {count }} &amp; =\operatorname{softmax}\left(\operatorname{FFN}\left(\left[\mathbf{h}^{\mathbf{U}} ; \mathbf{h}^{\mathbf{Q}<em 2="2">{2}} ; \mathbf{h}^{\mathbf{P}</em>\right]\right)\right)
\end{aligned}
$$}} ; \mathbf{h}^{\mathrm{CLS}</p>
<p>Negation One obvious but important linguistic phenomenon that prior work fails to capture is negation. We find there are many cases in DROP that require to perform logical negation on numbers. The question $\left(\mathbf{Q}<em i="i">{4}\right)$ in Figure 1 gives a qualitative example of this phenomenon. To model
this phenomenon, we assign a two-way categorical variable for each number to indicate whether a negation operation should be performed. Then we compute the probabilities of logical negation on the $i$-th number as:
$\mathbf{p}</em>}^{\text {negation }}=\operatorname{softmax}\left(\operatorname{FFN}\left(\left[\mathbf{u<em 2="2">{i} ; \mathbf{h}^{\mathbf{Q}</em>\right]\right)\right)$}} ; \mathbf{h}^{\mathbf{P}_{2}} ; \mathbf{h}^{\mathrm{CLS}</p>
<h3>3.3 Multi-Span Extraction</h3>
<p>Although existing reading comprehension tasks focus exclusively on finding one span of text as the final answer, DROP loosens the restriction so that the answer to the question may be several text spans. Therefore, specific adaption should be made to extend previous single-span extraction to multi-span scenario.</p>
<p>To do this, we propose directly predicting the number of spans and model it as a classification problem. This is achieved by computing a probability distribution on span amount as</p>
<p>$$
\mathbf{p}^{\text {span }}=\operatorname{softmax}\left(\operatorname{FFN}\left(\left[\mathbf{h}^{\mathbf{Q}<em 2="2">{2}} ; \mathbf{h}^{\mathbf{P}</em>\right]\right)\right)
$$}} ; \mathbf{h}^{\mathrm{CLS}</p>
<p>To extract non-overlapped spans to the specific amount, we adopt the non-maximum suppression (NMS) algorithm (Rosenfeld and Thurston, 1971) that is widely used in computer vision for pruning redundant bounding boxes, as shown in Algorithm 1. Concretely, the model first proposes a set of top- $K$ spans $\mathbf{S}$ according to the descending order of the span score, which is computed as $\mathbf{p}<em _mathrm_f="\mathrm{f">{\mathrm{g}}^{\text {start }} \mathbf{p}</em>}}^{\text {end }}$ for the span $(k, l)$. It also predicts the amount of extracted spans $t$ from $\mathbf{p}^{\text {span }}$, and initializes a new set $\hat{\mathbf{S}}$. Next, we add the span $\mathbf{s<em j="j">{i}$ that possesses the maximum span score to the set $\hat{\mathbf{S}}$, and remove it from $\mathbf{S}$. We also delete any remaining span $\mathbf{s}</em>$ reaches $t$.}$ that overlaps with $\mathbf{s}_{i}$, where the degree of overlap is measured using the text-level F1 function. This process is repeated for remaining spans in $\mathbf{S}$, until $\mathbf{S}$ is empty or the size of $\hat{\mathbf{S}</p>
<h3>3.4 Arithmetic Expression Reranking</h3>
<p>As discussed in $\S 3.2$, we model the phenomenon of discrete reasoning on numbers by learning to predict a plus, minus, or zero for each number in the passage. In this way, an arithmetic expression composed of signed numbers can be obtained, where the final answer can be deduced by performing simple arithmetic computation. However, since the sign of each number is only determined by the number representation and some coarsegrained global representations, the context information of the expression itself has not been considered. As a result, the model may predict some</p>
<div class="codehilite"><pre><span></span><code><span class="nt">Algorithm</span><span class="w"> </span><span class="nt">1</span><span class="w"> </span><span class="nt">Multi-span</span><span class="w"> </span><span class="nt">extraction</span>
<span class="nt">Input</span><span class="o">:</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">mathbf</span><span class="p">{</span><span class="err">p</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{start</span><span class="w"> </span><span class="p">}</span><span class="err">}</span><span class="w"> </span><span class="o">;</span><span class="w"> </span><span class="err">\</span><span class="nt">mathbf</span><span class="p">{</span><span class="err">p</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{end</span><span class="w"> </span><span class="p">}</span><span class="err">}</span><span class="w"> </span><span class="o">;</span><span class="w"> </span><span class="err">\</span><span class="nt">mathbf</span><span class="p">{</span><span class="err">p</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{span</span><span class="w"> </span><span class="p">}</span><span class="err">}\</span><span class="o">)</span>
<span class="w">    </span><span class="o">:</span><span class="w"> </span><span class="nt">Generate</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">set</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">mathbf</span><span class="p">{</span><span class="err">S</span><span class="p">}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">by</span><span class="w"> </span><span class="nt">extracting</span><span class="w"> </span><span class="nt">top-</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">K</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">spans</span>
<span class="w">    </span><span class="nt">Sort</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">mathbf</span><span class="p">{</span><span class="err">S</span><span class="p">}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">in</span><span class="w"> </span><span class="nt">descending</span><span class="w"> </span><span class="nt">order</span><span class="w"> </span><span class="nt">of</span><span class="w"> </span><span class="nt">span</span><span class="w"> </span><span class="nt">scores</span>
<span class="w">    </span><span class="err">\</span><span class="o">(</span><span class="nt">t</span><span class="o">=</span><span class="err">\</span><span class="nt">arg</span><span class="w"> </span><span class="err">\</span><span class="nt">max</span><span class="w"> </span><span class="err">\</span><span class="nt">mathbf</span><span class="p">{</span><span class="err">p</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{span</span><span class="w"> </span><span class="p">}</span><span class="err">}</span><span class="o">+</span><span class="nt">1</span><span class="err">\</span><span class="o">)</span>
<span class="w">    </span><span class="nt">Initialize</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">hat</span><span class="p">{</span><span class="err">\mathbf{S</span><span class="p">}</span><span class="err">}</span><span class="o">=</span><span class="err">\</span><span class="p">{</span><span class="w"> </span><span class="err">\</span><span class="p">}</span><span class="err">\</span><span class="o">)</span>
<span class="w">    </span><span class="nt">while</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">mathbf</span><span class="p">{</span><span class="err">S</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">neq</span><span class="err">\</span><span class="p">{</span><span class="w"> </span><span class="err">\</span><span class="p">}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">and</span><span class="w"> </span><span class="err">\</span><span class="o">(|</span><span class="err">\</span><span class="nt">hat</span><span class="p">{</span><span class="err">\mathbf{S</span><span class="p">}</span><span class="err">}</span><span class="o">|&lt;</span><span class="nt">t</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">do</span>
<span class="w">        </span><span class="nt">for</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">mathbf</span><span class="p">{</span><span class="err">s</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">in</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">mathbf</span><span class="p">{</span><span class="err">S</span><span class="p">}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">do</span>
<span class="w">            </span><span class="nt">Add</span><span class="w"> </span><span class="nt">span</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">mathbf</span><span class="p">{</span><span class="err">s</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">to</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">hat</span><span class="p">{</span><span class="err">\mathbf{S</span><span class="p">}</span><span class="err">}\</span><span class="o">)</span>
<span class="w">            </span><span class="nt">Remove</span><span class="w"> </span><span class="nt">span</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">mathbf</span><span class="p">{</span><span class="err">s</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">from</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">mathbf</span><span class="p">{</span><span class="err">S</span><span class="p">}</span><span class="err">\</span><span class="o">)</span>
<span class="w">            </span><span class="nt">for</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">mathbf</span><span class="p">{</span><span class="err">s</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">j</span><span class="p">}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">in</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">mathbf</span><span class="p">{</span><span class="err">S</span><span class="p">}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">do</span>
<span class="w">            </span><span class="nt">if</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">f</span><span class="w"> </span><span class="nt">1</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="err">\</span><span class="nt">mathbf</span><span class="p">{</span><span class="err">s</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="err">\</span><span class="nt">mathbf</span><span class="p">{</span><span class="err">s</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">j</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="o">)&gt;</span><span class="nt">0</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">then</span>
<span class="w">                </span><span class="nt">Remove</span><span class="w"> </span><span class="nt">span</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">mathbf</span><span class="p">{</span><span class="err">s</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">j</span><span class="p">}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">from</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">mathbf</span><span class="p">{</span><span class="err">S</span><span class="p">}</span><span class="err">\</span><span class="o">)</span>
<span class="w">    </span><span class="nt">return</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">hat</span><span class="p">{</span><span class="err">\mathbf{S</span><span class="p">}</span><span class="err">}\</span><span class="o">)</span>
</code></pre></div>

<p>obviously wrong expressions (e.g., the signs that have maximum probabilities are either minus or zero, resulting in a large negative value). Therefore, in order to further validate the prediction, it is necessary to rank several highly confident expression candidates using the representation summarized from the expression's context.</p>
<p>Specifically, we use beam search to produce top-ranked arithmetic expressions, which are sent back to the network for reranking. Since each expression consists of several signed numbers, we construct an expression representation by taking both the numbers and the signs into account. For each number in the expression, we gather its corresponding vector from the representation $\mathbf{U}$. As for the signs, we initialize an embedding matrix $\mathbf{E} \in \mathbb{R}^{3 \times 2 \times D}$, and find the sign embeddings for each signed number. In this way, given the $i$-th expression that contains $M$ signed numbers at most, we can obtain number vectors $\mathbf{V}<em i="i">{i} \in \mathbb{R}^{M \times 2 \times D}$ as well as sign embeddings $\mathbf{C}</em>$. Then the expression representation along with the reranking probability can be calculated as:} \in \mathbb{R}^{M \times 2 \times D</p>
<p>$$
\begin{aligned}
&amp; \boldsymbol{\alpha}<em i="i">{i}^{V}=\operatorname{softmax}\left(\mathbf{W}^{V}\left(\mathbf{V}</em>}+\mathbf{C<em i="i">{i}\right)\right) \
&amp; \mathbf{h}</em>}^{\mathbf{V}}=\boldsymbol{\alpha<em i="i">{i}^{V}\left(\mathbf{V}</em>}+\mathbf{C<em i="i">{i}\right) \
&amp; \mathbf{p}</em>}^{\text {arith }}=\operatorname{softmax}\left(\operatorname{FFN}\left(\left[\mathbf{h<em 2="2">{i}^{\mathbf{V}} ; \mathbf{h}^{\mathbf{Q}</em>\right]\right)\right)
\end{aligned}
$$}} ; \mathbf{h}^{\mathbf{P}_{2}} ; \mathbf{h}^{\mathrm{CLS}</p>
<h3>3.5 Training and Inference</h3>
<p>Since DROP does not indicate the answer type but only provides the answer string, we therefore adopt the weakly supervised annotation scheme, as suggested in Berant et al. (2013); Dua et al. (2019). We find all possible annotations that point to the gold answer, including matching spans, arithmetic expressions, correct count numbers, negation operations, and the number of spans. We use simple rules to search over all mentioned numbers to find potential negations. That is, if 100
minus a number is equal to the answer, then a negation occurs on this number. Besides, we only search the addition/subtraction of three numbers at most due to the exponential search space.</p>
<p>To train our model, we propose using a twostep training method composed of an inference step and a training step. In the first step, we use the model to predict the probabilities of sign assignments for numbers. If there exists any annotation of arithmetic expressions, we run beam search to produce expression candidates and label them as either correct or wrong, which are later used for supervising the reranking component. In the second step, we adopt the marginal likelihood objective function (Clark and Gardner, 2018), which sums over the probabilities of all possible annotations including the above labeled expressions. Notice that there are two objective functions for the multi-span component: one is a distantly-supervised loss that maximizes the probabilities of all matching spans, and the other is a classification loss that maximizes the probability on span amount.</p>
<p>At test time, the model first chooses the answer type and then performs specific prediction strategies. For the span type, we use Algorithm 1 for decoding. If the type is addition/subtraction, arithmetic expression candidates will be proposed and further reranked. The expression with the maximum product of cumulative sign probability and reranking probability is chosen. As for the counting type, we choose the number that has the maximum counting probability. Finally, if the type is negation, we find the number that possesses the largest negation probability, and then output the answer as 100 minus this number.</p>
<h2>4 Experiments</h2>
<h3>4.1 Implementation Details</h3>
<p>Dataset We consider the reading comprehension benchmark that requires Discrete Reasoning Over Paragraphs (DROP) (Dua et al., 2019) to train and evaluate our model. DROP contains crowdsourced, adversarially-created, 96.6 K questionanswer pairs, with 77.4 K for training, 9.5 K for validation, and another 9.6 K hidden examples for testing. Passages are extracted from Wikipedia articles and the answer to each question involves various types such as number, date, or text string. Some answers may even be a set of multiple spans of text in the passage. To find the answers, a com-</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Dev</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Test</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">F1</td>
</tr>
<tr>
<td style="text-align: left;">Heuristic Baseline (Dua et al., 2019)</td>
<td style="text-align: center;">4.28</td>
<td style="text-align: center;">8.07</td>
<td style="text-align: center;">4.18</td>
<td style="text-align: center;">8.59</td>
</tr>
<tr>
<td style="text-align: left;">Semantic Role Labeling (Carreras and Màrquez, 2004)</td>
<td style="text-align: center;">11.03</td>
<td style="text-align: center;">13.67</td>
<td style="text-align: center;">10.87</td>
<td style="text-align: center;">13.35</td>
</tr>
<tr>
<td style="text-align: left;">BiDAF (Seo et al., 2017)</td>
<td style="text-align: center;">26.06</td>
<td style="text-align: center;">28.85</td>
<td style="text-align: center;">24.75</td>
<td style="text-align: center;">27.49</td>
</tr>
<tr>
<td style="text-align: left;">QANet+ELMo (Yu et al., 2018)</td>
<td style="text-align: center;">27.71</td>
<td style="text-align: center;">30.33</td>
<td style="text-align: center;">27.08</td>
<td style="text-align: center;">29.67</td>
</tr>
<tr>
<td style="text-align: left;">BERT $_{\text {BASE }}$ (Devlin et al., 2019)</td>
<td style="text-align: center;">30.10</td>
<td style="text-align: center;">33.36</td>
<td style="text-align: center;">29.45</td>
<td style="text-align: center;">32.70</td>
</tr>
<tr>
<td style="text-align: left;">NAQANet (Dua et al., 2019)</td>
<td style="text-align: center;">46.20</td>
<td style="text-align: center;">49.24</td>
<td style="text-align: center;">44.07</td>
<td style="text-align: center;">47.01</td>
</tr>
<tr>
<td style="text-align: left;">NABERT $_{\text {BASE }}$</td>
<td style="text-align: center;">55.82</td>
<td style="text-align: center;">58.75</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">NABERT $_{\text {LARGE }}$</td>
<td style="text-align: center;">64.61</td>
<td style="text-align: center;">67.35</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">MTMSN $_{\text {BASE }}$</td>
<td style="text-align: center;">68.17</td>
<td style="text-align: center;">72.81</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">MTMSN $_{\text {LARGE }}$</td>
<td style="text-align: center;">$\mathbf{7 6 . 6 8}$</td>
<td style="text-align: center;">$\mathbf{8 0 . 5 4}$</td>
<td style="text-align: center;">$\mathbf{7 5 . 8 5}$</td>
<td style="text-align: center;">$\mathbf{7 9 . 8 8}$</td>
</tr>
<tr>
<td style="text-align: left;">Human Performance (Dua et al., 2019)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">92.38</td>
<td style="text-align: center;">95.98</td>
</tr>
</tbody>
</table>
<p>Table 1: The performance of MTMSN and other competing approaches on DROP dev and test set.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">BASE</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">LARGE</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">F1</td>
</tr>
<tr>
<td style="text-align: left;">MTMSN</td>
<td style="text-align: center;">68.2</td>
<td style="text-align: center;">72.8</td>
<td style="text-align: center;">76.7</td>
<td style="text-align: center;">80.5</td>
</tr>
<tr>
<td style="text-align: left;">w/o Add/Sub</td>
<td style="text-align: center;">46.7</td>
<td style="text-align: center;">51.3</td>
<td style="text-align: center;">53.8</td>
<td style="text-align: center;">58.0</td>
</tr>
<tr>
<td style="text-align: left;">w/o Count</td>
<td style="text-align: center;">62.5</td>
<td style="text-align: center;">66.4</td>
<td style="text-align: center;">71.8</td>
<td style="text-align: center;">75.6</td>
</tr>
<tr>
<td style="text-align: left;">w/o Negation</td>
<td style="text-align: center;">59.4</td>
<td style="text-align: center;">63.6</td>
<td style="text-align: center;">67.2</td>
<td style="text-align: center;">70.9</td>
</tr>
<tr>
<td style="text-align: left;">w/o Multi-Span</td>
<td style="text-align: center;">67.5</td>
<td style="text-align: center;">70.7</td>
<td style="text-align: center;">75.6</td>
<td style="text-align: center;">78.4</td>
</tr>
<tr>
<td style="text-align: left;">w/o Reranking</td>
<td style="text-align: center;">66.9</td>
<td style="text-align: center;">71.2</td>
<td style="text-align: center;">74.9</td>
<td style="text-align: center;">78.7</td>
</tr>
</tbody>
</table>
<p>Table 2: Ablation tests of base and large models on the DROP dev set.
prehensive understanding of the context as well as the ability of numerical reasoning are required.</p>
<p>Model settings We build our model upon two publicly available uncased versions of BERT: BERT $<em _LARGE="{LARGE" _text="\text">{\text {BASE }}$ and BERT $</em>$, and refer readers to Devlin et al. (2019) for details on model sizes. We use Adam optimizer with a learning rate of 3e5 and warmup over the first $5 \%$ steps to train. The maximum number of epochs is set to 10 for base models and 5 for large models, while the batch size is 12 or 24 respectively. A dropout probability of 0.1 is used unless stated otherwise. The number of counting class is set to 10 , and the maximum number of spans is 8 . The beam size is 3 by default, while the maximum amount of signed numbers $M$ is set to 4 . All texts are tokenized using Word-}}{ }^{2</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>| Model | EM | F1 |
| :-- | :--: | :--: |
| MTMSN | 76.7 | 80.5 |
| w/o Q/P Vectors | 75.1 | 79.2 |
| w/o CLS Vector | 74.0 | 78.4 |
| Q/P Vectors Using Last Hidden | 76.5 | 80.2 |
| w/o Gated Span Prediction | 75.8 | 79.7 |
| Combine Add/Sub with Negation | 75.5 | 79.4 |</p>
<p>Table 3: Ablation tests of different architecture choices using MTMSN $_{\text {LARGE }}$.</p>
<p>Piece vocabulary (Wu et al., 2016), and truncated to sequences no longer than 512 tokens.</p>
<p>Baselines Following the implementation of Augmented QANet (NAQANet) (Dua et al., 2019), we introduce a similar baseline called Augmented BERT (NABERT). The main difference is that we replace the encoder of QANet (Yu et al., 2018) with the pre-trained Transformer blocks (Devlin et al., 2019). Moreover, it also supports the prediction of various answer types such as span, arithmetic expression, and count number.</p>
<h3>4.2 Main Results</h3>
<p>Two metrics, namely Exact Match (EM) and F1 score, are utilized to evaluate models. We use the official script to compute these scores. Since the test set is hidden, we only submit the best single model to obtain test results.</p>
<p>Table 1 shows the performance of our model and other competitive approaches on the develop-</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Type</th>
<th style="text-align: right;">$(\%)$</th>
<th style="text-align: right;">NABERT</th>
<th style="text-align: right;"></th>
<th style="text-align: right;">MTMSN</th>
<th style="text-align: right;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;">EM</td>
<td style="text-align: right;">F1</td>
<td style="text-align: right;">EM</td>
<td style="text-align: right;">F1</td>
</tr>
<tr>
<td style="text-align: left;">Date</td>
<td style="text-align: right;">1.6</td>
<td style="text-align: right;">55.7</td>
<td style="text-align: right;">60.8</td>
<td style="text-align: right;">55.7</td>
<td style="text-align: right;">69.0</td>
</tr>
<tr>
<td style="text-align: left;">Number</td>
<td style="text-align: right;">61.9</td>
<td style="text-align: right;">63.8</td>
<td style="text-align: right;">64.0</td>
<td style="text-align: right;">80.9</td>
<td style="text-align: right;">81.1</td>
</tr>
<tr>
<td style="text-align: left;">Single Span</td>
<td style="text-align: right;">31.7</td>
<td style="text-align: right;">75.9</td>
<td style="text-align: right;">80.6</td>
<td style="text-align: right;">77.5</td>
<td style="text-align: right;">82.8</td>
</tr>
<tr>
<td style="text-align: left;">Multi Span</td>
<td style="text-align: right;">4.8</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">22.7</td>
<td style="text-align: right;">25.1</td>
<td style="text-align: right;">62.8</td>
</tr>
</tbody>
</table>
<p>Table 4: Performance breakdown of $\mathrm{NABERT}<em _LARGE="{LARGE" _text="\text">{\text {LARGE }}$ and $\mathrm{MTMSN}</em>$ by gold answer types.
ment and test sets. MTMSN outperforms all existing approaches by a large margin, and creates new state-of-the-art results by achieving an EM score of 75.85 and a F1 score of 79.88 on the test set. Since our best model utilizes BERT $}<em _LARGE="{LARGE" _text="\text">{\text {LARGE }}$ as encoder, we therefore compare MTMSN $</em>$ baseline. As we can see, our model obtains 12.07/13.19 absolute gain of EM/F1 over the baseline, demonstrating the effectiveness of our approach. However, as the human achieves 95.98 F1 on the test set, our results suggest that there is still room for improvement.}}$ with the NABERT $_{\text {LARGE }</p>
<h3>4.3 Ablation Study</h3>
<p>Component ablation To analyze the effect of the proposed components, we conduct ablation studies on the development set. As illustrated in Table 2, the use of addition and subtraction is extremely crucial: the EM/F1 performance of both the base and large models drop drastically by more than 20 points if it is removed. Predicting count numbers is also an important component that contributes nearly $5 \%$ gain on both metrics. Moreover, enhancing the model with the negation type significantly increases the F1 by roughly 9 percent on both models. In brief, the above results show that multi-type answer prediction is vitally important for handling different forms of answers, especially in cases where discrete reasoning abilities are required.</p>
<p>We also report the performance after removing the multi-span extraction method. The results reveal that it has a more negative impact on the F1 score. We interpret this phenomenon as follows: producing multiple spans that are partially matched with ground-truth answers is much easier than generating an exactly-matched set of multiple answers. Hence for multi-span scenarios, the gain of our method on F1 is relatively easier to obtain than the one on EM. Finally, to ablate arithmetic expression reranking, we simply use the arithmetic expression that has the maximum cumulative sign</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Type</th>
<th style="text-align: center;">NABERT</th>
<th style="text-align: right;"></th>
<th style="text-align: right;"></th>
<th style="text-align: right;">MTMSN</th>
<th style="text-align: right;"></th>
<th style="text-align: right;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">$(\%)$</td>
<td style="text-align: right;">EM</td>
<td style="text-align: right;">F1</td>
<td style="text-align: right;">$(\%)$</td>
<td style="text-align: right;">EM</td>
<td style="text-align: right;">F1</td>
</tr>
<tr>
<td style="text-align: left;">Span</td>
<td style="text-align: center;">43.0</td>
<td style="text-align: right;">67.9</td>
<td style="text-align: right;">74.2</td>
<td style="text-align: right;">42.7</td>
<td style="text-align: right;">72.2</td>
<td style="text-align: right;">81.0</td>
</tr>
<tr>
<td style="text-align: left;">Add/Sub</td>
<td style="text-align: center;">43.6</td>
<td style="text-align: right;">62.0</td>
<td style="text-align: right;">62.1</td>
<td style="text-align: right;">32.4</td>
<td style="text-align: right;">78.1</td>
<td style="text-align: right;">78.2</td>
</tr>
<tr>
<td style="text-align: left;">Count</td>
<td style="text-align: center;">13.4</td>
<td style="text-align: right;">62.4</td>
<td style="text-align: right;">62.4</td>
<td style="text-align: right;">13.4</td>
<td style="text-align: right;">70.4</td>
<td style="text-align: right;">70.4</td>
</tr>
<tr>
<td style="text-align: left;">Negation</td>
<td style="text-align: center;">0</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">11.5</td>
<td style="text-align: right;">96.3</td>
<td style="text-align: right;">96.3</td>
</tr>
</tbody>
</table>
<p>Table 5: Performance breakdown of NABERT $<em _LARGE="{LARGE" _text="\text">{\text {LARGE }}$ and MTMSN $</em>$ by predicted answer types.
probability instead. We find that our reranking mechanism gives $1.8 \%$ gain on both metrics for the large model. This confirms that validating expression candidates with their context information is beneficial for filtering out highly-confident but wrong predictions.}</p>
<p>Architecture ablation We further conduct a detailed ablation in Table 3 to evaluate our architecture designs. First, we investigate the effects of some "global vectors" used in our model. Specifically, we find that removing the question and passage vectors from all involved computation leads to $1.3 \%$ drop on F1. Ablating the representation of [CLS] token leads to even worse results. We also try to use the last hidden representation (denoted as $\mathrm{M}_{3}$ ) to calculate question and passage vectors, but find that does not work. Next, we remove the gating mechanism used during span prediction, and observe a nearly $0.8 \%$ decline on both metrics. Finally, we share parameters between the arithmetic expression component and the negation component, and find the performance drops by $1.1 \%$ on F1.</p>
<h3>4.4 Analysis and Discussion</h3>
<p>Performance breakdown We now provide a quantitative analysis by showing performance breakdown on the development set. Table 4 shows that our gains mainly come from the most frequent number type, which requires various types of symbolic, discrete reasoning operations. Moreover, significant improvements are also obtained in the multi-span category, where the F1 score increases by more than 40 points. This result further proves the validity of our multi-span extraction method.</p>
<p>We also give the performance statistics that are categorized according to the predicted answer types in Table 5. As shown in the Table, the main improvements are due to the addition/subtraction and negation types. We conjecture that there are two reasons for these improvements. First, our</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: EM/F1 scores of MTMSN ${ }_{\text {LARGE }}$ with different maximum numbers of spans.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: EM/F1 scores of MTMSN ${ }_{\text {LARGE }}$ with different beam sizes and amounts of signed numbers $(M)$.
proposed expression reranking mechanism helps validate candidate expressions. Second, a new inductive bias that enables the model to perform logical negation has been introduced. The impressive performance on the negation type confirms our judgement, and suggests that the model is able to find most of negation operations. In addition, we also observe promising gains brought by the span and count types. We think the gains are mainly due to the multi-span extraction method as well as architecture designs.</p>
<p>Effect of maximum number of spans To investigate the effect of maximum number of spans on multi-span extraction, we conduct an experiment on the dev set and show the curves in Figure 3. We vary the value from 2 to 12 , increased by 2 , and also include the extreme value 1 . According to the Figure, the best results are obtained at 8. A higher value could potentially increase the answer recall but damage the precision by making more predictions, and a smaller value may force the model to produce limited number of answers, resulting in high precision but low recall. Therefore, a value of 8 turns out to be a good trade-off between recall and precision. Moreover, when the value decreases to 1 , the multi-span extraction degrades to previous single-span scenario, and the performance drops significantly.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Configuration</th>
<th style="text-align: center;">Skipped</th>
<th style="text-align: center;">Kept</th>
<th style="text-align: center;">Ratio (\%)</th>
<th style="text-align: center;">F1</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Span</td>
<td style="text-align: center;">33752</td>
<td style="text-align: center;">43657</td>
<td style="text-align: center;">56.4</td>
<td style="text-align: center;">38.9</td>
</tr>
<tr>
<td style="text-align: left;">$+\boldsymbol{\Delta}$</td>
<td style="text-align: center;">6384</td>
<td style="text-align: center;">71025</td>
<td style="text-align: center;">91.7</td>
<td style="text-align: center;">59.2</td>
</tr>
<tr>
<td style="text-align: left;">$+\boldsymbol{\Delta}+\boldsymbol{\Delta}$</td>
<td style="text-align: center;">4282</td>
<td style="text-align: center;">73127</td>
<td style="text-align: center;">94.4</td>
<td style="text-align: center;">63.6</td>
</tr>
<tr>
<td style="text-align: left;">$+\boldsymbol{\Delta}+\boldsymbol{\Delta}+\odot$</td>
<td style="text-align: center;">1595</td>
<td style="text-align: center;">75814</td>
<td style="text-align: center;">97.9</td>
<td style="text-align: center;">72.8</td>
</tr>
</tbody>
</table>
<p>Table 6: Annotation statistics under different combinations of answer types in the DROP train set. "Kept" and "Skipped" mean the number of examples with or without annotation, respectively. $\boldsymbol{\Delta}$ refers to Add/Sub, $\boldsymbol{\Delta}$ denotes Count, and $\odot$ indicates Negation. F1 scores are benchmarked using MTMSN ${ }_{\text {BASE }}$ on the dev set.</p>
<p>Effect of beam size and $M$ We further investigate the effect of beam size and maximum amount of signed numbers in Figure 4. As we can see, a beam size of 3 leads to the best performance, likely because a larger beam size might confuse the model as too many candidates are ranked, on the other hand, a small size could be not sufficient to cover the correct expression. In addition, we find that the performance constantly decreases as the maximum threshold $M$ increases, suggesting that most of expressions only contain two or three signed numbers, and setting a larger threshold could bring in additional distractions.</p>
<p>Annotation statistics We list the annotation statistics on the DROP train set in Table 6. As we can see, only annotating matching spans results in a labeled ratio of $56.4 \%$, indicating that DROP includes various answer types beyond text spans. By further considering the arithmetic expression, the ratio increase sharply to $91.7 \%$, suggesting more than $35 \%$ answers need to be inferred with numeral reasoning. Continuing adding counting leads to a percentage of $94.4 \%$, and a final $97.9 \%$ coverage is achieved by additionally taking negation into account. More importantly, the F1 score constantly increases as more answer types are considered. This result is consistent with our observations in ablation study.</p>
<p>Error analysis Finally, to better understand the remaining challenges, we randomly sample 100 incorrectly predicted examples based on EM and categorize them into 7 classes. $38 \%$ of errors are incorrect arithmetic computations, $18 \%$ require sorting over multiple entities, $13 \%$ are due to mistakes on multi-span extraction, $10 \%$ are singlespan extraction problems, $8 \%$ involve miscounting, another $8 \%$ are wrong predictions on span number, the rest (5\%) are due to various reasons</p>
<p>such as incorrect preprocessing, negation error, and so on. See Appendix for some examples of the above error cases.</p>
<h2>5 Related Work</h2>
<p>Reading comprehension benchmarks Promising advancements have been made for reading comprehension due to the creation of many large datasets. While early research used cloze-style tests (Hermann et al., 2015; Hill et al., 2016), most of recent works (Rajpurkar et al., 2016; Joshi et al., 2017) are designed to extract answers from the passage. Despite their success, these datasets only require shallow pattern matching and simple logical reasoning, thus being well solved (Chen et al., 2016; Devlin et al., 2019). Recently, Dua et al. (2019) released a new benchmark named DROP that demands discrete reasoning as well as deeper paragraph understanding to find the answers. Saxton et al. (2019) introduced a dataset consisting of different types of mathematics problems to focuses on mathematical computation. We choose to work on DROP to test both the numerical reasoning and linguistic comprehension abilities.</p>
<p>Neural reading models Previous neural reading models, such as BiDAF (Seo et al., 2017), R-Net (Wang et al., 2017), QANet (Yu et al., 2018), Reinforced Mreader (Hu et al., 2018), are usually designed to extract a continuous span of text as the answer. Dua et al. (2019) enhanced prior single-type prediction to support various answer types such as span, count number, and addition/subtraction. Different from these approaches, our model additionally supports a new negation type to increase answer coverage, and learns to dynamically extract one or multiple spans. Morevoer, answer reranking has been well studied in several prior works (Cui et al., 2016; Wang et al., 2018a,b,c; Hu et al., 2019). We follow this line of work, but propose ranking arithmetic expressions instead of candidate answers.</p>
<p>End-to-end symbolic reasoning Combining neural methods with symbolic reasoning was considered by Graves et al. (2014); Sukhbaatar et al. (2015), where neural networks augmented with external memory are trained to execute simple programs. Later works on program induction (Reed and De Freitas, 2016; Neelakantan et al., 2016; Liang et al., 2017) extended this idea by using several built-in logic operations along with a key-
value memory to learn different types of compositional programs such as addition or sorting. In contrast to these works, MTMSN does not model various types of reasoning with a universal memory mechanism but instead deals each type with individual predicting strategies.</p>
<p>Visual question answering In computer vision community, the most similar work to our approach is Neural Module Networks (Andreas et al., 2016b), where a dependency parser is used to lay out a neural network composed of several pre-defined modules. Later, Andreas et al. (2016a) proposed dynamically choosing an optimal layout structure from a list of layout candidates that are produced by off-the-shelf parsers. Hu et al. (2017) introduced an end-to-end module network that learns to predict instance-specific network layouts without the aid of a parser. Compared to these approaches, MTMSN has a static network layout that can not be changed during training and evaluation, where pre-defined "modules" are used to handle different types of answers.</p>
<h2>6 Conclusion</h2>
<p>We introduce MTMSN, a multi-type multi-span network for reading comprehension that requires discrete reasoning over the content of paragraphs. We enhance a multi-type answer predictor to support logical negation, propose a multi-span extraction method for producing multiple answers, and design an arithmetic expression reranking mechanism to further confirm the prediction. Our model achieves 79.9 F1 on the DROP hidden test set, creating new state-of-the-art results. As future work, we would like to consider handling additional types such as sorting or multiplication/division. We also plan to explore more advanced methods for performing complex numerical reasoning.</p>
<h2>Acknowledgments</h2>
<p>We would like to thank the anonymous reviewers for their thoughtful comments and insightful feedback. This work was supported by the National Key Research and Development Program of China (2016YFB100101).</p>
<h2>References</h2>
<p>Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. 2016a. Learning to compose neural net-</p>
<p>works for question answering. In Proceedings of NAACL.</p>
<p>Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. 2016b. Neural module networks. In Proceedings of CVPR.</p>
<p>Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic parsing on freebase from question-answer pairs. In Proceedings of EMNLP.</p>
<p>Xavier Carreras and Lluís Márquez. 2004. Introduction to the conll-2004 shared task: Semantic role labeling. In Proceedings of CONLL.</p>
<p>Danqi Chen, Jason Bolton, and Christopher D Manning. 2016. A thorough examination of the cnn/daily mail reading comprehension task. arXiv preprint arXiv:1606.02858.</p>
<p>Christopher Clark and Matt Gardner. 2018. Simple and effective multi-paragraph reading comprehension. In Proceedings of ACL.</p>
<p>Yiming Cui, Zhipeng Chen, Si Wei, Shijin Wang, Ting Liu, and Guoping Hu. 2016. Attention-overattention neural networks for reading comprehension. arXiv preprint arXiv:1607.04423.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of NAACL.</p>
<p>Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. 2019. Drop: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In Proceedings of NAACL.</p>
<p>Alex Graves, Greg Wayne, and Ivo Danihelka. 2014. Neural turing machines. arXiv preprint arXiv:1410.5401.</p>
<p>Dan Hendrycks and Kevin Gimpel. 2016. Bridging nonlinearities and stochastic regularizers with gaussian error linear units. CoRR, abs/1606.08415.</p>
<p>Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 2015. Teaching machines to read and comprehend. In Proceedings of NIPS.</p>
<p>Felix Hill, Antoine Bordes, Sumit Chopra, and Jason Weston. 2016. The goldilocks principle: Reading childrens books with explicit memory representations. In Proceedings of ICLR.</p>
<p>Minghao Hu, Yuxing Peng, Zhen Huang, and Dongsheng Li. 2019. Retrieve, read, rerank: Towards end-to-end multi-document reading comprehension. In Proceedings of ACL.</p>
<p>Minghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu, Furu Wei, and Ming Zhou. 2018. Reinforced mnemonic reader for machine reading comprehension. In Proceedings of IJCAI.</p>
<p>Ronghang Hu, Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Kate Saenko. 2017. Learning to reason: End-to-end module networks for visual question answering. In Proceedings of ICCV.</p>
<p>Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of ACL.</p>
<p>Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. 2016. Layer normalization. arXiv preprint arXiv:1607.06450.</p>
<p>Chen Liang, Jonathan Berant, Quoc Le, Kenneth D Forbus, and Ni Lao. 2017. Neural symbolic machines: Learning semantic parsers on freebase with weak supervision. In Proceedings of ACL.</p>
<p>Arvind Neelakantan, Quoc V Le, and Ilya Sutskever. 2016. Neural programmer: Inducing latent programs with gradient descent. In Proceedings of ICLR.</p>
<p>Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text. In Proceedings of EMNLP.</p>
<p>Scott Reed and Nando De Freitas. 2016. Neural programmer-interpreters. In Proceedings of ICLR.</p>
<p>Azriel Rosenfeld and Mark Thurston. 1971. Edge and curve detection for visual scene analysis. IEEE Transactions on computers, (5):562-569.</p>
<p>David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. 2019. Analysing mathematical reasoning abilities of neural models. In Proceedings of ICLR.</p>
<p>Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. 2017. Bidirectional attention flow for machine comprehension. In Proceedings of ICLR.</p>
<p>Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al. 2015. End-to-end memory networks. In Proceedings of NIPS.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Proceedings of NIPS.</p>
<p>Shuohang Wang, Mo Yu, Jing Jiang, Wei Zhang, Xiaoxiao Guo, Shiyu Chang, Zhiguo Wang, Tim Klinger, Gerald Tesauro, and Murray Campbell. 2018a. Evidence aggregation for answer re-ranking in open-domain question answering. In Proceedings of ICLR.</p>
<p>Wenhui Wang, Nan Yang, Furu Wei, Baobao Chang, and Ming Zhou. 2017. Gated self-matching networks for reading comprehension and question answering. In Proceedings of ACL.</p>
<p>Yizhong Wang, Kai Liu, Jing Liu, Wei He, Yajuan Lyu, Hua Wu, Sujian Li, and Haifeng Wang. 2018b. Multi-passage machine reading comprehension with cross-passage answer verification. In Proceedings of $A C L$.</p>
<p>Zhen Wang, Jiachen Liu, Xinyan Xiao, Yajuan Lyu, and Tian Wu. 2018c. Joint training of candidate extraction and answer selection for reading comprehension. In Proceedings of ACL.</p>
<p>Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. 2016. Google's neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144.</p>
<p>Adams Wei Yu, David Dohan, Quoc Le, Thang Luong, Rui Zhao, and Kai Chen. 2018. Fast and accurate reading comprehension by combining self-attention and convolution. In Proceedings of ICLR.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2} \mathrm{BERT}<em _LARGE="{LARGE" _text="\text">{\text {BASE }}$ is the original version while $\mathrm{BERT}</em>$ is the model augmented with n-gram masking and synthetic self-training: https://github.com/ google-research/bert.&#160;}<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2739 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2739</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2739</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-70.html">extraction-schema-70</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <p><strong>Paper ID:</strong> paper-256827635</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2302.05507v2.pdf" target="_blank">Language Decision Transformers with Exponential Tilt for Interactive Text Environments</a></p>
                <p><strong>Paper Abstract:</strong> Text-based game environments are challenging because agents must deal with long sequences of text, execute compositional actions using text and learn from sparse rewards. We address these challenges by proposing Language Decision Transformers (LDTs), a framework that is based on transformer language models and decision transformers (DTs). Our LDTs extend DTs with 3 components: (1) exponential tilt to guide the agent towards high obtainable goals, (2) novel goal conditioning methods yielding better results than the traditional return-to-go (sum of all future rewards), and (3) a model of future observations that improves agent performance. LDTs are the first to address offline RL with DTs on these challenging games. Our experiments show that LDTs achieve the highest scores among many different types of agents on some of the most challenging Jericho games, such as Enchanter.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2739.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2739.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LDT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Language Decision Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An offline RL agent built by fine-tuning a pre-trained LongT5 encoder-decoder as a Decision Transformer to predict a trajectory goal-condition, the next action, and the next observation; uses exponential tilting to sample high-probability high-return goal conditions and an auxiliary next-observation loss (MB-RCP) to improve performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Language Decision Transformer (LDT)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Fine-tuned LongT5-base encoder-decoder adapted to Decision Transformer formulation that models sequences of {observation, goal-condition, action} and decodes predicted goal-condition, action, and next observation; uses exponential tilt to bias sampled goal-conditions and an auxiliary cross-entropy loss to predict next observation (termed MB-RCP).</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>Jericho (33 interactive fiction games; focus on Enchanter, Sorcerer, Spellbrkr, Spirit, Ztuu)</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>Text-based interactive fiction games with partially observable states presented as text (description, inventory, candidate actions), large compositional action spaces, long trajectories and sparse rewards; used as a challenging benchmark for text-game agents.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>implicit transformer context (long-context memory); no explicit external memory structure</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>sequential buffer encoded in transformer context tokens (history of o,g,a tokens), with most intermediate observations replaced by a placeholder token "<STATE>" to limit sequence length</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>past observations (o0 and current ot fully written; intermediate observations replaced by placeholders), past actions and past goal-conditions encoded as tokens in the context window</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td>not explicitly specified; practically truncated by replacing intermediate observations with a placeholder to keep input length manageable (i.e., effectively limited by model context window and tokenization choices)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td>attention over the transformer context (implicit retrieval via full/long attention on encoded tokens), not an explicit retrieval/query mechanism</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>context is extended each step by appending the latest predicted action and resulting observation (with intermediate history tokenization scheme); effectively updated after each action/step</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>conditioning the goal-prediction and action generation (tracking game history and recent observations), and as input to the auxiliary next-observation predictor to regularize learning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>LDT (which leverages long-context encoding of past interactions) outperforms prior baselines on Jericho: reported to achieve ~10% better performance on the hardest environments and up to ~30% better on average across games compared to previous methods; models trained with the auxiliary next-observation loss (λ=0.5, MB-RCP) perform better than models without it across all evaluated games (exact per-game numbers are reported in the paper's tables/figures).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>When the model is trained without the auxiliary next-observation prediction (λ=0.0), performance is worse across all games (paper reports consistent gains when including the auxiliary loss); when the model outputs goal-conditions without exponential tilt (α=0) it produces lower-quality goals and worse gameplay than when α is increased (α=10 or 20), which improves performance.</td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>Predicting the next observation as an auxiliary loss (MB-RCP) improves robustness and performance across all games — interpreted as leveraging an internal model / memory-like signal; however, compressing intermediate observations into a placeholder token (due to compute limits) is a limiting factor — richer intermediate state encoding might further help. The paper also notes that long-context transformer encoding (implicit memory) combined with exponential tilt for goal sampling yields better results than prior online RL methods that used explicit graph-based or case-based memories.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Explicit limitation from compute: many intermediate observations were replaced by a fixed token ('<STATE>') to shorten sequences, which reduces the information stored in memory; earlier experiments with a frozen encoder to compress intermediate states were inconclusive. The model still cannot generalize zero-shot to unseen games at current capacity. No explicit external memory (retrieval/database) was used, so memory is bounded by the transformer's context window and the chosen tokenization/truncation strategy.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Decision Transformers with Exponential Tilt for Interactive Text Environments', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2739.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2739.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KG-A2C</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>KG-A2C (Knowledge Graph A2C)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An online RL agent that builds and uses a knowledge graph to represent environment state at each step and learns a Q-value or policy based on that structured state representation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Graph constrained reinforcement learning for natural language action spaces</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>KG-A2C</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agent that constructs a knowledge-graph representation of the game state and uses that structured representation as input to an RL method (A2C variant) to learn policies in text games.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>Jericho</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>See LDT entry; KG-A2C focuses on representing and learning from structured state encodings in text-based interactive fiction.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>graph-based memory / knowledge graph</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>knowledge graph representing entities, relations and state facts (graph structure encoded for downstream policy/value networks)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>discovered objects, relationships, room connections, possibly past observations encoded as nodes/edges</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td>not specified in this paper (depends on graph construction implementation in the cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td>graph-encoding / graph neural network reads the knowledge graph; retrieval is via graph embedding and message passing</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>graph updated as game progresses when new observations / facts are discovered (per-step updates)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>to represent partially-observable environment state compactly so the RL agent can plan and select actions (reduces the need to keep raw text history)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Reported in prior work as a baseline for Jericho; in this paper KG-A2C is shown as a prior baseline whose average performance (dotted lines in figures) is lower than LDT with exponential tilt. Exact numeric values for KG-A2C are shown in the paper's baseline comparisons (figures), but not tabulated in the provided excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>The paper cites KG-A2C as an example of methods that encode state via knowledge graphs to handle long histories; LDT outperforms such methods in the reported comparisons, suggesting that pre-trained long-context language models can rival or exceed graph-based state encodings for these benchmarks when trained offline.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Not detailed in this paper beyond general commentary that online RL approaches using structured state encodings can be sample-inefficient compared to offline LLM-based methods.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Decision Transformers with Exponential Tilt for Interactive Text Environments', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2739.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2739.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Q*BERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Q*BERT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that uses a knowledge-graph-like representation (Q*BERT) to help solve text games by encoding game entities and structure for value estimation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Graph constrained reinforcement learning for natural language action spaces</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Q*BERT</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A model that builds structured representations of the environment (knowledge graph-style) and uses those encodings to learn value functions for text-game action selection.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>Jericho</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>See LDT entry.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>graph-based / symbolic memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>knowledge-graph-like structure encoding objects, rooms, relations</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>entities, relations and partial state facts derived from observations</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td>not specified</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td>graph-encoding and readout via specialized neural modules</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>updated as new observations arrive</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>to form a compact state representation to support value-based learning and decision making</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Cited as a prior online baseline; overall performance (as plotted in the paper's comparisons) is below the LDT results reported here. Exact metrics are not reproduced in the paper excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>Mentioned as an approach that leverages structured memory to encode game state; the paper positions LDT as outperforming such approaches when trained offline with pre-trained LLM backbones.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Not discussed in detail in this paper beyond general limitations of online RL approaches (sample inefficiency) and the practical complexity of constructing such graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Decision Transformers with Exponential Tilt for Interactive Text Environments', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2739.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2739.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SHA-KG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SHA-KG (Stacked Hierarchical Attention with Knowledge Graph)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A text-game agent that encodes game history using hierarchical/stacked attention mechanisms over a graph representation to learn value functions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deep reinforcement learning with stacked hierarchical attention for text-based games</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>SHA-KG</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Uses graph attention networks and hierarchical attention over encoded game history / graph structures to produce state encodings for RL learning.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>Jericho</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>See LDT entry.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>graph-based memory with hierarchical attention</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>graph (nodes/edges) combined with hierarchical attention layers to process history</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>past observations encoded into graph nodes, relations and hierarchical summaries</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td>not specified</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td>attention over graph-structured encodings (graph attention networks)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>per-step graph updates as observations are received</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>encode long histories and relational information for value/policy estimation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Included as a prior baseline in comparisons; the paper indicates LDT outperforms prior baselines including SHA-KG on the evaluated games (exact numeric baseline scores appear as dotted lines in the figures but are not enumerated in the provided excerpt).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>Cited as prior work that uses explicit graph-structured memory; no direct head-to-head memory ablation with LDT is performed, only overall performance comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Not discussed in detail in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Decision Transformers with Exponential Tilt for Interactive Text Environments', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2739.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2739.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RC-DQN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RC-DQN (Reading-Comprehension DQN / Retrieval-based DQN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An architecture that retrieves and encodes relevant previous observations (reading-comprehension style) with GRUs and then uses value learning (DQN) for action selection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Interactive fiction game playing as multi-paragraph reading comprehension with reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RC-DQN</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Retrieves relevant past observations, encodes them with sequence encoders (e.g., GRUs) in a reading-comprehension fashion, and uses value-based RL to select actions among candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>Jericho</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>See LDT entry.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented memory (recent observations retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>retrieval buffer of past observations that are encoded by sequence models (e.g., GRUs) for decision making</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>relevant previous observations / messages returned by the game engine, used to inform current decision</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td>not specified (retrieval mechanism chooses subset of past observations)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td>retrieval of relevant previous observations (reading-comprehension style retrieval), followed by GRU encoding</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>buffer of past observations is appended to each step; retrieval selects a subset when making decisions</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>to supply context beyond the immediate observation for value estimation and action selection</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Cited as a prior baseline; performance is lower than the LDT results in the paper's comparative plots. Exact numeric comparisons are shown in the paper's figures/tables (not fully reproduced in the excerpt).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>Mentioned as an approach that retrieves past observations to inform decisions; the LDT authors note that their offline LLM-based method, even with compressed intermediate history, outperforms such retrieval-based online methods in the tested settings.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Not discussed in detail in this paper beyond the general observation that online RL and retrieval-based methods tend to be sample-inefficient compared to offline LLM-based learning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Decision Transformers with Exponential Tilt for Interactive Text Environments', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2739.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2739.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CBR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Case-Based Reasoning (CBR) for text games</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that stores prior interactions (cases) in memory and uses a graph-attention network to encode similarity between current state and stored cases to inform action selection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Case-based reasoning for better generalization in textual reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Case-Based Reasoning (CBR)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Stores previous interactions as cases and uses graph attention to compute similarity between current state and stored cases, leveraging past successful interactions to guide action choice.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>Jericho</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>See LDT entry.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>case memory / episodic database</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>case database (stored prior interactions) plus graph-attention encoding to compute similarities</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>previous interactions (state-action-result triples), successful trajectories, possibly partial strategies</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td>not specified (depends on case base implementation in cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td>similarity-based retrieval via graph attention networks to find relevant prior cases</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>append new cases as new interactions are observed (typical CBR update strategy)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>to generalize from similar past interactions, avoid repeating mistakes, and reuse successful strategies</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Reported as a prior baseline; plotted in comparison figures and appears below LDT performance in the paper. Exact numbers are available in the referenced baseline results but are not enumerated in the excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>CBR is cited as an example where storing previous interactions helps generalization; the current paper reports LDT outperforms such approaches when using pre-trained long-context transformers and offline training.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Not discussed in detail in this paper; the paper notes generally that online-memory-based methods are sample-inefficient relative to offline LLM-based approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Decision Transformers with Exponential Tilt for Interactive Text Environments', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Case-based reasoning for better generalization in textual reinforcement learning <em>(Rating: 2)</em></li>
                <li>Graph constrained reinforcement learning for natural language action spaces <em>(Rating: 2)</em></li>
                <li>Interactive fiction game playing as multi-paragraph reading comprehension with reinforcement learning <em>(Rating: 2)</em></li>
                <li>Deep reinforcement learning with stacked hierarchical attention for text-based games <em>(Rating: 2)</em></li>
                <li>Keep calm and explore: Language models for action generation in text-based games <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2739",
    "paper_id": "paper-256827635",
    "extraction_schema_id": "extraction-schema-70",
    "extracted_data": [
        {
            "name_short": "LDT",
            "name_full": "Language Decision Transformer",
            "brief_description": "An offline RL agent built by fine-tuning a pre-trained LongT5 encoder-decoder as a Decision Transformer to predict a trajectory goal-condition, the next action, and the next observation; uses exponential tilting to sample high-probability high-return goal conditions and an auxiliary next-observation loss (MB-RCP) to improve performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Language Decision Transformer (LDT)",
            "agent_description": "Fine-tuned LongT5-base encoder-decoder adapted to Decision Transformer formulation that models sequences of {observation, goal-condition, action} and decodes predicted goal-condition, action, and next observation; uses exponential tilt to bias sampled goal-conditions and an auxiliary cross-entropy loss to predict next observation (termed MB-RCP).",
            "base_model_size": null,
            "game_benchmark_name": "Jericho (33 interactive fiction games; focus on Enchanter, Sorcerer, Spellbrkr, Spirit, Ztuu)",
            "game_description": "Text-based interactive fiction games with partially observable states presented as text (description, inventory, candidate actions), large compositional action spaces, long trajectories and sparse rewards; used as a challenging benchmark for text-game agents.",
            "uses_memory": true,
            "memory_type": "implicit transformer context (long-context memory); no explicit external memory structure",
            "memory_structure": "sequential buffer encoded in transformer context tokens (history of o,g,a tokens), with most intermediate observations replaced by a placeholder token \"&lt;STATE&gt;\" to limit sequence length",
            "memory_content": "past observations (o0 and current ot fully written; intermediate observations replaced by placeholders), past actions and past goal-conditions encoded as tokens in the context window",
            "memory_capacity": "not explicitly specified; practically truncated by replacing intermediate observations with a placeholder to keep input length manageable (i.e., effectively limited by model context window and tokenization choices)",
            "memory_retrieval_strategy": "attention over the transformer context (implicit retrieval via full/long attention on encoded tokens), not an explicit retrieval/query mechanism",
            "memory_update_strategy": "context is extended each step by appending the latest predicted action and resulting observation (with intermediate history tokenization scheme); effectively updated after each action/step",
            "memory_usage_purpose": "conditioning the goal-prediction and action generation (tracking game history and recent observations), and as input to the auxiliary next-observation predictor to regularize learning",
            "performance_with_memory": "LDT (which leverages long-context encoding of past interactions) outperforms prior baselines on Jericho: reported to achieve ~10% better performance on the hardest environments and up to ~30% better on average across games compared to previous methods; models trained with the auxiliary next-observation loss (λ=0.5, MB-RCP) perform better than models without it across all evaluated games (exact per-game numbers are reported in the paper's tables/figures).",
            "performance_without_memory": "When the model is trained without the auxiliary next-observation prediction (λ=0.0), performance is worse across all games (paper reports consistent gains when including the auxiliary loss); when the model outputs goal-conditions without exponential tilt (α=0) it produces lower-quality goals and worse gameplay than when α is increased (α=10 or 20), which improves performance.",
            "has_memory_ablation": true,
            "memory_effectiveness_findings": "Predicting the next observation as an auxiliary loss (MB-RCP) improves robustness and performance across all games — interpreted as leveraging an internal model / memory-like signal; however, compressing intermediate observations into a placeholder token (due to compute limits) is a limiting factor — richer intermediate state encoding might further help. The paper also notes that long-context transformer encoding (implicit memory) combined with exponential tilt for goal sampling yields better results than prior online RL methods that used explicit graph-based or case-based memories.",
            "memory_limitations": "Explicit limitation from compute: many intermediate observations were replaced by a fixed token ('&lt;STATE&gt;') to shorten sequences, which reduces the information stored in memory; earlier experiments with a frozen encoder to compress intermediate states were inconclusive. The model still cannot generalize zero-shot to unseen games at current capacity. No explicit external memory (retrieval/database) was used, so memory is bounded by the transformer's context window and the chosen tokenization/truncation strategy.",
            "comparison_with_other_memory_types": false,
            "best_memory_configuration": null,
            "uuid": "e2739.0",
            "source_info": {
                "paper_title": "Language Decision Transformers with Exponential Tilt for Interactive Text Environments",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "KG-A2C",
            "name_full": "KG-A2C (Knowledge Graph A2C)",
            "brief_description": "An online RL agent that builds and uses a knowledge graph to represent environment state at each step and learns a Q-value or policy based on that structured state representation.",
            "citation_title": "Graph constrained reinforcement learning for natural language action spaces",
            "mention_or_use": "mention",
            "agent_name": "KG-A2C",
            "agent_description": "Agent that constructs a knowledge-graph representation of the game state and uses that structured representation as input to an RL method (A2C variant) to learn policies in text games.",
            "base_model_size": null,
            "game_benchmark_name": "Jericho",
            "game_description": "See LDT entry; KG-A2C focuses on representing and learning from structured state encodings in text-based interactive fiction.",
            "uses_memory": true,
            "memory_type": "graph-based memory / knowledge graph",
            "memory_structure": "knowledge graph representing entities, relations and state facts (graph structure encoded for downstream policy/value networks)",
            "memory_content": "discovered objects, relationships, room connections, possibly past observations encoded as nodes/edges",
            "memory_capacity": "not specified in this paper (depends on graph construction implementation in the cited work)",
            "memory_retrieval_strategy": "graph-encoding / graph neural network reads the knowledge graph; retrieval is via graph embedding and message passing",
            "memory_update_strategy": "graph updated as game progresses when new observations / facts are discovered (per-step updates)",
            "memory_usage_purpose": "to represent partially-observable environment state compactly so the RL agent can plan and select actions (reduces the need to keep raw text history)",
            "performance_with_memory": "Reported in prior work as a baseline for Jericho; in this paper KG-A2C is shown as a prior baseline whose average performance (dotted lines in figures) is lower than LDT with exponential tilt. Exact numeric values for KG-A2C are shown in the paper's baseline comparisons (figures), but not tabulated in the provided excerpt.",
            "performance_without_memory": null,
            "has_memory_ablation": false,
            "memory_effectiveness_findings": "The paper cites KG-A2C as an example of methods that encode state via knowledge graphs to handle long histories; LDT outperforms such methods in the reported comparisons, suggesting that pre-trained long-context language models can rival or exceed graph-based state encodings for these benchmarks when trained offline.",
            "memory_limitations": "Not detailed in this paper beyond general commentary that online RL approaches using structured state encodings can be sample-inefficient compared to offline LLM-based methods.",
            "comparison_with_other_memory_types": null,
            "best_memory_configuration": null,
            "uuid": "e2739.1",
            "source_info": {
                "paper_title": "Language Decision Transformers with Exponential Tilt for Interactive Text Environments",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "Q*BERT",
            "name_full": "Q*BERT",
            "brief_description": "A method that uses a knowledge-graph-like representation (Q*BERT) to help solve text games by encoding game entities and structure for value estimation.",
            "citation_title": "Graph constrained reinforcement learning for natural language action spaces",
            "mention_or_use": "mention",
            "agent_name": "Q*BERT",
            "agent_description": "A model that builds structured representations of the environment (knowledge graph-style) and uses those encodings to learn value functions for text-game action selection.",
            "base_model_size": null,
            "game_benchmark_name": "Jericho",
            "game_description": "See LDT entry.",
            "uses_memory": true,
            "memory_type": "graph-based / symbolic memory",
            "memory_structure": "knowledge-graph-like structure encoding objects, rooms, relations",
            "memory_content": "entities, relations and partial state facts derived from observations",
            "memory_capacity": "not specified",
            "memory_retrieval_strategy": "graph-encoding and readout via specialized neural modules",
            "memory_update_strategy": "updated as new observations arrive",
            "memory_usage_purpose": "to form a compact state representation to support value-based learning and decision making",
            "performance_with_memory": "Cited as a prior online baseline; overall performance (as plotted in the paper's comparisons) is below the LDT results reported here. Exact metrics are not reproduced in the paper excerpt.",
            "performance_without_memory": null,
            "has_memory_ablation": false,
            "memory_effectiveness_findings": "Mentioned as an approach that leverages structured memory to encode game state; the paper positions LDT as outperforming such approaches when trained offline with pre-trained LLM backbones.",
            "memory_limitations": "Not discussed in detail in this paper beyond general limitations of online RL approaches (sample inefficiency) and the practical complexity of constructing such graphs.",
            "comparison_with_other_memory_types": null,
            "best_memory_configuration": null,
            "uuid": "e2739.2",
            "source_info": {
                "paper_title": "Language Decision Transformers with Exponential Tilt for Interactive Text Environments",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "SHA-KG",
            "name_full": "SHA-KG (Stacked Hierarchical Attention with Knowledge Graph)",
            "brief_description": "A text-game agent that encodes game history using hierarchical/stacked attention mechanisms over a graph representation to learn value functions.",
            "citation_title": "Deep reinforcement learning with stacked hierarchical attention for text-based games",
            "mention_or_use": "mention",
            "agent_name": "SHA-KG",
            "agent_description": "Uses graph attention networks and hierarchical attention over encoded game history / graph structures to produce state encodings for RL learning.",
            "base_model_size": null,
            "game_benchmark_name": "Jericho",
            "game_description": "See LDT entry.",
            "uses_memory": true,
            "memory_type": "graph-based memory with hierarchical attention",
            "memory_structure": "graph (nodes/edges) combined with hierarchical attention layers to process history",
            "memory_content": "past observations encoded into graph nodes, relations and hierarchical summaries",
            "memory_capacity": "not specified",
            "memory_retrieval_strategy": "attention over graph-structured encodings (graph attention networks)",
            "memory_update_strategy": "per-step graph updates as observations are received",
            "memory_usage_purpose": "encode long histories and relational information for value/policy estimation",
            "performance_with_memory": "Included as a prior baseline in comparisons; the paper indicates LDT outperforms prior baselines including SHA-KG on the evaluated games (exact numeric baseline scores appear as dotted lines in the figures but are not enumerated in the provided excerpt).",
            "performance_without_memory": null,
            "has_memory_ablation": false,
            "memory_effectiveness_findings": "Cited as prior work that uses explicit graph-structured memory; no direct head-to-head memory ablation with LDT is performed, only overall performance comparisons.",
            "memory_limitations": "Not discussed in detail in this paper.",
            "comparison_with_other_memory_types": null,
            "best_memory_configuration": null,
            "uuid": "e2739.3",
            "source_info": {
                "paper_title": "Language Decision Transformers with Exponential Tilt for Interactive Text Environments",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "RC-DQN",
            "name_full": "RC-DQN (Reading-Comprehension DQN / Retrieval-based DQN)",
            "brief_description": "An architecture that retrieves and encodes relevant previous observations (reading-comprehension style) with GRUs and then uses value learning (DQN) for action selection.",
            "citation_title": "Interactive fiction game playing as multi-paragraph reading comprehension with reinforcement learning",
            "mention_or_use": "mention",
            "agent_name": "RC-DQN",
            "agent_description": "Retrieves relevant past observations, encodes them with sequence encoders (e.g., GRUs) in a reading-comprehension fashion, and uses value-based RL to select actions among candidates.",
            "base_model_size": null,
            "game_benchmark_name": "Jericho",
            "game_description": "See LDT entry.",
            "uses_memory": true,
            "memory_type": "retrieval-augmented memory (recent observations retrieval)",
            "memory_structure": "retrieval buffer of past observations that are encoded by sequence models (e.g., GRUs) for decision making",
            "memory_content": "relevant previous observations / messages returned by the game engine, used to inform current decision",
            "memory_capacity": "not specified (retrieval mechanism chooses subset of past observations)",
            "memory_retrieval_strategy": "retrieval of relevant previous observations (reading-comprehension style retrieval), followed by GRU encoding",
            "memory_update_strategy": "buffer of past observations is appended to each step; retrieval selects a subset when making decisions",
            "memory_usage_purpose": "to supply context beyond the immediate observation for value estimation and action selection",
            "performance_with_memory": "Cited as a prior baseline; performance is lower than the LDT results in the paper's comparative plots. Exact numeric comparisons are shown in the paper's figures/tables (not fully reproduced in the excerpt).",
            "performance_without_memory": null,
            "has_memory_ablation": false,
            "memory_effectiveness_findings": "Mentioned as an approach that retrieves past observations to inform decisions; the LDT authors note that their offline LLM-based method, even with compressed intermediate history, outperforms such retrieval-based online methods in the tested settings.",
            "memory_limitations": "Not discussed in detail in this paper beyond the general observation that online RL and retrieval-based methods tend to be sample-inefficient compared to offline LLM-based learning.",
            "comparison_with_other_memory_types": null,
            "best_memory_configuration": null,
            "uuid": "e2739.4",
            "source_info": {
                "paper_title": "Language Decision Transformers with Exponential Tilt for Interactive Text Environments",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "CBR",
            "name_full": "Case-Based Reasoning (CBR) for text games",
            "brief_description": "A method that stores prior interactions (cases) in memory and uses a graph-attention network to encode similarity between current state and stored cases to inform action selection.",
            "citation_title": "Case-based reasoning for better generalization in textual reinforcement learning",
            "mention_or_use": "mention",
            "agent_name": "Case-Based Reasoning (CBR)",
            "agent_description": "Stores previous interactions as cases and uses graph attention to compute similarity between current state and stored cases, leveraging past successful interactions to guide action choice.",
            "base_model_size": null,
            "game_benchmark_name": "Jericho",
            "game_description": "See LDT entry.",
            "uses_memory": true,
            "memory_type": "case memory / episodic database",
            "memory_structure": "case database (stored prior interactions) plus graph-attention encoding to compute similarities",
            "memory_content": "previous interactions (state-action-result triples), successful trajectories, possibly partial strategies",
            "memory_capacity": "not specified (depends on case base implementation in cited work)",
            "memory_retrieval_strategy": "similarity-based retrieval via graph attention networks to find relevant prior cases",
            "memory_update_strategy": "append new cases as new interactions are observed (typical CBR update strategy)",
            "memory_usage_purpose": "to generalize from similar past interactions, avoid repeating mistakes, and reuse successful strategies",
            "performance_with_memory": "Reported as a prior baseline; plotted in comparison figures and appears below LDT performance in the paper. Exact numbers are available in the referenced baseline results but are not enumerated in the excerpt.",
            "performance_without_memory": null,
            "has_memory_ablation": false,
            "memory_effectiveness_findings": "CBR is cited as an example where storing previous interactions helps generalization; the current paper reports LDT outperforms such approaches when using pre-trained long-context transformers and offline training.",
            "memory_limitations": "Not discussed in detail in this paper; the paper notes generally that online-memory-based methods are sample-inefficient relative to offline LLM-based approaches.",
            "comparison_with_other_memory_types": null,
            "best_memory_configuration": null,
            "uuid": "e2739.5",
            "source_info": {
                "paper_title": "Language Decision Transformers with Exponential Tilt for Interactive Text Environments",
                "publication_date_yy_mm": "2023-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Case-based reasoning for better generalization in textual reinforcement learning",
            "rating": 2,
            "sanitized_title": "casebased_reasoning_for_better_generalization_in_textual_reinforcement_learning"
        },
        {
            "paper_title": "Graph constrained reinforcement learning for natural language action spaces",
            "rating": 2,
            "sanitized_title": "graph_constrained_reinforcement_learning_for_natural_language_action_spaces"
        },
        {
            "paper_title": "Interactive fiction game playing as multi-paragraph reading comprehension with reinforcement learning",
            "rating": 2,
            "sanitized_title": "interactive_fiction_game_playing_as_multiparagraph_reading_comprehension_with_reinforcement_learning"
        },
        {
            "paper_title": "Deep reinforcement learning with stacked hierarchical attention for text-based games",
            "rating": 2,
            "sanitized_title": "deep_reinforcement_learning_with_stacked_hierarchical_attention_for_textbased_games"
        },
        {
            "paper_title": "Keep calm and explore: Language models for action generation in text-based games",
            "rating": 1,
            "sanitized_title": "keep_calm_and_explore_language_models_for_action_generation_in_textbased_games"
        }
    ],
    "cost": 0.017227,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Language Decision Transformers with Exponential Tilt for Interactive Text Environments
17 Nov 2023</p>
<p>Nicolas Gontier nicolas.gontier@servicenow.com 
Pau Rodriguez 
Issam Laradji 
David Vazquez 
Christopher Pal </p>
<p>ServiceNow Research Quebec Artificial Intelligence Institute (Mila) Polytechnique Montreal Montreal
Canada</p>
<p>ServiceNow Research Montreal
Canada</p>
<p>ServiceNow Research Montreal
Canada</p>
<p>ServiceNow Research Montreal
Canada</p>
<p>ServiceNow Research Quebec Artificial Intelligence Institute (Mila) Polytechnique Montreal
Canada</p>
<p>CIFAR AI Chair Montreal
Canada</p>
<p>Language Decision Transformers with Exponential Tilt for Interactive Text Environments
17 Nov 20239D7BC84D2E95D5AD769A5E4FB3638042arXiv:2302.05507v2[cs.CL]
Text-based game environments are challenging because agents must deal with long sequences of text, execute compositional actions using text and learn from sparse rewards.We address these challenges by proposing Language Decision Transformers (LDTs), a framework that is based on transformer language models and decision transformers (DTs).Our LDTs extend DTs with 3 components:(1) exponential tilt to guide the agent towards high obtainable goals, (2) novel goal conditioning methods yielding better results than the traditional return-to-go (sum of all future rewards), and (3) a model of future observations that improves agent performance.LDTs are the first to address offline RL with DTs on these challenging games.Our experiments show that LDTs achieve the highest scores among many different types of agents on some of the most challenging Jericho games, such as Enchanter.</p>
<p>Introduction</p>
<p>People spend a significant fraction of their lives performing activities closely linked with natural languages, such as having conversations, writing e-mails, filling out forms, reading and writing documents, and so on.Recently, the excitement around the use of Large Language Models (LLMs) for dialogue has brought the setting of interactive dialogue into the spotlight.Interactive text-based games allow one to explore and test interactive agents, alternative neural architectures, and techniques.However, text environments remain challenging for existing Reinforcement Learning (RL) agents since the action space is vast due to the compositional nature of language, making exploration difficult.Fortunately, language has the advantage that knowledge can often be reused across environments, such as the fact that fire burns or that doors open.To solve real-world text-based tasks and play rich text-based games well, RL agents can also benefit from the knowledge about the human world acquired from large offline data sources by leveraging pre-trained LLMs.</p>
<p>In real-world settings, the low-performing behavior exhibited by online RL agents during learning makes them impractical to use with humans in the loop.This situation arises in many other contexts (Levine et al., 2020) and has motivated a lot of research on offline RL.Offline RL methods have a long history, but more recently, several approaches have been proposed that focused on using powerful transformer-based sequence models, including Trajectory Transformers (TTs) (Janner et al., 2021), and Decision Transformers (DTs) (Chen et al., 2021).However, these approaches are formulated and examined within continuous control robotics problems.Unlike the methods above, our approach is designed to handle the complexity and richness of human language by leveraging pre-trained LLMs. Figure 1: Overview of our approach: Noisy trajectories are generated from a high quality game walkthrough by taking 100 random steps at each 5% of the trajectory.The collection of trajectories on multiple games is used to train our LDT model offline to predict a goal condition, next action, and next observation.The LDT is then evaluated in each game environment, initialized with 5 random seeds.</p>
<p>Motivated by the analogy of textgames to intelligent text assistants helping people with various tasks, we assume that a few expensive expert demonstrations are available for learning.As such, we use the Jericho text games (Hausknecht et al., 2020), which provide a single golden path trajectory per game.To create a large and diverse dataset, we then generate trajectories with perturbations from that golden path as described in Section 4 and depicted in Figure 1.The complexity and richness of Jericho games make them a reasonable proxy for the kind of data one might obtain in real-world assistive agent settings.</p>
<p>In this work, we use a pre-trained Transformer language model that we fine-tune on offline game trajectories to predict in order: (i) a numerical trajectory quality measure used to condition the generation of the next actions (termed "goal condition"), (ii) next actions and (iii) future observations.To sample high-quality trajectories from our model, we convert distributions over discrete token representations of goal conditions into continuous ones, allowing us to maximize them through an exponential tilting technique.In addition, we compare different definitions of trajectory quality measures, and introduce an auxiliary loss to predict future observations.We will refer to trajectory quality measures as "goal conditions" in the rest of this paper and our approach as Language Decision Transformers (LDTs) with exponential tilt.Our approach is visualized in Figure 2. See Table 1 for a comparison of how our formulation for density estimation and decision-making is situated with respect to prior frameworks.We also note that none of these previous frameworks have been applied to text-based action spaces, so none have leveraged pre-trained LLMs as in our framework.</p>
<p>To conclude, our contributions can be summarized as follows: (1) Our work is the first to address the challenging Jericho text-based games in an offline return conditioned sequence learning setup, wherein we train models on noisy walkthrough trajectories from multiple games simultaneously.(2) We improve agent behavior with fewer assumptions by letting the model predict goal conditions in a manner where no knowledge of the maximum score is needed through our use of an exponential tilting technique.(Section 5.1).(3) We explore and empirically compare 3 novel definitions of goal conditioning that perform better than the return-to-go perspective of Decision Transformers (DTs).(Section 5.2). ( 4) We propose a novel auxiliary loss to train DTs that draws parallels to model-based RL and empirically shows better performance compared to the traditional model-free loss of DTs (Section 5.3).We test our proposed solutions on 33 different, realistic and complex text environments and show that LDTs performs 10% better than previous baselines on the hardest environments, and up to 30% better on average across all environments.</p>
<p>Methodology</p>
<p>Problem setup</p>
<p>Text-based games can be formulated as partially observable Markov decision processes (POMDP) described by (S, T , A, O, R, γ).The current game state s t ∈ S is partially observable in o t ∈ O which is often a text description of the current scene (inventory, location, items).The agent can take an action a t ∈ A to interact with the environment and causes a state change based on a transition function T (s t , a t ) leading to a new state s t+1 ∈ S. Some games are stochastic in that the same action for the same state can lead to different states.Once the agent transitions to the new state, a reward r t is given by an unknown reward function R(s t , a t ) that the game designers defined.The reward can either be positive, negative, or neutral.Offline Reinforcement Learning.The goal of the agent is to learn a policy π(a t |s t ) which maximizes the expected return E[ T t=0 r t ] in the POMDP by observing a series of static trajectories obtained in the same or similar environments.Each trajectory is defined as τ = (o 0 , a 0 , r 0 , o 1 , a 1 , r 1 , ..., o T , a T , r T ), and it is obtained by observing rollouts of arbitrary policies.This setup is similar to supervised learning, where models are trained from a static dataset.It is more difficult than online reinforcement learning since agents cannot interact with the environment to recollect more data.</p>
<p>Reinforcement Learning in text-based games.One of the main differences between traditional RL environments, such as Atari or Mujoco, and text-based environments is that both A and O consist of text.Therefore, due to the compositional nature of language, A is significantly more complex than in common RL scenarios, where the action space is restricted to a few well-defined actions.</p>
<p>To deal with such complexity, we model A, O and R with a large pre-trained language model: x i = LLM(x i |x 1:i−1 ), where x i is the i th text token in a text sequence of length L. The goal is that the LLM uses its pre-existing knowledge about the world (e.g., doors can be opened), to propose valid actions given an observation.</p>
<p>Decision Transformers.To perform offline learning on text-based games, we adapt the language model (particularly LongT5 (Guo et al., 2022)) to be a decision transformer (DT) (Chen et al., 2021) which abstracts reinforcement learning as a sequential modeling problem.DTs are trained with the language modeling objective on sequences of {g t , o t , a t } T t=0 triples, where the goal condition g t is defined as the undiscounted sum of future rewards, or return-to-go: g t = T i=t r i .Consequently, we have a model that can be conditioned on a desired goal (or return, in this case).In the following subsections, we discuss the novelties we bring to the original formulation of DTs.</p>
<p>Goal conditioning</p>
<p>One limitation of DTs is that the best final score of a game must be known to condition on it at the first step with g 0 (Chen et al., 2021).Although we have g 0 for the training trajectories, it is impossible to know the best target score when starting a new game.This is especially problematic for Jericho games where maximum scores vary greatly between games (Hausknecht et al., 2020).</p>
<p>One solution is to normalize g 0 during training with the maximum game score.This procedure leads to goal conditions between 0 and 1 for the training games and allows to use an initial goal condition of 1 at test time.However, this solution also assumes that we know the maximum score of every game since intermediate rewards returned by the environment r t also need to be normalized: g t+1 = g t − rt max score .To remove the dependence on manual goal conditioning and knowledge of the best obtainable score, we take a similar approach to Lee et al. (2022) and train the model on ordered sequences of {o t , g t , a t } T t=0 triples instead of {g t , o t , a t } T t=0 .Moving the goal condition g t after the observation o t allows us to predict the goal condition based on the current observation by modeling the joint probability of a t and g t as: P θ (a t , g t |o t ) = P θ (a t |g t , o t ) • P θ (g t |o t ).One challenge is that sampling g t can produce low and inaccurate target returns.To mitigate this issue, we perform exponential tilting on the predicted probabilities of g t .In particular we sample g t like so:
g t = argmax g P θ (g t |o t ) • exp(αg t ) ,(1)
with α ≥ 0 being a hyper-parameter that controls the amount of tilting we perform.This allows us to sample high but probable target returns.We compare results with α = {0, 1, 10, 20} in Section 5.1.</p>
<p>Another significant advantage of predicting the goal condition g t based on o t is that we can explore various strategies of goal conditions that cannot be defined manually at inference time.We describe below the original return-to-go used by decision transformers and three novel goal condition strategies.</p>
<p>Return-To-Go (RTG): g t = T i=t r i , is the original strategy of the return-to-go.It is the undiscounted sum of future rewards, which will be high at the beginning of trajectories achieving a high score.These values will decrease as the agent progresses since fewer future rewards will be available in a trajectory with intermediate rewards.</p>
<p>Immediate Reward (ImR): In the setting where g t = r t , each step is conditioned on the reward observed right after the predicted action.We expect that with this goal condition method, the agent will learn what type of actions usually yield higher rewards (opening chest -vs-moving in a direction).We expect this strategy to encourage the model to get high rewards as fast as possible.However, we expect this strategy to work well only for environments with dense reward signals.</p>
<p>Final Score (FinS): g t = T i=0 r i .In this setting, each step is conditioned on the final score achieved by the agent.The final score is defined as the sum of all rewards observed during the entire trajectory.Note that, unlike all the other goal condition definitions, this score will not change over the course of a trajectory.This setting is closer to the traditional RL paradigm in which we often define rewards based on the final performance of an agent: did it win or did it lose.We expect the agent to learn to differentiate successful from unsuccessful trajectories in this setting.Since the model is not conditioned on immediate rewards, we expect it will produce longer trajectories, which can eventually achieve higher final scores.</p>
<p>Average Return-To-Go (AvgRTG): g t = T i=t ri (T −t) .In this setting, each step is conditioned on the average of all future rewards.This is also defined as the return-to-go divided by the number of steps remaining.The motivation for this goal condition is that it will capture the sparsity of rewards in a trajectory, unlike all the others.To reduce the variance in the numbers observed between different games, all goal condition numbers during training are normalized by the maximum score of the current game: g t = int 100 • gt max score .At inference time, we can either manually specify goal condition numbers (assuming we know the game maximum score), or we can let the model predict those goal condition numbers with exponential tilt (more flexible).We experiment with all these goal condition definitions in our experiments and report results in Section 5.2.</p>
<p>Next State Prediction</p>
<p>To give more training signal to the model and make it more robust to stochastic environments, we also experiment with learning to predict the next observation o t+1 .Concretely, we predict o t+1 after taking action a t in state s t .Although the prediction of the next observation is not used to interact with the environment at test time, we believe that the agent will perform better if it can predict how its action will impact the world.Furthermore, predicting the next observation indirectly informs the model about the stochasticity of the environment.This technique draws parallels with the model-based paradigm in Reinforcement Learning, where the agent can predict how the environment will evolve after each action.Formally, the model estimates the following probability:
P θ (o t+1 , a t , g t |o t ) =P θ (o t+1 |a t , g t , o t ) • P θ (a t |g t , o t ) • P θ (g t |o t ),
(2) which is a type of Reward Conditioned Policy (RCP) with the additional term P θ (o t+1 |a t , g t , o t ).We call our technique model-based reward conditioned policy (MB-RCP).We compare our formulation to Density Estimation (L(θ)) Decision Making (π(a|s; η))  2022)).We compare our approach with Decision Transformers (DTs) (Chen et al., 2021), Reward Weighted Regression (RWR) (Peters &amp; Schaal, 2007;Dayan &amp; Hinton, 1997), Reward-Conditioned Policies (RCP) (Kumar et al., 2019) (also used by Multi-Game Decision Transformers (Lee et al., 2022)), Reweighted Behavior Cloning (RBC) (Piché et al., 2019) (also used by Trajectory Transformer (TT) (Janner et al., 2021)), and Implicit RL via supervised learning (IRvS) (Piché et al., 2022).Where s represents the state as encoded by the model and depends on the architecture and inputs used.
DTs log p θ (a t | o t , G t ) p θ (a | s t , G t ) RWR exp(η −1 G t ) log p θ (a t | o t ) p θ (a | s t ) RCP log p θ (a t | o t , G t )p θ (G t | o t ) p θ (a | s t , G)p θ (G | s t ) exp(η −1 G − κ(η)) RBC log p θ (G t | o t , a t )p θ (a t | o t ) p θ (G | s t , a)p θ (a | s t ) exp(η −1 G − κ(η)) IRvS log p θ (a t , G t | o t ) p θ (a, G | s t ) exp(η −1 G − κ(η)) MB-RCP (ours) log[p θ (o t+1 | a t , o t , G t ) p θ (a | s t , G)p θ (G | s t ) exp(η −1 G − κ(η)) •p θ (a t | o t , G t ) • p θ (G t | o t )]
prior work in Table 1.We are interested in using this additional prediction as a form of regularization and therefore treat predicting the next observation as an auxiliary loss, leading to:
L = (1 + λ) −1 L CE ([ĝ t ât ]; [g t a t ]) + λ • L CE (ô t+1 ; o t+1 ) ,(3)
with L CE being the regular cross entropy loss and λ being a hyper-parameter set to 0.5 in all our experiments.This weighted average prevents the model from spending too much of its representation power on the next observation prediction, as it is not strictly required to be able to interact in an environment.At inference time, only the next goal condition and next action predictions will be used.We perform an ablation study on this aspect of our approach by comparing models trained with (λ = 0.5) and without (λ = 0) this auxiliary loss and report our results in Section 5.3.</p>
<p>Related Work</p>
<p>Upside-down RL (UDRL) (Schmidhuber, 2019;Kumar et al., 2019;Piché et al., 2022) poses the task of learning a policy as a supervised learning problem where an agent is conditioned on an observation and a target reward to produce an action.Instead of generating the next action for a target reward, goal conditioning methods generate trajectories conditioned on an end-goal (Ghosh et al., 2019;Paster et al., 2020).Most relevant to our work, Chen et al. (2021) recast supervised RL as a sequence modeling problem with decision transformers (DTs), but they did not examine text environments.DTs have been extended to multi-task environments by training them on multiple Atari games (Lee et al., 2022).To address the problem of modelling text-based environments Furman et al. (2022) proposed DT-BERT for question answering in TextWorld environments (Côté et al., 2018).However, the maximum number of steps in their trajectories is 50, and the environments only differ in their number of rooms and objects.Wang et al. (2022) propose ScienceWorld, a text game environment similar to TextWorld and a Text Decicion Transformer (TDT) baseline.However, their TDT model predicts only the next action based on a given expected return and the previous observation.Here we go a step further and (i) propose different conditioning methods never considered before in DTs, (ii) predict the expected return with exponential tilt rather than relying on expert knowledge to condition on it, and (iii) predict next observation after the next action prediction.In addition, we train our agents on offline trajectories across multiple games and test them on complex and realistic environments using Jericho games (Hausknecht et al., 2020) with diverse dynamics and scenarios.</p>
<p>Jericho is a challenging python framework composed of 33 text-based interactive fiction games (Hausknecht et al., 2020).It was initially introduced with a new Template-DQN, and compared with the Deep Reinforcement Relevance Network (DRRN) (He et al., 2016).However, both methods are trained online, which requires an expensive simulator and requires domain-specific knowledge, such as the set of possible actions.Yao et al. (2020) proposed CALM, extending DRRNs to solve the problem of it needing to know the set of possible actions in advance.They use a GPT-2 (Radford et al., 2019) language model to generate a set of possible candidate actions for each game state.Then, they use an RL agent to select the best action among the (top-k=30) generated ones.</p>
<p>One of the main challenges of leveraging language models to solve Jericho games is to encode the full context of the game trajectory.As such, KG-A2C (Ammanabrolu &amp; Hausknecht, 2020) and Q*BERT (Ammanabrolu et al., 2020) use a knowledge graph to represent the environment state at each step and learn a Q-value function.SHA-KG (Xu et al., 2020) uses graph attention network (Veličković et al., 2018) to encode the game history and learn a value function.RC-DQN (Guo et al., 2020) uses a reading comprehension approach by retrieving relevant previous observations, encoding them with GRUs (Cho et al., 2014), and learning a Q-value function.DBERT-DRRN (Singh et al., 2021) leverages a DistilBERT to encode state and action and feed it to an MLP to learn a Q-value function.XTX (Tuyls et al., 2022) re-visits different frontiers in the state space and performs local exploration to overcome bottleneck states and dead-ends.CBR (Atzeni et al., 2022) stores previous interactions in memory and leverages a graph attention network (Veličković et al., 2018) to encode the similarity between states.The above previous methods are online-based RL, thus suffering from sample inefficiencies.Here, we take a simpler approach by leveraging long-context transformers like LongT5 (Guo et al., 2022) to model the sequence of state observations, target goal scores, and actions of past game trajectories as a sequence of tokens.Then, given a state observation, we leverage exponential tilt (Piché et al., 2022;Lee et al., 2022) to produce the action with the best possible target goal score.We find that our LDT approach is effective enough to outperform all previous methods that we have examined on Jericho games.(Shinn et al., 2023) converts feedback from the environment into natural language sentences used in language based form of reinforcement learning.</p>
<p>Experimental setup</p>
<p>The Jericho Engine Jericho2 is a well-known Python framework that consists of 33 text-based interactive fiction games that are challenging learning environments (Hausknecht et al., 2020).Developers manually create them, each having its own way of defining the rules and goals for each game, making the games quite diverse.Text adventure games are challenging on their own because of their combinatorially large action space and sparse rewards.Usually, text adventure games have a large action vocabulary (around 2000 words on average), and each action is made of multiple words (1 to 4 on average).This makes the action space as big as 2000 4 = 1.6 × 10 13 .To alleviate this issue, the Jericho benchmark provides a list of valid actions for each state.However, this makes the environment much slower as the game engine validates all possible actions against the simulator.In addition, the action space becomes dynamic as it changes from state to state.The above challenge in combination with extremely sparse rewards makes text adventure games very challenging for current RL methods.For brevity3 , we focus on 5 of the hardest Jericho games belonging to the Zork Universe: enchanter, sorcerer, spellbrkr, spirit, and ztuu.We report in Appendix B results on all 33 Jericho games.We generate trajectories for each of these games and train our model on the collection of all trajectories from all games.</p>
<p>Data Collection</p>
<p>Jericho provides one human walkthrough trajectory per game that achieves the maximum score.However, since some games are stochastic, every walkthrough is only valid for a specific default seed when initializing the game.To obtain a more diverse dataset with incorrect or partially correct trajectories, we propose to generate trajectories by following the walkthrough trajectories for some steps and then deviating from them.Concretely, to collect a large number of trajectories with different performances we follow the walkthrough trajectory for X% of its total number of steps and then take 100 additional random steps.We repeat that procedure 10 times for each X ∈ [0, 5, 10, ..., 85, 90, 95].When X = 0%, this is the same as a fully random trajectory.When X = 95%, the agent follows the walkthrough path for 95% of the steps and then takes 100 random steps.This results in a collection of 201 trajectories, including 1 original walkthrough for each game.Note that we also tried to include TDQN and DRRN trajectories trained on individual games, but these agents did not bring any significant information gain in our collection of trajectories.To not overfit on the default seed for each game, we ran the same procedure on 5 different seeds.This resulted in 1,005 trajectories of various lengths and qualities for each game.Note that only 1 of those obtain a 100% final score by following the walkthrough actions given by Jericho.We report in Appendix C the normalized scores (Figure 5) and lengths (Figure 6) observed in the collection of trajectories collected for each game.The top part of Figure 1 illustrates the data generation procedure.</p>
<p>Sequence Definition</p>
<p>To train an encoder-decoder architecture, trajectories are split between input and output sequences after a random number of steps t ∈ [0, T − 1] .The input sequence is then defined as [o 0 , g 0 , a 0 , o 1 , ..., g t−1 , a t−1 , o t ] and the output sequence as [g t , a t , o t+1 ] (also depicted in Figure 2).Each of these {o t , g t , a t } T t=0 elements are represented in natural language text (described below) and concatenated together to form input/output text sequence pairs.a t : intermediate actions are written as returned by agents playing the game, with the addition of special token delimiters: "Action: {a_t} </s></s>".g t : goal conditions are computed with one strategy among the ones described in Section 2.2 based on the list of intermediate rewards returned by the environment.Each goal condition is written in text like this: "GC: {g_t} </s></s>".o t : state observations are defined by multiple state characteristics available to Jericho games: (i) candidate actions available, (ii) the message returned by the game engine, (iii) the description of the current room, and (iv) the current inventory of the agent.Each observation is written in text like this: "Actions: {cand} </s></s> State: {msg} </s></s> Description: {desc} </s></s> Inventory: {inv} </s></s>", with {cand}, {msg}, {desc} and {inv} being the list of candidate actions, the game message, the description of the current room, and the inventory of the player respectively.However, as some game trajectories contain hundreds of steps, the current definition of {o t , g t , a t } T t=0 triples can make input sequences as long as tens of thousands of tokens.To shorten input sequences, we replaced state observations o t to be a single placeholder token "<STATE>" for all intermediate observations except the first (o 0 ) and current one (o t ) as depicted in Figure 2.</p>
<p>Experimental Results</p>
<p>Since Jericho games have long storylines, we leverage LongT5 (Guo et al., 2022), a text-to-text Transformer with a wide attention span.We use the pre-trained LongT5-base model as hosted by HuggingFace4 (Wolf et al., 2020) in all experiments as the base for our encoder-decoder architecture.We then fine-tuned the model for multiple epochs on the generated trajectories from Section 4. The hyperparameter settings can be found in Appendix A.</p>
<p>For each game, we initialize its environment with a random seed.We let the model predict the next goal condition and action at each step.The agent performs the predicted action, leading to the next observation in the environment.The model uses this observation as context for the next step.We run these steps in a cycle until we reach the end of the game and compute the final score.The game ends when the agent reaches the final state, or the model generates an invalid sequence.We repeat this process on 5 random seeds and take the average final score.The bottom part of Figure 1 illustrates the training and evaluation process.</p>
<p>The Effect of Exponential Tilt</p>
<p>In this section, we fine-tuned our model with the loss function described in Equation 3 on all generated trajectories split into input and output pairs (Section 4).The model was trained with the regular return-to-go goal condition (g t = T i=t r i ) and with λ = 0.5 for the auxiliary loss of predicting o t+1 .We tested the model on all games5 , normalized the obtained score based on the maximum human score for each game, and recorded the average across games and 5 random seeds for each game.Predicted GC / alpha=1</p>
<p>Predicted GC / alpha=10</p>
<p>Predicted GC / alpha=20</p>
<p>Imitation Learning TDQN (Hausknecht et al., 2020) DRRN (Hausknecht et al., 2020) KG-A2C (Ammanabrolu &amp; Hausknecht, 2020) CALM (Yao et al., 2020) SHA-KG (Xu et al., 2020) MPRC-DQN (Guo et al., 2020) RC-DQN (Guo et al., 2020) Bike+CBR (Atzeni et al., 2022) Figure 3: Average normalized score across different Jericho games (enchanter, sorcerer, spellbrkr, spirit, ztuu) with various amounts of exponential tilt ("Predicted GC" lines).We also report the performance of a model being conditioned on the optimal goal according to each game's maximum score ("Optimal GC" line).The average normalized score of various baselines is depicted in dotted lines.</p>
<p>To measure the effect of exponential tilt, the predicted g t were sampled according to Equation 1 with α = 0, 1, 10, 20 ("Predicted GC / alpha=α" in Figure 3).We also evaluated the model with the goal condition being manually given ("Optimal GC" in Figure 3) at each step.In the first step, the model is conditioned with g 0 = 100%, and at every next step g t is reduced by the amount of observed reward: g t+1 = g t − rt max score .This "Optimal GC" evaluation assumes we know the game's maximum score.We aim to achieve similar performance by simply predicting g t instead of manually defining it.</p>
<p>We report in Figure 3 the normalized score averaged across games for each method of predicting g t at different training stages of the model.During training the model is exposed to trajectories of various performances (detailed in Figure 5), so without any exponential tilt the model will output the most probable goal condition based on what it observed during training, which is less than ideal (solid red "Predicted GC / alpha=0" line).However, as we prioritize high numerical return-to-go over their likelihood (α increasing), the model's performance is getting closer to the "Optimal GC" performance, with α = 20 (solid orange line) being on par with the model that was manually given the "optimal" goal condition.In realistic scenarios, we do not have the optimal goal condition when starting a new game.In addition, predicting the goal condition offers greater flexibility in the design of goal conditions.We can now explore conditioning methods that would be impossible to define manually during run time.This is exactly what we explore in the next section.Overall, these results demonstrate that: (1) the numerical value of the goal condition has indeed an effect on the quality of the next generated answer, and (2) it is possible to recover the same performance as the "optimal" goal conditioning by increasing the amount of exponential tilt without knowing the game's max score.</p>
<p>Furthermore, we report in Figure 3 the average performance of previous works on the same set of games (dotted horizontal lines).Our offline method with exponential tilt beats previous methods with very little training when α = 10 or 20.However, since all previous methods were trained on each game in an online RL fashion, to fairly compare our approach we also trained an imitation learning (IL) baseline on human trajectories only (semi-dotted pink line).As expected, this offline RL baseline performs better than our approach without exponential tilt as it was trained on better trajectories, but as we increase exponential tilt (α = 10 or 20), our method outperforms the IL strategy.This can be explained by the diversity of interactions our agent saw during training compared to IL.This difference in performance also illustrates the stochasticity of the environments, as the IL baseline would perform close to 100% on deterministic games.</p>
<p>The Effect of Goal Conditioning Strategies</p>
<p>We fine-tuned 4 models with the loss function in Equation (3) on all generated trajectories split into input and output pairs (Section 4).Each model was trained with a different goal condition (Section 2.2) and with λ = 0.5 for the auxiliary loss predicting o t+1 .We tested the models on all games6 after 31.4ktraining steps and recorded the average score across 5 random seeds per game.In these experiments, the model generates goal conditions because at inference time, unlike with return-to-go (RTG), we cannot compute the immediate next reward (ImR) and the average return-to-go (AvgRTG), even if we know the game maximum score.To provide the immediate next  reward condition, we need to know at each step the maximum achievable reward among all candidate actions, which is infeasible in practice.To provide the average RTG condition, we need to know the number of steps remaining after each state, which is infeasible in practice.Fortunately, our model can generate goal conditions while leveraging the exponential tilt for producing better trajectories.All models in these experiments were evaluated by sampling g t according to Equation 1 with α = 10.</p>
<p>Table 2 reports the average score, standard deviation, and best score obtained on each game across 5 random seeds for all goal conditioning methods.Overall, these results show that the classical return-to-go conditioning method yields weaker performance than other methods in all environments.However, the best strategy depends on the game which can vary between ImR, FinS, or AvgRTG.These results further motivate the advantages of generating goal conditions that cannot be computed at runtime such as ImR and AvgRTG.Here we analyze the effect of predicting the next observation o t+1 as part of the loss function.We fine-tuned another 4 models, each with a different goal condition similar to the above section, but with the loss function described in Equation 3 with λ = 0.0 for the auxiliary loss of predicting o t+1 .We tested the models on the same set of games after 31.4ktraining steps and recorded the average score across 5 random seeds for each game.To compare the effect of the auxiliary loss, we averaged the scores across all goal conditioning methods.</p>
<p>The Effect of Predicting the Next Observation
λ = 0.0 λ = 0.
Table 3 reports the average score, standard deviation, and best score obtained on each game over 20 runs (5 random seeds × 4 goal conditioning methods) for models trained with (λ = 0.5) and without (λ = 0.0) the auxiliary loss on the predicted next observation o t+1 .In all games, models trained to predict the next observation o t+1 resulting from the predicted action a t and goal condition g t perform better than models trained to only predict the goal condition g t and next action a t .Overall, these results show that our proposed model-based reward-conditioned policy (MB-RCP) learning objective yields stronger performance than the classical reward-conditioned policy (RCP) objective.</p>
<p>Conclusion</p>
<p>In this work, we have proposed Language Decision Transformers (LDTs) as an offline reinforcement learning method for interactive text environments, and we have performed experiments using the challenging text-based games of Jericho.LDTs are built from pre-trained LLMs followed by training on multiple games simultaneously to predict: the trajectory goal condition, the next action, and the next observation.We have shown that by using exponential tilt, LDT-based agents get much better performance than otherwise.In fact, the model obtains similar performance as if it was conditioned on the optimal goal, despite the fact that in most realistic scenarios, we do not have access to that optimal goal condition.We have also explored different conditioning methods and observed that the traditional return-to-go was the weakest strategy.Finally we have seen that training the model to predict the next observation as an auxiliary loss improves performance.For future work, we plan on extending this framework to multiple and more diverse games and environments.We hope this work can provide a missing piece to the substantial advances in the application of large language models in the context of real-world interactive task-oriented dialogues.</p>
<p>Limitations.One limitation of this work is that we did not spend an extensive amount of effort in building high-quality online RL agents to train our offline agent.This is intended because we use Jericho games as a proxy for real-world chat agents helping people solve problems, and in such environments training a descent online agent is impractical as people would find it very challenging to interact with live RL agents.Another limiting factor of this work is the fact that due to computing resource limitations, intermediate states were replaced with fixed tokens.Earlier experiments tried to encode them with a frozen encoder network but results were inconclusive.Further research in long-context Transformers will eventually alleviate this limitation.Eventually, at current capacity, our models are unable to generalize to unseen games in zero-shot settings.</p>
<p>A Training Details</p>
<p>B Results on All Games</p>
<p>This section reports the results of models trained on all game trajectories at the same time (33 games ×1005 trajectories).Models are then evaluated on each individual games.</p>
<p>B.1 The Effect of Exponential Tilt</p>
<p>Similarly as in Section 5.1, we fine-tuned our model with the loss function described in Equation 3 on all generated trajectories split into input and output pairs (Section 4).The model was trained with the regular return-to-go goal condition (g t = T i=t r i ) and with λ = 0.5 for the auxiliary loss of predicting o t+1 .We tested the model on all games, normalized the obtained score based on the maximum human score for each game, and recorded the average across games and 5 random seeds for each game.Predicted GC / alpha=1</p>
<p>Predicted GC / alpha=10</p>
<p>Predicted GC / alpha=20 TDQN (Hausknecht et al., 2020) DRRN (Hausknecht et al., 2020) MPRC-DQN (Guo et al., 2020) RC-DQN (Guo et al., 2020) Figure 4: Average normalized score across all Jericho games with various amounts of exponential tilt ("Predicted GC" lines).We also report the performance of a model being conditioned on the optimal goal according to each game's maximum score ("Optimal GC" line).The average normalized score of various baselines is depicted in dotted lines.</p>
<p>We can observe the same conclusions as in Section 5.1: (1) the numerical value of the goal condition has indeed an effect on the quality of the next generated answer, (2) it is possible to recover the same performance as the "optimal" goal conditioning by increasing the amount of exponential tilt without knowing the game's maximum score, and (3) our offline method beats previous methods with very little training.Note: fewer baselines are reported than in Section 5.1 because many previous work do not report their performance on so many different environments.</p>
<p>B.2 The Effect of Goal Conditioning Strategies</p>
<p>Similarly as in Section 5.2, we fine-tuned 4 models with the loss function described in Equation 3 on all generated trajectories split into input and output pairs (Section 4).Each model was trained with a different goal condition (Section 2.2) and with λ = 0.5 for the auxiliary loss of predicting o t+1 .We tested the models on all games after 60k training steps and recorded the average score across 5 random seeds for each game.We can observe two things: (1) averaged over a wider set of games, all goal conditioning strategies yield similar performances, except Avg.RTG which is lower, and (2) results on enchanter, sorcerer, spellbrkr, spirit and ztuu are weaker than in Table 2.This is because although models are trained for twice as many steps, the training data is 6 times larger, hence the model observed 3 times less interaction from each game compared to our setting in Section 5.2.</p>
<p>B.3 The Effect of Predicting the Next Observation</p>
<p>Similarly as in Section 5.3, we fine-tuned another 4 models, each with a different goal condition similar to the above section, but with the loss function described in Equation 3 with λ = 0.0 for the auxiliary loss of predicting o t+1 .We tested the models on all games after 60k training steps and recorded the average score across 5 random seeds for each game.To compare the effect of the auxiliary loss, we averaged the scores across all goal conditioning methods.</p>
<p>Figure 2 :
2
Figure 2: Our Language Decision Transformer framework.A trajectory of length T is split at a random index t ∈ [0, T − 1].The model encodes the sequence of observations (o), goal conditions (g), and actions (a) up to time step t.The first o 1 and last o t observations are fully written, but to shorten the input sequence, the other intermediate observations are replaced by a special token.The decoder predicts the goal condition g t , action to take a t , and next observation o t+1 .</p>
<p>Other prior work examining text based agents and leveraging LLMs include: The SayCan work of Ahn et al. (2022) using LLMs as a value functions in a reinforcement learning setup for completing tasks in a real world robotics setting; the ReAct work of Yao et al. (2023) examines a prompt based few shot in-context learning solution based on a PaLM-540B model; and Reflexion</p>
<p>Figure 5 :
5
Figure 5: Proportion of trajectory normalized scores.In each sub-figure title, n is the number of trajectories and ms is the maximum score.The X-axis is the normalized score the trajectory achieves.The Y-axis is the proportion of trajectories finishing with that score.</p>
<p>Figure 6 :
6
Figure 6: Proportion of trajectory lengths.In each sub-figure title, n is the number of trajectories.The X-axis is the number of steps in a trajectory.The Y-axis is the proportion of trajectories of that length.</p>
<p>Table 1 :
1
Comparison of different policy training and action selection techniques (adapted from Piché et al. (</p>
<p>Table 2 :
2
Average and best score obtained on each game across 5 random seeds for each goal condition (GC) variation.The bottom row is the normalized average based on the best human score.</p>
<p>Table 3 :
3
Average and best score obtained on each game across 5 random seeds and 4 goal conditioning methods, with (λ = 0.5) and without (λ = 0.0) the auxiliary loss on the prediction of the next observation o t+1 .The bottom row is the normalized average based on the best human score.</p>
<p>Table 4 :
4
Average and best score obtained on each game across 5 random seeds for each goal condition (GC) variation.The bottom row is the normalized average based on the best human score.</p>
<p>https://github.com/microsoft/jericho
These games are also used in previous works, allowing us to compare our results. Most prior work has only evaluated on a subset of the 33 environments. We report in Appendix B results on all 33 games.
https://huggingface.co/google/long-t5-tglobal-base
games seen at training time: enchanter, sorcerer, spellbrkr, spirit, ztuu. We report in Appendix B results on all 33 games.
Games seen at training time: enchanter, sorcerer, spellbrkr, spirit, ztuu. We report in Appendix B results on all 33 games.
C Trajectories StatisticsIn this section, we report the normalized scores (Figure5) and lengths (Figure6) observed in the collection of trajectories collected for each game as described in Section 4.C.1 Trajectories Scores C.2 Trajectories Lengths
Anthony Michael Ahn, Noah Brohan, Yevgen Brown, Omar Chebotar, Byron Cortes, Chelsea David, Chuyuan Finn, Keerthana Fu, Karol Gopalakrishnan, Hausman, arXiv:2204.01691Do as i can, not as i say: Grounding language in robotic affordances. 2022arXiv preprint</p>
<p>Graph constrained reinforcement learning for natural language action spaces. Prithviraj Ammanabrolu, Matthew Hausknecht, International Conference on Learning Representations (ICLR). 2020</p>
<p>How to avoid being eaten by a grue: Structured exploration strategies for textual worlds. Prithviraj Ammanabrolu, Ethan Tien, Matthew Hausknecht, Mark O Riedl, arXiv:2006.074092020arXiv preprint</p>
<p>Case-based reasoning for better generalization in textual reinforcement learning. Mattia Atzeni, Shehzaad Zuzar Dhuliawala, Keerthiram Murugesan, Mrinmaya Sachan, International Conference on Learning Representations (ICLR). 2022</p>
<p>Decision transformer: Reinforcement learning via sequence modeling. Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, Igor Mordatch, Advances in Neural Information Processing Systems (NeurIPS). 2021</p>
<p>On the properties of neural machine translation: Encoder-decoder approaches. Kyunghyun Cho, Bart Van Merriënboer, Dzmitry Bahdanau, Yoshua Bengio, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation (SSST-8). 2014</p>
<p>Textworld: A learning environment for text-based games. Marc-Alexandre Côté, Akos Kádár, Xingdi Yuan, Ben Kybartas, Tavian Barnes, Emery Fine, James Moore, Matthew Hausknecht, Layla El Asri, Mahmoud Adada, Workshop on Computer Games. 2018</p>
<p>Using expectation-maximization for reinforcement learning. Peter Dayan, Geoffrey E Hinton, Neural Computation. 921997</p>
<p>A sequence modelling approach to question answering in text-based games. Greg Furman, Edan Toledo, Jonathan Shock, Jan Buys, Association for Computational Linguistics (ACL), 2022. Dibya Ghosh, Abhishek Gupta. Justin Fu, Ashwin Reddy, Coline Devin, Benjamin Eysenbach, Sergey Levine, OpenReview2019Learning to reach goals without reinforcement learning</p>
<p>LongT5: Efficient text-to-text transformer for long sequences. Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung, Yinfei Yang, American Association for Computational Linguistics2022</p>
<p>Interactive fiction game playing as multi-paragraph reading comprehension with reinforcement learning. Xiaoxiao Guo, Mo Yu, Yupeng Gao, Chuang Gan, Murray Campbell, Shiyu Chang, Conference on Empirical Methods in Natural Language Processing (EMNLP). 2020</p>
<p>Interactive fiction games: A colossal adventure. Matthew Hausknecht, Prithviraj Ammanabrolu, Marc-Alexandre Côté, Xingdi Yuan, Conference on Artificial Intelligence (AAAI). 2020</p>
<p>Deep reinforcement learning with a natural language action space. Ji He, Jianshu Chen, Xiaodong He, Jianfeng Gao, Lihong Li, Li Deng, Mari Ostendorf, 2016Association for Computational Linguistics</p>
<p>Offline reinforcement learning as one big sequence modeling problem. Michael Janner, Qiyang Li, Sergey Levine, Advances in Neural Information Processing Systems (NeurIPS). 2021</p>
<p>. Aviral Kumar, Xue Bin Peng, Sergey Levine, arXiv:1912.134652019Reward-conditioned policies. arXiv preprint</p>
<p>Multi-game decision transformers. Kuang-Huei Lee, Ofir Nachum, Sherry Yang, Lisa Lee, C Daniel Freeman, Sergio Guadarrama, Ian Fischer, Winnie Xu, Eric Jang, Henryk Michalewski, Igor Mordatch, Advances in Neural Information Processing Systems (NeurIPS). 2022</p>
<p>Offline reinforcement learning: Tutorial, review, and perspectives on open problems. Sergey Levine, Aviral Kumar, George Tucker, Justin Fu, arXiv:2005.016432020arXiv preprint</p>
<p>Planning from pixels using inverse dynamics models. Keiran Paster, Sheila A Mcilraith, Jimmy Ba, arXiv:2012.024192020arXiv preprint</p>
<p>Reinforcement learning by reward-weighted regression for operational space control. Jan Peters, Stefan Schaal, International Conference on Machine learning (ICML). 2007</p>
<p>Probabilistic planning with sequential monte carlo methods. Alexandre Piché, Valentin Thomas, Cyril Ibrahim, Yoshua Bengio, Chris Pal, International Conference on Learning Representations (ICLR). 2019</p>
<p>A probabilistic perspective on reinforcement learning via supervised learning. Alexandre Piché, Rafael Pardinas, David Vazquez, Christopher Pal, In ICLR Workshop on Generalizable Policy Learning in Physical World. 2022</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI blog. 1892019</p>
<p>Reinforcement learning upside down: Don't predict rewards-just map them to actions. Juergen Schmidhuber, arXiv:1912.028752019arXiv preprint</p>
<p>Reflexion: Language agents with verbal reinforcement learning. Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao, 2023</p>
<p>Pre-trained language models as prior knowledge for playing text-based games. Ishika Singh, Gargi Singh, Ashutosh Modi, arXiv:2107.084082021arXiv preprint</p>
<p>Multi-stage episodic control for strategic exploration in text games. Jens Tuyls, Shunyu Yao, M Sham, Kakade, Karthik R Narasimhan, International Conference on Learning Representations (ICLR). 2022</p>
<p>Graph attention networks. Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, Yoshua Bengio, International Conference on Learning Representations (ICLR). 2018</p>
<p>ScienceWorld: Is your agent smarter than a 5th grader?. Ruoyao Wang, Peter Jansen, Marc-Alexandre Côté, Prithviraj Ammanabrolu, 10.18653/v1/2022.emnlp-main.775Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational LinguisticsDecember 2022</p>
<p>Transformers: State-of-the-art natural language processing. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Clara Patrick Von Platen, Yacine Ma, Julien Jernite, Canwen Plu, Teven Xu, Sylvain Le Scao, Mariama Gugger, Quentin Drame, Alexander M Lhoest, Rush, Conference on Empirical Methods in Natural Language Processing: System Demonstrations (EMNLP). 2020</p>
<p>Deep reinforcement learning with stacked hierarchical attention for text-based games. Yunqiu Xu, Meng Fang, Ling Chen, Yali Du, Joey Tianyi Zhou, Chengqi Zhang, Advances in Neural Information Processing Systems. 2020</p>
<p>Keep calm and explore: Language models for action generation in text-based games. Shunyu Yao, Rohan Rao, Matthew Hausknecht, Karthik Narasimhan, arXiv:2010.029032020arXiv preprint</p>
<p>React: Synergizing reasoning and acting in language models. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Yuan Karthik R Narasimhan, Cao, The Eleventh International Conference on Learning Representations. 2023</p>
<p>and without (λ = 0.0) the auxiliary loss on the prediction of the next observation o t+1 . The bottom row is the normalized average based on the best human score. We can observe the same conclusions as in Section 5.3: models trained to predict the next observation o t+1 resulting from the predicted action a t and goal condition g t perform better than models trained to only predict the goal condition g t and next action a t . Overall, these results show that our proposed model-based reward-conditioned policy (MB-RCP). 32.78% 54.45% 50.78% 7378% Table 5: Average and best score obtained on each game across 5 random seeds and 4 goal conditioning methods, with (λ = 0.5). Average Normalized. learning objective yields stronger performance than the classical reward-conditioned policy (RCP) objective</p>            </div>
        </div>

    </div>
</body>
</html>
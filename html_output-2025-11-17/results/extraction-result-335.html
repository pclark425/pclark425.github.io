<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-335 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-335</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-335</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-15.html">extraction-schema-15</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <p><strong>Paper ID:</strong> paper-273502218</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2410.15863v2.pdf" target="_blank">Task-oriented Robotic Manipulation with Vision Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Vision Language Models (VLMs) play a crucial role in robotic manipulation by enabling robots to understand and interpret the visual properties of objects and their surroundings, allowing them to perform manipulation based on this multimodal understanding. Accurately understanding spatial relationships remains a non-trivial challenge, yet it is essential for effective robotic manipulation. In this work, we introduce a novel framework that integrates VLMs with a structured spatial reasoning pipeline to perform object manipulation based on high-level, task-oriented input. Our approach is the transformation of visual scenes into tree-structured representations that encode the spatial relations. These trees are subsequently processed by a Large Language Model (LLM) to infer restructured configurations that determine how these objects should be organised for a given high-level task. To support our framework, we also present a new dataset containing manually annotated captions that describe spatial relations among objects, along with object-level attribute annotations such as fragility, mass, material, and transparency. We demonstrate that our method not only improves the comprehension of spatial relationships among objects in the visual environment but also enables robots to interact with these objects more effectively. As a result, this approach significantly enhances spatial reasoning in robotic manipulation tasks. To our knowledge, this is the first method of its kind in the literature, offering a novel solution that allows robots to more efficiently organize and utilize objects in their surroundings.</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e335.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e335.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TreeReorg-GPT4o</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-driven hierarchical tree reorganization using GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pipeline where a Large Language Model (GPT-4o) receives an explicit, text-encoded hierarchical tree of objects (nodes), directed spatial relations (edges), and object attributes, and produces a transformed hierarchical tree that reorganizes object relationships to satisfy a high-level manipulation task (e.g., "stack the book").</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A general large language model (GPT-4o) used as a reasoning engine that consumes an explicit symbolic representation of a scene (textual hierarchical tree + attributes) and produces a reorganized tree encoding the desired target arrangement. No special fine-tuning for action generation is described; the model is prompted with the tree and a task description.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Tabletop task-oriented reorganization</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given a textual hierarchical tree encoding objects, directed spatial relations (limited to "on"/"on top of") and per-object attributes (fragility, mass, material, transparency), GPT-4o must output a transformed hierarchical tree that reorganizes the same objects (each used once) into a configuration satisfying a high-level instruction (e.g., "stack the book"). The LLM operates without direct visual input and must rely solely on the provided symbolic scene and attributes.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>object manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial+object-relational (hierarchical spatial relations + object properties); enables limited procedural reasoning insofar as reorganized tree implies manipulation plan</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pre-trained LLM weights + in-context task prompt; explicit structured scene input (hierarchical tree and attributes) constructed from manual captions and VLM attribute extraction</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>task prompting (single task prompt provided the tree and instruction); described as using the provided prompt to perform reorganization (no few-shot examples or fine-tuning reported)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>explicit symbolic representation: triplet subject-predicate-object converted to a directed hierarchical tree (text); per-node object attribute labels (fragility, mass, material, transparency) provided as text</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>not reported</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>The LLM can use explicit hierarchical relations and object attributes encoded in text to infer a logically consistent target arrangement (restructured tree) for task-oriented reorganizations; works with tree-structured, dependency-style spatial encodings (e.g., root = lowest-supporting surface, children = supported objects).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>No quantitative failures reported; limitations noted include restricted relation scope (only "on"/"on top of"), reliance on manually annotated relations, and absence of visual grounding which may prevent resolving ambiguities or generalizing beyond provided symbolic inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>none reported</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>none reported</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Providing an LLM with an explicit, symbolic hierarchical representation of spatial relations plus object attributes enables the model to reason about reorganization for manipulation tasks even without direct sensory input; the structured tree serves as an effective intermediate representation for planning-like outputs from a text-only LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Task-oriented Robotic Manipulation with Vision Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e335.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e335.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VLM-Attr</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fine-tuned Vision-Language Model for object attribute extraction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A fine-tuned vision-language model (inspired by Gao et al.) is used to extract per-object physical attributes — fragility, mass, material, transparency — from detected object crops to inform manipulation reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>fine-tuned Vision-Language Model (VLM) (inspired by Gao et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A fine-tuned VLM that processes detected object images to output attribute labels (fragility, mass, material, transparency). It is used as a perception-to-symbols module that converts visual object evidence into text attributes that populate nodes in the hierarchical tree.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Object attribute extraction for manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>For each detected object in synthetic tabletop scenes, the VLM predicts physical attributes (fragility, mass, material, transparency) that are used jointly with spatial relations to plan safe and task-appropriate reorganization.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>object perception / object-relational annotation</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>object-relational (physical properties / affordances)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>fine-tuning on perception-to-language supervision (paper cites inspiration from Gao et al.); visual input from object detection pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>fine-tuned VLM inference on object image crops</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>explicit textual attribute labels attached to object nodes (natural language descriptors produced by the VLM), i.e., symbolic attributes derived from visual input</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>not reported</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Enables downstream reasoning by providing relevant object properties (e.g., fragility) that inform organization decisions (e.g., avoid stacking heavy on fragile), as claimed qualitatively.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>No quantitative error analysis provided; potential failure modes include attribute misclassification and lack of coverage of other affordances; reliance on simulated images and manual spatial labels limits evaluation of robustness in real-world perception.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>none reported</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>none reported</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Converting visual object information into concise textual attributes allows an LLM to incorporate object-relational knowledge into symbolic scene reasoning; a VLM-as-attribute-extractor is a practical pipeline component for supplying object properties when the reasoning model lacks direct vision.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Task-oriented Robotic Manipulation with Vision Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e335.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e335.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SpatialTree</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Triplet-based hierarchical tree representation of spatial relations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A representation method that converts manually written spatial captions into subject-predicate-object triplets and then assembles them into a directed hierarchical tree (root = lowest supporting object, children = objects on top) that encodes scene spatial dependencies for LLM input.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>NLP parsing + tree construction (textual symbolic representation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An NLP pipeline parses human-authored captions describing pairwise spatial relations (limited to "on" and "on top of") into triplets and enforces rules (no cycles) to produce a tree where nodes contain object labels and attribute lists and edges denote support/containment relations.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Symbolic scene encoding for downstream reorganization</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Create an explicit, acyclic directed hierarchical tree from captioned pairwise spatial relations and object attributes; used as the sole environment description input to an LLM, representing scene geometry and object dependencies without raw images.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>representation for object manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial + object-relational (explicit symbolic spatial relations + per-object attributes)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>manual captions parsed to triplets; VLM-derived attributes appended to nodes; explicit symbolic encoding rather than implicit model weights</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>explicit conversion from natural-language captions to triplet relations and deterministic tree construction rules</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>explicit symbolic tree / scene graph-like structure serialized as text (triplets and hierarchical parent-child edges), with node-level attribute annotations</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>not reported</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Provides an interpretable, structured input that an LLM can use to plan reorganizations; disambiguates support relationships by imposing root/child conventions and cycle-avoidance rules.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Limited expressivity (only "on"/"on top of" relations), reliance on manual annotations for relations (labor-intensive), and possible inability to represent complex continuous spatial relations (distances, orientations) or occlusion.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>none reported</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>none reported</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Explicitly encoding pairwise spatial relations as triplets and assembling them into a hierarchical tree creates a compact, symbolic scene description that enables a language model to perform arrangement reasoning without raw sensory inputs; this representation bridges perception (attributes) and planning (reorganized tree outputs).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Task-oriented Robotic Manipulation with Vision Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Spatialvlm: Endowing vision-language models with spatial reasoning capabilities <em>(Rating: 2)</em></li>
                <li>Spatialrgpt: Grounded spatial reasoning in vision language models <em>(Rating: 2)</em></li>
                <li>Physically grounded vision-language models for robotic manipulation <em>(Rating: 2)</em></li>
                <li>PaLM-E: An embodied multimodal language model <em>(Rating: 2)</em></li>
                <li>RT-2: Vision-language-action models transfer web knowledge to robotic control <em>(Rating: 2)</em></li>
                <li>Reasoning paths with reference objects elicit quantitative spatial reasoning in large vision-language models <em>(Rating: 1)</em></li>
                <li>Object-centric instruction augmentation for robotic manipulation <em>(Rating: 1)</em></li>
                <li>Minigpt-4: Enhancing vision-language understanding with advanced large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-335",
    "paper_id": "paper-273502218",
    "extraction_schema_id": "extraction-schema-15",
    "extracted_data": [
        {
            "name_short": "TreeReorg-GPT4o",
            "name_full": "LLM-driven hierarchical tree reorganization using GPT-4o",
            "brief_description": "A pipeline where a Large Language Model (GPT-4o) receives an explicit, text-encoded hierarchical tree of objects (nodes), directed spatial relations (edges), and object attributes, and produces a transformed hierarchical tree that reorganizes object relationships to satisfy a high-level manipulation task (e.g., \"stack the book\").",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4o",
            "model_size": null,
            "model_description": "A general large language model (GPT-4o) used as a reasoning engine that consumes an explicit symbolic representation of a scene (textual hierarchical tree + attributes) and produces a reorganized tree encoding the desired target arrangement. No special fine-tuning for action generation is described; the model is prompted with the tree and a task description.",
            "task_name": "Tabletop task-oriented reorganization",
            "task_description": "Given a textual hierarchical tree encoding objects, directed spatial relations (limited to \"on\"/\"on top of\") and per-object attributes (fragility, mass, material, transparency), GPT-4o must output a transformed hierarchical tree that reorganizes the same objects (each used once) into a configuration satisfying a high-level instruction (e.g., \"stack the book\"). The LLM operates without direct visual input and must rely solely on the provided symbolic scene and attributes.",
            "task_type": "object manipulation",
            "knowledge_type": "spatial+object-relational (hierarchical spatial relations + object properties); enables limited procedural reasoning insofar as reorganized tree implies manipulation plan",
            "knowledge_source": "pre-trained LLM weights + in-context task prompt; explicit structured scene input (hierarchical tree and attributes) constructed from manual captions and VLM attribute extraction",
            "has_direct_sensory_input": false,
            "elicitation_method": "task prompting (single task prompt provided the tree and instruction); described as using the provided prompt to perform reorganization (no few-shot examples or fine-tuning reported)",
            "knowledge_representation": "explicit symbolic representation: triplet subject-predicate-object converted to a directed hierarchical tree (text); per-node object attribute labels (fragility, mass, material, transparency) provided as text",
            "performance_metric": "not reported",
            "performance_result": null,
            "success_patterns": "The LLM can use explicit hierarchical relations and object attributes encoded in text to infer a logically consistent target arrangement (restructured tree) for task-oriented reorganizations; works with tree-structured, dependency-style spatial encodings (e.g., root = lowest-supporting surface, children = supported objects).",
            "failure_patterns": "No quantitative failures reported; limitations noted include restricted relation scope (only \"on\"/\"on top of\"), reliance on manually annotated relations, and absence of visual grounding which may prevent resolving ambiguities or generalizing beyond provided symbolic inputs.",
            "baseline_comparison": "none reported",
            "ablation_results": "none reported",
            "key_findings": "Providing an LLM with an explicit, symbolic hierarchical representation of spatial relations plus object attributes enables the model to reason about reorganization for manipulation tasks even without direct sensory input; the structured tree serves as an effective intermediate representation for planning-like outputs from a text-only LLM.",
            "uuid": "e335.0",
            "source_info": {
                "paper_title": "Task-oriented Robotic Manipulation with Vision Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "VLM-Attr",
            "name_full": "Fine-tuned Vision-Language Model for object attribute extraction",
            "brief_description": "A fine-tuned vision-language model (inspired by Gao et al.) is used to extract per-object physical attributes — fragility, mass, material, transparency — from detected object crops to inform manipulation reasoning.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "fine-tuned Vision-Language Model (VLM) (inspired by Gao et al.)",
            "model_size": null,
            "model_description": "A fine-tuned VLM that processes detected object images to output attribute labels (fragility, mass, material, transparency). It is used as a perception-to-symbols module that converts visual object evidence into text attributes that populate nodes in the hierarchical tree.",
            "task_name": "Object attribute extraction for manipulation",
            "task_description": "For each detected object in synthetic tabletop scenes, the VLM predicts physical attributes (fragility, mass, material, transparency) that are used jointly with spatial relations to plan safe and task-appropriate reorganization.",
            "task_type": "object perception / object-relational annotation",
            "knowledge_type": "object-relational (physical properties / affordances)",
            "knowledge_source": "fine-tuning on perception-to-language supervision (paper cites inspiration from Gao et al.); visual input from object detection pipeline",
            "has_direct_sensory_input": true,
            "elicitation_method": "fine-tuned VLM inference on object image crops",
            "knowledge_representation": "explicit textual attribute labels attached to object nodes (natural language descriptors produced by the VLM), i.e., symbolic attributes derived from visual input",
            "performance_metric": "not reported",
            "performance_result": null,
            "success_patterns": "Enables downstream reasoning by providing relevant object properties (e.g., fragility) that inform organization decisions (e.g., avoid stacking heavy on fragile), as claimed qualitatively.",
            "failure_patterns": "No quantitative error analysis provided; potential failure modes include attribute misclassification and lack of coverage of other affordances; reliance on simulated images and manual spatial labels limits evaluation of robustness in real-world perception.",
            "baseline_comparison": "none reported",
            "ablation_results": "none reported",
            "key_findings": "Converting visual object information into concise textual attributes allows an LLM to incorporate object-relational knowledge into symbolic scene reasoning; a VLM-as-attribute-extractor is a practical pipeline component for supplying object properties when the reasoning model lacks direct vision.",
            "uuid": "e335.1",
            "source_info": {
                "paper_title": "Task-oriented Robotic Manipulation with Vision Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "SpatialTree",
            "name_full": "Triplet-based hierarchical tree representation of spatial relations",
            "brief_description": "A representation method that converts manually written spatial captions into subject-predicate-object triplets and then assembles them into a directed hierarchical tree (root = lowest supporting object, children = objects on top) that encodes scene spatial dependencies for LLM input.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "NLP parsing + tree construction (textual symbolic representation)",
            "model_size": null,
            "model_description": "An NLP pipeline parses human-authored captions describing pairwise spatial relations (limited to \"on\" and \"on top of\") into triplets and enforces rules (no cycles) to produce a tree where nodes contain object labels and attribute lists and edges denote support/containment relations.",
            "task_name": "Symbolic scene encoding for downstream reorganization",
            "task_description": "Create an explicit, acyclic directed hierarchical tree from captioned pairwise spatial relations and object attributes; used as the sole environment description input to an LLM, representing scene geometry and object dependencies without raw images.",
            "task_type": "representation for object manipulation",
            "knowledge_type": "spatial + object-relational (explicit symbolic spatial relations + per-object attributes)",
            "knowledge_source": "manual captions parsed to triplets; VLM-derived attributes appended to nodes; explicit symbolic encoding rather than implicit model weights",
            "has_direct_sensory_input": false,
            "elicitation_method": "explicit conversion from natural-language captions to triplet relations and deterministic tree construction rules",
            "knowledge_representation": "explicit symbolic tree / scene graph-like structure serialized as text (triplets and hierarchical parent-child edges), with node-level attribute annotations",
            "performance_metric": "not reported",
            "performance_result": null,
            "success_patterns": "Provides an interpretable, structured input that an LLM can use to plan reorganizations; disambiguates support relationships by imposing root/child conventions and cycle-avoidance rules.",
            "failure_patterns": "Limited expressivity (only \"on\"/\"on top of\" relations), reliance on manual annotations for relations (labor-intensive), and possible inability to represent complex continuous spatial relations (distances, orientations) or occlusion.",
            "baseline_comparison": "none reported",
            "ablation_results": "none reported",
            "key_findings": "Explicitly encoding pairwise spatial relations as triplets and assembling them into a hierarchical tree creates a compact, symbolic scene description that enables a language model to perform arrangement reasoning without raw sensory inputs; this representation bridges perception (attributes) and planning (reorganized tree outputs).",
            "uuid": "e335.2",
            "source_info": {
                "paper_title": "Task-oriented Robotic Manipulation with Vision Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Spatialvlm: Endowing vision-language models with spatial reasoning capabilities",
            "rating": 2,
            "sanitized_title": "spatialvlm_endowing_visionlanguage_models_with_spatial_reasoning_capabilities"
        },
        {
            "paper_title": "Spatialrgpt: Grounded spatial reasoning in vision language models",
            "rating": 2,
            "sanitized_title": "spatialrgpt_grounded_spatial_reasoning_in_vision_language_models"
        },
        {
            "paper_title": "Physically grounded vision-language models for robotic manipulation",
            "rating": 2,
            "sanitized_title": "physically_grounded_visionlanguage_models_for_robotic_manipulation"
        },
        {
            "paper_title": "PaLM-E: An embodied multimodal language model",
            "rating": 2,
            "sanitized_title": "palme_an_embodied_multimodal_language_model"
        },
        {
            "paper_title": "RT-2: Vision-language-action models transfer web knowledge to robotic control",
            "rating": 2,
            "sanitized_title": "rt2_visionlanguageaction_models_transfer_web_knowledge_to_robotic_control"
        },
        {
            "paper_title": "Reasoning paths with reference objects elicit quantitative spatial reasoning in large vision-language models",
            "rating": 1,
            "sanitized_title": "reasoning_paths_with_reference_objects_elicit_quantitative_spatial_reasoning_in_large_visionlanguage_models"
        },
        {
            "paper_title": "Object-centric instruction augmentation for robotic manipulation",
            "rating": 1,
            "sanitized_title": "objectcentric_instruction_augmentation_for_robotic_manipulation"
        },
        {
            "paper_title": "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
            "rating": 1,
            "sanitized_title": "minigpt4_enhancing_visionlanguage_understanding_with_advanced_large_language_models"
        }
    ],
    "cost": 0.0096025,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Task-oriented Robotic Manipulation with Vision Language Models ⋆</p>
<p>Nurhan Bulus Guran 
Department of Computer Science
Swansea University
SwanseaUnited Kingdom</p>
<p>Hanchi Ren 
Department of Computer Science
Swansea University
SwanseaUnited Kingdom</p>
<p>Jingjing Deng 
Department of Computer Science
Durham University
DurhamUnited Kingdom</p>
<p>Xianghua Xie x.xie@swansea.ac.uk 
Department of Computer Science
Swansea University
SwanseaUnited Kingdom</p>
<p>Task-oriented Robotic Manipulation with Vision Language Models ⋆
51F839F047312915D643E2D7457097FARobotic Manipulation • Vision Language Models(VLMs)
Vision Language Models (VLMs) play a crucial role in robotic manipulation by enabling robots to understand and interpret the visual properties of objects and their surroundings, allowing them to perform manipulation based on this multimodal understanding.Accurately understanding spatial relationships remains a non-trivial challenge, yet it is essential for effective robotic manipulation.In this work, we introduce a novel framework that integrates VLMs with a structured spatial reasoning pipeline to perform object manipulation based on highlevel, task-oriented input.Our approach is the transformation of visual scenes into tree-structured representations that encode the spatial relations.These trees are subsequently processed by a Large Language Model (LLM) to infer restructured configurations that determine how these objects should be organised for a given high-level task.To support our framework, we also present a new dataset containing manually annotated captions that describe spatial relations among objects, along with object-level attribute annotations such as fragility, mass, material, and transparency.We demonstrate that our method not only improves the comprehension of spatial relationships among objects in the visual environment but also enables robots to interact with these objects more effectively.As a result, this approach significantly enhances spatial reasoning in robotic manipulation tasks.To our knowledge, this is the first method of its kind in the literature, offering a novel solution that allows robots to more efficiently organize and utilize objects in their surroundings.</p>
<p>Introduction</p>
<p>Robotic manipulation using VLMs represents a new powerful paradigm [1].The robotic manipulation term describes a robot's capacity to physically interact with items in its surroundings in order to carry out operations tasks in daily life like moving or rearranging [2].</p>
<p>Understanding spatial relationships between objects and their surroundings is vital for robots in many fields, ranging from domestic assistance to industrial automation, as it enables them to function independently in dynamic, realworld environments.VLMs, like MiniGPT-4 [3] and LLaVA [4], have emerged as a promising method for improving robotic manipulation by enabling robots to comprehend and reason about the objects they interact with.The ability of VLMs to interpret objects' properties and to understand spatial relationships is essential for effective manipulation, as it allows robots to better perceive and interact with their environment.However, current VLMs fall short in fully comprehending spatial relationships, which are fundamental for successful robotic manipulation tasks.</p>
<p>Recent advancements in robotic manipulation have shown promising results in improving the robot's ability to recognize and interact with objects [5].While these methods have shown promising results, they suffer from significant limitations.VLMs can describe objects and their surroundings, but their ability to understand spatial relationships between objects remains limited.Additionally, existing datasets often lack the comprehensive annotations needed to capture object properties such as fragility, mass, material, and transparency, which are essential for reasoning about how objects should be manipulated [6].This gap hinders the development of robotic systems capable of sophisticated spatial reasoning, such as determining the correct organization or arrangement of objects in cluttered environments.</p>
<p>The limitations we observed in existing approaches relate to both the datasets and the techniques employed in the current research.The inability of robots to comprehend intricate relationships between objects results from insufficiently developed spatial reasoning abilities in VLMs and the lack of understanding of crucial object attributes in datasets.Furthermore, whole-scene descriptions, where object relationships are not represented, are often insufficient to capture the nuances of spatial dynamics, limiting the overall effectiveness of VLMs in robotic tasks.</p>
<p>In order to address these issues, the following question will be posed in this research: Can the representation of spatial relationships between objects, together with their corresponding properties, be transformed into structures that enhance a robot's ability to organize and manipulate objects in real-world environments?</p>
<p>To overcome these difficulties, we introduce a new dataset that is intended to enhance spatial reasoning.About 600 images were generated, each showing different objects organized on a desk.After each object is detected, important physical attributes including mass, fragility, material, and transparency are determined using an optimized Vision Language Model (VLM) [7].When determining the proper way for the manipulation of objects, these qualities offer vital information.In addition to detecting the objects and determining the attributes, we also manually annotated the images in order to depict the spatial relationships among the objects.Our main focus is on identifying the positional relationships that define their relative positions.In our proposed method, a tree structure is automatically created based on these captions, with nodes representing the ob-jects and edges indicating their spatial relationships.These tree structures are then fed into a language model together with the properties of the item to direct the reorganization of the objects into a more structured state according to a specified task.</p>
<p>Our main contributions are as follows:</p>
<p>• We introduce a new dataset of 600 images with manually annotated object attributes and spatial relationships to improve spatial reasoning in robotic tasks.• We propose a new method that transforms hierarchical structures representing object relationships into new hierarchical structures according to the given high-level task description.• Enhanced spatial reasoning in robotic manipulation, particularly improving the understanding and handling of complex spatial relationships.</p>
<p>Related Work</p>
<p>The ability to reason about spatial relationships is crucial for robots to effectively interact with and manipulate objects in their environment.While recent advancements in Language Models have shown promising capabilities, several limitations remain.</p>
<p>Spatial Reasoning in Language Models</p>
<p>SpatialVLM [8] focuses on endowing VLMs with explicit spatial reasoning capabilities, allowing them to answer queries about spatial arrangements in scenes.Similarly, SpatialRGPT [9] explores grounded spatial reasoning in VLMs, enabling them to interpret spatial relationships from visual input, which is essential for scene understanding.Liao et al. [10] delve into quantitative spatial reasoning in VLMs, examining their ability to reason about object sizes and distances, demonstrating the potential for more precise spatial understanding.These works collectively demonstrate the growing ability of Large Language Models (LLMs) to process and reason about spatial information, forming a foundation for integrating language-based spatial understanding into robotic systems.However current approaches often struggle with explicitly representing spatial relationships in a structured manner.They lack the explicit scene representation needed for planning how to change the arrangement of objects.</p>
<p>Language Models for Robotics</p>
<p>The application of LLMs to robotics has shown significant promise in enabling robots to perform complex tasks from natural language instructions.Driess et al. [11] introduce PaLM-E, an embodied multimodal language model that integrates language models with visual and robotic control, demonstrating capabilities in mobile manipulation and task and motion planning.RT-2 [12] further showcases the power of vision-language-action models for robotic control, allowing robots to leverage web knowledge to perform a variety of manipulation tasks based on language commands.Wen et al. [13] explore object-centric instruction augmentation, using multimodal LLMs to enrich language instructions with object location information, improving robotic manipulation performance.These studies highlight the potential of LLMs to bridge the gap for robot actions, enabling more intuitive and flexible robotic control.They produce robot actions from multimodal inputs.Instead of directly generating actions, our approach leverages LLMs to reason about how the spatial relationships need to change to satisfy a given prompt.This provides a crucial intermediate step for action generation and allows for more nuanced and effective robotic manipulation compared to methods that directly map language to actions or focus solely on spatial description or question answering.</p>
<p>Problem Statement</p>
<p>In robotic manipulation tasks, the ability to accurately understand and interpret the spatial relationships between objects is essential for effective operation, particularly in tasks such as organizing or assembling objects in specific configurations.While VLMs have shown promise in integrating visual and linguistic information to understand scenes, they often fail to capture complex spatial relationships.The current state-of-the-art VLMs rely heavily on pre-trained models that are not specifically designed to address the nuances of object interaction and spatial reasoning required for robotic tasks.Furthermore, existing datasets and models tend to overlook the physical properties of objects, which are crucial in determining how robots should interact with them [14].This limitation hinders the ability of robots to perform tasks that require precise manipulation based on object properties and spatial context.To advance robotic manipulation skills, it is imperative to bridge the gap between scene interpretation and action-oriented spatial reasoning.</p>
<p>In order to overcome this, our research suggests developing a unique dataset and approach that are intended to improve VLMs' spatial reasoning capabilities, particularly for robotic applications.Our goal is to increase the precision and effectiveness of robotic manipulation in dynamic, task-driven environments by integrating important object properties and hierarchical structures that depict spatial relationships.By addressing the issue of insufficient spatial thinking in existing VLMs, this work aims to pave the way for the development of more efficient robotic systems that can perform intricate manipulation tasks.</p>
<p>Fig. 1.Overview of our framework: Objects are first detected and their attributes are extracted using a fine-tuned VLM [7].Spatial relationships between objects are manually described and used to build tree structures representing these relationships.These tree structures are first combined with object attributes.Then this combination with a task-oriented prompt fed into a language model to generate a new representation according to the given task.</p>
<p>manipulation in task-specific contexts, we propose a comprehensive framework that integrates object detection, attribute assignment, and hierarchical spatial relationship modeling.The overall approach is illustrated in Figure 1.</p>
<p>Dataset</p>
<p>A dataset comprising images was generated, each depicting various objects arranged differently on a tabletop surface.To create this dataset, we utilized Blender [15], an open source 3D modeling and rendering software, to simulate and render synthetic scenes featuring diverse object configurations.These images served as the foundation for analyzing the spatial relationships among the objects.An object detection model [16] was employed to identify all objects within each image.</p>
<p>Following detection, and inspired by the work of Gao et al. [7], each detected object was processed using a fine-tuned VLM to extract its key physical attributes.Specifically, the model identified properties such as fragility, representing the degree to which an object is delicate and requires careful handling; mass, referring to the object's weight, which influences the force needed during ma-nipulation; transparency, indicating the level of opacity or clarity, which affects how the object interacts with light; and material, describing the substance from which the object is made, playing a significant role in determining its behavior under various physical conditions.These attributes are essential for informing manipulation strategies in task-oriented scenarios, enabling the robotic system to adapt its actions according to the specific characteristics of each object.</p>
<p>Task-Based Reorganization and Model Application</p>
<p>After detecting the objects and assigning their attributes, spatial relationship captions were manually written for each image.These captions described the positioning of each object relative to the others.The relationships between the detected objects are captured through captions associated with the images.We utilized a natural language processing (NLP) pipeline to extract these relations.This process involves analyzing the captions provided for each image, where objects and their spatial arrangements are described in natural language.The sentences were parsed and the spatial descriptions were converted into triplets that encode spatial relations in a subject-predicate-object format.</p>
<p>These triplets were then used to construct a hierarchical tree structure, where each object corresponds to a node containing its attributes, and directed edges represent the relationships between nodes.The process of constructing the tree followed a systematic approach.For example, if an "on top of" relationship was found between two objects, a directed edge was drawn from the object at the bottom (e.g., a table) to the object placed on top of it (e.g., a book), reflecting the physical arrangement between them.This approach results in a hierarchical tree structure, where the object at the lowest level in the scene, such as a table, serves as the root node, while the objects situated above it are connected as dependent child nodes.Each edge represents a meaningful spatial relationship, ensuring the hierarchy accurately mirrors the real-world arrangement of objects.The structure is designed with rules to avoid cycles and maintain logical connections, ensuring that no object can be both a parent and a child in the same loop, preserving the tree's integrity and preventing circular relationships.This tree structure provided a detailed representation of the spatial arrangement and physical properties of the objects.</p>
<p>The hierarchical tree structure, representing the spatial arrangements and physical properties of objects was provided as text input to the GPT-4o language model [17].The model was prompted with a specific tabletop manipulation task (e.g., "stack the book"), and its objective was to reorganize the initial tree structure accordingly, producing a more logically structured arrangement that fulfils the given task using only the provided object attributes and spatial information.Without direct visual input, the model relied solely on the hierarchical tree structure.Lacking any visual information about the environment, the model used only the provided hierarchical tree structure along with the attributes of the objects to perform the reorganization.The prompt given to the model was based on a scenario where a robotic arm, unable to perceive its environment visually, uses the provided tree structure to understand object relationships and perform a task.The prompt specified that the model could only use the given objects' labels and their respective properties to make logical decisions regarding their organization.The output expected from the model was a transformed tree structure representing the final state of the environment, with the same number of objects, ensuring each object was used only once.This process allowed the model to more efficient and coherent arrangement of the objects, resulting in a new hierarchical tree structure that better reflected the desired outcome.</p>
<p>This task-driven reorganization process utilized the initial and transformed hierarchical tree structures generated by the language model to derive a structured manipulation plan, simulating the rearrangement strategies a robot might employ in real-world scenarios.</p>
<p>Evaluation</p>
<p>To assess the effectiveness of our approach, we evaluate its performance using the proposed dataset, which is specifically designed to enhance spatial reasoning in robotic manipulation tasks.The dataset consists of synthetic images depicting various tabletop layouts with randomly arranged objects.These objects exhibit physical characteristics commonly found in real-world scenarios and are intended to improve the robot's understanding of spatial relationships and task-oriented manipulation.Sample images from the dataset are shown in Figure 2.</p>
<p>Object Understanding and Spatial Annotation</p>
<p>To prepare the visual data for spatial reasoning, we first employed the object detection [16] to identify and classify each object in the image.All objects in the images were successfully detected and assigned a unique identifier for further analysis.</p>
<p>Following detection, VLM [7] was used to extract key physical attributes of each object.These include material, fragility, mass, and transparency-properties essential for informed manipulation in task-oriented scenarios.For instance, a fragile object would require careful handling, while a transparent object might influence the robot's visual perception and decision-making [14].By incorporating both visual recognition and attribute extraction in a unified pipeline, this stage provides a comprehensive representation of each object, forming a foundational step for the spatial reasoning and task execution components of our framework.This representation allows the robot to make context-aware decisions, directly contributing to its ability to interact meaningfully with its environment.</p>
<p>Once the individual objects and their attributes were extracted, spatial relationships between each object pair were manually annotated.The scope of relationships was intentionally limited to "on" and "on top of" to ensure clarity and consistency throughout the dataset.These relations provide foundational information for understanding object dependencies and spatial hierarchies.To capture this structure, human-generated captions were created to describe the spatial configuration in each image.These captions, along with the retrieved  object attributes, were used to construct hierarchical tree structures that explicitly encode the spatial dependencies among objects.These trees serve as an interpretable and actionable guide for task-driven robotic manipulation.</p>
<p>Hierarchical Tree Structure</p>
<p>In constructing the hierarchical tree structure, a key priority was to develop a representation that is not only straightforward and comprehensible but also sufficiently detailed to provide meaningful input for language models.To achieve this, specific rules were defined to govern the organization of the hierarchy.</p>
<p>According to these guidelines, the object positioned at the lowest level of the spatial arrangement is designated as the root (parent) node, while subordinate nodes are assigned progressively toward the object located at the highest posi- This study represents a significant advancement in enhancing the robot's reasoning ability based on a given task prompt.By incorporating a hierarchical tree structure, the model not only organized objects more effectively but also demonstrated a sophisticated understanding of how objects should be arranged according to specific tasks and goals (as illustrated in Figure 3 and Figure 4).</p>
<p>In many VLMs, spatial relationships between objects are not fully understood, which limits their reasoning capabilities.However, the hierarchical structure we implemented enabled the language model to reason more effectively and accomplish the given tasks.This approach significantly improved the robot's ability to adapt to task requirements while interacting with its environment in a structured and safe manner.</p>
<p>The methodology provides a framework for advancing spatial reasoning capabilities for robotic manipulation tasks.It provides a robust foundation for further research into improving the autonomy and problem-solving capabilities of robots, paving the way for more advanced systems that can adapt to complex and dynamic environments with greater autonomy.</p>
<p>Conclusions</p>
<p>We introduce a new approach to robotic manipulation that addresses current limitations.By developing a new dataset that includes object details along with manually defined spatial relationships, we provide a comprehensive framework for improving robotic interaction with objects in a task-oriented context.Using a hierarchical tree structure to represent these relationships enables more accurate and context-aware manipulations when integrated into a language model.By closing a critical gap in the literature on spatial reasoning in robotic manipulation, our method provides a scalable and adaptable solution for a wide range of applications.Furthermore, by making our dataset and code publicly available, we aim to encourage collaboration within the research community and enable further advances in the field.</p>
<p>Fig. 2 .
2
Fig. 2. Sample synthetic images randomly arranged objects for task-oriented robotic manipulation from the dataset.</p>
<p>Fig. 3 .
3
Fig. 3. Task Simulation Examples.Initial images represent the generated images, while the second set illustrates the simulation results.</p>
<p>Fig. 4 .
4
Fig. 4.An example of initial and transformed hierarchical tree structures.</p>
<p>MethodologyIn this section, we present the details of the proposed methodology for enhancing spatial reasoning capabilities in robotic manipulation tasks. To improve robotic
Acknowledgements.Nurhan Bulus Guran is funded by Turkish Ministry of National Education, Republic of Türkiye.This work is supported by the EPSRC National Edge AI Hub (EP/Y007697/1).
Manipulate-anything: Automating real-world robots using vision-language models. J Duan, W Yuan, W Pumacay, Y R Wang, K Ehsani, D Fox, R Krishna, arXiv:2406.189152024arXiv preprint</p>
<p>Trends and challenges in robot manipulation. A Billard, D Kragic, Science. 364644684142019</p>
<p>Minigpt-4: Enhancing visionlanguage understanding with advanced large language models. D Zhu, J Chen, X Shen, X Li, M Elhoseiny, arXiv:2304.105922023arXiv preprint</p>
<p>Visual instruction tuning. H Liu, C Li, Q Wu, Y J Lee, Advances in neural information processing systems. 202436</p>
<p>A survey of embodied learning for object-centric robotic manipulation. Y Zheng, L Yao, Y Su, Y Zhang, Y Wang, S Zhao, Y Zhang, L.-P Chau, arXiv:2408.115372024arXiv preprint</p>
<p>Interactive learning of physical object properties through robot manipulation and database of object measurements. A Kruzliak, J Hartvich, S P Patni, L Rustler, J K Behrens, F J Abu-Dakka, K Mikolajczyk, V Kyrki, M Hoffmann, 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEEOct. 2024</p>
<p>Physically grounded vision-language models for robotic manipulation. J Gao, B Sarkar, F Xia, T Xiao, J Wu, B Ichter, A Majumdar, D Sadigh, 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE2024</p>
<p>Spatialvlm: Endowing vision-language models with spatial reasoning capabilities. B Chen, Z Xu, S Kirmani, B Ichter, D Driess, P Florence, D Sadigh, L Guibas, F Xia, 2024</p>
<p>Spatialrgpt: Grounded spatial reasoning in vision language models. A.-C Cheng, H Yin, Y Fu, Q Guo, R Yang, J Kautz, X Wang, S Liu, 2024</p>
<p>Reasoning paths with reference objects elicit quantitative spatial reasoning in large vision-language models. Y.-H Liao, R Mahmood, S Fidler, D Acuna, 2024</p>
<p>Palm-e: An embodied multimodal language model. D Driess, F Xia, M S Sajjadi, C Lynch, A Chowdhery, B Ichter, A Wahid, J Tompson, Q Vuong, T Yu, arXiv:2303.033782023arXiv preprint</p>
<p>Rt-2: Vision-language-action models transfer web knowledge to robotic control. A Brohan, N Brown, J Carbajal, Y Chebotar, X Chen, K Choromanski, T Ding, D Driess, A Dubey, C Finn, arXiv:2307.158182023arXiv preprint</p>
<p>Object-centric instruction augmentation for robotic manipulation. J Wen, Y Zhu, M Zhu, J Li, Z Xu, Z Che, C Shen, Y Peng, D Liu, F Feng, J Tang, 2024</p>
<p>Robotic perception of transparent objects: A review. J Jiang, G Cao, J Deng, T.-T Do, S Luo, 2023</p>
<p>Blender -a 3d modelling and rendering package. Online Blender, Community, 2023</p>
<p>. G Jocher, A Chaurasia, R Stoken, J Borovec, A Nanocode012, S Kwon, J Fang, J Fang, L Yu, T Laughing, A Hogan, I Imyhxy, A Suleiman, N Kharshiladze, F Ballesteros, L Bronk, 2023</p>
<p>Large language model. Openai, 2023. September 24, 2024Gpt-4</p>            </div>
        </div>

    </div>
</body>
</html>
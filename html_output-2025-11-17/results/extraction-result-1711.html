<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1711 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1711</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1711</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-33.html">extraction-schema-33</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <p><strong>Paper ID:</strong> paper-259643747</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2309.07974v1.pdf" target="_blank">A Data Source for Reasoning Embodied Agents</a></p>
                <p><strong>Paper Abstract:</strong> Recent progress in using machine learning models for reasoning tasks has been driven by novel model architectures, large-scale pre-training protocols, and dedicated reasoning datasets for fine-tuning. In this work, to further pursue these advances, we introduce a new data generator for machine reasoning that integrates with an embodied agent. The generated data consists of templated text queries and answers, matched with world-states encoded into a database. The world-states are a result of both world dynamics and the actions of the agent. We show the results of several baseline models on instantiations of train sets. These include pre-trained language models fine-tuned on a text-formatted representation of the database, and graph-structured Transformers operating on a knowledge-graph representation of the database. We find that these models can answer some questions about the world-state, but struggle with others. These results hint at new research directions in designing neural reasoning models and database representations. Code to generate the data will be released at github.com/facebookresearch/neuralmemory</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1711.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1711.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 2 (GPT-2, HuggingFace variant used)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pretrained autoregressive language model (GPT-2) that the paper fine-tunes on a textified representation of a simulated 3D gridworld in order to answer property, temporal and geometric questions about the environment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language Models are Unsupervised Multitask Learners</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>GPT-2 small (fine-tuned on Sequence Context)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Pretrained autoregressive transformer (GPT-2) used to read a flattened, templated text dump of the environment (Sequence Context) concatenated with the text query, and to generate the text answer tokens sequentially. The paper used the HuggingFace GPT-2 model and fine-tuned it on (C_t, Q_t) pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Large-scale internet text (unsupervised language modeling)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Pretrained GPT-2 weights from Radford et al. (2019) as provided via the HuggingFace library; paper reports using a GPT-2 small variant (paper states model size 325.89M parameters). The original GPT-2 was trained with unsupervised LM objectives on WebText-like internet text corpora (Radford et al. 2019).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>3D gridworld QA / embodied reasoning dataset (this paper's environment)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>A procedurally generated 3D gridworld (experiments commonly used a 15x15x15 world with 1 agent, 1 player, 4 NPCs) where scenes are recorded as time-stamped snapshots and converted to (context, query, answer) triples. Queries probe property, temporal, and geometric reasoning about the agent and objects (e.g., closest object, who moved the farthest, property queries). The task for the model is to predict text answers given the textified world state and a question.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>None (pretraining was standard language modeling over token sequences; no explicit action semantics in pretraining).</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Discrete scripted environment actions exist in the simulator (move, build, destroy, follow, command) and objects change positions; however the ML task is question answering over recorded states — the models do not output low-level motor commands in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>None in these experiments. GPT-2 was fine-tuned to read textual memories and output answers; the paper did not map high-level text semantics to embodied low-level control for acting.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>None for the models evaluated: the experiments abstract away perception by providing either a full templated text dump of the world-state or a structured relational memory; visual perceptual modules were not required.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>Pretrained GPT-2 (fine-tuned) achieved substantially better results than a randomly initialized GPT-2 on the paper's All-queries experiments: reported test error (exact-match error) for pretrained GPT-2 variants: ~14.9% test error (GPT-2 small + pretrain, Table 2, test loss 0.710).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>Randomly initialized GPT-2 (same architecture but no pretraining) performed much worse: reported test error 47.1% (Table 2, test loss 1.912). A Structured Context + Transformer baseline (trained from scratch) had test error ~19.7% (relational embedding variant) on the All-queries dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>Pretraining provided substantial sample-efficiency benefits: the paper reports experiments over training set sizes from 1k to 1M and notes that the pretrained GPT-2 learns much faster and significantly outperforms non-pretrained models when labeled-data is small; nevertheless, best results generally required on the order of >=100k fine-tuning examples (paper states: 'Both models require more than 100K training samples to achieve the best results').</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td>Non-pretrained GPT-2 failed to learn effectively with the same finetuning regime (very high test error at comparable data sizes); the structured Transformer trained from scratch still required large numbers of examples (order 100k+) to approach best performance.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td>Qualitative large gain: pretraining reduced error from 47.1% to ~14.9% on the same task/scale (All-queries) and yielded much better performance at small finetuning set sizes; the paper does not provide a single numeric 'x-fold' sample reduction but reports that pretrained GPT-2 requires far fewer labeled examples to reach competitive error than the randomly initialized LM and that both models only reach best performance past ~100k examples.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>1) Close match between the flattened textual memory format and LM's token-based input (textification of world state), 2) large-scale unsupervised pretraining provided language understanding and pattern priors that accelerate learning from limited supervised scene-QA examples, 3) shared vocabulary/tokenizer enabling reuse of pretrained embeddings and LM layers.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>1) Spatial/geometric reasoning remains difficult for flat LMs — GPT-2 struggled on many geometric queries, 2) context length limitations (GPT-2's 1024 token window) cause missing information when scenes grow, 3) no explicit action-control mapping was implemented so model is not a controller, 4) perception-to-language gap (vision was abstracted away here, limiting applicability to raw perceptual input).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Pretrained language models (GPT-2) can be effectively fine-tuned to answer textified, embodied-environment QA by leveraging their language priors and achieve substantially better sample efficiency and lower error than the same architecture trained from scratch; however, pretraining is not a panacea — spatial and temporal geometric reasoning remain challenging, and context-length and representation mismatches limit transfer as scene complexity grows.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Data Source for Reasoning Embodied Agents', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1711.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1711.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Ahn et al. 2022 (Do As I Can)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mentioned as a recent work showing how large language model pretraining can be used to influence planners for embodied agents; the paper cites it as related work but does not report experimental details.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>Not specified in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Paper only references this work in the introduction as an example where LM pretraining was leveraged for planner/controller behavior in embodied settings; no experimental or architectural details are provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Mentioned as related work demonstrating the broader trend of using language-model pretraining to influence embodied-agent planners; no details in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Data Source for Reasoning Embodied Agents', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1711.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1711.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Huang et al. 2022</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited as an example of prior work using large language model pretraining to inform or affect planners for embodied agents; only referenced in related work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>Not specified in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Only cited at a high level as part of the literature showing LM pretraining applied to embodied-agent planning; no experimental specifics are provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Paper only references this work as evidence of LMs informing embodied planners; details must be obtained from the original source.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Data Source for Reasoning Embodied Agents', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1711.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1711.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Fan et al. 2022 (MineDojo)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MineDojo (as cited in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced as a contemporary work building open-ended embodied agents with internet-scale knowledge and large-scale datasets; mentioned in the context of Minecraft-based agents using large pretraining resources.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>MineDojo: Building Open-Ended Embodied Agents with Internet-Scale Knowledge</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>Not directly used in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>The paper cites MineDojo as a concurrent project that creates a large-scale Minecraft simulation suite and leverages internet-scale data (videos, tutorials, wiki pages) to train embodied agents; the current paper does not use MineDojo models in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Internet-scale multimodal data (videos, tutorials, wiki pages) — cited in the referenced work but not detailed here</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Minecraft-based embodied tasks (as described in the referenced work)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Referenced work focuses on open-ended Minecraft tasks and uses large-scale multimodal pretraining; the present paper cites it to position their work in the landscape of Minecraft/embodied-agent research.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Mentioned to indicate recent trends combining large-scale internet data and Minecraft environments; this paper does not experiment with MineDojo models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Data Source for Reasoning Embodied Agents', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1711.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1711.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Baker et al. 2022 (VPT / video-based agents)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited as a contemporaneous example where video models are trained in Minecraft to predict next actions from scenes; used to show other pretraining-to-embodied examples in literature.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>Not specified in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Referenced work trains video-based models to predict next actions from Minecraft scenes (video pretraining), but the current paper only cites it as related work and does not use the model.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Video-based pretraining (cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Minecraft action prediction (as in the referenced work)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Referenced to illustrate other approaches where pretrained models (video or multimodal) are used for embodied action prediction in Minecraft; not used in experiments here.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as an example of video-pretraining applied to embodied action prediction; the present paper only references it and does not report its results.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Data Source for Reasoning Embodied Agents', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Do As I Can <em>(Rating: 2)</em></li>
                <li>MineDojo: Building Open-Ended Embodied Agents with Internet-Scale Knowledge <em>(Rating: 2)</em></li>
                <li>Video PreTraining (VPT): Learning to Act by Watching Unlabeled Online Videos <em>(Rating: 1)</em></li>
                <li>Language Models are Unsupervised Multitask Learners <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1711",
    "paper_id": "paper-259643747",
    "extraction_schema_id": "extraction-schema-33",
    "extracted_data": [
        {
            "name_short": "GPT-2",
            "name_full": "Generative Pre-trained Transformer 2 (GPT-2, HuggingFace variant used)",
            "brief_description": "A pretrained autoregressive language model (GPT-2) that the paper fine-tunes on a textified representation of a simulated 3D gridworld in order to answer property, temporal and geometric questions about the environment.",
            "citation_title": "Language Models are Unsupervised Multitask Learners",
            "mention_or_use": "use",
            "model_agent_name": "GPT-2 small (fine-tuned on Sequence Context)",
            "model_agent_description": "Pretrained autoregressive transformer (GPT-2) used to read a flattened, templated text dump of the environment (Sequence Context) concatenated with the text query, and to generate the text answer tokens sequentially. The paper used the HuggingFace GPT-2 model and fine-tuned it on (C_t, Q_t) pairs.",
            "pretraining_data_type": "Large-scale internet text (unsupervised language modeling)",
            "pretraining_data_details": "Pretrained GPT-2 weights from Radford et al. (2019) as provided via the HuggingFace library; paper reports using a GPT-2 small variant (paper states model size 325.89M parameters). The original GPT-2 was trained with unsupervised LM objectives on WebText-like internet text corpora (Radford et al. 2019).",
            "embodied_task_name": "3D gridworld QA / embodied reasoning dataset (this paper's environment)",
            "embodied_task_description": "A procedurally generated 3D gridworld (experiments commonly used a 15x15x15 world with 1 agent, 1 player, 4 NPCs) where scenes are recorded as time-stamped snapshots and converted to (context, query, answer) triples. Queries probe property, temporal, and geometric reasoning about the agent and objects (e.g., closest object, who moved the farthest, property queries). The task for the model is to predict text answers given the textified world state and a question.",
            "action_space_text": "None (pretraining was standard language modeling over token sequences; no explicit action semantics in pretraining).",
            "action_space_embodied": "Discrete scripted environment actions exist in the simulator (move, build, destroy, follow, command) and objects change positions; however the ML task is question answering over recorded states — the models do not output low-level motor commands in these experiments.",
            "action_mapping_method": "None in these experiments. GPT-2 was fine-tuned to read textual memories and output answers; the paper did not map high-level text semantics to embodied low-level control for acting.",
            "perception_requirements": "None for the models evaluated: the experiments abstract away perception by providing either a full templated text dump of the world-state or a structured relational memory; visual perceptual modules were not required.",
            "transfer_successful": true,
            "performance_with_pretraining": "Pretrained GPT-2 (fine-tuned) achieved substantially better results than a randomly initialized GPT-2 on the paper's All-queries experiments: reported test error (exact-match error) for pretrained GPT-2 variants: ~14.9% test error (GPT-2 small + pretrain, Table 2, test loss 0.710).",
            "performance_without_pretraining": "Randomly initialized GPT-2 (same architecture but no pretraining) performed much worse: reported test error 47.1% (Table 2, test loss 1.912). A Structured Context + Transformer baseline (trained from scratch) had test error ~19.7% (relational embedding variant) on the All-queries dataset.",
            "sample_complexity_with_pretraining": "Pretraining provided substantial sample-efficiency benefits: the paper reports experiments over training set sizes from 1k to 1M and notes that the pretrained GPT-2 learns much faster and significantly outperforms non-pretrained models when labeled-data is small; nevertheless, best results generally required on the order of &gt;=100k fine-tuning examples (paper states: 'Both models require more than 100K training samples to achieve the best results').",
            "sample_complexity_without_pretraining": "Non-pretrained GPT-2 failed to learn effectively with the same finetuning regime (very high test error at comparable data sizes); the structured Transformer trained from scratch still required large numbers of examples (order 100k+) to approach best performance.",
            "sample_complexity_gain": "Qualitative large gain: pretraining reduced error from 47.1% to ~14.9% on the same task/scale (All-queries) and yielded much better performance at small finetuning set sizes; the paper does not provide a single numeric 'x-fold' sample reduction but reports that pretrained GPT-2 requires far fewer labeled examples to reach competitive error than the randomly initialized LM and that both models only reach best performance past ~100k examples.",
            "transfer_success_factors": "1) Close match between the flattened textual memory format and LM's token-based input (textification of world state), 2) large-scale unsupervised pretraining provided language understanding and pattern priors that accelerate learning from limited supervised scene-QA examples, 3) shared vocabulary/tokenizer enabling reuse of pretrained embeddings and LM layers.",
            "transfer_failure_factors": "1) Spatial/geometric reasoning remains difficult for flat LMs — GPT-2 struggled on many geometric queries, 2) context length limitations (GPT-2's 1024 token window) cause missing information when scenes grow, 3) no explicit action-control mapping was implemented so model is not a controller, 4) perception-to-language gap (vision was abstracted away here, limiting applicability to raw perceptual input).",
            "key_findings": "Pretrained language models (GPT-2) can be effectively fine-tuned to answer textified, embodied-environment QA by leveraging their language priors and achieve substantially better sample efficiency and lower error than the same architecture trained from scratch; however, pretraining is not a panacea — spatial and temporal geometric reasoning remain challenging, and context-length and representation mismatches limit transfer as scene complexity grows.",
            "uuid": "e1711.0",
            "source_info": {
                "paper_title": "A Data Source for Reasoning Embodied Agents",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Ahn et al. 2022 (Do As I Can)",
            "name_full": "",
            "brief_description": "Mentioned as a recent work showing how large language model pretraining can be used to influence planners for embodied agents; the paper cites it as related work but does not report experimental details.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_agent_name": "Not specified in this paper",
            "model_agent_description": "Paper only references this work in the introduction as an example where LM pretraining was leveraged for planner/controller behavior in embodied settings; no experimental or architectural details are provided in this paper.",
            "pretraining_data_type": null,
            "pretraining_data_details": null,
            "embodied_task_name": null,
            "embodied_task_description": null,
            "action_space_text": null,
            "action_space_embodied": null,
            "action_mapping_method": null,
            "perception_requirements": null,
            "transfer_successful": null,
            "performance_with_pretraining": null,
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": null,
            "transfer_failure_factors": null,
            "key_findings": "Mentioned as related work demonstrating the broader trend of using language-model pretraining to influence embodied-agent planners; no details in this paper.",
            "uuid": "e1711.1",
            "source_info": {
                "paper_title": "A Data Source for Reasoning Embodied Agents",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Huang et al. 2022",
            "name_full": "",
            "brief_description": "Cited as an example of prior work using large language model pretraining to inform or affect planners for embodied agents; only referenced in related work.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_agent_name": "Not specified in this paper",
            "model_agent_description": "Only cited at a high level as part of the literature showing LM pretraining applied to embodied-agent planning; no experimental specifics are provided in this paper.",
            "pretraining_data_type": null,
            "pretraining_data_details": null,
            "embodied_task_name": null,
            "embodied_task_description": null,
            "action_space_text": null,
            "action_space_embodied": null,
            "action_mapping_method": null,
            "perception_requirements": null,
            "transfer_successful": null,
            "performance_with_pretraining": null,
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": null,
            "transfer_failure_factors": null,
            "key_findings": "Paper only references this work as evidence of LMs informing embodied planners; details must be obtained from the original source.",
            "uuid": "e1711.2",
            "source_info": {
                "paper_title": "A Data Source for Reasoning Embodied Agents",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Fan et al. 2022 (MineDojo)",
            "name_full": "MineDojo (as cited in paper)",
            "brief_description": "Referenced as a contemporary work building open-ended embodied agents with internet-scale knowledge and large-scale datasets; mentioned in the context of Minecraft-based agents using large pretraining resources.",
            "citation_title": "MineDojo: Building Open-Ended Embodied Agents with Internet-Scale Knowledge",
            "mention_or_use": "mention",
            "model_agent_name": "Not directly used in this paper",
            "model_agent_description": "The paper cites MineDojo as a concurrent project that creates a large-scale Minecraft simulation suite and leverages internet-scale data (videos, tutorials, wiki pages) to train embodied agents; the current paper does not use MineDojo models in experiments.",
            "pretraining_data_type": "Internet-scale multimodal data (videos, tutorials, wiki pages) — cited in the referenced work but not detailed here",
            "pretraining_data_details": null,
            "embodied_task_name": "Minecraft-based embodied tasks (as described in the referenced work)",
            "embodied_task_description": "Referenced work focuses on open-ended Minecraft tasks and uses large-scale multimodal pretraining; the present paper cites it to position their work in the landscape of Minecraft/embodied-agent research.",
            "action_space_text": null,
            "action_space_embodied": null,
            "action_mapping_method": null,
            "perception_requirements": null,
            "transfer_successful": null,
            "performance_with_pretraining": null,
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": null,
            "transfer_failure_factors": null,
            "key_findings": "Mentioned to indicate recent trends combining large-scale internet data and Minecraft environments; this paper does not experiment with MineDojo models.",
            "uuid": "e1711.3",
            "source_info": {
                "paper_title": "A Data Source for Reasoning Embodied Agents",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Baker et al. 2022 (VPT / video-based agents)",
            "name_full": "",
            "brief_description": "Cited as a contemporaneous example where video models are trained in Minecraft to predict next actions from scenes; used to show other pretraining-to-embodied examples in literature.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_agent_name": "Not specified in this paper",
            "model_agent_description": "Referenced work trains video-based models to predict next actions from Minecraft scenes (video pretraining), but the current paper only cites it as related work and does not use the model.",
            "pretraining_data_type": "Video-based pretraining (cited work)",
            "pretraining_data_details": null,
            "embodied_task_name": "Minecraft action prediction (as in the referenced work)",
            "embodied_task_description": "Referenced to illustrate other approaches where pretrained models (video or multimodal) are used for embodied action prediction in Minecraft; not used in experiments here.",
            "action_space_text": null,
            "action_space_embodied": null,
            "action_mapping_method": null,
            "perception_requirements": null,
            "transfer_successful": null,
            "performance_with_pretraining": null,
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": null,
            "transfer_failure_factors": null,
            "key_findings": "Cited as an example of video-pretraining applied to embodied action prediction; the present paper only references it and does not report its results.",
            "uuid": "e1711.4",
            "source_info": {
                "paper_title": "A Data Source for Reasoning Embodied Agents",
                "publication_date_yy_mm": "2023-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Do As I Can",
            "rating": 2,
            "sanitized_title": "do_as_i_can"
        },
        {
            "paper_title": "MineDojo: Building Open-Ended Embodied Agents with Internet-Scale Knowledge",
            "rating": 2,
            "sanitized_title": "minedojo_building_openended_embodied_agents_with_internetscale_knowledge"
        },
        {
            "paper_title": "Video PreTraining (VPT): Learning to Act by Watching Unlabeled Online Videos",
            "rating": 1,
            "sanitized_title": "video_pretraining_vpt_learning_to_act_by_watching_unlabeled_online_videos"
        },
        {
            "paper_title": "Language Models are Unsupervised Multitask Learners",
            "rating": 2,
            "sanitized_title": "language_models_are_unsupervised_multitask_learners"
        }
    ],
    "cost": 0.017964,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>A Data Source for Reasoning Embodied Agents
14 Sep 2023</p>
<p>Jack Lanchantin jacklanchantin@meta.com 
MetaAI</p>
<p>Sainbayar Sukhbaatar 
MetaAI</p>
<p>Gabriel Synnaeve 
MetaAI</p>
<p>Yuxuan Sun yuxuans@meta.com 
MetaAI</p>
<p>Kavya Srinet ksrinet@meta.com 
MetaAI</p>
<p>Arthur Szlam aszlam@meta.com 
MetaAI</p>
<p>A Data Source for Reasoning Embodied Agents
14 Sep 202339317DAA3C99830C67C1558934DC3026arXiv:2309.07974v1[cs.LG]agent s Sequence Context + GPT-2"marbles" Structured Context + Transformer"marbles"
Recent progress in using machine learning models for reasoning tasks has been driven by novel model architectures, large-scale pre-training protocols, and dedicated reasoning datasets for fine-tuning.In this work, to further pursue these advances, we introduce a new data generator for machine reasoning that integrates with an embodied agent.The generated data consists of templated text queries and answers, matched with world-states encoded into a database.The world-states are a result of both world dynamics and the actions of the agent.We show the results of several baseline models on instantiations of train sets.These include pre-trained language models fine-tuned on a text-formatted representation of the database, and graph-structured Transformers operating on a knowledge-graph representation of the database.We find that these models can answer some questions about the world-state, but struggle with others.These results hint at new research directions in designing neural reasoning models and database representations.Code to generate the data will be released at github.com/facebookresearch/neuralmemory.</p>
<p>Introduction</p>
<p>Advances in machine learning (ML) architectures (Vaswani et al. 2017), large datasets and model scaling for pre-training (Radford et al. 2019;Chowdhery et al. 2022;Zhang et al. 2022), modeling approaches (Nye et al. 2021;Wang et al. 2022), and multiple dedicated reasoning datasets (Yang et al. 2018a;Hudson and Manning 2019;Petroni et al. 2020;Vedantam et al. 2021) have driven progress in both building models that can succeed in aspects of "reasoning" and in automatically evaluating such capabilities.This has been evident particularly in the text setting, but also in computer vision (Krishna et al. 2016;Johnson et al. 2017;Yi et al. 2020).</p>
<p>In parallel, the last decade has seen advances in the ability to train embodied agents to perform tasks and affect change in their environments.These have been also powered in part by data, with many environments made available for exploring modeling approaches and benchmarking.In particular, with respect to "reasoning" in embodied agents, there have been works showing that adding inductive biases to support reasoning can lead to improved performance with end-to-end wilson speaker agent marbles pebbles Sequence Context: "agent is at location (10,7,8).agent has pitch 1.0 and yaw 2.5.agent has property agent.speaker is at location (8,7,0).speaker has pitch 0.8 and yaw 2.3.wilson is at location (0,7,9).wilson has pitch -0.8 and yaw 1.1.wilson has property rabbit.wilson has property brown.marbles is at location (10,7,3).marbles has pitch 1.1 and yaw 3.1.marbles has property chicken.marbles has property white.pebbles is at location (10,10,13).pebbles has pitch 0.5 and yaw -0.3.pebbles has property sheep.pebbles has property mottled.gus is at location (3,7,9).gus has pitch 0.4 and yaw -0.1.gus has property cow.gus has property black.inst_seg is at location (3,8,1).inst_seg has property arch.inst_seg is at location (10,8,12).inst_seg has property dome.inst_seg is at location (14,6,6).inst_seg has property hole."</p>
<p>Query: "what is closest from speaker?" Answer: "marbles" "inst_segs" represent the block items such as structures or holes.(Bottom): For a particular scene, we can generate a wide variety of queries.Here we show and example of a distance query asking which object is the closest to the speaker, which is the chicken named "marbles".</p>
<p>training (Zambaldi et al. 2018) and other works have shown how models can be augmented with extra supervision (Zhong, Rocktäschel, and Grefenstette 2019).</p>
<p>Recently, several works have shown how large languagemodel pre-training can be used to affect planners for embodied agents (Huang et al. 2022;Ahn et al. 2022).More generally, symbolic representations can be a hub for connecting perception, memory, and reasoning in embodied agents.</p>
<p>However, the growing literature in NLP reasoning models is missing data grounded in a dynamic and agent-alterable world.Models trained on traditional text datasets struggle to handle physically grounded queries such as those that involve geometric reasoning.In other words, recent large language models trained on internet data are not well equipped to simple questions about a physical environment such as "who is to my left?".Grounding large-language models may allow them more powerful reasoning; and vice versa, may help us use them as agent controllers.</p>
<p>In this work we describe a data source (i.e., a toolbox to generate data) designed to help train ML models grounded in a physical environment, allowing them to make the connection between perception, memory, and reasoning.It consists of context-question-answer triples, where the context corresponds to a dynamic and agent-affected 3d gridworld, and the questions may involve temporal or spatial reasoning, as well as questions about the agent's own actions.A sample generated from our data source is shown in Fig. 1.</p>
<p>While the environment allows rendering the world-context as a sequence of images, one of our goals is to support research toward answering the question "what are good formats for agent memory systems?".In pursuit of this, we abstract the context to a database format that does not require any perceptual modules, and provide code for converting the database into a templated text dump, as demonstrated in Fig. 1 (right top).Here, the order of the facts within each timestep are written randomly, and sequentially written according to the timesteps.Our hope is that the data source can be used for augmenting the training (or allowing the assembly) of reasoning embodied agents by bringing to bear the advances in reasoning in language models, or as a supplement to training language models with grounding from embodied agents.</p>
<p>We train baseline neural models to represent the database and process the queries.These include finetuning pre-trained language models on the text version of the database, and Transformers that input the structured database directly.We find that while certain queries are easily solved by these baselines, others, in particular -those having to deal with spatial geometry, are more difficult.</p>
<p>In short, the contributions of this paper are: Environment: We introduce an environment for embodied agents and a data source for generating data to train agents in this environment (detailed in the Environment, Queries, and Data section).We provide the code to generate world contexts, as well as complex queries.We hope this will aid researchers to isolate and tackle difficult problems for reasoning for embodied agents.Baselines: We evaluate the abilities of baseline models to answer queries in this environment (Experiments section).We compare different representations of the world context, including a pure text based representation as well as a more structured representation.</p>
<p>Environment, Queries, and Data</p>
<p>We propose a context-question-answer data generator for embodied agents.In this section, we outline the context, or environment we generate data in, the types of queries we create for the agent to solve, and specifics of the data samples.</p>
<p>Environment</p>
<p>We work in a finite three-dimensional gridworld.There is a primary agent, zero or more other agents, zero or more get/fetchable items, zero or more placeable/breakable blocks.The other agents come in two types: they might represent human "players" that can give commands to the agent and have the same capabilities as the agent, or animate non-player characters (NPCs) that follow simple random movement patterns.The placeable/breakable blocks have colors and integer grid coordinates; all other objects have float coordinates.Animate objects (the players, NPCs, and the agent) have a yaw and pitch pose representing the location they are looking, in addition to three location coordinates.</p>
<p>To build a scene, we generate some random objects (spheres, cubes, etc.), randomly place a number of NPCs, a player, and an agent.With some probability, the agent executes a command (to either: build an object, destroy an object, move to a location, dig a hole, follow an NPC, etc.) The execution of a command is scripted; the task executor is from the Minecraft agent in (Pratik et al. 2021).Whether or not the agent executes a task, the world steps a fixed number of times (so, e.g., NPCs may move or act).In the experiments described below, the world is fully observed at a fixed number of temporal snapshots, and all poses, object locations and NPC movements are recorded.However, not every world step is snapshotted, so the total sequence is not fully observed.</p>
<p>Following (Pratik et al. 2021), the environment is presented to the agent as an object-centered key-value store.Each object, NPC, and the agent's self have a "memid" keying a data structure that depends on the object type, and may contain string data (for example a name) or float or integer data (e.g. the pose of an NPC).The key-value store also has subjectpredicate-object triples (e.g."memid has tag mottled"); the triples themselves also have a unique memid as key.</p>
<p>The generated scene presented as the key-value store defines the agent's context, C. In our experiments below, we represent this context in one of two ways.The first is a text sequence (C t ), where for each snapshot step, all objects and their properties in the key-value store are flattened into a templated language, as shown in Fig. 1 (right).Multiple time snapshots of the context are represented by appending each successive event in the sequence.While this is in some sense simplistic, it allows ready application of large pre-trained language models for ingesting the context; and this kind of approach has been shown to be effective in (Thorne et al. 2021;Liu et al. 2021).</p>
<p>Alternatively, we can leverage the relational properties of the context by representing it as a graph where objects and properties are nodes, and the connections between them are the edges.For example, if we know that "bob" is a horse and is the color brown, the node for bob connects to a "horse" and a "brown" object.Specifically, this "Sequence Context" contains reference object nodes (C R ), which are the object instantiations, and triple nodes (C T ), which are the properties of the reference objects.Each reference object node holds the following information: reference_object_hash (R_id) is a unique identifier for each reference object, reference_objects_words holds identifier words of the object such as its name, and reference_objects_float is the floating point properties of the where R_id is the unique reference object identifier, and T_id is the unique triple property identifier (which connects to one of the reference objects via R_id).Context nodes (C {R,t} ) and query tokens (Q t ) are first featurized with learnable embedding layers.We process the featurized context and query jointly with a Transformer encoder that considers the context structure via relational embeddings (r ij ).Finally, a text decoder predicts tokens, and a memid decoder predicts relevant context memids (not pictured).object such as its (x, y, z) coordinates and pitch/yaw.These are combined into a single node (detailed in the Models section).Similarly, each property triple is composed of the following elements: triples_hash (T_id) contains a unique identifier for the triple, as well as the reference object hash that it is linked to, and triples_words are a descriptive text of a reference object's property such as "has_color blue".These are also combined into a single node.We therefore have nodes of both reference objects and triples, and the hashes encompass the edges or relationships between them.</p>
<p>We do not consider the text sequence or graph-structured representations to be canonical.On the contrary, our goal is stimulate research into what the correct representation of world state should be to allow easier model training and transfer between agents.We simply provide these two representations as examples and baselines.</p>
<p>Given the determined set of snapshots, queries are designed to be answerable.That is, all information is known to answer the questions.We leave to future work making ambiguous queries, but note here that it is not difficult to purposefully build and record queries that could theoretically be answered in some scene but cannot be answered in the particular scene instantiation, for example because they refer to an event that occurred between snapshots.It would otherwise be easy to restrict full observability within each snapshot.</p>
<p>Queries</p>
<p>The embodied agent environment allows for a rich set of possible queries to ask the agent.We structure the types of queries we use into three main categories, as covered in Table 1.Property queries are those which operate on the current state of the memory such as the current properties or locations of an object, and are given with an explicit relation.Property queries with a single clause can be read directly from the database or text dump without any "reasoning".Temporal queries are those which operate over spans of the memory such as the movement of object.Geometric queries are those concerned with the geometric structure of the environment such as how far away two object are from each other.Note that many queries are mixes of these types, and the categorization is blurred.</p>
<p>Within each query class, there are several different "clause" types.These can be combined as applicable into a multiclause query, where the clauses are combined by an "and" or "or" conjunction randomly.In addition, each clause can be negated by prepending the word "not".For example, "what are the name of the objects that do not have the property brown and where the x coordinate is less than 4".</p>
<p>The query, Q, is composed of one or both of the following representations: query_text (Q t ): a text representation of the query (e.g."find the reference_objects with color brown"), query_tree_logical_form (Q lf ) : a tree logical form representation of the query (e.g. two clauses in an "and" or "or" query are connected by a parent in the tree).</p>
<p>Given the context and query, the agent should return some answer, A. Depending on the query type, we randomly ask for one of the following answer types in the query: name ("what is the name of..."), properties ("what are the properties of..."), location ("what is the location of..."), distance ("how far is..."), and count ("how many...").In general, we are interested in predicting the text answer such as "brown" for the query "what is the color of the horse?".However, we may also be interested in pointing to the relevant context objects or properties (in the structural context representation).Thus, for each query, we provide both the text answer as well as the relevant context node IDs (from C R and C T ).</p>
<p>Data</p>
<p>With this data generation framework, we can create arbitrary amounts of simulated data.Each data sample contains a (C, Q, A) triple.There are several parameters of the world that affect the difficulty of question answering, including the size what is the name of the object that is closest to the cow?
max direction
what is the name of the object that is the most to my right?</p>
<p>distance between how far is the horse from you? distance from position what is the location 3 steps to your right?Table 1: Query clause types.We categorize the queries we can ask the agent into three separate classes.Within each class, there are several clause types.Italicized clauses cannot be combined with others.</p>
<p>of the 3d gridworld (e.g., 15x15x15), the set of possible objects in the world (e.g., bob, alice, ...), and the set of possible properties that can belong to the objects (e.g.cow, horse, blue, green, ...).The number of time-steps and number of snapshots also crucially affect the difficulty of the problem.Similarly, the distributions over the queries (e.g., how many and what kind of clauses) can make the problem more difficult.</p>
<p>Related Work</p>
<p>Real-world QA datasets have long been used to test different aspects of ML model performance such as reading comprehension (Rajpurkar et al. 2016;Hill et al. 2016), commonsense reasoning (Talmor et al. 2019), multi-hop reasoning (Yang et al. 2018b), and visual understanding (Agrawal et al. 2015;Hudson and Manning 2019).While real-world datasets can provide reliable performance benchmarks and better approximate the problems faced by practitioners, synthetic datasets can allow for more control and the ability to isolate the exact limitations of current models.Notably, bAbI (Weston et al. 2016) is a set of toy QA tasks testing various reasoning abilities over short text stories that showed the limitations recurrent neural networks.Since proposed, all bAbI tasks have been solved by novel memory architectures (Henaff et al. 2017;Dehghani et al. 2019).An gridworld environment for embodied agents with language instructions for tasks is described in (Chevalier-Boisvert et al. 2018); our work here is complementary, giving question-answer pairs based on abstracted environment histories.</p>
<p>CLEVR (Johnson et al. 2017) is a popular synthetic data for testing visual reasoning given text queries.(Yi et al. 2020) extends CLEVR to reasoning over temporal events in videos.Embodied question answering (EmbodiedQA) (Das et al. 2018) proposes a task where an agent must navigate an environment in order to answer a question.VideoNavQA (Cangea et al. 2019) was proposed in the EmbodiedQA domain to evaluate short video-question pairs.Our work has elements of each these.The agent is embodied, and might need to answer questions about its actions or hypotheticals, but does not need to act or change the current state of the environment to answer (as in EmbodiedQA).In comparison to the Embod-iedQA dataset where the agent has to query the environment to get more information, our setting doesn't require the agent to interact and get more information.As in (Yi et al. 2020), the agent needs to be able to reason over spatio-temporal events, in our case, including its own actions.As in CLEVR, we use programmatically generated queries to probe various reasoning modalities.One large difference between this work and those is that we do not focus on computer vision.While it is possible to render the scenes from our data generator, our goal is to be agnostic about perceptual modality and abstract away perceptual modeling, and to the extent possible, focus on the reasoning aspects of the data.Within the vision community, other works have approached the VQA problem from this angle (Yi et al. 2018).</p>
<p>Because the agent's abstracted world representation has a database-like structure, our work falls into the literature on ML for question answering on structured data, for example (Pasupat and Liang 2015).Our structured Transformer baseline is inspired by the literature on neural database representation, for example (Wang et al. 2019;Yin et al. 2020), and references therein.Our LM baseline is inspired by the many works that flatten or otherwise textify databases, and use pretrained language models as bases for neural query executors, e.g.(Thorne et al. 2020(Thorne et al. , 2021;;Liu et al. 2021).There are other fundamentally different approaches than these for neural query execution, for example (Ren, Hu, and Leskovec 2020); our hope is that our data source is useful for exploring these.(Tuan et al. 2022) introduce a Transformer to generate responses to questions by reasoning over differentiable knowledge graphs in both task-oriented and domain specific chit-chat dialogues.Our structured neural memory baseline follows works such as (Locatello et al. 2020;Santoro et al. 2018).In this work, the relational "objects' do not need to be discovered by the learning algorithm, and their properties explicitly given to the model to use in featurizing the objects.Our work is most related to the pigpen environment  of (Zellers et al. 2021).In comparison to that work, ours uses more impoverished templated language, but has a larger and more flexible space of queries.The orientation of our work is also different: in (Zellers et al. 2021)  TextWorld (Côté et al. 2018) and and QAit (Yuan et al. 2019) are text-based environments for game play and question answering that require interactive reasoning.The main difference is that our generator is grounded in a 3D gridworld scene, and there is both a user and interactive agent.</p>
<p>We build our data generator on top of the Droidlet agent (Pratik et al. 2021), in part using the grammar in (Srinet et al. 2020) to generate the queries and using the Droidlet agent memory to execute them.This work points the way towards using neural networks to execute the functions of the Droidlet agent memory (and hopefully can be used as a resource for training other agent-memory models).</p>
<p>Experiments</p>
<p>Since the agent's memory or state can take different forms (sequence and structured), we compare two separate models for answering queries about the context of the world.We consider four different datasets for our experiments, as covered in Table 1: Property queries, Temporal queries, Geometric queries, and All queries, where All queries is the three previous categories combined (each query type has roughly the same likelihood of occurring -we provide the configuration files in the code).</p>
<p>Each of these except for Properties (which do not require temporal information) is generated using two time snapshots with 50 world steps, which gives enough steps for actions to occur.Properties queries use one snapshot and zero world steps.For all queries, we place five NPCs in the world, one of which is a "player" that might have given a command.For all query types, we choose the world to be 15x15x15.</p>
<p>Models</p>
<p>We generate data as described in the Environment, Queries, and Data section, and analyze the performance of some baseline models trained on this data.</p>
<p>Text Sequence Context.Since the text sequence form of the context is English text, we use a language model to read the context (C t ) and query (Q t ), and predict the correct answer tokens sequentially (if there are multiple outputs, they are ordered alphabetically).We use the pretrained GPT-2 small model (Radford et al. 2019) from the HuggingFace library (Wolf et al. 2019) (licensed under the Apache License 2.0) to predict all relevant tokens sequentially:
Ŵ = GPT2([C t , Q t ]),(1)
where Ŵ ∈ R L×V is a soft-max normalized matrix of sequence length L and vocabulary size V , and [ ] is the concatenation operation.This model is fine-tuned using a sum of the cross entropy between each token prediction Ŵi and ground truth token W i :
L text = − i∈S j∈V W ij log Ŵij .(2)
Structured Context.While the text Sequence Context is useful in that it allows easy application of standard pretrained models, it may be that for certain tasks other representations are more appropriate.We also show results with simple models that are designed around the relational structure of the context.Given a set of ρ reference objects and τ triples, we first featurize the nodes using a learned convolutional layer given the word, float, and hash values as outlined in the Environment, Queries, and Data section.The output of the featurizer layer gives reference object embeddings C R ∈ R ρ×d , and triple embeddings C T ∈ R τ ×d .Similarly, text queries Q t ∈ R r×d are created using a learned lookup table from the query tokens.We use a Transformer (Vaswani et al. 2017) encoder to process the context and query.The output of the encoder is then used to predict both: the text answer as well as the relevant context memory IDs.We use a Transformer decoder for the text prediction, and a simple linear layer to predict the memory values: The pre-trained GPT-2 model is particularly useful when the number of training samples is small, and when reasoning over proprty queries.On the other hand, non-pre-trained GPT-2 style models do not learn, see Table 2.Both models require more than 100K training samples to achieve the best results.
(C ′ T , C ′ R , Q ′ t ) = Encoder([C T , C R , Q t ])(3)m = MemidDecoder([C ′ R , C ′ T ])(4)ŵ = TextDecoder([C ′ T , C ′ R , Q ′ t ]),(5)
where m ∈ R ρ is predicted relevant memids, and ŵ ∈ R V represents predicted relevant text tokens.This model is trained using a cross-entropy text token loss (eq 2) and cross-entropy memid loss from the relevant memids m ∈ R ρ :
L memid = − C i=1 mi log (m i ) .(6)
The total loss is a weighted sum of the two losses: L text + λ • L memid , where λ = 0.5.</p>
<p>In the Sequence Context, the entities are represented as a set, with defined relationships between them.Therefore, encoding temporal information is not straightforward.To do so, we add a special "time" embedding to each Sequence Context node.That is, for timestep 0, we add the time=0 embedding, for timestep 1, we add the time=1 embedding.</p>
<p>For the Sequence Context, the model will return the relevant memory IDs, a text sequence answer, or both.The text output prediction is shown in Fig. 2.</p>
<p>Relational Embedding.One important form of structure in data is relations where entities are connected to other entities.In our data, for example, triple nodes are connected to object nodes by R_id.Since the vanilla Transformer without positional embeddings treats input tokens as a set, it lacks a mechanism for taking account of this relational information.Thus, we propose a novel way of encoding relational information directly into the self-attention mechanism of the Sequence Context Transformer model.Specifically, we add an extra term to the softmax attention:
a ij = Softmax(q T i k j + q T i r ij )(7)
Here r ij is a relation embedding vector corresponding to the relation type between token i and j.Relation embeddings can be used to encode various types of relations.We note that the commonly used relative position embeddings (Sukhbaatar et al. 2015</p>
<p>Model and Training Details</p>
<p>All our models are trained using Adam (Kingma and Ba 2014) for 5,000 epochs, where each epoch is over a chunk of 10,000 training samples.Since we are generating the data, we vary the training samples from 1k to 1M, and use a validation set of 10k samples.We use a linear warmup of 10,000 steps and cosine decay (Loshchilov and Hutter 2016).For the GPT2 model, we consider learning rates {1e-4, 5e-4, 1e-5} using a batch size of 32.For the structured model, we consider learning rates {1e-4, 5e-4, 1e-5}, batch size 32, layers {2, 3}, and embedding dimensions {256, 512}.Hyperparameters were chosen with arbitrary initial values and increased until validation performance decreased or resources were depleted.The best performing structured model has 74.58M parameters, whereas GPT-2 small has 325.89M.All words are encoded with the GPT-2 tokenizer.</p>
<p>Results</p>
<p>Fig. 3 (left) shows the results for the four different dataset versions we consider.We report the exact match error for all data splits.That is,
Exact Match Error = 1 n n i=1 I y i ̸ = ŷi ,(8)
where y i ∈ R N are the N ground truth tokens for sample i, and ŷi are the top N predicted tokens.Fig. 3 (right) shows the loss curves for the All queries dataset.</p>
<p>For property and geometric queries, the Sequence Context + GPT-2 method performs the best.Since GPT-2 is pre-trained on millions of samples, it can easily learn basic property queries such as "what is the color of bob?".GPT-2 has a near zero test set loss for the properties queries.Geometric queries, while difficult for both models to answer is solved more effectively by GPT-2.that the performance is worse than that of the Structured Context + Transformer.This indicates that the primary reason for the GPT-2 model achieving the best results is due to the pretraining.Table 2 (bottom) shows a Structured Context + Transformer method variation for the All queries dataset.We consider an alternative to the relational embeddings proposed in the Models section, which is adding random hash embeddings to the nodes, or tokens, which are connected to each other in the Sequence Context representation.This method performs slightly worse than relational embeddings, hinting that a more explicit representation of the context structure is important.Fig. 5 shows a 2-snapshot scene from our test set.Finally, Fig. 4 shows the result of our baseline models when using varying amounts of training samples.Since our data source can generate arbitrary amounts of data, it's important to understand how much data is needed to solve certain problems.Notably, most of the datasets require at least 100,000 samples to achieve a reasonable validation loss.In addition, the Sequence + GPT-2 model significantly outperforms the Structured + Transformer model on the datasets with small amounts of training samples.</p>
<p>Conclusion</p>
<p>In this work, we introduced a framework for generating world state contexts with agents, queries, and their answers.This provides researchers with a flexible sandbox for training and testing reasoning in embodied agents.Notably, our sequential data format lets us easily evaluate the ability of large language models such as GPT to understand a physical world.</p>
<p>We show baseline results with two representations of the world state context: a pure text sequence representation that can be used by any off-the-shelf language model, as well as a structured representation of the entities in the world.We demonstrate the ability of the models to answer queries in several query domains, such as temporal or geometric.</p>
<p>We emphasize that our goal in this work is not to create a fixed static data set, but a resource for generating data.Many of our experimental choices (w.r.t.environment and query difficulty) were aimed to allow the baseline models some traction, but not able to solve the tasks completely.</p>
<p>In particular, we find it likely that large pre-trained language models could do better than the GPT-2 base models we used with the described data generation parameters.On the other hand, the difficulty of the problem can be scaled trivially by increasing the number of possible reference ob- ).speaker has pitch -0.4 and yaw 0.6.inst_seg is at location (8,6,4).inst_seg has property hollow_cube.inst_seg is at location (12,6,2).inst_seg has property hole.inst_seg is at location (7,6,4).inst_seg has property hole.agent is at location (13,7,0).agent has pitch -1.1 and yaw -3.1.agent has property agent.agent has property self.axle is at location (1,7,14).axle has pitch 0.5 and yaw 1.4.axle has property chicken.axle has property gray.john is at location (6,7,8).john has pitch -0.8 and yaw -0.5.john has property sheep.john has property white.bart is at location (1,7,12).bart has pitch 0.8 and yaw 2.7.bart has property sheep.bart has property white.honey is at location (5,7,4).honey has pitch -1.3 and yaw -0.5.honey has property chicken.honey has property black.speaker is at location (2,7,9).speaker has pitch -0.4 and yaw 0.6.inst_seg is at location (8,6,4).inst_seg has property hollow_cube.inst_seg is at location (12,6,2).inst_seg has property hole.inst_seg is at location (7,6,4).inst_seg has property hole" jects and properties, by increasing the number of reference objects in the world and increasing the number of instantiated properties; by making the world bigger, and by increasing the time-length of episodes and the number of snapshots recorded.If nothing else, these changes would quickly lead to the context being too large to fit in the memory of a standard LM, and necessitating a large-memory LM (Lample et al. 2019;Rae et al. 2019;Beltagy, Peters, and Cohan 2020); or leading to other approaches e.g.(Ji et al. 2017).Other, more subtle difficulties can be introduced in straightforward ways by reducing full observability, or otherwise asking queries that cannot be answered with the information in the context, requiring agents some amount of meta-cognition and/or environment actions to find the answers, as in (Das et al. 2018).</p>
<p>In future work, we plan to introduce more query types, including arithmetic and hypotheticals.We hope that researchers will use the data generator as a flexible resource for augmenting their own agent training or LM pre-training, or for exploring new models for database reasoning.</p>
<p>Appendix</p>
<p>Scene, Query, and Training Details Scene generation: Figure 6 shows the components of building a scene in our 3d gridworld.Parameters of the scene generator include: world size, number of timesteps, number of agents, number of players, number of NPCs, (names, properties, locations, pitch/yaw of the NPCs and agent), and actions.</p>
<p>In our experiments, we use 1 agent, 1 player, 4 NPCs generated from a set of 254 names (Abigail, Ace, Adam, ...), 5 NPC types (cow,pig,rabbit,chicken,sheep),6 NPC colors (brown,white,black,mottled,pink,yellow), and up to 5 shape types for block objects (hole, cube, hollow_cube, rectanguloid, hollow_rectanguloid, sphere, spherical_shell, pyramid, square, rectangle, circle, disk, triangle, dome, arch, ellipsoid, hollow_trianlge, hollow_rectangle, rectanguloid_frame).At each timestep, the agent, player, and NPCs can perform actions (move, build, destroy, follow, command).Some of these actions create new blocks or destroy existing ones.Note that the actions are not explicitly recorded in the context C. Actions such as "move" are implictly recorded by the object changing its location, building by the existence of new block objects, etc.</p>
<p>Query specifications: Figure 7 shows an overview of the query generation process.Currently there are 13 different clause types, some of which can be combined with others to create multi-clause queries.There are 4 different standard return types for the query: "name" (e.g."what is the name of ..."), "tag" (e.g."what is a tag of ..."), "count" ("how many ... are there "), and "location" (" what is the location of ...").Not every one of these is applicable for each of the 13 clause or query types.Specifically:</p>
<p>• For agent action queries (what did you do?), the return is always an action name, and none of the above entitycentric questions.• For geometric minimax queries (closest object, max direction), the return is always the name or tag of the object.• For all other query types where only one output is possible (based on the query alone, and not on the state or history of the world), count queries are not allowed.</p>
<p>Compute, resources, and availability Each model is trained on an internal cluster, using 4-16 Nvidia V100 GPUs.Training takes roughly 72 hours for each model.The best performing model has 74.58M, whereas the GPT-2 small model has 325.89M parameters.All hyperparameter configurations and PyTorch (Paszke et al. 1912) code will be released to both generate the data and train the models.</p>
<p>Detailed analysis</p>
<p>Memid loss.While the main objective is to predict the text answer, for the structured context representation, we are also interested in predicting relevant context node IDs (i.e."memids").Figure 9 shows the memid loss (L memid ) for the All queries dataset.Note that the GPT-2 model does not operate on the structured memory nodes, so there is no memid loss.This loss is lower than the text loss since the model only has to output the correct IDs from the small set of objects in the world, whereas the model has to predict the correct text tokens from all possible tokens (50,257 for the GPT-2 tokenizer).We observe that without the memid loss, the text prediction performance suffers.</p>
<p>Generating more difficult datasets.In the main manuscript, we show the results of baseline models where every scene has 4 NPCs in a 15x15x15 gridworld.We can generate scenes according to our specifications, making it simple to test the model capabilities on samples of varying difficulty.Figure 10 shows the results when adjusting two parameters of the world for the All queries dataset: larger gridworld size, and more NPCs.The baseline is a 4 NPC, 15x15x15 world.The first variation is a 4 NPC, 30x30x30 world, and the second is a 8 NPC, 15x15x15 world.The performance remains roughly the same for the larger world size.We attribute this to the fact that the models already have trouble solving the geometric queries in the smaller world size, so increasing it doesn't degrade the performance.However, the performance does decrease when we increase the number of NPCs.Notably, the Structured+Transformer model begins to outperform the Sequence+GPT-2 model in this setting.This is likely in part because in a world with more NPCs, the sequence length becomes longer than 1024 tokens, which is the length GPT-2 is trained on.As we clip the context at 1024 tokens, it is not able to answer queries where the relevant information is outside of the 1024 tokens.This highlights one inherent issue with naively using a pre-trained language model on this task.This problem will also be exacerbated when we increase the number of timesteps.However, note the literature on "long-memory" Transformer text models, e.g.(Lample et al. 2019;Rae et al. 2019;Choromanski et al. 2020;Beltagy, Peters, and Cohan 2020;Sukhbaatar et al. 2021).</p>
<p>There exist many similar datasets, with key differences in terms of the types of inputs available, and whether there is an active agent.Table 3 shows a comparison of our dataset vs. existing ones.The key differentiation is that we can represent the data in several forms: visual, relational, and text, as show in in Figure 8.</p>
<p>Out of domain generalization.In real world scenes, new objects and properties may arise at any given time.For example, the agent may encounter an animal with a new color that we've never seen before, but it should still be able to answer queries about it.To understand how the model performs in this setting, we generate a new Properties dataset test set with all new color properties that were not seeing during training (e.g.green , orange).Table 4 shows the results on the original colors Properties dataset vs the new colors Properties dataset.The Sequence+GPT-2 model generalizes significantly better than the Structured+Transformer model that was not pretrained.</p>
<p>Minecraft Setting</p>
<p>Minecraft is a suitable environment for agent reasoning for a variety of reasons.(Szlam et al. 2019) argue that the constraints of the Minecraft world (e.g.3-d voxel grid, simple physics) and the regularities in the head of the distribution of in-game tasks present a rich setting for natural language and world model understanding.Concurrent with our work, two works train video-based agents in the Minecraft setting.(Fan et al. 2022) introduce Minecraft simulation suite with thousands of open-ended tasks and an internet-scale knowledge base with Minecraft videos, tutorials, wiki pages, and forum discussions.They also introduce an agent that uses pretrained video-language models as a reward function.(Baker et al. 2022) train a video-model to predict the next action based on a scene in Minecraft.</p>
<p>Additional Examples</p>
<p>Figure 11 shows more examples of our environment with predictions from the two baseline models.Neither the Sequence+GPT-2 nor Structured+Transformer model are able to accurately answer many of the queries, motivating the need for more advanced modeling techniques.While these examples show basic gridworld examples, we can generate arbitrarily more difficult worlds, as described in Section .These examples demonstrate a small subset of possible worlds and queries generated from our data source.The potential scenes and queries that researchers can create are extensive, and can be carefully tailed toward the specific needs of a particular agent being trained."agent is at location (10,7,8).agent has pitch 1.0 and yaw 2.5.agent has property agent.speaker is at location (8,7,0).speaker has pitch 0.8 and yaw 2.3.wilson is at location (0,7,9).wilson has pitch -0.8 and yaw 1.1.wilson has property rabbit.wilson has property brown.marbles is at location (10,7,3).marbles has pitch 1.1 and yaw 3.1.marbles has property chicken.marbles has property mottled.pebbles is at location (10,10,13).pebbles has pitch 0.5 and yaw -0.3.pebbles has property sheep.pebbles has property mottled.gus is at location (3,7,9).gus has pitch 0.4 and yaw -0.1.gus has property cow.gus has property mottled.inst_seg is at location (3,8,1).inst_seg has property arch.inst_seg is at location (10,8,12).inst_seg has property dome.inst_seg is at location (14,6,6).inst_seg has property hole.inst_seg is at location (12,5,10).inst_seg has property hole."We vary two dataset parameters to make the queries more difficult: increasing the number of NPCs from 4 to 8, and increasing the world size from 15x15x15 to 30x30x30.We observe that as the number of NPCs grows, the queries become more difficult to answer, but the Structured+Transformer model begins to outperform the Sequence+GPT-2 model (on this dataset, the best GPT-2 val loss is 1.055, and the best Transformer val loss is 0.915).PT-2: "honey" ransformer: "honey" Sequence Context: "agent is at location (6,6,6).agent has pitch -0.3 and yaw -0.3.agent has property agent.agent has property self.jenna is at location (14,7,7).jenna has pitch -0.8 and yaw -1.4.jenna has property cow.jenna has property cow.jenna has property white.itsy is at location (0,7,13).itsy has pitch 1.4 and yaw -0.7.itsy has property pig.itsy has property white.itsy-bitsy is at location (11,7,5).itsy-bitsy has pitch 1.1 and yaw -1.4.itsy-bitsy has property pig.itsy-bitsy has property white.arrow is at location (9,7,8).arrow has pitch -0.3 and yaw -1.1.arrow has property rabbit.arrow has property mottled.speaker is at location (10,7,2).speaker has pitch 1.2 and yaw -2.0.inst_seg is at location (10,3,11).inst_seg has property triangle.inst_seg is at location (1,4,9).inst_seg has property hollow_cube.inst_seg is at location (4,5,4).inst_seg has property hole.agent is at location (6,6,6).agent has pitch -0.3 and yaw -0.3.agent has property agent.agent has property self.jenna is at location (14,7,7).jenna has pitch -0.8 and yaw -1.4.jenna has property cow.jenna has property white.itsy is at location (1,7,13).itsy has pitch 1.4 and yaw -0.7.itsy has property pig.itsy has property white.itsy-bitsy is at location (12,7,3).itsy-bitsy has pitch 1.1 and yaw -1.4.itsy-bitsy has property pig.itsy-bitsy has property white.arrow is at location (10,7,8).arrow has pitch -0.3 and yaw -1.1.arrow has property rabbit.arrow has property mottled.speaker is at location (10,7,2).speaker has pitch 1.2 and yaw -2.0.inst_seg is at location (10,3,11).inst_seg has property triangle.inst_seg is at location (1,4,9).inst_seg has property hollow_cube.inst_seg is at location (4,5,4).inst_seg has property hole" Query: "what is closest from speaker?" Answer: "hudson" Sequence Context: "agent is at location (1,7,5).agent has pitch 0.0 and yaw 0.7.agent has property agent.agent has property self.hugh is at location (9,7,3).hugh has pitch 0.9 and yaw -2.4.hugh has property chicken.hugh has property white.honey is at location (2,7,6).honey has pitch 0.1 and yaw -2.8.honey has property sheep.honey has property gray.athena is at location (1,7,11).athena has pitch -0.7 and yaw -3.1.athena has property cow.athena has property white.jewels is at location (5,7,0).jewels has pitch 0.2 and yaw -1.7.jewels has property cow.jewels has property white.speaker is at location (2,7,14).speaker has pitch 1. Query: "what is closest from speaker?" Answer: "hudson" Sequence Context: "agent is at location (10,7,1).agent has pitch 0.2 and yaw -3.0.agent has property agent.agent has property self.jewel is at location (2,7,8).jewel has pitch -1.4 and yaw -2.0.jewel has property cow.jewel has property gray.jester is at location (2,7,10).jester has pitch -0.2 and yaw -2.7.jester has property cow.jester has property brown.biablo is at location (14,7,11).biablo has pitch 0.8 and yaw 0.1.biablo has property chicken.biablo has property gray.sara is at location (7,7,3).sara has pitch 1.4 and yaw 1.8.sara has property sheep.sara has property brown.speaker is at location (11,7,9).speaker has pitch -1.4 and yaw -1.2.inst_seg is at location (10,8,2).inst_seg has property spherical_shell."In this example, both of the "pig" NPCs increased their x value, but the one named "itsy" increased the most.The GPT-2 model could not accurately predict the right answer.Middle: Multi-clause property query.In this example, but "athena" and "jewel" have the property "white", but only athena has a z coordinate greater than 7.The Transformer model could not accurately predict the right answer.Note that in queries such as this which don't require temporal reasoning, we consider the most recent timestep (t=1 in our expriments).Bottom: Geometric query.In this example, the NPC named "biablo" is the farthest from "sara" in the gridworld.</p>
<p>Val Text Loss</p>
<p>Figure 1 :
1
Figure 1: (Top): Example of a generated scene in our 3d gridworld.(Middle): Given the 3d scene, we can convert the information in the render to a text or structured representation.Here we show the text Sequence Context representation."inst_segs"represent the block items such as structures or holes.(Bottom): For a particular scene, we can generate a wide variety of queries.Here we show and example of a distance query asking which object is the closest to the speaker, which is the chicken named "marbles".</p>
<p>Figure 2 :
2
Figure 2: Structured context + Transformer model.The bottom left demonstrates the structured representation of the 3d gridworld,where R_id is the unique reference object identifier, and T_id is the unique triple property identifier (which connects to one of the reference objects via R_id).Context nodes (C {R,t} ) and query tokens (Q t ) are first featurized with learnable embedding layers.We process the featurized context and query jointly with a Transformer encoder that considers the context structure via relational embeddings (r ij ).Finally, a text decoder predicts tokens, and a memid decoder predicts relevant context memids (not pictured).</p>
<p>Figure 3 :
3
Figure 3: (left): Exact match error for the four different generated datasets.Sequence Context + GPT-2 outperform the Structured + Transformer method in all datasets.(middle, right): Loss curves for the All queries dataset.We show the mean loss with min/max error bars over all hyperparameters for the first 1,000 epochs.The pre-trained GPT-2 model learns much faster than the from-scratch relational model.</p>
<p>Figure 4 :
4
Figure4: Validation set exact match error using varying amounts of training data.The pre-trained GPT-2 model is particularly useful when the number of training samples is small, and when reasoning over proprty queries.On the other hand, non-pre-trained GPT-2 style models do not learn, see Table2.Both models require more than 100K training samples to achieve the best results.</p>
<p>"honey" Structured Context + Transformer: "honey" Sequence Context: "agent is at location (13,7,0).agent has pitch -1.1 and yaw -3.1.agent has property agent.agent has property self.axle is at location (0,7,14).axle has pitch 0.5 and yaw 1.4.axle has property chicken.axle has property gray…agent has pitch -1.1 and yaw -3.1.agent has property agent.agent has property self.axle is at location (1,7,14).…""agent is at location (13,7,0).agent has pitch -1.1 and yaw -3.1.agent has property agent.agent has property self.axle is at location (0,7,14).axle has pitch 0.5 and yaw 1.4.axle has property chicken.axle has property gray.john is at location(6,7,8).john has pitch -0.8 and yaw -0.5.john has property sheep.john has property white.bart is at location(2,7,12).bart has pitch 0.8 and yaw 2.7.bart has property sheep.bart has property white.honey is at location (3,7,0).honey has pitch -1.3 and yaw -0.5.honey has property chicken.honey has property black.speaker is at location (2,7,9</p>
<p>Figure 5 :
5
Figure 5: Sample scene with two snapshots.The chicken named "honey" moves from the bottom right toward the center, and some other objects move slightly.Both models correctly answer the query "which object moved the farthest?".</p>
<p>Figure 6 :
6
Figure6: Scene context (C) components.Our data source allows us to create highly modular and variable scenes.We first select a gridworld size, number of timesteps and number of snapshots, and then randomly generate a number of objects in the world, including the agent, the player, NPCs, and blocks.Certain objects can perform actions such as move and build.</p>
<p>Figure 7 :
7
Figure 7: Query Generation Flowchart.italicized clauses in the tree are standalone clauses, and cannot be combined with others.Non-italicized clauses can be combined with conjunctions to form multi-clause queries.Some return types are not compatible with certain clauses.</p>
<p>Figure 8 :
8
Figure 8: Three different data modalities available from our data generator for a single scene.(A) visual: not used in our models since we're primarily interested in learning memory representations.(B) text: flattened text used by the Sequence + GPT-2 model.(C) relational: structured object and property nodes used by the Structured + Transformer model.</p>
<p>Figure 9 :
9
Figure 9: Memid loss for the All queries dataset.This loss is minimized jointly with the text loss for the Structured+Transformer model.The memid prediction task is a simpler task than predicting the text tokens since the model only has to select from objects present in the scene vs all possible tokens.</p>
<p>Figure 1 :Figure 10 :
110
Figure 1: Results from different dataset complexities for the Structured+Transformer model</p>
<p>Query: which object increased x the most?Answer: itsy Sequence + GPT-2: itsy-bitsy Structured</p>
<p>is the name of the object that has the property white and where the z coordinate is greater than 7? "age and yaw 2.5.agent has p sarah has pitch -0.8 and has property brown.hud 1.1 and yaw 3.1.hudson mottled.pebbles is at loc yaw -0.3.pebbles has pr gus is at location (3,7,9).property cow.gus has pr (8,7,0).speaker has pitch (3,8,1).inst_seg has prop inst_seg has property do has property hole.inst_s property hole</p>
<p>Figure 11 :
11
Figure11: Top: Temporal query.In this example, both of the "pig" NPCs increased their x value, but the one named "itsy" increased the most.The GPT-2 model could not accurately predict the right answer.Middle: Multi-clause property query.In this example, but "athena" and "jewel" have the property "white", but only athena has a z coordinate greater than 7.The Transformer model could not accurately predict the right answer.Note that in queries such as this which don't require temporal reasoning, we consider the most recent timestep (t=1 in our expriments).Bottom: Geometric query.In this example, the NPC named "biablo" is the farthest from "sara" in the gridworld.</p>
<p>properties of the objects that have the name alice?tag what are the names of the objects that has the property brown?absolute cardinal what are the locations of the objects where the x coordinate is less than 4? Temporal cardinal what is the name of the object that increased x the most?relative what is the name of the object that moved to my left the most?farthest moved object what is the name of the object that moved the farthest?
Query Class Clause typesExamplePropertyname what are the location at time what was the location of bob at the beginning?object trackingwhere would the ball be if i moved to (4,7,2)?Geometricabsolute distance directionwhat is the count of the objects where the distance to (2, 6, 5) is greater than 3? what are the names of the objects to my right?closest object
action what did you do?</p>
<p>the learner is given only a few labeled QA examples (or dynamics prediction examples) but can use many "unsupervised" state-transitions to build a world-dynamics model.In our work, we allow large numbers of labeled context-query-answer examples; but the difficulty of the queries makes the task non-trivial.</p>
<p>Table 2 (
2
top) shows GPT-2 model variation studies for the All queries dataset.Notably, we test the performance of a Sequence Context + randomly initialized GPT-2 model (i.e. one with the same architecture, but not pretrained).We see
ModelVariationTrain Loss Test Loss Test ErrGPT-2sm + rand init sm + pretrain med + pretrain0.361 0.015 0.0121.912 0.710 0.63547.1% 14.9% 13.8%Transfomerrelational emb random hash0.230 0.6300.921 1.39319.7% 30.0%</p>
<p>Table 2 :
2
Variations of models (sm=small, med=medium).Pretraining is a key component of the GPT-2 model success, even when given large numbers of training examples.Relational embeddings result in a slightly lower test loss than randomhash in the graph-structured models.</p>
<p>Table 3 :
3
Comparison of different QA datasets."<em>" specifies that an unlimited amount of samples can potentially be generated.
Dataset CLEVR (Johnson et al. 2017) EmbodiedQA (Das et al. 2018) bAbI (Weston et al. 2016) BabyAI (Chevalier-Boisvert et al. 2018) TextWorld (Côté et al. 2018) PIGPeN (Zellers et al. 2021) QAit (Yuan et al. 2019) OursAvailable data types Visual Visual Text Text Text Text Text Visual, Text, RelationalEmbodied Agent Dataset size 1M ✓ 9K 20K</em> ✓ 5K<em> 150K 280k 500</em> ✓ 1M*Original ColorsNew ColorsLossExact Match Err LossExact Match ErrSequence+GPT-20.0605.0%0.64716.3%Structured+Transformer 0.0929.4%0.70437.0 %</p>
<p>Table 4 :
4
Out of domain generalization on the Properties dataset.The "Original Colors" test dataset uses the color properties such as "white" that the models were trained on.The "New Colors" test dataset exclusively uses new color properties such as "green" and "purple" that were never seen during training.The Structured+Transformer model sees a significant reduction in performance when introducing new color properties, while the Sequence+GPT model is relatively robust.
Train Text LossTransformer: 4 NPCs, 15x15x15 worldTransformer: 4 NPCs, 15x15x15 worldTransformer: 8 NPCs, 15x15x15 worldTransformer: 8 NPCs, 15x15x15 worldTransformer: 4 NPCs, 30x30x30 worldTransformer: 4 NPCs, 30x30x30 worldGPT-2: 4 NPCs, 15x15x15 worldGPT-2: 4 NPCs, 15x15x15 worldGPT-2: 8 NPCs, 15x15x15 worldGPT-2: 8 NPCs, 15x15x15 world3GPT-2: 4 NPCs, 30x30x30 world3GPT-2: 4 NPCs, 30x30x30 world22LossLoss11002004006008001,000002004006008001,000EpochsEpochs</p>
<p>VQA: Visual Question Answering. A Agrawal, J Lu, S Antol, M Mitchell, C L Zitnick, D Parikh, D Batra, International Journal of Computer Vision. 1232015</p>
<p>Do As I Can. M Ahn, A Brohan, N Brown, Y Chebotar, O Cortes, B David, C Finn, K Gopalakrishnan, K Hausman, A Herzog, arXiv:2204.01691Grounding Language in Robotic Affordances. Not As I Say2022arXiv preprint</p>
<p>Longformer: The long-document transformer. B Baker, I Akkaya, P Zhokhov, J Huizinga, J Tang, A Ecoffet, B Houghton, R Sampedro, J Clune, I Beltagy, M E Peters, A Cohan, arXiv:2206.11795arXiv:2004.05150Video PreTraining (VPT): Learning to Act by Watching Unlabeled Online Videos. 2022. 2020arXiv preprint</p>
<p>Systematic generalization with edge transformers. L Bergen, T O'donnell, D Bahdanau, Advances in Neural Information Processing Systems. 202134</p>
<p>Videonavqa: Bridging the gap between visual and embodied question answering. C Cangea, E Belilovsky, P Liò, A Courville, M Chevalier-Boisvert, D Bahdanau, S Lahlou, L Willems, C Saharia, T H Nguyen, Y Bengio, arXiv:1908.04950arXiv:1810.08272Babyai: A platform to study the sample efficiency of grounded language learning. 2019. 2018arXiv preprint</p>
<p>Rethinking attention with performers. K Choromanski, V Likhosherstov, D Dohan, X Song, A Gane, T Sarlos, P Hawkins, J Davis, A Mohiuddin, L Kaiser, arXiv:2009.14794arXiv:2204.02311Palm: Scaling language modeling with pathways. 2020. 2022arXiv preprint</p>
<p>Textworld: A learning environment for text-based games. M.-A Côté, A Kádár, X Yuan, B Kybartas, T Barnes, E Fine, J Moore, M Hausknecht, L E Asri, M Adada, Workshop on Computer Games. Springer2018</p>
<p>Embodied question answering. A Das, S Datta, G Gkioxari, S Lee, D Parikh, D Batra, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition2018</p>
<p>. M Dehghani, S Gouws, O Vinyals, J Uszkoreit, L Kaiser, ArXiv, abs/1807.038192019Universal Transformers</p>
<p>L Fan, G Wang, Y Jiang, A Mandlekar, Y Yang, H Zhu, A Tang, D.-A Huang, Y Zhu, A Anandkumar, arXiv:2206.08853MineDojo: Building Open-Ended Embodied Agents with Internet-Scale Knowledge. 2022arXiv preprint</p>
<p>The Goldilocks Principle: Reading Children's Books with Explicit Memory Representations. M Henaff, J Weston, A D Szlam, A Bordes, Y Lecun, F Hill, A Bordes, S Chopra, J Weston, W Huang, P Abbeel, D Pathak, I Mordatch, arXiv:2201.07207Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents. D A Hudson, C D Manning, 2017. 2016. 2022. 2019. 2019arXiv preprintIEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</p>
<p>Y Ji, C Tan, S Martschat, Y Choi, N A Smith, arXiv:1708.00781Dynamic entity representations in neural language models. 2017arXiv preprint</p>
<p>CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning. J Johnson, B Hariharan, L Van Der Maaten, L Fei-Fei, C L Zitnick, R B Girshick, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2017. 2017</p>
<p>D P Kingma, J Ba, arXiv:1412.6980Adam: A method for stochastic optimization. 2014arXiv preprint</p>
<p>Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations. R Krishna, Y Zhu, O Groth, J Johnson, K Hata, J Kravitz, S Chen, Y Kalantidis, L Li, D A Shamma, M S Bernstein, L Fei-Fei, G Lample, A Sablayrolles, M Ranzato, L Denoyer, H Jégou, Q Liu, B Chen, J Guo, Z Lin, J Lou, CoRR, abs/2107.07653TAPEX: Table Pre-training via Learning a Neural SQL Executor. 2016. 2019. 202132Advances in Neural Information Processing Systems</p>
<p>Object-centric learning with slot attention. F Locatello, D Weissenborn, T Unterthiner, A Mahendran, G Heigold, J Uszkoreit, A Dosovitskiy, T Kipf, Advances in Neural Information Processing Systems. 202033</p>
<p>SGDR: Stochastic Gradient Descent with Restarts. I Loshchilov, F Hutter, M Nye, A J Andreassen, G Gur-Ari, H Michalewski, J Austin, D Bieber, D Dohan, A Lewkowycz, M Bosma, D Luan, arXiv:2112.00114Show Your Work: Scratchpads for Intermediate Computation with Language Models. 2016. 2021arXiv preprint</p>
<p>Compositional Semantic Parsing on Semi-Structured Tables. P Pasupat, P Liang, A Paszke, S Gross, F Massa, A Lerer, J Bradbury, G Chanan, T Killeen, Z Lin, N Gimelshein, L Antiga, arXiv:1912.01703Pytorch: An imperative style, highperformance deep learning library. 2015. 1912. 2019arXiv preprintACL</p>
<p>KILT: a benchmark for knowledge intensive language tasks. F Petroni, A Piktus, A Fan, P Lewis, M Yazdani, N De Cao, J Thorne, Y Jernite, V Karpukhin, J Maillard, arXiv:2009.02252IEEE International Conference on Robotics and Automation (ICRA). 2020. 2021. 2021IEEEarXiv preprintdroidlet: modular, heterogenous, multi-modal agents</p>
<p>Language models are unsupervised multitask learners. A Radford, J Wu, R Child, D Luan, D Amodei, I Sutskever, OpenAI blog. 1892019</p>
<p>Compressive transformers for long-range sequence modelling. J W Rae, A Potapenko, S M Jayakumar, T P Lillicrap, arXiv:1911.055072019arXiv preprint</p>
<p>SQuAD: 100,000+ Questions for Machine Comprehension of Text. P Rajpurkar, J Zhang, K Lopyrev, P Liang, EMNLP. 2016</p>
<p>Query2box: Reasoning over knowledge graphs in vector space using box embeddings. H Ren, W Hu, J Leskovec, arXiv:2002.059692020arXiv preprint</p>
<p>Relational recurrent neural networks. A Santoro, R Faulkner, D Raposo, J Rae, M Chrzanowski, T Weber, D Wierstra, O Vinyals, R Pascanu, T Lillicrap, Advances in neural information processing systems. 201831</p>
<p>Self-Attention with Relative Position Representations. P Shaw, J Uszkoreit, A Vaswani, NAACL-HLT. 2018</p>
<p>CraftAssist instruction parsing: Semantic parsing for a voxel-world assistant. K Srinet, Y Jernite, J Gray, A Szlam, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational Linguistics2020</p>
<p>Not All Memories are Created Equal: Learning to Forget by Expiring. S Sukhbaatar, D Ju, S Poff, S Roller, A D Szlam, J Weston, A Fan, S Sukhbaatar, J Weston, R Fergus, Advances in neural information processing systems. 2021. 201528ICML</p>
<p>A Szlam, J Gray, K Srinet, Y Jernite, A Joulin, G Synnaeve, D Kiela, H Yu, Z Chen, S Goyal, arXiv:1907.09273Why Build an Assistant in Minecraft?. 2019arXiv preprint</p>
<p>Com-monsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge. A Talmor, J Herzig, N Lourie, J Berant, ArXiv, abs/1811.009372019</p>
<p>. J Thorne, M Yazdani, M Saeidi, F Silvestri, S Riedel, A Halevy, arXiv:2010.069732020Neural databases. arXiv preprint</p>
<p>J Thorne, M Yazdani, M Saeidi, F Silvestri, S Riedel, A Halevy, arXiv:2106.01074Database reasoning over text. 2021arXiv preprint</p>
<p>Y.-L Tuan, S Beygi, M Fazel-Zarandi, Q Gao, A Cervone, W Y Wang, arXiv:2203.10610Towards Large-Scale Interpretable Knowledge Graph Reasoning for Dialogue Systems. 2022arXiv preprint</p>
<p>Attention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, Ł Kaiser, I Polosukhin, Advances in neural information processing systems. 2017</p>
<p>Rat-sql: Relation-aware schema encoding and linking for text-to-sql parsers. R Vedantam, A Szlam, M Nickel, A Morcos, B M Lake, Pmlr, B Wang, R Shin, X Liu, O Polozov, M Richardson, X Wang, J Wei, D Schuurmans, Q Le, E Chi, D Zhou, J Weston, A Bordes, S Chopra, T Mikolov, L Debut, V Sanh, J Chaumond, C Delangue, A Moi, P Cistac, T Rault, R Louf, M Funtowicz, arXiv:1911.04942arXiv:1809.09600Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks. arXiv: Artificial Intelligence. 2021. 2019. 2022. 2016. 2019arXiv preprintHuggingface's transformers: State-of-the-art natural language processing. and Manning, C. D. 2018a. HotpotQA: A dataset for diverse, explainable multi-hop question answering</p>
<p>HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering. Z Yang, P Qi, S Zhang, Y Bengio, W W Cohen, R Salakhutdinov, C D Manning, EMNLP. 2018b</p>
<p>CLEVRER: CoLlision Events for Video REpresentation and Reasoning. K Yi, C Gan, Y Li, P Kohli, J Wu, A Torralba, J B Tenenbaum, ArXiv, abs/1910.014422020</p>
<p>Neural-symbolic vqa: Disentangling reasoning from vision and language understanding. K Yi, J Wu, C Gan, A Torralba, P Kohli, J Tenenbaum, Advances in neural information processing systems. 201831</p>
<p>TaBERT: Pretraining for joint understanding of textual and tabular data. P Yin, G Neubig, W.-T Yih, S Riedel, arXiv:2005.083142020arXiv preprint</p>
<p>Deep reinforcement learning with relational inductive biases. X Yuan, M.-A Côté, J Fu, Z Lin, C Pal, Y Bengio, A Trischler, V Zambaldi, D Raposo, A Santoro, V Bapst, Y Li, I Babuschkin, K Tuyls, D Reichert, T Lillicrap, E Lockhart, arXiv:1908.10909International conference on learning representations. 2019. 2018arXiv preprintInteractive language learning by question answering</p>
<p>Piglet: Language grounding through neuro-symbolic interaction in a 3d world. R Zellers, A Holtzman, M Peters, R Mottaghi, A Kembhavi, A Farhadi, Y Choi, arXiv:2106.001882021arXiv preprint</p>
<p>S Zhang, S Roller, N Goyal, M Artetxe, M Chen, S Chen, C Dewan, M Diab, X Li, X V Lin, arXiv:2205.01068OPT: Open Pre-trained Transformer Language Models. 2022arXiv preprint</p>
<p>V Zhong, T Rocktäschel, E Grefenstette, arXiv:1910.08210Rtfm: Generalising to novel environment dynamics via reading. 2019arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
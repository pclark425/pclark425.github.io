<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6229 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6229</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6229</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-122.html">extraction-schema-122</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <p><strong>Paper ID:</strong> paper-6e0b6ba5cae954a0643baeb00167965e88458fc3</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/6e0b6ba5cae954a0643baeb00167965e88458fc3" target="_blank">On the Blind Spots of Model-Based Evaluation Metrics for Text Generation</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> This work design and synthesize a wide range of potential errors and check whether they result in a commensurate drop in the metric scores, and finds that BERTScore is confused by truncation errors in summarization, and MAUVE is insensitive to errors at the beginning or middle of generations.</p>
                <p><strong>Paper Abstract:</strong> In this work, we explore a useful but often neglected methodology for robustness analysis of text generation evaluation metrics: stress tests with synthetic data. Basically, we design and synthesize a wide range of potential errors and check whether they result in a commensurate drop in the metric scores. We examine a range of recently proposed evaluation metrics based on pretrained language models, for the tasks of open-ended generation, translation, and summarization. Our experiments reveal interesting insensitivities, biases, or even loopholes in existing metrics. For example, we find that BERTScore is confused by truncation errors in summarization, and MAUVE (built on top of GPT-2) is insensitive to errors at the beginning or middle of generations. Further, we investigate the reasons behind these blind spots and suggest practical workarounds for a more reliable evaluation of text generation. We have released our code and data at https://github.com/cloudygoose/blindspot_nlg.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6229.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6229.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MAUVE-human correlation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MAUVE metric variants correlation with human judgments</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>This paper reproduces and extends MAUVE's human-correlation analysis for three MAUVE feature variants (GPT-2, RoBERTa, ELECTRA) on WebText, reporting Spearman rank correlations against pairwise human judgments on aspects 'human-like', 'interesting', and 'sensible'.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>open-ended text generation (WebText / continuation quality)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>MAUVE using features from GPT-2, RoBERTa-large, and ELECTRA-large</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Pairwise human preference judgments from Pillutla et al. (2021) on continuations: annotators see prompt + two continuations and rate using a 5-point Likert on three aspects (human-like, interesting, sensible). Pairwise scores converted to rankings with a Bradley–Terry model; authors used the released human annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_compared</strong></td>
                            <td>Spearman rank correlation between MAUVE scores (per variant) and Bradley–Terry–fitted human preference coefficients for each aspect.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_differences</strong></td>
                            <td>MAUVE-ELECTRA achieved the highest Spearman correlations across all three aspects (Human-like 0.976, Interesting 0.857, Sensible 0.976) compared to MAUVE-GPT2 (0.952, 0.738, 0.881) and MAUVE-RoBERTa (0.929, 0.786, 0.881). Thus ELECTRA features correlated better with human judgments in this reproduction.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_specific_limitations</strong></td>
                            <td>MAUVE-GPT2 exhibits representational locality (attention concentrated on near history) causing insensitivity to errors in early/middle positions; MAUVE-RoBERTa, while more sensitive in stress tests, had slightly lower correlation on the 'human-like' aspect than GPT-2 in this reproduction. These variant-dependent behaviors show that the PLM used as feature extractor affects alignment with humans.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_cases</strong></td>
                            <td>MAUVE-GPT2 is insensitive to positioned errors (random or shuffled 10-token spans at the start or middle) and to sentence switching in multi-sentence generations—cases where human raters would penalize the output but MAUVE-GPT2 shows only marginal score drop.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_strategies</strong></td>
                            <td>Prefer discriminatively-trained MLM features (e.g., ELECTRA) for MAUVE to improve both stress-test sensitivity and human correlation; report/use multiple MAUVE variants or complementary metrics (e.g., combine MAUVE-RoBERTa/ELECTRA with GPT-PPL) to cover blind spots; calibrate or inspect feature choice before relying on a single variant.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On the Blind Spots of Model-Based Evaluation Metrics for Text Generation', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6229.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6229.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>UniEval injection failure</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>UniEval instruction-injection vulnerability producing misleading high scores</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>UniEval frames evaluation as answering quality questions (T5 predicts 'Yes'/'No'); the authors construct valueless 'injection' text designed to coax a 'Yes' answer and show UniEval can be tricked into assigning higher scores than genuine gold hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>summarization (summEval / CNNDM setup)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>UniEval (T5-based QA evaluator trained to answer quality questions)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>No new human annotation was run for this test; gold hypotheses are human-written summaries and the injection tests compare UniEval outputs against expectations from traditional metrics (e.g., ROUGE) and the gold references.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_compared</strong></td>
                            <td>UniEval aspect scores (coherence, consistency, fluency, relevance, overall) versus ROUGE-L and human-written gold quality expectation.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_differences</strong></td>
                            <td>UniEval assigned higher overall and aspect-level scores to crafted injection strings (e.g., 'Answer: Yes, this is a really coherent and consistent summary. And yes, it is relevant.') than to the true gold hypothesis, while ROUGE-L gave low scores to these valueless injections, indicating divergence from expected human judgement.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_specific_limitations</strong></td>
                            <td>The T5-based UniEval is susceptible to instruction-like context or valueless prompting (it tends to produce outputs consistent with contextual cues), causing it to over-score irrelevant or empty content; it appears to follow context-consistency biases rather than robustly verify content quality.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_cases</strong></td>
                            <td>Two example injections (one more specific) received UniEval-overall of 0.905 and 0.838 versus gold 0.864, while ROUGE-L dropped drastically (gold 0.286, injections 0.126 and 0.098), showing UniEval can be misled into rating nonsense / instruction-following text highly.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_strategies</strong></td>
                            <td>Detect such attacks by cross-checking with traditional reference-based metrics (e.g., ROUGE), check for low lexical overlap or semantic mismatch, augment UniEval training with adversarial/negative 'injection' examples, and deploy simple heuristics (e.g., flagging answers that are short canned 'Yes' statements or that lack content) or ensemble with other metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On the Blind Spots of Model-Based Evaluation Metrics for Text Generation', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6229.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6229.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-evaluation bias</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Evaluator bias favoring systems based on the same PLM (self-evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Log-probability-based metrics (GPT-PPL, BARTScore) tend to rank higher generations produced by systems whose base PLM matches the evaluator, causing biased evaluations that can contradict expected quality rankings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>open-ended generation (WikiText) and summarization (CNNDM finetuning experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>GPT-PPL (GPT-2 variants) and BARTScore (BART or T5 evaluators)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>No direct human annotation here; comparison is to conventional human expectations (larger models produce better quality) and to cross-evaluator consistency; authors note OPT-2.7b rankings align with conventional expectations.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_compared</strong></td>
                            <td>Perplexity-based evaluator scores from different PLMs (as metric) across generations from generators finetuned from different sized PLMs; comparisons indicate ranking inconsistencies among evaluators.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_differences</strong></td>
                            <td>GPT-PPL evaluators favored continuations produced by generators based on the same GPT-2 size (e.g., GPT2-small evaluator ranked GPT2-small generations higher even though larger models are conventionally expected to be better). BARTScore evaluators also tended to favor generators based on the same underlying PLM (BART or T5), producing inconsistent rankings across evaluators.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_specific_limitations</strong></td>
                            <td>Log-probability metrics inherit model biases: evaluators give systematically higher likelihoods to text generated by models with similar parameterization/training (self-similarity), producing unfair advantages and inconsistent system rankings that do not necessarily reflect human preferences.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_cases</strong></td>
                            <td>Tables show GPT-PPL evaluator scores where GPT2-small evaluator ranked GPT2-small outputs higher than GPT2-large outputs; BARTScore experiments show BART-based evaluator favors BART-based generator variants over T5-based ones even when human-quality expectations may differ.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_strategies</strong></td>
                            <td>Avoid evaluating a generation system with a metric based on the very same PLM; when comparing systems built on different PLMs, complement log-probability metrics with reference-based or PLM-agnostic metrics and human evaluations; report multiple evaluator variants and use ensembles to reduce self-evaluation bias.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On the Blind Spots of Model-Based Evaluation Metrics for Text Generation', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6229.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6229.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PLM-based metrics vs human annotations (literature claim)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PLM-based evaluation metrics generally show higher correlation with human annotations (prior literature)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper cites prior work claiming PLM-based metrics (e.g., BERTScore, BARTScore) often achieve higher correlation with human judgments than classic n-gram metrics; the authors position their stress tests as complementary to such human-correlation evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>BERTScore: Evaluating text generation with BERT</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>general NLG evaluation (translation, summarization, open-ended generation)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>PLM-based metrics in general (e.g., BERT, BART, GPT-derived metrics)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Referenced prior human-correlation studies (various setups across cited works) showing improved correlation of PLM-based metrics with human annotations; this paper does not re-run those specific human studies beyond the MAUVE reproduction.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_compared</strong></td>
                            <td>General literature comparisons between PLM-based metrics and traditional metrics (BLEU, ROUGE) in terms of correlation with human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_differences</strong></td>
                            <td>Prior works report PLM-based metrics having higher correlation with human annotations than traditional n-gram metrics; the present paper emphasizes that strong human correlation does not guarantee absence of blind spots revealed by stress tests.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_specific_limitations</strong></td>
                            <td>Despite higher average correlation, PLM-based metrics can have blind spots (insensitivities, biases, loopholes) due to PLM flaws or metric design choices that are not uncovered by standard correlation evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_cases</strong></td>
                            <td>This paper demonstrates concrete blind spots (e.g., truncation confusion for BERTScore, injection for UniEval, positioned-error insensitivity for MAUVE-GPT2) that would not necessarily be obvious from correlation numbers alone.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_strategies</strong></td>
                            <td>Use stress tests in addition to human-correlation evaluations; combine multiple complementary metrics (reference-based and reference-free, different PLM features) and report precision/recall/f-measure components to reveal potential failure modes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On the Blind Spots of Model-Based Evaluation Metrics for Text Generation', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Mauve: Measuring the gap between neural text and human text using divergence frontiers <em>(Rating: 2)</em></li>
                <li>BERTScore: Evaluating text generation with BERT <em>(Rating: 2)</em></li>
                <li>BartScore: Evaluating generated text as text generation <em>(Rating: 2)</em></li>
                <li>Towards a unified multidimensional evaluator for text generation <em>(Rating: 2)</em></li>
                <li>Are finetuned language models zero-shot learners <em>(Rating: 1)</em></li>
                <li>The curious case of neural text degeneration <em>(Rating: 1)</em></li>
                <li>Are factuality checkers reliable? adversarial meta-evaluation of factuality in summarization <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6229",
    "paper_id": "paper-6e0b6ba5cae954a0643baeb00167965e88458fc3",
    "extraction_schema_id": "extraction-schema-122",
    "extracted_data": [
        {
            "name_short": "MAUVE-human correlation",
            "name_full": "MAUVE metric variants correlation with human judgments",
            "brief_description": "This paper reproduces and extends MAUVE's human-correlation analysis for three MAUVE feature variants (GPT-2, RoBERTa, ELECTRA) on WebText, reporting Spearman rank correlations against pairwise human judgments on aspects 'human-like', 'interesting', and 'sensible'.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "open-ended text generation (WebText / continuation quality)",
            "llm_judge_model": "MAUVE using features from GPT-2, RoBERTa-large, and ELECTRA-large",
            "human_evaluation_setup": "Pairwise human preference judgments from Pillutla et al. (2021) on continuations: annotators see prompt + two continuations and rate using a 5-point Likert on three aspects (human-like, interesting, sensible). Pairwise scores converted to rankings with a Bradley–Terry model; authors used the released human annotations.",
            "metrics_compared": "Spearman rank correlation between MAUVE scores (per variant) and Bradley–Terry–fitted human preference coefficients for each aspect.",
            "reported_differences": "MAUVE-ELECTRA achieved the highest Spearman correlations across all three aspects (Human-like 0.976, Interesting 0.857, Sensible 0.976) compared to MAUVE-GPT2 (0.952, 0.738, 0.881) and MAUVE-RoBERTa (0.929, 0.786, 0.881). Thus ELECTRA features correlated better with human judgments in this reproduction.",
            "llm_specific_limitations": "MAUVE-GPT2 exhibits representational locality (attention concentrated on near history) causing insensitivity to errors in early/middle positions; MAUVE-RoBERTa, while more sensitive in stress tests, had slightly lower correlation on the 'human-like' aspect than GPT-2 in this reproduction. These variant-dependent behaviors show that the PLM used as feature extractor affects alignment with humans.",
            "notable_failure_cases": "MAUVE-GPT2 is insensitive to positioned errors (random or shuffled 10-token spans at the start or middle) and to sentence switching in multi-sentence generations—cases where human raters would penalize the output but MAUVE-GPT2 shows only marginal score drop.",
            "mitigation_strategies": "Prefer discriminatively-trained MLM features (e.g., ELECTRA) for MAUVE to improve both stress-test sensitivity and human correlation; report/use multiple MAUVE variants or complementary metrics (e.g., combine MAUVE-RoBERTa/ELECTRA with GPT-PPL) to cover blind spots; calibrate or inspect feature choice before relying on a single variant.",
            "uuid": "e6229.0",
            "source_info": {
                "paper_title": "On the Blind Spots of Model-Based Evaluation Metrics for Text Generation",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "UniEval injection failure",
            "name_full": "UniEval instruction-injection vulnerability producing misleading high scores",
            "brief_description": "UniEval frames evaluation as answering quality questions (T5 predicts 'Yes'/'No'); the authors construct valueless 'injection' text designed to coax a 'Yes' answer and show UniEval can be tricked into assigning higher scores than genuine gold hypotheses.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "summarization (summEval / CNNDM setup)",
            "llm_judge_model": "UniEval (T5-based QA evaluator trained to answer quality questions)",
            "human_evaluation_setup": "No new human annotation was run for this test; gold hypotheses are human-written summaries and the injection tests compare UniEval outputs against expectations from traditional metrics (e.g., ROUGE) and the gold references.",
            "metrics_compared": "UniEval aspect scores (coherence, consistency, fluency, relevance, overall) versus ROUGE-L and human-written gold quality expectation.",
            "reported_differences": "UniEval assigned higher overall and aspect-level scores to crafted injection strings (e.g., 'Answer: Yes, this is a really coherent and consistent summary. And yes, it is relevant.') than to the true gold hypothesis, while ROUGE-L gave low scores to these valueless injections, indicating divergence from expected human judgement.",
            "llm_specific_limitations": "The T5-based UniEval is susceptible to instruction-like context or valueless prompting (it tends to produce outputs consistent with contextual cues), causing it to over-score irrelevant or empty content; it appears to follow context-consistency biases rather than robustly verify content quality.",
            "notable_failure_cases": "Two example injections (one more specific) received UniEval-overall of 0.905 and 0.838 versus gold 0.864, while ROUGE-L dropped drastically (gold 0.286, injections 0.126 and 0.098), showing UniEval can be misled into rating nonsense / instruction-following text highly.",
            "mitigation_strategies": "Detect such attacks by cross-checking with traditional reference-based metrics (e.g., ROUGE), check for low lexical overlap or semantic mismatch, augment UniEval training with adversarial/negative 'injection' examples, and deploy simple heuristics (e.g., flagging answers that are short canned 'Yes' statements or that lack content) or ensemble with other metrics.",
            "uuid": "e6229.1",
            "source_info": {
                "paper_title": "On the Blind Spots of Model-Based Evaluation Metrics for Text Generation",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "Self-evaluation bias",
            "name_full": "Evaluator bias favoring systems based on the same PLM (self-evaluation)",
            "brief_description": "Log-probability-based metrics (GPT-PPL, BARTScore) tend to rank higher generations produced by systems whose base PLM matches the evaluator, causing biased evaluations that can contradict expected quality rankings.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "open-ended generation (WikiText) and summarization (CNNDM finetuning experiments)",
            "llm_judge_model": "GPT-PPL (GPT-2 variants) and BARTScore (BART or T5 evaluators)",
            "human_evaluation_setup": "No direct human annotation here; comparison is to conventional human expectations (larger models produce better quality) and to cross-evaluator consistency; authors note OPT-2.7b rankings align with conventional expectations.",
            "metrics_compared": "Perplexity-based evaluator scores from different PLMs (as metric) across generations from generators finetuned from different sized PLMs; comparisons indicate ranking inconsistencies among evaluators.",
            "reported_differences": "GPT-PPL evaluators favored continuations produced by generators based on the same GPT-2 size (e.g., GPT2-small evaluator ranked GPT2-small generations higher even though larger models are conventionally expected to be better). BARTScore evaluators also tended to favor generators based on the same underlying PLM (BART or T5), producing inconsistent rankings across evaluators.",
            "llm_specific_limitations": "Log-probability metrics inherit model biases: evaluators give systematically higher likelihoods to text generated by models with similar parameterization/training (self-similarity), producing unfair advantages and inconsistent system rankings that do not necessarily reflect human preferences.",
            "notable_failure_cases": "Tables show GPT-PPL evaluator scores where GPT2-small evaluator ranked GPT2-small outputs higher than GPT2-large outputs; BARTScore experiments show BART-based evaluator favors BART-based generator variants over T5-based ones even when human-quality expectations may differ.",
            "mitigation_strategies": "Avoid evaluating a generation system with a metric based on the very same PLM; when comparing systems built on different PLMs, complement log-probability metrics with reference-based or PLM-agnostic metrics and human evaluations; report multiple evaluator variants and use ensembles to reduce self-evaluation bias.",
            "uuid": "e6229.2",
            "source_info": {
                "paper_title": "On the Blind Spots of Model-Based Evaluation Metrics for Text Generation",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "PLM-based metrics vs human annotations (literature claim)",
            "name_full": "PLM-based evaluation metrics generally show higher correlation with human annotations (prior literature)",
            "brief_description": "The paper cites prior work claiming PLM-based metrics (e.g., BERTScore, BARTScore) often achieve higher correlation with human judgments than classic n-gram metrics; the authors position their stress tests as complementary to such human-correlation evaluations.",
            "citation_title": "BERTScore: Evaluating text generation with BERT",
            "mention_or_use": "mention",
            "task_domain": "general NLG evaluation (translation, summarization, open-ended generation)",
            "llm_judge_model": "PLM-based metrics in general (e.g., BERT, BART, GPT-derived metrics)",
            "human_evaluation_setup": "Referenced prior human-correlation studies (various setups across cited works) showing improved correlation of PLM-based metrics with human annotations; this paper does not re-run those specific human studies beyond the MAUVE reproduction.",
            "metrics_compared": "General literature comparisons between PLM-based metrics and traditional metrics (BLEU, ROUGE) in terms of correlation with human judgments.",
            "reported_differences": "Prior works report PLM-based metrics having higher correlation with human annotations than traditional n-gram metrics; the present paper emphasizes that strong human correlation does not guarantee absence of blind spots revealed by stress tests.",
            "llm_specific_limitations": "Despite higher average correlation, PLM-based metrics can have blind spots (insensitivities, biases, loopholes) due to PLM flaws or metric design choices that are not uncovered by standard correlation evaluations.",
            "notable_failure_cases": "This paper demonstrates concrete blind spots (e.g., truncation confusion for BERTScore, injection for UniEval, positioned-error insensitivity for MAUVE-GPT2) that would not necessarily be obvious from correlation numbers alone.",
            "mitigation_strategies": "Use stress tests in addition to human-correlation evaluations; combine multiple complementary metrics (reference-based and reference-free, different PLM features) and report precision/recall/f-measure components to reveal potential failure modes.",
            "uuid": "e6229.3",
            "source_info": {
                "paper_title": "On the Blind Spots of Model-Based Evaluation Metrics for Text Generation",
                "publication_date_yy_mm": "2022-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Mauve: Measuring the gap between neural text and human text using divergence frontiers",
            "rating": 2
        },
        {
            "paper_title": "BERTScore: Evaluating text generation with BERT",
            "rating": 2
        },
        {
            "paper_title": "BartScore: Evaluating generated text as text generation",
            "rating": 2
        },
        {
            "paper_title": "Towards a unified multidimensional evaluator for text generation",
            "rating": 2
        },
        {
            "paper_title": "Are finetuned language models zero-shot learners",
            "rating": 1
        },
        {
            "paper_title": "The curious case of neural text degeneration",
            "rating": 1
        },
        {
            "paper_title": "Are factuality checkers reliable? adversarial meta-evaluation of factuality in summarization",
            "rating": 1
        }
    ],
    "cost": 0.014905999999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>On the Blind Spots of Model-Based Evaluation Metrics for Text Generation</h1>
<p>Tianxing $\mathrm{He}^{<em>}$<br>Univ. of Washington<br>goosehe@cs.w</em>.edu Jingyu Zhang*<br>Johns Hopkins Univ. Shanghai Jiao Tong Univ. <br> jzhan237@jhu.edu <br> Tianle Wang<br>Shanghai Jiao Tong Univ. <br> wtl666wtl@sjtu.edu.cn<br>Sachin Kumar<br>Carnegie Mellon Univ.<br>sachink@cs.cmu.edu</p>
<p>Kyunghyun Cho<br>New York Univ.<br>kyunghyun.cho@nyu.edu James Glass<br>Mass. Institute of Technology<br>glass@mit.edu</p>
<p>Yulia Tsvetkov<br>Univ. of Washington<br>yuliats@cs.washington.edu</p>
<h4>Abstract</h4>
<p>In this work, we explore a useful but often neglected methodology for robustness analysis of text generation evaluation metrics: stress tests with synthetic data. Basically, we design and synthesize a wide range of potential errors and check whether they result in a commensurate drop in the metric scores. We examine a range of recently proposed evaluation metrics based on pretrained language models, for the tasks of open-ended generation, translation, and summarization. Our experiments reveal interesting insensitivities, biases, or even loopholes in existing metrics. For example, we find that BERTScore is confused by truncation errors in summarization, and MAUVE (built on top of GPT-2) is insensitive to errors at the beginning or middle of generations. Further, we investigate the reasons behind these blind spots and suggest practical workarounds for a more reliable evaluation of text generation. We have released our code and data at https://github. com/cloudygoose/blindspot_nlg.</p>
<h2>1 Introduction</h2>
<p>Automatic evaluation of machine-generated text (Celikyilmaz et al., 2020) has been a core research challenge in the field of natural language generation (NLG), as difficult as language generation itself. Encouraged by the phenomenal success of large-scale pretraining (Devlin et al., 2019), a recent series of work proposed to base evaluation metrics on pretrained language models (PLMs) (Zhang et al., 2020; Yuan et al., 2021; Pillutla et al., 2021). For example, BERTScore (Zhang et al., 2020) computes a similarity score between the contextualized embeddings of the hypothesis and the reference text. PLM-based metrics have been shown to have higher correlations with human annotations for various tasks (Yuan et al., 2021), and are becoming increasingly popular in practice.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Motivation: The flaws of the underlying PLMs or certain design choices in the metrics could lead to potential blind spots in the evaluation.</p>
<p>However, PLMs have flaws. They could assign a high likelihood to degenerate, repetitive text (Holtzman et al., 2020) and could be insensitive to perturbations such as word order shuffling (Pham et al., 2021), negation (Ettinger, 2020), etc. These flaws, in combination with certain design choices, may lead to the metrics based on such PLMs being brittle and open to manipulation (Figure 1).</p>
<p>In this work, we develop a suite of stress tests with synthetic data for the robustness analysis of NLG metrics. In essence, we induce a variety of potential errors in clean text and examine the resulting drop in the metric scores. The tests are motivated by metric design choices, properties of PLMs, or general fluency/consistency errors. Our methodology facilitates full control over the synthesized error types, allowing us to test extreme or even adversarial scenarios that are not well covered in standard correlation-oriented evaluations.</p>
<p>Our tests are applied to a range of recently proposed and widely used PLM-based metrics for the tasks of open-ended generation, translation, and summarization. They reveal a number of glaring insensitivities, biases, and even loopholes in different metrics. Besides analyzing the reasons behind, we also provide practical suggestions and workarounds for a more reliable evaluation.</p>
<h2>2 Methodology</h2>
<p>We now discuss our methodology. For simplicity, in this section, let us assume a multi-reference translation dataset, where each sample has two reference translations produced by human translators, denoted by Ref-A and Ref-B. We will generalize our methodology to other tasks in $\S 3$.</p>
<p>We begin by computing a "base" metric score by considering Ref-A as hypotheses and Ref-B as references. Since Ref-A is produced by human translators, we assume that it is less likely to contain translation errors than machine-generated text, and it should be assigned a high score by the metric. Due to these two assumptions, and to disambiguate from the reference set (Ref-B), we term Ref-A as the gold hypothesis set.</p>
<p>For each test, we apply a synthesized error type (e.g., truncation) to the gold hypothesis set to construct a noised hypothesis set. We make sure that the amount or type of induced errors is sufficient to be distinctive from the original gold hypothesis (to be detailed in §5). The source texts and the references are left intact.</p>
<p>To determine whether a metric passes a test, a simple rank-based protocol is used: We claim that the metric fails the test for this dataset if the noised hypothesis set is not scored worse than the base score (from the gold set). ${ }^{1}$ This rank-based protocol can be easily extended to the comparison of different gradations of the same noise type (controlled by hyper-parameters). For example, a 20\%truncation is expected to rank lower than a $10 \%$ truncation, as more information is lost.</p>
<h2>3 Tasks and Datasets</h2>
<p>Our tests cover three ubiquitous text generation tasks: open-ended generation, translation, and summarization. We now describe the dataset used for each task and the setting for gold hypotheses.</p>
<p>For open-ended generation, we use the WikiText103 dataset (Merity et al., 2016). We randomly select 2000 paragraphs of length around 256 tokens from the dataset (preprocessing detailed in Appendix B.2). The samples typically contain seven or eight sentences. We divide them into two sets with 1000 samples each, and set one as the references and the other as the gold hypotheses. The reference set is only used for the MAUVE metric (more details given in Appendix A).</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>For summarization, we use the popular CNNDailymail (CNNDM) dataset (Hermann et al., 2015). Kryscinski et al. (2020) collected 10 additional human-annotated summaries (different from the original reference summary) for each of 100 samples in the test set. We set the CNNDM reference summaries to be the gold hypotheses, and use these 10 annotations as references. Correspondingly, the multi-reference version of metrics are used. The gold hypotheses typically contain three sentences.</p>
<p>For translation, we use the evaluation dataset from the WMT21 metrics shared task (Akhbardeh et al., 2021). We only use the source text and reference translations. We report results on the German-English (De-En) language pair, which contains 1000 translation pairs. There are two humantranslated references (human-A and human-B) for each sample. We use human-A as the gold hypothesis and human-B as the reference. We also repeat key experiments on the Chinese-English (Zh-En) data and obtain very similar observations. Therefore, we omit the Zh-En results for brevity.</p>
<p>Most samples in WMT only contain one sentence, which makes some of our tests impossible (e.g., sentence switching). For this reason, we build a paragraph-level translation dataset based on the Zh-En part of the TED-Talks task (Duh, 2018). It contains 100 samples, where each sample has two human-translated references and on average contains 7 sentences. We name this dataset as TEDMT, and discuss how we build it in Appendix B.1.</p>
<h2>4 Metrics</h2>
<p>For open-ended text generation, we test MAUVE (Pillutla et al., 2021), GPT-PPL and MLM-PPL (Salazar et al., 2020). We report the negated GPT/MLM-PPL so that all metric scores are the higher the better.</p>
<p>MAUVE is a reference-based metric computed using contextualized embeddings from PLMs. We explore MAUVE with GPT2-large, RoBERTalarge, and ELECTRA-large (Clark et al., 2020) features. In Pillutla et al. (2021), the exploration is centered around the GPT-2 feature. However, in this work we find the choice of feature has a crucial impact on the metric's robustness.</p>
<p>GPT-PPL denotes perplexity from the GPT2large (Radford et al., 2019) model. MLM-PPL is the masked language model perplexity from a RoBERTa-large model (Liu et al., 2019). We use a definition similar to the formulation in Salazar et al.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Blind Spot</th>
<th style="text-align: center;">Section</th>
<th style="text-align: center;">Affected Metrics (and Variant)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">positioned error</td>
<td style="text-align: center;">$\S 5.1$</td>
<td style="text-align: center;">MAUVE (-GPT2)</td>
</tr>
<tr>
<td style="text-align: center;">injection</td>
<td style="text-align: center;">$\S 5.2$</td>
<td style="text-align: center;">UniEval (-rel/-overall)</td>
</tr>
<tr>
<td style="text-align: center;">high-freq $n$-gram</td>
<td style="text-align: center;">$\S 5.3$</td>
<td style="text-align: center;">GPT-PPL, MLM-PPL</td>
</tr>
<tr>
<td style="text-align: center;">self-evaluation</td>
<td style="text-align: center;">$\S 5.4$</td>
<td style="text-align: center;">GPT-PPL, BARTScore (-faithful)</td>
</tr>
<tr>
<td style="text-align: center;">truncation</td>
<td style="text-align: center;">$\S 5.5$, App. I</td>
<td style="text-align: center;">BERTScore (-p/-f), BARTScore (-p/-f/-faithful), COMET-QE, PRISM-QE, ROUGE (-2/-L), MAUVE (-GPT2), UniEval (-overall)</td>
</tr>
<tr>
<td style="text-align: center;">sentence switching</td>
<td style="text-align: center;">$\S 5.5$</td>
<td style="text-align: center;">MAUVE (-GPT2/-RoBERTa), BARTScore (-r)</td>
</tr>
<tr>
<td style="text-align: center;">copy-source</td>
<td style="text-align: center;">App. D</td>
<td style="text-align: center;">COMET-QE, BARTSc (-t/-f/-faithful), BERTSc (-r), UniEval (-overall)</td>
</tr>
<tr>
<td style="text-align: center;">repetition</td>
<td style="text-align: center;">App. E</td>
<td style="text-align: center;">GPT-PPL, MLM-PPL, BARTScore (all variants)</td>
</tr>
<tr>
<td style="text-align: center;">BERT-diverge</td>
<td style="text-align: center;">App. I</td>
<td style="text-align: center;">COMET-QE</td>
</tr>
<tr>
<td style="text-align: center;">article removal</td>
<td style="text-align: center;">App. I</td>
<td style="text-align: center;">COMET-QE</td>
</tr>
<tr>
<td style="text-align: center;">noised punctuation</td>
<td style="text-align: center;">App. I</td>
<td style="text-align: center;">BARTScore (-r), ROUGE (-2/-L)</td>
</tr>
<tr>
<td style="text-align: center;">a few other fluency errors</td>
<td style="text-align: center;">App. I</td>
<td style="text-align: center;">BARTScore (-r)</td>
</tr>
</tbody>
</table>
<p>Table 1: A catalogue of the blind spots identified in this work for various metrics. Some of the tests are deferred to appendix to save space.
(2020) and provide details in Appendix A.</p>
<p>For translation and summarization, we test BERTScore (Zhang et al., 2020), MoverScore (Zhao et al., 2019), BARTScore (Yuan et al., 2021), UniEval (Zhong et al., 2022), COMET (Rei et al., 2020), PRISM (Thompson and Post, 2020), and BLEURT (Sellam et al., 2020). Among these metrics, PRISM and BLEURT are only applied for translation, and UniEval is only applied for summarization. While COMET was originally proposed for translation, Kasai et al. (2022b) showed it has superior human correlation for CNNDM. Therefore, we also include it for summarization. We also include the traditional metrics BLEU (for translation), and ROUGE-2/L (for summarization).</p>
<p>Both BERTScore and BARTScore have variants for precision (-p), recall (-r), and f-measure (-f). In addition, BARTScore has a faithfulness (-faithful) variant. We test two model options, namely BARTScore-cnn and BARTScore-para. ${ }^{2}$ UniEval reports scores on four aspects: coherence, consistency, fluency, and relevance, and the overall score is the average of the four.</p>
<p>By default, the metrics for translation and summarization are reference-based. ${ }^{3}$ COMET and PRISM have a quality estimation (QE) variant (Specia et al., 2021), where users do not need to provide any reference.</p>
<p>In most cases, we directly use the released package or code for each metric and follow the recommended hyper-parameter or variant setting. We defer further implementation details and variant explanations to Appendix A.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>5 Stress Tests and Results</h2>
<p>We organize our findings into subsections each containing a set of tests with the corresponding motivation, description, results, and implications with practical workarounds. In general we perform each test for all metrics, and we primarily discuss metrics found to be problematic for brevity.</p>
<p>We group and order our tests by their motivations: The positioned-error (§5.1) and injection (§5.2) tests are mainly motivated by certain metric design choices; The freq-ngram (§5.3) and selfevaluation (§5.4) tests are motivated by certain PLM properties; Finally, the fluency/consistency (§5.5) tests mimic general errors that human or machine writers could make. See Table 1 for a catalogue along with the metrics affected.</p>
<h3>5.1 The Positioned Error Test</h3>
<p>For MAUVE, the features for reference/hypothesis texts are extracted using the PLM representation of the final token. Hence, it could be suboptimal if the PLM is biased to encode only the local context (Khandelwal et al., 2018; He et al., 2021).</p>
<p>To test for this bias, we create synthetic errors by replacing a span of 10 consecutive tokens in different positions of the gold hypothesis with (1) 10 random tokens from the vocabulary, or (2) randomly shuffled tokens of the original span. We experiment with three different error positions by replacing the tokens at the very start, the middle, and the very end of the gold hypotheses. A robust metric should give a significantly lower score to this clearly modified distribution of the hypotheses.</p>
<p>Shown in Table 2, MAUVE-GPT2 shows only a marginal drop (around 3\%) for the random or shuffle errors in the start and middle positions. In comparison, MAUVE-RoBERTa penalizes errors in all positions severely, which aligns better with</p>
<table>
<thead>
<tr>
<th>Noise Type</th>
<th>MAUVE Variant</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>GPT-2</td>
<td>RoBERTa</td>
</tr>
<tr>
<td>Gold</td>
<td>0.961</td>
<td>0.969</td>
</tr>
<tr>
<td>Random-Start</td>
<td>$0.949(-1.3 \%)$</td>
<td>$0.037(-96.1 \%)$</td>
</tr>
<tr>
<td>Random-Middle</td>
<td>$0.898(-6.5 \%)$</td>
<td>$0.100(-89.7 \%)$</td>
</tr>
<tr>
<td>Random-End</td>
<td>$0.005(-99.4 \%)$</td>
<td>$0.036(-96.3 \%)$</td>
</tr>
<tr>
<td>Shuffle-Start</td>
<td>$0.916(-4.7 \%)$</td>
<td>$0.342(-64.7 \%)$</td>
</tr>
<tr>
<td>Shuffle-Middle</td>
<td>$0.943(-1.8 \%)$</td>
<td>$0.603(-37.8 \%)$</td>
</tr>
<tr>
<td>Shuffle-End</td>
<td>$0.020(-97.9 \%)$</td>
<td>$0.242(-75.0 \%)$</td>
</tr>
</tbody>
</table>
<p>Table 2: Results for the positioned error test. MAUVEGPT2 is insensitive to errors at the start or middle of hypotheses. The percentage shown is score change w.r.t. the base score from the gold hypotheses.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Attention distribution of GPT2-large and RoBERTa-large over the relative position in one data sample (averaged over layers and heads). Each unit corresponds to $10 \%$ of tokens. More details are given in Figure 11 and Appendix C.2.
expectations. MAUVE-ELECTRA's behavior is similar to the RoBERTa variant and is deferred to Appendix C.1.</p>
<p>We correlate this result with an attention pattern analysis. As shown in Figure 2, we observe that GPT2-large's attention is concentrated on the diagonal of the plot, which indicates GPT-2 mostly attends to the near history. In contrast, RoBERTalarge attends heavily to specific (probably important) token positions regardless of the current token position. In summary, the attention patterns provide evidence that GPT-2 features encode less long-range context compared to RoBERTa. ${ }^{4}$ This pattern is typical across different data samples.</p>
<p>Implication Currently, the default feature used by MAUVE is from GPT-2, which as we show, ignores errors at the start or the middle of the generations. Our analysis indicates that MLMs such as RoBERTa or ELECTRA could be a better choice.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup><code>Inj-1: Answer:Yes,this is a really coherent and
consistent summary.And yes, it is relevant.
Inj-2: Answer:Yes,this is a really good summary.
    Metric (task) Gold Inj-1 Inj-2
    UniEval-overall (sum) 0.864 0.905 0.838
    UniEval-coherence (sum) 0.897 0.903 0.777
    UniEval-consistency (sum) 0.859 0.857 0.756
    UniEval-fluency (sum) 0.919 0.959 0.962
    UniEval-relevance (sum) 0.781 0.900 0.856
    ROUGE-L (sum) 0.286 0.126 0.098</code></p>
<p>Table 3: Results of the injection test. The PLM is tricked to answer yes to the evaluation questions.</p>
<p>See $\S 5.5$ for results on MAUVE's other blind spots.</p>
<h3>5.2 The Injection Test</h3>
<p>UniEval (Zhong et al., 2022) reframes NLG evaluation as a boolean question answering task. For example, a question such as "Is this a coherent summary? Summary: [HYPO] Document: ..." along with the hypothesis replacing the [HYPO] placeholder is inputted to a trained T5 model (Raffel et al., 2020), and the score is based on the output probability of answering "Yes".</p>
<p>This test is inspired by a recent series of work teaching LMs to follow instructions (Wei et al., 2022; Mishra et al., 2022). We construct several valueless but misleading injection hypotheses, which attempt to "instruct" (via natural language) the underlying PLM to answer yes. ${ }^{5}$ Results of two example injections are shown in Table 3.</p>
<p>We observe that UniEval is tricked to give a high score to the valueless injection hypotheses, and the more specific injection (Inj-1) gets a higher score. This is surprising because UniEval is trained with constructed positive/negative samples, and it is not trained to follow instructions. We surmise this result is more related to the PLM's nature to make the output consistent with the context. More examples and discussion are given in Appendix F.</p>
<p>Implication The injection test shows that the metric's judgement can be misled by some valueless text span, which can be used for cheating. It can be detected by a low score from traditional metrics such as ROUGE (Table 3).</p>
<h3>5.3 The Frequent $n$-gram Test</h3>
<p>Due to the statistical nature of LMs, they have been known to favor frequent $n$-grams in the data. We now stress-test whether log-likelihood-based</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Step-wise next-token probability of a (partial) frequent 4-gram sequence given by GPT2-large, for GPT-PPL. The alternation between blue and red indicates the start of a new 4-gram.</p>
<p>metrics would wrongly favor a random sequence of frequent n-grams over the gold hypotheses.</p>
<p>For open-ended generation, we collect the top-k most frequent n-grams from the WikiText dataset. We then build synthetic hypotheses of length 256 by uniformly sampling n-grams from this collection and concatenating them (see Table 12 in Appendix G for an example). To a human evaluator, these sequences are completely random and should get a lower score than the gold hypotheses.</p>
<table>
<thead>
<tr>
<th>Metric (task)</th>
<th>Gold</th>
<th>Freq 4-gram</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>Top-10</td>
<td>Top-50</td>
<td>Top-100</td>
</tr>
<tr>
<td>GPT-PPL (wiki)</td>
<td>-25.640</td>
<td>-4.456</td>
<td>-11.640</td>
<td>-18.160</td>
</tr>
<tr>
<td>MLM-PPL (wiki)</td>
<td>-2.994</td>
<td>-1.139</td>
<td>-2.469</td>
<td>-3.971</td>
</tr>
<tr>
<td>n-rep-4gram (wiki)</td>
<td>-0.019</td>
<td>-0.539</td>
<td>-0.199</td>
<td>-0.120</td>
</tr>
</tbody>
</table>
<p>Table 4: Results for the frequent n-gram test. Both GPT-PPL and MLM-PPL deem the frequent 4-gram sequences as probable. We also include the (negated) rep-4gram metric (Welleck et al., 2020) for diversity.</p>
<p>Strikingly, as shown in Table 4 with 4-gram, we find that both GPT-PPL and MLM-PPL assign higher scores to the frequent n-gram sequences than gold. This gap further increases when we concentrate on more frequent n-grams. We present additional results with 3-gram in Appendix G.</p>
<p>To illustrate this issue, we plot step-wise next-token probability given by the underlying GPT2-large model. As shown in Figure 3, the probabilities exhibit a pattern that high-probability regions concentrate at the end of each 4-gram. We attribute this behavior to the LM's utilization of local context (Khandelwal et al., 2018).</p>
<p>We conduct similar tests on translation or summarization but do not observe problematic behavior from the metrics. We surmise the reason could be due to the poor alignment between the random n-gram sequence and the source/reference text.</p>
<table>
<thead>
<tr>
<th>Evaluator</th>
<th>Generator</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>GPT2-small</td>
<td></td>
<td></td>
</tr>
<tr>
<td>wiki-ft</td>
<td>GPT2-med</td>
<td></td>
<td></td>
</tr>
<tr>
<td>wiki-ft</td>
<td>GPT2-large</td>
<td></td>
<td></td>
</tr>
<tr>
<td>wiki-ft</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>GPT2-small</td>
<td>-21.08</td>
<td>-24.35</td>
<td>-24.36</td>
</tr>
<tr>
<td>GPT2-med</td>
<td>-23.20</td>
<td>-17.48</td>
<td>-19.06</td>
</tr>
<tr>
<td>GPT2-large</td>
<td>-22.87</td>
<td>-18.56</td>
<td>-15.04</td>
</tr>
<tr>
<td>OPT-2.7b</td>
<td>-24.24</td>
<td>-19.08</td>
<td>-17.20</td>
</tr>
</tbody>
</table>
<p>Table 5: Scores from GPT-PPL with different evaluator or generator. The evaluator model favors generation system based on itself.</p>
<p><strong>Implication</strong> This test shows that the affected metrics are biased towards frequent n-gram rather than global coherence. This test strengthens the importance of diversity metrics such as rep-4gram.</p>
<h3>5.4 The Self-Evaluation Bias</h3>
<p>Log-probability-based metrics (e.g., GPT-PPL) are based on generative models such as GPT-2 (Radford et al., 2019) or BART (Lewis et al., 2019). At the same time, these PLMs are also used as base models for developing new NLG systems (Yang and Klein, 2021). Naturally, we wonder whether this could cause some level of bias in the evaluation. In the following tests, we demonstrate this bias for the case of GPT-PPL and BARTScore.</p>
<p>For <strong>GPT-PPL</strong>, we construct a setting that mimics how it is used in practice: For the generator, we finetune GPT-2 models of different sizes (small, medium, and large), and use the models to generate continuations of prompts from the WikiText dataset. The details of finetuning are available in Appendix H. We use top-k sampling (Fan et al., 2018) with k = 50 to decode. For evaluator, we use GPT-2 models off-the-shelf.</p>
<p>For different combinations of generator and evaluator, the results are shown in Table 5. Conventional wisdom in the community is that the larger GPT model should generate higher-quality text, which correlates with the scores from the OPT-2.7b (Zhang et al., 2022) model. However, perplexities from GPT2-small and -medium violate these expectations, ranking generations from their own base models higher than those of larger models. We term this as the self-evaluation bias.</p>
<p><strong>BARTScore</strong> (Yuan et al., 2021) evaluates text generation quality as the log-probability of a seq2seq model. The default implementation relies on the finetuned BART-large model. Here, we test a hypothetical setting, where we base BARTScore on another popular PLM: T5 (Raffel et al., 2020). We</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Evaluator</th>
<th style="text-align: center;">Generator</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BT-base</td>
<td style="text-align: center;">BT-large</td>
<td style="text-align: center;">T5-small</td>
<td style="text-align: center;">T5-base</td>
</tr>
<tr>
<td style="text-align: center;">BT-base</td>
<td style="text-align: center;">-0.270</td>
<td style="text-align: center;">-0.361</td>
<td style="text-align: center;">-0.367</td>
<td style="text-align: center;">-0.392</td>
</tr>
<tr>
<td style="text-align: center;">BT-large</td>
<td style="text-align: center;">-0.357</td>
<td style="text-align: center;">-0.278</td>
<td style="text-align: center;">-0.390</td>
<td style="text-align: center;">-0.389</td>
</tr>
<tr>
<td style="text-align: center;">T5-small</td>
<td style="text-align: center;">-0.359</td>
<td style="text-align: center;">-0.397</td>
<td style="text-align: center;">-0.227</td>
<td style="text-align: center;">-0.362</td>
</tr>
<tr>
<td style="text-align: center;">T5-base</td>
<td style="text-align: center;">-0.335</td>
<td style="text-align: center;">-0.344</td>
<td style="text-align: center;">-0.331</td>
<td style="text-align: center;">-0.226</td>
</tr>
<tr>
<td style="text-align: center;">nPPL</td>
<td style="text-align: center;">-4.323</td>
<td style="text-align: center;">-3.684</td>
<td style="text-align: center;">-4.903</td>
<td style="text-align: center;">-3.803</td>
</tr>
<tr>
<td style="text-align: center;">BS-para-p</td>
<td style="text-align: center;">-3.790</td>
<td style="text-align: center;">-3.762</td>
<td style="text-align: center;">-3.847</td>
<td style="text-align: center;">-3.786</td>
</tr>
</tbody>
</table>
<p>Table 6: Scores from BARTScore-cnn-faithful using different PLMs as evaluator or generator. BT refers to BART and BS refers to BARTScore. Negated perplexity (nPPL) with the gold hypothesis are also reported for each model. In each row, scores marked by orange and bold are higher than scores marked by brown.
use the BARTScore-cnn-faithful variant, and finetune all models on the CNNDM dataset (details in Appendix H). The results are shown in Table 6. For this experiment, we do not assume the supremacy of one model over the other, as that requires more rigorous human evaluation.</p>
<p>We observe an interesting but worrisome phenomenon: BART and T5 based evaluators strongly favor generators based on their own respective base models. This bias extends to different-sized variants of the base models as well. It is, however, less pronounced for the reference-based variant BARTScore-para.</p>
<p>Implication Overall, these results show that the log-probability-based metrics could be unfairly biased towards their underlying PLMs. Basing the metric on different PLM could give inconsistent ranking for the same set of systems.</p>
<p>Hence, practitioners should avoid situations where the generation system and the metric are based on the exact same PLM, or where systems based on different types of PLMs are compared with a metric based on one of them. In such cases, the scores should be complemented with additional evaluations from reference-based metrics. ${ }^{6}$</p>
<h3>5.5 Fluency \&amp; Consistency Tests</h3>
<p>The tests we discussed so far have been motivated by certain metric design choices or properties of the underlying PLMs. In this section, we move to more general tests, where we synthesize a range of perturbations that mimic human or machine errors.</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h3>5.5.1 Noise Types and Setup</h3>
<p>Our tests cover two important aspects of natural language: fluency and consistency (some of our consistency tests are also related to coherence). Fluency tests focus on grammaticality, while consistency tests focus on temporal order, logic, or alignment with the source text.</p>
<p>Similar to previous sections, in each test we apply one type of noise to the gold hypothesis. The noise can be regarded as an exaggeration of the errors human or machine writers could make. In total, we design 10 fluency tests and 8 consistency tests. For brevity, we only discuss a subset of them in this section, which are listed in Table 7. The tests can generally be applied to all three tasks with a few exceptions (detailed in Appendix I).</p>
<p>Most tests involve a hyper-parameter influencing the amount of noise added. This enables us to test how the metric behaves as we induce different levels of noise. To quantify the noise level, we define noise-ratio, based on the Levenshtein distance:</p>
<p>$$
\frac{1}{|\mathcal{H}|} \sum_{h \in \mathcal{H}} \frac{\operatorname{Levenshtein}\left(h^{\prime}, h\right)}{\operatorname{len}(h)}
$$</p>
<p>where $\mathcal{H}$ is the set of gold hypotheses, and $h^{\prime}$ is the noised hypothesis. We employ the noise-ratio as a crude proxy to quantify the amount of noise across different noise types. ${ }^{7}$ For more details on the setup, please see Appendix I.</p>
<p>For each noise type, a robust metric should give monotonically decreasing scores with an increasing noise-ratio. We claim a metric fails the test if it deviates from this expectation.</p>
<h3>5.5.2 Results</h3>
<p>Results for a subset of metrics/tests are shown in Figure 4. Unsurprisingly, most tests are passed by the metrics. However, the truncation and sentence switching tests give striking results. We will focus on these two tests here, and defer more complete results and discussion to Appendix I.</p>
<p>A number of popular metrics fail the truncation test, including (some variants of) BARTScore, BERTScore, ROUGE, COMET, PRISM, UniEval, and MAUVE (Some figures are deferred to Appendix I), spanning across CNNDM, TED-MT, and WikiText datasets. This is undesirable because truncation not only makes the hypothesis disfluent but also causes a serious loss of information.</p>
<p><sup id="fnref2:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Noise Type</th>
<th style="text-align: left;">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Truncation</td>
<td style="text-align: left;">A portion of tokens at the end of the hypothesis are removed. e.g., She went to work. $\rightarrow$ She went</td>
</tr>
<tr>
<td style="text-align: center;">Article Removal</td>
<td style="text-align: left;">A random portion of articles (the/a/an) in the hypothesis are removed.</td>
</tr>
<tr>
<td style="text-align: center;">Preposition Removal</td>
<td style="text-align: left;">A random portion of prepositions are removed. e.g., She went to work. $\rightarrow$ She went work.</td>
</tr>
<tr>
<td style="text-align: center;">Verb Lemmatization</td>
<td style="text-align: left;">A random portion of verbs in the hypothesis are lemmatized. e.g., She went $\ldots \rightarrow$ She go $\ldots$</td>
</tr>
<tr>
<td style="text-align: center;">Sentence Switching</td>
<td style="text-align: left;">Several random pairs of sentences in the hypothesis are switched, breaking temporal/logical order.</td>
</tr>
<tr>
<td style="text-align: center;">Sentence Replacement</td>
<td style="text-align: left;">Several sentences in the hypothesis are replaced by a random irrelevant sentence.</td>
</tr>
<tr>
<td style="text-align: center;">Negation</td>
<td style="text-align: left;">A random portion of sentences are negated. e.g., She went $\ldots \rightarrow$ She did not go $\ldots$</td>
</tr>
</tbody>
</table>
<p>Table 7: Descriptions of a subset of the fluency (top) and consistency tests (bottom). Note that the truncation test not only breaks fluency but also causes loss of information. The complete set is described in Table 16 (Appendix I).</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Selected results for fluency \&amp; consistency tests. For each plot, the x -axis is noise-ratio and the y -axis is the metric score. The point at noise-ratio zero is the score for the gold hypotheses. Non-monotonically-decreasing curves are highlighted in bold. The shaded region indicates one standard deviation over 5 random seeds. Complete results are available in Appendix I.</p>
<p>The analysis in Figure 5 offers an insight into the reason behind, where the values of three variants of BERTScore under the truncation test are plotted. We observe that precision increases with more truncation, canceling out the decrease in recall and leading to a non-decreasing f-measure. We conjecture that this happens due to the property of the dataset, where earlier parts of different summaries (of the same article) are more likely to overlap than the rear spans. In Figure 8 (Appendix I), we show a similar observation for BARTScore-para.</p>
<p>In comparison, all metrics pass the truncation test for WMT. We believe the reason is that in the WMT data, the gold hypothesis and the reference are highly similar (They mostly only differ by a few tokens). Therefore, it would be easier for the metrics to catch the loss of information.</p>
<p>Two metrics fail the sentence switching test: BARTScore-para-recall (Figure 12), and MAUVE-</p>
<p>GPT2/RoBERTa (Figure 14). This result is more striking for MAUVE, as the hypotheses in WikiText typically contain a number of sentences, and the temporal or logical order is seriously disturbed by sentence switching (examples in Table 20, Appendix I). Note that considering the positioned error test of MAUVE, for the WikiText data, we intentionally do not switch the last sentence of the hypothesis paragraph.</p>
<p>Interestingly, MAUVE-ELECTRA passes sentence switching and other tests. We surmise this is due to the discriminative training of ELECTRA, making it sensitive to errors in the text. We also find that MAUVE-ELECTRA performs best in a human correlation evaluation (Appendix C.3). Therefore, within the scope of this work, ELECTRA is the best-performing feature for MAUVE. Appendix I contains more analysis on sentence switching.</p>
<p>However, also shown in Figure 4, MAUVE-</p>
<p>ELECTRA penalizes some error types more drastically (e.g., article/preposition removal) compared to other metrics, which means it may benefit from some further calibration, and we leave it as future work.</p>
<p>Implication Undesirable behaviors from the truncation test suggest that practitioners should either report all of the precision, recall, and f-measure for a complete picture or calibrate the f-measure to put more weight on recall than on precision.</p>
<p>The sentence switching test shows MAUVERoBERTa's insensitivity to the temporal/logical disorder. We suggest use MAUVE-RoBERTa in combination with GPT-PPL.</p>
<h2>6 Discussion</h2>
<p>The Copy-Source and the Repetition Tests To save space, the copy-source test is deferred to Appendix D because its results are relatively unsurprising. We also defer the repetition test to Appendix E, as it is motivated by the well-known degeneration problem (Holtzman et al., 2020).</p>
<p>Towards Automatic Detection The tests we design rely on some level of understanding of the PLMs, or a detailed examination of the metric definitions. A natural next question is whether we can automate this process. As a case study, we focus on BERTScore and build a toy example, showing that one can design an adversarial attack algorithm (Cheng et al., 2018) to detect sample-level anomaly. We defer it to Appendix J.</p>
<p>We devote the rest of this section to prevent potential misunderstandings since this work contains negative results.</p>
<p>For Metric Users The results in this work should be regarded as complementary to the impressive human correlation results in the literature. For example, BLEU passes all our tests in translation, however, it is outperformed by PLM-based metrics in human correlation evaluations (Zhang et al., 2020). If a metric fails one of our tests, it only means the metric needs improvement on that particular aspect. Our main message is not to discourage the use of PLM-based metrics, nor to devalue existing work by metric developers or users. Instead, we suggest use the metrics with caution and with awareness of the blind spots.</p>
<p>For Metric Developers While we have covered a large variety of stress tests in this work and we encourage future metric developers to use them
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: How the three variants of BERTScore react to the truncation test for the summarization task.
for robustness analysis, the set is not exhaustive. Even if a metric passes all our tests, it does not guarantee that the metric is blind-spot-free. We also encourage developers to come up with novel tests targeting certain underlying property of their proposed metric (e.g., the positioned error test we design for MAUVE).</p>
<h2>7 Related Work</h2>
<p>Analysis of NLG Metrics In comparison to the vast literature on NLG metric development or benchmarking (Mathur et al., 2020; Celikyilmaz et al., 2020; Gehrmann et al., 2021; Kasai et al., 2022b; Hämäläinen and Alnajjar, 2021), the robustness analysis of PLM-based metrics is an underexplored area, where exisiting work focused on a relatively small subset of metrics or a limited definition of robustness. For example, Vu et al. (2022) explored BERTScore's performance variation with changes in representation space and character perturbations. Kaster et al. (2021) propose a regression-based global explainability technique to disentangle metric scores along linguistic factors.</p>
<p>More related to our work, Hanna and Bojar (2021) conducted a fine-grained analysis of BERTScore on different error types. Caglayan et al. (2020) discussed some curious phenomena for a range of metrics. Chen et al. (2021) conducted diagnostic tests for factuality metrics with synthesized errors. Sun et al. (2022) found that some metrics are not robust to dialects. In comparison, this work is more comprehensive in that the design of our tests are inspired by a wider range of motivations, e.g., the properties of the underlying PLMs.</p>
<p>Synthetic Data for NLP Model Analysis The use of synthetic data has been proven to be a powerful tool to analyze the capabilities of NLP models in tasks including natural language inference (McCoy et al., 2019; Naik et al., 2018), question an-</p>
<p>swering (Ribeiro et al., 2019), reading comprehension (Sugawara et al., 2020) and text classification (Prabhakaran et al., 2019). Ribeiro et al. (2020) proposed a task-agnostic methodology, which synthesizes a large number of examinations for NLP models. Ruder et al. (2021) subsequently extended this methodology to a multilingual setting. Goel et al. (2021) built a more complete model evaluation system by integrating subpopulations, transformations, evaluation sets, and adversarial attacks. This work follows the same high-level spirit, while our focus is on NLG metrics.</p>
<p>Analysis of PLM This work takes inspiration from research analyzing the behavior of PLM's representations (Belinkov and Glass, 2019). Masked LMs such as BERT have been shown to be insensitive to word order (Pham et al., 2021), negation (Ettinger, 2020), and named entities (Balasubramanian et al., 2020). GPT-like models were shown to prefer repetitive text (Holtzman et al., 2020). Staliūnaitė and Iacobacci (2020) studies what types of linguistic knowledge BERT acquires with a focus on compositional and lexical semantics. There are also important lines of work on layer representation probing (Belinkov, 2022), or attention analysis (Dong et al., 2021; Ji et al., 2022).</p>
<h2>8 Conclusion</h2>
<p>Using PLMs for NLG metrics is a double-edged sword. While the metrics benefit from the models' powerful representations, their black-box nature may cause unexpected behavior. This work shows that stress tests, complementary to the standard human correlation tests, are powerful tools to cover corner cases, detect the metrics' blind spots, and point out aspects where the metric could improve.</p>
<p>As a major implication for metric users, we suggest using combinations of metrics so that they can cover each other's blind spots. While this has been an existing practice for a majority of work in the field, our results on the blind spots provide an explicit empirical argument for its importance. While we are still positive about the future of using PLM for NLG metrics, we call for more caution and awareness of potential blind spots from both metric users and developers. More generally speaking, a deeper understanding of the PLMs is in need.</p>
<h2>Limitations</h2>
<p>We have primarily focused our analysis on similarity or log-probability based metrics for NLG. There
are other important and interesting metrics that future work could examine. For example, Deng et al. (2021) developed a family of interpretable metrics for various NLG tasks with the concept of information alignment. Xu et al. (2022) recently proposed a metric based on stratified error synthesis. In addition, there are several task-specific metrics for paraphrase generation (Shen et al., 2022), image captioning (Hessel et al., 2021; Kasai et al., 2022a), dialogue (Mehri and Eskenazi, 2020), controlled text generation (Ke et al., 2022), etc., which would be interesting to evaluate.</p>
<p>In $\S 5.5$, we design a number of fluency and consistency tests. It would be interesting to expand this set to be broader or more sophisticated ( Ng et al., 2014). Also, there are other important aspects of text generation to consider, such as factuality (Wang et al., 2020; Pagnoni et al., 2021).</p>
<p>All of our diagnostic data are synthetically created. While it provides valuable insights on the metric's behavior, it does not have a good coverage of errors in real-world settings. Expanding our analysis to real-world errors in a scalable way would be an important future direction.</p>
<p>Last but not least, we evaluate our proposed stress tests only on English texts. However, many language-specific properties can induce potential blind spots for metrics, especially for low-resource languages (Haddow et al., 2022) where PLMs may provide poor text representations. An important future direction is expanding the tests to multilingual settings (Thompson and Post, 2020; Pires et al., 2019).</p>
<h2>Ethics Statement</h2>
<p>Although the goal of our study is for more reliable evaluation, there is a risk of dual use of our tests: We investigate stress tests to identify blind spots in existing generation metrics, but a subset of the approaches (e.g., copy-source or injection) could be used for cheating in an evaluation. By an explicit discussion of how these blind spots can be utilized, we hope to increase awareness in the community of scenarios in which the metrics are not perfect and could be manipulated. Towards mitigating the risks, we have discussed countermeasures that can be adopted to cover or detect such blind spots.</p>
<h2>Acknowledgements</h2>
<p>We sincerely thank Jungo Kasai and Xiaochuang Han for useful discussions. This material is based</p>
<p>upon work supported by the DARPA CMO under Contract No. HR001120C0124, by the National Science Foundation (NSF) under Grants No. IIS2203097, IIS2125201, IIS2040926, and NSF CAREER Grant No. IIS2142739. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the funding agencies.</p>
<h2>References</h2>
<p>Farhad Akhbardeh, Arkady Arkhangorodsky, Magdalena Biesialska, Ondřej Bojar, Rajen Chatterjee, Vishrav Chaudhary, Marta R. Costa-jussa, Cristina España-Bonet, Angela Fan, Christian Federmann, Markus Freitag, Yvette Graham, Roman Grundkiewicz, Barry Haddow, Leonie Harter, Kenneth Heafield, Christopher Homan, Matthias Huck, Kwabena Amponsah-Kaakyire, Jungo Kasai, Daniel Khashabi, Kevin Knight, Tom Kocmi, Philipp Koehn, Nicholas Lourie, Christof Monz, Makoto Morishita, Masaaki Nagata, Ajay Nagesh, Toshiaki Nakazawa, Matteo Negri, Santanu Pal, Allahsera Auguste Tapo, Marco Turchi, Valentin Vydrin, and Marcos Zampieri. 2021. Findings of the 2021 conference on machine translation (WMT21). In Proceedings of the Sixth Conference on Machine Translation, pages 1-88, Online. Association for Computational Linguistics.</p>
<p>Sriram Balasubramanian, Naman Jain, Gaurav Jindal, Abhijeet Awasthi, and Sunita Sarawagi. 2020. What's in a name? are BERT named entity representations just as good for any other name? In Proceedings of the 5th Workshop on Representation Learning for NLP, pages 205-214, Online. Association for Computational Linguistics.</p>
<p>Yonatan Belinkov. 2022. Probing Classifiers: Promises, Shortcomings, and Advances. Computational Linguistics, 48(1):207-219.</p>
<p>Yonatan Belinkov and James Glass. 2019. Analysis Methods in Neural Language Processing: A Survey. Transactions of the Association for Computational Linguistics, 7:49-72.</p>
<p>Ozan Caglayan, Pranava Madhyastha, and Lucia Specia. 2020. Curious case of language generation evaluation metrics: A cautionary tale. In Proceedings of the 28th International Conference on Computational Linguistics, pages 2322-2328, Barcelona, Spain (Online). International Committee on Computational Linguistics.</p>
<p>Asli Celikyilmaz, Elizabeth Clark, and Jianfeng Gao. 2020. Evaluation of text generation: A survey. ArXiv, abs/2006.14799.</p>
<p>Yiran Chen, Pengfei Liu, and Xipeng Qiu. 2021. Are factuality checkers reliable? adversarial meta-
evaluation of factuality in summarization. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2082-2095, Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Minhao Cheng, Jinfeng Yi, Huan Zhang, Pin-Yu Chen, and Cho-Jui Hsieh. 2018. Seq2sick: Evaluating the robustness of sequence-to-sequence models with adversarial examples. CoRR, abs/1803.01128.</p>
<p>Kevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning. 2020. Electra: Pre-training text encoders as discriminators rather than generators. In International Conference on Learning Representations.</p>
<p>Mingkai Deng, Bowen Tan, Zhengzhong Liu, Eric Xing, and Zhiting Hu. 2021. Compression, transduction, and creation: A unified framework for evaluating natural language generation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7580-7605, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Yue Dong, Chandra Bhagavatula, Ximing Lu, Jena D. Hwang, Antoine Bosselut, Jackie Chi Kit Cheung, and Yejin Choi. 2021. On-the-fly attention modulation for neural generation. In Findings of the Association for Computational Linguistics: ACLIJCNLP 2021, pages 1261-1274, Online. Association for Computational Linguistics.</p>
<p>Kevin Duh. 2018. The multitarget ted talks task. http://www.cs.jhu.edu/ kevinduh/a/ multitarget-tedtalks/.</p>
<p>Allyson Ettinger. 2020. What bert is not: Lessons from a new suite of psycholinguistic diagnostics for language models. Transactions of the Association for Computational Linguistics, 8:34-48.</p>
<p>Angela Fan, Mike Lewis, and Yann Dauphin. 2018. Hierarchical neural story generation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 889-898, Melbourne, Australia. Association for Computational Linguistics.</p>
<p>Sebastian Gehrmann, Tosin Adewumi, Karmanya Aggarwal, Pawan Sasanka Ammanamanchi, Anuoluwapo Aremu, Antoine Bosselut, Khyathi Raghavi Chandu, Miruna-Adriana Clinciu, Dipanjan Das, Kaustubh Dhole, Wanyu Du, Esin</p>
<p>Durmus, Ondřej Dušek, Chris Chinenye Emezue, Varun Gangal, Cristina Garbacea, Tatsunori Hashimoto, Yufang Hou, Yacine Jernite, Harsh Jhamtani, Yangfeng Ji, Shailza Jolly, Mihir Kale, Dhruv Kumar, Faisal Ladhak, Aman Madaan, Mounica Maddela, Khyati Mahajan, Saad Mahamood, Bodhisattwa Prasad Majumder, Pedro Henrique Martins, Angelina McMillan-Major, Simon Mille, Emiel van Miltenburg, Moin Nadeem, Shashi Narayan, Vitaly Nikolaev, Andre Niyongabo Rubungo, Salomey Osei, Ankur Parikh, Laura Perez-Beltrachini, Niranjan Ramesh Rao, Vikas Raunak, Juan Diego Rodriguez, Sashank Santhanam, João Sedoc, Thibault Sellam, Samira Shaikh, Anastasia Shimorina, Marco Antonio Sobrevilla Cabezudo, Hendrik Strobelt, Nishant Subramani, Wei Xu, Diyi Yang, Akhila Yerukola, and Jiawei Zhou. 2021. The GEM benchmark: Natural language generation, its evaluation and metrics. In Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021), pages 96-120, Online. Association for Computational Linguistics.</p>
<p>Karan Goel, Nazneen Fatema Rajani, Jesse Vig, Zachary Taschdjian, Mohit Bansal, and Christopher Ré. 2021. Robustness gym: Unifying the NLP evaluation landscape. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Demonstrations, pages 42-55, Online. Association for Computational Linguistics.</p>
<p>Barry Haddow, Rachel Bawden, Antonio Valerio Miceli Barone, Jindřich Helcl, and Alexandra Birch. 2022. Survey of low-resource machine translation. Computational Linguistics, 48(3):673-732.</p>
<p>Mika Hämäläinen and Khalid Alnajjar. 2021. Human evaluation of creative NLG systems: An interdisciplinary survey on recent papers. In Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021), pages 84-95, Online. Association for Computational Linguistics.</p>
<p>Michael Hanna and Ondřej Bojar. 2021. A fine-grained analysis of BERTScore. In Proceedings of the Sixth Conference on Machine Translation, pages 507-517, Online. Association for Computational Linguistics.</p>
<p>Tianxing He and James Glass. 2019. Detecting egregious responses in neural sequence-to-sequence models. In International Conference on Learning Representations.</p>
<p>Tianxing He, Jingzhao Zhang, Zhiming Zhou, and James Glass. 2021. Exposure bias versus selfrecovery: Are distortions really incremental for autoregressive text generation? In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 5087-5102, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman,
and Phil Blunsom. 2015. Teaching machines to read and comprehend. Advances in neural information processing systems, 28:1693-1701.</p>
<p>Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. 2021. CLIPScore: A reference-free evaluation metric for image captioning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7514-7528, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2020. The curious case of neural text degeneration. In International Conference on Learning Representations.
J. Edward Hu, Abhinav Singh, Nils Holzenberger, Matt Post, and Benjamin Van Durme. 2019. Large-scale, diverse, paraphrastic bitexts via sampling and clustering. In Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL), pages 44-54, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Jiabao Ji, Yoon Kim, James Glass, and Tianxing He. 2022. Controlling the focus of pretrained language generation models. In Findings of the Association for Computational Linguistics: ACL 2022, pages 32913306, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Jungo Kasai, Keisuke Sakaguchi, Lavinia Dunagan, Jacob Morrison, Ronan Le Bras, Yejin Choi, and Noah A. Smith. 2022a. Transparent human evaluation for image captioning. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3464-3478, Seattle, United States. Association for Computational Linguistics.</p>
<p>Jungo Kasai, Keisuke Sakaguchi, Ronan Le Bras, Lavinia Dunagan, Jacob Morrison, Alexander Fabbri, Yejin Choi, and Noah A. Smith. 2022b. Bidimensional leaderboards: Generate and evaluate language hand in hand. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3540-3557, Seattle, United States. Association for Computational Linguistics.</p>
<p>Marvin Kaster, Wei Zhao, and Steffen Eger. 2021. Global explainability of BERT-based evaluation metrics by disentangling along linguistic factors. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 89128925, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Pei Ke, Hao Zhou, Yankai Lin, Peng Li, Jie Zhou, Xiaoyan Zhu, and Minlie Huang. 2022. CTRLEval: An unsupervised reference-free metric for evaluating controlled text generation. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages</p>
<p>2306-2319, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Urvashi Khandelwal, He He, Peng Qi, and Dan Jurafsky. 2018. Sharp nearby, fuzzy far away: How neural language models use context. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 284-294, Melbourne, Australia. Association for Computational Linguistics.</p>
<p>Wojciech Kryscinski, Bryan McCann, Caiming Xiong, and Richard Socher. 2020. Evaluating the factual consistency of abstractive text summarization. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 9332-9346, Online. Association for Computational Linguistics.</p>
<p>Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. 2019. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461.</p>
<p>Linyang Li, Ruotian Ma, Qipeng Guo, Xiangyang Xue, and Xipeng Qiu. 2020. BERT-ATTACK: Adversarial attack against BERT using BERT. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6193-6202, Online. Association for Computational Linguistics.</p>
<p>Alisa Liu, Maarten Sap, Ximing Lu, Swabha Swayamdipta, Chandra Bhagavatula, Noah A. Smith, and Yejin Choi. 2021. DExperts: Decoding-time controlled text generation with experts and anti-experts. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 6691-6706, Online. Association for Computational Linguistics.</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach.</p>
<p>John I Marden. 1995. Analyzing and modeling rank data. Chapman Hall, London.</p>
<p>Nitika Mathur, Timothy Baldwin, and Trevor Cohn. 2020. Tangled up in BLEU: Reevaluating the evaluation of automatic machine translation evaluation metrics. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4984-4997, Online. Association for Computational Linguistics.</p>
<p>Tom McCoy, Ellie Pavlick, and Tal Linzen. 2019. Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference. In Proceedings of
the 57th Annual Meeting of the Association for Computational Linguistics, pages 3428-3448, Florence, Italy. Association for Computational Linguistics.</p>
<p>Shikib Mehri and Maxine Eskenazi. 2020. USR: An unsupervised and reference free evaluation metric for dialog generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 681-707, Online. Association for Computational Linguistics.</p>
<p>Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2016. Pointer sentinel mixture models. CoRR, abs/1609.07843.</p>
<p>Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. 2022. Cross-task generalization via natural language crowdsourcing instructions. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3470-3487, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Aakanksha Naik, Abhilasha Ravichander, Norman Sadeh, Carolyn Rose, and Graham Neubig. 2018. Stress test evaluation for natural language inference. In Proceedings of the 27th International Conference on Computational Linguistics, pages 2340-2353, Santa Fe, New Mexico, USA. Association for Computational Linguistics.</p>
<p>Hwee Tou Ng, Siew Mei Wu, Ted Briscoe, Christian Hadiwinoto, Raymond Hendy Susanto, and Christopher Bryant. 2014. The CoNLL-2014 shared task on grammatical error correction. In Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task, pages 1-14, Baltimore, Maryland. Association for Computational Linguistics.</p>
<p>Artidoro Pagnoni, Vidhisha Balachandran, and Yulia Tsvetkov. 2021. Understanding factuality in abstractive summarization with FRANK: A benchmark for factuality metrics. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4812-4829, Online. Association for Computational Linguistics.</p>
<p>Thang Pham, Trung Bui, Long Mai, and Anh Nguyen. 2021. Out of order: How important is the sequential order of words in a sentence in natural language understanding tasks? pages 1145-1160.</p>
<p>Krishna Pillutla, Swabha Swayamdipta, Rowan Zellers, John Thickstun, Sean Welleck, Yejin Choi, and Zaïd Harchaoui. 2021. Mauve: Measuring the gap between neural text and human text using divergence frontiers. In Neural Information Processing Systems.</p>
<p>Telmo Pires, Eva Schlinger, and Dan Garrette. 2019. How multilingual is multilingual BERT? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4996-5001, Florence, Italy. Association for Computational Linguistics.</p>
<p>Vinodkumar Prabhakaran, Ben Hutchinson, and Margaret Mitchell. 2019. Perturbation sensitivity analysis to detect unintended model biases. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 5740-5745, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1-67.</p>
<p>Ricardo Rei, Craig Stewart, Ana C Farinha, and Alon Lavie. 2020. COMET: A neural framework for MT evaluation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2685-2702, Online. Association for Computational Linguistics.</p>
<p>Marco Tulio Ribeiro, Carlos Guestrin, and Sameer Singh. 2019. Are red roses red? evaluating consistency of question-answering models. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6174-6184, Florence, Italy. Association for Computational Linguistics.</p>
<p>Marco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin, and Sameer Singh. 2020. Beyond accuracy: Behavioral testing of NLP models with CheckList. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 49024912, Online. Association for Computational Linguistics.</p>
<p>Sebastian Ruder, Noah Constant, Jan Botha, Aditya Siddhant, Orhan Firat, Jinlan Fu, Pengfei Liu, Junjie Hu, Dan Garrette, Graham Neubig, and Melvin Johnson. 2021. XTREME-R: Towards more challenging and nuanced multilingual evaluation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 10215-10245, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Julian Salazar, Davis Liang, Toan Q. Nguyen, and Katrin Kirchhoff. 2020. Masked language model scoring. In Annual Meeting of the Association for Computational Linguistics.</p>
<p>Thibault Sellam, Dipanjan Das, and Ankur P Parikh. 2020. Bleurt: Learning robust metrics for text generation. In Proceedings of ACL.</p>
<p>Lingfeng Shen, Lemao Liu, Haiyun Jiang, and Shuming Shi. 2022. On the evaluation metrics for paraphrase generation. In Proceedings of the 2022 Conference
on Empirical Methods in Natural Language Processing, pages 3178-3190, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.</p>
<p>Lucia Specia, Frédéric Blain, Marina Fomicheva, Chrysoula Zerva, Zhenhao Li, Vishrav Chaudhary, and André F. T. Martins. 2021. Findings of the WMT 2021 shared task on quality estimation. In Proceedings of the Sixth Conference on Machine Translation, pages 684-725, Online. Association for Computational Linguistics.</p>
<p>Ieva Staliūnaitė and Ignacio Iacobacci. 2020. Compositional and lexical semantics in RoBERTa, BERT and DistilBERT: A case study on CoQA. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7046-7056, Online. Association for Computational Linguistics.</p>
<p>Saku Sugawara, Pontus Stenetorp, Kentaro Inui, and Akiko Aizawa. 2020. Assessing the benchmarking capacity of machine reading comprehension datasets. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 8918-8927.</p>
<p>Jiao Sun, Thibault Sellam, Elizabeth Clark, Tu Vu, Timothy Dozat, Dan Garrette, Aditya Siddhant, Jacob Eisenstein, and Sebastian Gehrmann. 2022. Dialectrobust evaluation of generated text.</p>
<p>Brian Thompson and Matt Post. 2020. Automatic machine translation evaluation in many languages via zero-shot paraphrasing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 90-121, Online. Association for Computational Linguistics.</p>
<p>Jesse Vig and Yonatan Belinkov. 2019. Analyzing the structure of attention in a transformer language model. In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 63-76, Florence, Italy. Association for Computational Linguistics.</p>
<p>Doan Nam Long Vu, Nafise Sadat Moosavi, and Steffen Eger. 2022. Layer or representation space: What makes BERT-based evaluation metrics robust? In Proceedings of the 29th International Conference on Computational Linguistics, pages 3401-3411, Gyeongju, Republic of Korea. International Committee on Computational Linguistics.</p>
<p>Alex Wang, Kyunghyun Cho, and Mike Lewis. 2020. Asking and answering questions to evaluate the factual consistency of summaries. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5008-5020, Online. Association for Computational Linguistics.</p>
<p>Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V Le. 2022. Finetuned language models are zero-shot learners. In International Conference on Learning Representations.</p>
<p>Sean Welleck, Ilia Kulikov, Stephen Roller, Emily Dinan, Kyunghyun Cho, and Jason Weston. 2020. Neural text generation with unlikelihood training. In International Conference on Learning Representations.</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38-45, Online. Association for Computational Linguistics.</p>
<p>Wenda Xu, Yi-lin Tuan, Yujie Lu, Michael Saxon, Lei Li, and William Yang Wang. 2022. Not all errors are equal: Learning text generation metrics using stratified error synthesis. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing.</p>
<p>Kevin Yang and Dan Klein. 2021. FUDGE: Controlled text generation with future discriminators. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3511-3535, Online. Association for Computational Linguistics.</p>
<p>Weizhe Yuan, Graham Neubig, and Pengfei Liu. 2021. Bartscore: Evaluating generated text as text generation. In Advances in Neural Information Processing Systems, volume 34, pages 27263-27277. Curran Associates, Inc.</p>
<p>Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022. Opt: Open pretrained transformer language models.</p>
<p>Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: Evaluating text generation with BERT. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.</p>
<p>Yizhe Zhang, Michel Galley, Jianfeng Gao, Zhe Gan, Xiujun Li, Chris Brockett, and Bill Dolan. 2018. Generating informative and diverse conversational responses via adversarial information maximization. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems 31, pages 1815-1825. Curran Associates, Inc.</p>
<p>Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Christian M. Meyer, and Steffen Eger. 2019. MoverScore:</p>
<p>Text generation evaluating with contextualized embeddings and earth mover distance. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 563-578, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Ming Zhong, Yang Liu, Da Yin, Yuning Mao, Yizhu Jiao, Pengfei Liu, Chenguang Zhu, Heng Ji, and Jiawei Han. 2022. Towards a unified multidimensional evaluator for text generation. CoRR, abs/2210.07197.</p>
<h2>Supplemental Materials</h2>
<h2>A Implementation Details of Metrics or Tests</h2>
<p>MLM-PPL The high-level motivation for MLMPPL (Salazar et al., 2020) is using a bidirectional masked language model to compute a quantity similar to next-token perplexity in autoregressive models, by masking candidate tokens one by one and obtaining perplexity from masked token log probability. We follow a similar formulation of the "pseudo-perplexity" in Salazar et al. (2020). Given a sequence $\boldsymbol{W}=\left(\boldsymbol{w}<em _boldsymbol_W="|\boldsymbol{W">{1}, \ldots, \boldsymbol{w}</em>}|}\right)$, we replace a token $\boldsymbol{w<em _backslash="\backslash" t="t">{t}$ with the mask token [M], and predict it using all past and future tokens $\boldsymbol{W}</em>}=\left(\boldsymbol{w<em t-1="t-1">{1}, \ldots, \boldsymbol{w}</em>},[M], \boldsymbol{w<em _boldsymbol_W="|\boldsymbol{W">{t+1}, \ldots, \boldsymbol{w}</em>}|}\right)$. Let $\log P_{\mathrm{MLM}}\left(\boldsymbol{w<em _backslash="\backslash" t="t">{t} \mid \boldsymbol{W}</em>$ given its context. MLM-PPL is defined as below:}\right)$ denote the conditional log probability of predicting each token $\boldsymbol{w}_{t</p>
<p>$$
\begin{aligned}
&amp; \operatorname{MLM}-\operatorname{PPL}(\boldsymbol{W})= \
&amp; \exp \left(-\frac{1}{|\boldsymbol{W}|} \sum_{t=1}^{|\boldsymbol{W}|} \log P_{\mathrm{MLM}}\left(\boldsymbol{w}<em _backslash="\backslash" t="t">{t} \mid \boldsymbol{W}</em>\right)\right)
\end{aligned}
$$</p>
<p>MAUVE We use the default hyperparameter settings recommended in Pillutla et al. (2021). $c=5$ is set for the scaling constant. For the quantization algorithm, we use $k$-means with 500 iterations and $n / 10$ clusters, where $n$ is the number of generations.</p>
<p>We now explain why we set the reference set to be different from the gold set. According to the definition of MAUVE, if we set the gold and ref set to be exactly the same, then the score for the gold set will be 1.0 (full-score). In this setting, any stress test will be passed because the score of the perturbed set can only be lower. Since MAUVE is a distribution-based metric, in principle it is enough to ensure that the ref set is from the data distribution.</p>
<p>BERTScore As suggested by Zhang et al. (2020), the f-measure variant of BERTScore is used for translation. However, the paper does not have recommendations for summarization. Therefore we test all three variants (precision, recall, f-measure).</p>
<p>BARTScore As introduced in Yuan et al. (2021), BARTScore has four variants to tackle different scenarios, and each variant defines a pair of inputoutput for BART: precision (reference to hypothesis), recall (hypothesis to reference), f-measure, and faithfulness (source to hypothesis).</p>
<p>As suggested by the paper, for translation we use the f-measure. However, for summarization, the recommendations are a bit vague. In the main sections, we mainly report the faithfulness variant as it is used by the paper for the SummEval dataset (which is based on CNNDM). We also test the other three variants and defer their results to the appendix.</p>
<p>In addition to BARTScore-cnn and BARTScorepara, BARTScore also has a prompted modeling option which we currently do not have the capacity to test. We leave it as future work.</p>
<p>ROUGE Following common practice, we use the f-measure of ROUGE-2 or ROUGE-L.</p>
<p>Test Implementation Our test code for translation or summarization is built upon the released code from BARTScore. ${ }^{8}$ We also benefit from the Hugging Face library (Wolf et al., 2020). ${ }^{9}$ Some fluency and consistency tests are built using the spaCy library. ${ }^{10}$ For the negation test, we utilize released code from the NLP CheckList (Ribeiro et al., 2020). ${ }^{11}$</p>
<h2>B More Information on Datasets</h2>
<h2>B. 1 The TED-MT Dataset</h2>
<p>We find it hard to locate a public MT dataset satisfying: (1) Each sample has multiple references. (2) Each sample contains multiple sentences. Therefore, we decide to manually build one.</p>
<p>We build a paragraph-level translation dataset based on the Zh-En part of the Multitarget TED Talks Task (MTTT) (Duh, 2018). The original dataset contains consecutive sentences in a TED talk. We first manually form 100 coherent paragraphs by selecting spans of samples in the test and dev splits. Each paragraph contains at least 4 sentences and at most 10 sentences. Correspondingly, the English reference of the paragraph is the concatenation of the reference of each sentence.</p>
<p>One additional translation for each sample is needed. Two graduate students who are fluent in both English and Chinese help provide one additional translation for each paragraph. Each translator handles 50 samples. And then the translations are switched so that they can correct each other's errors. An example is given in Table 19. In our</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Noise Type</th>
<th style="text-align: center;">MAUVE Variant</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT2</td>
<td style="text-align: center;">RoBERTa</td>
<td style="text-align: center;">ELECTRA</td>
</tr>
<tr>
<td style="text-align: center;">Gold</td>
<td style="text-align: center;">$0.961_{[0.007]}$</td>
<td style="text-align: center;">$0.969_{[0.007]}$</td>
<td style="text-align: center;">$0.966_{[0.010]}$</td>
</tr>
<tr>
<td style="text-align: center;">Random-Start</td>
<td style="text-align: center;">$0.949_{[0.016]}(-1.3 \%)$</td>
<td style="text-align: center;">$0.037_{[0.007]}(-96.1 \%)$</td>
<td style="text-align: center;">$0.025_{[0.002]}(-97.4 \%)$</td>
</tr>
<tr>
<td style="text-align: center;">Random-Middle</td>
<td style="text-align: center;">$0.898_{[0.034]}(-6.5 \%)$</td>
<td style="text-align: center;">$0.100_{[0.013]}(-89.7 \%)$</td>
<td style="text-align: center;">$0.032_{[0.004]}(-96.6 \%)$</td>
</tr>
<tr>
<td style="text-align: center;">Random-End</td>
<td style="text-align: center;">$0.005_{[0.039]}(-99.4 \%)$</td>
<td style="text-align: center;">$0.036_{[0.014]}(-96.3 \%)$</td>
<td style="text-align: center;">$0.010_{[0.003]}(-99.0 \%)$</td>
</tr>
<tr>
<td style="text-align: center;">Shuffle-Start</td>
<td style="text-align: center;">$0.916_{[0.013]}(-4.7 \%)$</td>
<td style="text-align: center;">$0.342_{[0.027]}(-64.7 \%)$</td>
<td style="text-align: center;">$0.044_{[0.013]}(-95.5 \%)$</td>
</tr>
<tr>
<td style="text-align: center;">Shuffle-Middle</td>
<td style="text-align: center;">$0.943_{[0.001]}(-1.8 \%)$</td>
<td style="text-align: center;">$0.603_{[0.005]}(-37.8 \%)$</td>
<td style="text-align: center;">$0.164_{[0.001]}(-83.1 \%)$</td>
</tr>
<tr>
<td style="text-align: center;">Shuffle-End</td>
<td style="text-align: center;">$0.020_{[0.002]}(-97.9 \%)$</td>
<td style="text-align: center;">$0.242_{[0.024]}(-75.0 \%)$</td>
<td style="text-align: center;">$0.041_{[0.005]}(-95.7 \%)$</td>
</tr>
</tbody>
</table>
<p>Table 8: Complete results for the positioned error test. "Random" indicates the token span is replaced with random tokens from the vocabulary. "Shuffle" means the tokens within the span are shuffled in-place. MAUVE-GPT2 is insensitive to errors at the start and middle of hypotheses, while MAUVE-RoBERTa and -ELECTRA are more robust. The percentage shown is score change w.r.t. the gold hypotheses. The subscript shown is the standard deviation across 5 runs.
experiments, the original reference is set to be the the gold hypothesis, and the added translation is used as reference for the metrics.</p>
<p>We will make this dataset available in the public version of this manuscript.</p>
<h2>B. 2 WikiText Preprocessing</h2>
<p>For the gold/reference hypotheses of the WikiText103 dataset, we sample paragraphs with more than 256 tokens and conduct preprocessing to clean up dataset artifacts and special symbols. First, we trim extra space around ${$,', ',', '?', '!', ';', ';', '(', ')', "'s", '\%' $}$. Next, we remove the special token '@' in the dot '@.@' and hyphen '@-@' tokens. We also remove extra space around quotation marks. Finally, the text is truncated to the last full sentence under a total length of 256 , which is to ensure the gold hypotheses are of similar length.</p>
<h2>C Details on the Positioned Error Test</h2>
<h2>C. 1 Auxiliary Results</h2>
<p>The full set of results for the positioned error test is shown in Table 8. MAUVE-GPT2 is insensitive to errors at the start and middle positions. In contrast, both MAUVE-RoBERTa and MAUVE-ELECTRA give significantly lower scores for erroneous text compared to the gold hypothesis. We also observe MAUVE-ELECTRA is more sensitive compared to MAUVE-RoBERTa.</p>
<h2>C. 2 Attention Pattern Analysis</h2>
<p>Here we provide details about the attention pattern analysis. We input two random samples (non-cherry-picked) from the WikiText dataset to GPT2large and RoBERTa-large and visualize the attention distribution over the relative position in the</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Decoding</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">GPT2-small</td>
<td style="text-align: center;">Nucleus $p=0.9$</td>
</tr>
<tr>
<td style="text-align: center;">GPT2-small</td>
<td style="text-align: center;">Pure Sampling</td>
</tr>
<tr>
<td style="text-align: center;">GPT2-medium</td>
<td style="text-align: center;">Nucleus $p=0.9$</td>
</tr>
<tr>
<td style="text-align: center;">GPT2-medium</td>
<td style="text-align: center;">Pure Sampling</td>
</tr>
<tr>
<td style="text-align: center;">GPT2-large</td>
<td style="text-align: center;">Nucleus $p=0.95$</td>
</tr>
<tr>
<td style="text-align: center;">GPT2-large</td>
<td style="text-align: center;">Pure Sampling</td>
</tr>
<tr>
<td style="text-align: center;">GPT2-XL</td>
<td style="text-align: center;">Nucleus $p=0.95$</td>
</tr>
<tr>
<td style="text-align: center;">GPT2-XL</td>
<td style="text-align: center;">Pure Sampling</td>
</tr>
</tbody>
</table>
<p>Table 9: Generation settings for the test on MAUVE correlation with human judgment.
text. The sample is truncated to length 200 for the convenience of this analysis.</p>
<p>As shown in Figure 11, we average the attention distribution over all transformer layers and attention heads and then group $20 \times 20$ (attention-from and attention-to) tokens into one attention block for ease of presentation. We also include a highgranularity version where we group $2 \times 2$ tokens into one attention block.</p>
<h2>C. 3 MAUVE Correlation with Human Judgment</h2>
<p>We reproduce MAUVE's correlation with human judgment in Pillutla et al. (2021) on the three MAUVE variants based on GPT2, RoBERTa, and ELECTRA, on the WebText dataset with the released code. ${ }^{12}$ Note that Pillutla et al. (2021) only considered MAUVE-GPT2, and the correlation scores for the RoBERTa/ELECTRA variants were not tested.</p>
<p>We follow their pairwise setup of evaluation: Each annotator receives the prompt and continuation from two different generation settings and</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>MAUVE Variant</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>GPT2</td>
<td>RoBERTa</td>
<td>ELECTRA</td>
</tr>
<tr>
<td>Human-like</td>
<td>0.952</td>
<td>0.929</td>
<td>$\mathbf{0 . 9 7 6}$</td>
</tr>
<tr>
<td>Interesting</td>
<td>0.738</td>
<td>0.786</td>
<td>$\mathbf{0 . 8 5 7}$</td>
</tr>
<tr>
<td>Sensible</td>
<td>0.881</td>
<td>0.881</td>
<td>$\mathbf{0 . 9 7 6}$</td>
</tr>
</tbody>
</table>
<p>Table 10: Spearman rank correlation between MAUVE and human judgment on the WebText dataset for different metric variants.
selects the setting that is favored using a 5-point Likert scale. The annotators are asked about three aspects: whether the continuation is human-like, interesting, or sensible. There are 8 generation settings that consist of different (model, decoding) choices specified in Table 9 plus human written continuations. We use their provided human annotation directly. Also following Pillutla et al. (2021), we convert the pairwise preference scores into rankings by fitting a Bradley-Terry model (Marden, 1995), and compute the Spearman rank correlation between the MAUVE score and the fitted BradleyTerry coefficients. We refer readers to Pillutla et al. (2021) for more details.</p>
<p>The results are shown in Table 10. ${ }^{13}$ Compared to MAUVE-GPT2, although MAUVE-RoBERTa is slightly superior in the "interesting" aspect, it has a lower correlation on the human-like judgment. Nevertheless, MAUVE-ELECTRA shows a clearly superior correlation with human judgment on all three aspects compared to both the GPT-2 and RoBERTa variants. It also performs best in our stress tests.</p>
<h2>D The Copy-Source Test</h2>
<p>A number of metrics are based on the similarity between the hypothesis and the reference or source. Therefore, for tasks like summarization and translation, one could try to fool the metric by simply submitting a direct copy of the source text. We term it the copy-source test.</p>
<p>As reported in Table 11, for both translation and summarization datasets, we find that COMET-QE, BERTScore-r, several variants of BARTScore, and UniEval-overall not just fail to account for this simple trick but in fact obtain higher scores than gold hypotheses.</p>
<p><sup id="fnref8:0"><a class="footnote-ref" href="#fn:0">1</a></sup>| Metric (task) | GOLD | Copy-source |
| :--: | :--: | :--: |
| COMET(wmt) | 0.531 | -0.079 |
| COMET-QE(wmt) | 0.114 | 0.126 |
| COMET-QE (ted-mt) | 0.062 | 0.073 |
| BertSc-r(sum) | 0.266 | 0.332 |
| BertSc-p(sum) | 0.181 | -0.177 |
| BertSc-f(sum) | 0.223 | 0.065 |
| BartSc-cnn-p(sum) | -2.718 | -3.022 |
| BartSc-cnn-r(sum) | -3.249 | -2.834 |
| BartSc-cnn-f(sum) | -2.984 | -2.928 |
| BartSc-cnn-faithful(sum) | -1.376 | -0.368 |
| BS-cnn-failthful-noavg(sum) | -82.95 | -166.25 |
| BartSc-para-p(sum) | -4.023 | -4.218 |
| BartSc-para-r(sum) | -3.751 | -2.948 |
| BartSc-para-f(sum) | -3.887 | -3.583 |
| BartSc-para-faithful(sum) | -2.109 | -0.874 |
| COMET(sum) | -0.575 | -0.584 |
| COMET-QE(sum) | 0.059 | 0.048 |
| UniEval-coherence (sum) | 0.897 | 0.949 |
| UniEval-consistency (sum) | 0.859 | 0.946 |
| UniEval-fluency (sum) | 0.919 | 0.915 |
| UniEval-relevance (sum) | 0.781 | 0.869 |
| UniEval-overall (sum) | 0.864 | 0.920 |</p>
<p>Table 11: Results of the copy-source test. This simple trick could fool the metric and get scores higher than gold hypotheses. For COMET the scores from the copied source are very close to the gold hypothesis (marked in orange), which is undesirable.</p>
<p>We attribute these behaviors to some of the metrics' design choices. (1) COMET-QE relies on a cross-lingual RoBERTa encoder, but it does not check the language ID of the hypothesis. (2) BARTScore, computed as a length-averaged loglikelihood, fails to account for the length of the hypothesis, which in this case is the entire source article. While removing the average operation is a natural remedy and indeed leads to a lower score for the noised hypothesis (shown by BARTS-cnnnoavg in the table), it is not ideal as it would also favor overly short summaries. (3) BERTScore-r's behavior on summarization, on the other hand, is not surprising since it is recall-oriented, and is alleviated by using the f-measure. (4) The take on UniEval is more nuanced. Strictly speaking, the copied source does not degrade the four aspects UniEval reports. However, they lead to a misleadingly high overall score.</p>
<p>Implication The copy-source trick could be used to manipulate scores in a contest. Straightforward solutions can counter this trick. For example, contest organizers can implement checks for similarity between submitted hypotheses and the source text and reject the matches. For summarization,</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Test</th>
<th style="text-align: center;">Example</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Rep-2</td>
<td style="text-align: center;">$\ldots$ allegiance to one's family, despite the turmoil and dissensions that occur. dissensions that occur.</td>
</tr>
<tr>
<td style="text-align: center;">Freq 4-gram</td>
<td style="text-align: center;">$\ldots$ in the middle of the site of the the course of the as part of the the top of the on the billboard hot in the summer of for the rest of</td>
</tr>
</tbody>
</table>
<p>Table 12: Front-truncated examples of repetition (top) and the frequent n-gram (bottom) test on WikiText. Top50 4-grams are used.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Metric (task)</th>
<th style="text-align: center;">Gold</th>
<th style="text-align: center;">Repetition</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Rep-10</td>
<td style="text-align: center;">Rep-20</td>
<td style="text-align: center;">Rep-30</td>
</tr>
<tr>
<td style="text-align: center;">B-cnn-f (wmt)</td>
<td style="text-align: center;">$-2.168$</td>
<td style="text-align: center;">$-1.889$</td>
<td style="text-align: center;">$-1.721$</td>
<td style="text-align: center;">$-1.652$</td>
</tr>
<tr>
<td style="text-align: center;">B-para-f (wmt)</td>
<td style="text-align: center;">$-1.868$</td>
<td style="text-align: center;">$-1.956$</td>
<td style="text-align: center;">$-1.864$</td>
<td style="text-align: center;">$-1.839$</td>
</tr>
<tr>
<td style="text-align: center;">BLEURT (wmt)</td>
<td style="text-align: center;">0.716</td>
<td style="text-align: center;">0.666</td>
<td style="text-align: center;">0.683</td>
<td style="text-align: center;">0.689</td>
</tr>
<tr>
<td style="text-align: center;">B-cnn-p (sum)</td>
<td style="text-align: center;">$-2.718$</td>
<td style="text-align: center;">$-2.122$</td>
<td style="text-align: center;">$-1.675$</td>
<td style="text-align: center;">$-1.451$</td>
</tr>
<tr>
<td style="text-align: center;">B-cnn-r (sum)</td>
<td style="text-align: center;">$-3.249$</td>
<td style="text-align: center;">$-3.246$</td>
<td style="text-align: center;">$-3.251$</td>
<td style="text-align: center;">$-3.252$</td>
</tr>
<tr>
<td style="text-align: center;">B-cnn-f (sum)</td>
<td style="text-align: center;">$-2.984$</td>
<td style="text-align: center;">$-2.684$</td>
<td style="text-align: center;">$-2.463$</td>
<td style="text-align: center;">$-2.351$</td>
</tr>
<tr>
<td style="text-align: center;">B-cnn-faithful (sum)</td>
<td style="text-align: center;">$-1.376$</td>
<td style="text-align: center;">$-1.486$</td>
<td style="text-align: center;">$-1.224$</td>
<td style="text-align: center;">$-1.091$</td>
</tr>
<tr>
<td style="text-align: center;">B-para-p (sum)</td>
<td style="text-align: center;">$-4.023$</td>
<td style="text-align: center;">$-3.156$</td>
<td style="text-align: center;">$-2.630$</td>
<td style="text-align: center;">$-2.362$</td>
</tr>
<tr>
<td style="text-align: center;">B-para-r (sum)</td>
<td style="text-align: center;">$-3.751$</td>
<td style="text-align: center;">$-3.710$</td>
<td style="text-align: center;">$-3.693$</td>
<td style="text-align: center;">$-3.685$</td>
</tr>
<tr>
<td style="text-align: center;">B-para-f (sum)</td>
<td style="text-align: center;">$-3.887$</td>
<td style="text-align: center;">$-3.433$</td>
<td style="text-align: center;">$-3.162$</td>
<td style="text-align: center;">$-3.023$</td>
</tr>
<tr>
<td style="text-align: center;">B-para-faithful (sum)</td>
<td style="text-align: center;">$-2.109$</td>
<td style="text-align: center;">$-2.039$</td>
<td style="text-align: center;">$-1.759$</td>
<td style="text-align: center;">$-1.626$</td>
</tr>
<tr>
<td style="text-align: center;">GPT-PPL (wiki)</td>
<td style="text-align: center;">$-21.81$</td>
<td style="text-align: center;">$-15.48$</td>
<td style="text-align: center;">$-10.70$</td>
<td style="text-align: center;">$-8.080$</td>
</tr>
<tr>
<td style="text-align: center;">MLM-PPL (wiki)</td>
<td style="text-align: center;">$-2.635$</td>
<td style="text-align: center;">$-2.241$</td>
<td style="text-align: center;">$-2.019$</td>
<td style="text-align: center;">$-1.867$</td>
</tr>
<tr>
<td style="text-align: center;">n-rep-4gram (wiki)</td>
<td style="text-align: center;">$-0.007$</td>
<td style="text-align: center;">$-0.165$</td>
<td style="text-align: center;">$-0.287$</td>
<td style="text-align: center;">$-0.378$</td>
</tr>
</tbody>
</table>
<p>Table 13: Results for the repetition test. "B-" refers to "BARTScore-". Negated rep-4gram (Welleck et al., 2020), which measures the diversity, is also reported.
it would be useful to check whether the length of the hypothesis is within the expected range. For translation, a language ID check is helpful.</p>
<h2>E The Repetition Test</h2>
<p>It is well-known that GPT-like LMs suffer from a repetition problem-they tend to assign high likelihood to repetitive text (Holtzman et al., 2020).</p>
<p>For the repetition test, we append to each gold hypothesis $k$ copies of its last 4 -gram to create a synthetic repetition problem (termed as Rep- $k$ ), with an example available in Table 12. For this test, a robust metric should give a lower score for Rep$k$ compared to gold, because synthetic repetition degrades quality.</p>
<p>The experimental results for the repetition test are shown in Table 13. The repetition problem plagues a wider range of models than expected. In addition to GPT-PPL, we find BARTScore, and MLM-PPL (based on RoBERTa) also prefer repetitive text.</p>
<p>As an illustrated example of the repetition test,</p>
<p>Figure 6 shows the per-timestep next-token probability of a 4-gram repetitive text in the WikiText dataset, given by GPT-PPL. The first repetition of the 4 -gram "hard to miss." has a slightly higher probability compared to the original ending. As this 4-gram is repeated more times, the probability given by GPT-PPL becomes increasingly higher.</p>
<p>Implication For metric users, it has been an established practice (especially for open-ended generation) to report diversity metrics like rep-4gram (Welleck et al., 2020) or $n$-gram entropy (Zhang et al., 2018), as shown in Table 13. For metric developers, our results indicate that the degeneration issue can not be ignored even if the LM is not autoregressive.</p>
<h2>F Auxiliary Results for the Injection Test</h2>
<p>Table 14 contains auxiliary results of the injection test for UniEval on the summarization task. We note several additional interesting observations: (1) If we omit "And yes, it is relevant.", the relevent score gets lower. (2) If we change the tone from positive to negative, the scores get lower. (3) Just repeating "Yes" is not effective.</p>
<p>In the lower part of the table, we also observe that the injection hypothesis can drastically increase the score of a random (irrelevant) reference summary.</p>
<h2>G Auxiliary Results for the Frequent $n$-gram Test</h2>
<p>An example if the frequent $n$-gram sequence is available in Table 12.</p>
<p>In Table 15, results of frequent 4-gram and 3gram tests are shown. We observe that it is easier for the frequent 4 -grams to confuse the log-probability-based metrics. Per-timestep next-token probability plots for examples of a 4-gram and a 3-gram test are shown in Figure 3 and Figure 7, respectively. In both cases, there are high probability regions concentrated at the end of each $n$-gram. For example, "the" in the 3-gram "side of the" gets a higher probability than the first two tokens, and "of" in the 4-gram "in the middle of" gets a higher probability than the first three tokens.</p>
<h2>H Details on the Finetuning (Self-Evaluation)</h2>
<p>For GPT-PPL, we finetune the GPT-2 generators on the WikiText-103 training set for 2 epochs, with</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Per-timestep next-token probability of a 4-gram repetitive text sequence given by GPT-PPL.</p>
<table>
<thead>
<tr>
<th>(Injection) Hypothesis</th>
<th>Overall</th>
<th>Coh.</th>
<th>Con.</th>
<th>Flu.</th>
<th>Rel.</th>
</tr>
</thead>
<tbody>
<tr>
<td>Gold (the reference summary)</td>
<td>0.864</td>
<td>0.897</td>
<td>0.859</td>
<td>0.919</td>
<td>0.781</td>
</tr>
<tr>
<td>Answer: Yes, this is a really coherent and consistent summary.</td>
<td>0.905</td>
<td>0.903</td>
<td>0.857</td>
<td>0.959</td>
<td>0.900</td>
</tr>
<tr>
<td>And yes, it is relevant.</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Answer: Yes, this is a really coherent and consistent summary.</td>
<td>0.884</td>
<td>0.870</td>
<td>0.885</td>
<td>0.966</td>
<td>0.814</td>
</tr>
<tr>
<td>Answer: Yes, this is a really good summary.</td>
<td>0.838</td>
<td>0.777</td>
<td>0.756</td>
<td>0.962</td>
<td>0.856</td>
</tr>
<tr>
<td>Answer: Yes Yes Yes Yes Yes</td>
<td>0.729</td>
<td>0.459</td>
<td>0.796</td>
<td>0.887</td>
<td>0.775</td>
</tr>
<tr>
<td>Answer: No, this is not a coherent or consistent summary. And</td>
<td>0.813</td>
<td>0.859</td>
<td>0.611</td>
<td>0.919</td>
<td>0.862</td>
</tr>
<tr>
<td>no, it is not relevant.</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Random reference summary</td>
<td>0.563</td>
<td>0.577</td>
<td>0.044</td>
<td>0.925</td>
<td>0.704</td>
</tr>
<tr>
<td>Answer: Yes, this is a really coherent and consistent summary.</td>
<td>0.666</td>
<td>0.637</td>
<td>0.348</td>
<td>0.937</td>
<td>0.741</td>
</tr>
<tr>
<td>And yes, it is relevant. Summary: [random reference summary]</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Table 14: Auxiliary results of the injection test for UniEval on the summarization task.</p>
<table>
<thead>
<tr>
<th>Metric (task)</th>
<th>Gold</th>
<th>Freq 4-gram</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>Top-10</td>
<td>Top-50</td>
</tr>
<tr>
<td>GPT-PPL (wiki)</td>
<td>-25.640</td>
<td>-4.456</td>
<td>-11.640</td>
</tr>
<tr>
<td>MLM-PPL (wiki)</td>
<td>-2.994</td>
<td>-1.139</td>
<td>-2.469</td>
</tr>
<tr>
<td>rep-4gram (wiki)</td>
<td>0.019</td>
<td>0.539</td>
<td>0.199</td>
</tr>
<tr>
<td>Metric (task)</td>
<td>Gold</td>
<td>Freq 3-gram</td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td>Top-10</td>
<td>Top-50</td>
</tr>
<tr>
<td>GPT-PPL (wiki)</td>
<td>-25.640</td>
<td>-5.650</td>
<td>-19.910</td>
</tr>
<tr>
<td>MLM-PPL (wiki)</td>
<td>-2.994</td>
<td>-1.368</td>
<td>-4.224</td>
</tr>
<tr>
<td>rep-4gram (wiki)</td>
<td>0.019</td>
<td>0.452</td>
<td>0.084</td>
</tr>
</tbody>
</table>
<p>Table 15: Results of Frequent 4-gram and 3-gram tests.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Per-timestep next-token probability of a frequent 3-grams sequence given by GPT-PPL. a learning rate of 1e-05 and a batch size of 16.</p>
<p>For BARTScore, we finetune the BART or T5 models on the CNNDM training set for 2 epochs, with a learning rate of 1e-05 and a batch size of 8. Beam search with beam size 5 is used for decoding.</p>
<h3>I Auxiliary Description and Results of the Fluency and Consistency Tests</h3>
<p>More details on the setup: Most noise types involve randomness. For each hyper-parameter, we report mean and standard-deviation over five runs with different random seeds. For each noise type and task, we set the hyper-parameters so that the gaps of noise-ratio between test points are close to or larger than 5%. The same set of random seeds and hyper-parameters are shared across all metrics.</p>
<p>The full set of tests is described by Table 16. For the detailed hyper-parameter setting, please refer to our to-be-released code.</p>
<p>In general, the tests can be applied to all three tasks. But there are exceptions due to the properties of the dataset: (1) We do not apply BERT-diverge to the WikiText data, as the task's nature is open-ended. (2) We can not apply sentence switching to WMT as most samples only contain one sentence. (3) Due to similar reasons, we do not apply verb or named entity switching and sentence replacement</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Noise Type</th>
<th style="text-align: center;">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Truncation</td>
<td style="text-align: center;">A portion of tokens at the end of the hypothesis are removed. e.g., She went to.</td>
</tr>
<tr>
<td style="text-align: center;">Article Removal</td>
<td style="text-align: center;">A random portion of articles (the/a/an) in the hypothesis are removed. e.g., She went to office.</td>
</tr>
<tr>
<td style="text-align: center;">Preposition Removal</td>
<td style="text-align: center;">A random portion of prepositions are removed. e.g., She went the office.</td>
</tr>
<tr>
<td style="text-align: center;">Stop-word Removal</td>
<td style="text-align: center;">A random portion of stop-words are removed. e.g., She went office.</td>
</tr>
<tr>
<td style="text-align: center;">Verb Lemmatization</td>
<td style="text-align: center;">A random portion of verbs in the hypothesis are lemmatized. e.g., She go to the office.</td>
</tr>
<tr>
<td style="text-align: center;">Token Drop</td>
<td style="text-align: center;">A random portion of tokens are removed. e.g., She to the offce.</td>
</tr>
<tr>
<td style="text-align: center;">Repeated Token</td>
<td style="text-align: center;">A random portion of tokens are repeated once. e.g., She went to to the office.</td>
</tr>
<tr>
<td style="text-align: center;">Local Swap</td>
<td style="text-align: center;">A random portion of tokens are swapped with the token to the right of it. e.g., She to went the office.</td>
</tr>
<tr>
<td style="text-align: center;">Middle Swap</td>
<td style="text-align: center;">The left and right part of the sentence is swapped (The cut-off point is right in the middle of the length). This is to synthesize a wrong subject-verb-object (SVO) order. e.g., To the office she went.</td>
</tr>
<tr>
<td style="text-align: center;">Noised Punctuation</td>
<td style="text-align: center;">A random portion of the punctuations $\left{{ }^{\prime},{ }^{\prime}, ', ',{ }^{\prime} \mathbf{T}, '!\', ; '\right}$ are noised. For example, commas are replaced by periods and vice versa. e.g., She went to the office,</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: left;">Sentence Switching</th>
<th style="text-align: left;">Several random pairs of sentences in the hypothesis are switched, breaking temporal/logical order. e.g., <br> And she talked to her staff about Paris. She went to the office in Boston.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Sentence Replacement</td>
<td style="text-align: left;">Several sentences in the hypothesis are replaced by a random irrelevant sentence (from the same <br> dataset). This is an amazing game. And she talked to her staff about business.</td>
</tr>
<tr>
<td style="text-align: left;">Negation</td>
<td style="text-align: left;">A random portion of sentences are negated. e.g., She did not go to the office in Boston. And <br> she talked to her staff about Paris.</td>
</tr>
<tr>
<td style="text-align: left;">Generic Named Entity</td>
<td style="text-align: left;">A random portion of the named entities in the hypothesis are replaced by a generic phrase, destroying <br> the information. e.g., She went to the office in a place. And she talked to her staff <br> about a place.</td>
</tr>
<tr>
<td style="text-align: left;">Named Entity Switching</td>
<td style="text-align: left;">Several random pairs of named entities in the hypothesis are switched, breaking factuality. e.g., She <br> went to the office in Paris. And she talked to her staff about Boston.</td>
</tr>
<tr>
<td style="text-align: left;">Verb Switching</td>
<td style="text-align: left;">Several random pairs of verbs in the hypothesis are switched. e.g., She talked to the office in <br> Boston. And she went to her staff about business.</td>
</tr>
<tr>
<td style="text-align: left;">Noun Switching</td>
<td style="text-align: left;">Several random pairs of nouns in the hypothesis are switched. e.g., She went to the staff in <br> Boston. And she talked to her office about business.</td>
</tr>
<tr>
<td style="text-align: left;">BERT-diverge</td>
<td style="text-align: left;">A random portion of tokens in the hypothesis are replaced one by one by sampling from the top-10 <br> prediction of a masked language model (RoBERTa). At each step, one token at a random position is <br> replaced by [MASK], and inputed to RoBERTa for prediction. Since this process do not have access <br> to the source text, the semantics of the hypothesis would gradually diverge. e.g., She ran to the <br> office in Boston. And she talked to her staff about business.</td>
</tr>
</tbody>
</table>
<p>Table 16: Descriptions of the fluency tests (top) and consistency tests (bottom). Note that the truncation test not only breaks fluency, but also causes loss of information (consistency). For fluency tests, the example gold hypothesis is "She went to the office." For consistency tests, the example gold hypothesis is "She went to the office in Boston. And she talked to her staff about Paris." The gold hypothesis here is only for ease of explanation and it does not exist in the datasets.
to WMT. (4) Similarly, we do not apply named entity switching or generic named entity to TEDMT.</p>
<p>Compared to other tests, BERT-diverge is special in that its noise is generated automatically by an MLM, which is an interesting future direction for metric stress tests. One disadvantage of this approach is that we do not have a $100 \%$ guarantee that the perturbed hypothesis is indeed "diverged". However, we do not observe empirical evidence of this weakness in the quantitative (Most metrics drop drastically with this noise) or qualitative examination.</p>
<p>The complete results for the fluency and consistency tests are shown in Figure 14 for open-ended generation, Figure 12 for summarization, and Figure 15/ Figure 16 for translation. For visibility, we plot fluency test and consistency tests separately
for each metric. Failed tests are highlighted as bold lines.</p>
<p>Auxiliary Discussion of the Results We now discuss some interesting results which are not included in the main section.</p>
<p>For open-ended generation, both variants of MAUVE (-GPT2/-RoBERTa) fail the sentence switching test. Although MLM-PPL does not fail the test in terms of rank, the slope of the sentence switching curve is relatively much flatter than the other noise types, indicating an insensitivity.</p>
<p>Interestingly, while MAUVE-RoBERTa is robust to truncation, MAUVE-GPT2 only penalizes truncation in a binary manner. The score is much lower than gold for the first level of noise, but remains basically the same for other levels compared to the first level. This implies the GPT2 feature is not sensitive to the amount of information loss, which</p>
<h1>BERT-Diverge Perturbation Examples</h1>
<p>Gold: The biker still attempted to evade the car, however, brushed against the car at the rear end.
BERT-diverge: The biker narrowly managed to evade the car, however nearly brushed against the car in the immediate area. Relative COMET-QE Score Change: $+5.60 \%$
Gold: A security service monitors the curfew.
BERT-diverge: The security force enforced the laws.
Relative COMET-QE Score Change: $+2.95 \%$
Gold: Greens and SPD blamed the State government for shared responsibility.
BERT-diverge: Greens and others blamed the federal government for its failure.
Relative COMET-QE Score Change: $+18.61 \%$
Table 17: Examples of noise from BERT-diverge on WMT data. The semantics have clearly diverged, however, the scores from COMET-QE do not drop.
is problematic. From insights of the attention analysis (§5.1), we also attribute this to the locality of GPT2 embedding.</p>
<p>GPT-PPL and MLM-PPL are robust to truncation, but only penalize this error minimally as shown by the relatively flat slope of their truncation curves, which is not ideal.</p>
<p>For summarization, BARTScore-cnn/para-r fails a number of fluency tests involving stop-words, prepositions, etc. This suggests extra caution is needed when developing recall-orientated log-probability-based metrics.</p>
<p>ROUGE-2 and ROUGE-L fail the truncation and noised punctuation tests. ROUGE-2 also has a very marginal decrease in sentence switching, which is also undesirable.</p>
<p>Interestingly, BERT-diverge with COMET-QE is the only failure case for WMT (The same set of BERT-diverge noise is shared across metrics). A few examples are given in Table 17. We observe that the semantics of the hypotheses are clearly diverged, however, the scores from COMET-QE do not drop.</p>
<p>In addition, COMET-QE also fails article removal on summarization, while the reference-based COMET is more robust.</p>
<p>Analysis of Truncation In Figure 8, we show how different variants of BARTScore-para behave under the truncation test. We also observe that the recall variant behaves well, while the precision and faithful variants are confused. But, BARTScore-para-recall fails the sentence switching test. Therefore, we recommend reporting the recall variant in combination with other variants.</p>
<p>Analysis of Switching In Figure 9, we test switching different units of the hypothesis. Interestingly, MAUVE-GPT2/RoBERTa drops drastically
for all other types of units. ${ }^{14}$
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: How the variants of BARTScore-para react to the truncation test for the summarization task.</p>
<h2>J Can We Automate the Detection?</h2>
<p>The tests we design rely on various intuitions including some level of understanding of the underlying PLM's behavior, or a detailed examination of the metric definitions. A natural next question is whether we can automate this process. Ideally, we would like an algorithm to search for a noising transformation function $f$ of gold hypotheses that fools the targeted metric, while inducing perturbations visible to humans.</p>
<p>As a case study, we focus on BERTScore-f and build a toy example using a discrete-space adversarial attack algorithm (Cheng et al., 2018; Li et al., 2020; He and Glass, 2019) on WMT. Although it is only a preliminary attempt toward the ideal goal, the results show that it could be an interesting future direction.</p>
<p>On the high level, we design an enumerationbased algorithm that iteratively and greedily perturbs the hypothesis. Given a gold hypothesis $h$</p>
<p><sup id="fnref9:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{14}$ We use $\left{{ }^{\prime},{ }^{\prime}, \cdot, \cdot, \cdot^{\prime}, \boldsymbol{\gamma}^{\prime}, \boldsymbol{\eta}^{\prime}\right}$ to deliminate sub-sentences.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref8:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref9:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{7}$ One shortcoming of the Levenshtein distance is that it does not allow the switching operation. Therefore, for switching-based noise types, we divide the noise-ratio by 2 .&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
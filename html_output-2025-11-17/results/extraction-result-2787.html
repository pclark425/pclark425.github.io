<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2787 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2787</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2787</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-71.html">extraction-schema-71</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory systems to play text games, including details about the memory architecture, the text games played, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-267682093</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.09727v3.pdf" target="_blank">A Human-Inspired Reading Agent with Gist Memory of Very Long Contexts</a></p>
                <p><strong>Paper Abstract:</strong> Current Large Language Models (LLMs) are not only limited to some maximum context length, but also are not able to robustly consume long inputs. To address these limitations, we propose ReadAgent, an LLM agent system that increases effective context length up to 20x in our experiments. Inspired by how humans interactively read long documents, we implement ReadAgent as a simple prompting system that uses the advanced language capabilities of LLMs to (1) decide what content to store together in a memory episode, (2) compress those memory episodes into short episodic memories called gist memories, and (3) take actions to look up passages in the original text if ReadAgent needs to remind itself of relevant details to complete a task. We evaluate ReadAgent against baselines using retrieval methods, using the original long contexts, and using the gist memories. These evaluations are performed on three long-document reading comprehension tasks: QuALITY, NarrativeQA, and QMSum. ReadAgent outperforms the baselines on all three tasks while extending the effective context window by 3.5-20x.</p>
                <p><strong>Cost:</strong> 0.007</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2787",
    "paper_id": "paper-267682093",
    "extraction_schema_id": "extraction-schema-71",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.00671125,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>A Human-Inspired Reading Agent with Gist Memory of Very Long Contexts
22 Jul 2024</p>
<p>Kuang-Huei Lee 
Xinyun Chen 
Hiroki Furuta 
John Canny 
Ian Fischer 
A Human-Inspired Reading Agent with Gist Memory of Very Long Contexts
22 Jul 2024D7E4A149FE0BD13E9E74D5812FB18049arXiv:2402.09727v3[cs.CL]
Current Large Language Models (LLMs) are not only limited to some maximum context length, but also are not able to robustly consume long inputs.To address these limitations, we propose ReadAgent, an LLM agent system that increases effective context length up to 20× in our experiments.Inspired by how humans interactively read long documents, we implement ReadAgent as a simple prompting system that uses the advanced language capabilities of LLMs to (1) decide what content to store together in a memory episode, (2) compress those memory episodes into short episodic memories called gist memories, and (3) take actions to look up passages in the original text if ReadAgent needs to remind itself of relevant details to complete a task.We evaluate ReadAgent against baselines using retrieval methods, using the original long contexts, and using the gist memories.These evaluations are performed on three long-document reading comprehension tasks: QuALITY, NarrativeQA, and QMSum.ReadAgent outperforms the baselines on all three tasks while extending the effective context window by 3.5 − 20×.</p>
<p>Introduction</p>
<p>Transformer-based Large Language Models (LLMs) are highly capable of language understanding, but the amount of text that LLMs are able to read at one time is constrained.Not only is there an explicit context length limitation, but it has also been found that performance of LLMs tends to decline with increasingly long inputs even when they don't actually exceed the explicit context window (Liu et al., 2023;Shi et al., 2023).In contrast, humans can read, understand, and reason over very long texts, such as a series of Project website and demo: read-agent.github.io.Contribution statements: Appendix A.1 Google DeepMind.Correspondence to: Kuang-Huei Lee <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#108;&#101;&#101;&#107;&#104;&#64;&#103;&#111;&#111;&#103;&#108;&#101;&#46;&#99;&#111;&#109;">&#108;&#101;&#101;&#107;&#104;&#64;&#103;&#111;&#111;&#103;&#108;&#101;&#46;&#99;&#111;&#109;</a>,Ian Fischer <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#105;&#97;&#110;&#115;&#102;&#64;&#103;&#111;&#111;&#103;&#108;&#101;&#46;&#99;&#111;&#109;">&#105;&#97;&#110;&#115;&#102;&#64;&#103;&#111;&#111;&#103;&#108;&#101;&#46;&#99;&#111;&#109;</a>.interrelated books.</p>
<p>Proceedings of the</p>
<p>We posit that an underlying reason for this gap is inherent in the differences in reading approaches.Typically, we use LLMs to consume the exact given content word-byword and the process is relatively passive.On the other hand, humans read and reason over long text differently.First, the exact information tends to be forgotten quickly, whereas the fuzzier gist information, i.e. the substance irrespective of exact words, from past readings lasts much longer (Reyna &amp; Brainerd, 1995b;a;Reyna, 2012) 1 .Second, human reading is an interactive process.When we need to remind ourselves of relevant details in order to complete a task, such as answering a question, we look them up in the original text.</p>
<p>We think that using the fuzzy gist memory to capture global context and attending to local details together enables hu-mans to reason over very long context efficiently, in terms of how much information to process at once, and is also important for comprehension.For example, if we were to infer the intention of a fictional character's specific action described on a page in a novel, besides focusing on the surrounding pages, we likely also need to understand the overall story and the character's personality from reading the whole book (see Appendix D for more analysis).</p>
<p>Motivated by these observations, we propose ReadAgent, an LLM agent system that handles long content inspired by the human approach.ReadAgent is simple to implement and can be built entirely by prompting a previously-trained LLM.As illustrated in Figure 1, it takes three primary steps:</p>
<p>(1) episode pagination, where we prompt the LLM to decide where to pause in reading contiguous text; the content between pause points becomes an episode, which we refer to as pages in this work;</p>
<p>(2) memory gisting, where we prompt the LLM to compress each page into a shorter gist and associate the gist with a corresponding context (e.g. which page the gist was from) -this gives the episodic gist memory;</p>
<p>(3) interactive look-up, where the LLM looks at the given task and the complete set of gists in-context, makes decision on what page(s) to look up, combines the gists with these raw pages, and solves the task.</p>
<p>We evaluate ReadAgent by comparing against using only the gist memory without interactive look-up, using full text for datasets that can fit in the context window, and using retrieval methods to look up pages.ReadAgent outperforms all baselines across three challenging long-document comprehension tasks -QuALITY, NarrativeQA and QMSum -while increasing the effective context length significantly compared to the original LLM.On NarrativeQA Gutenberg test set, whose average length is 71k words and whose maximum is 343k words, ReadAgent improves the LLM rating (Section 4.1) by 12.97% and ROUGE-L by 31.98% over the best retrieval baseline and increases the effective context length by ∼ 20×.On QuALITY, where the articles can fit in an 8K context window, ReadAgent outperforms using full text with a 3.5× effective context length while saving 20.4% on the overall number of words consumed by the LLM (Section 3.3).</p>
<p>Finally, in Appendix E, we adapt ReadAgent to web navigation, which is a fundamentally very-long context agent setting.We find that ReadAgent is simple to adapt to this setting and shows promising performance.</p>
<p>Our primary contributions are:</p>
<p>• ReadAgent, our human-inspired LLM agent that generates gist memories and looks up information as needed for solving tasks on long contexts (Section 3).</p>
<p>• Demonstration of significant performance advantages and scalability through a comprehensive experimental evalu-ation on challenging long-context benchmarks, comparisons against popular baselines, and analysis (Section 4).</p>
<p>Related Work</p>
<p>Long-Context LLMs The most direct way to improve LLM long-context performance is to train or fine-tune LLMs with longer context windows (Beltagy et al., 2020;Zaheer et al., 2020;Guo et al., 2022;Ainslie et al., 2023;Tay et al., 2022;Chen et al., 2023c).Another approach is to explore new architectures or efficient implementations of the Transformer (Vaswani et al., 2017) attention layers to reduce the need of long-context fine-tuning (Chen et al., 2023b;Press et al., 2022;Xiao et al., 2023;Jin et al., 2024;Han et al., 2023).However, LLM performance tends to decline with increasingly long inputs even when they don't exceed the specified context length (Liu et al., 2023).LLM performance is also shown to be sensitive to distracting information in the context (Shi et al., 2023).Thus, the effective context length could be shorter than the explicit limit.Our approach is complimentary to these approaches, scaling the effective context length of the underlying model while reducing the amount of distracting information in context, and requiring neither architectural changes nor training.</p>
<p>Retrieval Retrieval Augmented Generation (RAG) techniques (Chen et al., 2017;Dinan et al., 2019;Lewis et al., 2020;Izacard &amp; Grave, 2021;Wu et al., 2022;Park et al., 2023;Zhong et al., 2023) allow an LLM to query taskrelevant information from a large database of documents or document pieces.Our work implements a form of retrieval by reasoning over a contextualized gist memory, all with zero-shot LLM prompting.This rethinking of retrieval directly leverages the strength and flexibility of LLM language understanding to reason about which documents to retrieve.Our approach is well-suited to densely-correlated long-document pieces, such as a series of books or a conversation history, but the database cannot scale arbitrarily, since the size of the gist memory is limited by the LLM's context length, and the gist memory's length correlates with the size of the database.In contrast, conventional retrieval approaches can handle larger databsases than our approach.</p>
<p>In this work, we compare against retrieval systems that use exactly the same set of documents as our approach.</p>
<p>LLM Agents for Long Texts LLMs can be used as agents to interactively handle very long texts.WebGPT (Nakano et al., 2021) and WebShop (Yao et al., 2022) learn browsing actions to search for the requested answer on the internet, despite not being designed to understand long documents.</p>
<p>The PEARL (Sun et al., 2023) system proposes action plans for better long-document comprehension through iterative prompting; Yuan et al. (2020) explicitly learns RL agents for similar purposes.Self-note (Lanchantin et al., 2023) amor-tizes reasoning steps and interleaves intermediate notes with the original documents to improve reasoning.Yang et al. (2022) generates long outputs through iterative reasoning.</p>
<p>However, these methods cannot address long input texts that exceed the LLM's context length.Similar to this work, MemWalker (Chen et al., 2023a) also reads long documents interactively through iterative prompting.It traverses a tree of different levels of summaries to search for task-related information.However, the hierarchical summary structure makes it difficult to reason over related but distant information at the same granularity (see Appendix H for more discussion).</p>
<p>ReadAgent</p>
<p>Figure 1 shows an overview of ReadAgent, which we describe in detail below.Note that the prompts presented in this section are examples, which may need to change according to the target task.We release the prompts for each task on read-agent.github.io.Please also refer to Appendix F for the prompt design details.</p>
<p>Gist Memory</p>
<p>A gist memory is an ordered collection of short gists of chunks of text from the original long context.Building a gist memory has two steps: pagination and memory gisting, described in turn below.</p>
<p>Episode Pagination When ReadAgent reads through a long text, it makes decisions on what content to store together in a memory episode by choosing where to pause reading.At each step, we provide the LLM some text that begins from the previous pause point and ends when it reaches a max words limit.We prompt the LLM to choose which point between paragraphs would be a natural point to pause, and then treat the content between the previous and current pause points as an episode, which we also refer as a page.This is episode pagination, which we implement with the following prompt.</p>
<p>As shown in the prompt, possible pause points are inserted between paragraphs as numbered tags (e.g.⟨13⟩), making this a multiple choice question for the LLM.We only start inserting these numbered tags after a min words threshold to make sure that each page has at least min words.</p>
<p>Example Pagination Prompt</p>
<p>You are given a passage that is taken from a larger text (article, book, ...) and some numbered labels between the paragraphs in the passage.Numbered labels are in angle brackets.For example, if the label number is 19, it shows as ⟨19⟩ in text.</p>
<p>Please choose a label where it is natural to break reading.</p>
<p>The label can be a scene transition, the end of a dialogue, the end of an argument, a narrative transition, etc.</p>
<p>Please answer with the break point label and explain.We subsequently prepend a page tag to each gist (e.g."⟨Page 2⟩\n{GIST CONTENT}") to contextualize it (indicate where the gist was from), and then concatenate all gists.This gives us the gist memory.We use the word "shorten" in the prompt to generate these summarizing gists as it tends to help preserve the narrative flow, making it more natural to concatenate.Using the word "summarize" tended to produce a restructured summary in our experiments.</p>
<p>The original page size is a key factor for how compressed the gist is.Let's say the smallest unit of text that we consider is a paragraph.Intuitively, a paragraph likely has some amount of mutual information with its neighbors.Thus, the larger chunk of text we group together, the more duplicated information we can remove.Empirically, compressing larger chunks of text with LLMs also tends to remove more details, which could affect performance.We control the page size by changing min words and max words in pagination.This trade-off is studied in Section 4.4.</p>
<p>Interactive Look-Up and Response</p>
<p>For a given task about a long document, we want ReadAgent to take actions to look up relevant details in the original text in addition to using its gist memory.As the gist memories are contextualized with page numbers, we simply prompt the LLM to answer which page(s) it would like to look up and read again given the specific task.In the following we discuss two look-up strategies: looking up all pages at once in parallel (ReadAgent-P) and sequentially looking up one page at a time (ReadAgent-S).</p>
<p>ReadAgent-P As in the following example prompt for question-answering, typically we give it a maximum number of pages that it can look up but also instruct it to use as few pages as possible to avoid unnecessary computational overhead and distracting information.The following prompt shows parallel look-up, where the model requests multiple pages in response to a single prompt.</p>
<p>Example Parallel Lookup Prompt (ReadAgent-P)</p>
<p>The The selected raw pages replace the gist(s) at the corresponding positions in memory, preserving the overall narrative flow.Then we prompt the LLM again with the task and the updated memory and ask it to solve the task (see example prompts in Appendix F).</p>
<p>ReadAgent-S We also study the sequential look-up strategy, where the model requests one page at a time, up to some maximum number of pages.In sequential look-up, the model gets to see the previously expanded pages before deciding which page to expand.This gives the model access to more information than parallel look-up, so we might expect it to perform better in some situations.However, the larger number of interactions with the model increases the computational cost, so sequential look-up should only be used on tasks where it provides clear benefits.</p>
<p>Example Sequential Lookup Prompt (ReadAgent-S)</p>
<p>The following text is what you remember from reading a meeting transcript, followed by a question about the transcript.</p>
<p>You may read multiple pages of the transcript again to refresh your memory and prepare to answer the question.Each page that you re-read can significantly improve your chance of answering the question correctly.</p>
<p>Computational Trade-offs and Scalability</p>
<p>Episode pagination, memory gisting and interactive lookups require iterative inference.As we show in the following, the additional cost is bounded linearly by a small factor, making our approach scale well with input length.</p>
<p>Pagination In theory, an LLM could read a document and directly provide the pagination in a single pass, so the minimum number of words the LLM must process is the length of the document.Our pagination algorithm splits the document into chunks of at most max words, and then guarantees that at least min words are consumed at each step.Thus, the ratio max words min words gives an upper bound on how many times the word length of the document the LLM must process using our algorithm.Gisting: Memory gisting is one additional pass of the raw input words, since each page is gisted independently.Look-ups: Parallel look-ups are conditioned on gists instead of the full text, and thus will be much shorter than one pass of the raw input words.Each step of a sequential look-up is similar to parallel look-ups and the overall cost is capped with the maximum number of look-ups allowed.Response: Finally, answering is also similar to parallel look-ups.There is additional overhead from the prompt templates, of course.</p>
<p>On the other hand, as generating gists is an one-time effort while the look-up and response steps operate mostly on gists that are much shorter than the original text, the one-time effort can be amortized when the same context is reused for multiple tasks.Thus, in such settings, ReadAgent can reduce the overall number of tokens to process.In particular, directly answering from the original QuALITY dev set (230 articles and 2086 questions) is 8,708,434 words consumed by the LLM, whereas using ReadAgent with 1-page lookup is 6,499,856 words (25.4% saving), up-to-2-page lookup is 6,933,357 words (20.4% saving), and up-to-5-page lookup is 7,503,084 words (13.8% saving).We can expect the savings to be more significant with higher compression rate and more downstream tasks.</p>
<p>ReadAgent Variants</p>
<p>In Appendix G, we discuss variants of ReadAgent that can be useful in different problem settings, including when the target task is known prior to reading the long document.In Appendix E, we describe adapting ReadAgent to work in the web navigation setting.</p>
<p>Experiments</p>
<p>We evaluate ReadAgent's long-document reading comprehension ability on three long-context question-answering challenges: QuALITY (Pang et al., 2022), NarrativeQA (Kočiskỳ et al., 2018) and QMSum (Zhong et al., 2021).Although ReadAgent does not require any model training, we develop the proposed method on the training sets and test on the validation, test and/or development sets to avoid any risk of overfitting system hyperparameters.</p>
<p>In this work, we primarily use the instruction-tuned PaLM 2-L (Anil et al., 2023) for our experiments and evaluation.The context length of PaLM 2-L is 8K tokens.Details of the model can be found in Anil et al. (2023).Additionally, we provide GPT-3.52 results in Appendix B, and experimental results on the web navigation setting in Appendix E.</p>
<p>One important performance measure of the techniques considered here is the compression rate (CR).As we want to measure the longest LLM context length that Read-Agent requires versus full-context length, we define this as CR ≡ 100 * (1 − word-count(in-context text)  word-count(full-context text) ) at the final response query, where the in-context text (gists and retrieved pages) length is the longest among all inference steps.</p>
<p>LLM Raters</p>
<p>NarrativeQA and QMSum both have one or more free-form reference responses.They are typically evaluated using syntactic matching metrics such as ROUGE (Lin, 2004) F-Measure.We additionally evaluate these datasets using an automatic LLM Rater as an alternative to human evaluation similar to Peng et al. ( 2023 In our implementation, we prompt the LLM to look at the question or instruction and compare the model's answer to the reference answer.The "Strict LLM Rater Prompt" shown below is for judging whether there is an exact match, and the "Permissive LLM Rater Prompt" is for judging whether there is an exact match or a partial match.We apply both prompts to all model responses.If either rater decides there is an exact match, we count it as an exact match.If the strict rater is negative but the permissive rater detects a partial match, we count it as a partial match.Otherwise, it's not a match.In the case that there are multiple reference answers, the response is compared against each reference answer in turn, and the highest rating is returned.</p>
<p>Strict LLM Rater Prompt</p>
<p>After reading some text, John was given the following question about the text: {QUESTION TEXT} John's answer to the question was: {MODEL RESPONSE TEXT} The ground truth answer was: {REFERENCE RESPONSE TEXT} Does John's answer agree with the ground truth answer?Please answer YES or NO.</p>
<p>Permissive LLM Rater Prompt</p>
<p>After reading some text, John was given the following question about the text: {QUESTION TEXT} John's answer to the question was: {MODEL RESPONSE TEXT} The ground truth answer was: {REFERENCE RESPONSE TEXT} Does John's answer agree with the ground truth answer?Please answer "Yes", "Yes, partially", or "No".If John's response has any overlap with the ground truth answer, answer "Yes, partially".If John's response contains the ground truth answer, answer "Yes".If John's response is more specific than the ground truth answer, answer "Yes".</p>
<p>Based on these raters, we define two different scores: LLM-Rating-1 (LR-1) is a strict evaluation score, where we count the percentage of exact matches over all examples; LLM-Rating-2 (LR-2) is permissive, where we count the percentage of exact and partial matches.</p>
<p>Baseline Methods</p>
<p>Retrieval-Augmented Generation (RAG) As discussed in Section 2, RAG (Lewis et al., 2020) is a popular approach to extend access to a large amount of text beyond what can fit in the LLM context window.In this paper we compare ReadAgent to RAG baselines using conventional retrieval methods to find relevant "pages" in a long text, where we reuse the pages generated by ReadAgent.We consider two relevance methods: Okapi BM25 (Robertson et al., 2009) and neural retrieval based on the Gemini API embedding model (models/embedding-001) 3 .The neural retrieval relevance score is defined as the dot product be-tween the question embedding vector and each page (or gist memory embedding vector in the case of NarrativeQA, see Section 4.3.2).For reading comprehension tasks, the pages are ranked by relevance to each question, and we prompt the LLM to look at the top-k pages as context for answering the question.In most retrieval settings, the database of documents is quite large, which makes the retrieval task more challenging.In our setting, ReadAgent and retrieval methods all use a per-document database, rather than per-dataset.For example, in QuALITY, there are hundreds of articles, each with multiple questions.The database for retrieval in each question is only the extracted pages from the corresponding article (typically less than 20 pages), rather than the thousands of pages from the entire dataset.</p>
<p>Full or Truncated Text Content</p>
<p>The maximum length of QuALITY dev articles is ∼6,000 words, which can fit into the PaLM 2-L context window.This allows us to evaluate ReadAgent against directly using the full long document for long-context reading comprehension.The maximum length of QMSum is over 26,000 words.Consequently, we choose to truncate the text to close to the context window limit (6,000 words for PaLM 2-L experiments) to ensure that the truncated text fits in the LLM's context, though this would generally be a weaker baseline.Finally, since the average length of NarrativeQA documents significantly exceeds the context window, it is less meaningful to perform the truncated-context comparison.</p>
<p>Gist Memory</p>
<p>We can also attempt to solve the given task by reasoning directly over the gist memory.Doing so helps us understand not only the importance of interactive look-up but also how using the LLM-compressed information alone compares to the full content and retrieval baselines.</p>
<p>Long-Context Reading Comprehension</p>
<p>4.3.1.QUALITY QuALITY (Pang et al., 2022) is a four-way multiple choice question answering challenge with text data from several different sources.QuALITY is evaluated using accuracy, with 25% corresponding to chance performance.</p>
<p>The dev set has an average length of 4,122 words and a maximum of 5,967.The gist memory has an average length of 650 words and a maximum of 1,264.Figure 2 shows the word statistics for the original text and the gists.The compression rate of the gists is 85.53%.See Appendix C for QuALITY pagination hyperparameters.</p>
<p>Table 1 shows the experimental results on QuALITY.The performance of ReadAgent increases as we increase the maximum number of pages allowed for look-up.ReadAgent-P (Look up 1-6 pages) achieves 86.91% and ReadAgent-S (Look up 1-6 pages) achieves 87.17% in ac-   and the maximum is 63,957 words.As the reference answers are free-form, we evaluate based on ROUGE (Lin, 2004) and the LLM Ratings (Section 4.1).The original main texts are replaced with the HTML-stripped version from SCROLLS (Shaham et al., 2022).
(# LU) LR-1 LR-2 R-1 R-2 R-L CR (# LU) LR-1 LR-2 R-1 R-2 R-L BM25 Retrieval
Because of the length of NarrativeQA articles, in order to fit the gists into the context window, we significantly expand the page size, resulting in stronger compression (Section 3.1).For example, the Gutenburg gists from the test set have 2,217 words on average and the maximum is 6,471 words, whereas the movie script gists have 2,155 words on average and the maximum is 4,511 words.Figures 3 and 4 show the word statistics for the original text and the gists in Gutenberg and movie scripts respectively.The compression rate of the gists is 96.80% for Gutenberg texts and 91.98% for movie scripts.See Appendices C and I for NarrativeQA pagination hyperparameters and more details.</p>
<p>For the neural retrieval models, we use the gist memory embedding vectors rather than the page embedding vectors because the Gemini API embedding model is limited to 10,000 characters (or less than 2,000 tokens, in expectation), which is too short for embedding full pages in our NarrativeQA experiments.However, using those embedding vectors, we then return the original pages to the LLM context as normal, and use those pages as described in Section 4.2.</p>
<p>Because the Gutenberg texts and the movie scripts have significantly different distributions, we present the results separately in</p>
<p>QMSUM</p>
<p>QMSum (Zhong et al., 2021) consists of meeting transcripts on various topics and associated questions or instructions.We use the concatenated version of QMSum provided by SCROLLS (Shaham et al., 2022).The transcripts tend to be quite long, ranging in length from 1,000 to 26,300 words, with an average length of about 10,000 words.Figure 5 shows the histograms of word counts for the QMSum training set.The answers are free form text, so the standard evaluation metric is ROUGE F-Measure.We additionally evaluate using our LLM Ratings (Section 4.1).See Appendices C and J for hyperparameters and additional results.In Table 3 and Table 11 (Appendix), we see that performance improves as the compression rate decreases, so techniques that look up more pages tend to do better than techniques that look up fewer pages.We also see that ReadAgent-S substantially outperforms ReadAgent-P (and all baselines).This performance improvement comes at a cost of up to six times as many requests in the retrieval phase.Since other datasets don't have such a strong performance improvement, we suspect that QMSum is in some sense a more challenging dataset, requiring the model to actively search through the gisted transcript to locate relevant information.This hypothesis seems reasonable, as meeting transcripts are much less structured than the documents, books, and movies found in QuALITY and NarrativeQA.</p>
<p>A large fraction of the tasks in QMSum are a request to provide a summary, rather than a concrete question about some content in the meeting.For many of these, the LLM refuses to look up any pages, instead responding with "I don't need to look up any pages.I can summarize the whole meeting based on what I already remember.",for example.Consequently, the average number of pages looked up for ReadAgent is much lower than the maximum allowed.However, on the tasks that actually involve a question, Read-Agent tends to use most or all of the available lookup pages.</p>
<p>In Tables 3 and 11, the ROUGE scores by themselves don't always show a clear trend.This is because as the length of the texts increase (corresponding to the compression rates decreasing), the response lengths increase as well.Longer response lengths result in lower ROUGE precision values, which pushes down the F-Measures.Consequently, for the ROUGE scores to increase as text length increases, the improvement to recall must be more substantial than the reduction to precision.This happens to some extent, but the effect size is small.Furthermore, including gists in the text substantially increases the response length, as is the case for GistMem and all the ReadAgent approaches.This increase is in spite of the fact that all models use the same question-answering prompt, so there is no prompt difference to cause the increased response lengths.This makes it much more challenging for GistMem and ReadAgent to outperform the retrieval methods in ROUGE score.Nevertheless, ReadAgent-S manages to have the highest ROUGE scores as well as the highest LLM ratings.Because of these issues with ROUGE, we consider the LLM ratings to be more informative for comparisons between these runs.However, the LLM ratings do not make it easy to compare with results using a different LLM to rate, such as GPT, and they also do not allow for easy comparisons with other works.The same observation applies to the NarrativeQA results above.</p>
<p>Ablation Study and Analysis</p>
<p>Retrieval Quality In Table 4, we compare using GistMem with neural retrieval to look up one page with using Read-Agent to look up one page.This is equivalent to replacing ReadAgent's prompt-based retrieval with neural retrieval.ReadAgent's retrieval performs better here.Episode pagination In this work we ask ReadAgent to decide where to pause reading and what information to store together in memory (Section 3.1), whereas in prior art, rulebased segmentation of text is typically used (Chen et al., 2023a;Wu et al., 2021).We compare the two approaches with similar page length on average in Table 5 to demonstrate that it is indeed beneficial to break at pause points that LLMs consider natural (e.g.scene transitions, ends of dialogue, narrative transitions, etc).</p>
<p>LLM Uniform Length ReadAgent-P (1-5 pgs) Acc.86.83% 85.71%</p>
<p>Table 5. ReadAgent accuracy on QuALITY with episode pagination based on LLM (PaLM 2-L) vs. uniform length pagination.</p>
<p>The compression trade-off</p>
<p>Conclusion</p>
<p>We</p>
<p>Impact Statement</p>
<p>As ReadAgent is built atop LLMs, it naturally inherits their impacts and risks.It also makes it possible to attempt to solve new problems that current LLMs cannot tackle, due to context length limitations.It is possible that ReadAgent could cause greater harms as a consequence, just as it could improve things, depending on how it is used.One risk that we were not able to study, but that seems particularly plausible, is of an increased tendency of the LLM to hallucinate when working with gist memories rather than full text.Since many details are elided in the gist memories, if the model is called upon to perform some task that requires those details, it may generate them itself without giving any indication that is the case.</p>
<p>A. Author Contributions</p>
<p>Kuang-Huei Lee developed the initial working prototype, the method and the experiments on QuALITY and NarrativeQA, was a main writer of the manuscript, and led the project overall.</p>
<p>Xinyun Chen developed the method, the LLM rater, and experiments on NarrativeQA, and significantly contributed to manuscript writing.</p>
<p>Hiroki Furuta developed the web navigation experiments, and significantly contributed to manuscript writing.</p>
<p>John Canny contributed in the initial conceptualization, advised the project, and helped with manuscript editing.</p>
<p>Ian Fischer co-proposed the core idea, developed the method and experiments on QMSum, and was a main writer of the manuscript.</p>
<p>B. Evaluation with GPT-3.5</p>
<p>Table 7 shows the results of running experiments using exactly the same setup as described in Section 4.3.1,but using GPT 3.5 Turbo rather than PaLM 2-L.GPT 3.5 Turbo has a context length of over 16,000 tokens, so the QuALITY dataset easily fits into context.We don't specifically tune prompts for GPT 3.5 Turbo, but instead use the same prompts that we use for PaLM 2-L.GPT 3.5 Turbo has a much harder time with this task than PaLM 2-L, but the same general trends hold.</p>
<p>Neural Retrieval is weaker than ReadAgent.ReadAgent-S achieves comparable performance to using the full article content.The gap between ReadAgent-P and ReadAgent-S appears to be larger using this model, but we found that ReadAgent-P is very restrictive of how many pages to look up (1.0 in average) even though we allow up to 5. We think that this can likely be remedied if we engineer the prompt for GPT 3.5 Turbo.Nonetheless, comparing to using top 3 from neural retrieval, ReadAgent-P still yields better accuracy and compression rate.Table 7. QuALITY results on the dev set of 230 docs and 2086 questions using GPT-3.5-turbo.CR is the compression rate.# LU is the number of lookups.We report 1 run for each experiment for cost considerations.</p>
<p>Method</p>
<p>C. Pagination Hyperparameters</p>
<p>Pagination Details As described in Section 3.1, max words and min words are two episode pagination hyperparameters.Table 8 gives their values for each of the experiments in Section 4. NarrativeQA Gutenberg 3000 500</p>
<p>NarrativeQA movie scripts 1000 600</p>
<p>Table 8.Pagination hyperparameters.</p>
<p>"off course" Question 2</p>
<p>What happened to Dameri while he was in custody of the government?(A) He picked up an accent from the guards (B) He slept almost the entire time (C) He learned horses were creatures that could be ridden (D) He was too shy to speak The correct answer is (B).ReadAgent chose (B).Neural retrieval chose (A).</p>
<p>Incorrect retrieval</p>
<p>The same story provides two examples of the consequences of incorrect retrieval, and the benefits of the gist memory.For the question above, ReadAgent looked up pages 3 and 4. Neural retrieval looked up pages 0, 1, 3, and 6.The correct answer is clearly stated on Page 4, and also clearly stated in the gist of Page 4. If the LLM had access to either of those, it should have been able to answer correctly.Instead, it was undoubtedly confused by Pages 0 and 1, where the alien learns an accent from one of the police officers in the initial encounter.</p>
<p>"off course" Question 3</p>
<p>How did Dameri Tass communicate in English?(A) He could communicate telepathically (B) He never was able to communicate in English (C) He used a handheld translation device (D) He acquired the knowledge from a human The correct answer is (D).ReadAgent chose (D).Neural retrieval chose (C).</p>
<p>For the question above, ReadAgent looked up pages 0 and 1. Neural retrieval looked up pages 0, 3, 4, and 6.The critical information was in Page 1, although Page 0 was also relevant.The remaining pages were only relevant in that they demonstrated that (B) was incorrect.Again, the gist memory was sufficient to answer the question correctly, in addition to providing clear signal about what pages are relevant to the question.But neural retrieval's selection of Page 0 without Page 1 made (C) seem plausible, as Page 0 discusses a device that the alien was clearly trying to use for communication.</p>
<p>E. ReadAgent for Web Navigation</p>
<p>We made an attempt to extend ReadAgent to decision making tasks.In particular, we apply ReadAgent for autonomous web navigation (Shi et al., 2017;Kim et al., 2023;Furuta et al., 2024), where the goal is to autonomously control browsers or computer interfaces to complete tasks with natural language instructions provided by users.Such instruction would be something like Book an appointment for applying new passport for one adult, Ellen Walker, with phone number 123-456-7890 and email address EW@gmail.comon April 4, 2023 at 1 pm in the post office nearest to zip code 60505.Don't send updates via text message).Example web agent actions include click, type, and select (e.g.click, type nearest post office, select April 4, 2023).Because real-world websites can have very long HTML, LLM web agents often struggle with context length if it operates on raw content (Gur et al., 2023).</p>
<p>E.1. Implementation</p>
<p>Pagination For HTML, we leverage the explicit HTML DOM tree structure, decomposing the HTML into snippets with elements at a target depth and their descendants.We test the depth from 5 to 7 and choose the best.We use these snippets as the "pages" instead of asking the LLM to paginate.</p>
<p>Memory Gisting Similar to ReadAgent for reading comprehension, we prompt the LLM to summarize snippets into gists zero-shot, and subsequently concatenate the gists.We contextualize the gists with snippet index number in a python dictionary-format (e.g.{"index": ..., "content": ...}).</p>
<p>Interactive Look-up In the interactive look-up step, the LLM looks at a given task instruction, previous action history, and the gists to decide which original HTML snippets it wants to look up.We experimented with parallel look-up (ReadAgent-P) in the web navigation setting for faster experiments.Finally, to predict next-step actions, the LLM reads the retrieved snippets again and predicts the target element id to interact with, the type of action operation (click, type or select), and the input value (if any).</p>
<p>E.2. Mind2Web</p>
<p>We evaluate ReadAgent for Web Navigation on the Mind2Web (Deng et al., 2023) dataset, a real-world planning and web action prediction benchmark, consisting of 2K instructions and episodes collected from 137 websites.The agent's task is to predict the next-step action (click, type and select) given HTML, task instruction, and previous action history.Mind2Web has three test set splits: cross-task (252 tasks from 69 websites), cross-website (177 tasks from 10 websites), and cross-domain (912 tasks from 73 websites), which was originally designed for different testing different type of generalization.However, since our approach is zero-shot without training, these splits do not serve their original purposes.</p>
<p>Baselines MindAct from the Mind2Web paper (Deng et al., 2023) first uses a DeBERTa-base (He et al., 2020) model trained for task-relevant element retrieval to get the top 50 relevant elements.Instead of directly predicting target element id (part of an action), it formulates this task as iterative multi-choice question-answering with target element ids sampled from the top 50 and uses the LLM to solve it for performance purpose (see Deng et al. (2023) for details).The same LLM also predicts the type of action and an optional value.MindAct (GPT-4) results are the state-of-the-art.We additional generate MindAct results with PaLM 2-L as a reference.</p>
<p>Following the reading comprehension experiments (Section 4), we also compare with using full raw HTML, retrieval with BM25, neural retrieval with Gemini API embedding model (models/embedding-001), and using the gists without look-up, which, like ReadAgent, are not trained for web navigation tasks.We ask the LLM to directly predict that target element id as it is a simpler and more tractable implementation in our setting.Table 10.Web navigation performance on Mind2Web (Deng et al., 2023).* marks models that are trained supervisedly for the web domain.GistMem and ReadAgent results are all also based on PaLM 2-L.We evaluate the performance in element accuracy (Ele.Acc), operation F1 (Op.F1), step success rate (Step SR), and episode success rate (SR).We also measure the compression rate (CR).The best performance across all the baselines is bolded, and the best across the approaches using PaLM 2-L is underlined.ReadAgent achieves consistently better performance than using raw HTML inputs (PaLM 2-L), retrieval methods, and MindAct (PaLM 2-L) with a trained Rank LM for HTML snippet retrieval.</p>
<p>E.3. Results</p>
<p>As shown in Table 10, ReadAgent achieves strong performance compared to the baselines.In particular, the results are even better than MindAct (PaLM 2-L), which uses the supervisedly learned Rank LM, despite ReadAgent not using models trained on the web navigation domain.Prior work shows that state-of-the-art LLMs alone are generally still weaker than the approaches using models specifically trained for the web navigation domain (Furuta et al., 2023).</p>
<p>Figure 6 shows that gisting effectively reduces the number input tokens.Most of the input gists require less than 8K tokens.For example, 97.4% of gisted inputs in cross-website split fits into the 8k context length, while only 51.5% of raw HTML can fit in the context window.The inputs are truncated for the parts that exceed the context length limit, which can significantly impact performance.</p>
<p>The results in Figure 6 and Table 10 indicate that even using the gist memory and ReadAgent retrieval causes truncation on many web pages.This is because the retrieved snippets are quite large, causing the compression rate to drop substantially.In spite of those issues, the ReadAgent results give real gains over using the full context.This indicates that even the truncated gists and retrieved pages are more informative than the truncated raw HTML when using an LLM with a small context length.</p>
<p>F. Prompt Design</p>
<p>We discuss our prompt design in this section to clarify some of our design decisions.In most cases, the exact phrasing of the prompt has negligible impacts on the outcome.For example, saying "You don't need to answer the question yet."versus "DO NOT answer the question in your response." in the look-up prompts did not lead to significant differences in the results and they can be used interchangeably.</p>
<p>The exact prompts that we use for each dataset can be found on https://read-agent.github.io/.</p>
<p>Pagination Prompt In the QuALITY and QMSum experiments where the target page length is short and the max words threshold is low, we tried including the previous page in the pagination prompt (Section 3.1), as we thought it could be helpful to have more surrounding context.However, it appears that it only benefits our QMSum experiments, compared to not including the previous page.We will leave this to future studies.</p>
<p>Look-up Prompt On QuALITY, we found adding "Take a deep breath and tell me: Which page(s) would you like to read again?" at the end of the look-up prompt improves response quality, when using PaLM 2-L and GPT-3.5.This is similar to what Yang et al. (2023) found.</p>
<p>Response Prompt For tasks that have free-form answers (QMSum and NarrativeQA), we explicitly ask the model to provide short and concise answers as follows.This helps mitigate the problem of verbose answers hurting ROUGE scores as we noted in Section 4.3.3.</p>
<p>Example Response Prompt for Free-form Answers</p>
<p>{GISTS AND EXPANDED PAGES} Question: {QUESTION} Answer the question based on the above passage and retrieved pages.Your answer should be short and concise.</p>
<p>For multiple-choice questions, we query with the following prompt and parse the response to extract the model choice.</p>
<p>Example Response Prompt for Multiple-Choice Questions</p>
<p>Read the following article and answer a multiple choice question.For example, if (C) is correct, answer with "Answer: (C) ..." Article: {GISTS AND EXPANDED PAGES} Question: {QUESTION} {OPTIONS} LLM Rater Prompts In Section 4.1, we introduce Strict LLM Rater Prompt for exact matches and Permissive LLM Rater Prompt for partial matches.The main reason for this design is that measuring exact matches can be overly strict as it disqualifies model answers with additional details.For example, assuming the reference answer is "the British" and the model answer is "the British army" (correct but with more details), using LLMs with "Strict LLM Rater Prompt" will tell us that this is not an exact match.Empirically, we found these LLM ratings aligning well with our own judgements.</p>
<p>When referring to model's responses in the LLM rater prompts, we choose to refer them with "John's answer to the question was:" because we want to prompt the LLM to judge whether the model-generated answer matches with the reference answer from a more objective, third-person view.Prior work, such as (Zheng et al., 2023), uses "Assistant" to refer to responses from different model calls.Here we instead choose a common name (John) that appears to be more natural.</p>
<p>G. ReadAgent Variants</p>
<p>G.1. Unconditional and Conditional ReadAgent</p>
<p>When working with a long text, it is possible that the user will know ahead of time what task is to be solved.In that case, conceivably the gisting step could include the task description in the prompt.In so doing, it is easy to imagine that the LLM could do a better job of compressing out information that is irrelevant to the task, thereby improving efficiency and reducing distraction.This approach would be Conditional ReadAgent.However, more generally, the task may not be known while preparing the gists, or it may be known that the gists need to be used for multiple different tasks, such as answering many questions about the text.Thus, by excluding the task in the gisting step, the LLM may produce more broadly useful gists, at the cost of reduced compression and increased distracting information.This setting would be Unconditional ReadAgent.We only explore the unconditional setting in this work, but we note that the conditional setting may be preferred in some situations.</p>
<p>G.2. ReadAgent for Specific Domains</p>
<p>Related to Appendix G.1, when applying ReadAgent to specific domains, it might be helpful to provide domain-specific instructions.For example, if we apply ReadAgent to understanding a programming library, it could be useful to provide more specific instruction to LLMs to extract abstract descriptions of things like purpose of the code, functionalities, important signatures of functions or classes from each file as gists.</p>
<p>G.3. Iterative Gisting</p>
<p>For a very long event history, such as a conversation, we may consider further compression of the older memory with iterative gisting to allow having longer contexts, similar to older memories of humans being fuzzier.Though this is not in the scope of this work, it may be useful for applications such as assistant agents, where context lengths can grow arbitrarily long over time as the user interacts with the agent.</p>
<p>H. Comparing ReadAgent and MemWalker</p>
<p>As discussed in Section 2, similar to our work, MemWalker (Chen et al., 2023a) also reads long documents interactively like an agent through iterative prompting, instead of forcing LLMs to process everything at once.It first constructs a summary tree where the lowest-level leafs are segments of raw text, the second level nodes are summaries of text segments, and the higher levels are summaries of summaries.Given a task, it traverses the tree from the root to search for task-related information.We think there are a few reasons to prefer the ReadAgent approach over MemWalker.</p>
<p>First, the reliability is a concern.Having LLMs traverse summary tree may not be a reliable process.In our best-effort re-implementation of MemWalker with PaLM 2-L, it unsatisfyingly achieves 66.73% on QuALITY.To put that into perspective, using full raw content is 85.83%, ReadAgent-P (look up 1-5 pages) is 86.63%,ReadAgent-S (look up 1-6 pages) is 86.88%, and using BM25 Top-1 is 70.55%.Part of the performance difference is caused by a high search failure rate.11.7% of the searches failed to finish after sufficient retries.This failure rate of our implementation is in a similar range to what the authors reported: 91.4% successes and 8.6% failures5 .In contrast, the failure rate of ReadAgent is mostly 0%.</p>
<p>Second, the hierarchical summary structure makes it difficult to reason over related but distant information at the same granularity.There isn't much detail preserved at the top levels of the hierarchy.For example, if the two most important text pieces are at the beginning and the end of a very long text, the essential information could be in the first and last leaf.As the agent traverses down to the first leaf, it could be difficult to go back up to the root and down to the last leaf.</p>
<p>The motivations of the two approaches are also different.MemWalker interacts with a summary tree and reasons over traversal trajectories, whereas ReadAgent interacts directly with documents and reasons over gist memories.</p>
<p>I. NarrativeQA Additional Details</p>
<p>Context Length Control As the NarrativeQA Gutenberg texts can be very long, the corresponding gists can sometimes exceed the context length.For those exceptionally long texts, we ask the LLM to go through the pages and think whether it makes sense to merge pages iteratively with the following prompt, and then re-gist the new set of pages.In so doing, we are able to increase the average page size and thus the compression rate (Figure 7).The gists and pages can both be long for NarrativeQA.Thus, in the interactive look-up step of ReadAgent-P, we prevent the retrieved pages from exceeding the context length by asking the model to sort the pages by importance with the prompt below and iteratively detecting whether adding any pages could go beyond the context window.For ReadAgent-S, we do a similar check to decide whether to early-stop the sequential look-up.</p>
<p>Example Parallel Lookup Prompt (ReadAgent-P) for NarrativeQA</p>
<p>The following text is what you remember from reading an article and a question related to it.You may read 1, 2 or 3 page(s) of the article again to refresh your memory to prepare yourself for the question.</p>
<p>Please respond with which page(s) you would like to read in the order of importance, beginning with the most important page number.</p>
<p>For</p>
<p>J. Additional QMSum Results</p>
<p>Figure 8 shows the histogram of word counts on the QMSum training set and the corresponding gist memories.</p>
<p>Table 11 shows the same results as Table 3, but on the QMSum test set.</p>
<p>41 st International Conference on Machine Learning, Vienna, Austria.PMLR 235, 2024.Copyright 2024 by the author(s).</p>
<p>Figure 1 .
1
Figure 1.ReadAgent workflow.</p>
<p>); Chiang et al. (2023); Zheng et al. (2023); Chiang &amp; Lee (2023).</p>
<p>Figure 3 .
3
Figure 3. Histogram of NarrativeQA (Gutenberg) test set word counts for the original text and the gists.</p>
<p>Figure 4 .
4
Figure 4. Histogram of NarrativeQA (movie) test set word counts for the original text and the gists.</p>
<p>Figure 5 .
5
Figure 5. Histogram of QMSum word counts for the original transcripts and the gisted transcripts.The gisted transcripts are all less than 5,000 words, allowing them to entirely fit into the context window of PaLM 2-L.</p>
<p>Figure 6 .
6
Figure 6.(Left) Histogram of raw HTML and gist tokens in the Mind2Web cross-website split.Most of the input gists require fewer than 8K tokens.(Right) Statistics of token counts of raw HTML and gists.</p>
<p>Figure 7 .
7
Figure 7. Histogram of NarrativeQA (Gutenberg) test set gists before and after page merging on the exceptionally long texts.</p>
<p>Figure 8 .
8
Figure 8. Histogram of QMSum word counts for the original transcripts and the gisted transcripts.The gisted transcripts are all less than 5,000 words, allowing them to entirely fit into the context windows of PaLM 2-L.</p>
<p>For example, if ⟨57⟩ is a good point to break, answer with "Break point: ⟨57⟩\n Because ..."Memory Gisting For each page, we prompt the LLM to shorten the exact content into a gist, or summary, as follows.
Passage:{...}{PARAGRAPH 5 TEXT}⟨5⟩{PARAGRAPH 6 TEXT}⟨6⟩{PARAGRAPH 7 TEXT}{...}Example Gisting PromptPlease shorten the following passage.Just give me a shortened version. DO NOT explain yourreason.Passage:{PAGE TEXT}</p>
<p>Table 1 .
1
Kočiskỳ et al., 2018)e dev set of 230 docs and 2086 questions using PaLM 2-L.CR is the compression rate.#LU is the number of lookups.We report means and standard deviations across 3 runs, except where inconsequential (CR and # LU).Kočiskỳ et al., 2018)has the longest context length on average among the three reading comprehension datasets we choose.The dataset is divided into books (Gutenberg) and move scripts.The Gutenberg test set have 70,619 words on average, and the maximum is 343,910 words; the movie scripts test set have 29,963 on average,
20Documents Document Gists1000100020003000 words400050006000Figure 2. Histogram of QuALITY document and gist word counts.curacy. Notably, starting from ReadAgent (Look up 1-2pages), it outperforms all baselines methods including us-ing the full original text, which could have been an upperbound on the performance -every other method reducesthe amount of text the LLM considers before generatingits response. However, this is not a surprising result. Priorwork shows that current LLMs are not able to effectively usethe full long context window (Liu et al., 2023), potentiallydue to training data sparsity, and distracting informationcan also reduce performance (Shi et al., 2023; Weston &amp;Sukhbaatar, 2023). The corresponding compression rate ofReadAgent (Look up 1-2 pages) is 72.17%, meaning that∼ 3.5× as many tokens can fit in the context window aftergisting.MethodCR (# LU)AccuracyBM25 RetrievalTop-189.27% (1)70.34% ± 0.06Top-278.96% (2)79.05% ± 0.05Top-368.50% (3)82.65% ± 0.05Top-458.57% (4)84.42% ± 0.13Neural Retrieval with Gemini APITop-189.91% (1)71.32% ± 0.19Top-280.08% (2)79.02% ± 0.10Top-370.28% (3)83.41% ± 0.10Top-460.68% (4)84.88% ± 0.03Full Raw Content0%85.83% ± 0.19GistMem85.53%77.52% ± 0.13ReadAgent-PLook up 1 pg76.00% (1.0) 84.13% ± 0.10Look up 1-2 pgs72.17% (1.6) 86.16% ± 0.12Look up 1-3 pgs69.36% (2.0) 86.59% ± 0.10Look up 1-4 pgs67.73% (2.2) 86.86% ± 0.00Look up 1-5 pgs66.45% (2.3) 86.83% ± 0.10Look up 1-6 pgs64.75% (2.5) 86.91% ± 0.08ReadAgent-S 1-6 pgs 58.53% (3.2) 87.17% ± 0.184.3.2. NARRATIVEQANarrativeQA (</p>
<p>Table 2 .
2
NarrativeQA
Top-197.63% (1)39.01%50.14%0.1660.0610.15697.42% (1)43.5%55.33%0.1760.0650.165Top-295.24% (2)49.34%60.76%0.2030.0790.19194.80% (2)51.70%64.53%0.2060.0820.194Top-393.34% (3)52.73%63.68%0.2080.0800.19593.02% (3)52.97%66.03%0.2100.0830.197Top-492.47% (4)53.59%64.26%0.2110.0820.19792.27% (4)53.60%66.16%0.2100.0840.197Neural Retrieval with Gemini APITop-198.19% (1)34.25%46.53%0.1460.0510.13498.14% (1)36.47%47.8%0.1500.0540.140Top-296.30% (2)44.69%54.96%0.1800.0690.16796.15% (2)44.48%56.17%0.1820.0700.170Top-394.62% (3)46.24%57.31%0.1910.0770.17894.42% (3)48.97%60.73%0.1950.0760.183Top-493.45% (4)48.59%59.21%0.1960.0790.18493.25% (4)50.62%62.05%0.2030.0800.191GistMem96.89%55.31%68.22%0.2330.0910.21896.80%55.79%71.19%0.2310.0920.217ReadAgent-PLook up 1 pg95.15% (0.94)58.92%71.89%0.2440.1010.23094.84% (0.93)59.98%73.23%0.2400.0980.226Look up 1-2 pgs94.79% (1.23)59.84%72.29%0.2390.0980.22494.36% (1.34)59.19%72.65%0.2310.0910.218Look up 1-3 pgs94.39% (1.50)59.84%71.89%0.2400.0980.22694.03% (1.61)59.63%72.84%0.2300.0930.217ReadAgent-S 1-2 pgs94.35% (1.38)57.89%71.14%0.2390.0970.22593.86% (1.46)60.48%72.48%0.2320.0950.219ReadAgent-S 1-3 pgs94.08% (1.57)58.52%71.49%0.2420.0980.22993.67% (1.57)60.55%72.79%0.2310.0950.219Movie Validation (57 docs &amp; 1699 questions)Movie Test (172 docs &amp; 5139 questions)BM25 RetrievalTop-197.07% (1)32.67%42.61%0.1560.0580.14496.61% (1)33.64%43.34%0.1540.0540.143Top-294.12% (2)39.97%50.21%0.1870.0700.17493.81% (2)42.50%53.05%0.1910.0720.178Top-391.18% (3)43.61%53.91%0.1980.0770.18591.00% (3)46.97%57.52%0.2070.0800.193Top-488.24% (4)46.85%57.62%0.2100.0840.19888.19% (4)50.18%60.13%0.2170.0850.202Neural Retrieval with Gemini APITop-197.07% (1)32.02%41.44%0.1530.0530.14296.67% (1)37.24%46.22%0.1300.0430.118Top-294.19% (2)43.20%51.38%0.1600.0570.14893.90% (2)46.49%54.60%0.1640.0610.151Top-391.29% (3)47.56%56.21%0.1760.0640.16391.14% (3)50.69%58.92%0.1860.0710.172Top-488.38% (4)49.09%59.33%0.1930.0750.18088.36% (4)52.13%59.41%0.1840.0720.171GistMem92.09%52.56%64.39%0.2420.1030.22791.98%54.68%64.00%0.2480.1050.234ReadAgent-PLook up 1 pg89.20% (0.99)53.38%65.57%0.2470.1060.23389.22% (0.98)57.68%68.01%0.2740.1160.260Look up 1-2 pgs87.68% (1.52)54.62%65.63%0.2380.0980.22388.10% (1.39)58.24%68.81%0.2700.1150.255Look up 1-3 pgs86.57% (1.91)54.91%65.86%0.2410.0990.22586.73% (1.89)58.82%69.12%0.2720.1160.257ReadAgent-S 1-2 pgs86.36% (1.98)59.33%68.28%0.2030.0820.18885.92% (1.98)63.33%72.06%0.2140.0860.199ReadAgent-S 1-3 pgs83.56% (2.95)59.45%68.81%0.2100.0870.19583.18% (2.95)64.53%73.06%0.2170.0900.202
results (PaLM 2-L).R-1, R-2, and R-L are ROUGE F-Measures.LR-1, and LR-2 are LLM-Ratings.</p>
<p>Table 2 .
2
ReadAgent again outperforms all the baselines across all subsets of NarrativeQA.46.57% ± 0.87 91.54% ± 0.30 32.90 ± 0.17 8.87 ± 0.23 21.15 ± 0.14 68.87 ± 0.60
MethodCR (# LU) LLM Rating-1 LLM Rating-2 ROUGE-1 ROUGE-2 ROUGE-L Resp. LengthBM25 RetrievalTop-195.69% (1.00) 32.48% ± 1.65 63.85% ± 1.51 27.53 ± 0.23 7.00 ± 0.14 18.45 ± 0.16 48.62 ± 0.28Top-291.48% (2.00) 29.41% ± 0.60 71.57% ± 1.48 28.85 ± 0.17 7.59 ± 0.08 19.34 ± 0.14 52.39 ± 0.49Top-386.93% (3.00) 34.80% ± 1.14 79.53% ± 0.35 30.69 ± 0.17 8.40 ± 0.11 20.64 ± 0.13 53.59 ± 0.35Top-482.55% (4.00) 35.66% ± 0.30 81.13% ± 0.35 31.10 ± 0.10 8.53 ± 0.06 20.36 ± 0.11 54.96 ± 0.42Top-578.13% (5.00) 39.09% ± 0.92 84.44% ± 0.46 31.16 ± 0.14 8.52 ± 0.08 20.69 ± 0.03 54.52 ± 0.13Top-673.97% (6.00) 37.87% ± 0.90 83.70% ± 0.87 31.06 ± 0.04 8.38 ± 0.06 20.43 ± 0.08 56.18 ± 0.44Neural Retrieval with Gemini APITop-195.99% (1.00) 34.80% ± 1.39 68.87% ± 0.62 27.86 ± 0.12 7.12 ± 0.04 18.76 ± 0.09 49.46 ± 0.23Top-292.02% (2.00) 40.32% ± 0.92 81.50% ± 0.46 30.17 ± 0.08 8.03 ± 0.03 19.80 ± 0.08 55.48 ± 0.27Top-387.93% (3.00) 40.93% ± 1.35 85.17% ± 1.25 31.36 ± 0.12 8.67 ± 0.10 20.68 ± 0.10 56.71 ± 0.27Top-483.71% (4.00) 40.56% ± 0.62 84.31% ± 0.87 31.52 ± 0.11 8.59 ± 0.10 20.40 ± 0.10 56.47 ± 0.71Top-579.47% (5.00) 40.20% ± 0.76 86.76% ± 0.60 31.32 ± 0.11 8.49 ± 0.11 20.49 ± 0.07 56.73 ± 0.91Top-675.44% (6.00) 40.81% ± 0.52 87.01% ± 0.35 31.92 ± 0.02 8.73 ± 0.09 20.82 ± 0.05 58.39 ± 0.31Truncated Raw ContentFirst 6k words32.59% (0.00) 14.71% ± 0.79 52.45% ± 0.69 25.42 ± 0.05 4.98 ± 0.09 16.58 ± 0.10 58.42 ± 0.11Last 6k words32.38% (0.00) 10.42% ± 0.62 35.66% ± 2.46 20.69 ± 0.19 3.44 ± 0.10 14.13 ± 0.08 44.23 ± 0.11GistMem83.13% (0.00) 40.20% ± 0.96 89.83% ± 0.76 31.00 ± 0.09 7.99 ± 0.04 20.15 ± 0.08 65.75 ± 0.20ReadAgent-PLook up 1 pg80.00% (0.98) 40.56% ± 0.46 89.46% ± 1.48 31.26 ± 0.09 8.22 ± 0.15 20.29 ± 0.07 63.78 ± 1.13Look up 1-2 pgs77.38% (1.71) 39.71% ± 1.87 89.71% ± 0.60 31.11 ± 0.04 8.01 ± 0.15 20.21 ± 0.04 64.73 ± 1.02Look up 1-3 pgs75.07% (2.53) 38.36% ± 1.21 89.71% ± 0.60 31.50 ± 0.29 8.15 ± 0.15 20.45 ± 0.24 63.91 ± 1.58Look up 1-4 pgs73.48% (3.08) 39.95% ± 1.51 90.56% ± 0.35 31.34 ± 0.05 8.08 ± 0.18 20.26 ± 0.07 63.40 ± 0.79Look up 1-5 pgs72.29% (3.50) 37.99% ± 0.96 87.75% ± 0.46 31.16 ± 0.10 8.06 ± 0.05 20.35 ± 0.12 65.22 ± 1.40Look up 1-6 pgs70.90% (3.97) 39.09% ± 2.04 88.24% ± 0.60 31.50 ± 0.30 8.05 ± 0.13 20.26 ± 0.13 66.70 ± 0.62ReadAgent-S 1-6 pgs 70.34% (3.55)</p>
<p>Table 3 .
3
QMSum validation results (PaLM 2-L) means and standard deviations across 3 runs.35 articles and 272 questions.CR is the compression rate.# LU is the number of lookups.Resp.Length is the length in words of the model's final response.</p>
<p>Table 4 .
4
ReadAgent retrieval vs. GistMem with neural retrieval.
MethodAccuracyGistMem + Neural Retrieval Top-182.65%ReadAgent-P (Look up 1 pg)84.13%</p>
<p>Table 6 .
6
Table 6 presents the empirical results of compression rate increasing as page size increases.Compression rate increases as the maximum number of words allowed per page increases on QuALITY.Our default setting of min/max words is 280/600.In the other three experiments, we scale min words proportionally with max words.
GistMemReadAgent-P (1-5 pgs)max wordsCRAccCRAcc40081.81% 78.91% 66.71%86.82%60085.53% 77.52% 66.45%86.83%80088.12% 76.22% 65.06%86.34%120091.38% 73.97% 61.77%85.67%
As the compression rate decreases, the gists are more useful for answering questions directly.However, for ReadAgent with look-ups, when the initial gist compression rate gets too high, accuracy suffers.</p>
<p>have presented ReadAgent, a simple interactive prompting system to mitigate the context length and context use limitations of current LLMs.ReadAgent outperforms other strong zero-shot (i.e., not trained or finetuned on the training set) baselines across standard performance metrics.These results demonstrate that LLMs are capable of generating compressed textual representations of long contexts that are useful for tasks that humans think are important, even without knowing those tasks ahead of time.They also demonstrate that LLMs are capable of reasoning interactively over such compressed representations, using them to decide what information needs to be retrieved to effectively perform a known task.ReadAgent increases the effective context length by up to 20× while outperforming conventional retrieval techniques.However, it does not give infinite context lengths, nor does it guarantee good performance when the gist memory itself is extremely long.Future work will need to address these fundamental limitations in LLMs.</p>
<p>example, if you only need to read Page 8, respond with "I want to look up Page [8] to ...".If you would like to read Page 12 and 7, respond with "I want to look up Page [12, 7] to ...".If you would like to read Page 15, 2 and 3, respond with "I want to look up Page [15, 2, 3] to ...".DO NOT select more pages if you don't need to.You don't need to answer the question yet.
Text:{GIST MEMORY}Question:{QUESTION}
Fuzzy-trace theory(Reyna &amp; Brainerd, 1995b) posits that people form two types of memory representations about a past event -verbatim and gist memories. Gist memories, often episodic, are fuzzy memories of past events, whereas verbatim memories contain details of past events. People prefer to reason with gists rather than with verbatim memories(Reyna,<br />
).
http://openai.com/api/
https://ai.google.dev/models/gemini
Available at http://aleph.gutenberg.org/3/0/0/3/30035//30035-h//30035-h.htm.
https://openreview.net/forum?id=H5XZLeXWPS
AcknowledgementsThe authors thank Sergey Ioffe, Rif A. Saurous, Yujin Tang, Sergio Guadarrama, Daliang Li, Felix Yu, and Rob Fergus for valuable feedback and discussion.D. Case StudyIn this section, we analyze reading comprehension examples to demonstrate where the ability to simultaneously think over long-range global context and focus on local information is important.We selected the short story "off course" by Mack Reynolds 4 because it is extremely short (2,712 words) and it is only broken into 8 pages, yet even so, neural retrieval using 4 pages gets three questions wrong that ReadAgent correctly answers.For this story, ReadAgent answers 6 of 8 questions correctly.Neural retrieval answers 3 of 8 correctly, and doesn't get either question correct that ReadAgent misses.Note that in all three examples, ReadAgent only chooses to select two pages, even though it is also permitted to select up to 4. This flexibility is another advantage that ReadAgent has over standard retrieval systems."off course" Gist Memory ⟨P0⟩ Patrolmen Dermott and Casey encounter Dameri Tass, an alien who has landed on Earth.Dameri attempts to communicate with them using a device that translates his thoughts into English.⟨P1⟩ The alien Dameri Tass used a helmet to learn English from Tim Casey, an Irish patrolman.He then became fascinated by a horse and wanted to use the helmet on the animal.Patrolman Dermott felt like he was in a shaggy dog story.⟨P2⟩ A helicopter arrived, interrupting the horse's inspection.Two Army officers exited and ordered a police cordon around the spacecraft.The alien spoke, surprising the general.More police and military personnel arrived.⟨P3⟩ Dameri Tass, an alien visitor, was whisked away to Washington and held incommunicado for several days.His arrival caused a global furor.Officials worried about the potential impact of his message on society.Eventually, the UN demanded that he be allowed to speak before the Assembly.The White House agreed and a date was set.⟨P4⟩ The world eagerly awaited a message from space.Dameri Tass, an envoy from a super-civilization, was expected to guide the world.Most people were ready to be guided, but some were not.The U.N. Secretary-General was nervous about introducing the envoy, as they knew very little about him.He had been asleep for most of his time on Earth and had only recently woken up.He spent his time playing with a dog, cat, and mouse.The Secretary-General was worried about what the envoy would say.⟨P5⟩ Dameri Tass, an alien, is brought to Earth and mistaken for an envoy from another planet.He reveals he is just a collector for a zoo.⟨P6⟩ Dameri Tass, an alien, mistakenly landed on Earth.He addressed a large crowd, criticizing their weapons, wars, and lack of a planet-wide government.He then left, refusing to take any Earth creatures with him, but expressing interest in horses.⟨P7⟩ The others watched as the first visitor from space hurriedly left Earth.Page # Starting sentence in text 0 Shure and begorra... 1The alien stooped down... 2Interest in the horse was ended...3"Sure, and it's quite a reception"... 4Excitement, anticipation... 5 "Here he comes,"... 6He straightened and started off... 7The others drew back... Table9. Pagination for "off course".Distracting retrievalThe first question gives an example of retrieval of distracting pages and the lack of global context provided by the gist memory causing the LLM to select the incorrect answer when using neural retrieval, even though it had also retrieved the pages that should have led to the correct answer.We provide the gist memory below and the story's pagination in Table9."off course" Question 1What was Dameri's purpose in landing on earth?(A) He wanted to witness an uncivilized planet and share knowledge (B) His spaceship needed to land for repairs (C) He heard reports that Earth had interesting animal specimens for his collection (D) He arrived on accident while exploring planets in the Galactic League The correct answer is (D).ReadAgent chose (D).Neural retrieval chose (C).For the question above, ReadAgent looked up pages 5 and 6.Neural retrieval looked up pages 3, 4, 5, and 6.Pages 4 and 5 both make prominent mention of animals, and Page 5 explicitly mentions that the alien is a collector for a zoo, so answer (C) seems reasonable based on the information on those pages.However, Pages 5 and 6, together with the global context from the gist memory, make it clear that (D) is the correct answer.Since neural retrieval provided both of those pages, the lack of the global context combined with the additional distractor pages led the LLM astray.
J Ainslie, T Lei, M De Jong, S Ontañón, S Brahma, Y Zemlyanskiy, D Uthus, M Guo, J Lee-Thorp, Y Tay, Y.-H Sung, S Sanghai, Colt5, Faster long-range transformers with conditional computation. 2023</p>
<p>. R Anil, A M Dai, O Firat, M Johnson, D Lepikhin, A Passos, S Shakeri, E Taropa, P Bailey, Z Chen, arXiv:2305.104032023arXiv preprint</p>
<p>Longformer: The long-document transformer. I Beltagy, M E Peters, A Cohan, 2020</p>
<p>Reading wikipedia to answer open-domain questions. D Chen, A Fisch, J Weston, A Bordes, 2017</p>
<p>Walking down the memory maze: Beyond context limit through interactive reading. H Chen, R Pasunuru, J Weston, A Celikyilmaz, arXiv:2310.050292023aarXiv preprint</p>
<p>Extending context window of large language models via positional interpolation. S Chen, S Wong, L Chen, Y Tian, 2023b</p>
<p>Y Chen, S Qian, H Tang, X Lai, Z Liu, S Han, J Jia, Longlora, arXiv:2309.12307Efficient fine-tuning of long-context large language models. 2023carXiv preprint</p>
<p>Can large language models be an alternative to human evaluations?. C.-H Chiang, H.-Y Lee, 10.18653/v1/2023.acl-long.870Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. A Rogers, J Boyd-Graber, N Okazaki, the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational LinguisticsJuly 20231</p>
<p>W.-L Chiang, Z Li, Z Lin, Y Sheng, Z Wu, H Zhang, L Zheng, S Zhuang, Y Zhuang, J E Gonzalez, I Stoica, E P Xing, Vicuna, An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. March 2023</p>
<p>X Deng, Y Gu, B Zheng, S Chen, S Stevens, B Wang, H Sun, Y Su, E Mind2web ; Dinan, S Roller, K Shuster, A Fan, M Auli, J Weston, arXiv:2306.06070Wizard of wikipedia: Knowledge-powered conversational agents. 2023. 2019arXiv preprintTowards a generalist agent for the web</p>
<p>Language model agents suffer from compositional generalization in web automation. H Furuta, Y Matsuo, A Faust, I Gur, H Furuta, K.-H Lee, O Nachum, Y Matsuo, A Faust, S S Gu, I Gur, arXiv:2311.18751The Twelfth International Conference on Learning Representations. 2023. 2024arXiv preprintMultimodal web navigation with instruction-finetuned foundation models</p>
<p>LongT5: Efficient text-totext transformer for long sequences. M Guo, J Ainslie, D Uthus, S Ontanon, J Ni, Y.-H Sung, Y Yang, 10.18653/v1/2022.findings-naacl.55Findings of the Association for Computational Linguistics: NAACL 2022. M Carpuat, M.-C De Marneffe, I V Meza Ruiz, Seattle, United StatesAssociation for Computational LinguisticsJuly 2022</p>
<p>A real-world webagent with planning, long context understanding, and program synthesis. I Gur, H Furuta, A Huang, M Safdari, Y Matsuo, D Eck, A Faust, arxiv:2307.128562023arXiv preprint</p>
<p>Lm-infinite: Simple on-the-fly length generalization for large language models. C Han, Q Wang, W Xiong, Y Chen, H Ji, S Wang, P He, X Liu, J Gao, W Chen, Deberta, arXiv:2308.16137arXiv:2006.03654Decodingenhanced bert with disentangled attention. 2023. 2020arXiv preprint</p>
<p>Leveraging passage retrieval with generative models for open domain question answering. G Izacard, E Grave, H Jin, X Han, J Yang, Z Jiang, Z Liu, C.-Y Chang, H Chen, X Hu, arXiv:2401.01325Llm maybe longlm: Selfextend llm context window without tuning. 2021. 2024arXiv preprint</p>
<p>The narrativeqa reading comprehension challenge. G Kim, P Baldi, S Mcaleer, T Kočiskỳ, J Schwarz, P Blunsom, C Dyer, K M Hermann, G Melis, E Grefenstette, J Lanchantin, S Toshniwal, J Weston, A Szlam, S Sukhbaatar, arXiv:2305.00833Transactions of the Association for Computational Linguistics. 62023. 2018. 2023arXiv preprintLearning to reason and memorize with self-notes</p>
<p>Retrieval-augmented generation for knowledgeintensive NLP tasks. P Lewis, E Perez, A Piktus, F Petroni, V Karpukhin, N Goyal, H Küttler, M Lewis, W.-T Yih, T Rocktäschel, Advances in Neural Information Processing Systems. 202033</p>
<p>ROUGE: A package for automatic evaluation of summaries. C.-Y Lin, Proceedings of the ACL Workshop: Text Summarization Branches Out. the ACL Workshop: Text Summarization Branches Out2004. 20040110</p>
<p>N F Liu, K Lin, J Hewitt, A Paranjape, M Bevilacqua, F Petroni, P Liang, arXiv:2307.03172Lost in the middle: How language models use long contexts. 2023arXiv preprint</p>
<p>R Nakano, J Hilton, S Balaji, J Wu, L Ouyang, C Kim, C Hesse, S Jain, V Kosaraju, W Saunders, arXiv:2112.09332Browser-assisted question-answering with human feedback. 2021arXiv preprint</p>
<p>Quality: Question answering with long input texts, yes!. R Y Pang, A Parrish, N Joshi, N Nangia, J Phang, A Chen, V Padmakumar, J Ma, J Thompson, H He, Proceedings of the 2022 Conference of the North American Chapter. the 2022 Conference of the North American ChapterHuman Language Technologies2022</p>
<p>Generative agents: Interactive simulacra of human behavior. J S Park, J O'brien, C J Cai, M R Morris, P Liang, M S Bernstein, Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology. the 36th Annual ACM Symposium on User Interface Software and Technology2023</p>
<p>Train short, test long: Attention with linear biases enables input length extrapolation. B Peng, C Li, P He, M Galley, J Gao, N Smith, M Lewis, arXiv:2304.03277International Conference on Learning Representations. 2023. 2022arXiv preprintInstruction tuning with gpt-4</p>
<p>Fuzzy-trace theory: Some foundational issues. V Reyna, C Brainerd, Learning and Individual differences. 721995a</p>
<p>A theory of medical decision making and health: fuzzy trace theory. V F Reyna, Medical decision making. 2862008</p>
<p>A new intuitionism: Meaning, memory, and development in fuzzy-trace theory. V F Reyna, Judgment and Decision making. 732012</p>
<p>Fuzzy-trace theory: An interim synthesis. V F Reyna, C J Brainerd, Learning and individual Differences. 711995b</p>
<p>Scrolls: Standardized comparison over long language sequences. S Robertson, H Zaragoza, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language Processing2009. 20223The probabilistic relevance framework: BM25 and beyond</p>
<p>Large language models can be easily distracted by irrelevant context. F Shi, X Chen, K Misra, N Scales, D Dohan, E H Chi, N Schärli, D Zhou, International Conference on Machine Learning. PMLR2023</p>
<p>World of bits: An open-domain platform for webbased agents. T Shi, A Karpathy, L Fan, J Hernandez, P Liang, International Conference on Machine Learning. 2017</p>
<p>S Sun, Y Liu, S Wang, C Zhu, M Iyyer, Pearl, arXiv:2305.14564Prompting large language models to plan and execute actions over long documents. 2023arXiv preprint</p>
<p>Efficient transformers: A survey. Y Tay, M Dehghani, D Bahri, D Metzler, 10.1145/3530811ACM Comput. Surv. 0360-0300556dec 2022</p>
<p>Attention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, L U Kaiser, I Polosukhin, I Guyon, U V Luxburg, S Bengio, H Wallach, Advances in Neural Information Processing Systems. R Fergus, S Vishwanathan, R Garnett, Curran Associates, Inc201730</p>
<p>System 2 attention (is something you might need too). J Weston, S Sukhbaatar, arXiv:2311.118292023arXiv preprint</p>
<p>J Wu, L Ouyang, D M Ziegler, N Stiennon, R Lowe, J Leike, Christiano , P , arXiv:2109.10862Recursively summarizing books with human feedback. 2021arXiv preprint</p>
<p>Memorizing transformers. Y Wu, M N Rabe, D Hutchins, C Szegedy, International Conference on Learning Representations. 2022</p>
<p>Efficient streaming language models with attention sinks. G Xiao, Y Tian, B Chen, S Han, M Lewis, arXiv:2309.174532023arXiv preprint</p>
<p>C Yang, X Wang, Y Lu, H Liu, Q V Le, D Zhou, X Chen, arXiv:2309.03409Large language models as optimizers. 2023arXiv preprint</p>
<p>Generating longer stories with recursive reprompting and revision. K Yang, Y Tian, N Peng, D Klein, Re3, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language Processing2022</p>
<p>Webshop: Towards scalable real-world web interaction with grounded language agents. S Yao, H Chen, J Yang, K Narasimhan, Advances in Neural Information Processing Systems. 202235</p>
<p>Interactive machine comprehension with information seeking agents. X Yuan, J Fu, M.-A Côté, Y Tay, C Pal, A Trischler, Proceedings of the 58th Annual Meeting of the. the 58th Annual Meeting of theAssociation for Computational Linguistics2020</p>
<p>Big bird: Transformers for longer sequences. M Zaheer, G Guruganesh, K A Dubey, J Ainslie, C Alberti, S Ontanon, P Pham, A Ravula, Q Wang, L Yang, A Ahmed, Advances in Neural Information Processing Systems. H Larochelle, M Ranzato, R Hadsell, M Balcan, H Lin, Curran Associates, Inc202033</p>
<p>Judging LLM-as-a-judge with MT-bench and chatbot arena. L Zheng, W.-L Chiang, Y Sheng, S Zhuang, Z Wu, Y Zhuang, Z Lin, Z Li, D Li, E Xing, H Zhang, J E Gonzalez, I Stoica, Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2023</p>
<p>Qmsum: A new benchmark for query-based multi-domain meeting summarization. M Zhong, D Yin, T Yu, A Zaidi, M Mutuma, R Jha, A Hassan, A Celikyilmaz, Y Liu, X Qiu, Proceedings of the 2021 Conference of the North American Chapter. the 2021 Conference of the North American ChapterHuman Language Technologies2021</p>
<p>Method CR (# LU) LLM Rating-1 LLM Rating-2 ROUGE-1 ROUGE-2 ROUGE-L Resp. W Zhong, L Guo, Q Gao, Y Wang, Memorybank, arXiv:2305.102502023arXiv preprintEnhancing large language models with long-term memory. Length BM25 Retrieval Top-1</p>
<p>. Neural Retrieval with Gemini API Top-1. </p>
<p>Truncated Raw Content First 6k words. 31.51% (0.00) 13.17% ± 1.05 47.81% ± 5.90 24.15 ± 1.42 4.89 ± 0.57 16.27 ± 0.96 61.43 ± 3.53 Last 6k words 33.80% (0.00) 13.76% ± 0.84 43.42% ± 0.00 22.90 ± 0.10 4.35 ± 0.04 15.69 ± 0.03 52.47 ± 0.39</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1549 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1549</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1549</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-30.html">extraction-schema-30</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-d7d5406eabff0e0e7fc5f7d8706297d85446e4b2</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/d7d5406eabff0e0e7fc5f7d8706297d85446e4b2" target="_blank">DOM-Q-NET: Grounded RL on Structured Language</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> DOM-Q-NET is introduced, a novel architecture for RL-based web navigation to address both the large discrete action space and the varying number of actions between the states, and 2x improvements in sample efficiency when training in the multi-task setting.</p>
                <p><strong>Paper Abstract:</strong> Building agents to interact with the web would allow for significant improvements in knowledge understanding and representation learning. However, web navigation tasks are difficult for current deep reinforcement learning (RL) models due to the large discrete action space and the varying number of actions between the states. In this work, we introduce DOM-Q-NET, a novel architecture for RL-based web navigation to address both of these problems. It parametrizes Q functions with separate networks for different action categories: clicking a DOM element and typing a string input. Our model utilizes a graph neural network to represent the tree-structured HTML of a standard web page. We demonstrate the capabilities of our model on the MiniWoB environment where we can match or outperform existing work without the use of expert demonstrations. Furthermore, we show 2x improvements in sample efficiency when training in the multi-task setting, allowing our model to transfer learned behaviours across tasks.</p>
                <p><strong>Cost:</strong> 0.005</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Reinforcement learning on web interfaces using workflow-guided exploration <em>(Rating: 2)</em></li>
                <li>World of bits: An open-domain platform for web-based agents <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1549",
    "paper_id": "paper-d7d5406eabff0e0e7fc5f7d8706297d85446e4b2",
    "extraction_schema_id": "extraction-schema-30",
    "extracted_data": [],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Reinforcement learning on web interfaces using workflow-guided exploration",
            "rating": 2
        },
        {
            "paper_title": "World of bits: An open-domain platform for web-based agents",
            "rating": 1
        }
    ],
    "cost": 0.004955,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>DOM-Q-NET: <br> Grounded RL on Structured Language</h1>
<p>Sheng Jia<br>University of Toronto<br>Vector Institute<br>sheng.jia@utoronto.ca</p>
<p>Jamie Kiros<br>Google Brain<br>kiros@google.com</p>
<p>Jimmy Ba<br>University of Toronto<br>Vector Institute<br>jba@cs.toronto.ca</p>
<h2>ABSTRACT</h2>
<h4>Abstract</h4>
<p>Building agents to interact with the web would allow for significant improvements in knowledge understanding and representation learning. However, web navigation tasks are difficult for current deep reinforcement learning (RL) models due to the large discrete action space and the varying number of actions between the states. In this work, we introduce DOM-Q-NET, a novel architecture for RL-based web navigation to address both of these problems. It parametrizes Q functions with separate networks for different action categories: clicking a DOM element and typing a string input. Our model utilizes a graph neural network to represent the tree-structured HTML of a standard web page. We demonstrate the capabilities of our model on the MiniWoB environment where we can match or outperform existing work without the use of expert demonstrations. Furthermore, we show 2 x improvements in sample efficiency when training in the multi-task setting, allowing our model to transfer learned behaviours across tasks.</p>
<h2>1 INTRODUCTION</h2>
<p>Over the past years, deep reinforcement learning (RL) has shown a huge success in solving tasks such as playing arcade games (Mnih et al., 2015) and manipulating robotic arms (Levine et al., 2016). Recent advances in neural networks allow RL agents to learn control policies from raw pixels without feature engineering by human experts. However, most of the deep RL methods focus on solving problems in either simulated physics environments where the inputs to the agents are joint angles and velocities, or simulated video games where the inputs are rendered graphics. Agents trained in such simulated environments have little knowledge about the rich semantics of the world.</p>
<p>The World Wide Web (WWW) is a rich repository of knowledge about the real world. To navigate in this complex web environment, an agent needs to learn about the semantic meaning of texts, images and the relationships between them. Each action corresponds to interacting with the Document Object Model (DOM) from tree-structured HTML. Tasks like finding a friend on a social network, clicking an interesting link, and rating a place on Google Maps can be framed as accessing a particular DOM element and modifying its value with the user input.</p>
<p>In contrast to Atari games, the difficulty of web tasks comes from their diversity, large action space, and sparse reward signals. A common solution for the agent is to mimic the expert demonstration by imitation learning in the previous works (Shi et al., 2017; Liu et al., 2018). Liu et al. (2018) achieved state-of-the-art performance with very few expert demonstrations in the MiniWoB (Shi et al., 2017) benchmark tasks, but their exploration policy requires constrained action sets, hand-crafted with expert knowledge in HTML.</p>
<p>In this work, our contribution is to propose a novel architecture, DOM-Q-NET, that parametrizes factorized Q functions for web navigation, which can be trained to match or outperform existing work on MiniWoB without using any expert demonstration. Graph Neural Network (Scarselli et al., 2009; Li et al., 2016; Kipf \&amp; Welling, 2016) is used as the main backbone to provide three levels of state and action representations.</p>
<p>In particular, our model uses the neural message passing and the readout (Gilmer et al., 2017) of the local DOM representations to produce neighbor and global representations for the web page.</p>
<p>We also propose to use three separate multilayer perceptrons (MLP) (Rumelhart et al., 1985) to parametrize a factorized Q function for different action categories: "click", "type" and "mode". The entire architecture is fully differentiable, and all of its components are jointly trained.</p>
<p>Moreover, we evaluate our model on multitask learning of web navigation tasks, and demonstrate the transferability of learned behaviors on the web interface. To our knowledge, this is the first instance that an RL agent solves multiple tasks in the MiniWoB at once. We show that the multi-task agent achieves an average of 2 x sample efficiency comparing to the single-task agent.</p>
<h1>2 BACKGROUND</h1>
<h3>2.1 REPRESENTING WEB PAGES USING DOMs</h3>
<p>The Document Object Model (DOM) is a programming interface for HTML documents and it defines the logical structure of such documents. DOMs are connected in a tree structure, and we frame web navigation as accessing a DOM and optionally modifying it by the user input. As an elementary object, each DOM has a "tag" and other attributes such as "class", "is focused", similar to the object in Object Oriented Programming. Browsers use those attributes to render web pages for users.</p>
<h3>2.2 REINFORCEMENT LEARNING</h3>
<p>In the traditional reinforcement learning setting, an agent interacts with an infinite-horizon, discounted Markov Decision Process (MDP) to maximize its total discounted future rewards. An MDP is defined as a tuple $(\mathcal{S}, \mathcal{A}, T, R, \gamma)$ where $\mathcal{S}$ and $\mathcal{A}$ are the state space and the action space respectively, $T\left(s^{\prime} \mid s, a\right)$ is the transition probability of reaching state $s^{\prime} \in \mathcal{S}$ by taking action $a \in \mathcal{A}$ from $s \in \mathcal{S}, R$ is the immediate reward by the transition, and $\gamma$ is a discount factor. The Q-value function for a tuple of actions is defined to be $Q^{\pi}(s, a)=\mathbb{E}\left[\sum_{t=0}^{T} \gamma^{t} r_{t}\right] s_{0}=s, a_{0}=a]$, where T is the number of timesteps till termination. The formula represents the expected future discounted reward starting from state $s$, performing action $a$ and following the policy until termination. The optimal Q-value function $Q^{<em>}(s, a)=\max _{\pi} Q^{\pi}(s, a), \forall s \in \mathcal{S}, a \in \mathcal{A}$ (Sutton \&amp; Barto, 1998) satisfies the Bellman optimality equation $Q^{</em>}(s, a)=\mathbb{E}<em a_prime="a^{\prime">{s^{\prime}}\left[r+\gamma \max </em>\right)\right]$.} \in \mathcal{A}} Q^{*}\left(s^{\prime}, a^{\prime</p>
<h3>2.3 GRAPH NEURAL NETWORKS</h3>
<p>For an undirected graph $G=(V, E)$, the Message Passing Neural Network (MPNN) framework (Gilmer et al., 2017) formulates two phases of the forward pass to update the node-level feature representations $h_{v}$, where $v \in V$, and graph-level feature vector $\hat{y}$. The message passing phase updates hidden states of each node by applying a vertex update function $U_{t}$ over the current hidden state and the message, $h_{v}^{t+1}=U_{t}\left(h_{v}^{t}, m_{v}^{t+1}\right)$, where the passed message $m_{v}^{t+1}$ is computed as $m_{v}^{t+1}=\sum_{\omega \in N(v)} M_{t}\left(h_{v}^{t}, h_{w}^{t}, e_{v w}\right) . N(v)$ denotes the neighbors of $v$ in $G$, and $e_{v w}$ is an edge feature. This process runs for T timesteps. The readout phase uses the readout function R, and computes the graph-level feature vector $\hat{y}=R\left(h_{v}^{T} \mid v \in G\right)$.</p>
<h3>2.4 REINFORCEMENT LEARNING WITH GRAPH NEURAL NETWORKS</h3>
<p>There has been work in robot locomotion that uses graph neural networks (GNNs) to model the physical body (Wang et al., 2018; Hamrick et al., 2018). NerveNet demonstrates that policies learned with GNN transfers better to other learning tasks than policies learned with MLP (Wang et al., 2018). It uses GNNs to parametrize the entire policy whereas DOM-Q-NET uses GNNs to provide representational modules for factorized Q functions. Note that the graph structure of a robot is static whereas the graph structure of a web page can change at each time step. Locomotion-based control tasks provide dense rewards whereas web navigation tasks are sparse reward problems with only 0/1 reward at the end of the episode. For our tasks, the model also needs to account for the dependency of actions on goal instructions.</p>
<p>2.5 Previous Work on RL on Web interfaces</p>
<p>Shi et al. (2017) constructed benchmark tasks, Mini World of Bits (MiniWoB), that consist of many toy tasks of web navigation. This environment provides both the image and HTML of a web page. Their work showed that the agent using the visual input cannot solve most of the tasks, even given the demonstrations. Then Liu et al. (2018) proposed DOM-NET architecture that uses a series of attention between DOM elements and the goal. With their workflow guided-exploration that uses the formal language to constrain the action space of an agent, they achieved state-of-the-art performance and sample efficiency in using demonstrations. Unlike these previous approaches, we aim to tackle web navigation without any expert demonstration or prior knowledge.</p>
<h1>3 NeURal DOM Q Network</h1>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Given the web page on the right, its DOM tree representation is shown as a graph where each DOM represents a node from $V$. Different colors indicate different tag attributes of DOMs. DOMs are embedded as a local module, $\boldsymbol{e}<em _neighbor="{neighbor" _text="\text">{\text {local }}$, and propagated by a GNN to produce a neighbor module, $\boldsymbol{e}</em>}}$. The global module, $\boldsymbol{e<em _dom="{dom" _text="\text">{\text {global }}$, is aggregated from the neighbor module. The $Q</em>$ respectively.}}$ stream uses all three modules whereas $Q_{\text {token }}$ and $Q_{\text {mode }}$ streams only use the global module. Here, Q values of the 'submit' and 'sr' token are computed by $Q_{\text {dom }}$ and $Q_{\text {token }</p>
<p>Consider the problem of navigating through multiple web pages or menus to locate a piece of information. Let $V$ be the set of DOMs in the current web page. There are often multiple goals that can be achieved in the same web environment. We consider goals that are presented to the agent in the form of a natural language sentence, e.g. "Select sr and click Submit" in Figure 1 and "Use the textbox to enter Kanesha and press Search, then find and click the 9th search result" in Figure 2. Let $\mathcal{G}$ represent the set of word tokens in the given goal sentence. The RL agent will only receive a reward if it successfully accomplishes the goal, so it is a sparse reward problem. The primary means of navigation are through interaction with the buttons and the text fields on the web pages.</p>
<p>There are two major challenges in representing the state-action value function for web navigation: the action space is enormous, and the number of actions can vary drastically between the states. We propose DOM-Q-NET to address both of the problems in the following.</p>
<h3>3.1 ACTION SPACE FOR WEB NAVIGATION</h3>
<p>In contrast to typical RL tasks that require choosing only one action $a$ from an action space, $\mathcal{A}$, such as choosing one from all combinations of controller's joint movements for Atari (Mnih et al., 2015), we frame acting on the web with three distinct categories of actions:</p>
<ul>
<li>
<p>DOM selection $a_{\text {dom }}$ chooses a single DOM in the current web page, $a_{\text {dom }} \in V$. The DOM selection covers the typical interactive actions such as clicking buttons or checkboxes as well as choosing which text box to fill in the string input.</p>
</li>
<li>
<p>Word token selection $a_{\text {token }} \in \mathcal{G}$ picks a work token from the given goal sentence to fill in the selected text box. The assumption that typed string comes from the goal instruction aligns with previous work (Liu et al., 2018).</p>
</li>
<li>Mode $a_{\text {mode }} \in{$ click, type $}$ tells the environment whether the agent's intention is to "click" or "type" when acting in the web page. $a_{\text {mode }}$ is represented as a binary action.</li>
</ul>
<p>At each time step, the environment receives a tuple of actions, namely $a=\left(a_{\text {dom }}, a_{\text {token }}, a_{\text {mode }}\right)$, though it does not process $a_{\text {token }}$ unless $a_{\text {mode }}=$ type.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: A successful trajectory executed by our model for search-engine. $S_{i}$ is the state, and $A_{i}=$ $\left(a_{\text {dom }}, a_{\text {token }}, a_{\text {mode }}\right)$ is a tuple of actions for the three distinct categories of actions at timestep i. $\operatorname{DOM}(x)$ represents the index of the corresponding element $x$ in the web page.</p>
<h1>3.2 FACTORIZED Q FUNCTION</h1>
<p>One way to represent the state-action value function is to consider all the permutations of $a_{\text {dom }}$ and $a_{\text {token }}$. For example, Mnih et al. (2015) considers the permutations of joystick direction and button clicking for Atari. For MiniWoB, this introduces an enormous action space with size $|V| \times|\mathcal{G}|$. The number of DOMs and goal tokens, $|V|$ and $|\mathcal{G}|$, can reach up to 60 and 18, and the total number of actions become over 1,000 for some hard tasks.</p>
<p>To reduce the action space, we consider a factorized state-action value function where the action values of $a_{\text {dom }}$ and $a_{\text {token }}$ are independent to each other. Formally, we define the optimal Q-value function as the sum of the individual value functions of the three action categories:</p>
<p>$$
Q^{<em>}(s, a)=Q^{</em>}\left(s, a_{\text {dom }}, a_{\text {token }}, a_{\text {mode }}\right)=Q^{<em>}\left(s, a_{\text {dom }}\right)+Q^{</em>}\left(s, a_{\text {token }}\right)+Q^{*}\left(s, a_{\text {mode }}\right)
$$</p>
<p>Under the independence assumption, we can find the optimal policy by selecting the greedy actions w.r.t. each Q-value function individually. Therefore, the computation cost for the optimal action of the factorized Q function is linear in the number of DOM elements and the number of word tokens rather than quadratic.</p>
<p>$$
a^{<em>}=\left(\underset{a_{\text {dom }}}{\arg \max } Q^{</em>}\left(s, a_{\text {dom }}\right), \underset{a_{\text {token }}}{\arg \max } Q^{<em>}\left(s, a_{\text {token }}\right), \underset{a_{\text {mode }}}{\arg \max } Q^{</em>}\left(s, a_{\text {mode }}\right)\right)
$$</p>
<h3>3.3 LEARNING STATE-ACTION EMBEDDINGS OF WEB PAGES</h3>
<p>Many actions on the web, clicking different checkboxes and filling unseen type of forms, share similar tag or class attributes. Our goal is to design a neural network architecture that effectively captures such invariance for web pages, and yet is flexible to deal with the varying number of DOM elements and goal tokens at different time steps. Furthermore, when locating a piece of information on the web, an agent needs to be aware of both the local information, e.g. the name of button and its surrounding texts, and the global information, e.g. the general theme, of the web page. The cue for clicking a particular button from the menu is likely scattered.</p>
<p>To address the above problem, we propose a GNN-based RL agent that computes the factorized Q-value for each DOM in the current web page, called DOM-Q-NET as shown in Figure 1. It uses additional information of tree-structured HTML to guide the learning of state-action representations, embeddings $\boldsymbol{e}$, which is shared among factorized Q networks. Explicitly modeling the HTML tree structure provides the relational information among the DOM elements to the agent. Given a web page, our model learns a concatenated embedding vector $\boldsymbol{e}^{i}=\left[\boldsymbol{e}<em _neighbor="{neighbor" _text="\text">{\text {local }}^{i}, \boldsymbol{e}</em>\right]$ using the low-level and high-level modules that correspond to node-level and graph-level outputs of the GNN.}}^{i}, \boldsymbol{e}_{\text {global }</p>
<p>Local Module $\boldsymbol{e}<em A="A" r="r" t="t">{\text {local }}^{i}$ is the concatenation of each embedded attribute $\boldsymbol{e}</em>$, in the goal sentence. Liu et al. (2018) uses the exact alignment to obtain tokens that appear in the goal sentence, but our method can detect synonyms that are not exactly matched.}$ of the DOM $v^{i}$, which includes the tag, class, focus, tampered, and text information of the DOM element. In particular, we use the maximum of cosine distance between the text and each goal token to measure the soft alignment of the DOM $v^{i}$ with the $j^{\text {th }}$ word embedding, $\boldsymbol{e}_{\text {goal }}^{j</p>
<p>$$
\boldsymbol{e}<em _Attr="{Attr" _text="\text">{\text {local }}^{i}=\left[\boldsymbol{e}</em>, \max }}^{i<em _Attr="{Attr" _text="\text">{j}\left(\cos \left(\boldsymbol{e}</em>\right)\right)\right]
$$}}^{i}, \boldsymbol{e}_{\text {goal }}^{j</p>
<p>This provides the unpropagated action representation of clicking each DOM, and is the skip connection of GNNs.</p>
<p>Neighbor Module $\boldsymbol{e}<em G="G" N="N">{\text {neighbor }}^{i}$ is the node representation that incorporates the neighbor context of the DOM $v^{i}$ using a graph neural network. The model performs the message passing between the nodes of the tree with the weights $\boldsymbol{w}</em>$ is an intermediate state for each step of the message passing, and we adopt Gated Recurrent Units (Cho et al., 2014) for the nonlinear vertex update (Li et al., 2016). This process is performed for T number of steps to obtain the final neighbor embeddings.}$. The local module is used to initialize this process. $\boldsymbol{m}^{t</p>
<p>$$
\begin{aligned}
\boldsymbol{m}<em N_i_="N(i)" _in="\in" k="k">{\text {neighbor }}^{i, t+1}=\sum</em>} w_{G N N} \boldsymbol{e<em _neighbor="{neighbor" _text="\text">{\text {neighbor }}^{k, t}, &amp; \boldsymbol{e}</em>}}^{i, 0}=\boldsymbol{e<em _neighbor="{neighbor" _text="\text">{\text {local }}^{i} \
\boldsymbol{e}</em>}}^{i, t+1}=G R U\left(\boldsymbol{e<em _neighbor="{neighbor" _text="\text">{\text {neighbor }}^{i, t}, \boldsymbol{m}</em>}}^{i, t+1}\right), &amp; \boldsymbol{e<em _neighbor="{neighbor" _text="\text">{\text {neighbor }}^{i}=\boldsymbol{e}</em>
\end{aligned}
$$}}^{i, T</p>
<p>By incorporating the context information, this module contains the state representation of the current page, and the propagated action representation of clicking the particular DOM, so the Q-value function can be approximated using only this module.</p>
<p>Global Module $\boldsymbol{e}_{\text {global }}$ is the high-level feature representation of the entire web page after the readout phase. It is used by all three factorized Q networks. We investigate two readout functions to obtain such global embedding with and without explicitly incorporating the goal information.</p>
<p>1) We use max-pooling to aggregate all of the DOM embeddings of the web page.</p>
<p>$$
\boldsymbol{e}<em _local="{local" _text="\text">{\text {global }}=\operatorname{maxpool}\left(\left{\left[\boldsymbol{e}</em> \in V\right}\right)
$$}}^{i}, \boldsymbol{e}_{\text {neighbor }}^{i}\right] \mid v^{i</p>
<p>2) We use goal-attention with the goal vector as an attention query. This is in contrast to Velickovic et al. (2018) where the attention is used in the message passing phase, and the query is not a task dependent representation. To have the goal vector $h_{\text {goal }}$, each goal token $e_{\text {token }}$ is concatenated with the one-hot positional encoding vector $e_{\text {pos }}$, as shown in Figure 1. Next, the position-wise feedforward network with ReLU activation is applied to each concatenated vector before max-pooling the goal representation. Motivated by Vaswani et al. (2017), we use scaled dot product attention with local embeddings as keys, and neighbor embeddings as values. Note that $\boldsymbol{E}<em _neighbor="{neighbor" _text="\text">{\text {local }}$ and $\boldsymbol{E}</em>}}$ are packed representations of $\left(\boldsymbol{e<em _local="{local" _text="\text">{\text {local }}^{1}, \ldots, \boldsymbol{e}</em>}}^{V}\right)$ and $\left(\boldsymbol{e<em _neighbor="{neighbor" _text="\text">{\text {neighbor }}^{1}, \ldots, \boldsymbol{e}</em>}}^{V}\right)$ respectively, where $\boldsymbol{E<em k="k">{\text {local }} \in \mathbb{R}^{\left(V, d</em>}\right)}, \boldsymbol{E<em k="k">{\text {neighbor }} \in \mathbb{R}^{\left(V, d</em>$ is the dimension of text token embeddings.}\right)}$, and $d_{k</p>
<p>$$
\boldsymbol{e}<em _goal="{goal" _text="\text">{a t t n}=\operatorname{softmax}\left(\frac{\boldsymbol{h}</em>}} \boldsymbol{E<em k="k">{\text {local }}^{T}}{\sqrt{d</em>}}}\right) \boldsymbol{E<em _global_attn="{global,attn" _text="\text">{\text {neighbor }}, \quad \boldsymbol{e}</em>}}=\left[\boldsymbol{e<em _attn="{attn" _text="\text">{\text {global }}, \boldsymbol{e}</em>\right]
$$}</p>
<p>The illustrative diagram is shown in Appendix 6.2, and a simpler method of concatenating the nodelevel feature with the goal vector is shown in Appendix 6.3. This method is also found to be effective in incorporating the goal information, but the size of the model increases.</p>
<p>Learning The Q-value function of choosing the DOM is parametrized by a two-layer MLP, $Q_{\text {dom }}^{i}=M L P\left(\boldsymbol{e}^{i} ; w_{\text {dom }}\right)$, where it takes the concatenation of DOM embeddings $\boldsymbol{e}^{i}=$ $\left[\boldsymbol{e}<em _neighbor="{neighbor" _text="\text">{\text {local }}^{i}, \boldsymbol{e}</em>}}^{i}, \boldsymbol{e<em _text="\text" _token="{token">{\text {global }}\right]$ as the input. Similarly, the Q-value functions for choosing the word token and the mode are computed using $M L P\left(\boldsymbol{e}</em>}}, \boldsymbol{e<em _text="\text" _token="{token">{\text {global }} ; w</em>}}\right)$ and $M L P\left(\boldsymbol{e<em _mode="{mode" _text="\text">{\text {global }} ; w</em>\right)$ be the model parameters including the embedding matrices, the weights of a graph neural network, and weights of the factorized Q-value function. The model parameters are updated by minimizing the squared TD error (Sutton, 1988):}}\right)$ respectively. See Figure 1. All the model parameters including the embedding matrices are learned from scratch. Let $\theta=\left(E, w_{G N N}, w_{\text {dom }}, w_{\text {token }}, w_{\text {mode }</p>
<p>$$
\min <em _left_s_="\left(s," a_="a," r_="r," s_prime="s^{\prime">{\theta} \mathbb{E}</em>\right]
$$}\right) \sim \text { replay }}\left[\left(y^{D Q N}-Q\left(s, a_{\text {dom }} ; \theta\right)-Q\left(s, a_{\text {token }} ; \theta\right)-Q\left(s, a_{\text {mode }} ; \theta\right)\right)^{2</p>
<p>where the transition pairs $\left(s, a, r, s^{\prime}\right)$ are sampled from the replay buffer and $y^{D Q N}$ is the factorized target Q-value with the target network parameters $\theta^{-}$as in the standard DQN algorithm.</p>
<p>$$
y^{D Q N}=r+\gamma\left(\max <em _dom="{dom" _text="\text">{a</em>\right)+\max }}^{\prime}} Q\left(s^{\prime}, a_{\text {dom }}^{\prime} ; \theta^{-<em _text="\text" _token="{token">{a</em>\right)+\max }}^{\prime}} Q\left(s^{\prime}, a_{\text {token }}^{\prime} ; \theta^{-<em _mode="{mode" _text="\text">{a</em>\right)\right)
$$}}^{\prime}} Q\left(s^{\prime}, a_{\text {mode }}^{\prime} ; \theta^{-</p>
<h1>3.4 Multitask Learning for Transferring Learned Behaviours</h1>
<p>To assess the effectiveness of transferring learned behaviours and solving multiple tasks by our model, we train a single agent acting in multiple environments. Transitions from different tasks are collected in a shared replay buffer, and the network is updated after performing an action in each environment. See Alg. 1 for details.</p>
<h2>4 EXPERIMENTS</h2>
<p>We first evaluate the generalization capability of the proposed model for large action space by comparing it against previous works. Tasks with various difficulties, as defined in Appendix 6.4, are chosen from MiniWoB. Next, we investigate the gain in sample efficiency with our model from multitask learning. We perform an ablation study to justify the effectiveness of each representational module, followed by the comparisons of gains in sample efficiency from goal-attention in multitask and single task settings. Hyperparameters are explained in Appendix 6.1.</p>
<h3>4.1 DOM-Q-NET Benchmark MiniWoB</h3>
<p>We use the Q-learning algorithm, with four components of Rainbow (Hessel et al., 2018), to train our agent because web navigation tasks are sparse reward problems, and an off-policy learning with a replay buffer is more sample-efficient. The four components are DDQN (Van Hasselt et al., 2016), Prioritized replay (Schaul et al., 2016), Multi-step learning (Sutton, 1988), and NoisyNet (Fortunato et al., 2018). To align with the settings used by Liu et al. (2018), we consider the tasks that only require clicking DOM elements and typing strings. The agent receives +1 reward if the task is completed correctly, and 0 reward otherwise. We perform $T=3$ steps of neural message passing for all the tasks except social-media, for which we use $T=7$ steps to address the large DOM space.
Evaluation metric: We plot the moving average of rewards for the last 100 episodes during training. We follow previous works (Shi et al., 2017; Liu et al., 2018), and report the success rate, which is the percentage of test episodes that ends up with reward +1 . Each reported success rate is based on the average of 4 different runs, and Appendix 6.6 explains our experiment protocol.</p>
<p>Results: Figure 3 shows that DOM-Q-NET reaches $100 \%$ success rate for most of the tasks selected by Liu et al. (2018), except for click-widget, social-media, and email-inbox. Our model still reaches $86 \%$ success rate for social-media, and the use of goal-attention enables the model to solve clickwidget and social-media with $100 \%$ success rate. We did not use any prior knowledge such as providing constraints on the action set during exploration, using pre-defined fields of the goal and showing expert demonstrations. Specifically, our model solves a long-horizon task, choose-date, that previous works with demonstrations were unable to solve. This task expects many similar actions, but has a large action space. Even using imitation learning or guided exploration, the neural network needs to learn a representation that generalizes for unseen diverse DOM states and actions, which our model proves to do.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Performance comparisons of DOM-Q-NET with Shi et al. (2017); Liu et al. (2018)
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Multitask Comparisons: 9-multitask DOM-Q-NET with goal-attention consistently has better sample efficiency. Results for other tasks are shown in Appendix 6.7.1. g_a=goal-attention.</p>
<h1>4.2 MULTITASK</h1>
<p>Two metrics are used for comparing the sample efficiency of multitask and single-task agents.</p>
<ul>
<li>$M_{\text {total }}$ multitask agent: total number of frames observed upon solving all the tasks. $M_{\text {total }}$ single-task agents: sum of the number of frames observed for solving each task.</li>
<li>$M_{\text {task }}$ : number of frames observed for solving a specific task.</li>
</ul>
<p>We trained a multitask agent solving 9 tasks with 2 x sample efficiency, using about $M_{\text {total }}=63000$ frames, whereas the single-task agents use $M_{\text {total }}=127000$ frames combined. Figure 4 shows the plots for 6 out of the 9 tasks. In particular, login-user and click-checkboxes are solved with 40000 fewer frames using multitask learning, but such gains are not as obvious when the task is simple, as in the case of navigate-tree. Next we included two hard tasks shown in Figure 5. Compared to the sample efficiency of observing $M_{\text {total }}=477000$ frames for solving 11 tasks by single-task agents, multitask agent has only observed $M_{\text {total }}=29000 \times 11=319000$ frames when the last socialmedia task is solved as shown in Figure 5. Additionally, the plots indicate that multitask learning with simpler tasks is more efficient in using observed frames for hard tasks, achieving better $M_{\text {task }}$ than multitask learning with only those two tasks. These results indicate that our model enables positive transfers of learned behaviours between distinct tasks.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Comparisons in sample efficiency for 2 hard tasks, social-media (left) and search-engine (right), by multitask learning. 9.multitask refers to the tasks discussed in Figure 4
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Ablation experiments for $\mathrm{l}=$ Local, $\mathrm{n}=$ Neighbor, $\mathrm{g}=$ Global modules. dom_q_net - g is the DOM-Q-NET without the global module. dom_q_net -l - g is the DOM-Q-NET with only neighbor module. dom_q_net-n-g is the DOM-Q-NET with only local module.</p>
<h1>4.3 Ablation Study on the DOM Representation Modules</h1>
<p>We perform ablation experiments to justify the effectiveness of using each module for the $Q_{\text {dom }}$ stream. We compare the proposed model against three discounted versions that omit some modules for computing $Q_{\text {dom }}:$ (a) $\boldsymbol{e}<em _local="{local" _text="\text">{\text {dom }}=\boldsymbol{e}</em>}}$, (b) $\boldsymbol{e<em _neighbor="{neighbor" _text="\text">{\text {dom }}=\boldsymbol{e}</em>}}$, (c) $\boldsymbol{e<em _local="{local" _text="\text">{\text {dom }}=\left[\boldsymbol{e}</em>$.
Figure 6 shows the two tasks chosen, and the failure case for click-checkboxes shows that DOM selection without the neighbor module will simply not work because many DOMs have the same attributes, and thus have exactly the same representations despite the difference in the context. Liu et al. (2018) addressed this issue by hand-crafting the message passing. The faster convergence of DOM-Q-NET to the optimal behaviour indicates the limitation of neighbor module and how global and local module provide shortcuts to the high-level and low-level representations of the web page.}}^{T}, \boldsymbol{e}_{\text {neighbor }}^{T}\right]^{T</p>
<h3>4.4 Effectiveness of Goal-Attention</h3>
<p>Most of the MiniWoB tasks have only one desired control policy such as "put a query word in the search, and find the matched link" where the word token for the query and the link have alignments with the DOMs. Hence, our model solves most of the tasks without feeding the goal representation to the network, with exceptions like click-widget. Appendix 6.7 shows comparisons of the model with different goal encoding methods including goal-attention. The effect of goal-attention is not obvious, as seen in some tasks. However, Figure 7 shows that the gain in sample efficiency from</p>
<p>using goal-attention is considerable in multitask learning settings, and this gain is much bigger than the gain in the single-task setting. This indicates that the agent successfully learns to pay attention to different parts of the DOM tree given different goal instructions when solving multiple tasks.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Effects of goal-attention for single and multi-task learning (g_a=goal attention)</p>
<h1>5 DISCUSSION</h1>
<p>We propose a new architecture for parameterizing factorized Q functions using goal-attention, local word embeddings, and a graph neural network. We contribute to the formulation of web navigation with this model. Without any demonstration, it solves relatively hard tasks with large action space, and transfers learned behaviours from multitask learning, which are two important factors for web navigation. For future work, we investigate exploration strategies for tasks like email-inbox where the environment does not have a simple instance of the task that the agent can use to generalize learned behaviours. Liu et al. (2018) demonstrated an interesting way to guide the exploration. Another work is to reduce the computational cost of evaluating the Q value for each DOM element. Finally, we intend on applying our methods to using search engines. Tasks like question answering could benefit from the ability of an agent to query search, navigate the results page and obtain relevant information for solving the desired goal. The ability to query and navigate search could also be used to bootstrap agents in realistic environments to obtain task-oriented knowledge and improve sample efficiency.</p>
<p>Acknowledgement: We acknowledge using the implementation of segment tree by Dopamine (Castro et al., 2018) for this project.</p>
<p>Reproducibility: Our code and demo are available at https://github.com/Sheng-J/DOM-Q-NET and https://www.youtube.com/channel/UCrGsYub9IKCYO8dIREC3dnQ respectively.</p>
<h2>REFERENCES</h2>
<p>Pablo Samuel Castro, Subhodeep Moitra, Carles Gelada, Saurabh Kumar, and Marc G. Bellemare. Dopamine: A research framework for deep reinforcement learning. CoRR, abs/1812.06110, 2018. URL http://arxiv.org/abs/1812.06110.</p>
<p>Kyunghyun Cho, Bart van Merrienboer, Çaglar Gülçehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using RNN encoder-decoder for statistical machine translation. CoRR, abs/1406.1078, 2014. URL http://arxiv.org/abs/1406. 1078 .</p>
<p>Meire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot, Jacob Menick, Ian Osband, Alex Graves, Vlad Mnih, Remi Munos, Demis Hassabis, Olivier Pietquin, et al. Noisy networks for exploration. In $I C L R, 2018$.</p>
<p>Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. In ICML, 2017.</p>
<p>Jessica B Hamrick, Kelsey R Allen, Victor Bapst, Tina Zhu, Kevin R McKee, Joshua B Tenenbaum, and Peter W Battaglia. Relational inductive bias for physical construction in humans and machines. arXiv preprint arXiv:1806.01203, 2018.</p>
<p>Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan Horgan, Bilal Piot, Mohammad Azar, and David Silver. Rainbow: Combining improvements in deep reinforcement learning. In AAAI, 2018.</p>
<p>Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR, abs/1412.6980, 2014. URL http://arxiv.org/abs/1412.6980.</p>
<p>Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. CoRR, abs/1609.02907, 2016. URL http://arxiv.org/abs/1609.02907.</p>
<p>Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep visuomotor policies. In $J M L R, 2016$.</p>
<p>Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. Gated graph sequence neural networks. In $I C L R, 2016$.</p>
<p>Evan Zheran Liu, Kelvin Guu, Panupong Pasupat, Tianlin Shi, and Percy Liang. Reinforcement learning on web interfaces using workflow-guided exploration. In $I C L R, 2018$.</p>
<p>Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. In Nature, 2015.</p>
<p>David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning internal representations by error propagation. Technical report, California Univ San Diego La Jolla Inst for Cognitive Science, 1985.</p>
<p>Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The graph neural network model. In IEEE Transactions on Neural Networks, 2009.</p>
<p>Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. In $I C L R, 2016$.</p>
<p>Tianlin Shi, Andrej Karpathy, Linxi Fan, Jonathan Hernandez, and Percy Liang. World of bits: An open-domain platform for web-based agents. In ICML, 2017.</p>
<p>Richard S Sutton. Learning to predict by the methods of temporal differences. In Machine learning, 1988.</p>
<p>Richard S Sutton and Andrew G Barto. Introduction to reinforcement learning, volume 135. MIT press Cambridge, 1998.</p>
<p>Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double qlearning. In AAAI, 2016.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, 2017.</p>
<p>Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. In $I C L R, 2018$.</p>
<p>Tingwu Wang, Renjie Liao, Jimmy Ba, and Sanja Fidler. Nervenet: Learning structured policy with graph neural networks. In $I C L R, 2018$.</p>
<h1>6 APPENDIX</h1>
<h3>6.1 HYPERPARAMETERS</h3>
<p>Following hyperparameters are used throught all the experiments presented in this paper.
Table 1: Hyperparameters for training with Rainbow DQN (4 components)</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Hyperparameter</th>
<th style="text-align: left;">Value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Optimization algorithm</td>
<td style="text-align: left;">Adam (Kingma \&amp; Ba, 2014)</td>
</tr>
<tr>
<td style="text-align: left;">Learning rate</td>
<td style="text-align: left;">0.00015</td>
</tr>
<tr>
<td style="text-align: left;">Batch Size</td>
<td style="text-align: left;">128</td>
</tr>
<tr>
<td style="text-align: left;">Discounted factor</td>
<td style="text-align: left;">0.99</td>
</tr>
<tr>
<td style="text-align: left;">DQN Target network update period</td>
<td style="text-align: left;">200 online network updates</td>
</tr>
<tr>
<td style="text-align: left;">Number of update per frame</td>
<td style="text-align: left;">1</td>
</tr>
<tr>
<td style="text-align: left;">Number of exploration steps</td>
<td style="text-align: left;">50</td>
</tr>
<tr>
<td style="text-align: left;">N steps (multi-step) bootstrap</td>
<td style="text-align: left;">8</td>
</tr>
<tr>
<td style="text-align: left;">Noisy Nets $\sigma_{0}$</td>
<td style="text-align: left;">0.5</td>
</tr>
<tr>
<td style="text-align: left;">Use DDQN</td>
<td style="text-align: left;">True</td>
</tr>
<tr>
<td style="text-align: left;">Easy Tasks: Number of steps for training</td>
<td style="text-align: left;">5000</td>
</tr>
<tr>
<td style="text-align: left;">Medium Tasks: Number of steps for training</td>
<td style="text-align: left;">50000</td>
</tr>
<tr>
<td style="text-align: left;">Hard Tasks: Number of steps for training</td>
<td style="text-align: left;">2000000</td>
</tr>
</tbody>
</table>
<p>Table 2: Hyperparameters for DOM-Q-NET</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Hyperparameter</th>
<th style="text-align: left;">Value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Vocabulary size: tag</td>
<td style="text-align: left;">80</td>
</tr>
<tr>
<td style="text-align: left;">Vocabulary size: text</td>
<td style="text-align: left;">400</td>
</tr>
<tr>
<td style="text-align: left;">Vocabulary size: class</td>
<td style="text-align: left;">80</td>
</tr>
<tr>
<td style="text-align: left;">Embedding dimension: tag</td>
<td style="text-align: left;">16</td>
</tr>
<tr>
<td style="text-align: left;">Embedding dimension: text</td>
<td style="text-align: left;">32</td>
</tr>
<tr>
<td style="text-align: left;">Embedding dimension: class</td>
<td style="text-align: left;">16</td>
</tr>
<tr>
<td style="text-align: left;">Dimension of Fully Connected(FC) layers</td>
<td style="text-align: left;">128</td>
</tr>
<tr>
<td style="text-align: left;">Number of FC layers for 3 factorized Q networks</td>
<td style="text-align: left;">2 each</td>
</tr>
<tr>
<td style="text-align: left;">Hidden Layer Activation</td>
<td style="text-align: left;">ReLU</td>
</tr>
<tr>
<td style="text-align: left;">Number of steps for neural message passing</td>
<td style="text-align: left;">3 (7 for social media task)</td>
</tr>
<tr>
<td style="text-align: left;">Max number of DOMS</td>
<td style="text-align: left;">160</td>
</tr>
<tr>
<td style="text-align: left;">Max number goal tokens</td>
<td style="text-align: left;">18</td>
</tr>
<tr>
<td style="text-align: left;">Out of Vocabulary Random vector generation</td>
<td style="text-align: left;">Choose-option, Click-Checkboxes</td>
</tr>
</tbody>
</table>
<p>Table 3: Hyperparameters for Replay Buffer</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Hyperparameter</th>
<th style="text-align: left;">Value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">$\alpha$ : prioritization exponent</td>
<td style="text-align: left;">0.5</td>
</tr>
<tr>
<td style="text-align: left;">$\beta$ for computing importance sampling weights</td>
<td style="text-align: left;">0</td>
</tr>
<tr>
<td style="text-align: left;">Single Task Buffer Size</td>
<td style="text-align: left;">15000</td>
</tr>
<tr>
<td style="text-align: left;">Multi Task Buffer Size</td>
<td style="text-align: left;">100000</td>
</tr>
</tbody>
</table>
<h3>6.2 GOAL-ATTENTION OUTPUT MODEL</h3>
<p>Figure 8 shows readout phase of the graph neural network using goal-attention. The graph-level feature vector, $h_{\text {global }}$, is computed by the weighted average of node-level representations processed with T steps of message passing, $\left{h_{1} \ldots . . h_{V}\right}$. The weights, $\left{\alpha_{1} \ldots . . \alpha_{V}\right}$, are computed with goal vector as query and node-level features without message passing as keys. For DOM-Q-NET, we use a scaled dot product attention (Vaswani et al., 2017) with local embeddings as keys and neighbor embeddings as values, as illustrated in 3.3.</p>
<p>We frame our goal-attention on the line of works for GNNs(Scarselli et al., 2009), and enable GNNs to be used as a parametrized state for goal-oriented RL.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: goal-attention</p>
<h1>6.3 GOAL ENCODER</h1>
<p>Three types of goal encoding module for global module are investigated.</p>
<ol>
<li>Goal vector concatenation with node-level features</li>
<li>Goal Attention, as illustrated in 6.2</li>
<li>Both Goal vector concatenation and attention, as shown in Figure9
<img alt="img-8.jpeg" src="img-8.jpeg" /></li>
</ol>
<p>Figure 9: goal-encoder
Benchmark results for multitask and 23 tasks in Appendix 6.7 also compare the performances of using different goal encoding modules.</p>
<h3>6.4 MiniWoB TASKs Difficulties Definition</h3>
<ul>
<li>
<p>Easy Task: Any task solvable under 5000 timesteps by single task DOM-Q-NET {click-dialog, click-test, focus-text, focus-text-2, click-test-2, click-button, click-link, click-button-sequence, click-tab, click-tab-2, Navigate-tree}</p>
</li>
<li>
<p>Medium Task: Any task solvable under 50000 timesteps by single task DOM-Q-NET {enter-text, click-widget, click-option, click-checkboxes, enter-text-dynamic, enterpassword, login-user, email-inbox-delete}</p>
</li>
<li>Hard Task: Any task solvable under 200000 timesteps by single task DOM-Q-NET, or any task for which the agent does not reach $100 \%$ success rate.
{choose-date, search-engine, social-media, email-inbox}</li>
</ul>
<h1>6.5 Multitask Learning</h1>
<div class="codehilite"><pre><span></span><code><span class="nx">Algorithm</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="nx">Multitask</span><span class="w"> </span><span class="nx">Learning</span><span class="w"> </span><span class="nx">with</span><span class="w"> </span><span class="nx">Shared</span><span class="w"> </span><span class="nx">Replay</span><span class="w"> </span><span class="nx">Buffer</span>
<span class="w">    </span><span class="nx">Given</span><span class="p">:</span>
<span class="w">    </span><span class="o">-</span><span class="w"> </span><span class="nx">an</span><span class="w"> </span><span class="nx">off</span><span class="o">-</span><span class="nx">policy</span><span class="w"> </span><span class="nx">RL</span><span class="w"> </span><span class="nx">algorithm</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathbb</span><span class="p">{</span><span class="nx">A</span><span class="p">},</span><span class="w"> </span><span class="err">\</span><span class="nx">quad</span><span class="w"> </span><span class="err">\</span><span class="nx">triangleright</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">e</span><span class="p">.</span><span class="nx">g</span><span class="p">.</span><span class="w"> </span><span class="nx">DQN</span><span class="p">,</span><span class="w"> </span><span class="nx">DDPG</span><span class="p">,</span><span class="w"> </span><span class="nx">NAF</span><span class="p">,</span><span class="w"> </span><span class="nx">SDQN</span>
<span class="w">    </span><span class="o">-</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">set</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">environments</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nx">multiple</span><span class="w"> </span><span class="nx">tasks</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathbb</span><span class="p">{</span><span class="nx">K</span><span class="p">},</span><span class="w"> </span><span class="err">\</span><span class="nx">quad</span><span class="w"> </span><span class="err">\</span><span class="nx">triangleright</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">e</span><span class="p">.</span><span class="nx">g</span><span class="p">.</span><span class="w"> </span><span class="nx">click</span><span class="o">-</span><span class="nx">checkboxes</span><span class="p">,</span><span class="w"> </span><span class="nx">social</span><span class="o">-</span><span class="nx">media</span>
<span class="w">    </span><span class="nx">Initialize</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">shared</span><span class="w"> </span><span class="nx">replay</span><span class="w"> </span><span class="nx">buffer</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">R</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="nx">Initialize</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathbb</span><span class="p">{</span><span class="nx">A</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">by</span><span class="w"> </span><span class="nx">initializing</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">shared</span><span class="w"> </span><span class="nx">network</span><span class="w"> </span><span class="nx">parameters</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">theta</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="nx">Initialize</span><span class="w"> </span><span class="nx">each</span><span class="w"> </span><span class="nx">environment</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">sample</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">s_</span><span class="p">{</span><span class="mi">0</span><span class="p">}</span><span class="o">^</span><span class="p">{(</span><span class="nx">k</span><span class="p">)}</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathrm</span><span class="p">{</span><span class="nx">i</span><span class="p">}=</span><span class="mi">1</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">M</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">do</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="nx">each</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">k</span><span class="w"> </span><span class="err">\</span><span class="k">in</span><span class="w"> </span><span class="err">\</span><span class="nx">mathbb</span><span class="p">{</span><span class="nx">K</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">do</span>
<span class="w">            </span><span class="nx">Sample</span><span class="w"> </span><span class="nx">an</span><span class="w"> </span><span class="nx">action</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">a_</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="o">^</span><span class="p">{(</span><span class="nx">k</span><span class="p">)}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">using</span><span class="w"> </span><span class="nx">behavioral</span><span class="w"> </span><span class="nx">policy</span><span class="w"> </span><span class="nx">from</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathbb</span><span class="p">{</span><span class="nx">A</span><span class="p">}:</span><span class="w"> </span><span class="nx">a_</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="o">^</span><span class="p">{(</span><span class="nx">k</span><span class="p">)}</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="nx">pi</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">s_</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="o">^</span><span class="p">{(</span><span class="nx">k</span><span class="p">)}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">            </span><span class="nx">Execute</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">action</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">a_</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="o">^</span><span class="p">{(</span><span class="nx">k</span><span class="p">)}</span><span class="w"> </span><span class="err">\</span><span class="nx">rightarrow</span><span class="w"> </span><span class="nx">k</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="k">observe</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">reward</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">r_</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="o">^</span><span class="p">{(</span><span class="nx">k</span><span class="p">)}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">new</span><span class="w"> </span><span class="nx">state</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">s_</span><span class="p">{</span><span class="nx">t</span><span class="o">+</span><span class="mi">1</span><span class="p">}</span><span class="o">^</span><span class="p">{(</span><span class="nx">k</span><span class="p">)}</span><span class="err">\</span><span class="p">)</span>
<span class="w">            </span><span class="nx">Store</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">transition</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">s_</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="o">^</span><span class="p">{(</span><span class="nx">k</span><span class="p">)},</span><span class="w"> </span><span class="nx">a_</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="o">^</span><span class="p">{(</span><span class="nx">k</span><span class="p">)},</span><span class="w"> </span><span class="nx">r_</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="o">^</span><span class="p">{(</span><span class="nx">k</span><span class="p">)},</span><span class="w"> </span><span class="nx">s_</span><span class="p">{</span><span class="nx">t</span><span class="o">+</span><span class="mi">1</span><span class="p">}</span><span class="o">^</span><span class="p">{(</span><span class="nx">k</span><span class="p">)}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">R</span><span class="err">\</span><span class="p">)</span>
<span class="w">            </span><span class="nx">Sample</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">minibatch</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">B</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">from</span><span class="w"> </span><span class="nx">R</span><span class="p">,</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">perform</span><span class="w"> </span><span class="nx">one</span><span class="w"> </span><span class="nx">step</span><span class="w"> </span><span class="nx">optimization</span><span class="w"> </span><span class="nx">w</span><span class="p">.</span><span class="nx">r</span><span class="p">.</span><span class="nx">t</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">theta</span><span class="err">\</span><span class="p">)</span>
<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="nx">episode</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nx">k</span><span class="w"> </span><span class="nx">terminated</span><span class="w"> </span><span class="k">then</span>
<span class="w">                </span><span class="nx">reset</span><span class="w"> </span><span class="nx">k</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">sample</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">s_</span><span class="p">{</span><span class="mi">0</span><span class="p">}</span><span class="o">^</span><span class="p">{(</span><span class="nx">k</span><span class="p">)}</span><span class="err">\</span><span class="p">)</span>
</code></pre></div>

<h3>6.6 EXPERIMENT Protocol</h3>
<p>We report the success rate of the 100 test episodes at the end of the training once the agent has converged to its highest performance. The final success rate reported in Figure 3 is based on the average of success rate from 4 different random seeds/runs. In detail, we evaluate the RL agent after training for a fixed number of frames depending on the difficulty of the task, as illustrated in Appendix 6.4. As shown in table 4, the results presented in this paper is based on a total of 536 experiments for the set of hyperparameters in table 1, 2, 3.</p>
<p>Table 4: Experiment statistics</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Number of tasks</th>
<th style="text-align: left;">23</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Number of tasks concurrently running for multitask</td>
<td style="text-align: left;">9</td>
</tr>
<tr>
<td style="text-align: left;">Number of goal encoding modules compared</td>
<td style="text-align: left;">4</td>
</tr>
<tr>
<td style="text-align: left;">$N_{1}=(23+9) * 4=128$</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Number of tasks for ablation study</td>
<td style="text-align: left;">2</td>
</tr>
<tr>
<td style="text-align: left;">Number of discounted models compared for ablation study</td>
<td style="text-align: left;">3</td>
</tr>
<tr>
<td style="text-align: left;">$N_{2}=2 * 3=6$</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Number of experiments for computing the average of a result</td>
<td style="text-align: left;">4</td>
</tr>
<tr>
<td style="text-align: left;">Number of experiments for 11 multitask learning</td>
<td style="text-align: left;">11</td>
</tr>
<tr>
<td style="text-align: left;">$N_{\text {total }}=(128+6+11) * 4=580$</td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<h3>6.7 Benchmark Results</h3>
<p>We present the learning curves of both single and multitask agents that provided the results reported in this paper. Each plot is accompanied with the learning curves of a DOM-Q-NET agent with different goal encoding modules 6.3. X-axis and Y-axis represent the timestep and moving average of last 100 rewards respectively. For medium and hard tasks, we also show the fraction of transitions</p>
<p>with positive/non-zero rewards in the replay buffer, and number of unique positive transitions sampled throughout the training. This is to demonstrate the sparsity of the reward for each task, and investigate whether the actual problem is due to the lack of exploration.
(Note that we are using multistep-bootstrap (Sutton, 1988) so some transitions that do not directly lead to the rewards are still analyzed as positive here)</p>
<h1>6.7.1 Multitask (9 TASKs) ReSults</h1>
<p>The following shows the results for 9 tasks used in Multitasking with different goal encoder modules.
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<h3>6.7.2 Some Easy and Medium Tasks</h3>
<p>The plots for very simple tasks with less than 1000 steps are omitted.
<img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p><img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<h1>6.7.3 MEDIUM TASKS WITH REPLAY BUFFER INFORMATION</h1>
<p>Benchmark DOM-Q-NET performance versus DOM-Q-NET + different goal encoding modules for global modules.</p>
<p>The plots on the left show average moving reward of last 100 episodes.
The plots on the center show fraction of postive transitions in replay buffer.
The plots on the right show unique number of postive transitions sampled at each sampling.
<img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p><img alt="img-13.jpeg" src="img-13.jpeg" /></p>
<h1>6.7.4 Hard TASKs</h1>
<p>The plots on the left show average moving reward of last 100 episodes.
The plots on the center show fraction of postive transitions in replay buffer.
The plots on the right show unique number of postive transitions sampled at each sampling.
<img alt="img-14.jpeg" src="img-14.jpeg" /></p>
<p><img alt="img-15.jpeg" src="img-15.jpeg" /></p>
<p>Publishing as a conference paper at ICLR 2019</p>            </div>
        </div>

    </div>
</body>
</html>
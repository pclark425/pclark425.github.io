<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2772 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2772</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2772</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-70.html">extraction-schema-70</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <p><strong>Paper ID:</strong> paper-232352537</p>
                <p><strong>Paper Title:</strong> <a href="https://www.aclweb.org/anthology/2021.naacl-main.247.pdf" target="_blank">Reading and Acting while Blindfolded: The Need for Semantics in Text Game Agents</a></p>
                <p><strong>Paper Abstract:</strong> Text-based games simulate worlds and interact with players using natural language. Recent work has used them as a testbed for autonomous language-understanding agents, with the motivation being that understanding the meanings of words or semantics is a key component of how humans understand, reason, and act in these worlds. However, it remains unclear to what extent artificial agents utilize semantic understanding of the text. To this end, we perform experiments to systematically reduce the amount of semantic information available to a learning agent. Surprisingly, we find that an agent is capable of achieving high scores even in the complete absence of language semantics, indicating that the currently popular experimental setup and models may be poorly designed to understand and leverage game texts. To remedy this deficiency, we propose an inverse dynamics decoder to regularize the representation space and encourage exploration, which shows improved performance on several games including Zork I. We discuss the implications of our findings for designing future agents with stronger semantic understanding.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2772.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2772.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DRRN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deep Reinforcement Relevance Network</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A value-based RL agent that encodes observations and candidate actions with separate GRU encoders and computes Q-values via an MLP; trained with TD loss and uses an experience replay buffer and a valid-action handicap for tractable action selection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>DRRN</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Q-network Q_phi(o,a) where observation and each action candidate are encoded by two separate GRU encoders (f_o, f_a) and concatenated to an MLP decoder g to produce Q-values; uses softmax over Q-values for action selection and prioritized experience replay for training.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>Jericho (12 interactive fiction games including ZORK I, ZORK III, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>Human-written interactive fiction (text-adventure) games with rich, diverse language-based observations and changing valid action sets; tasks require exploration, object manipulation, and long-horizon planning to maximize game score.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>experience replay buffer (external replay memory) and implicit memorization of Q-values mapping (obs,action) -> value</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>Prioritized replay buffer storing transition tuples (o, a, r, o'); implicit Q-value mapping acts like a learned table over encoded (observation, action) pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>Stored transitions: (observation text, action text, reward, next observation text); implicitly the Q-network stores learned Q-values for observation-action encodings.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td>Samples from prioritized experience replay (relevance/prioritization-based sampling) to compute TD updates.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>Transitions appended to the replay buffer during gameplay and used to update network parameters via TD loss; Q-values updated by gradient descent on TD targets.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>Stabilize learning via replay, allow memorization of high Q-value observation-action pairs so the learned policy can repeatedly choose high-reward trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Average normalized final score across 12 games: 0.21 (final) and 0.38 (maximum observed). Example per-game: ZORK I final score 39.4, max 53; PENTARI final 26.5, max 45.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>Experience replay plus the learned Q-network enables memorization of observation-action value mappings; however, learned GRU encoders can overfit to TD loss leading to degenerate representations that do not generalize to unseen states, reducing transfer performance.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Representations can degenerate/overfit (separating seen from unseen states), leading to poor transfer to new games and vulnerability to stochasticity; replay-based memorization does not guarantee semantic generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td>Among the representation strategies compared in this paper, fixed random (HASH) representations produced better average memorization performance, while INV-DY (semantic regularization) produced better exploration and transfer on some games.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2772.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2772.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HASH</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hashing representation variant of DRRN</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A DRRN variant that replaces learned GRU encoders with a fixed hash->random vector encoder for observations and actions, breaking semantic continuity so similar texts map to unrelated vectors; enables treating texts as unique identifiers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>HASH (hashed-text DRRN)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Same DRRN Q-network, but both observation and action encoders are fixed functions: h: L -> int (Python hash over token IDs) followed by G(seed) -> random Gaussian vector; these encoders are not trained and act as stable random identifiers.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>Jericho (12 interactive fiction games)</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>See DRRN entry; same set of interactive fiction games used for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Fixed identifier representations combined with experience replay; functions as a stable identifier-based memorization mechanism rather than learned semantic memory.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>A deterministic mapping from text string to fixed random vector (hash -> pseudo-random vector) plus the prioritized replay buffer storing transitions and learned Q-values over these fixed encodings.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>Fixed vectors representing each distinct observation/action string; replay buffer stores (hashed-obs, hashed-action, reward, hashed-next-obs) transitions and corresponding Q-value updates.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td>Same prioritized replay sampling for training; at inference the agent uses direct lookup via fixed encodings to compute Q-values.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>Fixed text->vector encodings are not updated; only Q-network parameters are updated using transitions sampled from replay.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>Primarily to memorize Q-values for each uniquely identified observation-action pair; fixed stable encodings simplify memorization and Q-learning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Average normalized final score across 12 games: 0.25 (final) and 0.39 (maximum observed); per-game examples: PENTARI final 51.9 (max 60), LIBRARY final 17 (max 21). Generally matches or outperforms DRRN on most games except a few (e.g., ZORK I final 35.5 vs DRRN 39.4).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>Breaking semantics and using fixed random encodings still allowed strong performance, indicating agents can succeed by memorizing identifiers rather than leveraging language semantics; fixed, stable representations can make Q-learning easier and improve memorization.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Poor semantic generalization and limited transfer: HASH transfer to a new but similar game is equivalent to training from scratch (representations are not learned); hashed representations are scattered (no semantic grouping) and do not extrapolate to unseen semantically-similar states.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td>HASH produced the best average normalized final score across the tested games (best memorization), but performed worse on transfer and on some games where semantic generalization (INV-DY) proved beneficial.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2772.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2772.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>INV-DY</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Inverse Dynamics regularized DRRN (Inverse Dynamics Decoder)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A DRRN extension that adds an inverse dynamics auxiliary task: predict the action representation from current and next observation encodings and decode actions back to text, with reconstruction losses and an intrinsic curiosity-like reward to encourage semantic, action-relevant representations and exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>INV-DY (DRRN + Inverse Dynamics Decoder)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>DRRN augmented with an MLP g_inv that predicts an action representation from concat(f_o(o), f_o(o')) and a GRU decoder d that reconstructs action text, plus an action reconstruction loss from f_a(a). The total loss is TD loss + λ1 * L_inv + λ2 * L_dec. An intrinsic reward r+ = L_inv is also used to drive exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>Jericho (12 interactive fiction games)</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>See DRRN entry; same interactive fiction benchmark used to evaluate influence of semantic regularization on gameplay and transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Implicit semantic memory encoded in learned observation/action embeddings (representation-based memory) plus standard experience replay; not an explicit episodic memory module.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>Learned vector embeddings for observations and actions (GRU encodings) that are regularized by inverse dynamics decoding; transitions are kept in a prioritized replay buffer.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>Learned semantic features of observations and actions (encoded vectors) that aim to capture action-relevant information; replay buffer of transitions used for training.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td>Training uses prioritized replay sampling; at inference the agent uses learned embeddings (f_o,f_a) to compute Q-values—no separate retrieval mechanism beyond forward encoding.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>Embeddings and decoder parameters updated continuously during RL via joint optimization of TD and inverse-dynamics/reconstruction losses; intrinsic reward encourages exploration where inverse dynamics is not yet learned.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>Encourage observation encodings to capture action-relevant semantics, improve exploration via intrinsic reward, and promote representations that generalize to unseen states (support transfer across games).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Average normalized final score across 12 games: 0.23 (final) and 0.40 (maximum observed). Notably, INV-DY discovered higher maximum scores on some games: ZORK I max observed 87 (per-run averages 54, 94, 113 across runs) where other models did not exceed 55; improved transfer performance when pretraining on ZORK I and fine-tuning on ZORK III (achieved ~1.0 score after 10k steps vs 0.4 best from-scratch).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>Regularizing encodings with inverse dynamics led to representations that extrapolate to unseen semantically-similar states and improved exploration and transfer in some games; semantic regularization helps when generalization beyond memorized mappings is required.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>INV-DY did not uniformly outperform memorization-focused HASH across all games; benefits are game-dependent (helpful for exploration/transfer-heavy tasks but not always best for final memorized performance). The paper also notes overall RL setup and valid-action handicap limit broader semantic learning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td>INV-DY was the best configuration for exploration and transfer (notably in ZORK I and transfer to ZORK III) because semantic regularization allowed extrapolation to unseen states; however HASH was superior for average memorization across the tested games.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2772.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2772.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MIN-OB</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MIN-OB (Minimizing Observation) variant</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A DRRN variant that minimizes observation input to only the location phrase (loc(o)), removing most observation semantic detail to isolate action semantics and test the role of observation information.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MIN-OB</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>DRRN where the observation encoder receives only the location name (loc(o)) instead of full textual observation; otherwise training and architecture remain the same.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>Jericho (subset of games tested)</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>Same interactive fiction games as DRRN; MIN-OB probes the effect of minimal observation signals on agent performance.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Experience replay buffer for transitions; no additional memory architecture.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>Prioritized replay buffer storing reduced transitions (location-only observations, actions, rewards, next location).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>Location phrases, actions, rewards, next locations; reduced observation content compared to full text.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td>Prioritized sampling from replay for TD updates.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>Replay buffer updated with reduced transitions during gameplay; Q-network updated via TD loss.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>By reducing observation content, the variant tests reliance on action semantics and whether agents can memorize value mappings with minimal observation details.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>MIN-OB achieved similar maximum scores to DRRN on many games but had lower final scores (did not reliably memorize high-scoring experiences); example average normalized final score 0.12 and max 0.35 across games (lower than DRRN).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>MIN-OB's poorer final episodic scores indicate that having richer observation identifiers (text details) helps the agent identify distinct states and memorize good experiences; reducing observations to location only harms memorization/stability.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Insufficient observation detail prevents the agent from distinguishing states needed to store/use useful value memories, leading to failure to reach high episodic scores despite exploring similar maxima.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Learning dynamic knowledge graphs to generalize on text-based games. <em>(Rating: 2)</em></li>
                <li>Graph constrained reinforcement learning for natural language action spaces. <em>(Rating: 2)</em></li>
                <li>Nail: A general interactive fiction agent. <em>(Rating: 2)</em></li>
                <li>Keep CALM and explore: Language models for action generation in text-based games. <em>(Rating: 1)</em></li>
                <li>How to avoid being eaten by a grue: Exploration strategies for text-adventure agents. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2772",
    "paper_id": "paper-232352537",
    "extraction_schema_id": "extraction-schema-70",
    "extracted_data": [
        {
            "name_short": "DRRN",
            "name_full": "Deep Reinforcement Relevance Network",
            "brief_description": "A value-based RL agent that encodes observations and candidate actions with separate GRU encoders and computes Q-values via an MLP; trained with TD loss and uses an experience replay buffer and a valid-action handicap for tractable action selection.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "DRRN",
            "agent_description": "Q-network Q_phi(o,a) where observation and each action candidate are encoded by two separate GRU encoders (f_o, f_a) and concatenated to an MLP decoder g to produce Q-values; uses softmax over Q-values for action selection and prioritized experience replay for training.",
            "base_model_size": null,
            "game_benchmark_name": "Jericho (12 interactive fiction games including ZORK I, ZORK III, etc.)",
            "game_description": "Human-written interactive fiction (text-adventure) games with rich, diverse language-based observations and changing valid action sets; tasks require exploration, object manipulation, and long-horizon planning to maximize game score.",
            "uses_memory": true,
            "memory_type": "experience replay buffer (external replay memory) and implicit memorization of Q-values mapping (obs,action) -&gt; value",
            "memory_structure": "Prioritized replay buffer storing transition tuples (o, a, r, o'); implicit Q-value mapping acts like a learned table over encoded (observation, action) pairs.",
            "memory_content": "Stored transitions: (observation text, action text, reward, next observation text); implicitly the Q-network stores learned Q-values for observation-action encodings.",
            "memory_capacity": null,
            "memory_retrieval_strategy": "Samples from prioritized experience replay (relevance/prioritization-based sampling) to compute TD updates.",
            "memory_update_strategy": "Transitions appended to the replay buffer during gameplay and used to update network parameters via TD loss; Q-values updated by gradient descent on TD targets.",
            "memory_usage_purpose": "Stabilize learning via replay, allow memorization of high Q-value observation-action pairs so the learned policy can repeatedly choose high-reward trajectories.",
            "performance_with_memory": "Average normalized final score across 12 games: 0.21 (final) and 0.38 (maximum observed). Example per-game: ZORK I final score 39.4, max 53; PENTARI final 26.5, max 45.",
            "performance_without_memory": null,
            "has_memory_ablation": false,
            "memory_effectiveness_findings": "Experience replay plus the learned Q-network enables memorization of observation-action value mappings; however, learned GRU encoders can overfit to TD loss leading to degenerate representations that do not generalize to unseen states, reducing transfer performance.",
            "memory_limitations": "Representations can degenerate/overfit (separating seen from unseen states), leading to poor transfer to new games and vulnerability to stochasticity; replay-based memorization does not guarantee semantic generalization.",
            "comparison_with_other_memory_types": true,
            "best_memory_configuration": "Among the representation strategies compared in this paper, fixed random (HASH) representations produced better average memorization performance, while INV-DY (semantic regularization) produced better exploration and transfer on some games.",
            "uuid": "e2772.0"
        },
        {
            "name_short": "HASH",
            "name_full": "Hashing representation variant of DRRN",
            "brief_description": "A DRRN variant that replaces learned GRU encoders with a fixed hash-&gt;random vector encoder for observations and actions, breaking semantic continuity so similar texts map to unrelated vectors; enables treating texts as unique identifiers.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "HASH (hashed-text DRRN)",
            "agent_description": "Same DRRN Q-network, but both observation and action encoders are fixed functions: h: L -&gt; int (Python hash over token IDs) followed by G(seed) -&gt; random Gaussian vector; these encoders are not trained and act as stable random identifiers.",
            "base_model_size": null,
            "game_benchmark_name": "Jericho (12 interactive fiction games)",
            "game_description": "See DRRN entry; same set of interactive fiction games used for evaluation.",
            "uses_memory": true,
            "memory_type": "Fixed identifier representations combined with experience replay; functions as a stable identifier-based memorization mechanism rather than learned semantic memory.",
            "memory_structure": "A deterministic mapping from text string to fixed random vector (hash -&gt; pseudo-random vector) plus the prioritized replay buffer storing transitions and learned Q-values over these fixed encodings.",
            "memory_content": "Fixed vectors representing each distinct observation/action string; replay buffer stores (hashed-obs, hashed-action, reward, hashed-next-obs) transitions and corresponding Q-value updates.",
            "memory_capacity": null,
            "memory_retrieval_strategy": "Same prioritized replay sampling for training; at inference the agent uses direct lookup via fixed encodings to compute Q-values.",
            "memory_update_strategy": "Fixed text-&gt;vector encodings are not updated; only Q-network parameters are updated using transitions sampled from replay.",
            "memory_usage_purpose": "Primarily to memorize Q-values for each uniquely identified observation-action pair; fixed stable encodings simplify memorization and Q-learning.",
            "performance_with_memory": "Average normalized final score across 12 games: 0.25 (final) and 0.39 (maximum observed); per-game examples: PENTARI final 51.9 (max 60), LIBRARY final 17 (max 21). Generally matches or outperforms DRRN on most games except a few (e.g., ZORK I final 35.5 vs DRRN 39.4).",
            "performance_without_memory": null,
            "has_memory_ablation": false,
            "memory_effectiveness_findings": "Breaking semantics and using fixed random encodings still allowed strong performance, indicating agents can succeed by memorizing identifiers rather than leveraging language semantics; fixed, stable representations can make Q-learning easier and improve memorization.",
            "memory_limitations": "Poor semantic generalization and limited transfer: HASH transfer to a new but similar game is equivalent to training from scratch (representations are not learned); hashed representations are scattered (no semantic grouping) and do not extrapolate to unseen semantically-similar states.",
            "comparison_with_other_memory_types": true,
            "best_memory_configuration": "HASH produced the best average normalized final score across the tested games (best memorization), but performed worse on transfer and on some games where semantic generalization (INV-DY) proved beneficial.",
            "uuid": "e2772.1"
        },
        {
            "name_short": "INV-DY",
            "name_full": "Inverse Dynamics regularized DRRN (Inverse Dynamics Decoder)",
            "brief_description": "A DRRN extension that adds an inverse dynamics auxiliary task: predict the action representation from current and next observation encodings and decode actions back to text, with reconstruction losses and an intrinsic curiosity-like reward to encourage semantic, action-relevant representations and exploration.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "INV-DY (DRRN + Inverse Dynamics Decoder)",
            "agent_description": "DRRN augmented with an MLP g_inv that predicts an action representation from concat(f_o(o), f_o(o')) and a GRU decoder d that reconstructs action text, plus an action reconstruction loss from f_a(a). The total loss is TD loss + λ1 * L_inv + λ2 * L_dec. An intrinsic reward r+ = L_inv is also used to drive exploration.",
            "base_model_size": null,
            "game_benchmark_name": "Jericho (12 interactive fiction games)",
            "game_description": "See DRRN entry; same interactive fiction benchmark used to evaluate influence of semantic regularization on gameplay and transfer.",
            "uses_memory": true,
            "memory_type": "Implicit semantic memory encoded in learned observation/action embeddings (representation-based memory) plus standard experience replay; not an explicit episodic memory module.",
            "memory_structure": "Learned vector embeddings for observations and actions (GRU encodings) that are regularized by inverse dynamics decoding; transitions are kept in a prioritized replay buffer.",
            "memory_content": "Learned semantic features of observations and actions (encoded vectors) that aim to capture action-relevant information; replay buffer of transitions used for training.",
            "memory_capacity": null,
            "memory_retrieval_strategy": "Training uses prioritized replay sampling; at inference the agent uses learned embeddings (f_o,f_a) to compute Q-values—no separate retrieval mechanism beyond forward encoding.",
            "memory_update_strategy": "Embeddings and decoder parameters updated continuously during RL via joint optimization of TD and inverse-dynamics/reconstruction losses; intrinsic reward encourages exploration where inverse dynamics is not yet learned.",
            "memory_usage_purpose": "Encourage observation encodings to capture action-relevant semantics, improve exploration via intrinsic reward, and promote representations that generalize to unseen states (support transfer across games).",
            "performance_with_memory": "Average normalized final score across 12 games: 0.23 (final) and 0.40 (maximum observed). Notably, INV-DY discovered higher maximum scores on some games: ZORK I max observed 87 (per-run averages 54, 94, 113 across runs) where other models did not exceed 55; improved transfer performance when pretraining on ZORK I and fine-tuning on ZORK III (achieved ~1.0 score after 10k steps vs 0.4 best from-scratch).",
            "performance_without_memory": null,
            "has_memory_ablation": false,
            "memory_effectiveness_findings": "Regularizing encodings with inverse dynamics led to representations that extrapolate to unseen semantically-similar states and improved exploration and transfer in some games; semantic regularization helps when generalization beyond memorized mappings is required.",
            "memory_limitations": "INV-DY did not uniformly outperform memorization-focused HASH across all games; benefits are game-dependent (helpful for exploration/transfer-heavy tasks but not always best for final memorized performance). The paper also notes overall RL setup and valid-action handicap limit broader semantic learning.",
            "comparison_with_other_memory_types": true,
            "best_memory_configuration": "INV-DY was the best configuration for exploration and transfer (notably in ZORK I and transfer to ZORK III) because semantic regularization allowed extrapolation to unseen states; however HASH was superior for average memorization across the tested games.",
            "uuid": "e2772.2"
        },
        {
            "name_short": "MIN-OB",
            "name_full": "MIN-OB (Minimizing Observation) variant",
            "brief_description": "A DRRN variant that minimizes observation input to only the location phrase (loc(o)), removing most observation semantic detail to isolate action semantics and test the role of observation information.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "MIN-OB",
            "agent_description": "DRRN where the observation encoder receives only the location name (loc(o)) instead of full textual observation; otherwise training and architecture remain the same.",
            "base_model_size": null,
            "game_benchmark_name": "Jericho (subset of games tested)",
            "game_description": "Same interactive fiction games as DRRN; MIN-OB probes the effect of minimal observation signals on agent performance.",
            "uses_memory": true,
            "memory_type": "Experience replay buffer for transitions; no additional memory architecture.",
            "memory_structure": "Prioritized replay buffer storing reduced transitions (location-only observations, actions, rewards, next location).",
            "memory_content": "Location phrases, actions, rewards, next locations; reduced observation content compared to full text.",
            "memory_capacity": null,
            "memory_retrieval_strategy": "Prioritized sampling from replay for TD updates.",
            "memory_update_strategy": "Replay buffer updated with reduced transitions during gameplay; Q-network updated via TD loss.",
            "memory_usage_purpose": "By reducing observation content, the variant tests reliance on action semantics and whether agents can memorize value mappings with minimal observation details.",
            "performance_with_memory": "MIN-OB achieved similar maximum scores to DRRN on many games but had lower final scores (did not reliably memorize high-scoring experiences); example average normalized final score 0.12 and max 0.35 across games (lower than DRRN).",
            "performance_without_memory": null,
            "has_memory_ablation": false,
            "memory_effectiveness_findings": "MIN-OB's poorer final episodic scores indicate that having richer observation identifiers (text details) helps the agent identify distinct states and memorize good experiences; reducing observations to location only harms memorization/stability.",
            "memory_limitations": "Insufficient observation detail prevents the agent from distinguishing states needed to store/use useful value memories, leading to failure to reach high episodic scores despite exploring similar maxima.",
            "comparison_with_other_memory_types": true,
            "best_memory_configuration": null,
            "uuid": "e2772.3"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Learning dynamic knowledge graphs to generalize on text-based games.",
            "rating": 2
        },
        {
            "paper_title": "Graph constrained reinforcement learning for natural language action spaces.",
            "rating": 2
        },
        {
            "paper_title": "Nail: A general interactive fiction agent.",
            "rating": 2
        },
        {
            "paper_title": "Keep CALM and explore: Language models for action generation in text-based games.",
            "rating": 1
        },
        {
            "paper_title": "How to avoid being eaten by a grue: Exploration strategies for text-adventure agents.",
            "rating": 1
        }
    ],
    "cost": 0.01354425,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Reading and Acting while Blindfolded: The Need for Semantics in Text Game Agents
June 6-11, 2021</p>
<p>Shunyu Yao shunyuy@princeton.edu 
Princeton University ‡ Microsoft Research</p>
<p>Karthik Narasimhan karthikn@princeton.edu 
Princeton University ‡ Microsoft Research</p>
<p>Matthew Hausknecht matthew.hausknecht@microsoft.com 
Princeton University ‡ Microsoft Research</p>
<p>Reading and Acting while Blindfolded: The Need for Semantics in Text Game Agents</p>
<p>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies
the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesJune 6-11, 20213097
Text-based games simulate worlds and interact with players using natural language. Recent work has used them as a testbed for autonomous language-understanding agents, with the motivation being that understanding the meanings of words or semantics is a key component of how humans understand, reason, and act in these worlds. However, it remains unclear to what extent artificial agents utilize semantic understanding of the text. To this end, we perform experiments to systematically reduce the amount of semantic information available to a learning agent. Surprisingly, we find that an agent is capable of achieving high scores even in the complete absence of language semantics, indicating that the currently popular experimental setup and models may be poorly designed to understand and leverage game texts. To remedy this deficiency, we propose an inverse dynamics decoder to regularize the representation space and encourage exploration, which shows improved performance on several games including ZORK I. We discuss the implications of our findings for designing future agents with stronger semantic understanding.</p>
<p>Introduction</p>
<p>Text adventure games such as ZORK I (Figure 1 (a)) have been a testbed for developing autonomous agents that operate using natural language. Since interactions in these games (input observations, action commands) are through text, the ability to understand and use language is deemed necessary and critical to progress through such games. Previous work has deployed a spectrum of methods for language processing in this domain, including word vectors (Fulda et al., 2017), recurrent neural networks (Narasimhan et al., 2015;, pre-trained language models (Yao * Work partly done during internship at Microsoft Research. Project page: https://blindfolded.cs. princeton.edu. et al., 2020), open-domain question answering systems , knowledge graphs Adhikari et al., 2020), and reading comprehension systems .</p>
<p>Meanwhile, most of these models operate under the reinforcement learning (RL) framework, where the agent explores the same environment in repeated episodes, learning a value function or policy to maximize game score. From this perspective, text games are just special instances of a partially observable Markov decision process (POMDP) (S, T, A, O, R, γ), where players issue text actions a ∈ A, receive text observations o ∈ O and scalar rewards r = R(s, a), and the underlying game state s ∈ S is updated by transition s = T (s, a).</p>
<p>However, what distinguishes these games from other POMDPs is the fact that the actions and observations are in language space L. Therefore, a certain level of decipherable semantics is attached to text observations o ∈ O ⊂ L and actions a ∈ A ⊂ L. Ideally, these texts not only serve as observation or action identifiers, but also provide clues about the latent transition function T and reward function R. For example, issuing an action "jump" based on an observation "on the cliff" would likely yield a subsequent observation such as "you are killed" along with a negative reward. Human players often rely on their understanding of language semantics to inform their choices, even on games they have never played before, while replacing texts with non-semantic identifiers such as their corresponding hashes (Figure 1 (c)) would likely render games unplayable for people. However, would this type of transformation affect current RL agents for such games? In this paper, we ask the following question: To what extent do current reinforcement learning agents leverage semantics in text-based games?</p>
<p>To shed light on this question, we investi-(a) ZORK I Observation 21: You are in the living room. There is a doorway to the east, a wooden door with strange gothic lettering to the west, which appears to be nailed shut, a trophy case, and a large oriental rug in the center of the room. You are carrying: A brass lantern . . .</p>
<p>Action 21: move rug</p>
<p>Observation 22: With a great effort, the rug is moved to one side of the room, revealing the dusty cover of a closed trap door... Living room... You are carrying: ... gate the Deep Reinforcement Relevance Network (DRRN) (He et al., 2016), a top-performing RL model that uses gated recurrent units (GRU) (Cho et al., 2014) to encode texts. We conduct three experiments on a set of interactive fiction games from the Jericho benchmark  to probe the effect of different semantic representations on the functioning of DRRN. These include (1) using just a location phrase as the input observation (Figure 1 (b)), (2) hashing text observations and actions (Figure 1 (c)), and (3) regularizing vector representations using an auxiliary inverse dynamics loss. While reducing observations to location phrases leads to decreased scores and enforcing inverse dynamics decoding leads to increased scores on some games, hashing texts to break semantics surprisingly matches or even outperforms the baseline DRRN on almost all games considered. This implies current RL agents for textbased games might not be sufficiently leveraging the semantic structure of game texts to learn good policies, and points to the need for developing better experiment setups and agents that have a finer grasp of natural language.</p>
<p>2 Models DRRN Baseline Our baseline RL agent DRRN (He et al., 2016) learns a Q-network Q φ (o, a) parametrized by φ. The model encodes the observation o and each action candidate a using two separate GRU encoders f o and f a , and then aggregates the representations to derive the Q-value through a MLP decoder g:
Q φ (o, a) = g(concat(f o (o), f a (a)))(1)
For learning φ, tuples (o, a, r, o ) of observation, action, reward and the next observation are sampled from an experience replay buffer and the following temporal difference (TD) loss is minimized:
L TD (φ) = (r + γ max a ∈A Q φ (o , a ) − Q φ (o, a)) 2
(2) During gameplay, a softmax exploration policy is used to sample an action:
π φ (a|o) = exp(Q φ (o, a)) a ∈A exp(Q φ (o, a ))(3)
Note that when the action space A is large, (2) and (3) become intractable. A valid action handicap  or a language model (Yao et al., 2020) can be used to generate a reduced action space for efficient exploration. For all the modifications below, we use the DRRN with the valid action handicap as our base model.</p>
<p>Reducing Semantics via Minimizing Observation (MIN-OB)</p>
<p>Unlike other RL domains such as video games or robotics control, at each step of text games the (valid) action space is constantly changing, and it reveals useful information about the current state. For example, knowing "unlock box" is valid leaks the existence of a locked box. Also, sometimes action semantics indicate its value even unconditional on the state, e.g. "pick gold" usually seems good. Given these, we minimize the observation to only a location phrase o → loc(o) (Figure 1 (b)) to isolate the action semantics: given a hash function from strings to integers h : L → Z, and a pseudo-random generator G : Z → R d that turns an integer seed to a random Gaussian vector, a hashing encoder (1) are trainable,f is fixed throughout RL, and ensures two texts that only differ by a word would have completely different representations. In this sense, hashing breaks semantics and only serves to identify different observations and actions in an abstract MDP problem (Figure 1 (c)):
Q loc φ (o, a) = g(f o (loc(o)), f a (a))f = G • h : L → R d can be composed. While f o and f a inQ hash φ (o, a) = g(f (o),f (a))
.</p>
<p>Regularizing Semantics via Inverse Dynamics</p>
<p>Decoding (INV-DY) The GRU representations in DRRN f o (o), f a (a) are only optimized for the TD loss (2). As a result, text semantics can degenerate during encoding, and the text representation might arbitrarily overfit to the Q-values. To regularize and encourage more game-related semantics to be encoded, we take inspiration from Pathak et al. (2017) and propose an inverse dynamics auxiliary task during RL. Given representations of current and next observations f o (o), f o (o ), we use a MLP g inv to predict the action representation, and a GRU decoder d to decode the action back to text * . The inverse dynamics loss is defined as
L inv (φ, θ) = − log p d (a|g inv (concat(f o (o), f o (o )))
where θ denote weights of g inv and d, and p d (a|x) is the probability of decoding token sequence a using GRU decoder d with initial hidden state as x. To also regularize the action encoding, action reconstruction from f a is also used as a loss term:</p>
<p>L dec (φ, θ) = − log p d (a|f a (a)) * Directly defining an L1/L2 loss between fa(a) and ginv(concat(fo(o), fo(o ))) in the representation space will collapse text representations together. And during experience replay, these two losses are optimized along with the TD loss:
L(φ, θ) = L TD (φ) + λ 1 L inv (φ, θ) + λ 2 L dec (φ, θ)
An intrinsic reward r + = L inv (φ, θ) is also used to explore toward where the inverse dynamics is not learned well yet. All in all, the aim of INV-DY is threefold: (1) regularize both action and observation representations to avoid degeneration by decoding back to the textual domain, (2) encourage f o to encode action-relevant parts of observations, and (3) provide intrinsic motivation for exploration.</p>
<p>Results</p>
<p>Setup We train on 12 games † from the Jericho benchmark . These human-written interactive fictions are rich, complex, and diverse in semantics. ‡ For each game, we train DRRN asynchronously on 8 parallel instances of the game environment for 10 5 steps, using a prioritized replay buffer. Following prior practice , we augment observations with location and inventory descriptions by issuing the 'look' and 'inventory' commands. We train three independent runs for each game and report their average score. For HASH, we use the Python built-in hash function to process text as a tuple of token IDs, and implement the random vector generator G by seeding PyTorch with the hash value. For INV-DY, we use λ 1 = λ 2 = 1.</p>
<p>Scores Table 1 reports the final score (the average score of the final 100 episodes during training), and the maximum score seen in each game for different models. Average normalized score (raw score divided by game total score) over all games is also reported. Compared to the base DRRN, MIN-OB turns out to explore similar maximum scores on † We omit games where DRRN cannot score. ‡ Please refer to  for more details about these games. most games (except DEEPHOME and DRAGON), but fails to memorize the good experience and reach high episodic scores, which suggests the importance of identifying different observations using language details. Most surprisingly, HASH has a lower final score than DRRN on only one game (ZORK I), while on PENTARI it almost doubles the DRRN final score. It is also the model with the best average normalized final score across games, which indicates that the DRRN model can perform as well without leveraging any language semantics, but instead simply by identifying different observations and actions with random vectors and memorizing the Q-values. Lastly, we observe on some games (DRAGON, OMNIQUEST, ZORK I) INV-DY can explore high scores that other models cannot. Most notably, on ZORK I the maximum score seen is 87 (average of 54, 94, 113), while any run of other models does not explore a score more than 55. This might indicate potential benefit of developing RL agents with more semantic representations.</p>
<p>Transfer We also investigate if representations of different models can transfer to a new language environment, which is a potential benefit of learning natural language semantics. So we consider the two most similar games in Jericho, ZORK I and ZORK III, fix the language encoders of different ZORK I models, and re-train the Q-network on ZORK III for 10,000 steps. As shown in Figure 2, INV-DY representations can achieve a score around 1, which surpasses the best result of models trained from scratch on ZORK III for 100,000 steps (around 0.4), showing great promise in better gameplay by leveraging language understanding from other games. HASH transfer is equivalent to training from scratch as the representations are not learnt, and a score around 0.3 is achieved. Finally, DRRN representations transfer worse than HASH, possibly due to overfitting to the TD loss (2).</p>
<p>Visualizations Finally, we use t-SNE (Maaten and Hinton, 2008) to visualize representations of some ZORK I walkthrough states in Figure 3. The first 30 walkthrough states (red, score 0-45) are well experienced by the models during exploration, whereas the last 170 states (blue, score 157-350) are unseen § . We also encircle the subset of states at location 'living room' for their shared semantics.</p>
<p>First, we note that the HASH representations for living room states are scattered randomly, unlike the other two models with GRU language encoders. Further, the base DRRN overfits to the TD loss (2), representing unseen states (blue) in a different subspace to seen states (red) without regarding their semantic similarity. IND-DY is able to extrapolate to unseen states and represent them similarly to seen states for their shared semantics, which may explain its better performance on this game.</p>
<p>Game stochasticity All the above experiments were performed using a fixed game random seed for each game, following prior work . To investigate if randomness in games affects our conclusions, we run one trial of each game with episode-varying random seeds ¶ . We find the average normalized score for the base DRRN, HASH, INV-DY to be all around 17%, with performance drop mainly on three stochastic games (DRAGON, ZORK I, ZORK III). Notably, the core finding that the base DRRN and HASH perform similarly still holds. Intuitively, even though the Q-values would be lower overall with unexpected transitions, RL would still memorize observations and actions that lead to high Q-values.</p>
<p>Discussion</p>
<p>At a high level, RL agents for text-based games succeed by (1) exploring trajectories that lead to high scores, and (2) learning representations to stably reach high scores. Our experiments show that a semantics-regularized INV-DY model manages to explore higher scores on some games (DRAGON, OMNIQUEST, ZORK I), while the HASH model manages to memorize scores better on other games (LIBRARY, LUDICORP, PENTARI) using just a fixed, random, non-semantic representation. This leads us to hypothesize two things. First, fixed, stable representations might make Q-learning easier. Second, it might be desirable to represent similar texts very differently for better gameplay, e.g. the Q-value can be much higher when a key object is mentioned, even if it only adds a few words to a long observation text. This motivates future thought into the structural vs. functional use of language semantics in these games.</p>
<p>Our findings also urge a re-thinking of the popular 'RL + valid action handicap' setup for these games. On one hand, RL sets training and evaluation in the same environment, with limited text corpora, and sparse, mostly deterministic rewards as the only optimization objective. Such a combination easily results in overfitting to the reward system of a specific game (Figure 2), or even just a specific stage of the game (Figure 3). On the other hand, the valid action handicap reduces the action set to a small size tractable for memorization, and reduces the language understanding challenge for the RL agent. Thus for future research on text-based games, we advocate for more attention towards alternative setups without RL or handicaps (Hausknecht et al., 2019;Yao et al., 2020;. Particularly, in a 'RL + no valid action handicap' setting, generating action candidates rather than simply choosing from a set entails more opportunities and challenges with respect to learning grounded language semantics (Yao et al., 2020). Additionally, training agents on a distribution of games and evaluating them on a separate set of unseen games would require more general semantic understanding. Semantic evaluation of these proposed paradigms is outside the scope of this paper, but we hope it will spark a productive discussion on the next steps toward building agents with stronger semantic understanding.</p>
<p>Ethical Considerations</p>
<p>Autonomous decision-making agents are potentially impactful in our society, and it is of great ethical consideration to make sure their understanding of the world and their objectives align with humans. Humans use natural language to convey and understand concepts as well as inform decisions, and in this work we investigate whether autonomous agents leverage language semantics similarly to humans in the environment of text-based games. Our findings suggest that the current generation of agents optimized for reinforcement learning objectives might not exhibit human-like language understanding, a phenomenon we should pay attention to and further study.</p>
<p>Figure 1 :
1(a): Sample original gameplay from ZORK I. (b) (c): Our proposed semantic ablations. (b) MIN-OB reduces observations to only the current location name, and (c) HASH replaces observation and action texts by their string hash values.</p>
<p>Figure 2 :
2Transfer results from ZORK I.</p>
<p>Figure 3
3: t-SNE visualization of seen and unseen state observations of ZORK I. DRRN (base) represents unseen states separated from seen states while INV-DY mixes them by semantic similarity.</p>
<p>). Breaking Semantics via Hashing (HASH) GRU encoders f o and f a in the Q-network (1) generally ensure that similar texts (e.g. a single word change) are given similar representations, and therefore similar values. To study whether such a semantics continuity is useful, we break it by hashing observation and action texts. Specifically,Table 1: Final/maximum score of different models.Game DRRN </p>
<p>MIN-OB 
HASH 
INV-DY </p>
<p>Max </p>
<p>balances 10 / 10 
10 / 10 
10 / 10 
10 / 10 
51 
deephome 57 / 66 
8.5 / 27 
58 / 67 
57.6 / 67 300 
detective 290 / 337 86.3 / 350 290 / 317 290 / 323 360 
dragon -5.0 / 6 
-5.4 / 3 
-5.0 / 7 
-2.7 / 8 
25 
enchanter 20 / 20 
20 / 40 
20 / 30 
20 / 30 
400 
inhumane 21.1 / 45 12.4 / 40 
21.9 / 45 19.6 / 45 90 
library 15.7 / 21 12.8 / 21 
17 / 21 
16.2 / 21 30 
ludicorp 12.7 / 23 11.6 / 21 
14.8 / 23 13.5 / 23 150 
omniquest 4.9 / 5 
4.9 / 5 
4.9 / 5 
5.3 / 10 
50 
pentari 26.5 / 45 21.7 / 45 
51.9 / 60 37.2 / 50 70 
zork1 39.4 / 53 29 / 46 
35.5 / 50 43.1 / 87 350 
zork3 0.4 / 4.5 
0.0 / 4 
0.4 / 4 
0.4 / 4 
7 </p>
<p>Avg. Norm .21 / .38 
.12 / .35 
.25 / .39 
.23 / .40 </p>
<p>§ The rest 150 states in the middle (score 45-157) are omitted as they might be seen by some model but not others. ¶ Randomness includes transition uncertainty (e.g. thief showing up randomly in ZORK I) and occasional paraphrasing of text observations.
AcknowledgementsWe appreciate helpful suggestions from anonymous reviewers as well as members of the Princeton NLP Group and MSR RL Group.
Learning dynamic knowledge graphs to generalize on text-based games. Ashutosh Adhikari, ( Xingdi, ) Eric, Marc-Alexandre Yuan, Mikulas Côté, Marc-Antoine Zelinka, Romain Rondeau, Pascal Laroche, Jian Poupart, Adam Tang, William L Trischler, Hamilton, NeurIPS. Ashutosh Adhikari, Xingdi (Eric) Yuan, Marc- Alexandre Côté, Mikulas Zelinka, Marc-Antoine Rondeau, Romain Laroche, Pascal Poupart, Jian Tang, Adam Trischler, and William L. Hamilton . 2020. Learning dynamic knowledge graphs to gen- eralize on text-based games. In NeurIPS 2020.</p>
<p>Graph constrained reinforcement learning for natural language action spaces. Prithviraj Ammanabrolu, Matthew J Hausknecht, 8th International Conference on Learning Representations. Addis Ababa, Ethiopia2020OpenReview.netPrithviraj Ammanabrolu and Matthew J. Hausknecht. 2020. Graph constrained reinforcement learning for natural language action spaces. In 8th Inter- national Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.</p>
<p>How to avoid being eaten by a grue: Exploration strategies for text-adventure agents. Prithviraj Ammanabrolu, Ethan Tien, Zhaochen Luo, Mark O Riedl, arXiv:2002.08795arXiv preprintPrithviraj Ammanabrolu, Ethan Tien, Zhaochen Luo, and Mark O Riedl. 2020. How to avoid being eaten by a grue: Exploration strategies for text-adventure agents. arXiv preprint arXiv:2002.08795.</p>
<p>On the properties of neural machine translation: Encoder-decoder approaches. Kyunghyun Cho, Dzmitry Bart Van Merriënboer, Yoshua Bahdanau, Bengio, 10.3115/v1/W14-4012Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation. SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical TranslationDoha, QatarAssociation for Computational LinguisticsKyunghyun Cho, Bart van Merriënboer, Dzmitry Bah- danau, and Yoshua Bengio. 2014. On the properties of neural machine translation: Encoder-decoder ap- proaches. In Proceedings of SSST-8, Eighth Work- shop on Syntax, Semantics and Structure in Statisti- cal Translation, pages 103-111, Doha, Qatar. Asso- ciation for Computational Linguistics.</p>
<p>What can you do with a rock? affordance extraction via word embeddings. Nancy Fulda, Daniel Ricks, Ben Murdoch, David Wingate, 10.24963/ijcai.2017/144Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI 2017. the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI 2017Melbourne, Australiaijcai.orgNancy Fulda, Daniel Ricks, Ben Murdoch, and David Wingate. 2017. What can you do with a rock? affor- dance extraction via word embeddings. In Proceed- ings of the Twenty-Sixth International Joint Con- ference on Artificial Intelligence, IJCAI 2017, Mel- bourne, Australia, August 19-25, 2017, pages 1039- 1045. ijcai.org.</p>
<p>Interactive fiction game playing as multi-paragraph reading comprehension with reinforcement learning. Xiaoxiao Guo, Mo Yu, Yupeng Gao, Chuang Gan, Murray Campbell, Shiyu Chang, 10.18653/v1/2020.emnlp-main.624Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Xiaoxiao Guo, Mo Yu, Yupeng Gao, Chuang Gan, Mur- ray Campbell, and Shiyu Chang. 2020. Interac- tive fiction game playing as multi-paragraph read- ing comprehension with reinforcement learning. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7755-7765, Online. Association for Computa- tional Linguistics.</p>
<p>Nail: A general interactive fiction agent. Matthew Hausknecht, Ricky Loynd, Greg Yang, Adith Swaminathan, Jason D Williams, arXiv:1902.04259arXiv preprintMatthew Hausknecht, Ricky Loynd, Greg Yang, Adith Swaminathan, and Jason D Williams. 2019. Nail: A general interactive fiction agent. arXiv preprint arXiv:1902.04259.</p>
<p>Interactive fiction games: A colossal adventure. Matthew J Hausknecht, Prithviraj Ammanabrolu, Marc-Alexandre Côté, Xingdi Yuan, The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference. New York, NY, USAAAAI Press2020The Tenth AAAI Symposium on Educational Advances in Artificial IntelligenceMatthew J. Hausknecht, Prithviraj Ammanabrolu, Marc-Alexandre Côté, and Xingdi Yuan. 2020. In- teractive fiction games: A colossal adventure. In The Thirty-Fourth AAAI Conference on Artificial In- telligence, AAAI 2020, The Thirty-Second Innova- tive Applications of Artificial Intelligence Confer- ence, IAAI 2020, The Tenth AAAI Symposium on Ed- ucational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pages 7903-7910. AAAI Press.</p>
<p>Deep reinforcement learning with a natural language action space. Ji He, Jianshu Chen, Xiaodong He, Jianfeng Gao, Lihong Li, Li Deng, Mari Ostendorf, 10.18653/v1/P16-1153Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. the 54th Annual Meeting of the Association for Computational LinguisticsBerlin, GermanyLong Papers1Association for Computational LinguisticsJi He, Jianshu Chen, Xiaodong He, Jianfeng Gao, Li- hong Li, Li Deng, and Mari Ostendorf. 2016. Deep reinforcement learning with a natural language ac- tion space. In Proceedings of the 54th Annual Meet- ing of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1621-1630, Berlin, Germany. Association for Computational Linguis- tics.</p>
<p>Visualizing data using t-sne. Laurens Van Der Maaten, Geoffrey Hinton, Journal of machine learning research. 9Laurens van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-sne. Journal of machine learning research, 9(Nov):2579-2605.</p>
<p>Language understanding for text-based games using deep reinforcement learning. Karthik Narasimhan, Tejas Kulkarni, Regina Barzilay, 10.18653/v1/D15-1001Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language ProcessingLisbon, PortugalAssociation for Computational LinguisticsKarthik Narasimhan, Tejas Kulkarni, and Regina Barzi- lay. 2015. Language understanding for text-based games using deep reinforcement learning. In Pro- ceedings of the 2015 Conference on Empirical Meth- ods in Natural Language Processing, pages 1-11, Lisbon, Portugal. Association for Computational Linguistics.</p>
<p>Curiosity-driven exploration by self-supervised prediction. Deepak Pathak, Pulkit Agrawal, Alexei A Efros, Trevor Darrell, PMLRProceedings of the 34th International Conference on Machine Learning. the 34th International Conference on Machine LearningSydney, NSW, Australia70Deepak Pathak, Pulkit Agrawal, Alexei A. Efros, and Trevor Darrell. 2017. Curiosity-driven exploration by self-supervised prediction. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, volume 70 of Proceedings of Machine Learn- ing Research, pages 2778-2787. PMLR.</p>
<p>Keep CALM and explore: Language models for action generation in text-based games. Shunyu Yao, Rohan Rao, Matthew Hausknecht, Karthik Narasimhan, 10.18653/v1/2020.emnlp-main.704Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational LinguisticsShunyu Yao, Rohan Rao, Matthew Hausknecht, and Karthik Narasimhan. 2020. Keep CALM and ex- plore: Language models for action generation in text-based games. In Proceedings of the 2020 Con- ference on Empirical Methods in Natural Language Processing (EMNLP), pages 8736-8754, Online. As- sociation for Computational Linguistics.</p>
<p>Deriving commonsense inference tasks from interactive fictions. Mo Yu, Xiaoxiao Guo, Yufei Feng, Xiaodan Zhu, Michael Greenspan, Murray Campbell, arXiv:2010.09788arXiv preprintMo Yu, Xiaoxiao Guo, Yufei Feng, Xiaodan Zhu, Michael Greenspan, and Murray Campbell. 2020. Deriving commonsense inference tasks from inter- active fictions. arXiv preprint arXiv:2010.09788.</p>            </div>
        </div>

    </div>
</body>
</html>
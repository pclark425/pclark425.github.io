<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2488 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2488</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2488</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-67.html">extraction-schema-67</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <p><strong>Paper ID:</strong> paper-252762204</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2210.03283v2.pdf" target="_blank">Design Amortization for Bayesian Optimal Experimental Design</a></p>
                <p><strong>Paper Abstract:</strong> Bayesian optimal experimental design is a sub-field of statistics focused on developing methods to make efficient use of experimental resources. Any potential design is evaluated in terms of a utility function, such as the (theoretically well-justified) expected information gain (EIG); unfortunately however, under most circumstances the EIG is intractable to evaluate. In this work we build off of successful variational approaches, which optimize a parameterized variational model with respect to bounds on the EIG. Past work focused on learning a new variational model from scratch for each new design considered. Here we present a novel neural architecture that allows experimenters to optimize a single variational model that can estimate the EIG for potentially infinitely many designs. To further improve computational efficiency, we also propose to train the variational model on a significantly cheaper-to-evaluate lower bound, and show empirically that the resulting model provides an excellent guide for more accurate, but expensive to evaluate bounds on the EIG. We demonstrate the effectiveness of our technique on generalized linear models, a class of statistical models that is widely used in the analysis of controlled experiments. Experiments show that our method is able to greatly improve accuracy over existing approximation strategies, and achieve these results with far better sample efficiency.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2488.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2488.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Amortized BOED (CNF + Deep Sets)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Design amortization for Bayesian optimal experimental design using conditional normalizing flows and set-invariant encoders</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method introduced in this paper that trains a single conditional variational posterior q_phi(θ | y, d) amortized over designs using a permutation-invariant set encoder (Deep Sets / attention) to produce a context and a conditional normalizing flow to represent the posterior; trained cheaply with a posterior-bound then evaluated with more accurate VNMC bounds to trade computation for information.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Design amortization with conditional normalizing flows</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>The system learns a single conditional variational posterior q_phi(θ | y, d) that generalizes across the design space. Key components: (1) a set-invariant encoder ENC_phi that takes per-unit design variables d_i and simulated outcomes y_i and aggregates via sum (with attention optionally inside the encoder) to produce a design context c_{y,d}; (2) a conditional normalizing flow (conditional base Gaussian + sequence of invertible conditional transforms, typically 4 affine coupling layers) parameterized by c_{y,d} that defines q_phi and allows sampling and density evaluation; (3) training uses the inexpensive posterior Barber–Agakov lower bound (O(N) cost) to fit q_phi across many randomly sampled designs (amortization), and then the fitted q_phi is used inside Variational Nested Monte Carlo (VNMC) / contrastive VNMC (cost O(NM)) to produce tighter EIG bounds at evaluation time. The architecture is trained end-to-end and is differentiable w.r.t. designs, enabling potential gradient-based design optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>General Bayesian experimental design for controlled experiments (demonstrated on generalized linear models: normal (known/unknown noise), logistic, binomial, categorical, multinomial).</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Select designs by maximizing Expected Information Gain (EIG): argmax_{d in D} EIG(d). The method provides efficient approximations to EIG for many candidate designs by amortizing a variational posterior across designs so many designs can be evaluated cheaply; training is performed on simulated model draws to learn q_phi across the design distribution and then used to rank/evaluate designs.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Number of Monte Carlo samples (N, M) and wall-clock time. Paper reports O(N) training cost for the posterior estimator, O(NM) for VNMC evaluation; concrete reported measurements include wall-clock times (e.g., 293s vs 920s) and sample counts used in experiments (e.g., posterior bound N=5000 for final evaluation, VNMC N=1000, M=31; NMC comparisons with N=30000, M=173).</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Expected Information Gain (EIG), i.e., mutual information between θ and outcomes y under design d: E_p(y|d)[H[p(θ)] - H[p(θ|y,d)]].</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Not an explicit exploration/exploitation bandit algorithm — selection is greedy with respect to the EIG objective (argmax_d EIG(d)). The paper focuses on efficient and accurate estimation of the EIG so that greedy maximization can be applied across many designs; no explicit sequential exploration schedule or multi-armed bandit balancing mechanism is proposed.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>No explicit diversity-promoting mechanism for hypothesis selection is proposed; diversity arises implicitly because EIG encourages sampling designs that reduce posterior entropy (which can favor informative and varied outcomes), and the set encoder supports exchangeable batches of units, but there is no explicit diversification constraint or repulsion objective.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed experimental-resource constraints (number of experimental units S / batch size) and computational budget (limits on Monte Carlo samples and wall-clock time / memory).</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Handles experimental budget by evaluating EIG under the given design-size S and choosing designs maximizing EIG; handles computational budget by (a) amortizing a single variational posterior across designs to avoid retraining per design, (b) training on the computationally cheap posterior lower bound (O(N)) and using the fitted model to evaluate more costly VNMC bounds (O(NM)), and (c) reducing required Monte Carlo samples by using learned importance proposals (VNMC) to obtain tight bounds with far fewer samples.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>No explicit 'breakthrough' or novelty metric is defined; EIG (expected information gain / mutual information) is used as the proxy objective for design value and thus as the surrogate for potential high-impact discoveries.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported metrics include wall-clock time (e.g., amortized method training took 291s; total evaluation for 50 designs 293s vs non-amortized 920s), sample-efficiency comparisons (VNMC + amortized q_phi required up to 167x fewer samples than NMC in reported experiments), and tightness of VNMC upper/lower bounds (VNMC bounds reported as nearly exact in several GLM cases). Specific sample settings reported: posterior-bound training N=50 per training step across batches; final evaluations used posterior bound N=5000 for the posterior estimator, VNMC N=1000, M=31, and NMC N=30000, M=173 for comparisons. Training was run for 5000 optimization steps with AdamW.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Nested Monte Carlo (NMC) estimator, non-amortized variational posterior forms from Foster et al. (2019) (the 'AB' model variational form q_phi(θ|y,d)=N(Ay, Σ)), and various architectural ablations (ResNet encoder, different flow transforms).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Amortized method produced much tighter posterior lower bounds correlated with ground truth, would select the best design in examples where the non-amortized variational form would not; wall-clock speed-up >3x (293s vs 920s) compared to the non-amortized variational form on a 50-design experiment; VNMC bounds using the amortized q_phi achieved near-exact EIG with 167× fewer samples than NMC in reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Reported gains: >3× faster wall-clock time for amortized posterior-bound training/evaluation on 50 designs; up to 167× fewer Monte Carlo samples required by VNMC with amortized q_phi compared to NMC to reach similar accuracy in EIG estimation.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Explicit trade-off studied: training on the cheap posterior lower bound (O(N)) vs directly optimizing the more accurate VNMC bounds (O(NM)). The paper empirically shows that (a) training with the posterior lower bound is computationally much cheaper and (b) the resulting variational posterior is effective when plugged into VNMC to produce nearly-exact bounds — i.e., a compute vs accuracy tradeoff where cheap training plus relatively inexpensive VNMC evaluation yields excellent overall performance. The paper also discusses estimator convergence rates: NMC converges slowly O(T^{-1/3}) (Rainforth et al.), whereas Monte Carlo-style and amortized variational approaches yield O(T^{-1/2}) rates in their relevant components, motivating use of variational amortization to reduce total sample complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Key recommendations: (1) Amortize a flexible conditional variational posterior across designs using set-invariant encoders + conditional normalizing flows to allow cheap evaluation of EIG across many designs; (2) Train the amortized variational posterior using the cheap posterior estimator (Barber–Agakov lower bound, O(N)) and then compute VNMC bounds (O(NM)) for accurate evaluation — this combination provides effective accuracy vs compute trade-offs; (3) Learned proposals that are wider (forward-KL style posterior estimator) are advantageous for importance-sampling-based VNMC because they avoid excessive importance-weight variance; (4) the learned q_phi is differentiable w.r.t. designs, suggesting gradient-based design optimization is feasible in future extensions.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2488.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2488.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VNMC / Contrastive VNMC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Variational Nested Monte Carlo (and Contrastive VNMC)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A variational importance-sampling based estimator for the EIG that uses a learned conditional proposal q_phi(θ|y,d) to estimate the marginal likelihood inside nested Monte Carlo, producing an upper bound (VNMC) and a contrastive lower-bound variant (CVNMC); minimizes variance and sample needs when q_phi approximates the true posterior.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Variational Bayesian Optimal Experimental Design</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Variational Nested Monte Carlo (VNMC) / Contrastive VNMC (CVNMC)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>VNMC replaces the naive nested Monte Carlo marginal-likelihood estimator with importance sampling using a learned variational proposal q_phi(θ|y,d). If q_phi equals the true posterior, VNMC is tight with M=1; otherwise consistency holds as M→∞. Contrastive VNMC includes the prior sample θ^0 in the importance denominator to produce a lower bound (CVNMC). These bounds (upper and contrastive lower) are used to tightly bracket the true EIG and are used in this paper for accurate evaluation after amortized training.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Bayesian optimal experimental design; general probabilistic models where posterior/marginal likelihood are intractable.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>VNMC reduces computational resource needs by using a learned importance proposal to lower variance and required nested samples M; resource allocation in the paper: use cheap posterior-bound training to learn q_phi then allocate computational budget to VNMC evaluations with moderate N and small M to obtain tight bounds.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Number of outer and inner Monte Carlo samples (N and M); VNMC cost O(NM). Paper example: VNMC evaluation used N=1000, M=31 in model experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Expected Information Gain (EIG) approximated via VNMC upper/lower bounds.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Not an explicit exploration/exploitation mechanism beyond maximizing (estimated) EIG; VNMC's role is to provide accurate EIG estimates that inform greedy selection.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>No explicit diversity mechanism; VNMC focuses on accurate marginal-likelihood estimation for EIG.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Computational budget defined by N, M and memory limits (e.g., 64GB RAM constrained choices of N and M in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>VNMC reduces required M by improving the importance proposal q_phi; contrastive VNMC provides a lower bound variant that can be used with the same q_phi to bracket EIG under limited M.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>None explicit — EIG is the objective used as a proxy for experimental value.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>VNMC bounds reported as nearly exact in many GLM experiments; experimental VNMC settings N=1000, M=31 produced tight bounds where NMC required much larger N and M (e.g., N=30000, M=173) to be comparable.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Nested Monte Carlo (NMC), posterior estimator lower bound, and non-amortized variational approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>VNMC with amortized q_phi provided much tighter bounds and required far fewer samples; VNMC upper/lower often nearly coincide and constrain the true EIG more tightly than NMC bounds in reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Empirical sample-efficiency improvement: VNMC + amortized q_phi required up to 167× fewer samples than NMC in experiments to obtain similar accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper shows that optimizing q_phi with the cheap posterior estimator yields a proposal that works well inside VNMC, giving a practical compute-accuracy tradeoff: cheap training + modest VNMC evaluation is more efficient than attempting to optimize q_phi directly w.r.t. VNMC bounds (which would be costlier).</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Using a variationally learned importance proposal is effective for reducing nested-sampling costs in EIG estimation; include the prior (contrastive term) to form lower bounds that help bracket the true EIG.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2488.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2488.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Posterior Estimator (Barber–Agakov)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Posterior variational lower bound (Barber–Agakov lower bound applied to EIG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A computationally cheap lower bound on EIG obtained by introducing a variational approximation q_phi(θ|y,d) to the posterior and using the Barber–Agakov bound; used in this paper as the main training objective for amortized q_phi because it costs O(N) per optimization step.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The IM Algorithm: A Variational Approach to Information Maximization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Posterior estimator (Barber–Agakov bound applied to EIG)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Defines a lower bound L_post(d)=E_{p(θ,y|d)}[log q_phi(θ|y,d) - log p(θ)] which is maximized w.r.t. φ. The Monte Carlo estimate of this bound converges at O(T^{-1/2}). Training on this bound tends to produce proposals that are wider than the true posterior (forward-KL-like behavior), which is beneficial for importance-sampling-based VNMC evaluation because it reduces importance-weight variance.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Bayesian experimental design inference and variational fitting of posterior approximations for EIG estimation.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Used as a cheap training objective to allocate computational training budget (many random designs and small N per batch) to learn a general q_phi across the design space; does not directly decide which experiments to run, but reduces computational cost of producing EIG estimates to enable evaluating many candidate designs.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Number of Monte Carlo samples N (cost O(N) per gradient step); wall-clock time reported for training (e.g., training took ~291s in an amortization experiment).</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Provides a lower bound estimate to EIG used for training and approximate ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>None explicit; produces an approximation used by EIG maximization to choose designs.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>No explicit diversity mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Computational budget for training (N per step, number of steps) and experimental budget (design size S encoded in inputs).</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Reduces computational training cost relative to optimizing VNMC bounds by operating at O(N) and amortizing across designs; produces a q_phi usable later in higher-cost VNMC evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>None explicit.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Posterior estimator yields a lower bound that in experiments was closer to ground truth than the NMC lower bound in many settings; training used batches of 50 designs and N=50 Monte Carlo samples per optimization step during training; final evaluations used larger N (e.g., 5000) for the posterior estimator's Monte Carlo estimates.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>NMC lower bound and VNMC bounds; non-amortized variational forms.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Posterior-based amortized q_phi generally produced better (tighter) lower bounds than NMC lower bounds and served as an effective proposal for VNMC, although its absolute tightness varies by model complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Training via the posterior estimator has cost O(N) vs VNMC's O(NM), enabling many designs to be covered cheaply and enabling large amortization speedups (wall-clock saving observed).</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper empirically validates that cheaper training on the posterior estimator yields a q_phi useful for subsequent VNMC evaluation, enabling a favorable compute-vs-accuracy tradeoff.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Recommend training amortized q_phi with the posterior estimator to cheaply cover design space, then use VNMC for accurate final EIG evaluations; posterior-estimator-produced q_phi tends to be sufficiently wide to serve as a robust importance proposal.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2488.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2488.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Nested Monte Carlo (NMC)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Nested Monte Carlo estimator for EIG</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A straightforward Monte Carlo estimator for EIG that estimates marginal likelihoods by inner Monte Carlo loops; consistent but converges slowly because of the nested structure, requiring many samples and thus high computation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>On nesting monte carlo estimators</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Nested Monte Carlo (NMC)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Estimates EIG by sampling θ^n ~ p(θ), y^n ~ p(y|θ^n,d) and estimating p(y^n|d) with an inner Monte Carlo average over M prior samples; estimator converges as N,M→∞, and optimal asymptotic allocation M ∝ sqrt(N) leads to O(T^{-1/3}) convergence in total samples T = N*M, which is slower than typical O(T^{-1/2}) Monte Carlo rates. In practice, NMC requires large N and M to obtain tight bounds, consuming substantial computation and memory.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>General-purpose baseline estimator for EIG in Bayesian experimental design.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>No learning-based allocation — NMC is an estimator used to evaluate EIG of candidate designs; resource allocation is via the choice of N and M (sample budget) and memory constraints. Used as a baseline ranking method for designs.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Total number of samples T = N*M and wall-clock time / memory (paper cites memory-limited choices of N and M due to 64GB RAM).</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>EIG estimated via nested Monte Carlo estimator (Eq. (4) in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>None — NMC only estimates EIG used by greedy design selection.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>None explicit.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Computational budget (samples N and M, memory limits) and experimental batch size (design dimension).</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>User chooses N and M; asymptotically optimal allocation M ∝ sqrt(N) referenced from Rainforth et al.; in practice memory constraints limit feasible N and M.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>None explicit beyond EIG estimates.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Used as baseline; in experiments NMC required N=30000 and M=173 for comparison in some settings, and produced much looser bounds than VNMC + amortized q_phi; NMC convergence rate cited as O(T^{-1/3}).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against VNMC, posterior estimator, and amortized variational approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>NMC produced much looser bounds and required orders of magnitude more samples to approach the accuracy of VNMC with amortized q_phi; NMC degraded rapidly as model complexity grew while VNMC remained reliable.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>N/A (serves as baseline showing inefficiency); other methods achieved up to 167× fewer samples than NMC for similar accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper uses NMC to illustrate the computational-accuracy tradeoff and motivate variational importance-sampling proposals (VNMC) and amortization; highlights slow convergence O(T^{-1/3}) and high variance due to importance-weights when naive proposals are used.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>References asymptotically optimal inner-outer allocation M ∝ sqrt(N) for NMC (Rainforth et al.), but shows that practical computational limits make NMC inefficient for complex models, motivating learned-importance approaches.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2488.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2488.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Foster et al. variational BOED framework</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Variational Bayesian Optimal Experimental Design (Foster et al., 2019, 2020)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior framework proposing four variational bounds for EIG estimation (including posterior estimator and VNMC) using variational approximations that can amortize across y samples but in Foster et al.'s initial experiments used per-design variational fits; this paper builds on and extends that framework by proposing a flexible amortized neural variational family.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Variational Bayesian Optimal Experimental Design</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Variational BOED (Foster et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Framework that derives multiple variational bounds for EIG estimation (posterior estimator / Barber–Agakov bound, VNMC, contrastive variants, etc.) and proposes using variational approximations q_phi for either the posterior p(θ|y,d) or the marginal likelihood p(y|d) to amortize computations. Original implementations used simpler parametric q forms and per-design optimization; this paper extends the framework with an amortized deep conditional variational family to evaluate many designs without retraining.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Bayesian optimal experimental design across disciplines where EIG is used as utility.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Provide bounds and variational objectives that enable allocating computational resources between variational fitting and Monte Carlo evaluation (choice of N and M, selection of whether to optimize q per design or amortize across designs).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Number of samples (N, M) and optimization time; Foster et al. showed variational approaches reduce nested sampling needs by amortization across y samples but their implementations were per-design.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Expected Information Gain (EIG) approximated via variational bounds.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Not an explicit bandit-style mechanism; methods are used to produce EIG estimates that feed greedy selection.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>No explicit diversity-promoting mechanism in the original framework; focus is on accurate EIG bounding.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Computational budget via sample counts and optimization cost; experimental constraints (design dimensionality and batch size) are inputs to the framework.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Provides lower and upper variational bounds (and contrastive variants) so practitioners can select computational budget allocations (optimize q, choose N and M) to obtain desired bound tightness.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>None explicit beyond EIG.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Original Foster et al. provided experiments showing viability on simple design problems; this paper cites Foster's framework as having per-design variational fits which are computationally expensive and less expressive than amortized deep variational families.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>NMC and other Monte Carlo approaches; the current paper compares to Foster et al.'s non-amortized variational forms and shows superior speed and accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>This paper reports that amortized deep variational models outperform the simpler per-design parametric variational forms used in Foster et al. (2019) both in accuracy of EIG estimates and computational efficiency (e.g., >3x faster in an amortization experiment).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>By moving from per-design parametric q to an amortized deep q_phi, the paper reports substantial runtime and sample-efficiency gains (see amortization and VNMC efficiency numbers above).</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper frames Foster et al.'s variational bounds as a foundation and demonstrates that combining amortization and flexible conditional flows yields a favorable tradeoff between upfront training cost and per-design evaluation cost.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Building expressive, amortized variational families and training them on cheaper bounds is an effective way to allocate computational resources across many candidate designs while preserving the ability to obtain accurate EIG estimates via VNMC at evaluation time.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Variational Bayesian Optimal Experimental Design <em>(Rating: 2)</em></li>
                <li>A unified stochastic gradient approach to designing bayesian-optimal experiments <em>(Rating: 2)</em></li>
                <li>On nesting monte carlo estimators <em>(Rating: 2)</em></li>
                <li>The IM Algorithm: A Variational Approach to Information Maximization <em>(Rating: 2)</em></li>
                <li>Deep Sets <em>(Rating: 1)</em></li>
                <li>Normalizing Flows for Probabilistic Modeling and Inference <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2488",
    "paper_id": "paper-252762204",
    "extraction_schema_id": "extraction-schema-67",
    "extracted_data": [
        {
            "name_short": "Amortized BOED (CNF + Deep Sets)",
            "name_full": "Design amortization for Bayesian optimal experimental design using conditional normalizing flows and set-invariant encoders",
            "brief_description": "A method introduced in this paper that trains a single conditional variational posterior q_phi(θ | y, d) amortized over designs using a permutation-invariant set encoder (Deep Sets / attention) to produce a context and a conditional normalizing flow to represent the posterior; trained cheaply with a posterior-bound then evaluated with more accurate VNMC bounds to trade computation for information.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Design amortization with conditional normalizing flows",
            "system_description": "The system learns a single conditional variational posterior q_phi(θ | y, d) that generalizes across the design space. Key components: (1) a set-invariant encoder ENC_phi that takes per-unit design variables d_i and simulated outcomes y_i and aggregates via sum (with attention optionally inside the encoder) to produce a design context c_{y,d}; (2) a conditional normalizing flow (conditional base Gaussian + sequence of invertible conditional transforms, typically 4 affine coupling layers) parameterized by c_{y,d} that defines q_phi and allows sampling and density evaluation; (3) training uses the inexpensive posterior Barber–Agakov lower bound (O(N) cost) to fit q_phi across many randomly sampled designs (amortization), and then the fitted q_phi is used inside Variational Nested Monte Carlo (VNMC) / contrastive VNMC (cost O(NM)) to produce tighter EIG bounds at evaluation time. The architecture is trained end-to-end and is differentiable w.r.t. designs, enabling potential gradient-based design optimization.",
            "application_domain": "General Bayesian experimental design for controlled experiments (demonstrated on generalized linear models: normal (known/unknown noise), logistic, binomial, categorical, multinomial).",
            "resource_allocation_strategy": "Select designs by maximizing Expected Information Gain (EIG): argmax_{d in D} EIG(d). The method provides efficient approximations to EIG for many candidate designs by amortizing a variational posterior across designs so many designs can be evaluated cheaply; training is performed on simulated model draws to learn q_phi across the design distribution and then used to rank/evaluate designs.",
            "computational_cost_metric": "Number of Monte Carlo samples (N, M) and wall-clock time. Paper reports O(N) training cost for the posterior estimator, O(NM) for VNMC evaluation; concrete reported measurements include wall-clock times (e.g., 293s vs 920s) and sample counts used in experiments (e.g., posterior bound N=5000 for final evaluation, VNMC N=1000, M=31; NMC comparisons with N=30000, M=173).",
            "information_gain_metric": "Expected Information Gain (EIG), i.e., mutual information between θ and outcomes y under design d: E_p(y|d)[H[p(θ)] - H[p(θ|y,d)]].",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Not an explicit exploration/exploitation bandit algorithm — selection is greedy with respect to the EIG objective (argmax_d EIG(d)). The paper focuses on efficient and accurate estimation of the EIG so that greedy maximization can be applied across many designs; no explicit sequential exploration schedule or multi-armed bandit balancing mechanism is proposed.",
            "diversity_mechanism": "No explicit diversity-promoting mechanism for hypothesis selection is proposed; diversity arises implicitly because EIG encourages sampling designs that reduce posterior entropy (which can favor informative and varied outcomes), and the set encoder supports exchangeable batches of units, but there is no explicit diversification constraint or repulsion objective.",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Fixed experimental-resource constraints (number of experimental units S / batch size) and computational budget (limits on Monte Carlo samples and wall-clock time / memory).",
            "budget_constraint_handling": "Handles experimental budget by evaluating EIG under the given design-size S and choosing designs maximizing EIG; handles computational budget by (a) amortizing a single variational posterior across designs to avoid retraining per design, (b) training on the computationally cheap posterior lower bound (O(N)) and using the fitted model to evaluate more costly VNMC bounds (O(NM)), and (c) reducing required Monte Carlo samples by using learned importance proposals (VNMC) to obtain tight bounds with far fewer samples.",
            "breakthrough_discovery_metric": "No explicit 'breakthrough' or novelty metric is defined; EIG (expected information gain / mutual information) is used as the proxy objective for design value and thus as the surrogate for potential high-impact discoveries.",
            "performance_metrics": "Reported metrics include wall-clock time (e.g., amortized method training took 291s; total evaluation for 50 designs 293s vs non-amortized 920s), sample-efficiency comparisons (VNMC + amortized q_phi required up to 167x fewer samples than NMC in reported experiments), and tightness of VNMC upper/lower bounds (VNMC bounds reported as nearly exact in several GLM cases). Specific sample settings reported: posterior-bound training N=50 per training step across batches; final evaluations used posterior bound N=5000 for the posterior estimator, VNMC N=1000, M=31, and NMC N=30000, M=173 for comparisons. Training was run for 5000 optimization steps with AdamW.",
            "comparison_baseline": "Nested Monte Carlo (NMC) estimator, non-amortized variational posterior forms from Foster et al. (2019) (the 'AB' model variational form q_phi(θ|y,d)=N(Ay, Σ)), and various architectural ablations (ResNet encoder, different flow transforms).",
            "performance_vs_baseline": "Amortized method produced much tighter posterior lower bounds correlated with ground truth, would select the best design in examples where the non-amortized variational form would not; wall-clock speed-up &gt;3x (293s vs 920s) compared to the non-amortized variational form on a 50-design experiment; VNMC bounds using the amortized q_phi achieved near-exact EIG with 167× fewer samples than NMC in reported experiments.",
            "efficiency_gain": "Reported gains: &gt;3× faster wall-clock time for amortized posterior-bound training/evaluation on 50 designs; up to 167× fewer Monte Carlo samples required by VNMC with amortized q_phi compared to NMC to reach similar accuracy in EIG estimation.",
            "tradeoff_analysis": "Explicit trade-off studied: training on the cheap posterior lower bound (O(N)) vs directly optimizing the more accurate VNMC bounds (O(NM)). The paper empirically shows that (a) training with the posterior lower bound is computationally much cheaper and (b) the resulting variational posterior is effective when plugged into VNMC to produce nearly-exact bounds — i.e., a compute vs accuracy tradeoff where cheap training plus relatively inexpensive VNMC evaluation yields excellent overall performance. The paper also discusses estimator convergence rates: NMC converges slowly O(T^{-1/3}) (Rainforth et al.), whereas Monte Carlo-style and amortized variational approaches yield O(T^{-1/2}) rates in their relevant components, motivating use of variational amortization to reduce total sample complexity.",
            "optimal_allocation_findings": "Key recommendations: (1) Amortize a flexible conditional variational posterior across designs using set-invariant encoders + conditional normalizing flows to allow cheap evaluation of EIG across many designs; (2) Train the amortized variational posterior using the cheap posterior estimator (Barber–Agakov lower bound, O(N)) and then compute VNMC bounds (O(NM)) for accurate evaluation — this combination provides effective accuracy vs compute trade-offs; (3) Learned proposals that are wider (forward-KL style posterior estimator) are advantageous for importance-sampling-based VNMC because they avoid excessive importance-weight variance; (4) the learned q_phi is differentiable w.r.t. designs, suggesting gradient-based design optimization is feasible in future extensions.",
            "uuid": "e2488.0"
        },
        {
            "name_short": "VNMC / Contrastive VNMC",
            "name_full": "Variational Nested Monte Carlo (and Contrastive VNMC)",
            "brief_description": "A variational importance-sampling based estimator for the EIG that uses a learned conditional proposal q_phi(θ|y,d) to estimate the marginal likelihood inside nested Monte Carlo, producing an upper bound (VNMC) and a contrastive lower-bound variant (CVNMC); minimizes variance and sample needs when q_phi approximates the true posterior.",
            "citation_title": "Variational Bayesian Optimal Experimental Design",
            "mention_or_use": "use",
            "system_name": "Variational Nested Monte Carlo (VNMC) / Contrastive VNMC (CVNMC)",
            "system_description": "VNMC replaces the naive nested Monte Carlo marginal-likelihood estimator with importance sampling using a learned variational proposal q_phi(θ|y,d). If q_phi equals the true posterior, VNMC is tight with M=1; otherwise consistency holds as M→∞. Contrastive VNMC includes the prior sample θ^0 in the importance denominator to produce a lower bound (CVNMC). These bounds (upper and contrastive lower) are used to tightly bracket the true EIG and are used in this paper for accurate evaluation after amortized training.",
            "application_domain": "Bayesian optimal experimental design; general probabilistic models where posterior/marginal likelihood are intractable.",
            "resource_allocation_strategy": "VNMC reduces computational resource needs by using a learned importance proposal to lower variance and required nested samples M; resource allocation in the paper: use cheap posterior-bound training to learn q_phi then allocate computational budget to VNMC evaluations with moderate N and small M to obtain tight bounds.",
            "computational_cost_metric": "Number of outer and inner Monte Carlo samples (N and M); VNMC cost O(NM). Paper example: VNMC evaluation used N=1000, M=31 in model experiments.",
            "information_gain_metric": "Expected Information Gain (EIG) approximated via VNMC upper/lower bounds.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Not an explicit exploration/exploitation mechanism beyond maximizing (estimated) EIG; VNMC's role is to provide accurate EIG estimates that inform greedy selection.",
            "diversity_mechanism": "No explicit diversity mechanism; VNMC focuses on accurate marginal-likelihood estimation for EIG.",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Computational budget defined by N, M and memory limits (e.g., 64GB RAM constrained choices of N and M in experiments).",
            "budget_constraint_handling": "VNMC reduces required M by improving the importance proposal q_phi; contrastive VNMC provides a lower bound variant that can be used with the same q_phi to bracket EIG under limited M.",
            "breakthrough_discovery_metric": "None explicit — EIG is the objective used as a proxy for experimental value.",
            "performance_metrics": "VNMC bounds reported as nearly exact in many GLM experiments; experimental VNMC settings N=1000, M=31 produced tight bounds where NMC required much larger N and M (e.g., N=30000, M=173) to be comparable.",
            "comparison_baseline": "Nested Monte Carlo (NMC), posterior estimator lower bound, and non-amortized variational approaches.",
            "performance_vs_baseline": "VNMC with amortized q_phi provided much tighter bounds and required far fewer samples; VNMC upper/lower often nearly coincide and constrain the true EIG more tightly than NMC bounds in reported experiments.",
            "efficiency_gain": "Empirical sample-efficiency improvement: VNMC + amortized q_phi required up to 167× fewer samples than NMC in experiments to obtain similar accuracy.",
            "tradeoff_analysis": "Paper shows that optimizing q_phi with the cheap posterior estimator yields a proposal that works well inside VNMC, giving a practical compute-accuracy tradeoff: cheap training + modest VNMC evaluation is more efficient than attempting to optimize q_phi directly w.r.t. VNMC bounds (which would be costlier).",
            "optimal_allocation_findings": "Using a variationally learned importance proposal is effective for reducing nested-sampling costs in EIG estimation; include the prior (contrastive term) to form lower bounds that help bracket the true EIG.",
            "uuid": "e2488.1"
        },
        {
            "name_short": "Posterior Estimator (Barber–Agakov)",
            "name_full": "Posterior variational lower bound (Barber–Agakov lower bound applied to EIG)",
            "brief_description": "A computationally cheap lower bound on EIG obtained by introducing a variational approximation q_phi(θ|y,d) to the posterior and using the Barber–Agakov bound; used in this paper as the main training objective for amortized q_phi because it costs O(N) per optimization step.",
            "citation_title": "The IM Algorithm: A Variational Approach to Information Maximization",
            "mention_or_use": "use",
            "system_name": "Posterior estimator (Barber–Agakov bound applied to EIG)",
            "system_description": "Defines a lower bound L_post(d)=E_{p(θ,y|d)}[log q_phi(θ|y,d) - log p(θ)] which is maximized w.r.t. φ. The Monte Carlo estimate of this bound converges at O(T^{-1/2}). Training on this bound tends to produce proposals that are wider than the true posterior (forward-KL-like behavior), which is beneficial for importance-sampling-based VNMC evaluation because it reduces importance-weight variance.",
            "application_domain": "Bayesian experimental design inference and variational fitting of posterior approximations for EIG estimation.",
            "resource_allocation_strategy": "Used as a cheap training objective to allocate computational training budget (many random designs and small N per batch) to learn a general q_phi across the design space; does not directly decide which experiments to run, but reduces computational cost of producing EIG estimates to enable evaluating many candidate designs.",
            "computational_cost_metric": "Number of Monte Carlo samples N (cost O(N) per gradient step); wall-clock time reported for training (e.g., training took ~291s in an amortization experiment).",
            "information_gain_metric": "Provides a lower bound estimate to EIG used for training and approximate ranking.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "None explicit; produces an approximation used by EIG maximization to choose designs.",
            "diversity_mechanism": "No explicit diversity mechanism.",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Computational budget for training (N per step, number of steps) and experimental budget (design size S encoded in inputs).",
            "budget_constraint_handling": "Reduces computational training cost relative to optimizing VNMC bounds by operating at O(N) and amortizing across designs; produces a q_phi usable later in higher-cost VNMC evaluation.",
            "breakthrough_discovery_metric": "None explicit.",
            "performance_metrics": "Posterior estimator yields a lower bound that in experiments was closer to ground truth than the NMC lower bound in many settings; training used batches of 50 designs and N=50 Monte Carlo samples per optimization step during training; final evaluations used larger N (e.g., 5000) for the posterior estimator's Monte Carlo estimates.",
            "comparison_baseline": "NMC lower bound and VNMC bounds; non-amortized variational forms.",
            "performance_vs_baseline": "Posterior-based amortized q_phi generally produced better (tighter) lower bounds than NMC lower bounds and served as an effective proposal for VNMC, although its absolute tightness varies by model complexity.",
            "efficiency_gain": "Training via the posterior estimator has cost O(N) vs VNMC's O(NM), enabling many designs to be covered cheaply and enabling large amortization speedups (wall-clock saving observed).",
            "tradeoff_analysis": "Paper empirically validates that cheaper training on the posterior estimator yields a q_phi useful for subsequent VNMC evaluation, enabling a favorable compute-vs-accuracy tradeoff.",
            "optimal_allocation_findings": "Recommend training amortized q_phi with the posterior estimator to cheaply cover design space, then use VNMC for accurate final EIG evaluations; posterior-estimator-produced q_phi tends to be sufficiently wide to serve as a robust importance proposal.",
            "uuid": "e2488.2"
        },
        {
            "name_short": "Nested Monte Carlo (NMC)",
            "name_full": "Nested Monte Carlo estimator for EIG",
            "brief_description": "A straightforward Monte Carlo estimator for EIG that estimates marginal likelihoods by inner Monte Carlo loops; consistent but converges slowly because of the nested structure, requiring many samples and thus high computation.",
            "citation_title": "On nesting monte carlo estimators",
            "mention_or_use": "use",
            "system_name": "Nested Monte Carlo (NMC)",
            "system_description": "Estimates EIG by sampling θ^n ~ p(θ), y^n ~ p(y|θ^n,d) and estimating p(y^n|d) with an inner Monte Carlo average over M prior samples; estimator converges as N,M→∞, and optimal asymptotic allocation M ∝ sqrt(N) leads to O(T^{-1/3}) convergence in total samples T = N*M, which is slower than typical O(T^{-1/2}) Monte Carlo rates. In practice, NMC requires large N and M to obtain tight bounds, consuming substantial computation and memory.",
            "application_domain": "General-purpose baseline estimator for EIG in Bayesian experimental design.",
            "resource_allocation_strategy": "No learning-based allocation — NMC is an estimator used to evaluate EIG of candidate designs; resource allocation is via the choice of N and M (sample budget) and memory constraints. Used as a baseline ranking method for designs.",
            "computational_cost_metric": "Total number of samples T = N*M and wall-clock time / memory (paper cites memory-limited choices of N and M due to 64GB RAM).",
            "information_gain_metric": "EIG estimated via nested Monte Carlo estimator (Eq. (4) in paper).",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "None — NMC only estimates EIG used by greedy design selection.",
            "diversity_mechanism": "None explicit.",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Computational budget (samples N and M, memory limits) and experimental batch size (design dimension).",
            "budget_constraint_handling": "User chooses N and M; asymptotically optimal allocation M ∝ sqrt(N) referenced from Rainforth et al.; in practice memory constraints limit feasible N and M.",
            "breakthrough_discovery_metric": "None explicit beyond EIG estimates.",
            "performance_metrics": "Used as baseline; in experiments NMC required N=30000 and M=173 for comparison in some settings, and produced much looser bounds than VNMC + amortized q_phi; NMC convergence rate cited as O(T^{-1/3}).",
            "comparison_baseline": "Compared against VNMC, posterior estimator, and amortized variational approaches.",
            "performance_vs_baseline": "NMC produced much looser bounds and required orders of magnitude more samples to approach the accuracy of VNMC with amortized q_phi; NMC degraded rapidly as model complexity grew while VNMC remained reliable.",
            "efficiency_gain": "N/A (serves as baseline showing inefficiency); other methods achieved up to 167× fewer samples than NMC for similar accuracy.",
            "tradeoff_analysis": "Paper uses NMC to illustrate the computational-accuracy tradeoff and motivate variational importance-sampling proposals (VNMC) and amortization; highlights slow convergence O(T^{-1/3}) and high variance due to importance-weights when naive proposals are used.",
            "optimal_allocation_findings": "References asymptotically optimal inner-outer allocation M ∝ sqrt(N) for NMC (Rainforth et al.), but shows that practical computational limits make NMC inefficient for complex models, motivating learned-importance approaches.",
            "uuid": "e2488.3"
        },
        {
            "name_short": "Foster et al. variational BOED framework",
            "name_full": "Variational Bayesian Optimal Experimental Design (Foster et al., 2019, 2020)",
            "brief_description": "A prior framework proposing four variational bounds for EIG estimation (including posterior estimator and VNMC) using variational approximations that can amortize across y samples but in Foster et al.'s initial experiments used per-design variational fits; this paper builds on and extends that framework by proposing a flexible amortized neural variational family.",
            "citation_title": "Variational Bayesian Optimal Experimental Design",
            "mention_or_use": "mention",
            "system_name": "Variational BOED (Foster et al.)",
            "system_description": "Framework that derives multiple variational bounds for EIG estimation (posterior estimator / Barber–Agakov bound, VNMC, contrastive variants, etc.) and proposes using variational approximations q_phi for either the posterior p(θ|y,d) or the marginal likelihood p(y|d) to amortize computations. Original implementations used simpler parametric q forms and per-design optimization; this paper extends the framework with an amortized deep conditional variational family to evaluate many designs without retraining.",
            "application_domain": "Bayesian optimal experimental design across disciplines where EIG is used as utility.",
            "resource_allocation_strategy": "Provide bounds and variational objectives that enable allocating computational resources between variational fitting and Monte Carlo evaluation (choice of N and M, selection of whether to optimize q per design or amortize across designs).",
            "computational_cost_metric": "Number of samples (N, M) and optimization time; Foster et al. showed variational approaches reduce nested sampling needs by amortization across y samples but their implementations were per-design.",
            "information_gain_metric": "Expected Information Gain (EIG) approximated via variational bounds.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Not an explicit bandit-style mechanism; methods are used to produce EIG estimates that feed greedy selection.",
            "diversity_mechanism": "No explicit diversity-promoting mechanism in the original framework; focus is on accurate EIG bounding.",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Computational budget via sample counts and optimization cost; experimental constraints (design dimensionality and batch size) are inputs to the framework.",
            "budget_constraint_handling": "Provides lower and upper variational bounds (and contrastive variants) so practitioners can select computational budget allocations (optimize q, choose N and M) to obtain desired bound tightness.",
            "breakthrough_discovery_metric": "None explicit beyond EIG.",
            "performance_metrics": "Original Foster et al. provided experiments showing viability on simple design problems; this paper cites Foster's framework as having per-design variational fits which are computationally expensive and less expressive than amortized deep variational families.",
            "comparison_baseline": "NMC and other Monte Carlo approaches; the current paper compares to Foster et al.'s non-amortized variational forms and shows superior speed and accuracy.",
            "performance_vs_baseline": "This paper reports that amortized deep variational models outperform the simpler per-design parametric variational forms used in Foster et al. (2019) both in accuracy of EIG estimates and computational efficiency (e.g., &gt;3x faster in an amortization experiment).",
            "efficiency_gain": "By moving from per-design parametric q to an amortized deep q_phi, the paper reports substantial runtime and sample-efficiency gains (see amortization and VNMC efficiency numbers above).",
            "tradeoff_analysis": "Paper frames Foster et al.'s variational bounds as a foundation and demonstrates that combining amortization and flexible conditional flows yields a favorable tradeoff between upfront training cost and per-design evaluation cost.",
            "optimal_allocation_findings": "Building expressive, amortized variational families and training them on cheaper bounds is an effective way to allocate computational resources across many candidate designs while preserving the ability to obtain accurate EIG estimates via VNMC at evaluation time.",
            "uuid": "e2488.4"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Variational Bayesian Optimal Experimental Design",
            "rating": 2,
            "sanitized_title": "variational_bayesian_optimal_experimental_design"
        },
        {
            "paper_title": "A unified stochastic gradient approach to designing bayesian-optimal experiments",
            "rating": 2,
            "sanitized_title": "a_unified_stochastic_gradient_approach_to_designing_bayesianoptimal_experiments"
        },
        {
            "paper_title": "On nesting monte carlo estimators",
            "rating": 2,
            "sanitized_title": "on_nesting_monte_carlo_estimators"
        },
        {
            "paper_title": "The IM Algorithm: A Variational Approach to Information Maximization",
            "rating": 2,
            "sanitized_title": "the_im_algorithm_a_variational_approach_to_information_maximization"
        },
        {
            "paper_title": "Deep Sets",
            "rating": 1
        },
        {
            "paper_title": "Normalizing Flows for Probabilistic Modeling and Inference",
            "rating": 1,
            "sanitized_title": "normalizing_flows_for_probabilistic_modeling_and_inference"
        }
    ],
    "cost": 0.0176045,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Design Amortization for Bayesian Optimal Experimental Design</p>
<p>Noble Kennamer nkenname@uci.edu 
Department of Computer Science
University of California Irvine</p>
<p>Steven Walton 
Department of Computer Science
University of Oregon</p>
<p>Alexander Ihler 
Department of Computer Science
University of California Irvine</p>
<p>Design Amortization for Bayesian Optimal Experimental Design</p>
<p>Bayesian optimal experimental design is a sub-field of statistics focused on developing methods to make efficient use of experimental resources. Any potential design is evaluated in terms of a utility function, such as the (theoretically well-justified) expected information gain (EIG); unfortunately however, under most circumstances the EIG is intractable to evaluate. In this work we build off of successful variational approaches, which optimize a parameterized variational model with respect to bounds on the EIG. Past work focused on learning a new variational model from scratch for each new design considered. Here we present a novel neural architecture that allows experimenters to optimize a single variational model that can estimate the EIG for potentially infinitely many designs. To further improve computational efficiency, we also propose to train the variational model on a significantly cheaper-to-evaluate lower bound, and show empirically that the resulting model provides an excellent guide for more accurate, but expensive to evaluate bounds on the EIG. We demonstrate the effectiveness of our technique on generalized linear models, a class of statistical models that is widely used in the analysis of controlled experiments. Experiments show that our method is able to greatly improve accuracy over existing approximation strategies, and achieve these results with far better sample efficiency.</p>
<p>Introduction</p>
<p>Conducting experiments is often a resource-intensive endeavour, motivating experimenters to design their experiments to be maximally informative given the resources available. Optimal experimental design (OED) aims to address this challenge by developing approaches to define a utility, U (d), of a possible designs, d, and algorithms for evaluating and optimizing this utility over all feasible designs D. OED has been used widely across science and engineering, including systems biology (Liepe et al. 2013), geostatistics (Diggle and Lophaven 2006), manufacturing (Antony 2001) and more (Goos and Jones 2011).</p>
<p>In this work we focus on evaluating the expected information gain (EIG), a commonly used utility function in Bayesian optimal experiment design (BOED) (Chaloner and Verdinelli 1995;Ryan et al. 2016). We specify our model, composed of a likelihood and prior p(y|θ, d)p(θ) for design d, possible experimental outcomes y and latent variables θ.</p>
<p>The EIG is then defined to be:
EIG(d) = E p(y|d) <a href="1">H[p(θ)] − H[p(θ|y, d)]</a>
where H[·] is the entropy function. The experimenter then seeks the solution to argmax d∈D EIG(d), where D is the set of all feasible designs. The EIG has sound theoretical justifications, proven to be optimal in certain settings (Sebastiani and Wynn 2000; Bernardo and Smith 2009). While powerful, this framework is limited by the difficulty in evaluating the EIG due to the intractability of the posterior distribution p(θ|y, d). Foster et al. (2019) proposed four variational bounds for efficiently approximating the EIG. The method involves defining a variational distribution, q φ (·) that will approximate either the posterior distribution p(θ|y, d) or the marginal likelihood p(y|d). The parameters φ of this variational distribution are optimized according to the proposed bounds. In principle their method allows for estimating the EIG for arbitrarily complex models using flexible variational models, however their implementations focused on simpler variational forms that required fitting a new variational model for every possible design. In this work we focus on design amortization by proposing a novel deep learning architecture based on conditional normalizing flows (NF) (Papamakarios et al. 2021;Kobyzev, Prince, and Brubaker 2020) and set invariant models (Zaheer et al. 2017) to define a flexible variational distribution q φ (·|d) that only needs to be trained once, but can then accurately estimate the EIG for potentially infinitely many designs. Our experiments will show how design amortization can dramatically improve computational efficiency and how our more flexible variational form can make much more accurate approximations to the EIG over competing methods. We provide our code here 1 .</p>
<p>Background</p>
<p>Scientists consistently face the challenge of having to conduct experiments under limited resources, and must design their experiments to use these resources as efficiently as possible. BOED provides a conceptually clear framework for doing so. We assume we are given a model with design variables d, experimental outcomes y and latent parameters θ about which we wish to learn. We have prior information on the latent variables encoded in a prior distribution, p(θ), and a likelihood that predicts experimental outcomes from a design and latent variables, p(y|θ, d). Via Bayes rule, these two functions combine to give us the posterior distribution p(θ|y, d) ∝ p(y|θ, d)p(θ) representing our state of knowledge about the latent variables after conducting an experiment with design d and observing outcomes y. For example the design variables, d, could represent the environmental conditions and chemical concentrations of a medium used to culture a strain of bacteria, which produces an important chemical compound. This design problem becomes more complex with increasing dimension of d, for example, if we have S petri dishes to work on (often called the experimental units or subjects). The experimental outcomes, y, would represent the amount of the chemical compound yielded from growing the culture in each of the conditions of d, and the latent variables θ represent parameters that define how the design variables d mediate the yield of the chemical compound y. After conducting the experiment and observing y, we can quantify our information gain (IG) as:
IG(y, d) = H[p(θ)] − H<a href="2">p(θ|y, d)</a>
However, this gain cannot be evaluated before conducting the experiment, as it requires knowing the outcomes y. However, taking the expectation of the information gain with respect to the outcomes, p(y|d), gives the EIG:
EIG(d) = E p(θ,y|d) log p(θ|y, d) p(θ) = E p(θ,y|d) log p(y|θ, d) p(y|d)(3)
Nested Monte Carlo: Typically p(θ|y, d) and p(y|d) are intractable, making the EIG challenging to compute. One common approach to approximating EIG is to use a nested Monte Carlo (NMC) estimator (Myung, Cavagnaro, and Pitt 2013;Vincent and Rainforth 2017;Rainforth et al. 2018):
µ N M C = 1 N N n=1
log p(y n |θ n,0 , d)</p>
<p>1 M M m=1 p(y n |θ n,m , d) where θ n,m ∼ p(θ) and y n ∼ p(y|θ n,0 , d) (4) Rainforth et al. (2018) showed that NMC is a consistent estimator converging as N, M − → ∞. They also showed that it is asymptotically optimal to set M ∝ √ N , resulting in the overall convergence rate of O(T − 1 3 ), where T is the total number of samples drawn (i.e. T = N M for NMC). However, this is much slower than the O(T − 1 2 ) rate of standard Monte Carlo estimators (Robert and Casella 1999), in which the total number of samples is simply T = N .</p>
<p>The slow convergence of the NMC estimator can be limiting in practical applications of BOED. The inefficiencies can be traced to requiring an independent estimate of the marginal likelihood, p(y n |d), for each y n (the denominator of Eq. (4)). Inspired by this, Foster et al. (2019) proposed employing techniques from variational inference by defining a functional approximation to either p(θ|y, d) or p(y|d), and allowing these estimators to amortize across the samples of y n for more efficient estimation of the EIG. In this work we focus on two of the four estimators they proposed: the posterior estimator and variational nested Monte Carlo.</p>
<p>Posterior Estimator: The posterior estimator is an application of the Barber-Agakov bound to BOED, which was originally proposed for estimating the mutual information in noisy communication channels (Barber and Agakov 2003). It requires defining a variational approximation q φ (θ|y, d) to the posterior distribution, giving a lower bound to the EIG:
EIG(d) ≥ L post (d) E p(θ,y|d) log q φ (θ|y, d) p(θ) ≈ 1 N N n=1 log q φ (θ n |y n , d) p(θ n )
where y n , θ n ∼ p(y, θ|d).</p>
<p>By maximizing this bound with respect to the variational parameters φ, we can learn a variational form that can efficiently estimate the EIG. A Monte Carlo estimate of this bound converges with rate O(T − 1 2 ), and if the true posterior distribution is within the class of functions defined by the variational form q φ , the bound can be made tight (dependent on the optimization) (Foster et al. 2019).</p>
<p>Variational Nested Monte Carlo: The second bound we discuss is variational nested Monte Carlo (VNMC). It is closely related to NMC, but differs by applying a variational approximation q φ (θ|y, d) as an importance sampler to estimate the marginal likelihood term in NMC:
EIG(d) ≤ U V N M C (d, M ) E   log p(y|θ 0 , d) 1 M M m=1 p(y,θm|d) q φ (θm|y,d)   (6)
where the expectation is taken with respect to y, θ 0:M ∼ p(y, θ 0 |d) M m=1 q φ (θ m |y, d). By minimizing this upper bound with respect to the variational parameters φ, we can learn an importance distribution that allows for much more efficient computation of the EIG. Note that if q φ (θ|y, d) exactly equals the posterior distribution, the bound is tight and requires only a single nested sample (M = 1). Even if the variational form does not equal the posterior, the bound remains consistent as M − → ∞. Finally, it is worth noting that by taking q φ (θ|y, d) = p(θ), the estimator simply reduces to NMC.</p>
<p>It was further shown by Foster et al. (2020) that VNMC can be easily made into a lower bound by including θ 0 (the sample from the prior) when estimating the marginal likelihood, a method we denote as contrastive VNMC (CVNMC):
EIG(d) ≥ L CoV N M C (d, M ) E   log p(y|θ 0 , d) 1 M +1 M m=0 p(y,θm|d) q φ (θm|y,d)   (7)
where the expectation is taken with respect to y, θ 0:M ∼ p(y, θ 0 |d) M m=1 q φ (θ m |y, d). We can also employ this same technique to regular NMC to estimate both lower and upper bounds.</p>
<p>Note that the upper bound (6) and lower bound (7) are particularly useful when evaluating the performance of our method in settings where ground truth is not available. In these cases we can examine the bound pairs produced by NMC and by VNMC to assess which set more tightly constrains the true value.</p>
<p>Practical considerations. In this work, we apply the same flexible mathematical framework proposed in Foster et al. (2019). However, Foster et al. (2019) adopted a "classical" variational distribution setting, in which the variational form q φ is selected to take a standard, parametric form. They found this approach effective, but tested only on very simple design problems, with only one experimental unit at a time. Their variational models only incorporate the design implicitly, requiring a separate optimization for every design to be considered 2 . Unfortunately, as we show in the experiments this approach is not effective on more complex design problems. Instead, we propose a far more flexible, deep learning based distributional form that incorporates the design explicitly, allowing us to amortize training across and apply our trained model to evaluation of all (potentially continuously many) designs in our feasible set.</p>
<p>Method</p>
<p>We are interested in learning a parameterized function, q φ (θ|y, d), for approximating the posterior distribution. We now describe our proposed deep learning architecture for amortizing over designs, allowing practitioners to train a single model that is capable of evaluating the EIG for potentially infinitely many designs. We also discuss how we can efficiently train this model using the (simpler and cheaper) equation (5), then use the resulting approximation in the more accurate bounds provided by VNMC, (6)-(7). This advances the work in Foster et al. (2019) by providing a highly flexible variational form that can be used in a wide variety of contexts and an inexpensive procedure to train it. Figure 1 shows a high level representation of our architecture. Broadly, it consists of two major components. The first is a learnable function for taking in the design variables d and simulated experimental outcomes from the model, y, and producing a design context vector, c y,d , that will be used to define a conditional distribution. We focus on the common case where the experimental units lack any meaningful order and our learnable function must therefore be permutation invariant. We can incorporate this inductive bias into our model by making our function follow the general form of set functions proposed in Zaheer et al. (2017). In the sequel, we denote this component as our set invariant 2 Although subsequent work (Foster et al. 2020) considered evolving both the design and distribution q simultaneously, even that work remains focused on a single (if evolving) design.  Figure 1: A high-level schematic of our architecture for amortizing over designs. The first component (left) takes in the design variables and simulated observations and produces a design context, c y,d . In many experiments the individual units being experimented on are exchangeable, thus we use a set invariant architecture. The second (right) is a conditional normalizing flow, conditioned on the design context produced by the first component. Together, they define our variational posteriors q φ (θ|y, d), amortized over designs.</p>
<p>Neural Architecture</p>
<p>model. The second major component is a learnable distribution conditioned on the design context produced by the set invariant model. In this work we use conditional normalizing flows, which consist of a base distribution and sequence of invertible transformations with tractable Jacobian determinants to maintain proper accounting of the probability density through the transformations. Both the base distribution and transformations are learnable and conditioned on the design context.</p>
<p>Set Invariant Model.</p>
<p>It is often the case that the individual units being experimented on do not posses an inherent ordering -for example, subjects in a randomized controlled clinical trial, or the petri dishes in our previous example. Suppose we would like to find the optimally informative design for an experiment with S experimental units, where d i and y i denote the design variables and simulated outcomes of unit i, respectively. In this setting we want our design context to be invariant to permutations in its inputs, e.g., reordering the individuals in the trial should not change our results. Learning permutation invariant functions is an active area of research (e.g., Bloem-Reddy and Teh 2020). In this work we follow the general form proposed by Zaheer et al. (2017), where our set invariant model is defined as,
c y,d = EMIT φEMIT S i=0 ENC φENC (y i , d i ) .(8)
In particular, we define two learnable functions. The set encoder ENC φENC (y i , d i ) takes as input the design variables and simulated outcomes for each individual experimental unit. Its output is an intermediary representation for each experimental unit, which are aggregated together by summation; the permutation invariance of the sum ensures invariance of the overall function. The aggregated representation is then passed through the set emitter function EMIT φEMIT (·), which creates the final design context used in the conditional normalizing flow. In our experiments we find substantially improved performance using attention layers (Vaswani et al. 2017) in the set encoder, which allows for interactions between the experimental units before aggregation. In this case we should denote our set encoder as
ENC φ encode (y i , d i |y −i , d −i )
where y −i d −i denote all simulated outcomes and design variables except for the ith unit. Structuring our design encoder function in this way gives two major advantages. First, permutation invariance does not need to be learned by the function since it is already present by construction; this can make learning more efficient and reduces the total number of weights via weight sharing. Second, the function is able to encode designs with a variable number of experimental units, S, as long as the d i and y i have the same size for all units.</p>
<p>Note that not all experimental design problems are permutation invariant in the experimental units. For example, in some settings there could be a temporal component in the design variables, in which case we could replace our set invariant function with an order-based model such as a recurrent neural network.</p>
<p>Conditional Normalizing Flow. Normalizing flows define an expressive class of learnable probability distributions, which has been used in generative modeling and probabilistic inference (Papamakarios et al. 2021;Kobyzev, Prince, and Brubaker 2020). The main idea of normalizing flow based models is to represent a random variable θ as a transformation θ = T (x) of a random variable x sampled from a base distribution p(x). The key property is that the transformation T must be invertible and differentiable. This allows us to obtain p(θ) via a change of variables,
p(θ) = p(x) | det J T (x)| −1 (9) where x = T −1 (θ) and det J T (x)
is the determinant of the Jacobian at x. Both the transformations and base distribution may have learnable parameters. This provides a highly flexible class of distributions that can be both sampled and efficiently evaluated.</p>
<p>In our setting we would like to learn not just a single distribution, but rather a conditional distribution given design variables and experimental outcomes. This conditioning on d is key to allowing us to amortize over all possible designs. To this end, we learn a sequence of K conditional transformations T φi (·|c y,d ) and a conditional base distribution p φ0 (x|c y,d ) which together define our variational approximation to the posterior distribution amortized over designs.
q φ (θ|c y,d ) = p φ0 (x 0 |c y,d ) K i=1 | det J T φ i (x i |c y,d )| −1 (10) where θ = T φ K • T φ K−1 • . . . T φ1 (x 0 )
, with all transformations and base distribution conditioned on the design context (8). The full architecture, including the set invariant model and conditional normalizing flow is trained end-to-end.</p>
<p>Variational Posterior Training</p>
<p>The posterior estimator and the (contrastive) VNMC bounds all require learning a variational approximation to the posterior distribution. In both Foster et al. Foster et al. (2020) each bound was trained separately, learning its own variational approximation. However in all cases the variational approximation produced by training one of the bounds can be used for evaluating any other bound, since they all only require an approximate posterior distribution. Ideally, we would like to train using only the posterior estimator since it is much cheaper -a total cost of only O(N )whereas both VNMC bounds have a total cost of O(N M ). However, it is not obvious that training on the (also less accurate) bound should still provide good EIG estimates when used in the VNMC bounds. Our experiments show that it is surprisingly effective across a broad range of models. Intuitively speaking, this is possible because all bounds share the same optimum -the true posterior distribution. Moreover, because the posterior estimator takes its expectation with respect to the model p(y, θ|d), the variational approximation q φ (θ|y, d) will in general be wider than the true posterior distribution, akin to variational inference using the "forward" Kullback-Liebler (KL) divergence, and in contrast to the more commonly used "reverse KL" variational optimization methods that result in underdispersed and modeseeking optima. This property also makes the posterior estimator's q φ an excellent choice for importance sampling (as in VNMC), in which a too-narrow proposal distribution can lead to high variance in the importance weights, causing a small number of samples to dominate the estimator (Owen 2013, Chapter. 9).</p>
<p>Related Work</p>
<p>Our approach builds on the framework for BOED developed in Foster et al. (2019), which proposed four variational bounds for estimating EIG. The framework itself is quite flexible, capable of accommodating a wide variety of models (e.g., implicit vs explicit likelihoods), sequential experimentation (of arbitrary batch size) and arbitrary variational forms q. However, their experiments were limited to only single experimental units, and used simple variational forms that cannot amortize over designs (requiring a separate training procedure for each proposed design). In this work we propose a deep learning architecture which can easily be scaled to approximate arbitrarily complex distributions. In addition, our architecture can amortize over designs, allowing us to train a single variational model capable of estimating the EIG for potentially infinitely or continuously many designs. We also show that we can train our model using the cheaper posterior bound, then use its optimized approximate posterior within the VNMC bounds for a more accurate final approximation. We show that, using our proposed variational form, we can achieve highly accurate EIG estimates across a spectrum of complex design problems. While a few other EIG approximations have been proposed (see, e.g., Foster et al. (2019); Ryan et al. (2016)), in light of the experimental results of Foster et al. (2019) we mainly compare our experimental performance relative to NMC.</p>
<p>Experiments</p>
<p>We perform three types of experiments: amortization, model and architecture experiments. Our amortization experiment shows the dramatic increase in efficiency from amortization, and better EIG estimation provided by our more complex variational forms compared to those used in Foster et al. (2019). Model experiments examine how the benchmark method, NMC, breaks down as model complexity grows while our methods remain reliable for accurately estimating the EIG. Architecture experiments measure the impact of key componets in our variational approximation and serve as a guide to using our method effectively.</p>
<p>In all experiments we focus on estimating designs for different types of generalized linear models (GLMs) (McCullagh and Nelder 1989). GLMs are a very common model class used to analyze controlled experiments and are regularly used in applications of optimal experimental design (Goos and Jones 2011). Our GLMs have the general pattern,
θ ∼ N (µ p , Σ p ) r = g −1 (Dθ) y ∼ Exponential Family Distribution(r).(11)
Here, θ is a N p + 1 dimensional parameter vector, where N p is the number of predictors (+1 for the intercept term). D is a N E × (N p + 1) design matrix, where N E is the number of experimental units. The inverse link function, g −1 , defines the type of GLM. Finally, µ p and Σ p are the prior mean and covariance of the parameters. Our experiments cover six GLMs: normal (known observation noise), normal unknown (unknown observation noise), logistic, binomial, categorical and multinomial. For the normal model with known observation noise we take σ = 1; for the normal model with unknown observation noise 3 we use the prior σ ∼ InverseGamma(a p , b p ) with a p = b p = 3.5. For the binomial model, we assume 10 random trials; we use 3 classes for the categorical model, and 10 trials and 3 classes for the multinomial model. All experiments in the sequel are run for designs with 5 experimental units. Our implementations made significant use of Pyro  to implement the inference procedures and NFlows (Durkan et al. 2020) to construct our conditional normalizing flows. All training was done on a single Nvidia 2080TI and evaluation was done on an Intel I7-9800X with 64 GB of RAM.</p>
<p>Amortization Experiments</p>
<p>While providing an excellent framework, the variational forms used in Foster et al. (2019) are too simple to be effective on the GLM models we consider. Additionally, their work required training a new variational model for every design being approximated, while we propose a method that can amortize over designs. Our closest model to those tested in Foster et al. (2019) is our normal (linear) model, similar to their "AB model"; we apply their variational form for the AB model (given in their appendix) in order to perform a comparison. We set N p = 5 and Σ p = 5I, i.e., diagonal with variance 5.</p>
<p>We generate 50 random designs with N E = 5 experimental units and compare the quality of EIG approximations 3 In this case, the observation noise is included as the standard deviation in the normal distribution that samples y. given by the posterior estimator as well as total wall clock time. The precise architecture and training procedure we use are described in the appendix. Figure 2 shows the results of this experiment: our method produces a much tighter lower bound that is highly correlated with the true values; selecting the highest estimate would pick the design with highest true EIG in the set. In contrast, the variational form used in Foster et al. (2019) yields a much looser and less correlated bound, which would select the 6th best design if used. Moreover, our method is more than 3× faster (293 seconds compared to 920 seconds), showing the benefit of amortizing over designs. In fact, this speed-up understates how much more computationally efficient our method is, given that it leaves us with a model that can estimate the EIG for arbitrarily many designs without additional training. Training took 291 of our method's 293 seconds; evaluating an additional 50 designs, then, would take virtually the same amount of time, compared to double the time required for the non-amortized approach. The non-amortized training is prohibitively slow, while moreover for our other GLM models it is often not clear what variational form from Foster et al. (2019) could be applied; for these reasons, in the rest of the experiments we compare only to standard NMC.</p>
<p>Model Experiments</p>
<p>In our model experiments we vary the GLMs in two ways: the number of predictors (not including the intercept) and the diagonal components of the prior covariance (all offdiagonal terms are zero). The number of predictors N p is varied from 1 to 5 and the diagonal of the prior covariance in {1, 5, 25}. For the neural network architecture we use attention layers for the set encoder, a residual network for the set emitter (He et al. 2016), a full rank Gaussian distribution for the conditional base of the normalzing flow and four affine coupling layers each parameterized with a residual network. Additional details of the architecture and training parame-ters can be found in the appendix. In all experiments we train the variational approximation using the posterior estimator. During training, new designs are generated randomly from a multivariate normal distribution with identity covariance in dimension N p + 1. For final evaluation we generate 50 new random designs and estimate the posterior bound with N = 5000 samples, while the VNMC bounds are estimated with N = 1000 and M = 31 samples and nested samples, and the NMC bounds are estimated with N = 30000 and M = 173 samples and nested samples. The number of samples for VNMC and NMC were selected based on the maximum number of samples that fit into memory (64 GB RAM) for the largest model (multinomial with 5 predictors). Figure 3 shows EIG evaluations for 50 randomly generated designs for the 5 estimators: posterior, VNMC upper, VNMC lower (contrastive VNMC), NMC upper and NMC lower (contrastive NMC). Since this figure pertains to the linear model we can calculate the ground truth EIG exactly, shown in solid black. For visual clarity we sort the designs in order of ground truth EIG value. We see that all estimators perform reasonably well on the easiest form of the model (1 predictor with unit prior covariance). However, even in this case the VNMC bounds (upper and lower) more tightly constrain the ground truth -in fact both are nearly exact. In addition the posterior bound (a lower bound) is consistently above the NMC lower bound and closer to the truth. These trends become magnified as the prior covariance and number of predictors increase. In all cases the VNMC bounds are nearly exact, while the performance of NMC degrades rapidly with problem difficulty. Again, the posterior estimator remains above and closer to truth than the NMC lower bound. Figure 4 shows exactly the same set of experiments, but for the linear unknown model. In this case we can calculate a high-quality Monte Carlo estimate of the ground truth thanks to conjugacy, and sort the designs to be evaluated in order of this ground truth. The results are largely consistent with those from the linear model with known observation noise: the VNMC bounds constrain the ground truth much more tightly than the NMC bounds. However in this case the posterior estimator is only above the NMC lower bound in the two hardest cases (5 predictors and 5 or 25 diagonal covariance). Figures 6, 7, 8, 9, in the appendix, show the same set of experiments but for the logistic, binomial, categorical and multinomial models. In none of these cases can we calculate ground truth, so all plots order their designs by the benchmark NMC (upper). Even without ground truth we still clearly see the lower and upper bounds of VNMC are much closer together and below/above their NMC counterparts in practically all cases. One exception is the logistic model, where as long as the model only contains one predictor variable, the NMC bounds are as tight as the VNMC bounds; however by 5 predictors the VNMC bounds are tighter. In fact the VNMC lower and upper bounds agree with each other in all cases, suggesting they are closely estimating the true EIG. We also see that the VNMC lower and upper bounds are touching across all settings of the binomial model and all but the hardest in the categorical and multino-  Figure 3: Results for estimating EIG in the linear model with known observation noise. The x-axis ranges over the index of 50 randomly selected designs, each with 5 experimental units. Due to the dimensionality, the designs lack a meaningful order; for visual clarity we plot them in the sorted order of the true EIG. The rows vary the number of predictors (N p ∈ {1, 5}) while columns show changes in the diagonal of the prior covariance matrix, {1, 5, 25}, from informative to uninformative. NMC and VNMC methods can estimate both upper and lower bounds, while the posterior estimator only provides a lower bound. Our proposed method gives much tighter bounds on the truth than the competing NMC (with 167× fewer samples). The shading shows one standard deviation of our estimates over 20 runs. mial models (where they are still much tighter than NMC), again suggesting that our VNMC estimators are nearly exact. The results for the posterior estimator are more mixed -sometimes it is above the NMC lower bound and sometimes below. Nevertheless, the variational posterior learned using the posterior estimator can be used to compute VNMC bounds on the EIG that are much tighter than NMC.</p>
<p>Our experiments show that training a single variational posterior, amortizing over designs, we can calculate the EIG much more accurately than the competing NMC benchmark, nearly calculating ground truth exactly. Moreover, the experiments show that training on the posterior estimator can provide a variational distribution that remains effective for estimation using the more costly VNMC bounds (see Section 3). Not only does VNMC provide far more accurate estimates, it does so with many fewer samples -more than two order of magnitudes (167×) fewer samples than NMC.</p>
<p>Architecture Experiments</p>
<p>We next investigate the importance of architectural decisions for the neural networks defining q φ (θ|y, d). We compare using attention layers vs. residual layers in the set encoder, and transform type and number in the normalizing flow. We compare using 4 vs. 8 transforms, and test affine coupling transforms (Dinh, Sohl-Dickstein, and Bengio 2017), rational quadratic (RQ) splines (Durkan et al. 2019), and no transform (just the conditional base distribution). We run all combinations for the linear unknown and binomial model with 5 predictors and 25 on the diagonal of the prior co- variance. Note that the true posterior for the linear unknown model is t-distributed, while the binomial is not analytically expressible, so we expect the use of normalizing flows to be advantageous over just the normal base distribution. The rest of the architecture components are the same as the Model Experiments and full details can be found in the appendix. Figure 5 shows the loss curves of the experiments for the linear unknown model. The inset plot on the top right shows the loss curves across all epochs, while the main plot shows a detail of the last 50 optimization steps. Each optimization step is run on a batch of 50 designs, so this plot indicates final performance on 2500 randomly generated designs. Specifically the loss is − N i=1 log q φ (θ i |y i , d) where y i , θ i ∼ p(y, θ|d) and N = 50 -the cross entropy of the variational posterior. Empirically, we see that using attention layers in the set encoder is the most important architectural decision (lower 5 curves vs upper); all networks using attention layers achieved superior performance to all networks using ResNets regardless of the other architectural settings. Beyond this, we see that using an affine coupling layer is also important, but see little difference between 4 and 8 transform layers. Surprisingly, the RQ transforms perform no better than having no transform. This is because the RQ transforms are restricted to the range of (0, 1), with linear tails outside. Even after training, almost all parameter samples are outside this range by the time they reach the spline; the linear tails effectively skip the flow layers completely, explaining why its performance is comparable to models with no transform layers. In the appendix, Figure 11 shows the performance of a subset of these models at evaluating the EIG for 50 random designs, highlighting that the loss curves' values are directly related to the accuracy and sample efficiency of the EIG estimate. Figures 10 and  12 show the same plots for the binomial model and further support these conclusions.</p>
<p>Conclusion</p>
<p>In this paper we expand on the work of Foster et al. (2019), which proposed variational bounds for estimating the EIG for Bayesian optimal experimental design. In particular we Figure 5: Results for our architecture experiments on the linear unknown model with 5 predictors and diagonal prior covariance 25. We vary the architecture of the set encoder (attention vs. resnet), the normalizing flow transform type (affine coupling, affine spline, or no transform), and the number of transforms (4 or 8). The main plot shows the loss for the posterior estimator over the last 50 steps of training; each step is performed over a batch of 50 random designs (2500 designs total). The inset plot shows the loss curves over all 5000 training steps, indicating all architectures have converged. Further discussion is given in the text.</p>
<p>propose a deep learning architecture incorporating set invariance and conditional normalizing flows that allows us to train a single model capable of estimating the EIG across the design space. Our experiments show that this architecture is highly effective at estimating EIG, and that design amortization provides significant computational speed ups. For cases where ground truth can be calculated, our model's VNMC bounds are nearly exact, while in cases without ground truth our VNMC upper and lower bounds are often sufficiently tight to suggest they are exact. These estimates are significantly more accurate than those of standard NMC while requiring far (167×) fewer samples, as well as far more accurate and efficient than the simpler, non-amortized variational forms used in (Foster et al. 2019). We also demonstrate that we can train our model using the much cheaper posterior estimator bound, with cost O(N ), then evaluate using this fitted model within the more accurate but costly VNMC bounds, O(N M ). Together, we provide a method for faster and more accurate approximation of the EIG across many possible designs. In future work, we plan to extend our approach to design optimization tasks; on this point, we observe that our variational form, q φ (θ|y, d), is differentiable with respect to the designs, d, suggesting it can generalize and potentially improve on the gradient-based design optimization objectives proposed by Foster et al. (2020).</p>
<p>where the variational parameters are φ = {A, Σ} with A being a N E × (N p + 1) matrix and Σ a (N p + 1) × (N p + 1) positive definite matrix. As we can see this form incorporates the design d only implicitly, and cannot amortize over designs. Recall that N E is the number of experimental units and N p is the number of predictors (adding 1 for the intercept). We train using the AdamW optimizer for 5000 steps with a learning rate of .001 with β 0 = .9 and β 1 = .999.</p>
<p>Additional Model Experiment Results</p>
<p>We include result plots from our Model Experiments on the logistic, binomial, categorical and binomial model in Figures 6,7,8,9,respectively. For discussion see the main text. Figure 10 shows the same plot as Figure 5, but for the binomial model. As discussed in the main text, we see that attention layers in the set encoder is the most important architectural decision, followed by the use of affine coupling layers. Figures 11 and 12 show EIG estimates from a subset of the models trained in Figures 5 and 10 for 50 randomly generated designs. We can see that models that achieved lower loss values are correspondingly more accurate for EIG estimation. See further discussion in main text.</p>
<p>Additional Architecture Experiment Results</p>
<p>Full Architecture Details</p>
<p>For the design amortization experiments and the model experiments in sections 5 and 5 we used a common neural network architecture across all models types, which we specify in detail for reproducibility. Note that, between all neural network layers described in the sequel is a ReLU activation function.</p>
<p>For a given experimental design matrix D of size N E × (N p +1) we simulate from the model the expected outcomes y, a vector of size N E . We concatenate these together to construct an input matrix, C, of size N E ×(N p +1+1) to our Set Invariant Model. Each row of this input matrix is then passed through an embedding network, which is a residual network with 2 residual blocks of dimension 64 (we define a residual block as 2 linear layers where the input to the first is added to the output of the second), creating an internal representation R of size N E × 120. We now pass R through 2 attention layers with 12 heads (head dimension is 10). Each attention layer is followed by a dropout layer, then a linear projection with 32 dimensions and another dropout layer; each dropout layer has a dropout probability 0.1. This completes the set encoder, creating an internal representation for each experimental unit. These representations are then passed through the permutation invariant aggregator function, for which we use summation.</p>
<p>Aggregation produces a single vector, regardless of the number of experimental units. This representation is then passed through the Emitter Network of the Set Invariant Model. The Emitter Network is simply a residual network with 2 residual blocks each with linear layers of dimension 128. This concludes the Set Invariant Model creating the design context c y,d .</p>
<p>We next provide the details of our conditional normalizing flow, following the sampling direction from the base distribution to the transforms. For the base distribution, we use a full rank multivariate normal distribution conditioned on the design context c y,d . This distribution is parameterized by a residual network with 2 blocks and linear dimension of size 64. The last layer produces the mean vector of the normal distribution, and the entries of a lower triangular matrix that represents the Cholesky decomposition of the the covariance matrix. This lower triangular matrix is then left-multiplied with its transpose to creating the covariance of the base distribution. We can sample from this distribution straightforwardly.</p>
<p>The samples are then passed through 4 conditional affine coupling layers (unless stated otherwise in the paper). Each affine coupling layer contains a residual neural network with two residual blocks with linear layers of dimension 128. The residual network takes as inputs the samples produced from the base distribution, concatenated with the context c y,d . The network outputs the parameters for an affine transformation that is applied to the samples, which are then passed through a random permutation before the next affine coupling layer. The final affine coupling layer outputs the samples θ from our variational model q φ (θ|y, d).</p>
<p>The forward procedure produces samples from our variational model, q(θ|y, d). In order to evaluate q(θ|y, d) on samples simulated from the model, p(y, θ|d), we simply pass yandd through the set invariant model to compute the design context, c y,d , and then pass the simulated samples θ back through the inverse of the affine coupling layers to the base distribution, which can be evaluated. This now defines both directions of our variational model. This completes the architecture details used in the Model Experiments. In the architecture experiments, we use the same architectural skeleton, but alter it as necessary. Specifically, we test using a ResNet within the Set Encoder instead of the attention layers; this ResNet consists of 4 residual blocks with linear layers of dimension 64. We also test using rational quadratic coupling splines in place of the affine coupling layers; these spline transformations also use ResNets with 2 residual blocks of 128 dimensions for the linear layers which output the parameters of an RQ spline with 20 buckets and linear tails.</p>
<p>All models are trained with the AdamW optimizer with a learning rate of 5 × 10 −4 and β 0 = 0.9 and β 1 = 0.999 for 5000 steps, where each step consists of a batch of 50 randomly generated designs. During training we use the posterior estimator to define the loss, and set N = 50 for the Monte Carlo estimator. We found no meaningful variation on the random seeds.  Logistic Figure 6: Same as figure 3, but for the logistic model. For this model we cannot compute ground truth, but we can still infer superior performance for our methods by observing how VNMC produces tighter bounds, both lower and upper compared to NMC. Designs are ordered on the x-axis via the baseline NMC.       figure 5 provide meaningful difference in the quality of EIG approximation. For a subset of the architectures trained we estimate the two VNMC based and the posterior bound using the same number of samples. We can see that the architectures that achieved lower loss values during training achieve greater estimation accuracy given the number of samples. Figure 12: Same as in figure 11, but for the binomial model.</p>
<p>Figure 2 :
2Comparing our method and the (non-amortized) variational form used by Foster et al. (2019) on the normal (linear) model with 5 predictors and 5 on the diagonal of the prior covariance (similar to the AB model in Foster et al. (2019)). For clarity we only show the posterior estimator values. Our method is &gt; 3× faster (293s vs. 920s) and significantly more accurate. See text for further analysis.</p>
<p>Figure 4 :
4Same asFigure 3, but for the linear model with unknown observation noise.</p>
<p>Figure 7 :
7Same as figure 3, but for the binomial model. For this model we cannot compute ground truth, but we can still infer superior performance for our methods by looking at how VNMC produces much tighter bounds, both lower and upper.</p>
<p>Figure 8 :Figure 9 :
89Same as figure 3, but for the categorical model. For this model we cannot compute ground truth, but we can still infer superior performance for our methods by looking at how VNMC produces much tighter bounds, both lower and upper. Same as figure 3, but for the multinomial model. For this model we cannot compute ground truth, but we can still infer superior performance for our methods by looking at how VNMC produces much tighter bounds, both lower and upper.</p>
<p>Figure 10 :
10Same as figure 5, but for the binomial model.</p>
<p>Figure 11 :
11Here we show that the loss values in</p>
<p>Design (Sorted by nmc)1.0 </p>
<p>1.2 </p>
<p>1.4 </p>
<p>1.6 </p>
<p>EIG(Design) </p>
<p>Num Predictors=1 </p>
<p>Prior Covariance=1.0 </p>
<p>lower_nmc 
upper_nmc 
lower_v_nmc 
upper_v_nmc 
post </p>
<p>1.6 </p>
<p>1.8 </p>
<p>2.0 </p>
<p>2.2 </p>
<p>2.4 </p>
<p>Prior Covariance=5.0 </p>
<p>1.75 </p>
<p>2.00 </p>
<p>2.25 </p>
<p>2.50 </p>
<p>2.75 </p>
<p>3.00 </p>
<p>Prior Covariance=25.0 </p>
<p>Design (Sorted by nmc) </p>
<p>1.8 </p>
<p>2.0 </p>
<p>2.2 </p>
<p>2.4 </p>
<p>2.6 </p>
<p>2.8 </p>
<p>EIG(Design) </p>
<p>Num Predictors=5 </p>
<p>redacted for anonymity during review.
Appendix Full Amortization Experiment DetailsThe full details of the architecture we used for our method and training parameters are described in the Appendix under "Full Architecture Details". For amortization comparison we adopted the variational form used for the AB model in(Foster et al. 2019)as it is the most similar to the model that we test on. The variational form is: q φ (θ|y, d) = N (Ay, Σ).(12)
Improving the manufacturing process quality using design of experiments: a case study. J Antony, International Journal of Operations &amp; Production Management. Antony, J. 2001. Improving the manufacturing process qual- ity using design of experiments: a case study. International Journal of Operations &amp; Production Management.</p>
<p>The IM Algorithm: A Variational Approach to Information Maximization. D Barber, F V Agakov, NIPS. Barber, D.; and Agakov, F. V. 2003. The IM Algorithm: A Variational Approach to Information Maximization. In NIPS, 201-208.</p>
<p>Bayesian theory. J M Bernardo, A F Smith, John Wiley &amp; Sons405Bernardo, J. M.; and Smith, A. F. 2009. Bayesian theory, volume 405. John Wiley &amp; Sons.</p>
<p>Pyro: Deep Universal Probabilistic Programming. E Bingham, J P Chen, M Jankowiak, F Obermeyer, N Pradhan, T Karaletsos, R Singh, P A Szerlip, P Horsfall, N D Goodman, J. Mach. Learn. Res. 206Bingham, E.; Chen, J. P.; Jankowiak, M.; Obermeyer, F.; Pradhan, N.; Karaletsos, T.; Singh, R.; Szerlip, P. A.; Hors- fall, P.; and Goodman, N. D. 2019. Pyro: Deep Universal Probabilistic Programming. J. Mach. Learn. Res., 20: 28:1- 28:6.</p>
<p>Probabilistic Symmetries and Invariant Neural Networks. B Bloem-Reddy, Y W Teh, J. Mach. Learn. Res. 21Bloem-Reddy, B.; and Teh, Y. W. 2020. Probabilistic Sym- metries and Invariant Neural Networks. J. Mach. Learn. Res., 21: 90-1.</p>
<p>Bayesian experimental design: A review. K Chaloner, I Verdinelli, Statistical Science. Chaloner, K.; and Verdinelli, I. 1995. Bayesian experimental design: A review. Statistical Science, 273-304.</p>
<p>Bayesian geostatistical design. P Diggle, S Lophaven, Scandinavian Journal of Statistics. 331Diggle, P.; and Lophaven, S. 2006. Bayesian geostatistical design. Scandinavian Journal of Statistics, 33(1): 53-64.</p>
<p>Density estimation using Real NVP. L Dinh, J Sohl-Dickstein, S Bengio, 5th International Conference on Learning Representations. Toulon, FranceConference Track Proceedings. OpenReview. netDinh, L.; Sohl-Dickstein, J.; and Bengio, S. 2017. Density estimation using Real NVP. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenRe- view.net.</p>
<p>. C Durkan, A Bekasov, I Murray, G Papamakarios, Durkan, C.; Bekasov, A.; Murray, I.; and Papamakarios, G.</p>
<p>H Neural Spline Flows ; Wallach, H Larochelle, A Beygelzimer, F ; D&apos;alché-Buc, E Fox, R Garnett, Advances in Neural Information Processing Systems. Curran Associates, Inc32Neural Spline Flows. In Wallach, H.; Larochelle, H.; Beygelzimer, A.; d'Alché-Buc, F.; Fox, E.; and Garnett, R., eds., Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc.</p>
<p>C Durkan, A Bekasov, I Murray, G Papamakarios, nflows: normalizing flows in PyTorch. Durkan, C.; Bekasov, A.; Murray, I.; and Papamakarios, G. 2020. nflows: normalizing flows in PyTorch.</p>
<p>A unified stochastic gradient approach to designing bayesian-optimal experiments. A Foster, M Jankowiak, E Bingham, P Horsfall, Y W Teh, T Rainforth, N Goodman, H Wallach, H Larochelle, A Beygelzimer, F ; D&apos;alché-Buc, E Fox, R Garnett, M Jankowiak, M O&apos;meara, Y W Teh, T Rainforth, PMLRInternational Conference on Artificial Intelligence and Statistics. Curran Associates, Inc. Foster32Advances in Neural Information Processing SystemsFoster, A.; Jankowiak, M.; Bingham, E.; Horsfall, P.; Teh, Y. W.; Rainforth, T.; and Goodman, N. 2019. Variational Bayesian Optimal Experimental Design. In Wallach, H.; Larochelle, H.; Beygelzimer, A.; d'Alché-Buc, F.; Fox, E.; and Garnett, R., eds., Advances in Neural Information Pro- cessing Systems, volume 32. Curran Associates, Inc. Foster, A.; Jankowiak, M.; O'Meara, M.; Teh, Y. W.; and Rainforth, T. 2020. A unified stochastic gradient approach to designing bayesian-optimal experiments. In International Conference on Artificial Intelligence and Statistics, 2959- 2969. PMLR.</p>
<p>Optimal design of experiments: a case study approach. P Goos, B Jones, John Wiley &amp; SonsGoos, P.; and Jones, B. 2011. Optimal design of experi- ments: a case study approach. John Wiley &amp; Sons.</p>
<p>Deep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionHe, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep resid- ual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recogni- tion, 770-778.</p>
<p>Normalizing flows: An introduction and review of current methods. I Kobyzev, S J Prince, M A Brubaker, IEEE transactions on pattern analysis and machine intelligence. 43Kobyzev, I.; Prince, S. J.; and Brubaker, M. A. 2020. Nor- malizing flows: An introduction and review of current meth- ods. IEEE transactions on pattern analysis and machine intelligence, 43(11): 3964-3979.</p>
<p>Maximizing the information content of experiments in systems biology. J Liepe, S Filippi, M Komorowski, M P Stumpf, PLoS computational biology. 911002888Liepe, J.; Filippi, S.; Komorowski, M.; and Stumpf, M. P. 2013. Maximizing the information content of experiments in systems biology. PLoS computational biology, 9(1): e1002888.</p>
<p>Generalized Linear Models. P Mccullagh, J Nelder, Hall/CRC Monographs on Statistics &amp; Applied Probability. Taylor &amp; Francis. ISBN 9780412317606. McCullagh, P.; and Nelder, J. 1989. Generalized Linear Models, Second Edition. Chapman &amp; Hall/CRC Mono- graphs on Statistics &amp; Applied Probability. Taylor &amp; Fran- cis. ISBN 9780412317606.</p>
<p>A tutorial on adaptive design optimization. J I Myung, D R Cavagnaro, M A Pitt, Journal of mathematical psychology. 573-4Myung, J. I.; Cavagnaro, D. R.; and Pitt, M. A. 2013. A tutorial on adaptive design optimization. Journal of mathe- matical psychology, 57(3-4): 53-67.</p>
<p>Monte Carlo theory, methods and examples. A B Owen, Owen, A. B. 2013. Monte Carlo theory, methods and exam- ples.</p>
<p>Normalizing Flows for Probabilistic Modeling and Inference. G Papamakarios, E T Nalisnick, D J Rezende, S Mohamed, B Lakshminarayanan, J. Mach. Learn. Res. 2257Papamakarios, G.; Nalisnick, E. T.; Rezende, D. J.; Mo- hamed, S.; and Lakshminarayanan, B. 2021. Normalizing Flows for Probabilistic Modeling and Inference. J. Mach. Learn. Res., 22(57): 1-64.</p>
<p>On nesting monte carlo estimators. T Rainforth, R Cornish, H Yang, A Warrington, F Wood, PMLRInternational Conference on Machine Learning. Rainforth, T.; Cornish, R.; Yang, H.; Warrington, A.; and Wood, F. 2018. On nesting monte carlo estimators. In In- ternational Conference on Machine Learning, 4267-4276. PMLR.</p>
<p>Monte Carlo statistical methods. C P Robert, G Casella, Springer2Robert, C. P.; and Casella, G. 1999. Monte Carlo statistical methods, volume 2. Springer.</p>
<p>A review of modern computational algorithms for Bayesian optimal design. International Statistical Review. E G Ryan, C C Drovandi, J M Mcgree, A N Pettitt, 84Ryan, E. G.; Drovandi, C. C.; McGree, J. M.; and Pettitt, A. N. 2016. A review of modern computational algorithms for Bayesian optimal design. International Statistical Re- view, 84(1): 128-154.</p>
<p>Maximum entropy sampling and optimal Bayesian experimental design. P Sebastiani, H P Wynn, Journal of the Royal Statistical Society: Series B (Statistical Methodology). 621Sebastiani, P.; and Wynn, H. P. 2000. Maximum entropy sampling and optimal Bayesian experimental design. Jour- nal of the Royal Statistical Society: Series B (Statistical Methodology), 62(1): 145-157.</p>
<p>Attention is All you Need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, L U Kaiser, I Polosukhin, I Guyon, U V Luxburg, S Bengio, H Wallach, R Fergus, S Vishwanathan, R Garnett, Advances in Neural Information Processing Systems. Curran Associates, Inc30Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, L. u.; and Polosukhin, I. 2017. Attention is All you Need. In Guyon, I.; Luxburg, U. V.; Bengio, S.; Wallach, H.; Fergus, R.; Vishwanathan, S.; and Garnett, R., eds., Advances in Neural Information Process- ing Systems, volume 30. Curran Associates, Inc.</p>
<p>The DARC Toolbox: automated, flexible, and efficient delayed and risky choice experiments using Bayesian adaptive design. B T Vincent, T Rainforth, Psyarxiv, M Zaheer, S Kottur, S Ravanbakhsh, B Poczos, R R Salakhutdinov, A J Smola, I Guyon, U V Luxburg, S Bengio, H Wallach, R Fergus, S Vishwanathan, R Garnett, Advances in Neural Information Processing Systems. Curran Associates, Inc30Deep SetsVincent, B. T.; and Rainforth, T. 2017. The DARC Toolbox: automated, flexible, and efficient delayed and risky choice experiments using Bayesian adaptive design. PsyArXiv. Zaheer, M.; Kottur, S.; Ravanbakhsh, S.; Poczos, B.; Salakhutdinov, R. R.; and Smola, A. J. 2017. Deep Sets. In Guyon, I.; Luxburg, U. V.; Bengio, S.; Wallach, H.; Fer- gus, R.; Vishwanathan, S.; and Garnett, R., eds., Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-481 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-481</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-481</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-18.html">extraction-schema-18</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <p><strong>Paper ID:</strong> paper-236976307</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2108.05198v2.pdf" target="_blank">Natural Language-Guided Programming</a></p>
                <p><strong>Paper Abstract:</strong> In today's software world with its cornucopia of reusable software libraries, when a programmer is faced with a programming task that they suspect can be completed through the use of a library, they often look for code examples using a search engine and then manually adapt found examples to their specific context of use. We put forward a vision based on a new breed of developer tools that have the potential to largely automate this process. The key idea is to adapt code autocompletion tools such that they take into account not only the developer's already-written code but also the intent of the task the developer is trying to achieve next, formulated in plain natural language. We call this practice of enriching the code with natural language intent to facilitate its completion natural language-guided programming. To show that this idea is feasible we design, implement and benchmark a tool that solves this problem in the context of a specific domain (data science) and a specific programming language (Python). Central to the tool is the use of language models trained on a large corpus of documented code. Our initial experiments confirm the feasibility of the idea but also make it clear that we have only scratched the surface of what may become possible in the future. We end the paper with a comprehensive research agenda to stimulate additional research in the budding area of natural language-guided programming.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e481.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e481.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Metric mismatch (BLEU/IoU vs human)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mismatch between automated code-similarity metrics (BLEU, IoU) and human usefulness judgments</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper identifies that standard string- and token-based metrics (BLEU, IoU/Jaccard) do not perfectly reflect human judgments of predicted code usefulness; IoU correlates better than BLEU but still systematically under- and over-estimates usefulness in particular patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>NLGP assistant evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Human evaluation and automatic-metric evaluation of code predictions produced by three GPT-2–based NLGP models on a curated Jupyter-notebook benchmark (201 cases).</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>inline comments / short natural-language intents</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>Jupyter notebook code cells / predicted code snippets</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>metric misalignment / evaluation metric inadequacy</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Automated metrics (BLEU and IoU) that compare predicted code to a single reference target often disagree with expert human judgments of usefulness. IoU correlates more strongly with human usefulness than BLEU, but IoU still underestimates usefulness when predicted code contains extra (but relevant) statements, when predictions use different but correct syntactic formulations, or when missing import statements disproportionately affect token overlap.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>evaluation metrics / benchmark scoring</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>human evaluation study (three expert annotators) with correlation analysis between metric scores and human usefulness ratings; manual inspection of mismatch cases</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Computed BLEU and Intersect-over-Union (IoU/Jaccard) on tree-sitter tokenization; Pearson correlation between metric scores and human-assigned 'usefulness' (converted to 0-1); manual analysis of 30 high-discrepancy cases</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Automated metrics provide an imperfect proxy for human usefulness; IoU shows higher Pearson correlation with human usefulness (e.g., across models IoU,H ≈ 0.69) than BLEU (BLEU,H ≈ 0.63), but systematic mismatches mean relying solely on these metrics can mis-rank models or undercount useful predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Across all models IoU had higher correlation overall (IoU,H ≈ 0.69) than BLEU (BLEU,H ≈ 0.63); manual inspection of 30 large-discrepancy cases found 12/30 (40%) where extra-but-relevant code reduced IoU, 7/30 (23%) where syntactically different-but-correct predictions caused mismatch, and 6/30 (20%) where missing imports dominated the metric difference.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Metrics rely on surface syntactic/token overlap and single-reference targets; they do not capture semantic equivalence, additional useful statements, or the lesser importance of auxiliary tokens (like imports) to human judges.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Use human evaluation for calibration; prefer IoU over BLEU for this domain; explore new metrics better tailored to code semantics (e.g., API-aware measures, multi-reference targets, executable/semantic testing); manual analysis to identify failure modes.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Partial: IoU correlated better than BLEU (reported IoU,H ≈ 0.69 vs BLEU,H ≈ 0.63 across models). Manual identification of failure patterns clarified causes but no fully automatic superior metric was produced in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>code generation / software engineering / machine learning</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Natural Language-Guided Programming', 'publication_date_yy_mm': '2021-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e481.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e481.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Non-executable online examples</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Non-executable and obsolete online code examples (e.g., Stack Overflow snippets)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior empirical studies report that a large fraction of code snippets found in online documentation and Q&A are not immediately executable or are obsolete, creating a gap between natural language examples/documentation and working code.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>From Query to Usable Code: An Analysis of Stack Overflow Code Snippets</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Example embedding / documentation mining</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Mining code examples from online documentation and Q&A (Stack Overflow) as sources of natural-language–annotated code for training or reuse.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>documentation and Q&A text (explanatory comments / examples)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>documentation code snippets / Stack Overflow code examples</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>non-executable / obsolete examples / incomplete snippets</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Many code examples found in documentation or on Q&A sites are not immediately usable: they may be partial, omit necessary imports/setup, rely on older API versions, or contain errors, causing a mismatch between the documented intent and runnable code.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>example sources / documentation / knowledge bases</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>cited prior empirical study that attempted to execute mined snippets and measured executability</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Execution attempt of mined code snippets and reporting percentage executable; cited figure of 25.61% of Python snippets being immediately executable (from prior study)</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Using such examples as training data or as copy-paste sources for developers increases the chance of runtime errors, incompatibilities, and incorrect model learning; it undermines automated example embedding and reproducibility.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Prior study reported only 25.61% of Python snippets were readily executable; other studies report high rates of obsolescence in Stack Overflow answers (examples cited in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Documentation and community examples are often partial, intended for exposition rather than full reuse; APIs evolve and examples are not updated; examples omit environmental/setup details.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Prefer curated datasets; inject or synthesize missing context (e.g., imports) when feasible; use docstring-injection and cleanup heuristics; encourage better documentation practices.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Not quantitatively demonstrated in this paper; paper uses heuristics (nbconvert, comment processing) and docstring injection to partially address missing-intent annotations but does not resolve executability rate of external snippets.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>software engineering / program synthesis / NLP-for-code</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Natural Language-Guided Programming', 'publication_date_yy_mm': '2021-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e481.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e481.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Missing/ambiguous intent in code</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Lack of explicit natural-language intent in code (missing or ambiguous comments/docstrings)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Not all code is documented with inline comments or docstrings describing developer intent, which limits training and inference for NLGP systems that rely on natural-language intent paired with code.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>NLGP model training pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Training data construction for NLGP models using Jupyter notebooks where natural comments and markdown cells serve as intent signals.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>inline comments and markdown cell text (natural comments)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>Jupyter notebook code cells / .py conversions</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>missing or insufficient natural-language annotations</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Large portions of code lack explicit intent annotations; when comments are absent the model must infer intent from code context alone, which degrades performance. The authors therefore explored a 'docstring injection' synthetic approach to add intent labels, but this is an approximation and only partially effective.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>training data (annotation) and at inference when user provides intent</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Comparison of models trained on datasets with no comments vs injected docstrings vs natural comments; empirical evaluation on benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Human evaluation scores (usefulness, coverage, precision, compatibility) and metric scores (BLEU/IoU) across three models (no-comments, docstring, natural).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Models trained with natural comments performed best overall (higher human-perceived usefulness and compatibility); no-comments model performed substantially worse (BLEU 0.06, IoU 0.27, lower human scores).</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Training dataset initially had comments removed for some experiments; docstring injection resolved ~51.3% of call-site resolutions and docstring titles were available for 64.6% of callable entities encountered in crawling, indicating substantial coverage gaps.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Source repositories often lack sufficient inline comments/docstrings aligned with call sites; dynamic typing and incomplete docstrings make automatic alignment hard.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Docstring injection (synthetic annotations): resolve call sites to entities and inject the first-sentence docstring as an inline comment for a sampled subset (20% sampling in experiments); also train on natural comments where available.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Docstring-injected model outperformed no-comments baseline and was close to natural-comments model on several metrics; natural model still had better intent coverage. Docstring injection resolved only a subset of call-sites (51.3%), limiting its reach.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning for code / data science tooling</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Natural Language-Guided Programming', 'publication_date_yy_mm': '2021-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e481.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e481.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Limited context window / missing imports</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Model context window limitations leading to missing imports and incorrect predictions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Transformer model context length limits cause relevant parts of a user's codebase (e.g., import statements or function definitions) to fall outside the window, producing predictions that call APIs not present or miss required imports.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GPT-2–based NLGP assistant (max context 700 tokens in experiments, GPT-2 default 1024)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Language-model-based code autocompletion that conditions on previous tokens up to a fixed context window (700 token max used in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>inline intent comments / surrounding code context</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>notebook-derived Python code context / predicted code</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>context truncation causing omitted preprocessing or imports / incomplete context</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Because the model only sees a bounded window of prior tokens, import statements and definitions required for correct completions sometimes lie outside the window, causing the model to produce predictions that reference unavailable functions or omit required imports.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>model input context selection / inference-time context window</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Error analysis of model outputs and observation that predictions sometimes propose function calls that do not exist in context; reasoning about training where model saw calls without imports due to truncated context.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Qualitative error analysis and anecdotal examples; no single numeric metric but authors note 'a significant proportion' of mistakes related to dropped context and provide concrete examples and discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Leads to incorrect or non-runnable predictions, reducing usefulness and potentially harming developer trust; contributes to lower coverage/precision scores observed in human evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Described as a significant proportion of model mistakes (no exact percentage given); inference context length set to 700 tokens in experiments to keep latency acceptable.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Transformer quadratic scaling limits practical sequence length; lack of selection strategy for which parts of a long file to include in context.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Proposed strategies: smarter context selection heuristics, more efficient tokenizations, linear/subquadratic transformer variants, and including precise library version info so the model can tailor suggestions.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Not evaluated quantitatively in this paper; authors report it as an open research direction and give anecdotal rationale that these approaches should reduce such errors.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>deep learning for code / developer tooling</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Natural Language-Guided Programming', 'publication_date_yy_mm': '2021-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e481.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e481.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>API versioning / obsolete API bias</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bias toward older or more frequent API versions in model predictions leading to obsolete suggestions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Training on existing code repositories biases model predictions toward the API usage patterns present in the training data, which may be older or more common versions, producing suggestions that are incompatible with a developer's current library versions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>NLGP assistant trained on harvested GitHub notebooks</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Language models trained on publicly available notebooks and code without explicit per-repository dependency/version metadata.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>documentation strings / inline comments / example usage in corpora</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>library API call patterns in training code vs developer runtime environment</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>version mismatch / obsolete API usage</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>The model can suggest API calls or call signatures corresponding to older or more common versions present in the training data; without knowledge of the specific versions used in the user's environment, suggestions may be obsolete or incompatible.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>model training data bias / suggestion tailoring to environment</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Conceptual analysis and referencing prior findings about obsolete Stack Overflow answers; discussion of limitations rather than a specific experimental quantification in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>No direct quantitative measurement in this paper; authors cite external studies on Stack Overflow obsolescence and discuss risk.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Potential to produce suggestions that fail when executed in a user's environment; undermines reliability and reproducibility of automatically suggested code.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Prior study cited notes high rates of obsolescence in community answers (e.g., only 20.5% of a sample of known-obsolete answers were updated); no direct prevalence number from the authors' dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Training corpora do not include explicit package version metadata; APIs evolve and examples are not updated.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Augment NLGP tools with precise versions of libraries used by the developer so suggestions can be tailored to the developer's environment; prefer up-to-date sources and curated examples.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Not evaluated quantitatively in this work; proposed as an important future direction.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>software engineering / ML-driven developer tools</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Natural Language-Guided Programming', 'publication_date_yy_mm': '2021-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e481.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e481.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Docstring-injection limits</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Limitations of docstring-injection for synthetic intent annotation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper introduces a docstring-injection method to synthesize intent comments at call sites but reports partial coverage and inaccuracies due to Python's dynamic nature and docstring availability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>docstring-injection preprocessing pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Automated pipeline that resolves callable entities in training code to harvested docstring titles and inserts them as inline comments at a sampled fraction of resolved call sites.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>injected docstring titles as synthetic inline comments</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>training .py files converted from notebooks; AST-resolved call sites</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>incomplete resolution / partial coverage of call sites</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Because Python is dynamically typed, static resolution only succeeds for a subset of call sites; the pipeline resolved 51.3% of call sites to entity names and harvested docstrings existed for 64.6% of visited callable entities, so many calls remain unannotated and injection cannot fully substitute for natural comments.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>training-data annotation stage (call-site resolution and docstring availability)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Automated AST-based flow analysis to resolve call sites and counting resolved vs unresolved sites; reporting of coverage statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Reported statistics: 51.3% of call sites resolved to one of the stored entity names; mapping built that captured docstrings for 64.6% of visited callable entities; docstring injection sampling rate used = 20%.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Docstring-injected model outperforms no-comments baseline but remains behind or close to natural-comments model in some metrics; limited resolution reduces the reach and realism of synthetic intents.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Resolution succeeded for ~51.3% of call sites; docstring titles available for ~64.6% of entities visited by crawler; injection performed on a sampled 20% of resolved call sites in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Dynamic typing and name binding in Python inhibit precise static resolution; many libraries lack docstrings or have terse docstrings not aligned with call-site intent.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Improve static analysis for better resolution, increase coverage of docstring harvesting, vary sampling rates, combine natural comments with injected docstrings, and refine injected text to better match developer intent.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Partial: docstring-injected model gave substantial gains over no-comments model (per human ratings and BLEU/IoU), but did not fully match natural-comments model; exact numeric improvements presented in evaluation (e.g., BLEU and IoU differences in Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>ML for code / dataset curation</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Natural Language-Guided Programming', 'publication_date_yy_mm': '2021-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e481.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e481.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Annotation variability</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Inter-annotator disagreement and subjectivity in benchmark creation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Human curation of context/intent/target triplets exhibits moderate agreement and measurable variability that affects the benchmark and interpretation of evaluation outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>NLGP benchmark creation and curation</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Mining candidate context-intent-target triplets from evaluation split of notebooks and human curation with three reviewers per sample to produce a 201-case benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>inline comments sampled as intents and human-rewritten intents</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>target code snippets extracted from notebooks</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>curation variability / ambiguous selection of intent and target</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Annotators sometimes disagree on whether a comment expresses a suitable intent or which lines of subsequent code address the intent; differences in accepted samples, reformulations, and selected target lines introduce benchmark noise.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>benchmark curation and labeling</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Inter-annotator agreement statistics computed during curation; measurement of matching rates and kappa statistic.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Fleiss' kappa for accept/skip decision = 0.515 (moderate agreement); among accepted candidates annotators had identical intent in 74% of cases and identical target code selection in 71%; average edit distance for differing intents = 3.3 tokens; average difference in target code = 3.2 lines.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Benchmark contains some subjectivity and label noise which can affect model evaluation and comparisons; small benchmark size (201) amplifies impact of annotation variability on statistical conclusions.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Moderate agreement: kappa = 0.515; exact identity rates: 74% same intent, 71% same target among accepted cases.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Ambiguity in natural-language intents and multiple valid code ways to satisfy a short intent; annotator subjective choices when reformulating or selecting minimal target code.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Use multiple annotators and consensus rules (accept when 2/3 agree and selections differ by ≤2 lines); provide detailed annotation guidelines and interface; increase benchmark size and diversify annotators in future work.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Effective to some extent: these rules reduced noisy samples and produced a 201-sample benchmark, but residual variability remains and is acknowledged as limitation.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>dataset curation / human annotation for ML</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Natural Language-Guided Programming', 'publication_date_yy_mm': '2021-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>From Query to Usable Code: An Analysis of Stack Overflow Code Snippets <em>(Rating: 2)</em></li>
                <li>An Empirical Study of Obsolete Answers on Stack Overflow <em>(Rating: 2)</em></li>
                <li>BLEU: A Method for Automatic Evaluation of Machine Translation <em>(Rating: 2)</em></li>
                <li>Evaluating Large Language Models Trained on Code <em>(Rating: 2)</em></li>
                <li>On the Naturalness of Software <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-481",
    "paper_id": "paper-236976307",
    "extraction_schema_id": "extraction-schema-18",
    "extracted_data": [
        {
            "name_short": "Metric mismatch (BLEU/IoU vs human)",
            "name_full": "Mismatch between automated code-similarity metrics (BLEU, IoU) and human usefulness judgments",
            "brief_description": "The paper identifies that standard string- and token-based metrics (BLEU, IoU/Jaccard) do not perfectly reflect human judgments of predicted code usefulness; IoU correlates better than BLEU but still systematically under- and over-estimates usefulness in particular patterns.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "NLGP assistant evaluation",
            "system_description": "Human evaluation and automatic-metric evaluation of code predictions produced by three GPT-2–based NLGP models on a curated Jupyter-notebook benchmark (201 cases).",
            "nl_description_type": "inline comments / short natural-language intents",
            "code_implementation_type": "Jupyter notebook code cells / predicted code snippets",
            "gap_type": "metric misalignment / evaluation metric inadequacy",
            "gap_description": "Automated metrics (BLEU and IoU) that compare predicted code to a single reference target often disagree with expert human judgments of usefulness. IoU correlates more strongly with human usefulness than BLEU, but IoU still underestimates usefulness when predicted code contains extra (but relevant) statements, when predictions use different but correct syntactic formulations, or when missing import statements disproportionately affect token overlap.",
            "gap_location": "evaluation metrics / benchmark scoring",
            "detection_method": "human evaluation study (three expert annotators) with correlation analysis between metric scores and human usefulness ratings; manual inspection of mismatch cases",
            "measurement_method": "Computed BLEU and Intersect-over-Union (IoU/Jaccard) on tree-sitter tokenization; Pearson correlation between metric scores and human-assigned 'usefulness' (converted to 0-1); manual analysis of 30 high-discrepancy cases",
            "impact_on_results": "Automated metrics provide an imperfect proxy for human usefulness; IoU shows higher Pearson correlation with human usefulness (e.g., across models IoU,H ≈ 0.69) than BLEU (BLEU,H ≈ 0.63), but systematic mismatches mean relying solely on these metrics can mis-rank models or undercount useful predictions.",
            "frequency_or_prevalence": "Across all models IoU had higher correlation overall (IoU,H ≈ 0.69) than BLEU (BLEU,H ≈ 0.63); manual inspection of 30 large-discrepancy cases found 12/30 (40%) where extra-but-relevant code reduced IoU, 7/30 (23%) where syntactically different-but-correct predictions caused mismatch, and 6/30 (20%) where missing imports dominated the metric difference.",
            "root_cause": "Metrics rely on surface syntactic/token overlap and single-reference targets; they do not capture semantic equivalence, additional useful statements, or the lesser importance of auxiliary tokens (like imports) to human judges.",
            "mitigation_approach": "Use human evaluation for calibration; prefer IoU over BLEU for this domain; explore new metrics better tailored to code semantics (e.g., API-aware measures, multi-reference targets, executable/semantic testing); manual analysis to identify failure modes.",
            "mitigation_effectiveness": "Partial: IoU correlated better than BLEU (reported IoU,H ≈ 0.69 vs BLEU,H ≈ 0.63 across models). Manual identification of failure patterns clarified causes but no fully automatic superior metric was produced in this paper.",
            "domain_or_field": "code generation / software engineering / machine learning",
            "reproducibility_impact": true,
            "uuid": "e481.0",
            "source_info": {
                "paper_title": "Natural Language-Guided Programming",
                "publication_date_yy_mm": "2021-08"
            }
        },
        {
            "name_short": "Non-executable online examples",
            "name_full": "Non-executable and obsolete online code examples (e.g., Stack Overflow snippets)",
            "brief_description": "Prior empirical studies report that a large fraction of code snippets found in online documentation and Q&A are not immediately executable or are obsolete, creating a gap between natural language examples/documentation and working code.",
            "citation_title": "From Query to Usable Code: An Analysis of Stack Overflow Code Snippets",
            "mention_or_use": "mention",
            "system_name": "Example embedding / documentation mining",
            "system_description": "Mining code examples from online documentation and Q&A (Stack Overflow) as sources of natural-language–annotated code for training or reuse.",
            "nl_description_type": "documentation and Q&A text (explanatory comments / examples)",
            "code_implementation_type": "documentation code snippets / Stack Overflow code examples",
            "gap_type": "non-executable / obsolete examples / incomplete snippets",
            "gap_description": "Many code examples found in documentation or on Q&A sites are not immediately usable: they may be partial, omit necessary imports/setup, rely on older API versions, or contain errors, causing a mismatch between the documented intent and runnable code.",
            "gap_location": "example sources / documentation / knowledge bases",
            "detection_method": "cited prior empirical study that attempted to execute mined snippets and measured executability",
            "measurement_method": "Execution attempt of mined code snippets and reporting percentage executable; cited figure of 25.61% of Python snippets being immediately executable (from prior study)",
            "impact_on_results": "Using such examples as training data or as copy-paste sources for developers increases the chance of runtime errors, incompatibilities, and incorrect model learning; it undermines automated example embedding and reproducibility.",
            "frequency_or_prevalence": "Prior study reported only 25.61% of Python snippets were readily executable; other studies report high rates of obsolescence in Stack Overflow answers (examples cited in paper).",
            "root_cause": "Documentation and community examples are often partial, intended for exposition rather than full reuse; APIs evolve and examples are not updated; examples omit environmental/setup details.",
            "mitigation_approach": "Prefer curated datasets; inject or synthesize missing context (e.g., imports) when feasible; use docstring-injection and cleanup heuristics; encourage better documentation practices.",
            "mitigation_effectiveness": "Not quantitatively demonstrated in this paper; paper uses heuristics (nbconvert, comment processing) and docstring injection to partially address missing-intent annotations but does not resolve executability rate of external snippets.",
            "domain_or_field": "software engineering / program synthesis / NLP-for-code",
            "reproducibility_impact": true,
            "uuid": "e481.1",
            "source_info": {
                "paper_title": "Natural Language-Guided Programming",
                "publication_date_yy_mm": "2021-08"
            }
        },
        {
            "name_short": "Missing/ambiguous intent in code",
            "name_full": "Lack of explicit natural-language intent in code (missing or ambiguous comments/docstrings)",
            "brief_description": "Not all code is documented with inline comments or docstrings describing developer intent, which limits training and inference for NLGP systems that rely on natural-language intent paired with code.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "NLGP model training pipeline",
            "system_description": "Training data construction for NLGP models using Jupyter notebooks where natural comments and markdown cells serve as intent signals.",
            "nl_description_type": "inline comments and markdown cell text (natural comments)",
            "code_implementation_type": "Jupyter notebook code cells / .py conversions",
            "gap_type": "missing or insufficient natural-language annotations",
            "gap_description": "Large portions of code lack explicit intent annotations; when comments are absent the model must infer intent from code context alone, which degrades performance. The authors therefore explored a 'docstring injection' synthetic approach to add intent labels, but this is an approximation and only partially effective.",
            "gap_location": "training data (annotation) and at inference when user provides intent",
            "detection_method": "Comparison of models trained on datasets with no comments vs injected docstrings vs natural comments; empirical evaluation on benchmark",
            "measurement_method": "Human evaluation scores (usefulness, coverage, precision, compatibility) and metric scores (BLEU/IoU) across three models (no-comments, docstring, natural).",
            "impact_on_results": "Models trained with natural comments performed best overall (higher human-perceived usefulness and compatibility); no-comments model performed substantially worse (BLEU 0.06, IoU 0.27, lower human scores).",
            "frequency_or_prevalence": "Training dataset initially had comments removed for some experiments; docstring injection resolved ~51.3% of call-site resolutions and docstring titles were available for 64.6% of callable entities encountered in crawling, indicating substantial coverage gaps.",
            "root_cause": "Source repositories often lack sufficient inline comments/docstrings aligned with call sites; dynamic typing and incomplete docstrings make automatic alignment hard.",
            "mitigation_approach": "Docstring injection (synthetic annotations): resolve call sites to entities and inject the first-sentence docstring as an inline comment for a sampled subset (20% sampling in experiments); also train on natural comments where available.",
            "mitigation_effectiveness": "Docstring-injected model outperformed no-comments baseline and was close to natural-comments model on several metrics; natural model still had better intent coverage. Docstring injection resolved only a subset of call-sites (51.3%), limiting its reach.",
            "domain_or_field": "machine learning for code / data science tooling",
            "reproducibility_impact": true,
            "uuid": "e481.2",
            "source_info": {
                "paper_title": "Natural Language-Guided Programming",
                "publication_date_yy_mm": "2021-08"
            }
        },
        {
            "name_short": "Limited context window / missing imports",
            "name_full": "Model context window limitations leading to missing imports and incorrect predictions",
            "brief_description": "Transformer model context length limits cause relevant parts of a user's codebase (e.g., import statements or function definitions) to fall outside the window, producing predictions that call APIs not present or miss required imports.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "GPT-2–based NLGP assistant (max context 700 tokens in experiments, GPT-2 default 1024)",
            "system_description": "Language-model-based code autocompletion that conditions on previous tokens up to a fixed context window (700 token max used in experiments).",
            "nl_description_type": "inline intent comments / surrounding code context",
            "code_implementation_type": "notebook-derived Python code context / predicted code",
            "gap_type": "context truncation causing omitted preprocessing or imports / incomplete context",
            "gap_description": "Because the model only sees a bounded window of prior tokens, import statements and definitions required for correct completions sometimes lie outside the window, causing the model to produce predictions that reference unavailable functions or omit required imports.",
            "gap_location": "model input context selection / inference-time context window",
            "detection_method": "Error analysis of model outputs and observation that predictions sometimes propose function calls that do not exist in context; reasoning about training where model saw calls without imports due to truncated context.",
            "measurement_method": "Qualitative error analysis and anecdotal examples; no single numeric metric but authors note 'a significant proportion' of mistakes related to dropped context and provide concrete examples and discussion.",
            "impact_on_results": "Leads to incorrect or non-runnable predictions, reducing usefulness and potentially harming developer trust; contributes to lower coverage/precision scores observed in human evaluation.",
            "frequency_or_prevalence": "Described as a significant proportion of model mistakes (no exact percentage given); inference context length set to 700 tokens in experiments to keep latency acceptable.",
            "root_cause": "Transformer quadratic scaling limits practical sequence length; lack of selection strategy for which parts of a long file to include in context.",
            "mitigation_approach": "Proposed strategies: smarter context selection heuristics, more efficient tokenizations, linear/subquadratic transformer variants, and including precise library version info so the model can tailor suggestions.",
            "mitigation_effectiveness": "Not evaluated quantitatively in this paper; authors report it as an open research direction and give anecdotal rationale that these approaches should reduce such errors.",
            "domain_or_field": "deep learning for code / developer tooling",
            "reproducibility_impact": true,
            "uuid": "e481.3",
            "source_info": {
                "paper_title": "Natural Language-Guided Programming",
                "publication_date_yy_mm": "2021-08"
            }
        },
        {
            "name_short": "API versioning / obsolete API bias",
            "name_full": "Bias toward older or more frequent API versions in model predictions leading to obsolete suggestions",
            "brief_description": "Training on existing code repositories biases model predictions toward the API usage patterns present in the training data, which may be older or more common versions, producing suggestions that are incompatible with a developer's current library versions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "NLGP assistant trained on harvested GitHub notebooks",
            "system_description": "Language models trained on publicly available notebooks and code without explicit per-repository dependency/version metadata.",
            "nl_description_type": "documentation strings / inline comments / example usage in corpora",
            "code_implementation_type": "library API call patterns in training code vs developer runtime environment",
            "gap_type": "version mismatch / obsolete API usage",
            "gap_description": "The model can suggest API calls or call signatures corresponding to older or more common versions present in the training data; without knowledge of the specific versions used in the user's environment, suggestions may be obsolete or incompatible.",
            "gap_location": "model training data bias / suggestion tailoring to environment",
            "detection_method": "Conceptual analysis and referencing prior findings about obsolete Stack Overflow answers; discussion of limitations rather than a specific experimental quantification in this paper.",
            "measurement_method": "No direct quantitative measurement in this paper; authors cite external studies on Stack Overflow obsolescence and discuss risk.",
            "impact_on_results": "Potential to produce suggestions that fail when executed in a user's environment; undermines reliability and reproducibility of automatically suggested code.",
            "frequency_or_prevalence": "Prior study cited notes high rates of obsolescence in community answers (e.g., only 20.5% of a sample of known-obsolete answers were updated); no direct prevalence number from the authors' dataset.",
            "root_cause": "Training corpora do not include explicit package version metadata; APIs evolve and examples are not updated.",
            "mitigation_approach": "Augment NLGP tools with precise versions of libraries used by the developer so suggestions can be tailored to the developer's environment; prefer up-to-date sources and curated examples.",
            "mitigation_effectiveness": "Not evaluated quantitatively in this work; proposed as an important future direction.",
            "domain_or_field": "software engineering / ML-driven developer tools",
            "reproducibility_impact": true,
            "uuid": "e481.4",
            "source_info": {
                "paper_title": "Natural Language-Guided Programming",
                "publication_date_yy_mm": "2021-08"
            }
        },
        {
            "name_short": "Docstring-injection limits",
            "name_full": "Limitations of docstring-injection for synthetic intent annotation",
            "brief_description": "The paper introduces a docstring-injection method to synthesize intent comments at call sites but reports partial coverage and inaccuracies due to Python's dynamic nature and docstring availability.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "docstring-injection preprocessing pipeline",
            "system_description": "Automated pipeline that resolves callable entities in training code to harvested docstring titles and inserts them as inline comments at a sampled fraction of resolved call sites.",
            "nl_description_type": "injected docstring titles as synthetic inline comments",
            "code_implementation_type": "training .py files converted from notebooks; AST-resolved call sites",
            "gap_type": "incomplete resolution / partial coverage of call sites",
            "gap_description": "Because Python is dynamically typed, static resolution only succeeds for a subset of call sites; the pipeline resolved 51.3% of call sites to entity names and harvested docstrings existed for 64.6% of visited callable entities, so many calls remain unannotated and injection cannot fully substitute for natural comments.",
            "gap_location": "training-data annotation stage (call-site resolution and docstring availability)",
            "detection_method": "Automated AST-based flow analysis to resolve call sites and counting resolved vs unresolved sites; reporting of coverage statistics.",
            "measurement_method": "Reported statistics: 51.3% of call sites resolved to one of the stored entity names; mapping built that captured docstrings for 64.6% of visited callable entities; docstring injection sampling rate used = 20%.",
            "impact_on_results": "Docstring-injected model outperforms no-comments baseline but remains behind or close to natural-comments model in some metrics; limited resolution reduces the reach and realism of synthetic intents.",
            "frequency_or_prevalence": "Resolution succeeded for ~51.3% of call sites; docstring titles available for ~64.6% of entities visited by crawler; injection performed on a sampled 20% of resolved call sites in experiments.",
            "root_cause": "Dynamic typing and name binding in Python inhibit precise static resolution; many libraries lack docstrings or have terse docstrings not aligned with call-site intent.",
            "mitigation_approach": "Improve static analysis for better resolution, increase coverage of docstring harvesting, vary sampling rates, combine natural comments with injected docstrings, and refine injected text to better match developer intent.",
            "mitigation_effectiveness": "Partial: docstring-injected model gave substantial gains over no-comments model (per human ratings and BLEU/IoU), but did not fully match natural-comments model; exact numeric improvements presented in evaluation (e.g., BLEU and IoU differences in Table 4).",
            "domain_or_field": "ML for code / dataset curation",
            "reproducibility_impact": true,
            "uuid": "e481.5",
            "source_info": {
                "paper_title": "Natural Language-Guided Programming",
                "publication_date_yy_mm": "2021-08"
            }
        },
        {
            "name_short": "Annotation variability",
            "name_full": "Inter-annotator disagreement and subjectivity in benchmark creation",
            "brief_description": "Human curation of context/intent/target triplets exhibits moderate agreement and measurable variability that affects the benchmark and interpretation of evaluation outcomes.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "NLGP benchmark creation and curation",
            "system_description": "Mining candidate context-intent-target triplets from evaluation split of notebooks and human curation with three reviewers per sample to produce a 201-case benchmark.",
            "nl_description_type": "inline comments sampled as intents and human-rewritten intents",
            "code_implementation_type": "target code snippets extracted from notebooks",
            "gap_type": "curation variability / ambiguous selection of intent and target",
            "gap_description": "Annotators sometimes disagree on whether a comment expresses a suitable intent or which lines of subsequent code address the intent; differences in accepted samples, reformulations, and selected target lines introduce benchmark noise.",
            "gap_location": "benchmark curation and labeling",
            "detection_method": "Inter-annotator agreement statistics computed during curation; measurement of matching rates and kappa statistic.",
            "measurement_method": "Fleiss' kappa for accept/skip decision = 0.515 (moderate agreement); among accepted candidates annotators had identical intent in 74% of cases and identical target code selection in 71%; average edit distance for differing intents = 3.3 tokens; average difference in target code = 3.2 lines.",
            "impact_on_results": "Benchmark contains some subjectivity and label noise which can affect model evaluation and comparisons; small benchmark size (201) amplifies impact of annotation variability on statistical conclusions.",
            "frequency_or_prevalence": "Moderate agreement: kappa = 0.515; exact identity rates: 74% same intent, 71% same target among accepted cases.",
            "root_cause": "Ambiguity in natural-language intents and multiple valid code ways to satisfy a short intent; annotator subjective choices when reformulating or selecting minimal target code.",
            "mitigation_approach": "Use multiple annotators and consensus rules (accept when 2/3 agree and selections differ by ≤2 lines); provide detailed annotation guidelines and interface; increase benchmark size and diversify annotators in future work.",
            "mitigation_effectiveness": "Effective to some extent: these rules reduced noisy samples and produced a 201-sample benchmark, but residual variability remains and is acknowledged as limitation.",
            "domain_or_field": "dataset curation / human annotation for ML",
            "reproducibility_impact": true,
            "uuid": "e481.6",
            "source_info": {
                "paper_title": "Natural Language-Guided Programming",
                "publication_date_yy_mm": "2021-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "From Query to Usable Code: An Analysis of Stack Overflow Code Snippets",
            "rating": 2,
            "sanitized_title": "from_query_to_usable_code_an_analysis_of_stack_overflow_code_snippets"
        },
        {
            "paper_title": "An Empirical Study of Obsolete Answers on Stack Overflow",
            "rating": 2,
            "sanitized_title": "an_empirical_study_of_obsolete_answers_on_stack_overflow"
        },
        {
            "paper_title": "BLEU: A Method for Automatic Evaluation of Machine Translation",
            "rating": 2,
            "sanitized_title": "bleu_a_method_for_automatic_evaluation_of_machine_translation"
        },
        {
            "paper_title": "Evaluating Large Language Models Trained on Code",
            "rating": 2,
            "sanitized_title": "evaluating_large_language_models_trained_on_code"
        },
        {
            "paper_title": "On the Naturalness of Software",
            "rating": 1,
            "sanitized_title": "on_the_naturalness_of_software"
        }
    ],
    "cost": 0.01842975,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Natural Language-Guided Programming</p>
<p>Geert Heyman geert.heyman@nokia-bell-labs.com 
Nokia Bell Labs
Belgium</p>
<p>Rafael Huysegems rafael.huysegems@nokia-bell-labs.com 
Nokia Bell Labs
Belgium</p>
<p>Pascal Justen pascal.justen@nokia-bell-labs.com 
Nokia Bell Labs
Belgium</p>
<p>Tom Van Cutsem tom.van_cutsem@nokia-bell-labs.com 
Nokia Bell Labs
Belgium</p>
<p>Natural Language-Guided Programming
code completioncode predictionnatural language-guided programmingexample-centric program- ming
1 According to a 2019 survey ran by Open Source consultancy firm Tidelift up to 93% of surveyed applications used open source components and up to 70% of the codebases of surveyed applications consisted of open source code. https://tidelift.com/subscription/managed-open-source-survey 2 https://medium.com/analytics-vidhya/pytorch-is-growing-tensorflow-isnot-6986c5e52d6f, retrieved April 2021. 3</p>
<p>In today's software world with its cornucopia of reusable software libraries, when a programmer is faced with a programming task that they suspect can be completed through the use of a library, they often look for code examples using a search engine and then manually adapt found examples to their specific context of use. We put forward a vision based on a new breed of developer tools that have the potential to largely automate this process. The key idea is to adapt code autocompletion tools such that they take into account not only the developer's already-written code but also the intent of the task the developer is trying to achieve next, formulated in plain natural language. We call this practice of enriching the code with natural language intent to facilitate its completion natural language-guided programming.</p>
<p>To show that this idea is feasible we design, implement and benchmark a tool that solves this problem in the context of a specific domain (data science) and a specific programming language (Python). Central to the tool is the use of language models trained on a large corpus of documented code. Our initial experiments confirm the feasibility of the idea but also make it clear that we have only scratched the surface of what may become possible in the future. We end the paper with a comprehensive research agenda to stimulate additional research in the budding area of natural language-guided programming.</p>
<p>CCS Concepts: • Software and its engineering → Integrated and visual development environments; • Computing methodologies → Natural language processing.</p>
<p>Keywords: code completion, code prediction, natural language-guided programming, example-centric programming</p>
<p>Introduction</p>
<p>In many areas of software development, developers find themselves spoiled with thousands of readily available software libraries (also commonly called modules or packages). The growing practice of building software out of open source components further adds to that trend 1 , making modern software stacks highly diverse and fast-evolving.</p>
<p>To make this more concrete, consider a data scientist using Python for data analysis. A trained data scientist familiar with the ecosystem will typically combine a variety of libraries to complete any given job. She might use Python's built-in libraries to download or manipulate raw data files, use the popular Pandas library to manipulate tabular data, use scientific computing packages such as NumPy to manipulate numerical data, use large machine learning frameworks such as Scikit-learn to train predictive models on the data and finally use one of the many popular data visualization libraries such as Matplotlib or Plotly to chart her insights.</p>
<p>If the data scientist is in need of training sophisticated deep neural network models to fit the data, she is spoiled with choice among multiple large and high-quality libraries that will help her do that with just a few lines of code. Two of the most widely used libraries are Tensorflow and Pytorch. Unfortunately for our data scientist, these major machine learning toolkits are constantly tweaking their APIs. In the last three years alone, Tensorflow has received no less than 16 major releases while Pytorch saw 9 major releases 2 .</p>
<p>Keeping up with the large and ever-changing "API surface" of all of these combined library dependencies poses a serious learning challenge to any prospective data scientist. This problem is neither specific to data science nor is it specific to Python. Other domains of software development feature a similarly rich and sprawling software library ecosystem [44].</p>
<p>Example Embedding to the Rescue?</p>
<p>Luckily the growing body of APIs is accompanied by growing sources of online documentation. The proliferation of online code examples embedded in library documentation, tutorials or Q&amp;A websites such as Stack Overflow 3 has led to a programming phenomenon called Example Embedding [4], Figure 1. In natural language-guided programming, a programmer formulates tasks using natural language in a specific code context. An NLGP assistant suggests relevant code that matches the task and the context. also known as example-centric programming [5]. A developer engages in example embedding when they search for code examples (in local or online repositories), copy-paste the code into their code editor and then adapt the code to fit their specific needs and their specific code context.</p>
<p>The activity of example embedding is largely done manually, with little dedicated tool support, which can make it time-consuming and error-prone. Indeed, prior empirical studies in software engineering found that up to 35% of a developer's worktime can be spent on code search [43]. There is also evidence that suggests that code found in documentation or online knowledge bases is rarely immediately usable. For example, one study of Stack Overflow found that a mere 25.61% of Python code snippets could be readily executed, and the figures were even worse for other languages [47]. Even when the developer does find a high-quality code example, they must still edit the code to fit their specific context, e.g. by renaming variables or deleting superfluous statements. These edits are an opportunity for bugs to creep into the developer's codebase.</p>
<p>Given these observations, we conjecture that automating the activity of example embedding has the potential to positively affect both developer productivity as well as code quality.</p>
<p>Automating Example Embedding</p>
<p>We envision that this common practice of example embedding will become more and more automated through new tools that leverage advances in machine learning and natural language processing. We will refer to the practice of using tools to automate the example embedding process as natural language-guided programming (abbreviated NLGP). In a coding environment that supports natural language-guided programming, when the programmer is faced with a task that they believe can be solved using a library, they simply state their intent using natural language in-line in the code and an NLGP tool suggests code that 1) addresses the task and 2) fits the context, choosing variable and function names that match the already existing code. We will refer to such a developer tool as an NLGP assistant. The diagram in Figure 1 describes the basic NLGP workflow.</p>
<p>Much like refactoring assistants in modern IDEs now help software developers more quickly and reliably apply refactorings to their codebase, we envision that natural languageguided programming tools will help developers more quickly and reliably perform example embedding. The goal of an NLGP assistant is to help programmers write idiomatic code for tasks that can be solved using available libraries.</p>
<p>Paper Contributions</p>
<p>• We demonstrate our vision of natural language-guided programming in the domain of data science and machine learning using Python libraries (Section 2). • We contribute the design and implementation of an NLGP assistant for this domain. The core of our approach is based on language models (Section 3) which require training on a large corpus of documented code.</p>
<p>We develop three language model variants to study the impact of natural language intent on prediction quality together with a benchmark and a user evaluation (Section 4). 4  would be like to perform data analysis using natural languageguided programming. Before we get started, we give a brief background on the typical programming environment used by data scientists.</p>
<p>Background: the Python Data Science Stack</p>
<p>In recent years Python has become the language of choice for an increasing number of data scientists, data engineers, and machine learning researchers. 5 As mentioned in the introduction, one reason for this is Python's large ecosystem of scientific computing libraries, sometimes called the "Python data science stack" or simply the "data stack". 6 Table 1 lists key projects in this stack. A Jupyter notebook is an interactive coding environment composed of cells. The two most commonly used cells are code cells and markdown cells. Markdown cells contain simple markup text and can be rendered to a variety of formats. Code cells may contain code in one of the languages supported by the Jupyter protocol. Here we will focus only on Python code.</p>
<p>A code cell can be executed after which the result of the code is inserted as output in the notebook. Jupyter has builtin support to render certain program values as rich media (such as tables or graphics). This allows a data scientist to easily inspect the intermediate output of their data transformations. This interactive style of programming aligns well with NLGP because it allows for programmers to rapidly test and explore auto-generated code by executing it.</p>
<p>Natural Language-Guided Programming in</p>
<p>Jupyter Let us put ourselves in the position of Dana the data scientist. Dana is tasked with analyzing stock market prices stored in a comma-separated value (CSV) file.</p>
<p>Dana knows that Pandas is the go-to library to manipulate tabular data like this, so she starts by importing the library: import pandas as pd 5 https://towardsdatascience.com/top-programming-languages-for-datascience-in-2020-3425d756e2a7, retrieved April 2021 6 https://hub.packtpub.com/python-data-stack/, retrieved April 2021. The Pandas library offers a data structure called a "dataframe" to manipulate tabular data. Dana now needs to read the CSV file into memory and convert it into such a dataframe. She knows that Pandas has an API for this but forgot the details. Rather than looking up the right API call in the documentation or searching the Web for an example, using an NLGP assistant Dana can simply write her intent as a single-line comment in the code: import pandas as pd # read stock_data.csv Dana then triggers her NLGP assistant using a hotkey, which looks at her code and her intent and produces the following suggestion: df = pd.read_csv('stock_data.csv', delimiter=',') That suggestion looks relevant so Dana hits 'enter' to insert the line into the code cell: import pandas as pd # read stock_data.csv df = pd.read_csv('stock_data.csv', delimiter=', ') Once the suggested code is merged, Dana is free to modify it. Perhaps the CSV file used a ';' separator rather than a ',' separator. Dana can easily update the suggested parameter. At this point, Dana can run the code to inspect the contents of df and verify that the data was parsed correctly. Figure 2 shows the output of the above code cell in a Jupyter notebook environment. The call to df.head() was added to visualize the first five rows of the dataframe. Now Dana would like to select only a subset of the columns. Again, rather than looking up how to do this in the documentation, she can state her intent as a comment:</p>
<p>In the above code, the comments in blue indicate the programmer's "intent". The NLGP assistant is invoked after entering the intent. The code following each intent is generated by the NLGP assistant. The code suggestions presented above are actual code predictions made by the tool introduced in Section 4.</p>
<p>The above NLGP session focused on assisting Dana with the Pandas library only, but we expect an NLGP assistant to be able to offer suggestions for tasks requiring other libraries through the same unified interface. For example, Dana might also use the popular Scikit-learn library for machine learning tasks. Example intents that she might use to describe such tasks include:</p>
<p>• "split the data in a training and a validation set"</p>
<p>• "cluster the data using K-means"</p>
<p>• "plot a confusion matrix of the classifier"</p>
<p>The user stating their intent is an integral part of natural language-guided programming. However, we make no assumptions on how that intent is communicated to the NLGP assistant. In the NLGP session described above, the developer states their intent as an in-line comment in the code. Other NLGP assistants may offer a text field separate from the code to guide the code autocompletion. One potential benefit of inlining the intent in the code is that it self-documents the interaction with the tool, forming a potential source of future data to better learn the translation from intent to code.</p>
<p>In the next section, we introduce language models as a general technique for predicting text or code. Subsequently, we show how an NLGP assistant for Python can be built using this technique.</p>
<p>Background: Language Models</p>
<p>Language models are statistical models that estimate the probability of a given sequence of tokens, such as words in an English-language text with respect to a reference corpus. This is a very general problem with many applications, including text autocompletion, speech recognition and machine translation [20]. In the past decade, advances in the field of deep learning, such as the Transformer model architecture [38], have significantly improved the effectiveness of language models on practical applications. One particularly successful set of language models based on Transformers is the GPT (Generative Pretrained Transformer) model family including GPT-2 [29] and its successor GPT-3 [6]. GPT models have been trained on large volumes of text from public Web pages. Their capability to generate seemingly humanwritten text has received widespread attention both within and outside the research community. 7 Hindle et al. [17] were the first to formally observe that code, like natural language, is repetitive and predictable and that language models can also be used to create effective statistical models of source code, paving the way towards new kinds of code completion tools. Hindle et al.'s work was based on (adapted versions of) n-gram models and in recent years there has been an ongoing debate about what type of language models (based on n-grams or deep learning) is best for modeling source code [15,21]. In recent years, in line with what has been observed in NLP in general, language models based on deep learning such as Transformers have been shown to achieve production-quality levels of code completion, with companies offering code completion products based on this technology. 8 We now review how language models can be used to generate sequences that fit a given context. A language model estimates the probability distribution ( | 1 , ..., −2 , −1 ) of the n ℎ token given the previous tokens in the sequence. With this conditional probability distribution, we can predict the most likely token in a given context. By adding the predicted token to the context, we can iteratively expand the prediction to form sequences of arbitrary length. Generating the most likely sequence is intractable 9 so in practice approximate algorithms such as beam search [12] are used to explore the search space.</p>
<p>To make the problem tractable, most language models make the simplifying assumption that only depends on a window with the previous tokens: ( | 1 , ..., −2 , −1 ) = ( | − , ..., −2 , −1 ). For instance, GPT-2 language models can process a maximum of 1024 tokens at a time, which means that can be at most 1023.</p>
<p>Language models can be applied to different types of token sequences: tokens can correspond to words, subword units, or individual characters. When applying language models to source code, where the number of unique identifiers tends to be large [17], subword units are desirable. When we discuss training of language models on code in this work, we assume the use of byte-pair encoding (BPE) [11,32] as used in GPT-2. BPE is a compression algorithm for splitting words into subword tokens such that the most frequent (sub)words are tokenized as a single token. For example, applying the GPT-2 tokenizer to the code string 'b = np.zeros(10)' would result in the following subword units: 'b', '␣=', '␣np', '.' , 'zer', 'os', '(', '10' and ')'.</p>
<p>In the next section, we describe how an effective NLGP assistant can be built based on the GPT-2 model.</p>
<p>Building an NLGP Assistant using Language Models</p>
<p>To study the feasibility of NLGP we build and evaluate a prototype of an NLGP assistant for Python. Our NLGP assistant uses a language model to autocomplete code cells based on both existing code in the cell, as well as the developer's intent, specified as a comment (as introduced in Section 2). The language model is trained on a collection of preprocessed Jupyter notebooks (details of our dataset are covered in Section 4.2). We first introduce three strategies for preprocessing the data, leading to three distinct language models. Next, we cover more details on how we prepare the data and train the models. Finally, we report on an initial user study to evaluate the quality of the models' code predictions.</p>
<p>Language Models for NLGP</p>
<p>The starting point for all of the language models trained in this paper is the GPT-2 Medium model checkpoint released by OpenAI [29]. The model checkpoint was pretrained by OpenAI on general-purpose English-language text crawled from the Web. From preliminary experiments we concluded that starting from a pretrained model gave significantly better results than starting from an equivalent randomly initialized transformer model that was not pretrained on text.</p>
<p>To train a language model to be able to autocomplete code based on existing code and a natural language intent, we need relevant training data. The challenge here lies in finding a sufficiently large amount of code that is self-documented with the developer's intent. Given that there exists no sufficiently large dataset of Python code that is explicitly annotated with the developer's intent using natural language 10 , we need creative ways to teach the language model how to associate natural language intent with code. One assumption is to rely on textual comments in the code. We consider three distinct ways to use comments in code to train language models:</p>
<p>No Comments The no comments model is trained on a dataset where all original comments are stripped from the training data. This model serves as a baseline and will allow us to quantify how important it is to consider natural language intent in addition to pure code.</p>
<p>Docstring Comments The docstring model is trained on a dataset where we also first strip all comments from the training data. However, here we annotate a selection of call sites with synthetic comments. These comments contain a summary of the called method's or function's docstring. The intuition is that a docstring typically contains a short onesentence description of the intent of the function or method. We describe this procedure in detail in Section 4.4.</p>
<p>By annotating the call site with the docstring, we hope to teach the model to associate code context preceding the call with keywords from the docstring and the subsequent method or function call. This setup is meant to assess the feasibility of NLGP models in domains where code is not documented with relevant comments.</p>
<p>Natural Comments The natural model is trained on comments interleaved with code as they naturally occur in Jupyter notebooks. This includes text in markdown cells as well as in-line comments in code cells. In this dataset no call sites are annotated with docstrings.</p>
<p>Jupyter Notebook Dataset</p>
<p>Jupyter notebooks are a mix of executable code and descriptive text. This makes them an interesting source for collecting training and evaluation data for an NLGP assistant. To construct a dataset, we searched GitHub for all projects that contain at least one Jupyter notebook, have a permissive license and received at least one star. Next, we apply a heuristic to filter out project forks: when multiple projects have the same name, only the project with the most stars is retained. We then download all notebooks in the project and convert them to .py source files using the nbconvert tool. 11,12 This tool converts any non-code cells into inline comments. We parse the .py files using a Python3 parser and reject any files that contain parse errors. The resulting files are split 90/10 across a training and evaluation set. We ensure that notebooks that belong to the same GitHub project end up in the same split. In this way, we obtain 297,845 and 32,967 .py files for training and evaluation purposes respectively.</p>
<p>Each .py file in the training split was further preprocessed and cleaned using following heuristics:</p>
<p>• Any markdown content before the first code cell delimiter is removed; • Comments that were inserted by nbconvert to delimit code cells (# In [], # In [1] , # In [2], etc. ) are replaced by a special &lt;|cell|&gt; token; • Comments are separated from the subsequent code by a special &lt;|endofcomment|&gt; token (more details below); • Multi-line comments are truncated to a maximum of two lines; • Markdown header symbols, which are inserted by the nbconvert tool, are stripped (e.g., # ## some title is converted to # some title); • Non-English comments are stripped. We used the cld3 tool 13 to automatically detect the language; • Empty cells and empty comments are removed.</p>
<p>• Spaces are replaced by special whitespace tokens (e.g., ' ' is replaced by a single '&lt;|4space|&gt;' token).</p>
<p>Language Model Setup for Intent-Guided Code Prediction</p>
<p>To use a language model to generate predictions in an NLGP context, two issues remain: 1) What is the stopping criterium (when has the model predicted enough code to address the intent)?; 2) How to force the model to predict source code instead of autocompleting the inline comment with more natural language? If the model were to autocomplete the intent, it may inadvertently change its meaning, which is undesirable.</p>
<p>To address these challenges, we introduce additional symbols &lt;|endofcomment|&gt; and &lt;|cell|&gt; to encode structural information, as illustrated in the following example:</p>
<p>... # Choose number of features automatically # use RFECV to select features &lt;|endofcomment|&gt; rfe = RFECV(random_forest, n_jobs=−1, step=1) rfe.fit(X_train, y_train) feature_scores['RFECV'] = X.shape[1] − \ rfe.ranking_.astype(float).reshape(−1, 1) &lt;|cell|&gt; # output number of features &lt;|endofcomment|&gt; print("#features=", np.sum(rfe.support_)) &lt;|cell|&gt; An &lt;|endofcomment|&gt; token is inserted after every inline comment that is followed by source code. That is, for multiple successive inline comment lines, we only insert the token after the last comment line. At prediction time, we append this symbol to the end of the user intent to prompt 13 https://github.com/google/cld3 the model to predict source code rather than to autocomplete the comment. A &lt;|cell|&gt; token is inserted at the end of every Jupyter notebook code cell. At prediction time, no more tokens are predicted after the model has predicted a &lt;|cell|&gt; token. We found that this simple heuristic works well in practice, but there is room to experiment with more sophisticated stopping criteria in future work.</p>
<p>As a final step, we concatenate all preprocessed .py files into a single training file using the &lt;|endoftext|&gt; symbol to encode the original file boundaries.</p>
<p>We generate predictions using beam search with a beamwidth of 3, where the prediction of the &lt;|cell|&gt; token signals that a beam hypothesis is complete. We enforce that the model predicts between 10 and 150 tokens by setting the probability of the stopping token to zero for the first 10 tokens and by stopping the beam search procedure after the beam hypotheses are 150 tokens long. The maximum context length is set to 700 tokens.</p>
<p>We made a slight adjustment to the GPT-2 model and the GPT-2 tokenizer to ensure that our special tokens (&lt;|4space|&gt;, &lt;|endoftext|&gt; , &lt;|endofcomment|&gt;, etc.) are tokenized as a single token and are encoded with their own set of (embedding) parameters that are initialized at random and trained from scratch. We use the transformers library [42] to make these changes.</p>
<p>In Section 4.5, we describe how we used the evaluation split to create a labeled test set.</p>
<p>Docstring Comment Injection</p>
<p>The docstring model is trained on a synthetic dataset where all naturally occurring comments in the training data are first removed, after which a random sample of call sites is instrumented with new comments taken from docstrings. More specifically, when a call is made to a documented library API, an additional inline comment is added to the code, describing the purpose of the call. The goal is to augment the source code with comments that capture the intent of the calls using short natural language statements.</p>
<p>For example, given the following snippet of Python code:
from sklearn.cluster import KMeans k = KMeans() k.fit(Xtrain) y = k.predict(Xtest)
The goal is to transform it into:</p>
<p>from sklearn.cluster import KMeans # K−Means clustering k = KMeans() # Compute k−means clustering k.fit(Xtrain) # Predict closest cluster for each sample y = k.predict(Xtest) Figure 3. High-level process flow to inject Python docstrings into code. Figure 3 depicts the high-level process that we followed to implement this transformation. The first objective is to create a mapping from the names of callable program entities (functions, methods, constructors) to their docstrings:</p>
<ol>
<li>From the python source files in the training set, the root module names are extracted and counted.   Table 2 lists a short fragment of the entity-docstring mapping for two entities from the sklearn library. In a second phase, we parse the source files in the dataset and visit all call sites using an AST walker. For each call site, we try to resolve the call to a named entity, e.g. the call k.predict() would resolve to sklearn.cluster.KMeans().predict(). Because of Python's dynamic typing, we are only able to resolve a subset of calls using a basic program flow analysis. Still, this allows us to resolve 51.3% of call sites to one of the entity names stored in the mapping.</li>
</ol>
<p>In a final phase, the AST annotator chooses a random sample of resolved call sites and then inserts the associated docstring title in front of the call. The docstring is always inserted on the previous line of the statement enclosing the visited call site. In our experiments we chose a sampling rate of 20%. A deeper study of the effect of the sampling rate on the prediction quality is left as future work.</p>
<p>Creating an NLGP Benchmark</p>
<p>To assess the prediction quality of an NLGP assistant we need a good benchmark. As no such benchmark exists, we set out to create our own.</p>
<p>A benchmark for NLGP requires realistic scenarios (test cases) where an NLGP assistant needs to complete the code based on a natural language intent. Each test case is a triplet / / containing a code context ; a natural language intent , provided in the form of an inline comment; and a target code snippet , a reference code snippet that addresses the intent and is a natural completion of the code context . We created a benchmark containing such triplets in two stages. In a first generation stage we automatically mine candidate triplets from the Jupyter notebook dataset. In a second curation stage we filter remaining candidates based on human review.</p>
<p>Generation Stage To create realistic and unbiased / / triplets, we chose to mine triplets from our Jupyter notebook dataset. More specifically, we sample candidate test cases only from source files that were set aside for evaluation (i.e. not occurring in the training set):</p>
<ol>
<li>We scan the source files for lines that only contain an inline comment and whitespace. 2. Next, we filter out non-English comments and comments that are longer than 10 tokens. This cut-off was informed by studying the query lengths of the user study done by Xu et. al [46]: over 97% user queries consisted of 10 tokens or less. 3. From the remaining comments, we then sample at random and create a set of candidate test cases / / : • The candidate context is extracted from the start of the comment's source file up to the line of the comment. • The candidate intent is set to the sampled comment including any leading whitespace. • The candidate target code is set to all the code (excluding comments) that follows the comment. 4. We filter out candidates that overlap with code in the training set. Specifically, we concatenate the last three non-empty lines in the candidate context with the candidate intent and check if the resulting piece of code occurs in the training dataset. If an exact match is found, the candidate is dropped.</li>
</ol>
<p>Curation Stage Mined candidate test cases / / were reviewed by human annotators and refined into representative test cases / / : 1. We generate three non-overlapping batches of candidate test cases. Each batch contained 200 distinct cases. 2. The 3 batches were assigned for review to 9 human reviewers. Each batch was assigned for review to a group of 3 reviewers. As such, a total of 600 candidate test cases were reviewed, each case receiving 3 reviews. 3. Annotators were asked to decide i) whether the candidate test case is relevant, ii) were allowed to slightly rephrase the candidate intent (e.g. rephrasing a comment in the code like "and now let's plot the data" to a more succinct intent like "plot the data"), and iii) were requested to mark in the candidate target code which specific lines of code best addressed the intent. Appendix A.1 provides further details about the annotation process, including the detailed guidelines that were given to the annotators, a screenshot of the annotation interface, and statistics about the inter-annotator agreement. 4. When 2 out of 3 reviewers judged that a candidate test case is relevant and the difference between their respective target code selections was not more than 2 lines, the test case was added to the benchmark. 201 out of 600 code snippets were selected in this way. 5. We postprocess the resulting test cases such that: • All the code before the first line of target code is moved to the context • Import statements in the context that are only required for the target code are moved to the target code because it is unrealistic to assume a user will have written such import statements before issuing a query • All comments in the target code (if any) are stripped After going through the curation stage we end up with a benchmark of 201 representative test cases / / that we can now use to validate the quality of code predictions made by the models. Table 3 displays some key statistics about the benchmark.</p>
<p>Evaluation</p>
<p>We now assess the performance of our language models on the NLGP benchmark. Recall that we trained models on three distinct datasets:</p>
<p>No comments A model trained on only code, with comments stripped out. This model serves as a baseline to measure the importance of natural language intent; Docstring A model trained on code augmented with injected docstring comments on 20% of calls to APIs from libraries documented with pydoc docstrings. Natural A model trained on code including all the comments that occur naturally in the code.</p>
<p>For each model, we create a code prediction for each / / triplet in our benchmark. We provide concrete examples of / / / cases in Appendix A.3. The average prediction latency on an 11GB GeForce GTX 1080 Ti GPU was 1.87 seconds 14 .</p>
<p>To assess how well the generated code prediction compares to the reference code , we set up a human evaluation study. We first introduce the study, then discuss how the human evaluation results correlate with standard text comparison metrics such as BLEU [28]. 4.6.1 Human Evaluation Study: Setup. Using the prepared / / / entries, we ran a small human evaluation study where the first three authors of the paper manually scored the code predictions of each model across four dimensions: usefulness, coverage, precision, and compatibility. Specifically, for the predictions of each model on 100 test cases in our benchmark, each participant rates the following statements on a 4-point scale (Strongly disagree, Disagree, Agree, Strongly agree):</p>
<p>Usefulness The predicted code is helpful for implementing the given intent; Coverage The predicted code completely covers the intent; Precision The predicted code mostly contains code that is relevant to the intent; Compatibility The predicted code is compatible with the code context (e.g., it reuses variable names from the context if appropriate)</p>
<p>To avoid that the annotators have a bias to a certain model, the predictions were presented in random order and the annotation interface did not display the model name. Figure 4 reports the answer distributions to the survey questions for each model. The results indicate that the models trained on comments significantly outperform the no comments model. As expected, it is more difficult for models to guess the programmer's intent from only undocumented code.</p>
<p>Human evaluation study: results.</p>
<p>Both the docstring and natural models exhibit decent performance with similar overall scores even though the natural model results in a better intent coverage. This difference can be attributed to the fact that the original inline comments are more diverse than docstring titles supporting the fact that the natural model can translate a more diverse set of intents. The relatively small gap between the two does indicate that even in domains where code is not heavily documented the NLGP approach is feasible when a procedure similar to our docstring-injection is feasible. The models score particularly well w.r.t. compatibility, implying that the models can generate code predictions customized to the code context.</p>
<p>Finally, the usefulness scores of both the docstring and natural models reflect that the majority of their predictions are considered useful. These results support the feasibility of natural language-guided programming and suggest that our NLGP assistant prototype may already be helpful to support data scientists in practice.</p>
<p>Metrics.</p>
<p>Running human evaluation studies to validate code prediction models is time-consuming. For this reason, it is desirable to have good metrics that can automatically score predicted code. One way to accomplish this is to compare the predicted code with the reference target code (also called the "ground truth" code).</p>
<p>Previous work to measure the quality of predicted code [8,13,19,45,50] mostly treats the code as plain text and uses the Bilingual evaluation under study (BLEU) score [28]. BLEU is a standard metric in natural language text translation. The BLEU score is based on the ratio of common n-grams found in the prediction and the reference text (the target code). Intersect-over-Union (IoU), also known as Jaccard distance, between sets of tokens derived from code fragments has also been used to evaluate code prediction tools. For example, Murali et al. compute Jaccard distance between sets of API calls occurring in the predicted and the target code [26].</p>
<p>We computed both BLEU and IoU metrics for the code predictions in our benchmark and correlated them with the usefulness scores that were assigned in the human evaluation study. Our goal here is to measure how well these metrics can act as a proxy for the average usefulness score assigned by human experts.</p>
<p>Before applying the metrics, the predicted code and the target code are tokenized using the tree-sitter library [37].</p>
<p>The tokenization is illustrated in the following example.  Table 4 shows the metric results for each model. BLEU,H and IoU,H denote the Pearson correlation between the metrics and human judgments for usefulness, computed using the Pearson product-moment correlation method [41]. 15 We observe that across all the models in our test, the IoU metric correlates more strongly with human judgements than BLEU. Furthermore, the correlation factors for IoU are also more consistent across models. Figure 5 visualizes the relation between the two metrics (BLEU and IoU) and the user-perceived usefulness with the predictions of the natural model. The best linear fit for the data points is shown in blue, while the red dotted line visualizes the theoretical line on which the metric and usefulness would have perfect correlations.  We manually examined 30 cases where the difference between IoU score and usefulness score was larger than 0.33. Under a perfect correlation, this difference would correspond to one step higher or lower on the annotation scale (e.g. the difference between 'Disagree' and 'Agree'). We found that IoU tends to underestimate the human-assigned usefulness and identified three root causes for the mismatches:</p>
<p>In 12 of 30 failure cases, the prediction contained the target code but also included other code, such as instructions to display or print variables. While this additional code was often relevant, it significantly decreased the IoU score.</p>
<p>In 7 of 30 failure cases, the model predicted code that satisfied the expressed intent but was syntactically significantly different from the target code. In some cases, the predicted code was more compact and idiomatic than the actual target code. These cases are inherently difficult for any metric that relies purely on the syntactic similarity between the predicted and target code.</p>
<p>In 6 of 30 failure cases, IoU was overly sensitive to a missing import statement in the predicted code, particularly when the code to predict was short, while the annotators seem to care less.</p>
<p>From these observations we conclude that exploring new metrics for both NLGP and code prediction models in general is a relevant area for further research.</p>
<p>Summary</p>
<p>We introduced three code prediction models for Python trained on three different datasets: a model trained on only code (no comments), code with injected docstring comments (docstring) and code with unmodified comments (natural). From an initial benchmark and user study we find that a model trained on natural comments leads to better results in an NLGP context (i.e. predictions based on both prior code and an explicit natural language intent). In future work we want to explore whether we can further boost the prediction quality beyond the natural model by combining natural with injected comments and adapting the docstring comments to better match how developers express intent.</p>
<p>Related work</p>
<p>Example-Centric Programming</p>
<p>As mentioned in the introduction, example-centric programming [5] tools are a precursor to natural language-guided programming tools. These tools help users more quickly identify code examples from local or online repositories. BluePrint [5] allows Adobe Flex users to find relevant code examples from online documentation from within the Adobe Flex Builder IDE. BluePrint takes as input a natural language search query and augments this query with the programming language and framework version used by the developer. Unlike NLGP assistants, Blueprint does not take into account the specific code context and does not adapt the code examples to the specific context of use.</p>
<p>Code assistant tools like Prospector [23] and PARSEWeb [36] focus on the problem of helping developers navigate complex object-oriented APIs. These approaches share with NLGP the idea of mining common coding idioms from existing code repositories, but do not employ natural language intent to guide the search.</p>
<p>Statistical Code Prediction Models</p>
<p>We cover related work that specifically frames code autocompletion as a statistical code prediction problem. We divide related work into three categories, depending on what input the prediction model uses: context-only models use only the existing code context to predict subsequent code tokens; intent-only models use only natural language intent as input without regard for prior code context; finally context+intent models use both.</p>
<p>Context-Only Models Tabnine [35] and Kite [22] are recent examples of proprietary code autocompletion tools that were trained on code context only. For both, the aim is to complete the line of code that the developer is actively editing. Tabnine uses a GPT-2 language model [29] trained on open source code. A detailed study of GPT-2 autocompletion was carried out by Svyatkovskiy et al. [34]. They also discussed optimizations such as completion caching to enable efficient deployments of these models. While such statistical code completers can be very effective, they assume that the developer already knows how to start implementing a task. If a developer were to invoke such tools at the end of an inline comment as one would write for an NLGP assistant, these tools would try to autocomplete the comment rather than the next lines of code.</p>
<p>Intent-Only Models Some approaches in this category focus on predicting only API call(s) while others try to predict the entire target code. Raghothaman et al. [30] use the clickthrough data of a web search engine to train a model that can translate from user queries into APIs that are likely relevant to the query. They then post-process the relevant APIs into type-safe code examples. This approach does not adapt the generated code to the context of use. Gu et al. [13] use an RNN encoder-decoder to generate API usage sequences for a given natural language (NL) query. Srinivasan et al. [19] predict Java methods given an NL query and a summary of the rest of the class, with a custom LSTM-based encoder-decoder model with attention. Clement et al. [8] use a T5 encoder-decoder transformer trained on different objectives to predict the Python implementation of a method given its signature and (if available) its docstring. The authors use the Python subset of the CodesearchNet dataset [18] and scrape GitHub repositories for methods with and without docstrings. Yin and Neubig [50] generate Python code using an LSTM-based encoder-decoder that produces syntactically correct code by construction. Xu et al. [46] performed a user study with a code prediction plugin based on an ensemble of TRANX [50] and a code search model. The plugin suggests code snippets based on a natural language query that is issued within an IDE. Their setup is therefore closely related to natural language-guided programming, except that the plugin does not leverage the surrounding code. The user study did not provide conclusive evidence that the plugin had a significant influence on programmer productivity: neither on the speed with which programmers solved tasks nor on the correctness of the implementations.</p>
<p>Context+Intent Models Murali et al. [25] predict a Java method body given 1) NL keywords and 2) API calls or classes that should be used. Based on these inputs a probabilistic encoder-decoder named "Gaussian Encoder-Decoder" (GED) was used to learn a distribution over simplified control-flow graphs ("program sketches"). Agashe et al. [2] study both API call prediction and full code prediction based on the preceding code and a natural language query. They experimented with LSTMs and small Transformer models without pretraining on a large text corpus. Orlanski and Gittens [27] studied code generation from StackOverflow questions, which often embed code snippets to provide additional context.</p>
<p>Chen et al. [7] introduce Codex, a series of large language models (up to 12B parameters) based on the GPT-3 [6] architecture. By training the models on a large corpus (159GB) of open source code they find that Codex performs significantly better than GPT-3 on the task of predicting full code solutions to programming problems from natural language docstrings. Whereas our work on NLGP focuses on generating small snippets of code to help a programmer more effectively explore and use known APIs, Codex is evaluated on generating functionally correct code. Testing functional correctness of code requires executable unit tests which may not always be available in practical settings.</p>
<p>Code Prediction Benchmark Data</p>
<p>In our search for usable benchmarks, we find that existing benchmarks typically consist of intent and target code while benchmarks suitable to test NLGP assistants require context, intent, and target code.</p>
<p>Intent/Target Benchmarks: Yin et al. [49] create a curated dataset called CoNaLa [9] from Stack Overflow posts. Heyman et al. [16] created a benchmark for "annotated code search": the retrieval of code snippets (target code) annotated with a short natural language description (intent). Yao et al. [48] mined question-code (Intent/target) pairs in Python and SQL. Hamel Husain et al. [18] collected query/code (intent/target) pairs from Bing search queries that have high clickthrough rates to code written in Go, Java, JS, PHP Python or Ruby. Barone et al. [3] extracted 100K target/intent samples from Github projects. The intent is retrieved from docstrings that describe function declarations and bodies. Chen et al. [7] introduce HumanEval, a benchmark of 164 manually composed programming problems and their Python solutions, consisting of a function signature, docstring, function body and several unit tests. While this benchmark is useful to measure functional correctness of generated code, the benchmark problems are typical programming challenges focused on mathematical concepts using built-in abstractions like numbers and lists, and are therefore not suitable to assess code generation for programmer intents in API-rich settings such as the Python data science domain. The problems are also self-contained, not requiring prior code context.</p>
<p>Context/Intent/Target Benchmarks: Agashe et al.</p>
<p>[2] created a dataset of 3.7K curated examples called JuICe. The samples are extracted from Jupyter notebooks containing Python code. As these notebooks were originally created as student assignments, the natural language intents tend to be long, descriptive and often contain information that is only loosely related to the target code. An average intent in the dataset measures 58.33 tokens and is therefore less suited for NLGP, where we expect the intent to be formulated as a short query of between 3 and 10 tokens. This expectation is based in part on observations from a user study conducted by Xu et al. [46].</p>
<p>Program Synthesis</p>
<p>Program synthesis methods [14] study the broader problem of generating programs from specifications. Specifications can range from highly formal and unambiguous (e.g. a formula in logic) to informal and ambiguous (e.g. input-output examples, natural language or program sketches [33]). Most closely related to NLGP is the idea of program synthesis from natural language input [10]. These methods focus on translating a natural language intent (often just a single sentence) into a short program that covers the intent. A key difference with NLGP is that these methods typically focus on helping end-users in specific domains: the natural language input is restricted to a specific application domain and the programs are written in a domain-specific language (DSL) that is often custom-built to solve a specific problem. This contrasts with NLGP which is aimed at helping professional software developers solve a variety of tasks using a general-purpose programming language.</p>
<p>A Research Agenda for Natural Language-Guided Programming</p>
<p>As with any proposal that aims to offer radically new ways to program computers, the idea of writing code guided by free-form natural language brings with it a whole new range of problems and unexplored areas. What research questions does the programming community need to address to turn natural language-guided programming from a research idea into a reliable "proven" method of programming?</p>
<p>We list significant open questions that remain unanswered by the case study presented in this work. Each of these represents a major avenue for future research in natural language-guided programming:</p>
<p>More Diverse Training Data It is to be expected that training models on more source code will further increase the quality of code predictions. In addition, rather than simply training models on more code, it would be useful to consider additional sources of NL intent/code pairs, such as tutorial documentation or Q&amp;A forum threads (such as those found on Stack Overflow).</p>
<p>Better Benchmark Datasets Progress in machine learning and NLP is often driven by high-quality benchmarks (e.g., the GLUE benchmark [40]). In the same vein, we believe better benchmarks for code prediction are a key enabler for better NLGP assistants. In this work we have taken the first steps towards this goal, but our benchmark remains limited in size (201 examples) and in scope (Python data science). We hope that the community will advance these efforts.</p>
<p>Better Metrics Development of a benchmark not only entails creating curated triplets of realistic code contexts, NL intents and ground-truth code completions, but also entails finding better code scoring metrics whose output correlates even better with user-perceived usefulness. Right now, the most effective way to measure the usefulness of a code-prediction tool is to have human experts rate the predicted code in relation to the stated task and the given code context. This method is not very scalable, especially when considering comparing multiple (or multiple versions of) code prediction models.</p>
<p>What is needed is an easy to calculate and objective metric that can score the output of code prediction models with reference to one or more ground-truth solutions.</p>
<p>In this work, we used standard metrics such as BLEU and IoU to compute the similarity between predicted code and the ground-truth target code. We have shown that these metrics correlate with user-perceived usefulness to some extent (Section 4.6.3). There is ample opportunity to improve upon these metrics with new metrics more specifically tailored to code.</p>
<p>Effect on Productivity Does NLGP positively affect developer productivity as measured by e.g. the time to complete set programming tasks? Even though the ultimate goal of code-prediction models is to maximize the productivity of developers and the quality of the code, there is a relative paucity of research that quantifies these claims. Recent work by Xu et al. [46] aims to address this through a controlled user study where two groups of programmers were tasked to complete a set of well-defined programming tasks, with and without the help of a code-prediction tool. The results from the study were inconclusive as to the positive effect of the code-prediction tool under study. There is a clear need for more of these studies with larger participation, more diverse tasks and more code-prediction tools.</p>
<p>Effect on Code Quality Does NLGP positively affect the quality of code as measured by e.g. reported bugs attributed to code (partially) suggested by NLGP assistants? Does NLGP positively affect the maintainability of code? Effect on Learning Curve What is the effect of NLGP on the learning curve of a developer? For example, are NLGP assistants better suited to junior developers or are they helpful across many levels of prior coding experience? Are NLGP assistants more useful for developers new to a project or do they remain useful even for senior developers on the team? Impact of Text-to-Code Ratio How does the effectiveness of NLGP relate to the ratio of code versus natural language text in coding environments? Our case study focused on Jupyter notebooks where the ratio of natural language text (in-line comments, markdown text cells) compared to code is likely higher than in a typical Python script (a '.py' source file). It is intuitively clear that a higher ratio of text-to-code will help NLGP, but we have yet to establish objective relationships between text-to-code ratio and NLGP effectiveness as measured through benchmarks. Inference Latency For modern neural architectures such as Transformers, deep learning researchers have observed that larger models perform better. Our initial experience in training GPT-2 models of various sizes (not further detailed in this paper) for the NLGP task confirms this observation. We have deliberately kept the model size constrained to keep the latency of code predictions within an acceptable threshold of 2 seconds. We expect increasing the inference speed of language models by algorithmic or hardware improvements will be an important driver to enable larger and therefore better code predictions. Effective Use of Code Context How to leverage the code context more effectively? Because for transformer architectures such as GPT-2, memory and time complexity scale quadratically in the sequence length, it is infeasible to provide an entire code file as context to such models. Our case study uncovered that a significant proportion of the mistakes occurred when relevant code (e.g. import statements) fell out of the model's context window. For example, we observed that the language models we trained at times predict functional calls that look relevant but do not exist. We conjecture that this phenomenon is caused by training with a limited context window, because during training the model will at times be forced to predict function calls without seeing its definition or imports. Therefore, new strategies to select what parts of a code file will be included in the context window, more efficient tokenization methods (i.e. encoding the same code with fewer tokens), and exploring linear/subquadratic transformer variants could all lead to more informed predictions. API Versioning Building an NLGP assistant by training a model on existing code runs the risk of biasing the model's predictions towards older or more frequently used versions of common APIs. Ideally, an NLGP tool would also have access to the precise versions of the libraries used by the developer so that it can tailor its code suggestions to those versions. This would help overcome a key limitation of examplecentric programming, as studies of Stack Overflow found that code found in answers to coding questions was frequently obsolete [31], with one study finding that for a sample of known-obsolete answers only 20.5% were updated to reflect the latest API usage [51].</p>
<p>Impact of Interactive Programming Can interactive programming environments further improve the effectiveness of NLGP by giving the NLGP assistant access to (a description of) the runtime values manipulated by the code?</p>
<p>Interactive programming environments such as notebook environments (Jupyter, Zeppelin, Observable, etc.) or IDEs that prominently support read-eval-print loops (e.g. Dr-Racket, BlueJ) offer the capability to execute small code fragments and get immediate feedback on their runtime effects. For example, in a Jupyter notebook, the output of a code cell is often inserted as rich output in the notebook environment itself (as a graphic, a structured table or as plain text).</p>
<p>Taking this one step further, "Live Programming" environments [24] aim to merge code and runtime context even further, giving near-continuous feedback on the runtime values stored in program variables. We conjecture that an NLGP assistant could make effective use of these additional context inputs to improve its suggestions.</p>
<p>Conclusion</p>
<p>We define natural language-guided programming as the programming practice of using intelligent code completion tools to automate routine programming tasks by stating the desired intent using natural language. An NLGP assistant is a tool that autocompletes a piece of code guided by natural language intent.</p>
<p>We demonstrate natural language-guided programming for automating routine data science and machine learning tasks using Python. We contribute the design, implementation and evaluation of a proof-of-concept NLGP assistant based on language modeling.</p>
<p>We conduct experiments with pretrained models (GPT-2), revealing that preparation of the data to contain a good mix of natural language intent and code is critical to improve code prediction quality. Our experiments suggest that comments that occur naturally in the code are sufficient for language models to learn the relationship between intent and code. Our docstring injection method further indicates that NLGP can be made feasible in domains where the source code lacks good inline comments.</p>
<p>We construct a curated benchmark to measure the quality of code predictions. Our initial human evaluation study provides evidence that our best models can generate code predictions that expert data scientists find useful, and that are compatible with the context of use. As such, our work can be seen as a first step towards making automatic example embedding a reality.</p>
<p>Much work remains to be done to turn NLGP from an initial idea into a practical, reliable programming practice. We end the paper with a Research Agenda for NLGP, inviting the programming research community to work on better benchmarks, to set up user studies to quantify the impact on productivity, and to invent novel metrics to automate the scoring of code predictions. Our initial experiments reveal inconsistencies between widely used metrics and human judgments, which we hope will inspire others to invent better alternatives.</p>
<p>• Skip candidates for which the candidate intent:</p>
<p>only contains commented source code does not express the intent of (part of) the candidate target code (e.g. skip comments from exercise notebooks such as "Start your code here" ) -is domain-specific and cannot be translated into the target code without expert knowledge about a particular domain. The intent should be expressed in terms that refer to the functionality of Python/ Python libraries, it should not express the more high-level, domain-specific goal for why the libraries are needed. Note that this guideline does not imply that the intent has to include library names or API calls. • Skip candidates for which the target code:</p>
<p>does not contain at least one non-trivial API call (e.g. "Fg = Fn * g"); -exclusively consists of setup/initialization code; -is a non-idiomatic implementation of the intent • For the remaining test cases:</p>
<p>select the target code from the candidate target code reformulate the candidate intent to make it more realistic, if necessary. For example, "and now plot the data" can be reformulated as "plot the data". However, to avoid that the intent would be systematically biased towards the annotator preferences, annotators are not allowed to further reformulate the original intent. Similarly, annotators are instructed to not correct potential typos. Figure 6. Screenshot of the user interface for annotating candidate test cases to create the NLGP benchmark.</p>
<p>A.1.2 Annotation Interface. Figure 6 shows the user interface for annotating candidate test cases to create the NLGP benchmark. It is implemented as a web service using the Label Studio framework. 16 A.1.3 Inter-Annotator Agreement. We evaluated the inter-annotator agreement on three aspects: the decision to accept or skip a test case, the intent selection, and the target code selection. The Fleiss kappa score with regard to accepting/skipping test cases is 0.515. This reflects moderate agreement [39]. Out of the candidate test cases that were accepted by at least two annotators, annotators had the same intent for 74% of the cases and had annotated the same target code for 71% of the cases. For the cases without exact agreement, the intents had an average edit distance of 3.3 tokens and the target code snippets differed with an average of 3.2 lines of code. Figure 7 gives insight into what libraries are used in the target code fragments in the NLGP benchmark. For each of the 201 test cases, we analyzed the imports and calls in the target code and attempt to resolve these to their root module. We rely on a similar resolution method to what was used for the docstring injection (see 4.4). Note that there will be cases where the root module is not resolved correctly, but  We display each test case with the truncated context in black, the intent in blue and the target code in red.</p>
<p>A.2 NLGP Benchmark Library Distribution</p>
<p>overall the method should be accurate enough to capture the module distribution.</p>
<p>A.3 Examples</p>
<p>In Figure 8, we list three test cases from the NLGP benchmark and the predictions made by the three models under test. We also provide the scores assigned by BLEU and IoU as well as the average usefulness score assigned by the users. Note that due to space constraints we truncated the context code.</p>
<p>Figure 2 .
2Jupyter notebook with the result of executing the code cell after inserting the NLGP assistant's suggested code.</p>
<p>Figure 4 .
4Distributions of the answers in the prediction quality survey: three data science experts each assess model predictions for 100 test cases w.r.t. claims about their usefulness, coverage of the intent, precision, and compatibility with the existing source code.</p>
<p>Figure 5 .
5Scatter plots that map out the relation between the metric scores (horizontal axis) and the user-perceived usefulness (vertical axis) for the predictions of the natural model.</p>
<p>Figure 7 .
7Plot of the frequencies with which modules are used in the target code of the 201 test cases in the NLGP benchmark.</p>
<p>Figure 8 .
8Three test cases from the NLGP benchmark and the predictions for the natural, docstring and no comments models.</p>
<p>Table 1 .
1Key projects in the Python Data StackProject 
Purpose 
NumPy 
N-dimensional arrays and extensive math 
operations. 
SciPy 
Advanced math (solvers, optimizers). 
Pandas 
Rich data manipulation for tabular data. 
Matplotlib 
2D data plotting. 
Scikit-learn Comprehensive machine learning toolkit. 
Jupyter 
Interactive notebooks with text, code, 
math and graphics. </p>
<p>The 250 most frequently used, non-standard Python library root module names are kept. 2. A blank virtual environment is created in which packages, together with their package dependencies are installed using the 'pip' command[1]. For most packages, pip is able to install via root module name (e.g. numpy, sklearn, etc). Only a few need an explicit modulepackage name mapping. 3. Using a custom crawler program all installed packages and standard python libraries/packages are recursively scanned to find all callable program entities.For each 
harvested entity, the fully qualified pathname (FQPN) 
and the first sentence from the associated docstring 
are automatically extracted and stored in the mapping 
table. 
4. We obtain a mapping from FQPN to docstring titles for 
64.6% of the visited callable entities. Entities without 
an associated docstring are ignored and not recorded 
in the mapping. </p>
<p>Table 2 .
2Example entity-docstring mappings sklearn.cluster.KMeans().predict() 'Predict closest cluster each sample ... 'Fully qualified path name 
Docstring title 
sklearn.cluster.KMeans() 
'K-Means clustering' </p>
<p>Table 3 .
3Summary statistics for the NLGP benchmark. LoC stands for 'lines of code".number of samples 
201 
average LoC context 
268 
average LoC target code 
2.45 
average # tokens in intent 5.39 </p>
<p>Table 4 .
4Average BLEU and IoU score on the benchmark, along with their correlation coefficients with humanassigned usefulness scores.model 
BLEU IoU </p>
<p>BLEU,H 
IoU,H </p>
<p>natural 
0.25 
0.45 0.62 
0.70 
docstring 
0.18 
0.40 0.57 
0.65 
no comments 0.06 
0.27 0.73 
0.74 
all models 
0.16 
0.37 0.63 
0.69 </p>
<p>The models are shared on https://huggingface.co/Nokia, and the benchmark and user annotations can be downloaded from https://zenodo.org/ record/5384768#.YTDsN9MzZUJ.
See e.g. this New York Times article dd. July 29, 2020: https://www.nytimes. com/2020/07/29/opinion/gpt-3-ai-automation.html 8 See Tabnine blog dd. July 15, 2019 https://web.archive.org/web/ 20201204055827if_/https://www.tabnine.com/blog/deep, archived Dec. 4, 2020 and GitHub CoPilot, https://copilot.github.com/, retrieved Aug. 3, 2021.9 The space and time complexity for generating the most likely sequence scales exponentially: ( | | ), where | | is the vocabulary size and is the sequence length.
We survey relevant datasets in Section 5. 11 https://nbconvert.readthedocs.io/en/latest/12 We skipped notebooks containing code written in languages other than Python (e.g. Julia, R), as well as notebooks under .ipynb_checkpoint/ folders.
In work following the reported experiments, we used the ONNX runtime framework (https://www.onnxruntime.ai/) to bring down the average prediction latency of these models down to 0.3 seconds, which is sufficiently fast to enable interactive code predictions within an IDE.
To be able to correlate the 4-point reviewer scale with the other metrics, we convert from the 4-point scale to the interval 0-1 as follows: Strongly disagree = 0, Disagree = 1/3, Agree = 2/3, Strongly agree = 1.
https://labelstud.io/
AcknowledgmentsWe would like to thank our colleagues Frederik Vandeputte, Bart Theeten, Maayan Goldstein, Guillermo Rodriguez-Navas and Cecilia Gonzalez-Alvarez for discussions and their help collecting and labeling the data used in our experiments.A AppendixA.1 Annotation Process A.1.1 Annotation Guidelines. The annotators received the following guidelines:
JuICe: A Large Scale Distantly Supervised Dataset for Open Domain Contextbased Code Generation. Rajas Agashe, Srinivasan Iyer, Luke Zettlemoyer, 10.18653/v1/D19-1546Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). Association for Computational Linguistics. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). Association for Computational LinguisticsHong Kong, ChinaRajas Agashe, Srinivasan Iyer, and Luke Zettlemoyer. 2019. JuICe: A Large Scale Distantly Supervised Dataset for Open Domain Context- based Code Generation. In Proceedings of the 2019 Conference on Empir- ical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). As- sociation for Computational Linguistics, Hong Kong, China, 5436-5446. https://doi.org/10.18653/v1/D19-1546</p>
<p>A parallel corpus of Python functions and documentation strings for automated code documentation and code generation. Antonio Valerio, Miceli Barone, Rico Sennrich, arXiv:1707.02275Antonio Valerio Miceli Barone and Rico Sennrich. 2017. A parallel corpus of Python functions and documentation strings for automated code documentation and code generation. CoRR abs/1707.02275 (2017). arXiv:1707.02275 http://arxiv.org/abs/1707.02275</p>
<p>Example Embedding. Ohad Barzilay, 10.1145/2089131.2089135Proceedings of the 10th SIGPLAN Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software (Onward! 2011). the 10th SIGPLAN Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software (Onward! 2011)New York, NY, USAAssociation for Computing MachineryOhad Barzilay. 2011. Example Embedding. In Proceedings of the 10th SIGPLAN Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software (Onward! 2011). Association for Computing Machinery, New York, NY, USA, 137-144. https://doi.org/10.1145/ 2089131.2089135</p>
<p>Example-Centric Programming: Integrating Web Search into the Development Environment. Joel Brandt, Mira Dontcheva, Marcos Weskamp, Scott R Klemmer, 10.1145/1753326.1753402Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI '10). the SIGCHI Conference on Human Factors in Computing Systems (CHI '10)New York, NY, USAAssociation for Computing MachineryJoel Brandt, Mira Dontcheva, Marcos Weskamp, and Scott R. Klemmer. 2010. Example-Centric Programming: Integrating Web Search into the Development Environment. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI '10). Association for Computing Machinery, New York, NY, USA, 513-522. https://doi.org/ 10.1145/1753326.1753402</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners. H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. LinScott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec RadfordCurran Associates, Inc33Advances in Neural Information Processing SystemsTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners. In Advances in Neural Information Processing Systems, H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (Eds.), Vol. 33. Curran Associates, Inc., 1877-1901. https://proceedings.neurips.cc/paper/ 2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, arXiv:cs.LG/2107.03374Ilya Sutskever, and Wojciech Zaremba. 2021. Evaluating Large Language Models Trained on Code. Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec RadfordDave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin; Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlishMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Fe- lipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, An- drew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. Evaluating Large Lan- guage Models Trained on Code. arXiv:cs.LG/2107.03374</p>
<p>Alexey Svyatkovskiy, and Neel Sundaresan. 2020. PyMT5: multi-mode translation of natural language and Python code with transformers. Colin B Clement, Dawn Drain, Jonathan Timcheck, arXiv:cs.LG/2010.03150Colin B. Clement, Dawn Drain, Jonathan Timcheck, Alexey Svy- atkovskiy, and Neel Sundaresan. 2020. PyMT5: multi-mode trans- lation of natural language and Python code with transformers. arXiv:cs.LG/2010.03150</p>
<p>CoNaLa: The Code/Natrual Language Challenge. Retrieved. Conala, CoNaLa. 2021. CoNaLa: The Code/Natrual Language Challenge. Re- trieved April 2, 2021 from https://conala-corpus.github.io/</p>
<p>Program Synthesis Using Natural Language. Aditya Desai, Sumit Gulwani, Vineet Hingorani, Nidhi Jain, Amey Karkare, Mark Marron, R Sailesh, Subhajit Roy, 10.1145/2884781.2884786Proceedings of the 38th International Conference on Software Engineering (ICSE '16). the 38th International Conference on Software Engineering (ICSE '16)New York, NY, USAAssociation for Computing MachineryAditya Desai, Sumit Gulwani, Vineet Hingorani, Nidhi Jain, Amey Karkare, Mark Marron, Sailesh R, and Subhajit Roy. 2016. Program Synthesis Using Natural Language. In Proceedings of the 38th Inter- national Conference on Software Engineering (ICSE '16). Association for Computing Machinery, New York, NY, USA, 345-356. https: //doi.org/10.1145/2884781.2884786</p>
<p>A new algorithm for data compression. Philip Gage, https:/dl.acm.org/doi/10.5555/177910.177914C Users Journal. 12Philip Gage. 1994. A new algorithm for data compression. C Users Journal 12, 2 (1994), 23-38. https://dl.acm.org/doi/10.5555/177910. 177914</p>
<p>Alex Graves, arXiv:1211.3711Sequence transduction with recurrent neural networks. arXiv preprintAlex Graves. 2012. Sequence transduction with recurrent neural net- works. arXiv preprint arXiv:1211.3711 (2012).</p>
<p>Deep API Learning. Xiaodong Gu, Hongyu Zhang, Dongmei Zhang, Sunghun Kim, 10.1145/2950290.2950334Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering (FSE 2016). the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering (FSE 2016)New York, NY, USAAssociation for Computing MachineryXiaodong Gu, Hongyu Zhang, Dongmei Zhang, and Sunghun Kim. 2016. Deep API Learning. In Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering (FSE 2016). Association for Computing Machinery, New York, NY, USA, 631-642. https://doi.org/10.1145/2950290.2950334</p>
<p>. Sumit Gulwani, Oleksandr Polozov, Rishabh Singh, 10.1561/2500000010Program Synthesis. Foundations and Trends in Programming Languages. 4Sumit Gulwani, Oleksandr Polozov, and Rishabh Singh. 2017. Program Synthesis. Foundations and Trends in Programming Languages 4, 1-2 (2017), 1-119. https://doi.org/10.1561/2500000010</p>
<p>Are Deep Neural Networks the Best Choice for Modeling Source Code. J Vincent, Premkumar Hellendoorn, Devanbu, 10.1145/3106237.3106290Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering. the 2017 11th Joint Meeting on Foundations of Software EngineeringNew York, NY, USAAssociation for Computing MachineryVincent J. Hellendoorn and Premkumar Devanbu. 2017. Are Deep Neural Networks the Best Choice for Modeling Source Code?. In Pro- ceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering (ESEC/FSE 2017). Association for Computing Machinery, New York, NY, USA, 763-773. https://doi.org/10.1145/3106237.3106290</p>
<p>Geert Heyman, Tom Van Cutsem, arXiv:cs.IR/2008.12193Neural Code Search Revisited: Enhancing Code Snippet Retrieval through Natural Language Intent. Geert Heyman and Tom Van Cutsem. 2020. Neural Code Search Re- visited: Enhancing Code Snippet Retrieval through Natural Language Intent. arXiv:cs.IR/2008.12193</p>
<p>On the Naturalness of Software. Abram Hindle, Earl T Barr, Zhendong Su, Mark Gabel, Premkumar Devanbu, https:/dl.acm.org/doi/10.5555/2337223.2337322Proceedings of the 34th International Conference on Software Engineering (ICSE '12). the 34th International Conference on Software Engineering (ICSE '12)IEEE PressAbram Hindle, Earl T. Barr, Zhendong Su, Mark Gabel, and Premkumar Devanbu. 2012. On the Naturalness of Software. In Proceedings of the 34th International Conference on Software Engineering (ICSE '12). IEEE Press, 837-847. https://dl.acm.org/doi/10.5555/2337223.2337322</p>
<p>Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, arXiv:cs.LG/1909.09436Miltiadis Allamanis, and Marc Brockschmidt. 2020. CodeSearchNet Challenge: Evaluating the State of Semantic Code Search. Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc Brockschmidt. 2020. CodeSearchNet Challenge: Evaluating the State of Semantic Code Search. arXiv:cs.LG/1909.09436</p>
<p>Mapping Language to Code in Programmatic Context. Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, Luke Zettlemoyer, 10.18653/v1/D18-1192Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational LinguisticsSrinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. 2018. Mapping Language to Code in Programmatic Context. In Proceed- ings of the 2018 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Brussels, Bel- gium, 1643-1652. https://doi.org/10.18653/v1/D18-1192</p>
<p>Speech and Language Processing: An introduction to speech recognition, computational linguistics and natural language processing. Daniel Jurafsky, H James, Martin, https:/dl.acm.org/doi/book/10.5555/1214993Daniel Jurafsky and James H Martin. 2008. Speech and Language Pro- cessing: An introduction to speech recognition, computational linguistics and natural language processing. https://dl.acm.org/doi/book/10.5555/ 1214993</p>
<p>Maybe deep neural networks are the best choice for modeling source code. Rafael-Michael Karampatsis, Charles Sutton, arXiv:1903.05734arXiv preprintRafael-Michael Karampatsis and Charles Sutton. 2019. Maybe deep neural networks are the best choice for modeling source code. arXiv preprint arXiv:1903.05734 (2019).</p>
<p>Stay in flow. Kite, Code faster. RetrievedKite. 2021. Code faster. Stay in flow. Retrieved April 2, 2021 from https://www.kite.com/</p>
<p>Jungloid Mining: Helping to Navigate the API Jungle. David Mandelin, Lin Xu, Rastislav Bodík, Doug Kimelman, 10.1145/1065010.1065018Proceedings of the 2005 ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI '05). the 2005 ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI '05)New York, NY, USAAssociation for Computing MachineryDavid Mandelin, Lin Xu, Rastislav Bodík, and Doug Kimelman. 2005. Jungloid Mining: Helping to Navigate the API Jungle. In Proceedings of the 2005 ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI '05). Association for Computing Machinery, New York, NY, USA, 48-61. https://doi.org/10.1145/1065010.1065018</p>
<p>Usable Live Programming. Sean Mcdirmid, 10.1145/2509578.2509585Proceedings of the 2013 ACM International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software (Onward! 2013). the 2013 ACM International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software (Onward! 2013)New York, NY, USAAssociation for Computing MachinerySean McDirmid. 2013. Usable Live Programming. In Proceedings of the 2013 ACM International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software (Onward! 2013). Association for Computing Machinery, New York, NY, USA, 53-62. https://doi. org/10.1145/2509578.2509585</p>
<p>Bayesian Sketch Learning for Program Synthesis. Vijayaraghavan Murali, Swarat Chaudhuri, Chris Jermaine, arXiv:1703.05698Vijayaraghavan Murali, Swarat Chaudhuri, and Chris Jermaine. 2017. Bayesian Sketch Learning for Program Synthesis. CoRR abs/1703.05698 (2017). arXiv:1703.05698 http://arxiv.org/abs/1703.05698</p>
<p>Neural Sketch Learning for Conditional Program Generation. Vijayaraghavan Murali, Letao Qi, Swarat Chaudhuri, Chris Jermaine, International Conference on Learning Representations. Vijayaraghavan Murali, Letao Qi, Swarat Chaudhuri, and Chris Jer- maine. 2018. Neural Sketch Learning for Conditional Program Gener- ation. In International Conference on Learning Representations.</p>
<p>Reading StackOverflow Encourages Cheating: Adding Question Text Improves Extractive Code Generation. Gabriel Orlanski, Alex Gittens, 10.18653/v1/2021.nlp4prog-1.8Proceedings of the 1st Workshop on Natural Language Processing for Programming. the 1st Workshop on Natural Language Processing for ProgrammingAssociation for Computational Linguistics, OnlineGabriel Orlanski and Alex Gittens. 2021. Reading StackOverflow En- courages Cheating: Adding Question Text Improves Extractive Code Generation. In Proceedings of the 1st Workshop on Natural Language Processing for Programming (NLP4Prog 2021). Association for Compu- tational Linguistics, Online, 65-76. https://doi.org/10.18653/v1/2021. nlp4prog-1.8</p>
<p>BLEU: A Method for Automatic Evaluation of Machine Translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, 10.3115/1073083.1073135Proceedings of the 40th Annual Meeting on Association for Computational Linguistics (ACL '02). the 40th Annual Meeting on Association for Computational Linguistics (ACL '02)USAAssociation for Computational LinguisticsKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: A Method for Automatic Evaluation of Machine Translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics (ACL '02). Association for Computational Linguistics, USA, 311-318. https://doi.org/10.3115/1073083.1073135</p>
<p>Language Models are Unsupervised Multitask Learners. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language Models are Unsupervised Multitask Learners. (2019).</p>
<p>SWIM: Synthesizing What i Mean: Code Search and Idiomatic Snippet Synthesis. Mukund Raghothaman, Yi Wei, Youssef Hamadi, 10.1145/2884781.2884808Proceedings of the 38th International Conference on Software Engineering (ICSE '16). the 38th International Conference on Software Engineering (ICSE '16)New York, NY, USAAssociation for Computing MachineryMukund Raghothaman, Yi Wei, and Youssef Hamadi. 2016. SWIM: Synthesizing What i Mean: Code Search and Idiomatic Snippet Syn- thesis. In Proceedings of the 38th International Conference on Software Engineering (ICSE '16). Association for Computing Machinery, New York, NY, USA, 357-367. https://doi.org/10.1145/2884781.2884808</p>
<p>Toxic Code Snippets on Stack Overflow. Chaiyong Ragkhitwetsagul, Jens Krinke, Matheus Paixao, Giuseppe Bianco, Rocco Oliveto, 10.1109/TSE.2019.2900307IEEE Transactions on Software Engineering. 47Chaiyong Ragkhitwetsagul, Jens Krinke, Matheus Paixao, Giuseppe Bianco, and Rocco Oliveto. 2021. Toxic Code Snippets on Stack Over- flow. IEEE Transactions on Software Engineering 47, 3 (2021), 560-581. https://doi.org/10.1109/TSE.2019.2900307</p>
<p>Neural Machine Translation of Rare Words with Subword Units. Rico Sennrich, Barry Haddow, Alexandra Birch, 10.18653/v1/P16-1162Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. the 54th Annual Meeting of the Association for Computational LinguisticsBerlin, GermanyAssociation for Computational Linguistics1Long Papers)Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural Ma- chine Translation of Rare Words with Subword Units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguis- tics (Volume 1: Long Papers). Association for Computational Linguistics, Berlin, Germany, 1715-1725. https://doi.org/10.18653/v1/P16-1162</p>
<p>Program Synthesis by Sketching. Armando Solar-Lezama, Ph.D. Dissertation. USA. 3353225Advisor(s) Bodik, RastislavArmando Solar-Lezama. 2008. Program Synthesis by Sketching. Ph.D. Dissertation. USA. Advisor(s) Bodik, Rastislav. AAI3353225.</p>
<p>Intellicode compose: Code generation using transformer. Alexey Svyatkovskiy, Shengyu Shao Kun Deng, Neel Fu, Sundaresan, Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering. the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software EngineeringAlexey Svyatkovskiy, Shao Kun Deng, Shengyu Fu, and Neel Sundare- san. 2020. Intellicode compose: Code generation using transformer. In Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering. 1433-1443.</p>
<p>Code faster with AI completions. Tabnine, RetrievedTabnine. 2021. Code faster with AI completions. Retrieved April 2, 2021 from https://www.tabnine.com/</p>
<p>Parseweb: A Programmer Assistant for Reusing Open Source Code on the Web. Suresh Thummalapenta, Tao Xie, 10.1145/1321631.1321663Proceedings of the Twenty-Second IEEE/ACM International Conference on Automated Software Engineering (ASE '07). the Twenty-Second IEEE/ACM International Conference on Automated Software Engineering (ASE '07)New York, NY, USAAssociation for Computing MachinerySuresh Thummalapenta and Tao Xie. 2007. Parseweb: A Programmer Assistant for Reusing Open Source Code on the Web. In Proceedings of the Twenty-Second IEEE/ACM International Conference on Automated Software Engineering (ASE '07). Association for Computing Machinery, New York, NY, USA, 204-213. https://doi.org/10.1145/1321631.1321663</p>
<p>tree-sitter: An incremental parsing system for programming tools. tree sitter. 2021. Retrievedtree sitter. 2021. tree-sitter: An incremental parsing system for pro- gramming tools. Retrieved April 23, 2021 from https://github.com/ tree-sitter/tree-sitter</p>
<p>Attention is All you Need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Illia Kaiser, Polosukhin, Advances in Neural Information Processing Systems. I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. GarnettCurran Associates, Inc30Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. 2017. Attention is All you Need. In Advances in Neural Information Pro- cessing Systems, I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (Eds.), Vol. 30. Cur- ran Associates, Inc. https://proceedings.neurips.cc/paper/2017/file/ 3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf</p>
<p>Understanding interobserver agreement: the kappa statistic. J Anthony, Joanne M Viera, Garrett, Fam med. 37Anthony J Viera, Joanne M Garrett, et al. 2005. Understanding in- terobserver agreement: the kappa statistic. Fam med 37, 5 (2005), 360-363.</p>
<p>GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel Bowman, 10.18653/v1/W18-5446Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP. the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLPBrussels, BelgiumAssociation for Computational LinguisticsAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2018. GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP. Association for Computational Linguistics, Brussels, Belgium, 353-355. https://doi.org/10.18653/v1/W18-5446</p>
<p>Pearson correlation coefficient. Wikipedia, RetrievedWikipedia. 2021. Pearson correlation coefficient. Retrieved April 23, 2021 from https://en.wikipedia.org/wiki/Pearson_correlation_ coefficient</p>
<p>HuggingFace's Transformers: State-of-the-art Natural Language Processing. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Jamie Brew, arXiv:1910.03771Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, and Jamie Brew. 2019. HuggingFace's Transformers: State-of-the-art Natural Language Processing. CoRR abs/1910.03771 (2019). arXiv:1910.03771 http://arxiv.org/abs/1910. 03771</p>
<p>What Do Developers Search for on the Web?. Xin Xia, Lingfeng Bao, David Lo, Pavneet Singh Kochhar, Ahmed E Hassan, Zhenchang Xing, 10.1007/s10664-017-9514-4Empirical Softw. Engg. 22Xin Xia, Lingfeng Bao, David Lo, Pavneet Singh Kochhar, Ahmed E. Hassan, and Zhenchang Xing. 2017. What Do Developers Search for on the Web? Empirical Softw. Engg. 22, 6 (Dec. 2017), 3149-3185. https://doi.org/10.1007/s10664-017-9514-4</p>
<p>Why reinventing the wheels? An empirical study on library reuse and re-implementation. Bowen Xu, Le An, Ferdian Thung, Foutse Khomh, David Lo, 10.1007/s10664-019-09771-0Empir Software Eng. 25Bowen Xu, Le An, Ferdian Thung, Foutse Khomh, and David Lo. 2020. Why reinventing the wheels? An empirical study on library reuse and re-implementation. Empir Software Eng 25 (2020), 755-789. https: //doi.org/10.1007/s10664-019-09771-0</p>
<p>Incorporating External Knowledge through Pretraining for Natural Language to Code Generation. F Frank, Zhengbao Xu, Pengcheng Jiang, Bogdan Yin, Graham Vasilescu, Neubig, 10.18653/v1/2020.acl-main.538Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsFrank F. Xu, Zhengbao Jiang, Pengcheng Yin, Bogdan Vasilescu, and Graham Neubig. 2020. Incorporating External Knowledge through Pre- training for Natural Language to Code Generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, Online, 6045-6052. https: //doi.org/10.18653/v1/2020.acl-main.538</p>
<p>F Frank, Bogdan Xu, Graham Vasilescu, Neubig, arXiv:cs.SE/2101.11149IDE Code Generation from Natural Language: Promise and Challenges. Frank F. Xu, Bogdan Vasilescu, and Graham Neubig. 2021. In-IDE Code Generation from Natural Language: Promise and Challenges. arXiv:cs.SE/2101.11149</p>
<p>From Query to Usable Code: An Analysis of Stack Overflow Code Snippets. Di Yang, Aftab Hussain, Cristina Videira Lopes, 10.1145/2901739.2901767Proceedings of the 13th International Conference on Mining Software Repositories (MSR '16). the 13th International Conference on Mining Software Repositories (MSR '16)New York, NY, USAAssociation for Computing MachineryDi Yang, Aftab Hussain, and Cristina Videira Lopes. 2016. From Query to Usable Code: An Analysis of Stack Overflow Code Snippets. In Proceedings of the 13th International Conference on Mining Software Repositories (MSR '16). Association for Computing Machinery, New York, NY, USA, 391-402. https://doi.org/10.1145/2901739.2901767</p>
<p>International World Wide Web Conferences Steering Committee, Republic and Canton of Geneva. Ziyu Yao, Daniel S Weld, Wei-Peng Chen, Huan Sun, 10.1145/3178876.3186081Proceedings of the 2018 World Wide Web Conference (WWW '18). the 2018 World Wide Web Conference (WWW '18)StaQC: A Systematically Mined Question-Code Dataset from Stack OverflowZiyu Yao, Daniel S. Weld, Wei-Peng Chen, and Huan Sun. 2018. StaQC: A Systematically Mined Question-Code Dataset from Stack Overflow. In Proceedings of the 2018 World Wide Web Conference (WWW '18). International World Wide Web Conferences Steering Committee, Republic and Canton of Geneva, CHE, 1693-1703. https: //doi.org/10.1145/3178876.3186081</p>
<p>Learning to Mine Aligned Code and Natural Language Pairs from Stack Overflow. Pengcheng Yin, Bowen Deng, Edgar Chen, Bogdan Vasilescu, Graham Neubig, 10.1145/3196398.3196408Proceedings of the 15th International Conference on Mining Software Repositories (MSR '18). the 15th International Conference on Mining Software Repositories (MSR '18)New York, NY, USAAssociation for Computing MachineryPengcheng Yin, Bowen Deng, Edgar Chen, Bogdan Vasilescu, and Graham Neubig. 2018. Learning to Mine Aligned Code and Natu- ral Language Pairs from Stack Overflow. In Proceedings of the 15th International Conference on Mining Software Repositories (MSR '18). Association for Computing Machinery, New York, NY, USA, 476-486. https://doi.org/10.1145/3196398.3196408</p>
<p>TRANX: A Transitionbased Neural Abstract Syntax Parser for Semantic Parsing and Code Generation. Pengcheng Yin, Graham Neubig, arXiv:cs.CL/1810.02720Pengcheng Yin and Graham Neubig. 2018. TRANX: A Transition- based Neural Abstract Syntax Parser for Semantic Parsing and Code Generation. arXiv:cs.CL/1810.02720</p>
<p>An Empirical Study of Obsolete Answers on Stack Overflow. Haoxiang Zhang, Shaowei Wang, Tse-Hsun Chen, Ying Zou, Ahmed E Hassan, 10.1109/TSE.2019.2906315IEEE Transactions on Software Engineering. 47Haoxiang Zhang, Shaowei Wang, Tse-Hsun Chen, Ying Zou, and Ahmed E. Hassan. 2021. An Empirical Study of Obsolete Answers on Stack Overflow. IEEE Transactions on Software Engineering 47, 4 (2021), 850-862. https://doi.org/10.1109/TSE.2019.2906315</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1220 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1220</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1220</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-28.html">extraction-schema-28</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <p><strong>Paper ID:</strong> paper-211205094</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2002.08795v1.pdf" target="_blank">How To Avoid Being Eaten By a Grue: Exploration Strategies for Text-Adventure Agents</a></p>
                <p><strong>Paper Abstract:</strong> Text-based games -- in which an agent interacts with the world through textual natural language -- present us with the problem of combinatorially-sized action-spaces. Most current reinforcement learning algorithms are not capable of effectively handling such a large number of possible actions per turn. Poor sample efficiency, consequently, results in agents that are unable to pass bottleneck states, where they are unable to proceed because they do not see the right action sequence to pass the bottleneck enough times to be sufficiently reinforced. Building on prior work using knowledge graphs in reinforcement learning, we introduce two new game state exploration strategies. We compare our exploration strategies against strong baselines on the classic text-adventure game, Zork1, where prior agent have been unable to get past a bottleneck where the agent is eaten by a Grue.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1220.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1220.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zork1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zork1 (interactive fiction text-adventure)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A classic text-adventure / dungeon-crawler interactive fiction used as the primary environment in this paper; characterized by rooms, objects, branching quests and explicit bottlenecks (e.g., a dark cellar where the agent dies unless a lamp is lit).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Zork1</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Fantasy/adventure interactive fiction (dungeon-crawler style) where the agent issues text commands to navigate rooms, manipulate objects, and complete quests across a map of rooms and connections.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td>Conditional access based on actions/state (e.g., a dark cellar that kills the agent unless a lamp is lit; boarded/locked doors and other narrative gating that require specific actions or items to proceed).</td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Represented as a sparse, branching (quest) graph often modeled as a directed acyclic graph for quest structure (branching paths with critical bottleneck nodes).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>N/A (environment-level entry)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>total game score progression; sample efficiency / convergence speed (steps to pass bottleneck) discussed qualitatively</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>memory-based / structured (policies that exploit world structure and long-term dependencies, e.g., ones that remember to light lamp before entering cellar)</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>Bottleneck nodes in the quest graph (e.g., entering dark cellar) critically determine agent progress; conditional (state-dependent) edges such as darkness/death create hard-to-cross transitions that require long-term, non-immediately-rewarded actions (e.g., lighting a lamp earlier) to succeed.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Policies that incorporate persistent world knowledge (knowledge-graph based) and explicit mechanisms for handling long-term dependencies and backtracking perform better; chained policies that freeze and restart at bottleneck states faster overcome sequential bottlenecks in the acyclic quest graph.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How To Avoid Being Eaten By a Grue: Exploration Strategies for Text-Adventure Agents', 'publication_date_yy_mm': '2020-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1220.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1220.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KG-A2C-chained</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Knowledge-Graph A2C with policy chaining and bottleneck detection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An extension of KG-A2C that detects bottlenecks (no score improvement within a patience window), freezes the policy that reached the best seen state, and trains a new policy from that state with backtracking to avoid freezing a globally suboptimal policy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Zork1</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Fantasy/adventure interactive fiction used to evaluate chained exploration; navigation requires remembering earlier actions (e.g., lighting lamp) to safely pass certain rooms.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td>Conditional narrative constraints (e.g., darkness causing immediate death unless lamp is lit); implicit room-to-room constraints inferred from navigation actions.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Treated as a sparse, largely acyclic quest graph; algorithm assumes sequential decision problems that can be represented as acyclic directed graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>KG-A2C-chained</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Actor-Critic (A2C) agent with a knowledge-graph based state representation; adds bottleneck detection (patience parameter), policy freezing/chaining, and backtracking buffer of n prior states to restart training from promising precursor states.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>sample efficiency / convergence speed (time/steps to pass bottleneck) and total score growth; measured qualitatively and via learning curves (score vs training steps).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>chained / staged policies that can freeze and re-train from bottleneck states (memory-augmented policies leveraging structured state representation).</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>Chaining policies by detecting bottlenecks is especially effective for environments whose quest structure can be represented as acyclic directed graphs: it increases sample efficiency and faster convergence to policies that pass bottlenecks compared to Go-Explore style sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Explicitly chaining policies and using backtracking is particularly well-suited to sequential acyclic graphs and speeds up overcoming bottleneck nodes; knowledge-graph state information improves bottleneck detection and sample efficiency for chaining.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How To Avoid Being Eaten By a Grue: Exploration Strategies for Text-Adventure Agents', 'publication_date_yy_mm': '2020-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1220.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1220.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KG-A2C-Explore</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Knowledge-Graph A2C combined with Go-Explore Phase 1 (knowledge-graph cell representation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An adaptation of Go-Explore for text games where KG-A2C is trained in parallel and cells (archive entries) are encoded using a snapshot of the knowledge graph together with the game state, and cells are chosen (weighted by score) for focused exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Zork1</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Fantasy/adventure interactive fiction; navigation evaluated under Go-Explore style archival exploration using knowledge-graph snapshots as cell encodings.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td>Action-conditional constraints (e.g., darkness in cellar requires lamp to be lit prior to entry); these are represented in the knowledge graph and affect reachability.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Sparse, branching quest graph with critical bottleneck nodes; cells are used to index promising states rather than operate on explicit full graph metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>KG-A2C-Explore (Go-Explore variant)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Go-Explore Phase 1 style exploration using a KG-A2C policy: an archive of cells is maintained where each cell is represented by the knowledge-graph snapshot + last seen game state; cells are sampled (score-weighted) and KG-A2C runs for a fixed number of steps (cell step size) to expand cells and train the network.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>cell-based exploration progress (archive expansion), sample efficiency, convergence speed to policies that pass bottlenecks, and eventual total score achieved.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>cell-guided exploration policies that exploit structured state encodings (knowledge graph) to prioritize promising states.</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>Knowledge-graph based cell encodings provide better identification of promising states in the quest graph than raw textual observation encodings, improving the ability to find trajectories that pass bottlenecks; however, KG-A2C-Explore converges slower than KG-A2C-chained but still consistently passes the critical bottleneck.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Policies that combine archival cell exploration with structured state representations (knowledge graphs) better recognize promising places in the state graph; Go-Explore style sampling benefits from graph-aware cell representations when topology contains long-term dependencies and bottlenecks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How To Avoid Being Eaten By a Grue: Exploration Strategies for Text-Adventure Agents', 'publication_date_yy_mm': '2020-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1220.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1220.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Knowledge Graph (KG) state</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Knowledge-graph based state representation (triples extracted from observations)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A persistent graph of subject–relation–object triples extracted from textual observations that encodes rooms, objects, item possession, affordances, and inferred navigational relations; used as a map-like structured state representation for partial observability and navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Zork1 (and general text-adventure domains)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Structured symbolic map of the text world: nodes include rooms and objects; edges encode relations, object locations, possession, and inferred room adjacency from navigation actions.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td>KG encodes conditional state information (e.g., item in inventory, room attributes like darkness) and inferred navigational edges, enabling policies to reason about conditional access and prerequisites for transitions.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>KG forms a sparse, incremental map-like graph where rooms and objects are linked; connectivity is inferred from navigation verbs and observations rather than an explicit pre-provided adjacency matrix.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>KG-A2C / agents using KG</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>State encoder that converts observations into a structured graph (triples), used to constrain action templates and as cell encodings for archival exploration; improves handling of partial observability and long-term dependencies.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>sample efficiency and improved identification of promising states (qualitative and by improved final scores / ability to pass bottlenecks).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>memory-augmented / graph-aware policies that use persistent structured state for planning and for constraining action generation.</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>Knowledge graphs improve sample efficiency for bottleneck detection and exploration because they capture stateful prerequisites (e.g., possession and room attributes) that determine whether certain transitions are feasible; KG-based cells outperform purely textual observation encodings in Go-Explore style selection of promising states.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Policies leveraging structured persistent representations require less exploration to detect and cross bottlenecks; KG enables constrained action filling which focuses exploration on admissible, state-relevant actions and reduces wasted exploration in combinatorial action spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How To Avoid Being Eaten By a Grue: Exploration Strategies for Text-Adventure Agents', 'publication_date_yy_mm': '2020-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1220.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1220.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bottleneck detection & chaining</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bottleneck detection via patience window and policy chaining with backtracking</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A procedural method to detect locally limiting (bottleneck) states by monitoring score improvement over a patience window, freezing the policy that reached the best state, and training a new policy from that state with backtracking across a buffer of prior states to avoid freezing suboptimal policies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Zork1 (generalizable to other text-adventure quests modeled as DAGs)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Works on sequential/quest-structured text worlds where progress is measured by cumulative reward and certain states prevent further progress unless specific earlier actions have been taken.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td>Method explicitly targets state-dependent constraints that block further reward progression (e.g., conditional transitions such as dark rooms requiring an item/action).</td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Assumes a sequential decision graph often approximated as acyclic directed graph for the quest structure; backtracking moves training start to predecessor nodes in the observed trajectory buffer.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>KG-A2C-chained / A2C-chained (method applies to agent training)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Training-time meta-policy that pauses/freezes policies at detected bottlenecks and re-initializes training from stored prior states (buffer of n) to find alternative precursor policies, ensuring exploration focuses on escaping local optima.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>sample efficiency / steps-to-pass-bottleneck (qualitative improvement reported); uses 'patience' (number of steps without score improvement) as a detection hyperparameter.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>chained/staged policies with local restarts and backtracking mechanisms to escape local optima at bottleneck nodes.</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>Particularly effective when the environment's quest graph is sequential/acyclic: chaining reduces the number of necessary reattempts to find the long-term action sequence required to pass a bottleneck, increasing sample efficiency relative to undirected exploratory sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Backtracking and policy-chaining help avoid freezing suboptimal policies at bottlenecks; policies that can restart from precursor states and re-explore are better suited to environments with deep long-term dependencies and sparse reward beyond bottlenecks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How To Avoid Being Eaten By a Grue: Exploration Strategies for Text-Adventure Agents', 'publication_date_yy_mm': '2020-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1220.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1220.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>A2C-Explore</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>A2C with Go-Explore style archival exploration using recurrent observation encoding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline that pairs standard A2C (text-only recurrent observation encoder) with a Go-Explore Phase 1 style archive where cells are represented by RNN-encoded textual observations rather than knowledge graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Zork1</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Text-adventure evaluated under cell-based Go-Explore Phase 1 archival exploration using textual observation encodings as cell representations.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td>Narrative/conditional constraints (same domain constraints as other agents) but without explicit KG encoding to represent prerequisites.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Sparse branching quest graph; archival sampling operates over compressed observational encodings rather than an explicit symbolic graph.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>A2C-Explore</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A2C agent using recurrent neural network encoding of textual observations; used within a Go-Explore Phase 1 archive where cells are defined by RNN observation encodings; cell step size hyperparameter controls per-cell rollout length.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>convergence speed and final score achieved; Go-Explore cell-based archive expansion (qualitative comparison to KG variant).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>reactive / recurrent observation-encoded policies combined with archival exploration, but limited by insufficient structural state representation for long-term bottlenecks.</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>RNN-based cell encodings converge faster to lower-reward trajectories but fail to pass critical bottlenecks in Zork1; lack of structured state (KG) makes identifying promising precursor states for crossing bottlenecks less reliable.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Policies relying solely on recurrent textual encodings may converge quickly but to suboptimal behaviors that cannot cross topology-imposed bottlenecks; lacking a persistent structured map hurts exploration in environments with long-term dependencies.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How To Avoid Being Eaten By a Grue: Exploration Strategies for Text-Adventure Agents', 'publication_date_yy_mm': '2020-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Graph constrained reinforcement learning for natural language action spaces. <em>(Rating: 2)</em></li>
                <li>Go-explore: a new approach for hard-exploration problems. <em>(Rating: 2)</em></li>
                <li>Exploration based language learning for text-based games. <em>(Rating: 2)</em></li>
                <li>Textworld: A learning environment for text-based games. <em>(Rating: 2)</em></li>
                <li>Interactive fiction games: A colossal adventure. <em>(Rating: 1)</em></li>
                <li>Algorithmic improvements for deep reinforcement learning applied to interactive fiction. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1220",
    "paper_id": "paper-211205094",
    "extraction_schema_id": "extraction-schema-28",
    "extracted_data": [
        {
            "name_short": "Zork1",
            "name_full": "Zork1 (interactive fiction text-adventure)",
            "brief_description": "A classic text-adventure / dungeon-crawler interactive fiction used as the primary environment in this paper; characterized by rooms, objects, branching quests and explicit bottlenecks (e.g., a dark cellar where the agent dies unless a lamp is lit).",
            "citation_title": "",
            "mention_or_use": "use",
            "environment_name": "Zork1",
            "environment_description": "Fantasy/adventure interactive fiction (dungeon-crawler style) where the agent issues text commands to navigate rooms, manipulate objects, and complete quests across a map of rooms and connections.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": true,
            "dead_ends_count": null,
            "door_constraints_present": true,
            "door_constraints_description": "Conditional access based on actions/state (e.g., a dark cellar that kills the agent unless a lamp is lit; boarded/locked doors and other narrative gating that require specific actions or items to proceed).",
            "graph_connectivity": "Represented as a sparse, branching (quest) graph often modeled as a directed acyclic graph for quest structure (branching paths with critical bottleneck nodes).",
            "environment_size": null,
            "agent_name": "N/A (environment-level entry)",
            "agent_description": null,
            "exploration_efficiency_metric": "total game score progression; sample efficiency / convergence speed (steps to pass bottleneck) discussed qualitatively",
            "exploration_efficiency_value": null,
            "success_rate": null,
            "optimal_policy_type": "memory-based / structured (policies that exploit world structure and long-term dependencies, e.g., ones that remember to light lamp before entering cellar)",
            "topology_performance_relationship": "Bottleneck nodes in the quest graph (e.g., entering dark cellar) critically determine agent progress; conditional (state-dependent) edges such as darkness/death create hard-to-cross transitions that require long-term, non-immediately-rewarded actions (e.g., lighting a lamp earlier) to succeed.",
            "comparison_across_topologies": false,
            "topology_comparison_results": null,
            "policy_structure_findings": "Policies that incorporate persistent world knowledge (knowledge-graph based) and explicit mechanisms for handling long-term dependencies and backtracking perform better; chained policies that freeze and restart at bottleneck states faster overcome sequential bottlenecks in the acyclic quest graph.",
            "uuid": "e1220.0",
            "source_info": {
                "paper_title": "How To Avoid Being Eaten By a Grue: Exploration Strategies for Text-Adventure Agents",
                "publication_date_yy_mm": "2020-02"
            }
        },
        {
            "name_short": "KG-A2C-chained",
            "name_full": "Knowledge-Graph A2C with policy chaining and bottleneck detection",
            "brief_description": "An extension of KG-A2C that detects bottlenecks (no score improvement within a patience window), freezes the policy that reached the best seen state, and trains a new policy from that state with backtracking to avoid freezing a globally suboptimal policy.",
            "citation_title": "here",
            "mention_or_use": "use",
            "environment_name": "Zork1",
            "environment_description": "Fantasy/adventure interactive fiction used to evaluate chained exploration; navigation requires remembering earlier actions (e.g., lighting lamp) to safely pass certain rooms.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": true,
            "dead_ends_count": null,
            "door_constraints_present": true,
            "door_constraints_description": "Conditional narrative constraints (e.g., darkness causing immediate death unless lamp is lit); implicit room-to-room constraints inferred from navigation actions.",
            "graph_connectivity": "Treated as a sparse, largely acyclic quest graph; algorithm assumes sequential decision problems that can be represented as acyclic directed graphs.",
            "environment_size": null,
            "agent_name": "KG-A2C-chained",
            "agent_description": "Actor-Critic (A2C) agent with a knowledge-graph based state representation; adds bottleneck detection (patience parameter), policy freezing/chaining, and backtracking buffer of n prior states to restart training from promising precursor states.",
            "exploration_efficiency_metric": "sample efficiency / convergence speed (time/steps to pass bottleneck) and total score growth; measured qualitatively and via learning curves (score vs training steps).",
            "exploration_efficiency_value": null,
            "success_rate": null,
            "optimal_policy_type": "chained / staged policies that can freeze and re-train from bottleneck states (memory-augmented policies leveraging structured state representation).",
            "topology_performance_relationship": "Chaining policies by detecting bottlenecks is especially effective for environments whose quest structure can be represented as acyclic directed graphs: it increases sample efficiency and faster convergence to policies that pass bottlenecks compared to Go-Explore style sampling.",
            "comparison_across_topologies": false,
            "topology_comparison_results": null,
            "policy_structure_findings": "Explicitly chaining policies and using backtracking is particularly well-suited to sequential acyclic graphs and speeds up overcoming bottleneck nodes; knowledge-graph state information improves bottleneck detection and sample efficiency for chaining.",
            "uuid": "e1220.1",
            "source_info": {
                "paper_title": "How To Avoid Being Eaten By a Grue: Exploration Strategies for Text-Adventure Agents",
                "publication_date_yy_mm": "2020-02"
            }
        },
        {
            "name_short": "KG-A2C-Explore",
            "name_full": "Knowledge-Graph A2C combined with Go-Explore Phase 1 (knowledge-graph cell representation)",
            "brief_description": "An adaptation of Go-Explore for text games where KG-A2C is trained in parallel and cells (archive entries) are encoded using a snapshot of the knowledge graph together with the game state, and cells are chosen (weighted by score) for focused exploration.",
            "citation_title": "here",
            "mention_or_use": "use",
            "environment_name": "Zork1",
            "environment_description": "Fantasy/adventure interactive fiction; navigation evaluated under Go-Explore style archival exploration using knowledge-graph snapshots as cell encodings.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": true,
            "dead_ends_count": null,
            "door_constraints_present": true,
            "door_constraints_description": "Action-conditional constraints (e.g., darkness in cellar requires lamp to be lit prior to entry); these are represented in the knowledge graph and affect reachability.",
            "graph_connectivity": "Sparse, branching quest graph with critical bottleneck nodes; cells are used to index promising states rather than operate on explicit full graph metrics.",
            "environment_size": null,
            "agent_name": "KG-A2C-Explore (Go-Explore variant)",
            "agent_description": "Go-Explore Phase 1 style exploration using a KG-A2C policy: an archive of cells is maintained where each cell is represented by the knowledge-graph snapshot + last seen game state; cells are sampled (score-weighted) and KG-A2C runs for a fixed number of steps (cell step size) to expand cells and train the network.",
            "exploration_efficiency_metric": "cell-based exploration progress (archive expansion), sample efficiency, convergence speed to policies that pass bottlenecks, and eventual total score achieved.",
            "exploration_efficiency_value": null,
            "success_rate": null,
            "optimal_policy_type": "cell-guided exploration policies that exploit structured state encodings (knowledge graph) to prioritize promising states.",
            "topology_performance_relationship": "Knowledge-graph based cell encodings provide better identification of promising states in the quest graph than raw textual observation encodings, improving the ability to find trajectories that pass bottlenecks; however, KG-A2C-Explore converges slower than KG-A2C-chained but still consistently passes the critical bottleneck.",
            "comparison_across_topologies": false,
            "topology_comparison_results": null,
            "policy_structure_findings": "Policies that combine archival cell exploration with structured state representations (knowledge graphs) better recognize promising places in the state graph; Go-Explore style sampling benefits from graph-aware cell representations when topology contains long-term dependencies and bottlenecks.",
            "uuid": "e1220.2",
            "source_info": {
                "paper_title": "How To Avoid Being Eaten By a Grue: Exploration Strategies for Text-Adventure Agents",
                "publication_date_yy_mm": "2020-02"
            }
        },
        {
            "name_short": "Knowledge Graph (KG) state",
            "name_full": "Knowledge-graph based state representation (triples extracted from observations)",
            "brief_description": "A persistent graph of subject–relation–object triples extracted from textual observations that encodes rooms, objects, item possession, affordances, and inferred navigational relations; used as a map-like structured state representation for partial observability and navigation.",
            "citation_title": "",
            "mention_or_use": "use",
            "environment_name": "Zork1 (and general text-adventure domains)",
            "environment_description": "Structured symbolic map of the text world: nodes include rooms and objects; edges encode relations, object locations, possession, and inferred room adjacency from navigation actions.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": null,
            "dead_ends_count": null,
            "door_constraints_present": null,
            "door_constraints_description": "KG encodes conditional state information (e.g., item in inventory, room attributes like darkness) and inferred navigational edges, enabling policies to reason about conditional access and prerequisites for transitions.",
            "graph_connectivity": "KG forms a sparse, incremental map-like graph where rooms and objects are linked; connectivity is inferred from navigation verbs and observations rather than an explicit pre-provided adjacency matrix.",
            "environment_size": null,
            "agent_name": "KG-A2C / agents using KG",
            "agent_description": "State encoder that converts observations into a structured graph (triples), used to constrain action templates and as cell encodings for archival exploration; improves handling of partial observability and long-term dependencies.",
            "exploration_efficiency_metric": "sample efficiency and improved identification of promising states (qualitative and by improved final scores / ability to pass bottlenecks).",
            "exploration_efficiency_value": null,
            "success_rate": null,
            "optimal_policy_type": "memory-augmented / graph-aware policies that use persistent structured state for planning and for constraining action generation.",
            "topology_performance_relationship": "Knowledge graphs improve sample efficiency for bottleneck detection and exploration because they capture stateful prerequisites (e.g., possession and room attributes) that determine whether certain transitions are feasible; KG-based cells outperform purely textual observation encodings in Go-Explore style selection of promising states.",
            "comparison_across_topologies": false,
            "topology_comparison_results": null,
            "policy_structure_findings": "Policies leveraging structured persistent representations require less exploration to detect and cross bottlenecks; KG enables constrained action filling which focuses exploration on admissible, state-relevant actions and reduces wasted exploration in combinatorial action spaces.",
            "uuid": "e1220.3",
            "source_info": {
                "paper_title": "How To Avoid Being Eaten By a Grue: Exploration Strategies for Text-Adventure Agents",
                "publication_date_yy_mm": "2020-02"
            }
        },
        {
            "name_short": "Bottleneck detection & chaining",
            "name_full": "Bottleneck detection via patience window and policy chaining with backtracking",
            "brief_description": "A procedural method to detect locally limiting (bottleneck) states by monitoring score improvement over a patience window, freezing the policy that reached the best state, and training a new policy from that state with backtracking across a buffer of prior states to avoid freezing suboptimal policies.",
            "citation_title": "here",
            "mention_or_use": "use",
            "environment_name": "Zork1 (generalizable to other text-adventure quests modeled as DAGs)",
            "environment_description": "Works on sequential/quest-structured text worlds where progress is measured by cumulative reward and certain states prevent further progress unless specific earlier actions have been taken.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": null,
            "dead_ends_count": null,
            "door_constraints_present": null,
            "door_constraints_description": "Method explicitly targets state-dependent constraints that block further reward progression (e.g., conditional transitions such as dark rooms requiring an item/action).",
            "graph_connectivity": "Assumes a sequential decision graph often approximated as acyclic directed graph for the quest structure; backtracking moves training start to predecessor nodes in the observed trajectory buffer.",
            "environment_size": null,
            "agent_name": "KG-A2C-chained / A2C-chained (method applies to agent training)",
            "agent_description": "Training-time meta-policy that pauses/freezes policies at detected bottlenecks and re-initializes training from stored prior states (buffer of n) to find alternative precursor policies, ensuring exploration focuses on escaping local optima.",
            "exploration_efficiency_metric": "sample efficiency / steps-to-pass-bottleneck (qualitative improvement reported); uses 'patience' (number of steps without score improvement) as a detection hyperparameter.",
            "exploration_efficiency_value": null,
            "success_rate": null,
            "optimal_policy_type": "chained/staged policies with local restarts and backtracking mechanisms to escape local optima at bottleneck nodes.",
            "topology_performance_relationship": "Particularly effective when the environment's quest graph is sequential/acyclic: chaining reduces the number of necessary reattempts to find the long-term action sequence required to pass a bottleneck, increasing sample efficiency relative to undirected exploratory sampling.",
            "comparison_across_topologies": false,
            "topology_comparison_results": null,
            "policy_structure_findings": "Backtracking and policy-chaining help avoid freezing suboptimal policies at bottlenecks; policies that can restart from precursor states and re-explore are better suited to environments with deep long-term dependencies and sparse reward beyond bottlenecks.",
            "uuid": "e1220.4",
            "source_info": {
                "paper_title": "How To Avoid Being Eaten By a Grue: Exploration Strategies for Text-Adventure Agents",
                "publication_date_yy_mm": "2020-02"
            }
        },
        {
            "name_short": "A2C-Explore",
            "name_full": "A2C with Go-Explore style archival exploration using recurrent observation encoding",
            "brief_description": "A baseline that pairs standard A2C (text-only recurrent observation encoder) with a Go-Explore Phase 1 style archive where cells are represented by RNN-encoded textual observations rather than knowledge graphs.",
            "citation_title": "",
            "mention_or_use": "use",
            "environment_name": "Zork1",
            "environment_description": "Text-adventure evaluated under cell-based Go-Explore Phase 1 archival exploration using textual observation encodings as cell representations.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": true,
            "dead_ends_count": null,
            "door_constraints_present": true,
            "door_constraints_description": "Narrative/conditional constraints (same domain constraints as other agents) but without explicit KG encoding to represent prerequisites.",
            "graph_connectivity": "Sparse branching quest graph; archival sampling operates over compressed observational encodings rather than an explicit symbolic graph.",
            "environment_size": null,
            "agent_name": "A2C-Explore",
            "agent_description": "A2C agent using recurrent neural network encoding of textual observations; used within a Go-Explore Phase 1 archive where cells are defined by RNN observation encodings; cell step size hyperparameter controls per-cell rollout length.",
            "exploration_efficiency_metric": "convergence speed and final score achieved; Go-Explore cell-based archive expansion (qualitative comparison to KG variant).",
            "exploration_efficiency_value": null,
            "success_rate": null,
            "optimal_policy_type": "reactive / recurrent observation-encoded policies combined with archival exploration, but limited by insufficient structural state representation for long-term bottlenecks.",
            "topology_performance_relationship": "RNN-based cell encodings converge faster to lower-reward trajectories but fail to pass critical bottlenecks in Zork1; lack of structured state (KG) makes identifying promising precursor states for crossing bottlenecks less reliable.",
            "comparison_across_topologies": false,
            "topology_comparison_results": null,
            "policy_structure_findings": "Policies relying solely on recurrent textual encodings may converge quickly but to suboptimal behaviors that cannot cross topology-imposed bottlenecks; lacking a persistent structured map hurts exploration in environments with long-term dependencies.",
            "uuid": "e1220.5",
            "source_info": {
                "paper_title": "How To Avoid Being Eaten By a Grue: Exploration Strategies for Text-Adventure Agents",
                "publication_date_yy_mm": "2020-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Graph constrained reinforcement learning for natural language action spaces.",
            "rating": 2,
            "sanitized_title": "graph_constrained_reinforcement_learning_for_natural_language_action_spaces"
        },
        {
            "paper_title": "Go-explore: a new approach for hard-exploration problems.",
            "rating": 2,
            "sanitized_title": "goexplore_a_new_approach_for_hardexploration_problems"
        },
        {
            "paper_title": "Exploration based language learning for text-based games.",
            "rating": 2,
            "sanitized_title": "exploration_based_language_learning_for_textbased_games"
        },
        {
            "paper_title": "Textworld: A learning environment for text-based games.",
            "rating": 2,
            "sanitized_title": "textworld_a_learning_environment_for_textbased_games"
        },
        {
            "paper_title": "Interactive fiction games: A colossal adventure.",
            "rating": 1,
            "sanitized_title": "interactive_fiction_games_a_colossal_adventure"
        },
        {
            "paper_title": "Algorithmic improvements for deep reinforcement learning applied to interactive fiction.",
            "rating": 1,
            "sanitized_title": "algorithmic_improvements_for_deep_reinforcement_learning_applied_to_interactive_fiction"
        }
    ],
    "cost": 0.0128575,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>HOW TO AVOID BEING EATEN BY A GRUE: EXPLO- RATION STRATEGIES FOR TEXT-ADVENTURE AGENTS</p>
<p>Prithviraj Ammanabrolu raj.ammanabrolu@gatech.edu 
Georgia Institute of Technology</p>
<p>Ethan Tien etien@gatech.edu 
Georgia Institute of Technology</p>
<p>Zhaochen Luo zluo@gatech.edu 
Georgia Institute of Technology</p>
<p>Mark O Riedl riedl@gatech.edu 
Georgia Institute of Technology</p>
<p>HOW TO AVOID BEING EATEN BY A GRUE: EXPLO- RATION STRATEGIES FOR TEXT-ADVENTURE AGENTS</p>
<p>Text-based games-in which an agent interacts with the world through textual natural language-present us with the problem of combinatorially-sized actionspaces. Most current reinforcement learning algorithms are not capable of effectively handling such a large number of possible actions per turn. Poor sample efficiency, consequently, results in agents that are unable to pass bottleneck states, where they are unable to proceed because they do not see the right action sequence to pass the bottleneck enough times to be sufficiently reinforced. Building on prior work using knowledge graphs in reinforcement learning, we introduce two new game state exploration strategies. We compare our exploration strategies against strong baselines on the classic text-adventure game, Zork1, where prior agent have been unable to get past a bottleneck where the agent is eaten by a Grue.</p>
<p>INTRODUCTION AND BACKGROUND</p>
<p>Many reinforcement learning algorithms are designed for relatively small discrete or continuous action spaces and so have trouble scaling. Text-adventure games-or interaction fictions-are simulations in which both an agents' state and action spaces are in textual natural language. An example of a one turn agent interaction in the popular text-game Zork1 can be seen in Fig. 1a. Text-adventure games provide us with multiple challenges in the form of partial observability, commonsense reasoning, and a combinatorially-sized state-action space. Text-adventure games are structured as long puzzles or quests, interspersed with bottlenecks. The quests can usually be completed through multiple branching paths. However, games can also feature one or more bottlenecks. Bottlenecks are areas that an agent must pass through in order to progress to the next section of the game regardless of what path the agent has taken to complete that section of the quest (Stolle &amp; Precup, 2002). In this work, we focus on more effectively exploring this space and surpassing these bottlenecks-building on prior work that focuses on tackling the other problems.</p>
<p>Formally, we use the definition of text-adventure games as seen in Côté et al. (2018) and Hausknecht et al. (2019). These games are partially observable Markov decision processes (POMDPs), represented as a 7-tuple of S, T, A, Ω, O, R, γ representing the set of environment states, mostly deterministic conditional transition probabilities between states, the vocabulary or words used to compose text commands, observations returned by the game, observation conditional probabilities, reward function, and the discount factor respectively. For our purposes, understanding the exact state and action spaces we use in this work is critical and so we define each of these in relative depth.</p>
<p>Action-Space. To solve Zork1, the cannonical text-adventure games, requires the generation of actions consisting of up to five-words from a relatively modest vocabulary of 697 words recognized by the games parser. This results in O(697 5 ) = 1.64 × 10 14 possible actions at every step. To facilitate text-adventure game playing, Hausknecht et al. (2019) introduce Jericho 1 , a framework for interacting with text-games. They propose a template-based action space in which the agent first selects a template, consisting of an action verb and preposition, and then filling that in with relevant entities (e.g.</p>
<p>[get] [f rom] ). Zork1 has 237 templates, each with up to two blanks, yielding a template-action space of size O(237 × 697 2 ) = 1.15 × 10 8 . This space is still far larger than most used by previous approaches applying reinforcement learning to text-based games.</p>
<p>Observation: West of House You are standing in an open field west of a white house, with a boarded front door. There is a small mailbox here.</p>
<p>Action: Open mailbox</p>
<p>Observation: Opening the small mailbox reveals a leaflet.</p>
<p>Action: Read leaflet</p>
<p>Observation: (Taken) "WELCOME TO ZORK! ZORK is a game of adventure, danger, and low cunning. In it you will explore some of the most amazing territory ever seen by mortals. No computer should be without one!" (a) Excerpt from the initial stages of Zork1.  State-Representation. Prior work has shown that knowledge graphs are effective in terms of dealing with the challenges of partial observability (Ammanabrolu &amp; Riedl 2019a;2019b). A knowledge graph is a set of 3-tuples of the form subject, relation, object . These triples are extracted from the observations using Stanford's Open Information Extraction (OpenIE) (Angeli et al., 2015). Human-made text-adventure games often contain relatively complex semi-structured information that OpenIE is not designed to parse and so they add additional rules to ensure that the correct information is parsed. The graph itself is more or less a map of the world, with information about objects' affordances and attributes linked to the rooms that they are place in a map. The graph also makes a distinction with respect to items that are in the agent's possession or in their immediate surrounding environment. An example of what the knowledge graph looks like and specific implementation details can be found in Appendix A.2.</p>
<p>Ammanabrolu &amp; Hausknecht (2020) introduce the KG-A2C, 2 which uses a knowledge graph based state-representation to aid in the section of actions in a combinatorially-sized action-spacespecifically they use the knowledge graph to constrain the kinds of entities that can be filled in the blanks in the template action-space. They test their approach on Zork1, showing the combination of the knowledge graph and template action selection resulted in improvements over existing methods. They note that their approach reaches a score of 40 which corresponds to a bottleneck in Zork1 where the player is eaten by a "grue" (resulting in negative reward) if the player has not first lit a lamp. The lamp must be lit many steps after first being encountered, in a different section of the game; this action is necessary to continue exploring but doesnt immediately produce any positive reward. That is, there is a long term dependency between actions that is not immediately rewarded, as seen in Figure 1b. Others using artificially constrained action spaces also report an inability to pass through this bottleneck (Zahavy et al., 2018;Jain et al., 2019). They pose a significant challenge for these methods because the agent does not see the correct action sequence to pass the bottleneck enough times. This is in part due to the fact that for that sequence to be reinforced, the agent needs to reach the next possible reward beyond the bottleneck.</p>
<p>More efficient exploration strategies are required to pass bottlenecks. Our contributions are twofold. We first introduce a method that detects bottlenecks in text-games using the overall reward gained and the knowledge graph state. This method freezes the policy used to reach the bottleneck and restarts the training from there on out, additionally conducting a backtracking search to ensure that a sub-optimal policy has not been frozen. The second contribution explore how to leverage knowledge graphs to improve existing exploration algorithms for dealing with combinatorial actionspaces such as Go-Explore (Ecoffet et al., 2019). We additionally present a comparative ablation study analyzing the performance of these methods on the popular text-game Zork1.</p>
<p>EXPLORATION METHODS</p>
<p>In this section, we describe methods to explore combinatorially sized action spaces such as textgames-focusing especially on methods that can deal with their inherent bottleneck structure. We first describe our method that explicitly attempts to detect bottlenecks and then describe how an exploration algorithm such as Go Explore (Ecoffet et al., 2019) can leverage knowledge graphs.</p>
<p>KG-A2C-chained An example of a bottleneck can be seen in Figure 1b. We extend the KG-A2C algorithm as follows. First, we detect bottlenecks as states where the agent is unable to progress any further. We set a patience parameter and if the agent has not seen a higher score in patience steps, the agent assumes it has been limited by a bottleneck. Second, when a bottleneck is found, we freeze the policy that gets the agent to the state with the highest score. The agent then begins training a new policy from that particular state.</p>
<p>Simply freezing the policy that led to the bottleneck, however, can potentially result in a policy one that is globally sub-optimal. We therefore employ a backtracking strategy that restarts exploration from each of the n previous steps-searching for a more optimal policy that reaches that bottleneck. At each step, we keep track of a buffer of n states and admissible actions that led up to that locally optimal state. We force the agent to explore from this state to attempt to drive it out of the local optima. If it is further unable to find itself out of this local optima, we refresh the training process again, but starting at the state immediately before the agent reaches the local optima. If this continues to fail, we continue to iterate through this buffer of seen states states up to that local optima until we either find a more optimal state or we run out of states to refresh from, in which we terminate the training algorithm.</p>
<p>KG-A2C-Explore Go-Explore (Ecoffet et al., 2019) is an algorithm that is designed to keep track of sub-optimal and under-explored states in order to allow the agent to explore upon more optimal states that may be a result of sparse rewards. The Go-Explore algorithm consists of two phases, the first to continuously explore until a set of promising states and corresponding trajectories are found on the basis of total score, and the second to robustify this found policy against potential stochasticity in the game. Promising states are defined as those states when explored from will likely result in higher reward trajectories. Since the text games we are dealing with are mostly deterministic, with the exception of Zork in later stages, we only focus on using Phase 1 of the Go-Explore algorithm to find an optimal policy. Madotto et al. (2020) look at applying Go-Explore to text-games on a set of simpler games generated using the game generation framework TextWorld (Côté et al., 2018). Instead of training a policy network in parallel to generate actions used for exploration, they use a small set of "admissible actions"-actions guaranteed to change the world state at any given step during Phase 1-to explore and find high reward trajectories. This space of actions is relatively small (of the order of 10 2 per step) and so finding high reward trajectories in larger action-spaces such as in Zork would be infeasible Go-Explore maintains an archive of cells-defined as a set of states that map to a single representation-to keep track of promising states. Ecoffet et al. (2019) simply encodes each cell by keeping track of the agent's position and Madotto et al. (2020) use the textual observations encoded by recurrent neural network as a cell representation. We improve on this implementation by training the KG-A2C network in parallel, using the snapshot of the knowledge graph in conjunction with the game state to further encode the current state and use this as a cell representation. At each step, Go-Explore chooses a cell to explore at random (weighted by score to prefer more advanced cells). The KG-A2C will run for a number of steps, starting with the knowledge graph state and the last seen state of the game from the cell. This will generate a trajectory for the agent while further training the KG-A2C at each iteration, creating a new representation for the knowledge graph as well as a new game state for the cell. After expanding a cell, Go-Explore will continue to sample cells by weight to continue expanding its known states. At the same time, KG-A2C will benefit from the heuristics of selecting preferred cells and be trained on promising states more often.   </p>
<p>EVALUATION</p>
<p>We compare our two exploration strategies to the following baselines and ablations:</p>
<p>• KG-A2C This is the exact same method presented in Ammanabrolu &amp; Hausknecht (2020) with no modifications.</p>
<p>• A2C Represents the same approach as KG-A2C but with all the knowledge graph components removed. The state representation is text only encoded using recurrent networks.</p>
<p>• A2C-chained Is a variation on KG-A2C-chained where we use our policy chaining approach with the A2C method to train the agent instead of KG-A2C.</p>
<p>• A2C-Explore Uses A2C in addition to the exploration strategy seen in KG-A2C-Explore. The cell representations here are defined in terms of the recurrent network based encoding of the textual observation. Figure 2 shows that agents utilizing knowledge-graphs in addition to either enhanced exploration method far outperform the baseline A2C and KG-A2C. KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40, whereas A2C-Explore gets to the bottleneck but cannot surpass it.</p>
<p>There are a couple of key insights that can be drawn from these results The first is that the knowledge graph appears to be critical; it is theorized to help with partial observability. However the knowledge graph representation isn't sufficient in that the knowledge graph representation without enhanced exploration methods cannot surpass the bottleneck. A2C-chained-which explores without a knowledge graph-fails to even outperform the baseline A2C. We hypothesize that this is due to the knowledge graph aiding implicitly in the sample efficiency of bottleneck detection and subsequent exploration. That is, exploring after backtracking from a potentially detected bottleneck is much more efficient in the knowledge graph based agent.</p>
<p>The Go-Explore based exploration algorithm sees less of a difference between agents. A2C-Explore converges more quickly, but to a lower reward trajectory that fails to pass the bottleneck, whereas KG-A2C-Explore takes longer to reach a similar reward but consistently makes it through the bottleneck. The knowledge graph cell representation appears to thus be a better indication of what a promising state is as opposed to just the textual observation.</p>
<p>Comparing the advanced exploration methods when using the knowledge graph, we see that both agents successfully pass the bottleneck corresponding to entering the cellar and lighting the lamp and reach comparable scores within a margin of error. KG-A2C-chained is significantly more sample efficient and converges faster. We can infer that chaining policies by explicitly detecting bottlenecks lets us pass it more quickly than attempting to find promising cell representations with Go-Explore. This form of chained exploration with backtracking is particularly suited to sequential decision making problems that can be represented as acyclic directed graphs as in Figure 1b. The bottleneck seen at a score of around 40 is when the player first enters the cellar on the right side of the map. The cellar is dark and you need to immediately light the lamp to see anything.</p>
<p>Attempting to explore the cellar in the dark results in you being instantly killed by a monster known as a "grue".</p>
<p>A.2 KNOWLEDGE GRAPH RULES</p>
<p>We make no changes from the graph update rules used by Ammanabrolu &amp; Hausknecht (2020). Candidate interactive objects are identified by performing part-of-speech tagging on the current observation, identifying singular and proper nouns as well as adjectives, and are then filtered by checking if they can be examined using the command examine OBJ. Only the interactive objects not found in the inventory are linked to the node corresponding to the current room and the inventory items are linked to the "you" node. The only other rule applied uses the navigational actions performed by the agent to infer the relative positions of rooms, e.g. kitchen, down, cellar when the agent performs go down when in the kitchen to move to the cellar.</p>
<p>A.3 HYPERPARAMETERS</p>
<p>Hyperparameters used for our agents are given below. Patience and buffer size are used for the policy chaining method as described in Section 2. Cell step size is a parameter used for Go-Explore and describes how many steps are taken when exploring in a given cell state. Base hyperparameters for KG-A2C are taken from Ammanabrolu &amp; Hausknecht (2020) and the same parameters are used for A2C.</p>
<p>Agent</p>
<p>Hyperparameters A2C-chained patience=35 buffer size n=40 batch size=32 KG-A2C-chained patience=35 buffer size n=40 batch size=32 A2C-Explore cell step size=30 batch size=1 KG-A2C-Explore cell step size=30 batch size=1</p>
<p>of the quest structure as a directed acyclic graph in Zork1 demonstrating bottlenecks. Each node represents an action that needs to be taken to finish the quest. Green nodes represent potential positive rewards. By our definition, entering the kitchen and lighting the lamp after entering the cellar are likely bottleneck candidates.</p>
<p>Figure 1 :
1An overall example of an excerpt and quest structure of Zork1.</p>
<p>curves for select experiments. The dotted line represents the bottleneck of lighting the lamp.</p>
<p>Figure 2 :
2Ablation results on Zork1, averaged across 5 independent runs.</p>
<p>Figure 3 :
3Map of Zork1 annotated with rewards. These rewards correspond to the quest structure seen inFigure 1b. Taken from Ammanabrolu &amp; Hausknecht (is one of the first text-adventure games and heavily influences games released later in terms of narrative style and game structure. It is a dungeon crawler where the player must explore a vast world and collect a series of treasures. It was identified byHausknecht et al. (2019) as a moonshot game and has been the subject of much work in leaning agents(Yin &amp; May, 2019;Zahavy et al., 2018;Tessler et al., 2019;Jain et al., 2019). Rewards are given to the player when they collect treasures as well as when important intermediate milestones needed to further explore the world are passed.Figure 3andFigure 1bshow us a map of the world of Zork1 and the corresponding quest structure.
https://github.com/microsoft/jericho
https://github.com/rajammanabrolu/KG-A2C</p>
<p>Graph constrained reinforcement learning for natural language action spaces. Prithviraj Ammanabrolu, Matthew Hausknecht, International Conference on Learning Representations. Prithviraj Ammanabrolu and Matthew Hausknecht. Graph constrained reinforcement learning for natural language action spaces. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=B1x6w0EtwH.</p>
<p>Playing text-adventure games with graph-based deep reinforcement learning. Prithviraj Ammanabrolu, Mark O Riedl, Proceedings of 2019 Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019. 2019 Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019Prithviraj Ammanabrolu and Mark O. Riedl. Playing text-adventure games with graph-based deep reinforcement learning. In Proceedings of 2019 Annual Conference of the North American Chap- ter of the Association for Computational Linguistics: Human Language Technologies, NAACL- HLT 2019, 2019a.</p>
<p>Transfer in deep reinforcement learning using knowledge graphs. Prithviraj Ammanabrolu, Mark O Riedl, abs/1908.06556CoRRPrithviraj Ammanabrolu and Mark O. Riedl. Transfer in deep reinforcement learning using knowl- edge graphs. CoRR, abs/1908.06556, 2019b.</p>
<p>Leveraging Linguistic Structure For Open Domain Information Extraction. Gabor Angeli, Johnson Premkumar, Melvin Jose, Christopher D Manning, Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing. the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing1Gabor Angeli, Johnson Premkumar, Melvin Jose, and Christopher D. Manning. Leveraging Lin- guistic Structure For Open Domain Information Extraction. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Confer- ence on Natural Language Processing (Volume 1: Long Papers), 2015.</p>
<p>Textworld: A learning environment for text-based games. Ákos Marc-Alexandre Côté, Xingdi Kádár, Ben Yuan, Tavian Kybartas, Emery Barnes, James Fine, Matthew Moore, Layla El Hausknecht, Mahmoud Asri, Wendy Adada, Adam Tay, Trischler, abs/1806.11532CoRRMarc-Alexandre Côté,Ákos Kádár, Xingdi Yuan, Ben Kybartas, Tavian Barnes, Emery Fine, James Moore, Matthew Hausknecht, Layla El Asri, Mahmoud Adada, Wendy Tay, and Adam Trischler. Textworld: A learning environment for text-based games. CoRR, abs/1806.11532, 2018.</p>
<p>Go-explore: a new approach for hard-exploration problems. Adrien Ecoffet, Joost Huizinga, Joel Lehman, Kenneth O Stanley, Jeff Clune, abs/1901.10995CoRRAdrien Ecoffet, Joost Huizinga, Joel Lehman, Kenneth O. Stanley, and Jeff Clune. Go-explore: a new approach for hard-exploration problems. CoRR, abs/1901.10995, 2019.</p>
<p>Interactive fiction games: A colossal adventure. Matthew Hausknecht, Prithviraj Ammanabrolu, Marc-Alexandre Côté, Xingdi Yuan, abs/1909.05398CoRRMatthew Hausknecht, Prithviraj Ammanabrolu, Marc-Alexandre Côté, and Xingdi Yuan. Interactive fiction games: A colossal adventure. CoRR, abs/1909.05398, 2019.</p>
<p>Algorithmic improvements for deep reinforcement learning applied to interactive fiction. CoRR, abs. Vishal Jain, William Fedus, Hugo Larochelle, Doina Precup, Marc G Bellemare, Vishal Jain, William Fedus, Hugo Larochelle, Doina Precup, and Marc G. Bellemare. Algo- rithmic improvements for deep reinforcement learning applied to interactive fiction. CoRR, abs/1911.12511, 2019.</p>
<p>Exploration based language learning for text-based games. Andrea Madotto, Mahdi Namazifar, Joost Huizinga, Piero Molino, Adrien Ecoffet, Huaixiu Zheng, Alexandros Papangelis, Dian Yu, Chandra Khatri, Gokhan Tur, abs/2001.08868CoRRAndrea Madotto, Mahdi Namazifar, Joost Huizinga, Piero Molino, Adrien Ecoffet, Huaixiu Zheng, Alexandros Papangelis, Dian Yu, Chandra Khatri, and Gokhan Tur. Exploration based language learning for text-based games. CoRR, abs/2001.08868, 2020.</p>
<p>Learning options in reinforcement learning. Martin Stolle, Doina Precup, Proceedings of the 5th International Symposium on Abstraction, Reformulation and Approximation. the 5th International Symposium on Abstraction, Reformulation and ApproximationBerlin, HeidelbergSpringer-Verlag212223ISBN 3540439412Martin Stolle and Doina Precup. Learning options in reinforcement learning. In Proceedings of the 5th International Symposium on Abstraction, Reformulation and Approximation, pp. 212223, Berlin, Heidelberg, 2002. Springer-Verlag. ISBN 3540439412.</p>
<p>Action assembly: Sparse imitation learning for text based games with combinatorial action spaces. Chen Tessler, Tom Zahavy, Deborah Cohen, J Daniel, Shie Mankowitz, Mannor, abs/1905.09700CoRRChen Tessler, Tom Zahavy, Deborah Cohen, Daniel J Mankowitz, and Shie Mannor. Action as- sembly: Sparse imitation learning for text based games with combinatorial action spaces. CoRR, abs/1905.09700, 2019.</p>
<p>Comprehensible context-driven text game playing. CoRR, abs. Xusen Yin, Jonathan May, Xusen Yin and Jonathan May. Comprehensible context-driven text game playing. CoRR, abs/1905.02265, 2019.</p>
<p>Learn what not to learn: Action elimination with deep reinforcement learning. Tom Zahavy, Matan Haroush, Nadav Merlis, J Daniel, Shie Mankowitz, Mannor, Advances in Neural Information Processing Systems. S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. GarnettCurran Associates, Inc31Tom Zahavy, Matan Haroush, Nadav Merlis, Daniel J Mankowitz, and Shie Mannor. Learn what not to learn: Action elimination with deep reinforcement learning. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Infor- mation Processing Systems 31, pp. 3562-3573. Curran Associates, Inc., 2018.</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1664 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1664</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1664</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-32.html">extraction-schema-32</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <p><strong>Paper ID:</strong> paper-251018675</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2207.10821v2.pdf" target="_blank">Rethinking Sim2Real: Lower Fidelity Simulation Leads to Higher Sim2Real Transfer in Navigation</a></p>
                <p><strong>Paper Abstract:</strong> If we want to train robots in simulation before deploying them in reality, it seems natural and almost self-evident to presume that reducing the sim2real gap involves creating simulators of increasing fidelity (since reality is what it is). We challenge this assumption and present a contrary hypothesis -- sim2real transfer of robots may be improved with lower (not higher) fidelity simulation. We conduct a systematic large-scale evaluation of this hypothesis on the problem of visual navigation -- in the real world, and on 2 different simulators (Habitat and iGibson) using 3 different robots (A1, AlienGo, Spot). Our results show that, contrary to expectation, adding fidelity does not help with learning; performance is poor due to slow simulation speed (preventing large-scale learning) and overfitting to inaccuracies in simulation physics. Instead, building simple models of the robot motion using real-world data can improve learning and generalization.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1664.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1664.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KinematicSim</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Kinematic simulation / kinematically-trained high-level policies</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Low-fidelity, teleportation-style simulation that integrates commanded center-of-mass velocities at 1Hz (Euler integration) and directly places the robot at the resulting pose; used to train high-level visual navigation policies that command CoM velocities while abstracting away low-level joint and contact dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Legged robots (Unitree A1, Unitree AlienGo, Boston Dynamics Spot)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>Quadrupedal robots used for long-range visual PointGoal navigation; perceive via an egocentric depth camera and egomotion (GPS+Compass) and execute motion via manufacturer or open low-level controllers on hardware.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>visual navigation / embodied robotics</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>Habitat and iGibson (with kinematic stepping mode)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Photorealistic indoor environment renderers built on 3D scene scans (HM3D + Gibson). When used in kinematic mode the environment provides visual sensor rendering and collision checking but does not run high-frequency rigid-body/contact physics for robot motion.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>low-fidelity physics (kinematic abstraction / teleportation)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>Photorealistic rendering of scenes, collision checks for teleportation safety, egocentric depth sensor rendering, coarse egomotion and goal specification.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>No high-frequency rigid-body dynamics, no contact or joint-level dynamics, low-level controllers and actuator dynamics not modeled (robot is "teleported" using integrated CoM velocities), no detailed contact friction/impulses.</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>LAB: a 325 m^2 cluttered office lobby with furniture (couches, cushions, bookshelves, tables); real Boston Dynamics Spot equipped with Intel RealSense D435 depth camera and onboard Boston Dynamics black-box low-level walking controller; BD collision-avoidance kept on with tight threshold.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>PointGoal visual navigation (navigate to a relative goal coordinate using depth imagery and egomotion)</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Model-free reinforcement learning (DD-PPO distributed PPO with ResNet-18 encoder + LSTM policy)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Success Rate (SR) and Success weighted by Path Length (SPL)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td>Kinematic policies achieved the highest simulation generalization across robots and simulators (examples: A1: 62.1% SR in iGibson kinematic evaluation reported; overall kinematic-trained policies dominated dynamic-trained ones in sim evaluations across Habitat/iGibson).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td>On Spot (real LAB): kinematically-trained policies achieved 100% SR and SPL ≈ 82–83% (averaged over evaluated episodes/seeds); training with actuation-noise variants increased SPL further.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Mismatch arises primarily from overfitting of learned policies to simulator physics when physics is modeled; missing or mismatched low-level controllers (black-box hardware controllers) and controller-level biases; limited simulation speed for high-fidelity sims leading to insufficient training experience; actuator/actuation noise and imperfect omnidirectionality of robot motion.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Hierarchical decomposition (train only high-level policy in kinematic sim while relying on robust hardware low-level controllers), high simulation speed enabling much larger experience (≈10× more steps), and adding real-world-derived actuation-noise models during kinematic training to improve robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>For high-level navigation tasks that can be abstracted in CoM motion, high-fidelity low-level physics is not required and can even harm transfer; instead, simulation speed and accurate low-dimensional modeling of actuation noise are the key fidelity requirements.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Kinematic (low-fidelity) training outperformed dynamic (high-fidelity) training in both sim and real tests: e.g., kinematic policies achieved 100% SR on real Spot vs. dynamic policies 40–67% SR; in sim, kinematic-trained policies often had higher SR than dynamic-trained policies even under dynamic evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Training high-level navigation policies in lower-fidelity (kinematic) simulation leads to better sim-to-real transfer for PointGoal navigation on legged robots because (1) low-fidelity sims are faster enabling far more training experience and better high-level reasoning, and (2) they avoid overfitting to imperfect/incorrect low-level physics and controllers; modest injection of real actuation-noise into kinematic sims further improves real-world robustness and path efficiency.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1664.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1664.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DynamicSim</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dynamic simulation / dynamically-trained policies (rigid-body + contact)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>High-fidelity physics simulation using Bullet with 240Hz physics stepping and low-level controllers that convert CoM velocity commands to joint torques; intended to model contact dynamics and joint-level behavior for training end-to-end control.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Legged robots (Unitree A1, Unitree AlienGo, Boston Dynamics Spot in simulation)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>Quadrupedal robots whose low-level dynamics (joint torques, foot contact, gait execution) are simulated and used to train policies that command CoM velocities but rely on simulated controllers (Raibert-style or MPC) to realize them.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>visual navigation / embodied robotics</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>Habitat (with Bullet) and iGibson (PyBullet)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Photorealistic scene rendering combined with full rigid-body, contact, and actuator-level physics simulated using Bullet at 240Hz, with low-level controllers (Raibert or MPC) producing joint torques.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>higher-fidelity physics (rigid-body dynamics + contact + controller-level simulation)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>Rigid-body mechanics, contact dynamics, foot contacts, inverse kinematics & joint torque control, modelled low-level controllers (Raibert-style; evaluated also with MPC), physics integration at 1/240s.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>Simulator/controller approximations exist (e.g., Raibert controller is an approximation and differs from hardware black-box controllers); real hardware controller specifics and some real-world friction/actuator behaviors remain mismatched despite higher-fidelity modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Same LAB lobby; on hardware the robot uses manufacturer black-box low-level walking controllers (Spot) which are not fully replicable in simulation, leading to mismatches between simulated controllers and hardware.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>PointGoal visual navigation (same task as kinematic case)</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Model-free reinforcement learning (DD-PPO) trained with dynamic low-level controller in the loop (Raibert during training; MPC used for evaluation in some experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Success Rate (SR) and SPL</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td>Dynamic policies often achieved reasonable performance in the simulator they were trained in (e.g., A1: Habitat Dynamic training produced higher SR in Habitat-Dynamic) but showed large performance drops when evaluated in other simulators or with different low-level controllers (example: A1 Habitat Dynamic 49.8% SR vs iGibson Dynamic 22.3% SR).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td>On real Spot, dynamic-trained policies produced lower SR (≈40–67%) and lower SPL compared to kinematic-trained policies; also more collisions and more commanded actions on average.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Overfitting to specific simulated low-level controller dynamics; mismatch between simulated low-level controllers (Raibert or MPC) and the real robot's black-box controller; simulator inaccuracies in contact and complex actuator behaviors; slow simulation speeds limit the amount of training data and exacerbate overfitting.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>If dynamic training were to transfer well, it would require highly accurate replication of the robot's low-level controller and precise physics fidelity plus sufficient simulation speed to gather large-scale training experience; without those, dynamic training overfits and degrades transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>The paper finds that increased low-level physics fidelity alone is insufficient and can be detrimental unless the simulated low-level controller and dynamics closely match hardware; high-fidelity physics must be coupled with accurate controller modeling and sufficient training scale to be beneficial.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Dynamic-trained policies underperformed compared to kinematic-trained ones in both sim and real transfer: dynamic policies had lower SR in real tests (40–67% vs 100% for kinematic) and exhibited large sim-to-sim gaps indicating overfitting to simulator/controller details.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>High-fidelity dynamic simulation can hurt sim-to-real transfer for high-level navigation policies because learned policies overfit to simulator-specific low-level dynamics and because dynamic sims are much slower (reducing training scale); accurate modeling of low-level controllers is critical but often infeasible for closed-source hardware, making dynamic training less robust for zero-shot transfer.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1664.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1664.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ActuationNoise</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Real-actuation noise modeling for kinematic simulation (Gaussian injection)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A low-dimensional, data-driven model of actuation error (difference between commanded and actual CoM velocities) fitted as Gaussian noise (decoupled and coupled datasets) and injected into the kinematic simulator to increase robustness of learned high-level policies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Boston Dynamics Spot (data collection) / kinematically-trained navigation policy (usage)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>Spot robot used to collect actuation error samples by commanding random CoM velocities and measuring resultant velocities via SDK; noise model applied during kinematic training to simulate realistic actuation error.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>visual navigation / embodied robotics (robustification for sim2real)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>Habitat and iGibson (kinematic mode with noise injection)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Photorealistic environments with kinematic stepping; before stepping, sampled Gaussian actuation noise (separate models for decoupled and coupled actuation) is added to commanded CoM velocities so that resulting pose updates mimic real actuator errors.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>low-fidelity physics with injected real-world actuation noise (improved representational fidelity for actuators)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>Low-dimensional actuator tracking error (difference between commanded and executed CoM linear and angular velocities) modeled as Gaussian with empirically estimated mean and variance; captures asymmetry and higher variance in lateral vs forward directions.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>Does not model full joint-level dynamics, contact specifics, or history-dependent nonlinear actuator behaviors; uses IID Gaussian assumptions (diagonal variance) rather than time-series or state-conditioned noise.</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Noise data collected on Spot in an empty room by commanding random velocities (decoupled and coupled) and measuring final velocities using the Boston Dynamics SDK; dataset size: 6,000 samples (2,000 per direction for decoupled).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>PointGoal visual navigation with improved robustness to actuation errors on Spot</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>DD-PPO RL with noise samples injected into action (sample from fitted Gaussian per velocity dimension before applying kinematic step)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Success Rate (SR), SPL (path efficiency), number of collisions, number of actions commanded</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td>Policies trained with actuation-noise injection performed similarly in simulation but were trained to be robust to noisy actuation.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td>On Spot, kinematic policies trained with decoupled noise achieved 100% SR and increased SPL by ≈4.6% over no-noise kinematic policy; coupled-noise training increased SPL by ≈5.6% and reduced collisions and number of actions (e.g., 22.7 actions and 2.8 collisions vs 26.4 actions and 3.1 collisions for no-noise).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>Gaussian actuation-noise sampled per control-dimension; two variants: decoupled (separate Gaussians for forward, lateral, angular based on 2,000 samples each) and coupled (single multivariate-like approach using 6,000 joint samples); injected at each action during training.</td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Unmodeled actuation tracking errors and asymmetric lateral vs forward variance; without noise modeling policies exploit simulator-perfect actuation and take risky high-velocity actions that fail on hardware.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Collecting a small real-world dataset of actuation errors and injecting the learned low-dimensional noise model into kinematic simulation substantially increases robustness and real-world path efficiency without modeling full dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>For navigation abstracted to CoM commands, accurate low-dimensional actuation noise models (not full dynamics) are sufficient to close remaining gaps and improve zero-shot transfer; modeling more detailed low-level physics is not necessary for this class of tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Kinematic + actuation-noise > kinematic-no-noise > dynamic-trained policies in real-world SR and SPL; injecting noise increased SPL by ~4.6–5.6% and reduced collisions versus no-noise kinematic training.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A small, empirical, low-dimensional actuation-noise model collected from hardware and injected into a kinematic simulator significantly improves real-world navigation robustness and efficiency, demonstrating that targeted augmentation of low-fidelity sims can close remaining sim2real gaps without expensive high-fidelity physics.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Sim-to-real: Learning agile locomotion for quadruped robots <em>(Rating: 2)</em></li>
                <li>Sim-to-real transfer of robotic control with dynamics randomization <em>(Rating: 2)</em></li>
                <li>Closing the sim-to-real loop: Adapting simulation randomization with real world experience <em>(Rating: 2)</em></li>
                <li>Sim2real predictivity: Does evaluation in simulation predict real-world performance? <em>(Rating: 2)</em></li>
                <li>DD-PPO: Learning near-perfect pointgoal navigators from 2.5 billion frames <em>(Rating: 1)</em></li>
                <li>Learning robust perceptive locomotion for quadrupedal robots in the wild <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1664",
    "paper_id": "paper-251018675",
    "extraction_schema_id": "extraction-schema-32",
    "extracted_data": [
        {
            "name_short": "KinematicSim",
            "name_full": "Kinematic simulation / kinematically-trained high-level policies",
            "brief_description": "Low-fidelity, teleportation-style simulation that integrates commanded center-of-mass velocities at 1Hz (Euler integration) and directly places the robot at the resulting pose; used to train high-level visual navigation policies that command CoM velocities while abstracting away low-level joint and contact dynamics.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "Legged robots (Unitree A1, Unitree AlienGo, Boston Dynamics Spot)",
            "agent_system_description": "Quadrupedal robots used for long-range visual PointGoal navigation; perceive via an egocentric depth camera and egomotion (GPS+Compass) and execute motion via manufacturer or open low-level controllers on hardware.",
            "domain": "visual navigation / embodied robotics",
            "virtual_environment_name": "Habitat and iGibson (with kinematic stepping mode)",
            "virtual_environment_description": "Photorealistic indoor environment renderers built on 3D scene scans (HM3D + Gibson). When used in kinematic mode the environment provides visual sensor rendering and collision checking but does not run high-frequency rigid-body/contact physics for robot motion.",
            "simulation_fidelity_level": "low-fidelity physics (kinematic abstraction / teleportation)",
            "fidelity_aspects_modeled": "Photorealistic rendering of scenes, collision checks for teleportation safety, egocentric depth sensor rendering, coarse egomotion and goal specification.",
            "fidelity_aspects_simplified": "No high-frequency rigid-body dynamics, no contact or joint-level dynamics, low-level controllers and actuator dynamics not modeled (robot is \"teleported\" using integrated CoM velocities), no detailed contact friction/impulses.",
            "real_environment_description": "LAB: a 325 m^2 cluttered office lobby with furniture (couches, cushions, bookshelves, tables); real Boston Dynamics Spot equipped with Intel RealSense D435 depth camera and onboard Boston Dynamics black-box low-level walking controller; BD collision-avoidance kept on with tight threshold.",
            "task_or_skill_transferred": "PointGoal visual navigation (navigate to a relative goal coordinate using depth imagery and egomotion)",
            "training_method": "Model-free reinforcement learning (DD-PPO distributed PPO with ResNet-18 encoder + LSTM policy)",
            "transfer_success_metric": "Success Rate (SR) and Success weighted by Path Length (SPL)",
            "transfer_performance_sim": "Kinematic policies achieved the highest simulation generalization across robots and simulators (examples: A1: 62.1% SR in iGibson kinematic evaluation reported; overall kinematic-trained policies dominated dynamic-trained ones in sim evaluations across Habitat/iGibson).",
            "transfer_performance_real": "On Spot (real LAB): kinematically-trained policies achieved 100% SR and SPL ≈ 82–83% (averaged over evaluated episodes/seeds); training with actuation-noise variants increased SPL further.",
            "transfer_success": true,
            "domain_randomization_used": false,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": "Mismatch arises primarily from overfitting of learned policies to simulator physics when physics is modeled; missing or mismatched low-level controllers (black-box hardware controllers) and controller-level biases; limited simulation speed for high-fidelity sims leading to insufficient training experience; actuator/actuation noise and imperfect omnidirectionality of robot motion.",
            "transfer_enabling_conditions": "Hierarchical decomposition (train only high-level policy in kinematic sim while relying on robust hardware low-level controllers), high simulation speed enabling much larger experience (≈10× more steps), and adding real-world-derived actuation-noise models during kinematic training to improve robustness.",
            "fidelity_requirements_identified": "For high-level navigation tasks that can be abstracted in CoM motion, high-fidelity low-level physics is not required and can even harm transfer; instead, simulation speed and accurate low-dimensional modeling of actuation noise are the key fidelity requirements.",
            "fine_tuning_in_real_world": false,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": true,
            "fidelity_comparison_results": "Kinematic (low-fidelity) training outperformed dynamic (high-fidelity) training in both sim and real tests: e.g., kinematic policies achieved 100% SR on real Spot vs. dynamic policies 40–67% SR; in sim, kinematic-trained policies often had higher SR than dynamic-trained policies even under dynamic evaluation.",
            "key_findings": "Training high-level navigation policies in lower-fidelity (kinematic) simulation leads to better sim-to-real transfer for PointGoal navigation on legged robots because (1) low-fidelity sims are faster enabling far more training experience and better high-level reasoning, and (2) they avoid overfitting to imperfect/incorrect low-level physics and controllers; modest injection of real actuation-noise into kinematic sims further improves real-world robustness and path efficiency.",
            "uuid": "e1664.0"
        },
        {
            "name_short": "DynamicSim",
            "name_full": "Dynamic simulation / dynamically-trained policies (rigid-body + contact)",
            "brief_description": "High-fidelity physics simulation using Bullet with 240Hz physics stepping and low-level controllers that convert CoM velocity commands to joint torques; intended to model contact dynamics and joint-level behavior for training end-to-end control.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "Legged robots (Unitree A1, Unitree AlienGo, Boston Dynamics Spot in simulation)",
            "agent_system_description": "Quadrupedal robots whose low-level dynamics (joint torques, foot contact, gait execution) are simulated and used to train policies that command CoM velocities but rely on simulated controllers (Raibert-style or MPC) to realize them.",
            "domain": "visual navigation / embodied robotics",
            "virtual_environment_name": "Habitat (with Bullet) and iGibson (PyBullet)",
            "virtual_environment_description": "Photorealistic scene rendering combined with full rigid-body, contact, and actuator-level physics simulated using Bullet at 240Hz, with low-level controllers (Raibert or MPC) producing joint torques.",
            "simulation_fidelity_level": "higher-fidelity physics (rigid-body dynamics + contact + controller-level simulation)",
            "fidelity_aspects_modeled": "Rigid-body mechanics, contact dynamics, foot contacts, inverse kinematics & joint torque control, modelled low-level controllers (Raibert-style; evaluated also with MPC), physics integration at 1/240s.",
            "fidelity_aspects_simplified": "Simulator/controller approximations exist (e.g., Raibert controller is an approximation and differs from hardware black-box controllers); real hardware controller specifics and some real-world friction/actuator behaviors remain mismatched despite higher-fidelity modeling.",
            "real_environment_description": "Same LAB lobby; on hardware the robot uses manufacturer black-box low-level walking controllers (Spot) which are not fully replicable in simulation, leading to mismatches between simulated controllers and hardware.",
            "task_or_skill_transferred": "PointGoal visual navigation (same task as kinematic case)",
            "training_method": "Model-free reinforcement learning (DD-PPO) trained with dynamic low-level controller in the loop (Raibert during training; MPC used for evaluation in some experiments)",
            "transfer_success_metric": "Success Rate (SR) and SPL",
            "transfer_performance_sim": "Dynamic policies often achieved reasonable performance in the simulator they were trained in (e.g., A1: Habitat Dynamic training produced higher SR in Habitat-Dynamic) but showed large performance drops when evaluated in other simulators or with different low-level controllers (example: A1 Habitat Dynamic 49.8% SR vs iGibson Dynamic 22.3% SR).",
            "transfer_performance_real": "On real Spot, dynamic-trained policies produced lower SR (≈40–67%) and lower SPL compared to kinematic-trained policies; also more collisions and more commanded actions on average.",
            "transfer_success": false,
            "domain_randomization_used": false,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": "Overfitting to specific simulated low-level controller dynamics; mismatch between simulated low-level controllers (Raibert or MPC) and the real robot's black-box controller; simulator inaccuracies in contact and complex actuator behaviors; slow simulation speeds limit the amount of training data and exacerbate overfitting.",
            "transfer_enabling_conditions": "If dynamic training were to transfer well, it would require highly accurate replication of the robot's low-level controller and precise physics fidelity plus sufficient simulation speed to gather large-scale training experience; without those, dynamic training overfits and degrades transfer.",
            "fidelity_requirements_identified": "The paper finds that increased low-level physics fidelity alone is insufficient and can be detrimental unless the simulated low-level controller and dynamics closely match hardware; high-fidelity physics must be coupled with accurate controller modeling and sufficient training scale to be beneficial.",
            "fine_tuning_in_real_world": false,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": true,
            "fidelity_comparison_results": "Dynamic-trained policies underperformed compared to kinematic-trained ones in both sim and real transfer: dynamic policies had lower SR in real tests (40–67% vs 100% for kinematic) and exhibited large sim-to-sim gaps indicating overfitting to simulator/controller details.",
            "key_findings": "High-fidelity dynamic simulation can hurt sim-to-real transfer for high-level navigation policies because learned policies overfit to simulator-specific low-level dynamics and because dynamic sims are much slower (reducing training scale); accurate modeling of low-level controllers is critical but often infeasible for closed-source hardware, making dynamic training less robust for zero-shot transfer.",
            "uuid": "e1664.1"
        },
        {
            "name_short": "ActuationNoise",
            "name_full": "Real-actuation noise modeling for kinematic simulation (Gaussian injection)",
            "brief_description": "A low-dimensional, data-driven model of actuation error (difference between commanded and actual CoM velocities) fitted as Gaussian noise (decoupled and coupled datasets) and injected into the kinematic simulator to increase robustness of learned high-level policies.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "Boston Dynamics Spot (data collection) / kinematically-trained navigation policy (usage)",
            "agent_system_description": "Spot robot used to collect actuation error samples by commanding random CoM velocities and measuring resultant velocities via SDK; noise model applied during kinematic training to simulate realistic actuation error.",
            "domain": "visual navigation / embodied robotics (robustification for sim2real)",
            "virtual_environment_name": "Habitat and iGibson (kinematic mode with noise injection)",
            "virtual_environment_description": "Photorealistic environments with kinematic stepping; before stepping, sampled Gaussian actuation noise (separate models for decoupled and coupled actuation) is added to commanded CoM velocities so that resulting pose updates mimic real actuator errors.",
            "simulation_fidelity_level": "low-fidelity physics with injected real-world actuation noise (improved representational fidelity for actuators)",
            "fidelity_aspects_modeled": "Low-dimensional actuator tracking error (difference between commanded and executed CoM linear and angular velocities) modeled as Gaussian with empirically estimated mean and variance; captures asymmetry and higher variance in lateral vs forward directions.",
            "fidelity_aspects_simplified": "Does not model full joint-level dynamics, contact specifics, or history-dependent nonlinear actuator behaviors; uses IID Gaussian assumptions (diagonal variance) rather than time-series or state-conditioned noise.",
            "real_environment_description": "Noise data collected on Spot in an empty room by commanding random velocities (decoupled and coupled) and measuring final velocities using the Boston Dynamics SDK; dataset size: 6,000 samples (2,000 per direction for decoupled).",
            "task_or_skill_transferred": "PointGoal visual navigation with improved robustness to actuation errors on Spot",
            "training_method": "DD-PPO RL with noise samples injected into action (sample from fitted Gaussian per velocity dimension before applying kinematic step)",
            "transfer_success_metric": "Success Rate (SR), SPL (path efficiency), number of collisions, number of actions commanded",
            "transfer_performance_sim": "Policies trained with actuation-noise injection performed similarly in simulation but were trained to be robust to noisy actuation.",
            "transfer_performance_real": "On Spot, kinematic policies trained with decoupled noise achieved 100% SR and increased SPL by ≈4.6% over no-noise kinematic policy; coupled-noise training increased SPL by ≈5.6% and reduced collisions and number of actions (e.g., 22.7 actions and 2.8 collisions vs 26.4 actions and 3.1 collisions for no-noise).",
            "transfer_success": true,
            "domain_randomization_used": true,
            "domain_randomization_details": "Gaussian actuation-noise sampled per control-dimension; two variants: decoupled (separate Gaussians for forward, lateral, angular based on 2,000 samples each) and coupled (single multivariate-like approach using 6,000 joint samples); injected at each action during training.",
            "sim_to_real_gap_factors": "Unmodeled actuation tracking errors and asymmetric lateral vs forward variance; without noise modeling policies exploit simulator-perfect actuation and take risky high-velocity actions that fail on hardware.",
            "transfer_enabling_conditions": "Collecting a small real-world dataset of actuation errors and injecting the learned low-dimensional noise model into kinematic simulation substantially increases robustness and real-world path efficiency without modeling full dynamics.",
            "fidelity_requirements_identified": "For navigation abstracted to CoM commands, accurate low-dimensional actuation noise models (not full dynamics) are sufficient to close remaining gaps and improve zero-shot transfer; modeling more detailed low-level physics is not necessary for this class of tasks.",
            "fine_tuning_in_real_world": false,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": true,
            "fidelity_comparison_results": "Kinematic + actuation-noise &gt; kinematic-no-noise &gt; dynamic-trained policies in real-world SR and SPL; injecting noise increased SPL by ~4.6–5.6% and reduced collisions versus no-noise kinematic training.",
            "key_findings": "A small, empirical, low-dimensional actuation-noise model collected from hardware and injected into a kinematic simulator significantly improves real-world navigation robustness and efficiency, demonstrating that targeted augmentation of low-fidelity sims can close remaining sim2real gaps without expensive high-fidelity physics.",
            "uuid": "e1664.2"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Sim-to-real: Learning agile locomotion for quadruped robots",
            "rating": 2,
            "sanitized_title": "simtoreal_learning_agile_locomotion_for_quadruped_robots"
        },
        {
            "paper_title": "Sim-to-real transfer of robotic control with dynamics randomization",
            "rating": 2,
            "sanitized_title": "simtoreal_transfer_of_robotic_control_with_dynamics_randomization"
        },
        {
            "paper_title": "Closing the sim-to-real loop: Adapting simulation randomization with real world experience",
            "rating": 2,
            "sanitized_title": "closing_the_simtoreal_loop_adapting_simulation_randomization_with_real_world_experience"
        },
        {
            "paper_title": "Sim2real predictivity: Does evaluation in simulation predict real-world performance?",
            "rating": 2,
            "sanitized_title": "sim2real_predictivity_does_evaluation_in_simulation_predict_realworld_performance"
        },
        {
            "paper_title": "DD-PPO: Learning near-perfect pointgoal navigators from 2.5 billion frames",
            "rating": 1,
            "sanitized_title": "ddppo_learning_nearperfect_pointgoal_navigators_from_25_billion_frames"
        },
        {
            "paper_title": "Learning robust perceptive locomotion for quadrupedal robots in the wild",
            "rating": 1,
            "sanitized_title": "learning_robust_perceptive_locomotion_for_quadrupedal_robots_in_the_wild"
        }
    ],
    "cost": 0.014808249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Rethinking Sim2Real: Lower Fidelity Simulation Leads to Higher Sim2Real Transfer in Navigation</p>
<p>Joanne Truong truong.j@gatech.edu 
Georgia Institute of Technology</p>
<p>Max Rudolph maxrudolph@gatech.edu 
Georgia Institute of Technology</p>
<p>Naoki Yokoyama nyokoyama@gatech.edu 
Georgia Institute of Technology</p>
<p>Sonia Chernova chernova@gatech.edu 
Georgia Institute of Technology</p>
<p>Dhruv Batra dbatra@gatech.edu 
Georgia Institute of Technology</p>
<p>MetaAI</p>
<p>Akshara Rai akshararai@fb.com 
MetaAI</p>
<p>Rethinking Sim2Real: Lower Fidelity Simulation Leads to Higher Sim2Real Transfer in Navigation
Sim2RealDeep Reinforcement LearningVisual-Based Navigation
If we want to train robots in simulation before deploying them in reality, it seems natural and almost self-evident to presume that reducing the sim2real gap involves creating simulators of increasing fidelity (since reality is what it is). We challenge this assumption and present a contrary hypothesis -sim2real transfer of robots may be improved with lower (not higher) fidelity simulation. We conduct a systematic large-scale evaluation of this hypothesis on the problem of visual navigation -in the real world, and on 2 different simulators (Habitat and iGibson) using 3 different robots (A1, AlienGo, Spot). Our results show that, contrary to expectation, adding fidelity does not help with learning; performance is poor due to slow simulation speed (preventing large-scale learning) and overfitting to inaccuracies in simulation physics. Instead, building simple models of the robot motion using real-world data can improve learning and generalization.</p>
<p>Introduction</p>
<p>The sim2real paradigm consists of training robots in simulation (potentially for billions of simulation steps corresponding to decades of experience [1]) before deploying them in reality. The last few years have seen significant investments -the development of new simulators [2][3][4][5][6][7][8][9][10][11][12], curation and annotation of 3D scans and assets [13][14][15], and development of techniques for overcoming the sim2real gap [16][17][18][19] -resulting in a number of successful demonstrations of sim2real transfer [20][21][22][23][24][25]. However, no simulator is a perfect replica of reality and the main challenge in this paradigm is overcoming the sim2real gap, defined as the drop in a robot's performance in the real-world (compared to simulation). It seems natural and almost self-evident to presume that reducing this sim2real gap involves creating simulators of increasing physics fidelity, and this sometimes forms the default operating hypothesis of the field.</p>
<p>We challenge this convention and present a counter-intuitive idea -sim2real transfer of robots may be improved not by increasing but by decreasing simulation fidelity. Specifically, we propose that instead of training robots entirely in simulation, we use classical ideas from hierarchical robot control [26] to decompose the policy into a 'high-level policy' (that is trained solely in simulation) and a 'low-level controller' (that is designed entirely on hardware and may even be a black-box controller shipped by a manufacturer). This decomposition means that the simulator does not need to model low-level dynamics, which can save both simulation time (since there is no need to simulate expensive low-level controllers), and developer time spent building and designing these controllers.</p>
<p>We conduct a systematic large-scale evaluation of our hypothesis on the task of PointGoal (visual) Navigation [27] in unknown environments -using 2 simulators (Habitat and iGibson) and 3 different robots (A1, AlienGo, Spot). We train policies using two physics fidelities -kinematic and dynamic. Kinematic simulation uses abstracted physics and 'teleports' the robot to the next state using Euler integration; kinematic policies command robot center-of-mass (CoM) linear and angular velocities. Dynamic simulation consists of rigid-body mechanics and simulates contact dynamics (via Bullet [12]); dynamic policies command CoM linear and angular velocities, which are converted to robot joint-torques by a low-level controller operating at 240 Hz. We find that across all robots, a kinematically trained policy outperforms dynamic policies, even when evaluated using dynamic simulation and control. Additionally, we show that the trained kinematic policy can be transferred to a real  Figure 1: Left: We train visual navigation policies at two levels of fidelity -kinematic and dynamic. In kinematic control (top), the robot is 'teleported' to the next state using Euler integration. In dynamic control (bottom), the robot's velocity commands are converted to leg joint-torques and rigid-body physics is simulated at 240Hz. Right: We evaluate the kinematic and dynamic trained policies in simulation (top) and the real-world (bottom) across 5 identical episodes. The kinematic policy achieves a 100% success rate in all 5 episodes, and the robot takes similar paths in both simulation and the real-world. On the other hand, the dynamic policy achieves a 20-60% success rate, and the trajectories taken in simulation and the real-world do not correlate, pointing towards a larger sim2real gap.</p>
<p>Spot robot, which ships with manufacturer-provided 'black-box' low-level controllers that cannot be accurately simulated. In contrast, dynamic policies fail to achieve efficient navigation behavior on Spot, due to the sim2real gap and less simulation experience.</p>
<p>The reasons for these improvements are perhaps unsurprising in hindsight -learning-based methods overfit to simulators, and present-day physics simulators have approximations and imperfections that do not transfer to the real-world. A second equally significant mechanism is also in playlower fidelity simulation is typically faster, enabling policies to be trained with more experience under a fixed wall-clock budget. Even when the kinematic policies were trained for 2.3× less wallclock time than the dynamic policies (with the same compute), the kinematic policies were able to learn from 10× the amount of data. While our results are presented on legged locomotion and visual navigation, the underlying principle -of architecting hierarchical policies and only training the highlevel policy in an abstracted simulation -is broadly applicable. We hope that our work leads to a rethink in how the research community pursues sim2real and in how we develop the simulators of tomorrow. Specifically, our findings suggest that instead of investing in higher-fidelity physics, the field should prioritize simulation speed for tasks that can be represented with abstract action spaces.</p>
<p>Related Work</p>
<p>Visual Navigation. Recent works have shown that large-scale indoor environments and simulators like Habitat [2,3] and iGibson [4] can enable end-to-end learning of navigation policies from large amounts of agent-or expert-generated data [28][29][30] on simple, wheeled systems. This is in contrast to the typical mapping and planning paradigm used in classical robotics, which can suffer when the quality of maps is low [31] or requires expensive equipment like LiDAR [32]. In this work, we show that such end-to-end learning is also possible for complex, legged robots.</p>
<p>Sim2real for Legged Robots. Sim2real quadrupedal locomotion has been widely studied in the past several decades [22,[33][34][35][36], with most learning low-level skills in simulation and transferring them to hardware [37], or adapting them online to reduce the sim2real gap [38,39]. However, these policies are typically blind, and use only proprioceptive sensors on the robot to determine actions [23,25]. In contrast, an autonomous robot needs to respond to its environment, and take visual input into account. Some works have proposed learning visual policies in simulation and applying them to the real-world [22,24,40], and other works leverage expensive LiDAR sensors for external sensing [41]. These works use learned or hand-designed physically simulated low-level controllers; we show that physics simulations can be detrimental to learning high-performing sim2real policies, even for complex legged robots. Work from [42] also utilize similar simulator simplifications to increase simulation speed in training navigation policies for 1 robot in a single room, but does not discuss how this formulation affects sim2real transfer. In our work we present a rigorous (multirobot, multi-simulator) study of the effect of different simulation fidelities on visual navigation.</p>
<p>Abstracted Task-space Learning. Abstracted (hierarchical, high-level) action spaces are common in robotics literature. Examples include task and motion planning for manipulation [43][44][45], legged locomotion [46,47], navigation [20], etc. Several works reason over symbolic actions like pick and place, or hierarchical policies with discrete/continuous attributes [33-35, 48, 49], or even abstracted dynamics models [36]. While the ideas of abstracted/hierarchical policies are fairly common, typically both the high-and low-level policies are learned in simulation and transferred to reality [33,34,36], often augmented with techniques like domain randomization [37] and real-word adaption [18]. Instead, we use an abstracted simulator, which does not model low-level physics, and learn high-level policies that are transferred to the real-world in a zero-shot manner.</p>
<p>Experimental Setup</p>
<p>Task: PointGoal Navigation. In the task of PointGoal Navigation [27], a robot is initialized in an unknown environment and is tasked with navigating to a goal coordinate without access to a pre-built map of the environment. The goal is specified relative to the robot's starting location for the episode (i.e., "go to ∆x, ∆y"). The robot has access to an egocentric depth sensor and an egomotion sensor (sometimes referred to as GPS+Compass in this literature) from which the robot derives the goal location relative to its current pose. An episode is considered successful when the robot reaches the goal position within a success radius (typically half of the robot's body length). The robot operates within constraints of maximum number of steps per episode (150 for Spot) and velocity limits (± 0.5 m/s for linear and ± 0.3 rad/s for angular velocities on Spot). We linearly scale the linear and angular velocity limits for A1 and Aliengo to be proportional to the length of each robot's leg, and inversely scale the maximum number of steps allowed. In effect, smaller robots have a smaller maximum allowed velocity to improve stability during execution, but are allowed more steps to reach the goal. The exact parameters used for each robot is described in the appendix. For evaluation, we report the success rate (SR), and Success inversely weighted by Path Length (SPL) [27], which measures the efficiency of the trajectory taken with respect to the ground-truth shortest path.</p>
<p>Robot Platforms. We study visual navigation for 3 quadrupedal robots -A1 and Aliengo from Unitree [50], and Spot from Boston Dynamics (BD) [51] in simulation. In the real-world, we show sim2real transfer of the learned navigation policies to Spot. To have a consistent camera setup across all the robots, we attach an Intel RealSense D435 camera to Spot in the real-world, and use this camera for visual inputs to the policy. In our hardware experiments, we want to measure how often our sim2real policies lead to collisions without jeopardizing safety. We achieve this balance as follows: the BD collision-avoidance capability is kept turned on, set to trigger at a tight threshold of 0.10m. Next, we track the number of times the robot comes within 0.20m of any obstacle (as measured by any of the 5 onboard depth cameras). This gap (between 0.20m and 0.10m) allows us to record possible collisions while preventing actual ones. While the BD API allows for highlevel navigation without access to a map, it cannot navigate around obstacles autonomously, without a map. In our work, we consider complex, long-range navigation paths (up to 30m) in cluttered environments with many obstacles; the goals are unreachable with the just BD navigation API.</p>
<p>Simulation Environments. We use two simulation platforms -Habitat [2,3] and iGibson [4] for training and evaluation. Both simulators support rendering of photorealistic environments; Habitat uses a low-level (C++) integration with the Bullet physics engine [12], while iGibson leverages Py-Bullet, the Python-based integration of Bullet. Thus, while the underlying physics engines between A1 Aliengo Spot Spot-Real the two are the same, Habitat runs ∼ 1200% faster than iGibson [3]. This allows us to train policies faster with Habitat than with iGibson even when using identical policies and compute.</p>
<p>Dataset. For training and evaluation, we use a combination of the Habitat-Matterport (HM3D) [13] and Gibson [52] 3D datasets. The two datasets combined consist of over 1000 high-resolution 3D scans of real-world indoors environments, and consists of realistic clutter. We generate training and evaluation episodes compatible with our robots for the HM3D and Gibson scenes following the procedure described in [2]. Specifically, we restrict the geodesic distance from the start and positions to be between 1 and 30m, and increase navigation complexity by rejecting paths that consist of nearstraight lines, with few obstacles. As described in [2], both of these heuristics result in complex, but navigable paths. Additionally, we check for collisions along the sampled paths using the URDF of the largest robot (Spot) to ensure that all paths are navigable. Real-World Test Environment. The realworld evaluation environment, LAB, is a 325m 2 lobby in a commercial office building. The lobby contains furniture such a couches, cushions, bookshelves and tables. We specify a set of 5 waypoints as the start and end locations for the navigation episodes in LAB with an average episode length of 10m. We match the furniture layout to the position captured in the 3D scan ( Figure 3) to run identical evaluation experiments in both simulation and the realworld. The scan of LAB is not part of training.</p>
<p>Kinematic and Dynamic Control for Visual Navigation</p>
<p>As illustrated in Figure 4, our proposed approach is hierarchical, with (1) a high-level visual navigation policy that commands desired center of mass (CoM) motion at 1Hz, and (2) a low-level controller that follows this desired motion. We consider controllers at two levels of abstraction -'kinematic' and 'dynamic'. The kinematic controller simply integrates the desired velocity and outputs a CoM position at 1Hz; kinematic simulation then teleports the robot to the desired state. The dynamic controller uses a low-level controller that commands joint torques at 240Hz; dynamic simulation models rigid-body and contact dynamics via Bullet (with a physics step-size of 1 /240 sec). We provide details of all three of these pieces (high-level policy, kinematic and dynamic controllers) next.</p>
<p>High-level Visual Navigation Policies. The high-level policy takes as input an egocentric depth image, and the goal location relative to the robot's current pose. The output of the policy is a 3-dimensional vector, representing the desired CoM forward, lateral, and angular velocities (V x , V y , ω). The neural network architecture consists of a ResNet-18 visual encoder and a 2-layer LSTM policy. Using a recurrent policy allows the policy to learn temporal dependencies through the hidden state. The final layer of the policy parameterizes a Gaussian action distribution from which the action is sampled. The policy is trained using DD-PPO [1], a distributed reinforcement learning method, in both the Habitat and iGibson simulators. Our reward function is derived from [22], with an added penalty for backward velocities, which can lead to collisions and hurts performance.</p>
<p>Kinematic Control and Simulation. In kinematic control, the final state of the robot is calculated by integrating the desired CoM velocity commanded by the high-level navigation policy at 1Hz. The robot is directly moved to the desired pose, without running a physics simulation. In both Habitat and iGibson, the robot is kept in place if being at the new desired state would result in a collision. The objective of the kinematic control is to abstract away the low-level physics interactions between the robot and its environment. This has two advantages: (1) it avoids the need to accurately model low-level controllers, especially for closed-source robots like Spot; (2) it enables faster simulation speed by avoiding high-frequency physics integration, conducive to model-free RL that requires large amounts of experience. On the other hand, teleporting the robot to the desired state might remove necessary dynamics, such as poor tracking of low-level controllers. In Section 5, we propose how to incorporate such low-level characteristics into a kinematic simulation using real-world data.</p>
<p>Dynamic Control in Simulation and Hardware. We experiment with two different low-level dynamic controllers for quadruped robots. The first is an expert-designed Raibert-style controller from [22], which consists of a footstep generator and an inverse kinematic solver that commands desired joint angles from CoM velocities. The joint angles are converted to joint torques using a linear feedback controller, and applied to the simulation. This controller was shown to achieve sim2real transfer for A1 [22]. However, on other robots in our experiments, it shows relatively poor tracking of highlevel commands. Thus, we also experiment with another model-predictive control (MPC) dynamic controller from [38], which commands joint torques directly. This controller has been applied to real-world A1 robot [53,54] and shows better tracking of desired velocities for our test robots, as compared to the Raibert controller from [22]. However, MPC is prohibitively slow and cannot be used for training RL policies. Thus, we use Raibert for training dynamic policies, but evaluate using MPC. 1 This difference in train and evaluation dynamics controllers has multiple purposes: (1) the evaluation using MPC improves performance of most policies, including dynamic policies, due to its better ability to track high-level commands; (2) the difference between the two dynamic controllers in simulation is also a proxy for the difference between our low-level controllers and closed-source controllers from Spot. If a dynamic policy cannot transfer from Raibert to MPC, it has a low chance of transfer to Spot which has black box BD controllers, or even other robots in the real-world.</p>
<p>Both dynamic controllers model the low-level physics interactions between the robot and the environment. This makes them considerably slower than the kinematic controller, making training RL policies challenging. Moreover, for Spot, the low-level controller implementation is not openly available, making it hard to reproduce the low-level controller in simulation. For commercial legged robots that come with black-box controllers, kinematic simulations are the ideal fidelity for learning navigation policies. Our experiments in Section 5 show that the added fidelity of dynamic controllers does not benefit policy learning, or sim2real transfer.</p>
<p>Impact of Low-level Controllers on Policy Learning. The low-level controller used in dynamic simulations can have a significant impact on the learned policy. Low-level controllers both in sim and real are biased, and the status quo is to make them biased in the same way, as the policy learns to compensate for the bias error. For example, [55] add learned actuation noise to their simulation, while [37] measure hardware characteristics and add them to the simulation. However, given the high-dimensional nature of low-level physics, it is very difficult to ensure that the biases incorporated in simulation actually hold for a larger range of motions that the data was collected on. Thus, there are several iterations of data-collection, bias updates, training and deployment needed for good performance. Instead, kinematic controllers are unbiased by design, and can easily incorporate hardware bias through low-dimensional CoM motion noise models created from a small amount of real-world data, as shown in our experiments in Section 5.</p>
<p>Results and Analysis</p>
<p>In this section, we first study generalization of visual navigation policies across simulators (trained in one sim, tested in another) and across controllers (trained with one controller, tested with another). This shows the importance of fast simulation for learning high-level policies by comparing performance of kinematic and dynamic policies trained for the same wall-clock time. Next, we examine the performance of the different policies at zero-shot sim2real transfer on the Spot robot.</p>
<p>How large is the sim2sim gap? High for dynamic, and low for kinematic policies. We exhaustively study the combinatorial space of experiments -policies trained under 2 training conditions (with kinematic and dynamic simulation) × 2 evaluation conditions (kinematic and dynamic simulation) × 2 simulators (Habitat and iGibson) × 3 robots (A1, Aliengo, Spot). For each condition, we train and report results with 3 random seeds. Each policy is trained using 8 GPUs for 3 days, resulting in a cumulative training budget of 6,912 GPU-hours (288 GPU-days). The average success rates are presented in Figure 5. Rows represent the evaluation conditions as tuples (simulator, fidelity), while columns represent the training conditions. We evaluate all policies across 1,100 episodes from 110 unique scenes in the HM3D + Gibson validation split. Figure 5: Average success rates for sim2sim and kinematic2dynamic transfer for A1, Aliengo and Spot. We see that the kinematic trained policies perform the best overall (red quadrants), and also often outperform the dynamic trained policies, even when evaluated using dynamic control (green quadrants vs. orange quadrants).</p>
<p>We make two key observations here:</p>
<ol>
<li>
<p>Kinematic-trained policies perform best overall, for all robots. In all cases, kinematic policies outperform the dynamic policies, even when evaluated using dynamic control, e.g. 62.1% SR for A1 in (iGibson, Kinematic) vs. 24.2% SR in (iGibson, Dynamic), Fig. 5, left. This is a surprising result because the kinematic policies are being evaluated in an out-of-distribution setting, which was never seen or accounted for during training. On the other hand, the dynamic policies are being evaluated in the domain that they were trained in, hence do not require control-related generalization.</p>
</li>
<li>
<p>Dynamic policies are not robust to different dynamic simulations. The dynamic policies from the two simulations observe significant performance drops when evaluated in the other dynamic simulation. This points to the dynamic policies overfitting to the simulator dynamics during training, failing to generalize to a new setting, see e.g. column 3, rows 3 and 4; 49.8% SR for A1 in (Habitat, Dynamic) vs. 22.3% SR in (iGibson, Dynamic). (iGibson, Dynamic) shows poor performance in both iGibson and Habitat, with slightly poorer performance in Habitat. This sensitivity to simulation makes training dynamics policies difficult, especially when the controller for the real-world robot is unknown. Even if the real-world controller is known, simulation physics and real-world are different, and sim2real transfer of the learned policy can suffer (as evidenced by low sim2sim transfer). On the other hand, kinematic policies, that have been trained with no physics, can generalize to the different dynamic controllers. Both of these results go to show that not only kinematic trained policies are able to learn the task well, they have learned to reason without overfitting to simulation physics, making their chances of successful sim2real transfer high.</p>
</li>
</ol>
<p>Why do kinematic-trained policies outperform dynamic ones? Scale. We plot the evaluation performance of both policies in Habitat kinematic and dynamic simulation in Figure 6. We train both policies to convergence-3 days for the kinematic policies, and 7 days for the dynamic policies. While the kinematic policies are trained for 2.3× less wall-clock time, they still outperform the longer trained dynamic policies, even when evaluated out-of-distribution using dynamic control (+12% SR). Kinematic training is much faster than training dynamically (right, Fig. 6); with kinematic training, the robot is able to learn from approximately 10× more steps of experience (500M steps vs. 50M steps). This increased experience allows the kinematic policies to learn intelligent high-level reasoning. We contend that for any computational budget, there will always be more complex tasks that are bottlenecked by that budget. Wall-clock time is the true limitation for learning-based sim2real approaches (not experience, as different simulators have different speeds). How large is the sim2real gap for kinematic and dynamic trained policies? We evaluate the kinematic and dynamic policies on a Spot robot in the novel LAB environment described in Section 3. Note that scans of LAB were not part of training. We evaluate 3 seeds of each policy over 5 episodes in the real-world and report the average success rate (SR) and Success weighted by Path Length (SPL) [27] in Table 1 (reported as a percentage for readability). Each control type is tested in 15 real-world episodes; one run of the Spot robot navigating LAB is shown in Figure 7. Success in the real-world is measured by computing final distance from the goal position using egomotion estimates provided by the Boston Dynamics SDK.  Table 1: Zero-shot sim2real transfer performance for the visual navigation policies. Success rate (SR) and path efficiency (SPL) are high for kinematic policies, while dynamic policies have lower performance due to the dynamics gap between the low-level control in training and the controller on the robot in the real-world.</p>
<p>As reported in Table 1, all kinematic policies achieve a high success rate of 100% and SPL of 82-83% (rows 3 and 4). On the other hand, the success rate drops to 40-67% for the dynamic policies (rows 1 and 2). We notice that the dynamic policies typically commanded lower velocities, and often get stuck around obstacles (Figure 8). This is shown in the higher number of actions commanded and higher collision count for both dynamic policies; on average, a dynamic policy trained in Habitat took 107.9 actions, and collided 41.2 times (row 1, columns 8 and 9), whereas a kinematic policy also trained in Habitat took 26.4 actions, and collided 3.1 times (row 3, columns 8 and 9). We attribute this to the impoverished experience of the dynamic policies; the policies did not learn robust navigation policies that could avoid obstacles during navigation. Additionally, they overfit to the low-level behavior, which can be unstable at high velocities in sim, but not on hardware. Figure  8 (left) shows that the kinematic policy commands higher forward velocities, while the dynamic Figure 7: One run of the Spot robot navigating the real-world LAB environment using a kinematically trained policy from AI Habitat. The robot successfully navigates a hallway, moves around furniture and turns into the next hallway before stopping. In contrast, the native BD controllers without a map can only reach visible goals.</p>
<p>policy commands slower velocities (right), which are often not achieved by the robot likely due to an obstacle. 2 Successfully executed commands appear on the diagonal.</p>
<p>To improve kinematic simulation fidelity, we model actuation noise (difference between commanded and true velocity) on Spot and use it during kinematic training, similar to [16]. We collect 6,000 samples of decoupled (linear and angular velocities are actuated separately) and coupled (linear and angular velocities are actuated together) actuation noise. The parameters for noise in each dimension, and details about data collection and modeling can be found in the appendix. During training, we sample from the Gaussian distribution for each dimension, and add it to the policy's predicted velocity. We see that policies trained with noise also achieve 100% success in the real-world (rows 5 and 6), and are able to increase path efficiency (SPL) (4.6% using decoupled (row 4 vs. 5), and 5.6% using coupled actuation noise (row 4 vs. 6)). The number of collisions and commanded actions are also lower for these policies, compared to kinematic policies trained with no noise (22.7 actions and 2.8 collisions vs. 26.4 actions and 3.1 collisions). These improvements are due to the added robustness that training with noise provides -uncertainty during training forces the policy to take less risky actions resulting in fewer collisions in the real-world.</p>
<p>Conclusion, Limitations, and Future Work</p>
<p>In this work, we study the role of simulation fidelity for sim2real of visual navigation policies on three simulated and one real legged robot. Contrary to expectations, we find that higher simulation fidelity does not enable learning better high-level visual navigation policies. Dynamic policies tend to overfit to low-level simulation details, resulting in poor transfer to the real-world. On the other hand, kinematic policies are able to generalize well. These results raise important questions about the need for simulation fidelity for sim2real, especially in abstracted action spaces.</p>
<p>One limitation of this work is that we assume access to a robust 'black box' controller on hardware. While most robots come shipped with manufacturer-provided controllers, the level of accuracy may differ between robots, and more robust noise modeling may be needed to better characterize the actuation noise. In the future, we plan to improve the modeling of real-world actuation noise by using a neural network conditioned on previous states and actions of the robot. Another limitation of this work is that we specifically address only tasks that can be abstracted with high-level actions, like navigation or object rearrangement (pick, place, open, close). We agree that there are many tasks that cannot be learned in a kinematic simulation -e.g. tasks that require reasoning about low-level interactions with the environment, like dexterous manipulation, or low-level walking controllers. It is essential to create high-fidelity simulators for sim2real on such tasks. </p>
<p>Robot Details</p>
<p>Additional Evaluation Results</p>
<p>We present additional results using the Raibert controller for evaluation in Figure 1 (row 4). The policies are evaluated across 3 seeds, using the HM3D + Gibson validation split which consists of 1,100 episodes from 110 unique scenes. Our results are consistent with evaluation using the MPC controller-kinematic trained policies still outperform the dynamic trained policies, even when evaluated using dynamic control 1 (68.9 % SR for Aliengo in Habitat, Kinematic vs. 45.4 % SR in Habitat, Dynamic, Fig. 1, middle). Figure 1: Average success rates for sim2sim and kinematic2dynamic transfer for A1, Aliengo and Spot. Dynamic evaluation in iGibson is performed using the Raibert controller [1]. We see that the kinematic trained policies still perform the best overall (red quadrants), and also often outperforms the dynamic trained policies, even when evaluated using dynamic control (green quadrants vs. orange quadrants).</p>
<p>Actuation Noise Modeling Details</p>
<p>We collect actuation noise (difference between the commanded and true velocity of the robot) on the Boston Dynamics Spot robot by commanding the robot at a random velocity for 1Hz in an empty room and measuring the final velocity. Noise is collected in a decoupled and coupled manner described below:</p>
<ol>
<li>Decoupled: Random velocities ( ∼ U(−0.5, 0.5)) are commanded in the forward, lateral, and angular directions separately. When collecting data for the forward direction, the side-ways direction velocity is commanded zero velocity; the opposite is true when collecting data for the sideways direction. We collect 2,000 datapoints for each direction. 2. Coupled: Random velocities ( ∼ U(−0.5, 0.5)) are commanded in the forward, lateral, and angular directions at the same time.</li>
</ol>
<p>Each dataset contains 6,000 data points, with decoupled data containing 2000 data points for each direction. We choose to model the uncertainty in the robot's actuation with a standard bivariate Gaussian with a diagonal variance similar to [2]. The collected data is used to generate mean and variance parameters for a Gaussian distribution describing the noise in each dimension, as shown in Table 2. The Gaussian models are then used to inject noise into the the kinematic simulation during training time through the following method: 1) the policy predicts a velocity, 2) the Gaussian distributions for each direction are sampled, 3) the sampled noise is added to the policy's predicted velocity, and 4) the robot's state is updated according to the noisy velocity.</p>
<p>The two different noise collection approaches aim to study the effects data collection has on the resulting noise model. Our experiments show that both noise models perform better in the realworld than no noise modeling, and coupled noise performs slightly better than decoupled.  It is also important to note that while Spot (and other legged robots) can move in all directions, these robots are not necessarily omnidirectional platforms, since they cannot move in all directions equally well. To illustrate this, we collect displacement errors on Spot in forward and lateral direc- tions while commanding random desired CoM movements ( Figure 2). If the robot were perfectly omnidirectional, we would expect the means and variances for the forward and lateral direction to be small and the same. While the mean error in both directions is close to 0, the standard deviations in the forward and lateral directions are significantly larger and asymmetric. In the forward direction, the standard deviation is 0.097 meters, and in the lateral direction it is 0.139 meters. This tracking error, which increases with commanded velocity, motivated the choice of saturating commanded desired velocity at 0.5 m/s. This behavior is observed on the Spot robot using Boston Dynamics walking controllers, which is a very good, highly tuned controller for the robot. We would expect any open-sourced controller which is not tuned for a particular robot to only be worse.</p>
<p>Additional Low-level Controller Details</p>
<p>We use two different kinds of low-level controllers in our work-an expert-designed Raibert controller from [1] (modified to allow for lateral movement), and a model-predictive control (MPC) controller from [3]. The Raibert controller takes in desired CoM velocities (v x des , v y des , ω des ) from the high-level policy to calculate the desired foot placement location, following equations 1-3 from [1]. The footstep trajectory is followed using inverse kinematics. The MPC controller uses a contact schedule to determine each leg's contact state and compute the optimal joint torque for each leg.</p>
<p>5 Additional Policy Details 5.1 High-level policy parameters.</p>
<p>We use PPO with Generalized Advantage Estimation (GAE). We use a discount factor of 0.99, and GAE parameter of 0.95. We use the Adam optimizer, with a learning rate of 2.5e-4. We run 8 agents in parallel (in different environments) per GPU, and each agent collects a rollout of 128 frames of experience. We use 8 GPUs, for a total of 64 parallel workers.</p>
<p>Reward function.</p>
<p>Our reward function is derived from [1], with an added penalty for backward velocities, which can lead to collisions and hurts performance. Specifically, our reward function is defined as:</p>
<p>r t (a t , s t ) = R geo + R coll + R f all + R success + R slack + R backward (1)</p>
<p>R geo is a shaped reward, denoting the change in geodesic distance to the goal between two timesteps. R coll is a penalty for collisions. We set the collision penalty to -0.03. R f all is a penalty if the robot falls over. We set the falling penalty to -5.0, and terminate the episode. R success is the terminal reward for completing the episode. We set the terminal reward to 10.0. R slack is a slack penalty used to encourage the robot to reach the goal as fast as possible. We set the slack penalty to -0.002. R backward is a penalty for moving backwards, as moving backwards can lead to collisions. We set the backwards penalty to -0.03.</p>
<p>Dynamic Simulation Overfitting Details.</p>
<p>We define overfitting as the drop in performance when testing on a different controller and/or simulator than training. This is a natural generalization of the standard definition of overfitting in supervised learning (accuracy on IID training dataset -accuracy on IID testing dataset). We train dynamic policies for all three robots to congergence (Figure ??) In Table 3, we show the success rate on Habitat-Dynamic (training scenario) -success rate on iGibson-Dynamic (testing scenario) for all 3 robots. Note that these are all evaluations on the same houses/scenes/environments (from a held-out evaluation set) and the only factor changing is the simulator. We can clearly see that the gap is always positive, indicating that policies trained on Habitat-Dynamic perform worse when evaluated on iGibson-Dynamic compared to evaluation on Habitat-Dynamic. As can be expected, in all but one case, the performance gaps are increasing with more RL training, though this is not strictly necessary. A well-trained high-level policy can learn to reason intelligently about navigation (even with dynamic controllers), and then perform well across simulators.  Table 3: We measure the performance gap for dynamic policies between evaluations in Habiat-Dynamic and iGibson-Dynamic. The gap is always postive, and in all cases but one, the performance gaps increase with more RL training, demonstrating that the dynamic policies overfit to the simulator and controller it was trained on.</p>
<p>Figure 2 :
2Robots used for training and evaluation.</p>
<p>Figure 3 :
3The real-world testing environment is a part of a large commercial building and contains clutter from furniture such as tables, bookshelves, and couches.</p>
<p>Figure 4 :
4Our architecture for PointGoal Navigation on a legged robot. A high-level visual navigation policy predicts CoM linear and angular velocities. The velocities are passed into either a kinematic or dynamic lowlevel controller to step the robot in simulation. In the real-world, we directly send the velocity commands from the high-level policy to the robot, and uses the low-level controller from Boston Dynamics for movement.</p>
<p>Figure 6 :
6Success rate of PointNav policies with A1 trained and evaluated in Habitat with kinematic or dynamic control. Left: Kinematic policies outperform the dynamic trained policies (+12% SR), even when evaluated using dynamic control. Right: Using kinematic control, we can train our robot for 10× more steps of experience than with dynamic control under identical compute budgets, despite training for 2.3× less wall-clock time.</p>
<p>Figure 8 :
8Commanded vs. resultant velocities during real-world trajectory rollouts for dynamically and kinematically trained policies.</p>
<p>Figure 2 :
2We collect displacement errors on Spot in forward and lateral directions. The standard deviation for the displacement errors between the forward and lateral directions are large and asymmetric, demonstrating that the robot is not perfectly omnidirectional.</p>
<p>Table 1 :
1Robot specific parameters used for training and evaluation. The maximum number of steps and velocity limits for each robots are set in proportion to the robot's leg length.Parameter 
A1 
Aliengo 
Spot </p>
<p>1 
Success radius (m) 
0.24 
0.32 
0.425 
2 
Maximum number of steps 
326 
268 
150 
3 
Linear velocity limits (m/s) 
± 
0.23 
± 
0.28 
± 
0.50 
4 
Angular velocity limits (rad/s) 
± 
0.14 
± 
0.17 
± 
0.3 
5 
Leg length (m) 
0.2 
0.25 
0.44 </p>
<p>Table 2 :
2We fit a bivariate Gaussian to the actuation noise collected on a real Spot robot. During kinematic training, we sample from the noise models and inject the realistic actuation noise to the robot's desired final state.
Evaluation using Raibert[22] can be found in the appendix.
Actual velocity is measured using the Boston Dynamics SDK.
For all robots and training sim/controller except A1, iGibson-Kinematic 6th Conference on Robot Learning (CoRL 2022), Auckland, New Zealand.
AcknowledgmentsThe Georgia Tech effort was supported in part by NSF, ONR YIP, ARO PECASE. JT was supported by an Apple Scholars in AI/ML PhD Fellowship. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the U.S. Government, or any sponsor.License for dataset used Gibson Database of Spaces. License at http://svl.stanford.edu/ gibson2/assets/GDS_agreement.pdfAppendix
DD-PPO: Learning near-perfect pointgoal navigators from 2.5 billion frames. E Wijmans, A Kadian, A Morcos, S Lee, I Essa, D Parikh, M Savva, D Batra, International Conference on Learning Representations (ICLR. 2020E. Wijmans, A. Kadian, A. Morcos, S. Lee, I. Essa, D. Parikh, M. Savva, and D. Batra. DD- PPO: Learning near-perfect pointgoal navigators from 2.5 billion frames. In International Conference on Learning Representations (ICLR), 2020.</p>
<p>Habitat: A Platform for Embodied AI Research. M Savva, A Kadian, O Maksymets, Y Zhao, E Wijmans, B Jain, J Straub, J Liu, V Koltun, J Malik, D Parikh, D Batra, International Conference on Computer Vision (ICCV). M. Savva, A. Kadian, O. Maksymets, Y. Zhao, E. Wijmans, B. Jain, J. Straub, J. Liu, V. Koltun, J. Malik, D. Parikh, and D. Batra. Habitat: A Platform for Embodied AI Research. In Interna- tional Conference on Computer Vision (ICCV), 2019.</p>
<p>Habitat 2.0: Training home assistants to rearrange their habitat. A Szot, A Clegg, E Undersander, E Wijmans, Y Zhao, J Turner, N Maestre, M Mukadam, D Chaplot, O Maksymets, A Gokaslan, V Vondrus, S Dharur, F Meier, W Galuba, A Chang, Z Kira, V Koltun, J Malik, M Savva, D Batra, Advances in Neural Information Processing Systems (NeurIPS). 2021A. Szot, A. Clegg, E. Undersander, E. Wijmans, Y. Zhao, J. Turner, N. Maestre, M. Mukadam, D. Chaplot, O. Maksymets, A. Gokaslan, V. Vondrus, S. Dharur, F. Meier, W. Galuba, A. Chang, Z. Kira, V. Koltun, J. Malik, M. Savva, and D. Batra. Habitat 2.0: Training home assistants to rearrange their habitat. In Advances in Neural Information Processing Systems (NeurIPS), 2021.</p>
<p>igibson 1.0: a simulation environment for interactive tasks in large realistic scenes. B Shen, F Xia, C Li, R Martín-Martín, L Fan, G Wang, C Pérez-D&apos;arpino, S Buch, S Srivastava, L P Tchapmi, M E Tchapmi, K Vainio, J Wong, L Fei-Fei, S Savarese, 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEEpage acceptedB. Shen, F. Xia, C. Li, R. Martín-Martín, L. Fan, G. Wang, C. Pérez-D'Arpino, S. Buch, S. Srivastava, L. P. Tchapmi, M. E. Tchapmi, K. Vainio, J. Wong, L. Fei-Fei, and S. Savarese. igibson 1.0: a simulation environment for interactive tasks in large realistic scenes. In 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), page accepted. IEEE, 2021.</p>
<p>. Isaac Nvidia, Sim, Nvidia. Isaac Sim. https://developer.nvidia.com/isaac-sim, 2020.</p>
<p>Robothor: An open simulation-to-real embodied AI platform. M Deitke, W Han, A Herrasti, A Kembhavi, E Kolve, R Mottaghi, J Salvador, D Schwenk, E Vanderbilt, M Wallingford, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionM. Deitke, W. Han, A. Herrasti, A. Kembhavi, E. Kolve, R. Mottaghi, J. Salvador, D. Schwenk, E. VanderBilt, M. Wallingford, et al. Robothor: An open simulation-to-real embodied AI plat- form. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recogni- tion, pages 3164-3174, 2020.</p>
<p>C Gan, J Schwartz, S Alter, M Schrimpf, J Traer, J Freitas, J Kubilius, A Bhandwaldar, N Haber, M Sano, arXiv:2007.04954A platform for interactive multi-modal physical simulation. arXiv preprintC. Gan, J. Schwartz, S. Alter, M. Schrimpf, J. Traer, J. De Freitas, J. Kubilius, A. Bhandwaldar, N. Haber, M. Sano, et al. ThreeDWorld: A platform for interactive multi-modal physical simulation. arXiv preprint arXiv:2007.04954, 2020.</p>
<p>Rlbench: The robot learning benchmark &amp; learning environment. S James, Z Ma, D R Arrojo, A J Davison, IEEE Robotics and Automation Letters. 52S. James, Z. Ma, D. R. Arrojo, and A. J. Davison. Rlbench: The robot learning benchmark &amp; learning environment. IEEE Robotics and Automation Letters, 5(2):3019-3026, 2020.</p>
<p>SAPIEN: A simulated part-based interactive environment. F Xiang, Y Qin, K Mo, Y Xia, H Zhu, F Liu, M Liu, H Jiang, Y Yuan, H Wang, L Yi, A X Chang, L J Guibas, H Su, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). F. Xiang, Y. Qin, K. Mo, Y. Xia, H. Zhu, F. Liu, M. Liu, H. Jiang, Y. Yuan, H. Wang, L. Yi, A. X. Chang, L. J. Guibas, and H. Su. SAPIEN: A simulated part-based interactive environ- ment. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2020.</p>
<p>Mujoco: A physics engine for model-based control. E Todorov, T Erez, Y Tassa, 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems. IEEEE. Todorov, T. Erez, and Y. Tassa. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 5026- 5033. IEEE, 2012.</p>
<p>C D Freeman, E Frey, A Raichuk, S Girgin, I Mordatch, O Bachem, arXiv:2106.13281Brax-a differentiable physics engine for large scale rigid body simulation. arXiv preprintC. D. Freeman, E. Frey, A. Raichuk, S. Girgin, I. Mordatch, and O. Bachem. Brax-a differen- tiable physics engine for large scale rigid body simulation. arXiv preprint arXiv:2106.13281, 2021.</p>
<p>Pybullet, a python module for physics simulation for games, robotics and machine learning. E Coumans, Y Bai, E. Coumans and Y. Bai. Pybullet, a python module for physics simulation for games, robotics and machine learning. 2016.</p>
<p>Habitat-matterport 3d dataset (hm3d): 1000 large-scale 3d environments for embodied ai. S K Ramakrishnan, A Gokaslan, E Wijmans, O Maksymets, A Clegg, J Turner, E Undersander, W Galuba, A Westbury, A X Chang, arXiv:2109.08238arXiv preprintS. K. Ramakrishnan, A. Gokaslan, E. Wijmans, O. Maksymets, A. Clegg, J. Turner, E. Un- dersander, W. Galuba, A. Westbury, A. X. Chang, et al. Habitat-matterport 3d dataset (hm3d): 1000 large-scale 3d environments for embodied ai. arXiv preprint arXiv:2109.08238, 2021.</p>
<p>A Chang, A Dai, T Funkhouser, M Halber, M Niessner, M Savva, S Song, A Zeng, Y Zhang, arXiv:1709.06158Matterport3d: Learning from rgb-d data in indoor environments. arXiv preprintA. Chang, A. Dai, T. Funkhouser, M. Halber, M. Niessner, M. Savva, S. Song, A. Zeng, and Y. Zhang. Matterport3d: Learning from rgb-d data in indoor environments. arXiv preprint arXiv:1709.06158, 2017.</p>
<p>A X Chang, T Funkhouser, L Guibas, P Hanrahan, Q Huang, Z Li, S Savarese, M Savva, S Song, H Su, arXiv:1512.03012An information-rich 3D model repository. arXiv preprintA. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang, Z. Li, S. Savarese, M. Savva, S. Song, H. Su, et al. ShapeNet: An information-rich 3D model repository. arXiv preprint arXiv:1512.03012, 2015.</p>
<p>Bi-directional domain adaptation for sim2real transfer of embodied navigation agents. J Truong, S Chernova, D Batra, IEEE Robotics and Automation Letters (RA-L). 62J. Truong, S. Chernova, and D. Batra. Bi-directional domain adaptation for sim2real transfer of embodied navigation agents. IEEE Robotics and Automation Letters (RA-L), 6(2):2634-2641, 2021.</p>
<p>Closing the sim-to-real loop: Adapting simulation randomization with real world experience. Y Chebotar, A Handa, V Makoviychuk, M Macklin, J Issac, N Ratliff, D Fox, 2019 International Conference on Robotics and Automation (ICRA). IEEEY. Chebotar, A. Handa, V. Makoviychuk, M. Macklin, J. Issac, N. Ratliff, and D. Fox. Closing the sim-to-real loop: Adapting simulation randomization with real world experience. In 2019 International Conference on Robotics and Automation (ICRA), pages 8973-8979. IEEE, 2019.</p>
<p>Sim-to-real transfer of robotic control with dynamics randomization. X B Peng, M Andrychowicz, W Zaremba, P Abbeel, 2018 IEEE international conference on robotics and automation (ICRA). X. B. Peng, M. Andrychowicz, W. Zaremba, and P. Abbeel. Sim-to-real transfer of robotic control with dynamics randomization. In 2018 IEEE international conference on robotics and automation (ICRA), 2018.</p>
<p>Domain randomization for transferring deep neural networks from simulation to the real world. J Tobin, R Fong, A Ray, J Schneider, W Zaremba, P Abbeel, 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEEJ. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, and P. Abbeel. Domain randomization for transferring deep neural networks from simulation to the real world. In 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2017.</p>
<p>Sim2real predictivity: Does evaluation in simulation predict real-world performance?. A Kadian, J Truong, A Gokaslan, A Clegg, E Wijmans, S Lee, M Savva, S Chernova, D Batra, IEEE Robotics and Automation Letters. 2020RA-LA. Kadian, J. Truong, A. Gokaslan, A. Clegg, E. Wijmans, S. Lee, M. Savva, S. Chernova, and D. Batra. Sim2real predictivity: Does evaluation in simulation predict real-world performance? IEEE Robotics and Automation Letters (RA-L), 2020.</p>
<p>Success weighted by completion time: A dynamics-aware evaluation criteria for embodied navigation. N Yokoyama, S Ha, D Batra, 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). 2021N. Yokoyama, S. Ha, and D. Batra. Success weighted by completion time: A dynamics-aware evaluation criteria for embodied navigation. In 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2021.</p>
<p>Learning navigation skills for legged robots with learned robot embeddings. J Truong, D Yarats, T Li, F Meier, S Chernova, D Batra, A Rai, International Conference on Intelligent Robots and Systems (IROS). 2020J. Truong, D. Yarats, T. Li, F. Meier, S. Chernova, D. Batra, and A. Rai. Learning naviga- tion skills for legged robots with learned robot embeddings. In International Conference on Intelligent Robots and Systems (IROS), 2020.</p>
<p>Learning quadrupedal locomotion over challenging terrain. J Lee, J Hwangbo, L Wellhausen, V Koltun, M Hutter, 10.1126/scirobotics.abc5986Science Robotics. 5475986J. Lee, J. Hwangbo, L. Wellhausen, V. Koltun, and M. Hutter. Learning quadrupedal locomotion over challenging terrain. Science Robotics, 5(47):eabc5986, 2020. doi: 10.1126/scirobotics.abc5986. URL https://www.science.org/doi/abs/10.1126/ scirobotics.abc5986.</p>
<p>Learning robust perceptive locomotion for quadrupedal robots in the wild. T Miki, J Lee, J Hwangbo, L Wellhausen, V Koltun, M Hutter, 10.1126/scirobotics.abk2822Science Robotics. 762T. Miki, J. Lee, J. Hwangbo, L. Wellhausen, V. Koltun, and M. Hutter. Learning robust per- ceptive locomotion for quadrupedal robots in the wild. Science Robotics, 7(62):eabk2822, 2022. doi:10.1126/scirobotics.abk2822. URL https://www.science.org/doi/abs/10. 1126/scirobotics.abk2822.</p>
<p>Rma: Rapid motor adaptation for legged robots. A Kumar, RSSZ Fu, RSSD Pathak, RSSJ Malik, RSSRobotics: Science and Systems. 2021A. Kumar, Z. Fu, D. Pathak, and J. Malik. Rma: Rapid motor adaptation for legged robots. Robotics: Science and Systems (RSS), 2021.</p>
<p>C R Garrett, R Chitnis, R Holladay, B Kim, T Silver, L P Kaelbling, T Lozano-Pérez, arXiv:2010.01083Integrated task and motion planning. arXiv preprintC. R. Garrett, R. Chitnis, R. Holladay, B. Kim, T. Silver, L. P. Kaelbling, and T. Lozano-Pérez. Integrated task and motion planning. arXiv preprint arXiv:2010.01083, 2020.</p>
<p>P Anderson, A Chang, D S Chaplot, A Dosovitskiy, S Gupta, V Koltun, J Kosecka, J Malik, R Mottaghi, M Savva, arXiv:1807.06757On Evaluation of Embodied Navigation Agents. arXiv preprintP. Anderson, A. Chang, D. S. Chaplot, A. Dosovitskiy, S. Gupta, V. Koltun, J. Kosecka, J. Ma- lik, R. Mottaghi, M. Savva, et al. On Evaluation of Embodied Navigation Agents. arXiv preprint arXiv:1807.06757, 2018.</p>
<p>Habitat-web: Learning embodied objectsearch strategies from human demonstrations at scale. R Ramrakhya, E Undersander, D Batra, A Das, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionR. Ramrakhya, E. Undersander, D. Batra, and A. Das. Habitat-web: Learning embodied object- search strategies from human demonstrations at scale. In Proceedings of the IEEE/CVF Con- ference on Computer Vision and Pattern Recognition, pages 5173-5183, 2022.</p>
<p>Learning exploration policies for navigation. T Chen, S Gupta, A Gupta, arXiv:1903.01959arXiv preprintT. Chen, S. Gupta, and A. Gupta. Learning exploration policies for navigation. arXiv preprint arXiv:1903.01959, 2019.</p>
<p>Learning to explore using active neural slam. D S Chaplot, D Gandhi, S Gupta, A Gupta, R Salakhutdinov, arXiv:2004.05155arXiv preprintD. S. Chaplot, D. Gandhi, S. Gupta, A. Gupta, and R. Salakhutdinov. Learning to explore using active neural slam. arXiv preprint arXiv:2004.05155, 2020.</p>
<p>Combining optimal control and learning for visual navigation in novel environments. S Bansal, V Tolani, S Gupta, J Malik, C Tomlin, Conference on Robot Learning. PMLRS. Bansal, V. Tolani, S. Gupta, J. Malik, and C. Tomlin. Combining optimal control and learning for visual navigation in novel environments. In Conference on Robot Learning, pages 420-429. PMLR, 2020.</p>
<p>Real-time loop closure in 2d lidar slam. W Hess, D Kohler, H Rapp, D Andor, ICRA. W. Hess, D. Kohler, H. Rapp, and D. Andor. Real-time loop closure in 2d lidar slam. In ICRA, 2016.</p>
<p>Multi-agent manipulation via locomotion using hierarchical sim2real. O Nachum, M Ahn, H Ponte, S Gu, V Kumar, arXiv:1908.05224arXiv preprintO. Nachum, M. Ahn, H. Ponte, S. Gu, and V. Kumar. Multi-agent manipulation via locomotion using hierarchical sim2real. arXiv preprint arXiv:1908.05224, 2019.</p>
<p>Learning fast adaptation with meta strategy optimization. W Yu, J Tan, Y Bai, E Coumans, S Ha, arXiv:1909.12995arXiv preprintW. Yu, J. Tan, Y. Bai, E. Coumans, and S. Ha. Learning fast adaptation with meta strategy optimization. arXiv preprint arXiv:1909.12995, 2019.</p>
<p>Learning generalizable locomotion skills with hierarchical reinforcement learning. T Li, N Lambert, R Calandra, F Meier, A Rai, arXiv:1909.12324arXiv preprintT. Li, N. Lambert, R. Calandra, F. Meier, and A. Rai. Learning generalizable locomotion skills with hierarchical reinforcement learning. arXiv preprint arXiv:1909.12324, 2019.</p>
<p>Planning in learned latent action spaces for generalizable legged locomotion. T Li, R Calandra, D Pathak, Y Tian, F Meier, A Rai, IEEE Robotics and Automation Letters. 62T. Li, R. Calandra, D. Pathak, Y. Tian, F. Meier, and A. Rai. Planning in learned latent action spaces for generalizable legged locomotion. IEEE Robotics and Automation Letters, 6(2): 2682-2689, 2021.</p>
<p>J Tan, T Zhang, E Coumans, A Iscen, Y Bai, D Hafner, S Bohez, V Vanhoucke, arXiv:1804.10332Simto-real: Learning agile locomotion for quadruped robots. arXiv preprintJ. Tan, T. Zhang, E. Coumans, A. Iscen, Y. Bai, D. Hafner, S. Bohez, and V. Vanhoucke. Sim- to-real: Learning agile locomotion for quadruped robots. arXiv preprint arXiv:1804.10332, 2018.</p>
<p>Learning agile robotic locomotion skills by imitating animals. X B Peng, RSSE Coumans, RSST Zhang, RSST.-W Lee, RSSJ Tan, RSSS Levine, RSSRobotics: Science and Systems. 2020X. B. Peng, E. Coumans, T. Zhang, T.-W. Lee, J. Tan, and S. Levine. Learning agile robotic locomotion skills by imitating animals. Robotics: Science and Systems (RSS), 2020.</p>
<p>Bayesian optimization using domain knowledge on the atrias biped. A Rai, R Antonova, S Song, W Martin, H Geyer, C Atkeson, 2018 IEEE International Conference on Robotics and Automation (ICRA). IEEEA. Rai, R. Antonova, S. Song, W. Martin, H. Geyer, and C. Atkeson. Bayesian optimiza- tion using domain knowledge on the atrias biped. In 2018 IEEE International Conference on Robotics and Automation (ICRA), pages 1771-1778. IEEE, 2018.</p>
<p>Coupling vision and proprioception for navigation of legged robots. Z Fu, A Kumar, A Agarwal, H Qi, J Malik, D Pathak, CVPRZ. Fu, A. Kumar, A. Agarwal, H. Qi, J. Malik, and D. Pathak. Coupling vision and proprio- ception for navigation of legged robots. CVPR, 2022.</p>
<p>Learning to walk in minutes using massively parallel deep reinforcement learning. N Rudin, D Hoeller, P Reist, M Hutter, Conference on Robot Learning. PMLRN. Rudin, D. Hoeller, P. Reist, and M. Hutter. Learning to walk in minutes using massively parallel deep reinforcement learning. In Conference on Robot Learning, pages 91-100. PMLR, 2022.</p>
<p>Learning a state representation and navigation in cluttered and dynamic environments. D Hoeller, L Wellhausen, F Farshidian, M Hutter, IEEE Robotics and Automation Letters. 63D. Hoeller, L. Wellhausen, F. Farshidian, and M. Hutter. Learning a state representation and navigation in cluttered and dynamic environments. IEEE Robotics and Automation Letters, 6 (3):5081-5088, 2021.</p>
<p>C R Garrett, R Chitnis, R Holladay, B Kim, T Silver, L P Kaelbling, T Lozano-Pérez, Integrated task and motion planning. Annual review of control, robotics, and autonomous systems. 4C. R. Garrett, R. Chitnis, R. Holladay, B. Kim, T. Silver, L. P. Kaelbling, and T. Lozano-Pérez. Integrated task and motion planning. Annual review of control, robotics, and autonomous systems, 4:265-293, 2021.</p>
<p>Hierarchical task and motion planning in the now. L P Kaelbling, T Lozano-Pérez, 2011 IEEE International Conference on Robotics and Automation. IEEEL. P. Kaelbling and T. Lozano-Pérez. Hierarchical task and motion planning in the now. In 2011 IEEE International Conference on Robotics and Automation, pages 1470-1477. IEEE, 2011.</p>
<p>Efficient and interpretable robot manipulation with graph neural networks. Y Lin, A S Wang, E Undersander, A Rai, IEEE Robotics and Automation Letters. Y. Lin, A. S. Wang, E. Undersander, and A. Rai. Efficient and interpretable robot manipulation with graph neural networks. IEEE Robotics and Automation Letters, 2022.</p>
<p>Optimization-based locomotion planning, estimation, and control design for the atlas humanoid robot. S Kuindersma, R Deits, M Fallon, A Valenzuela, H Dai, F Permenter, T Koolen, P Marion, R Tedrake, Autonomous robots. 403S. Kuindersma, R. Deits, M. Fallon, A. Valenzuela, H. Dai, F. Permenter, T. Koolen, P. Marion, and R. Tedrake. Optimization-based locomotion planning, estimation, and control design for the atlas humanoid robot. Autonomous robots, 40(3):429-455, 2016.</p>
<p>Using deep reinforcement learning to learn highlevel policies on the atrias biped. T Li, H Geyer, C G Atkeson, A Rai, ICRA. IEEET. Li, H. Geyer, C. G. Atkeson, and A. Rai. Using deep reinforcement learning to learn high- level policies on the atrias biped. In ICRA, pages 263-269. IEEE, 2019.</p>
<p>Transporter networks: Rearranging the visual world for robotic manipulation. A Zeng, P Florence, J Tompson, S Welker, J Chien, M Attarian, T Armstrong, I Krasin, D Duong, V Sindhwani, J Lee, Conference on Robot Learning (CoRL). 2020A. Zeng, P. Florence, J. Tompson, S. Welker, J. Chien, M. Attarian, T. Armstrong, I. Krasin, D. Duong, V. Sindhwani, and J. Lee. Transporter networks: Rearranging the visual world for robotic manipulation. Conference on Robot Learning (CoRL), 2020.</p>
<p>SORNet: Spatial object-centric representations for sequential manipulation. W Yuan, C Paxton, K Desingh, D Fox, 5th Annual Conference on Robot Learning. W. Yuan, C. Paxton, K. Desingh, and D. Fox. SORNet: Spatial object-centric representations for sequential manipulation. In 5th Annual Conference on Robot Learning, 2021. URL https: //openreview.net/forum?id=mOLu2rODIJF.</p>
<p>Unitree robotics. Unitree robotics. https://www.unitree.com/.</p>
<p>. Boston dynamics. Boston dynamics. https://www.bostondynamics.com/spot.</p>
<p>Gibson env: Real-world perception for embodied agents. F Xia, A R Zamir, Z He, A Sax, J Malik, S Savarese, CVPR. F. Xia, A. R. Zamir, Z. He, A. Sax, J. Malik, and S. Savarese. Gibson env: Real-world percep- tion for embodied agents. In CVPR, 2018.</p>
<p>Fast and efficient locomotion via learned gait transitions. Y Yang, T Zhang, E Coumans, J Tan, B Boots, Conference on Robot Learning. PMLRY. Yang, T. Zhang, E. Coumans, J. Tan, and B. Boots. Fast and efficient locomotion via learned gait transitions. In Conference on Robot Learning, pages 773-783. PMLR, 2022.</p>
<p>Model-based motion imitation for agile, diverse and generalizable quadupedal locomotion. T Li, J Won, S Ha, A Rai, arXiv:2109.13362arXiv preprintT. Li, J. Won, S. Ha, and A. Rai. Model-based motion imitation for agile, diverse and general- izable quadupedal locomotion. arXiv preprint arXiv:2109.13362, 2021.</p>
<p>Learning agile and dynamic motor skills for legged robots. J Hwangbo, J Lee, A Dosovitskiy, D Bellicoso, V Tsounis, V Koltun, M Hutter, arXiv:1901.08652arXiv preprintReferencesJ. Hwangbo, J. Lee, A. Dosovitskiy, D. Bellicoso, V. Tsounis, V. Koltun, and M. Hutter. Learn- ing agile and dynamic motor skills for legged robots. arXiv preprint arXiv:1901.08652, 2019. References</p>
<p>Learning navigation skills for legged robots with learned robot embeddings. J Truong, D Yarats, T Li, F Meier, S Chernova, D Batra, A Rai, International Conference on Intelligent Robots and Systems (IROS). 2020J. Truong, D. Yarats, T. Li, F. Meier, S. Chernova, D. Batra, and A. Rai. Learning naviga- tion skills for legged robots with learned robot embeddings. In International Conference on Intelligent Robots and Systems (IROS), 2020.</p>
<p>A Murali, T Chen, K V Alwala, D Gandhi, L Pinto, S Gupta, A Gupta, arXiv:1906.08236Pyrobot: An open-source robotics framework for research and benchmarking. arXiv preprintA. Murali, T. Chen, K. V. Alwala, D. Gandhi, L. Pinto, S. Gupta, and A. Gupta. Py- robot: An open-source robotics framework for research and benchmarking. arXiv preprint arXiv:1906.08236, 2019.</p>
<p>Learning agile robotic locomotion skills by imitating animals. X B Peng, RSSE Coumans, RSST Zhang, RSST.-W Lee, RSSJ Tan, RSSS Levine, RSSRobotics: Science and Systems. 2020X. B. Peng, E. Coumans, T. Zhang, T.-W. Lee, J. Tan, and S. Levine. Learning agile robotic locomotion skills by imitating animals. Robotics: Science and Systems (RSS), 2020.</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-807 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-807</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-807</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-21.html">extraction-schema-21</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <p><strong>Paper ID:</strong> paper-252185365</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2209.04105v2.pdf" target="_blank">An Analysis of Deep Reinforcement Learning Agents for Text-based Games</a></p>
                <p><strong>Paper Abstract:</strong> Text-based games(TBG) are complex environments which allow users or computer agents to make textual interactions and achieve game goals.In TBG agent design and training process, balancing the efficiency and performance of the agent models is a major challenge. Finding TBG agent deep learning modules' performance in standardized environments, and testing their performance among different evaluation types is also important for TBG agent research. We constructed a standardized TBG agent with no hand-crafted rules, formally categorized TBG evaluation types, and analyzed selected methods in our environment.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e807.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e807.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dynamic Belief Graph agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Learning dynamic belief graphs to generalize on text-based games (Adhikari et al., 2020) agent</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Agent that constructs and maintains a dynamic knowledge/ belief graph from textual observations and uses that structured belief as the agent's state representation to improve generalization across text-based games; includes pretrained graph-update objectives (Observation Generation and Contrastive Observation Classification).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning dynamic belief graphs to generalize on text-based games</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Dynamic Belief Graph agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Uses a neural knowledge-graph (belief graph) as the central state representation; a learned graph-updater module consumes textual observations and updates nodes/relations in the graph. The graph is encoded (e.g. by a GCN-style encoder) and fed to an RL policy that learns actions conditioned on the graph-encoded belief. The paper also describes pretraining objectives for the graph-updater: Observation Generation (OG) and Contrastive Observation Classification (COC).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Text-based games (TextWorld / text-adventure benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Partially observable text-adventure environments in which the agent receives textual observations describing the current room, objects and effects of actions; full underlying state (map, object states) is not directly observable.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>Knowledge/ belief graph (constructed from observations); pretrained graph-updater modules trained with Observation Generation (OG) and Contrastive Observation Classification (COC).</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Structured knowledge graph (nodes and relations), reconstructed text observations (OG decoder outputs), and classification scores (COC outputs).</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>Explicit dynamic knowledge/belief graph maintained and updated by a learned neural graph-updater module; the graph encodes entities, relations and inferred facts from text observations.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>A learned graph-updater consumes each textual observation and either (a) reconstructs the observation (OG) to learn to map observation→graph updates, or (b) discriminates true vs. fake observations (COC) to shape updater representations; outputs modify nodes/edges in the belief graph which is then encoded for policy.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Learned policy (graph-encoded state) with a world-model style belief graph; described as improving generalization rather than explicit search-based planning.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Maintaining an explicit dynamic belief graph and pretraining the graph-updater (OG and COC) is proposed to help agents generalize across games by providing a structured, up-to-date belief state extracted from partial textual observations.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e807.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e807.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KG-based World Model agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Playing text-adventure games with graph-based deep reinforcement learning (Ammanabrolu & Riedl) / Learning knowledge graph-based world models of textual environments</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Agents that build and use knowledge graphs/world models extracted from text observations to represent partially observable environments; graphs are encoded (e.g., with GCNs) and used by downstream RL policies or action-selection modules.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Playing text-adventure games with graph-based deep reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Knowledge-Graph-based world-model agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Constructs a knowledge graph from textual observations using information-extraction (sometimes supported by pretrained QA encoders such as DrQA or ALBERT) and encodes the graph with a graph encoder (e.g., GCN). The encoded graph serves as the agent's state representation and is used by a reinforcement-learning policy to select natural-language actions; some works also pretrain QA/observation modules (Jericho-QA / ALBERT) to improve graph extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Text-based games (TextWorld, Jericho / interactive fiction)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Partially observable textual game worlds where the agent receives local textual descriptions (rooms, objects, events) but must infer unseen state; graphs provide a structured memory of discovered facts and objects.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>Pretrained QA encoders (e.g., DrQA), ALBERT pretraining on Jericho-QA dataset, knowledge graph construction/updater modules, graph encoders (GCN).</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Structured knowledge graph (entities/relations), encoder embeddings/features, QA outputs used to populate the graph.</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>Knowledge graph / world model that records discovered entities, relations and object states; the graph is the explicit belief representation used by the agent.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>Pretrained encoders extract facts from textual observations and add/update nodes and edges in the knowledge graph; the updated graph is encoded and passed to the policy (details of update mechanics are in the cited works, not expanded here).</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Graph-based deep RL — a learned policy that conditions on a graph-encoded belief; the graph can also be used to constrain or guide action selection.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td>Graph-based state representation (used to reason about connectivity/objects); specific path-planning algorithms are not described in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Knowledge-graph world models provide a structured belief that can be encoded for policy learning and can be sped up or improved using pretrained QA/encoder modules (e.g., DrQA, ALBERT trained on Jericho-QA); however, some KG extraction methods rely on hand-crafted rules.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e807.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e807.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Graph-Constrained RL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph constrained reinforcement learning for natural language action spaces (Ammanabrolu & Hausknecht, 2019)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Method that leverages a knowledge graph to constrain and reduce the natural-language action space, using graph-derived information to mask or score candidate actions and make RL tractable in large action spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Graph constrained reinforcement learning for natural language action spaces</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Graph-constrained RL agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agent uses a knowledge graph extracted from observations to generate constraints or masks over the large natural-language action set, reducing candidate actions and informing the RL policy's action selection; graph features are encoded and integrated with the policy network.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Text-based games / text-adventure environments (TextWorld etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Partially observable textual environments with large natural-language action spaces; constraining actions based on graph-derived knowledge addresses combinatorial action selection.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>Knowledge graph / graph-updater module used to generate action constraints or masks.</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Structured graph (entities/relations) and action masks / action scores derived from graph state.</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>Knowledge graph representing discovered facts; used both as belief and to prune action space.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>Graph is updated from textual observations and agent experience (survey notes that text is turned into knowledge graphs), and the resulting graph is used to produce action constraints; precise update mechanisms are in the cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Learned policy with graph-based constraints (not explicit search); action selection guided by graph-derived masks/scores.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Applying graph-derived constraints reduces the effective action space and helps RL agents operate in large natural-language action domains; structured graphs are useful both as belief and as an action-pruning tool.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e807.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e807.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Commonsense-KG agents</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Agents leveraging commonsense knowledge (Murugesan et al., 2020/2021)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Agents that augment environment-derived state graphs with external commonsense knowledge graphs to form richer state representations that can improve learning efficiency in text-based games.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Enhancing text-based reinforcement learning agents with commonsense knowledge</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Commonsense-augmented RL agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Joins the environment state graph with external commonsense graph information (external KBs) and encodes the combined graph for policy learning; aims to leverage external knowledge to infer implicit relations and reduce sample complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Text-based games / TextWorld</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Partially observable text environments where external commonsense can fill implicit knowledge gaps (e.g., affordances, typical object relations) to aid decision-making.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>External commonsense knowledge graphs (unspecified in survey summary) and graph-encoding modules.</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Structured graph data (commonsense triples), combined graph embeddings/features.</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>A combined state+commonsense graph representation that functions as the agent's belief state.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>Survey notes that state and commonsense graphs are jointly leveraged; explicit update mechanics are not detailed in this survey text.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Learned policy conditioned on graph-augmented state representation.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Incorporating external commonsense graphs into the state representation is proposed to improve efficiency and effectiveness, although the survey does not report numerical comparisons here.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e807.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e807.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Ledeepchef (Recipe Manager)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ledeepchef deep reinforcement learning agent for families of text-based games (Adolphs & Hofmann, 2020)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A domain-specialized agent for cooking-textworld games that incorporates a pretrained Recipe Manager module which predicts remaining subgoals/items needed to complete the recipe, aiding task decomposition and planning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Ledeepchef deep reinforcement learning agent for families of text-based games</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Ledeepchef (with Recipe Manager)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Deep RL agent augmented with a pretrained, task-specific Recipe Manager that predicts what subgoals or items remain to complete the current recipe; Recipe Manager outputs are used to guide the agent's action choices in the cooking domain.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>TextWorld cooking recipe games</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Partially observable cooking games in TextWorld where the agent must collect items and perform cooking actions across rooms; requires tracking multiple subgoals and object states.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>Pretrained Recipe Manager module (task-specific external module).</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Predictions of remaining subgoals/items (task-progress signals, structured labels or short text outputs).</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>Task-progress predictions from the Recipe Manager serve as a form of belief about which items/subtasks remain; this augments the agent's internal state.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>Recipe Manager predicts what's left to complete tasks; agent uses these predictions to prioritize actions and track task completion (survey does not provide algorithmic update details).</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Subgoal-guided RL; learned policy augmented by pretrained task-specific subgoal predictions rather than general search-based planning.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Task-specific pretrained modules (like a Recipe Manager) can help agents decompose and solve narrow-domain text games, but such modules are limited in applicability to other domains.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e807.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e807.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MC planning + language value estimates</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Monte-carlo planning and learning with language action value estimates (Jang et al., 2020)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Approach that combines Monte-Carlo planning/rollouts with learned language action-value estimators to perform search in large natural-language action spaces in text games.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Monte-carlo planning and learning with language action value estimates</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Monte-Carlo planning agent with language action-value estimators</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Performs Monte-Carlo planning (internal rollouts/search) where rollouts are guided or scored by a learned language action-value estimator (a neural model that predicts values for natural-language actions); blends planning with learned value estimation in text-based domains.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Text-based games / text-adventure benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Partially observable text environments with very large action spaces where rollouts and planning can help evaluate long-horizon consequences of language actions.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>Monte-Carlo planning/search algorithm (planner) and a learned action-value estimator (neural model).</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Numerical action-value estimates, rollout evaluation scores, candidate action rankings.</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Monte-Carlo planning / rollout-based search guided by learned action-value estimates (hybrid planning + learned policy/value).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Combining Monte-Carlo planning with learned language action-value estimates is proposed as a way to search effectively in large natural-language action spaces, providing a planning mechanism in partially observable text domains.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e807.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e807.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Map-familiarization agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Agents using map familiarization, curriculum learning, and bandit feedback (Yin & May, 2019)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Agents that explicitly learn and store map/topological information (map familiarization) to accelerate navigation and transfer learning across families of text-based games, often combined with curriculum learning and bandit feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learn how to cook a new recipe in a new house: Using map familiarization, curriculum learning, and bandit feedback to learn families of text-based adventure games</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Map-familiarization agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Learns a map-like memory of the environment (rooms and connectivity) during exploration (map familiarization) and uses curriculum learning plus bandit feedback to transfer navigation and task skills across different house/game instances; the learned map functions as an internal navigation aid/belief.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>TextWorld families of games (e.g., cooking in different houses)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Partially observable multi-room environments where navigation and repeated visitation matter; map structure varies across instances so agents must generalize map knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>Internal map memory / map-building component; curriculum learning and bandit-feedback mechanisms (training tools).</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Map/topological representations (room graph), familiarity signals, exploration prioritization metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>An internal map memory that records rooms and connections (topological belief) used to guide future navigation decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>Agent explores to build/update the internal map; newly observed room descriptions and discovered connections update the map representation (survey references map familiarization but does not detail exact update rules).</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Map-based navigation and learned policies augmented by curriculum and bandit feedback to prioritize exploration; not described as explicit shortest-path search in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td>Map familiarization / learned map used to support navigation and transfer; exact path-planning algorithm is not specified in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Map familiarization coupled with curriculum learning and bandit feedback is proposed to improve navigation and transfer across similar text-based games by giving agents an explicit spatial/topological belief.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Learning dynamic belief graphs to generalize on text-based games <em>(Rating: 2)</em></li>
                <li>Playing text-adventure games with graph-based deep reinforcement learning <em>(Rating: 2)</em></li>
                <li>Learning knowledge graph-based world models of textual environments <em>(Rating: 2)</em></li>
                <li>Graph constrained reinforcement learning for natural language action spaces <em>(Rating: 2)</em></li>
                <li>Enhancing text-based reinforcement learning agents with commonsense knowledge <em>(Rating: 2)</em></li>
                <li>Efficient text-based reinforcement learning by jointly leveraging state and commonsense graph representations <em>(Rating: 2)</em></li>
                <li>Ledeepchef deep reinforcement learning agent for families of text-based games <em>(Rating: 2)</em></li>
                <li>Monte-carlo planning and learning with language action value estimates <em>(Rating: 2)</em></li>
                <li>Learn how to cook a new recipe in a new house: Using map familiarization, curriculum learning, and bandit feedback to learn families of text-based adventure games <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-807",
    "paper_id": "paper-252185365",
    "extraction_schema_id": "extraction-schema-21",
    "extracted_data": [
        {
            "name_short": "Dynamic Belief Graph agent",
            "name_full": "Learning dynamic belief graphs to generalize on text-based games (Adhikari et al., 2020) agent",
            "brief_description": "Agent that constructs and maintains a dynamic knowledge/ belief graph from textual observations and uses that structured belief as the agent's state representation to improve generalization across text-based games; includes pretrained graph-update objectives (Observation Generation and Contrastive Observation Classification).",
            "citation_title": "Learning dynamic belief graphs to generalize on text-based games",
            "mention_or_use": "mention",
            "agent_name": "Dynamic Belief Graph agent",
            "agent_description": "Uses a neural knowledge-graph (belief graph) as the central state representation; a learned graph-updater module consumes textual observations and updates nodes/relations in the graph. The graph is encoded (e.g. by a GCN-style encoder) and fed to an RL policy that learns actions conditioned on the graph-encoded belief. The paper also describes pretraining objectives for the graph-updater: Observation Generation (OG) and Contrastive Observation Classification (COC).",
            "environment_name": "Text-based games (TextWorld / text-adventure benchmarks)",
            "environment_description": "Partially observable text-adventure environments in which the agent receives textual observations describing the current room, objects and effects of actions; full underlying state (map, object states) is not directly observable.",
            "is_partially_observable": true,
            "external_tools_used": "Knowledge/ belief graph (constructed from observations); pretrained graph-updater modules trained with Observation Generation (OG) and Contrastive Observation Classification (COC).",
            "tool_output_types": "Structured knowledge graph (nodes and relations), reconstructed text observations (OG decoder outputs), and classification scores (COC outputs).",
            "belief_state_mechanism": "Explicit dynamic knowledge/belief graph maintained and updated by a learned neural graph-updater module; the graph encodes entities, relations and inferred facts from text observations.",
            "incorporates_tool_outputs_in_belief": true,
            "belief_update_description": "A learned graph-updater consumes each textual observation and either (a) reconstructs the observation (OG) to learn to map observation→graph updates, or (b) discriminates true vs. fake observations (COC) to shape updater representations; outputs modify nodes/edges in the belief graph which is then encoded for policy.",
            "planning_approach": "Learned policy (graph-encoded state) with a world-model style belief graph; described as improving generalization rather than explicit search-based planning.",
            "uses_shortest_path_planning": null,
            "navigation_method": null,
            "performance_with_tools": null,
            "performance_without_tools": null,
            "has_tool_ablation": null,
            "key_findings": "Maintaining an explicit dynamic belief graph and pretraining the graph-updater (OG and COC) is proposed to help agents generalize across games by providing a structured, up-to-date belief state extracted from partial textual observations.",
            "uuid": "e807.0"
        },
        {
            "name_short": "KG-based World Model agent",
            "name_full": "Playing text-adventure games with graph-based deep reinforcement learning (Ammanabrolu & Riedl) / Learning knowledge graph-based world models of textual environments",
            "brief_description": "Agents that build and use knowledge graphs/world models extracted from text observations to represent partially observable environments; graphs are encoded (e.g., with GCNs) and used by downstream RL policies or action-selection modules.",
            "citation_title": "Playing text-adventure games with graph-based deep reinforcement learning",
            "mention_or_use": "mention",
            "agent_name": "Knowledge-Graph-based world-model agent",
            "agent_description": "Constructs a knowledge graph from textual observations using information-extraction (sometimes supported by pretrained QA encoders such as DrQA or ALBERT) and encodes the graph with a graph encoder (e.g., GCN). The encoded graph serves as the agent's state representation and is used by a reinforcement-learning policy to select natural-language actions; some works also pretrain QA/observation modules (Jericho-QA / ALBERT) to improve graph extraction.",
            "environment_name": "Text-based games (TextWorld, Jericho / interactive fiction)",
            "environment_description": "Partially observable textual game worlds where the agent receives local textual descriptions (rooms, objects, events) but must infer unseen state; graphs provide a structured memory of discovered facts and objects.",
            "is_partially_observable": true,
            "external_tools_used": "Pretrained QA encoders (e.g., DrQA), ALBERT pretraining on Jericho-QA dataset, knowledge graph construction/updater modules, graph encoders (GCN).",
            "tool_output_types": "Structured knowledge graph (entities/relations), encoder embeddings/features, QA outputs used to populate the graph.",
            "belief_state_mechanism": "Knowledge graph / world model that records discovered entities, relations and object states; the graph is the explicit belief representation used by the agent.",
            "incorporates_tool_outputs_in_belief": true,
            "belief_update_description": "Pretrained encoders extract facts from textual observations and add/update nodes and edges in the knowledge graph; the updated graph is encoded and passed to the policy (details of update mechanics are in the cited works, not expanded here).",
            "planning_approach": "Graph-based deep RL — a learned policy that conditions on a graph-encoded belief; the graph can also be used to constrain or guide action selection.",
            "uses_shortest_path_planning": null,
            "navigation_method": "Graph-based state representation (used to reason about connectivity/objects); specific path-planning algorithms are not described in this survey.",
            "performance_with_tools": null,
            "performance_without_tools": null,
            "has_tool_ablation": null,
            "key_findings": "Knowledge-graph world models provide a structured belief that can be encoded for policy learning and can be sped up or improved using pretrained QA/encoder modules (e.g., DrQA, ALBERT trained on Jericho-QA); however, some KG extraction methods rely on hand-crafted rules.",
            "uuid": "e807.1"
        },
        {
            "name_short": "Graph-Constrained RL",
            "name_full": "Graph constrained reinforcement learning for natural language action spaces (Ammanabrolu & Hausknecht, 2019)",
            "brief_description": "Method that leverages a knowledge graph to constrain and reduce the natural-language action space, using graph-derived information to mask or score candidate actions and make RL tractable in large action spaces.",
            "citation_title": "Graph constrained reinforcement learning for natural language action spaces",
            "mention_or_use": "mention",
            "agent_name": "Graph-constrained RL agent",
            "agent_description": "Agent uses a knowledge graph extracted from observations to generate constraints or masks over the large natural-language action set, reducing candidate actions and informing the RL policy's action selection; graph features are encoded and integrated with the policy network.",
            "environment_name": "Text-based games / text-adventure environments (TextWorld etc.)",
            "environment_description": "Partially observable textual environments with large natural-language action spaces; constraining actions based on graph-derived knowledge addresses combinatorial action selection.",
            "is_partially_observable": true,
            "external_tools_used": "Knowledge graph / graph-updater module used to generate action constraints or masks.",
            "tool_output_types": "Structured graph (entities/relations) and action masks / action scores derived from graph state.",
            "belief_state_mechanism": "Knowledge graph representing discovered facts; used both as belief and to prune action space.",
            "incorporates_tool_outputs_in_belief": true,
            "belief_update_description": "Graph is updated from textual observations and agent experience (survey notes that text is turned into knowledge graphs), and the resulting graph is used to produce action constraints; precise update mechanisms are in the cited work.",
            "planning_approach": "Learned policy with graph-based constraints (not explicit search); action selection guided by graph-derived masks/scores.",
            "uses_shortest_path_planning": null,
            "navigation_method": null,
            "performance_with_tools": null,
            "performance_without_tools": null,
            "has_tool_ablation": null,
            "key_findings": "Applying graph-derived constraints reduces the effective action space and helps RL agents operate in large natural-language action domains; structured graphs are useful both as belief and as an action-pruning tool.",
            "uuid": "e807.2"
        },
        {
            "name_short": "Commonsense-KG agents",
            "name_full": "Agents leveraging commonsense knowledge (Murugesan et al., 2020/2021)",
            "brief_description": "Agents that augment environment-derived state graphs with external commonsense knowledge graphs to form richer state representations that can improve learning efficiency in text-based games.",
            "citation_title": "Enhancing text-based reinforcement learning agents with commonsense knowledge",
            "mention_or_use": "mention",
            "agent_name": "Commonsense-augmented RL agent",
            "agent_description": "Joins the environment state graph with external commonsense graph information (external KBs) and encodes the combined graph for policy learning; aims to leverage external knowledge to infer implicit relations and reduce sample complexity.",
            "environment_name": "Text-based games / TextWorld",
            "environment_description": "Partially observable text environments where external commonsense can fill implicit knowledge gaps (e.g., affordances, typical object relations) to aid decision-making.",
            "is_partially_observable": true,
            "external_tools_used": "External commonsense knowledge graphs (unspecified in survey summary) and graph-encoding modules.",
            "tool_output_types": "Structured graph data (commonsense triples), combined graph embeddings/features.",
            "belief_state_mechanism": "A combined state+commonsense graph representation that functions as the agent's belief state.",
            "incorporates_tool_outputs_in_belief": true,
            "belief_update_description": "Survey notes that state and commonsense graphs are jointly leveraged; explicit update mechanics are not detailed in this survey text.",
            "planning_approach": "Learned policy conditioned on graph-augmented state representation.",
            "uses_shortest_path_planning": null,
            "navigation_method": null,
            "performance_with_tools": null,
            "performance_without_tools": null,
            "has_tool_ablation": null,
            "key_findings": "Incorporating external commonsense graphs into the state representation is proposed to improve efficiency and effectiveness, although the survey does not report numerical comparisons here.",
            "uuid": "e807.3"
        },
        {
            "name_short": "Ledeepchef (Recipe Manager)",
            "name_full": "Ledeepchef deep reinforcement learning agent for families of text-based games (Adolphs & Hofmann, 2020)",
            "brief_description": "A domain-specialized agent for cooking-textworld games that incorporates a pretrained Recipe Manager module which predicts remaining subgoals/items needed to complete the recipe, aiding task decomposition and planning.",
            "citation_title": "Ledeepchef deep reinforcement learning agent for families of text-based games",
            "mention_or_use": "mention",
            "agent_name": "Ledeepchef (with Recipe Manager)",
            "agent_description": "Deep RL agent augmented with a pretrained, task-specific Recipe Manager that predicts what subgoals or items remain to complete the current recipe; Recipe Manager outputs are used to guide the agent's action choices in the cooking domain.",
            "environment_name": "TextWorld cooking recipe games",
            "environment_description": "Partially observable cooking games in TextWorld where the agent must collect items and perform cooking actions across rooms; requires tracking multiple subgoals and object states.",
            "is_partially_observable": true,
            "external_tools_used": "Pretrained Recipe Manager module (task-specific external module).",
            "tool_output_types": "Predictions of remaining subgoals/items (task-progress signals, structured labels or short text outputs).",
            "belief_state_mechanism": "Task-progress predictions from the Recipe Manager serve as a form of belief about which items/subtasks remain; this augments the agent's internal state.",
            "incorporates_tool_outputs_in_belief": true,
            "belief_update_description": "Recipe Manager predicts what's left to complete tasks; agent uses these predictions to prioritize actions and track task completion (survey does not provide algorithmic update details).",
            "planning_approach": "Subgoal-guided RL; learned policy augmented by pretrained task-specific subgoal predictions rather than general search-based planning.",
            "uses_shortest_path_planning": null,
            "navigation_method": null,
            "performance_with_tools": null,
            "performance_without_tools": null,
            "has_tool_ablation": null,
            "key_findings": "Task-specific pretrained modules (like a Recipe Manager) can help agents decompose and solve narrow-domain text games, but such modules are limited in applicability to other domains.",
            "uuid": "e807.4"
        },
        {
            "name_short": "MC planning + language value estimates",
            "name_full": "Monte-carlo planning and learning with language action value estimates (Jang et al., 2020)",
            "brief_description": "Approach that combines Monte-Carlo planning/rollouts with learned language action-value estimators to perform search in large natural-language action spaces in text games.",
            "citation_title": "Monte-carlo planning and learning with language action value estimates",
            "mention_or_use": "mention",
            "agent_name": "Monte-Carlo planning agent with language action-value estimators",
            "agent_description": "Performs Monte-Carlo planning (internal rollouts/search) where rollouts are guided or scored by a learned language action-value estimator (a neural model that predicts values for natural-language actions); blends planning with learned value estimation in text-based domains.",
            "environment_name": "Text-based games / text-adventure benchmarks",
            "environment_description": "Partially observable text environments with very large action spaces where rollouts and planning can help evaluate long-horizon consequences of language actions.",
            "is_partially_observable": true,
            "external_tools_used": "Monte-Carlo planning/search algorithm (planner) and a learned action-value estimator (neural model).",
            "tool_output_types": "Numerical action-value estimates, rollout evaluation scores, candidate action rankings.",
            "belief_state_mechanism": null,
            "incorporates_tool_outputs_in_belief": null,
            "belief_update_description": null,
            "planning_approach": "Monte-Carlo planning / rollout-based search guided by learned action-value estimates (hybrid planning + learned policy/value).",
            "uses_shortest_path_planning": false,
            "navigation_method": null,
            "performance_with_tools": null,
            "performance_without_tools": null,
            "has_tool_ablation": null,
            "key_findings": "Combining Monte-Carlo planning with learned language action-value estimates is proposed as a way to search effectively in large natural-language action spaces, providing a planning mechanism in partially observable text domains.",
            "uuid": "e807.5"
        },
        {
            "name_short": "Map-familiarization agent",
            "name_full": "Agents using map familiarization, curriculum learning, and bandit feedback (Yin & May, 2019)",
            "brief_description": "Agents that explicitly learn and store map/topological information (map familiarization) to accelerate navigation and transfer learning across families of text-based games, often combined with curriculum learning and bandit feedback.",
            "citation_title": "Learn how to cook a new recipe in a new house: Using map familiarization, curriculum learning, and bandit feedback to learn families of text-based adventure games",
            "mention_or_use": "mention",
            "agent_name": "Map-familiarization agent",
            "agent_description": "Learns a map-like memory of the environment (rooms and connectivity) during exploration (map familiarization) and uses curriculum learning plus bandit feedback to transfer navigation and task skills across different house/game instances; the learned map functions as an internal navigation aid/belief.",
            "environment_name": "TextWorld families of games (e.g., cooking in different houses)",
            "environment_description": "Partially observable multi-room environments where navigation and repeated visitation matter; map structure varies across instances so agents must generalize map knowledge.",
            "is_partially_observable": true,
            "external_tools_used": "Internal map memory / map-building component; curriculum learning and bandit-feedback mechanisms (training tools).",
            "tool_output_types": "Map/topological representations (room graph), familiarity signals, exploration prioritization metrics.",
            "belief_state_mechanism": "An internal map memory that records rooms and connections (topological belief) used to guide future navigation decisions.",
            "incorporates_tool_outputs_in_belief": true,
            "belief_update_description": "Agent explores to build/update the internal map; newly observed room descriptions and discovered connections update the map representation (survey references map familiarization but does not detail exact update rules).",
            "planning_approach": "Map-based navigation and learned policies augmented by curriculum and bandit feedback to prioritize exploration; not described as explicit shortest-path search in the survey.",
            "uses_shortest_path_planning": null,
            "navigation_method": "Map familiarization / learned map used to support navigation and transfer; exact path-planning algorithm is not specified in this survey.",
            "performance_with_tools": null,
            "performance_without_tools": null,
            "has_tool_ablation": null,
            "key_findings": "Map familiarization coupled with curriculum learning and bandit feedback is proposed to improve navigation and transfer across similar text-based games by giving agents an explicit spatial/topological belief.",
            "uuid": "e807.6"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Learning dynamic belief graphs to generalize on text-based games",
            "rating": 2,
            "sanitized_title": "learning_dynamic_belief_graphs_to_generalize_on_textbased_games"
        },
        {
            "paper_title": "Playing text-adventure games with graph-based deep reinforcement learning",
            "rating": 2,
            "sanitized_title": "playing_textadventure_games_with_graphbased_deep_reinforcement_learning"
        },
        {
            "paper_title": "Learning knowledge graph-based world models of textual environments",
            "rating": 2,
            "sanitized_title": "learning_knowledge_graphbased_world_models_of_textual_environments"
        },
        {
            "paper_title": "Graph constrained reinforcement learning for natural language action spaces",
            "rating": 2,
            "sanitized_title": "graph_constrained_reinforcement_learning_for_natural_language_action_spaces"
        },
        {
            "paper_title": "Enhancing text-based reinforcement learning agents with commonsense knowledge",
            "rating": 2,
            "sanitized_title": "enhancing_textbased_reinforcement_learning_agents_with_commonsense_knowledge"
        },
        {
            "paper_title": "Efficient text-based reinforcement learning by jointly leveraging state and commonsense graph representations",
            "rating": 2,
            "sanitized_title": "efficient_textbased_reinforcement_learning_by_jointly_leveraging_state_and_commonsense_graph_representations"
        },
        {
            "paper_title": "Ledeepchef deep reinforcement learning agent for families of text-based games",
            "rating": 2,
            "sanitized_title": "ledeepchef_deep_reinforcement_learning_agent_for_families_of_textbased_games"
        },
        {
            "paper_title": "Monte-carlo planning and learning with language action value estimates",
            "rating": 2,
            "sanitized_title": "montecarlo_planning_and_learning_with_language_action_value_estimates"
        },
        {
            "paper_title": "Learn how to cook a new recipe in a new house: Using map familiarization, curriculum learning, and bandit feedback to learn families of text-based adventure games",
            "rating": 2,
            "sanitized_title": "learn_how_to_cook_a_new_recipe_in_a_new_house_using_map_familiarization_curriculum_learning_and_bandit_feedback_to_learn_families_of_textbased_adventure_games"
        }
    ],
    "cost": 0.02194725,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>An Analysis of Deep Reinforcement Learning Agents for Text-based Games
1993</p>
<p>Chen Chen 
The University of Sydney Camperdown
2006NSWAustralia</p>
<p>Yue Dai 
The University of Sydney Camperdown
2006NSWAustralia</p>
<p>Josiah Poon josiah.poon@sydney.edu.au 
The University of Sydney Camperdown
2006NSWAustralia</p>
<p>Caren Han caren.han@sydney.edu.au 
The University of Sydney Camperdown
2006NSWAustralia</p>
<p>An Analysis of Deep Reinforcement Learning Agents for Text-based Games</p>
<p>Journal of Artificial Intelligence Research
11993Submitted 6/91; published 9/91
Text-based games(TBG) are complex environments which allow users or computer agents to make textual interactions and achieve game goals.In TBG agent design and training process, balancing the efficiency and performance of the agent models is a major challenge. Finding TBG agent deep learning modules' performance in standardized environments, and testing their performance among different evaluation types is also important for TBG agent research. We constructed a standardized TBG agent with no hand-crafted rules, formally categorized TBG evaluation types, and analyzed selected methods in our environment.</p>
<p>Introduction</p>
<p>Text-based games are designed for humans to play and finish quests using text replies. During one episode of a text-based game, when a player takes one action, the game environment provides some text descriptions about the current game state, typically about a place or some effects caused by the player's last action. Based on the text description(feedback), the player makes the next move, keeps collecting information, and tries to reach a winning state with the least steps possible.</p>
<p>Research on Text-based games mainly focuses on building goal-oriented smart agents. The methods in Text-based games could be applied to interactive language-related applications, such as the Internet of Things (IoT), smart assistants, online technical support/chatbots, and so on. The latest methods in this area include Reinforcement Learning and Natural Language Understanding modules. This combination enables smart agents to be trained through live language interactions and learn to understand the environment, improve themselves to perform actions and achieve goals.</p>
<p>The language models are typically trained offline in many traditional natural language tasks. However, in wide-ranging applications, like the smart agents in home pods, we expect the smart agent to adapt to different users in day-to-day interactions and figure out actions that need to take based on specific user language features and habits without hard-coded settings and improve user satisfaction. Text-based Games research is performed in a similar environment, where the agent explores the text environment, takes actions, collects results, and finally finds an optimal mapping of language input and actions to take without manually labelling the input data and training a model offline. Some early papers in this field use hand-crafted rules as the core of the agents. As we expect Text-based game agents capable of online learning and improvement, this paper focuses only on Deep Reinforcement Learning agents. We also emphasize the deep learning module capability and efficiency analysis in this field, which is the critical component of agent online learning performance. The development path of this field is described in table 2. At the early years, the models were established on single-setting games, later with the development of Text-based Game experiment platforms. The agents are trained on multiple games with different settings. More methods that have been successfully applied to the Natural Language Processing field are introduced into Text-based game agents, including graphs and external knowledge.</p>
<p>The most regularly used platform in this field is Textworld. Due to the complexity of Text-based game agents, even trained on the same platform, hand-crafted rules/language features and other component differences make it hard to compare the effectiveness of deep learning module performance alone. To offer a clearer view of method effectiveness among benchmarked works, We collected some deep learning modules and methods from related research works, benchmarked them under the same environments, and experimented with the same hyperparameters in different game scenarios.</p>
<p>Some new approaches to the Text-based game task have shown outstanding results, like graph-aided techniques. Rather than learning games in one set, or using as many features as possible, new challenges in this field includes using less hand-crafted features and training the agent to solve unseen games.</p>
<p>The motivation for this survey is to examine literature in this field after 2015, analyze it in depth and benchmark some methods in a standardized environment. We summarize approaches introduced so far, present our benchmark results and analysis, and point out some future directions. In this paper, we consider three questions:</p>
<p>• Q1: How do different agent models balance efficiency and performance?</p>
<p>• Q2: How do some modules perform on an agent without hand-crafting rules and features in standardized environments?</p>
<p>• Q3: Can models perform consistently in different evaluation types?</p>
<p>Related analysis and surveys</p>
<p>Most of the Text-based Games agent field methods are evaluated in separate research works. Though each research work selects benchmarks to present the significance of the findings, the evaluations are conducted with different games and settings. They might incorporate different levels of hand-crafted rules, with additional modules on top of the Deep Neural Network structure. Several surveys in this field collected methods and results within the research field. Inspired by these works, we decided to focus our analysis on deep learning modules used in TBG agents, review the related methods more in-depth, test selected models in a standardized evaluation setting and present our results and analysis.</p>
<p>Structure of the analysis</p>
<p>This analysis begins with an overview of the literature and related methodology. We introduce the methodology development trend in this field, review and classify the methods, and explain how they are applied. We review the evaluation platforms and metrics used in related research works and then introduce our ablation test on selected methods within a standardized environment. Finally, we summarize our analysis of the methods and findings and address the issues in this field for future direction.</p>
<p>Overview of the literature</p>
<p>Text-based game agents are designed to explore the game environment, extract useful information from text features provided, and learn optimal policy to give correct text actions. Efforts have been made to improve text encoding components, processing text features, and efficiently producing text actions. Issues are learning global policy, accurately constructing game state representations, and reducing state space.</p>
<p>Text-based game is generally framed as a reinforcement learning problem. In early years, many hand-crafted methods are introduced to try to solve games by analyzing text patterns and crafting templates. Deep Reinforcement learning is later introduced into this field. Test environments like Textworld are introduced to enable learning from a batch of Table1 summarizes major methodologies introduced into this field, ranked by timeline. The two earliest papers (Narasimhan, Kulkarni, &amp; Barzilay, 2015;He, Chen, He, Gao, Li, Deng, &amp; Ostendorf, 2015) (2015, table 1) introducing deep reinforcement learning into textbased games are inspired by NLP methods. Popularly methods like word embeddings and MLP are introduced for agents to process and encode text descriptions. A word embedding sequence represents the game state, and vanilla DQN is used as RL algorithm. With the two earliest works, more variations of neural network architecture, such as RNN and Transformer are introduced. The actor-critic algorithm was introduced as an alternative Reinforcement Learning framework to DQN. Various graph methods are also introduced as a new type of state representation. More pre-trained modules are included in text-based game agents for enhancement.</p>
<p>Text-based game platforms are also under continuous development. The earliest works only focus on one particular game. Later platforms allow the evaluation to be done on different games. On top of different games, Textworld (Côté, Kádár, Yuan, Kybartas, Barnes, Fine, Moore, Hausknecht, El Asri, Adada, et al., 2018)(2018,table1) offers a large batch generation of similar games, which allow agents to be trained on a large training set to allow testing on unseen games.</p>
<p>The latest development of text-based game agent research is more focused on a graph representation of game state(2019,table1), and several graph generation methods are introduced. Recently, researchers also focus on enhancing agent language understanding capability by including pre-trained language processing components into agents' neural network architecture.</p>
<p>Reinforcement Learning Methods</p>
<p>As text-based game agent training requires interaction with the game environment, most related research works are built under the Reinforcement Learning(RL) framework. Various RL methods have been applied, including game states and action interpretation, RL algorithms and reward functions. In this section, we introduce basic concepts of major methodologies and classify related works in an ontology ( Figure 3) for reader's reference.</p>
<p>State Representation</p>
<p>Under the Reinforcement Learning framework, a text-based game environment can be modelled as POMDP, and the agents do not have direct access to the game states. One of the challenges for solving text-based games, even for a human, is interpreting visible game features and choosing the correct data formation to represent the game's state as accurately as possible.</p>
<p>Embedding only</p>
<p>Research works like (Narasimhan et al., 2015;He et al., 2015) only use word embedding sequences generated from game state text description as game representations. Pre-trained, and non-pretrained methods are adopted, which is discussed in the word embedding section. Some researchers also concatenate features like game file names as game state representations.</p>
<p>Graph aided</p>
<p>Graph has been widely used as a feature to solve problems in many fields, such as bioinformatics and computer vision. For text-based games like Textworld games, the game states can be accurately represented by the graph. The accurate graph representation is usually not accessible to game players. Using pretrained neural networks or hand-crafted rules, some researchers try turning game state text description and agent experience into knowledge graphs. Neural network architectures like GCN can encode the generated knowledge graph.</p>
<p>Action Space</p>
<p>Action space flexibility describes a text-based game agent's ability to adapt to complex action text and action sets. Some agents can only cope with a small fixed action set, and others can deal with action text with varying lengths, large vocabulary, and complex combinations.</p>
<p>Non-flexible</p>
<p>Text-based game agents' deep learning models could be designed with a fixed output fit a small action set. For example, (Yuan, Côté, Sordoni, Laroche, Combes, Hausknecht, &amp; Trischler, 2018) fixed their network output shapes to 2 and 5, to deal with a fixed 2*5 (10 different action choices in total) game action set. This design has different sets of neural network layer weights for different action choices, which could be beneficial to predict value more accurately for each action. Yet due to the network output size only fits a fixed group of actions, this design is hard to generalize to games with longer action text sentences and varying action choices.</p>
<p>Flexible</p>
<p>To deal with action text sentences with arbitrary length and larger vocabulary, agent neural network models could be designed to have only one output node to predict state-action values. This kind of model is designed to predict values for all available actions and choose one to act based on the policy. These models have a stronger ability to generalize to different games. Yet, it doesn't have other groups of weight trained for each action, which makes the training process harder than non-flexible type models.</p>
<p>Template-based</p>
<p>Research works such as (Zahavy, Haroush, Merlis, Mankowitz, &amp; Mannor, 2018;Jain, Fedus, Larochelle, Precup, &amp; Bellemare, 2020) have text action templates based on human gameplay experience. They only use the deep learning model to predict appropriate words to fill in the text action templates. This model type shares the same generalization problem with the non-flexible type, and the whole template needs to be redesigned to generalize to different games. Also, these models will need human gameplay experience to determine the most suitable text templates.</p>
<p>Reinforcement Learning Algorithm</p>
<p>Reinforcement Learning frameworks such as policy gradient, Q-learning and Actor-Critic are widely adopted in game solving and bioinformatics fields. (Narasimhan et al., 2015) firstly adopted Deep Q-Learning to solve text-based games, and some following works adopted the same framework. Actor-critic is a widely used algorithm in recent years and has been successfully applied to complex problems like Go game.</p>
<p>Deep Q-Learning</p>
<p>Deep Q-learning(DQN) is a less complex yet powerful Reinforcement Learning algorithm. DQN is widely used in solving games like Atari. Q-learning uses the below function to update Q value and solve the game by approximating the Q value for each state.
Q ( s t , a t ) = γ max a t+1 Q(s t+1 , a t+1 ) + r t(1)
where s t , a t , r t demotes state,action and reward at time step t.</p>
<p>For the text-based games field, (Narasimhan et al., 2015) firstly adopted DQN to approximate the Q value for state-action and state-object pairs and select optimal pairs as the input for each text game state description.  also used DQN, and their result showed that, with some additional reward design, DQN could support the text-based games to solve complicated games and generalize the performance on unseen games.</p>
<p>Actor-Critic</p>
<p>Actor-critic is considered a more advanced algorithm than DQN, and its capability has been proven to solve very complex games such as the GO game. Researchers have used Advantage Actor Critic(A2C) to train text-based game agents. A2C use below formula to calculate value v(s t ) ,return R(s t , a t ) and advantage Adv(s t , a t )for game states and actions.
R(s t , a t ) = γ l v(s t+l ) + l k=1 γ k−1 r t+k (2) Adv(s t , a t ) = R(s t , a t ) − v(s t )(3)
A2C is considered a reliable agent training method as it reduces variances using advantage. Yet most TBG research work doesn't compare performance between applying Q-learning and A2C, as it requires neural network architecture change.</p>
<p>Reward Functions</p>
<p>The goal of a Text-based game agent is to collect game scores from the environment as much as possible. A game score can be directly adapted as a reward to the agent. Researchers also designed reward functions to encourage certain agent behaviours, such as choosing actions likely to lead the agent to unexplored game states. For example, the exploratory reward function designed by  encourages the agent to explore unseen states.</p>
<p>Deep Learning Architectures and modules</p>
<p>Encoder Type</p>
<p>As text-based game agents need to process text information given by game environment, the architecture of the text encoder type have a large influence on agent performance. The agent model also needs to be lite, as the agents are trained by sampling from game environments, and the text encoder needs to be efficient enough to support agent exploration.</p>
<p>MLP</p>
<p>Multilayer Perceptron(MLP) is a feed-forward neural network consisting of an input layer, one or more hidden layers and an output layer. MLP is a basic form of neural network and is widely used in domains like classification, regression and reinforcement learning. (He et al., 2015) uses MLP to encode text inputs in TBGs. For each state/action text pair, DRRN (He et al., 2015) feeds the state embedding vectors and action embedding vectors into two MLPs separately, then approximate the Q value based on the inner product of vectors from the last hidden layers of both MLPs.</p>
<p>CNN</p>
<p>Because of its outstanding performance in deep learning, Convolutional Neural Network(CNN) has become one of the most popular neural networks. Although CNN is more popular in computer vision, its performance in other domains like time series prediction and signal identification is also noticeable. The basic architecture of CNN includes an input layer, convolutional layers, pooling layers, fully-connected layers and an output layer. Various CNN models like VGGNet (Simonyan &amp; Zisserman, ), GoogLeNet (Szegedy, Liu, Jia, Sermanet, Reed, Anguelov, Erhan, Vanhoucke, &amp; Rabinovich, 2015) and ResNet (He et al., 2015) differ in the number of layers, number and size of kernels, etc.. For TBG tasks, (Zahavy et al., 2018;Yin &amp; May, 2019a;Yin, Weischedel, &amp; May, 2020;Yin &amp; May, 2019b) leverage CNN to encode game description text input. All the models are modified based on (Kim, 2014). (Zahavy et al., 2018;Yin &amp; May, 2019a) use the similar CNN structure in (Kim, 2014). (Yin et al., 2020;Yin &amp; May, 2019b) only use a lite version of CNN, which consists of an input layer, Convolutional layer and max-pooling layer. The output of the max-pooling layer is the encoding of text input. Besides, (Yin &amp; May, 2019a, 2019bYin et al., 2020) add position embedding into CNN input to assist model learning.</p>
<p>LSTM/GRU</p>
<p>A recurrent Neural Network(RNN) is widely used in processing sequential data. It is commonly used in tasks, like machine translation, sentiment analysis, video analysis, speech recognition, etc. Long Short-Term Memory(LSTM) and Gated Recurrent Unit(GRU) are two typical forms of RNN. The input of both LSTM and GRU is word embedding, and each cell generates an output and a hidden state passed to the next cell. Research works such as (Narasimhan et al., 2015;Yuan et al., 2018) tokenize game state description text, map one-hot embeddings to higher dimensional word embedding sequences, and use LSTM to encode the embedding sequence as state encoding. Other works such as (Adolphs &amp; Hofmann, 2020;Hausknecht, Ammanabrolu, Côté, &amp; Yuan, 2020) utilize GRU. Both output of LSTM/GRU and the hidden state of the last cell can be used as the game state encoding.</p>
<p>Transformer</p>
<p>Since Transformer (Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, &amp; Polosukhin, 2017) was proposed, it has quickly become the hot field in research because of its great success. Although the original transformer is designed for NLP tasks, variants of transformers have entered other domains like computer vision and audio processing. The transformer in (Vaswani et al., 2017) has an encoder and a decoder. Both encoder and decoder take the word embedding sequence generated from game state text, and then add positional embedding onto the word embedding sequence. This sequence is used as the transformer encoder input. For TBG field, in (Adhikari, Yuan, Côté, Zelinka, Rondeau, Laroche, Poupart, Tang, Trischler, &amp; Hamilton, 2020), (Yuan, Côté, Fu, Lin, Pal, Bengio, &amp; Trischler, 2019) and (Yin et al., 2020) use transformer encoder to encode game state description text, and use the transformer encoder output as state or action encoding.</p>
<p>BERT</p>
<p>Bidirectional Encoder Representations from Transformers(BERT) (Kenton &amp; Toutanova, 2019) is a pre-trained transformer-based model. It achieves many state-of-art performances in NLP tasks. In the TBG research field, research works like (Nahian, Frazier, Harrison, &amp; Riedl, 2021;Yin et al., 2020) use BERT for state description text encoding. In these works, part of the weights of the encoder is frozen, and only a small part of the encoder layers will be fine-tuned during training, which makes the training more efficient. Compared to the Transformer encoder, the pre-trained BERT model normally uses it's own tokenizer and produces word segment embeddings. It has a much more number of parameters to capture information from a huge corpus. Among all the encoder types, BERT is the heaviest model.</p>
<p>Word Embedding</p>
<p>Word embeddings are used to represent tokens in given texts. Word embeddings can be pre-trained from the selected corpus and fixed for later use. Researchers sometimes unfreeze pretrained embedding weights to let downstream tasks update them. Most text-based game agents use embedding sequences to represent the game state or action text. (Adhikari et al., 2020), (Yuan et al., 2019), (Nahian et al., 2021) and (Yin et al., 2020) leverage pre-trained word embedding models and keep the embeddings fixed during training the agent. Among them, (Adhikari et al., 2020) and (Yuan et al., 2019) use fastText (Mikolov, Grave, Bojanowski, Puhrsch, &amp; Joulin, 2018), (Yin et al., 2020) use Glove (Pennington, Socher, &amp; Manning, 2014) and BERT, (Nahian et al., 2021) use BERT. (Jang, Seo, Lee, &amp; Kim, 2020) use spaCy (Honnibal, Montani, Van Landeghem, &amp; Boyd, 2020) which is a NLP library that provides static pre-trained word embeddings. In addition, (He et al., 2015) uses bag-of-words(BOW), (Narasimhan et al., 2015) compares BOW and bag-of-bigrams(BI) representation with trained LSTM embedding. (Yao, Narasimhan, &amp; Hausknecht, 2021) uses hash values as word embedding and shows the agent can achieve a high score even without language semantics.</p>
<p>Pretrained</p>
<p>Non-pretrained</p>
<p>Most of the agents discussed in this paper either fine-tune the pre-trained word embeddings or train the embeddings from scratch. Research works like (Narasimhan et al., 2015),  initialize the word embeddings as either zeros or random vectors. Some research works like (Yao et al., 2021) and (Murugesan, Atzeni, Shukla, Sachan, Kapanipathi, &amp; Talamadupula, 2020) initialize the embedding using pre-trained Glove model, (Zahavy et al., 2018) uses word2vec (Mikolov, Chen, Corrado, &amp; Dean, 2013) model instead. Based on the experiment from (Yin et al., 2020), although fully fine-tuning BERT outperforms freezing it during training, the former sacrifices the latter's advantage in training speed. (Yao, Rao, Hausknecht, &amp; Narasimhan, 2020) conducts similar experiments, and they compare finetuning pre-trained GPT-2(Radford, Wu, Child, Luan, Amodei, Sutskever, et al., 2019)-a model that only consists of decoder part of transformer-and learning a random initialized GPT-2, the result shows pre-trained GPT-2 has better performance.</p>
<p>Pretrained Modules</p>
<p>Pretrained modules are widely used in text-based game agent designs. With pretrained modules, text-based game agents are initialized with knowledge before exploring game environments. Using pretrained modules can improve agent performance, yet potentially could damage agent efficiency and generalization ability towards different games.</p>
<p>Supervised</p>
<p>(Ammanabrolu &amp; Riedl, 2019) trains a paired question encoder and answer encoder using DrQA (Chen, Fisch, Weston, &amp; Bordes, 2017) method. The data are generated from an agent that is accessible to the shortest solution of the game. The weights of the encoders and the embedding learned are used to initialize the agent. Ammanabrolu et al. (Ammanabrolu &amp; Riedl, 2019) only tested this method on TextWorld games with a 'home' theme in this paper. Later, they extended the question-answering pairs to a set of Jericho games and proposed Jericho-QA (Ammanabrolu, Tien, Hausknecht, &amp; Riedl, 2020). In this paper, Ammanabrolu et al.  pre-trained ALBERT (Lan, Chen, Goodman, Gimpel, Sharma, &amp; Soricut, 2019) with Jericho-QA. The information extracted by ALBERT from text observation is used to update the knowledge graph during training. (Adolphs &amp; Hofmann, 2020) pre-trained a task-specific module called Recipe Manager. It is used to predict what's left to complete the tasks. As mentioned, it can only be used in the cooking games generated by TextWorld. (Yao et al., 2020) pre-trains CALM with ClubFloyd-a dataset containing state-action pairs of TBGs-to generate a relatively small size of possible actions at each turn of the game. (Adhikari et al., 2020) proposed two methods to pre-train knowledge graph updater, which are Observation Generation (OG) and Contrastive Observation Classification (COC). OG trains the graph updater with a decoder to reconstruct text observation. COC instead trains the model to differentiate between true observation and a fake one.</p>
<p>Unsupervised</p>
<p>Benchmark Environments</p>
<p>Text-based game platforms</p>
<p>Single games</p>
<p>At an early stage in the Text-based Games agent research field, researchers like (Narasimhan et al., 2015) (He et al., 2015) use single game environments like Evennia and Ifarchive. The games typically offer limited difficulty settings and a few fixed game maps. For this kind of environment, the agent only needs to learn how to solve a single game, which Reinforcement Learning tabular methods can handle, as state and action combinations are very limited.</p>
<p>Text-based game platforms</p>
<p>In 2018, Cote et al.  built the first platform to support text-based game research. The environment supports building a set of games with more detailed language descriptions, which enables agents to be trained to understand the language and solve a class of games. Hausknecht (Hausknecht, Ammanabrolu, Marc-Alexandre, &amp; Xingdi, 2019) developed the Jericho platform, included additional features like games state trees, and connected it to the Textworld platform.</p>
<p>Urbanek et al.(Jack Urbanek, 2019) developed a Light environment and combined the game with dialogue. The games in this platform do not have complicated rooms and tasks like Cook in Textworld, but it supports training agents that perform tasks and chat at the same time.</p>
<p>Text-based game agent evaluation settings</p>
<p>Text-based game agent learning and evaluation settings can be classified as follows: 1.Single game The agent learns how to play one game and achieve more game scores possible. Some single games with very large maps and multiple subtasks can be hard to deal with, like Zork.  Adolphs &amp; Hofmann, 2020;Hausknecht et al., 2020;Xu et al., 2020;Ammanabrolu et al., 2020;Murugesan et al., 2020;Yin &amp; May, 2019a;Yao et al., 2020;Yin et al., 2020;Yin &amp; May, 2019b;Jang et al., 2020;Yao et al., 2021) Pretrained (Narasimhan et al., 2015;He et al., 2015;Adhikari et al., 2020;Yuan et al., 2019;Nahian et al., 2021;Yin et al., 2020;Jang et al., 2020;Yao et al., 2021;Ammanabrolu &amp; Riedl, 2021) Encoder Type</p>
<p>Model Features</p>
<p>Transformer (Adhikari et al., 2020;Yuan et al., 2019;Nahian et al., 2021;Yin et al., 2020) LSTM/GRU (Narasimhan et al., 2015;Yuan et al., 2018;Ammanabrolu &amp; Riedl, 2019;Jain et al., 2020;Adolphs &amp; Hofmann, 2020;Hausknecht et al., 2020;Xu, Fang, Chen, Du, Zhou, &amp; Zhang, 2020;Ammanabrolu et al., 2020;Murugesan et al., 2020;Ammanabrolu &amp; Riedl, 2021;Murugesan, Atzeni, Kapanipathi, Talamadupula, Sachan, &amp; Campbell, 2021;Yao et al., 2020;Yin et al., 2020;Yin &amp; May, 2019b;Jang et al., 2020;Yao et al., 2021) CNN (Zahavy et al., 2018;Yin &amp; May, 2019a;Yin et al., 2020;Yin &amp; May, 2019b) MLP (He et al., 2015) Figure 3: Ontology for TBG agent methods</p>
<p>Evaluation Types</p>
<p>generalization Adolphs &amp; Hofmann, 2020;Adhikari et al., 2020;Yuan et al., 2019;Yin et al., 2020;Yin &amp; May, 2019b) multiple games Ammanabrolu &amp; Riedl, 2019;Jain et al., 2020;Adolphs &amp; Hofmann, 2020;Adhikari et al., 2020;Murugesan et al., 2020;Ammanabrolu &amp; Riedl, 2021;Yuan et al., 2019;Murugesan et al., 2021;Yao et al., 2020;Yin &amp; May, 2019b) single game (Narasimhan et al., 2015;He et al., 2015;Zahavy et al., 2018;Yuan et al., 2018;Jain et al., 2020;Hausknecht et al., 2020;Ammanabrolu et al., 2020;Murugesan et al., 2020;Ammanabrolu &amp; Riedl, 2021;Yin &amp; May, 2019a;Nahian et al., 2021;Jang et al., 2020;Yao et al., 2021) Figure 4: Evaluation Types of related works 2.Multiple games The agent learns to solve a set of games with similar difficulty and settings. Each game in the set can have different objects and room configurations, and the text description for rooms can also be different. Still, the general task and difficulty are very similar in the game set. An agent is evaluated with average performance among all the games in the set. 3. Generalisation The agent learns from a set of games for training and is evaluated by the performance on the training set and on some unseen games to test if the learnt policy generalises to unseen language descriptions and settings.</p>
<p>Most of the methods in this field are only evaluated in limited settings, and we classify the evaluation methods used in related works in ontology figure 4.</p>
<p>Evaluation metrics</p>
<p>Two metrics are generally adopted in this text based game agent evaluation:</p>
<p>(1) The average game completion rate: (s o )/(s m * b), where s o = obtained score by the agent in a game, s m = maximum game score, b = evaluation set size;</p>
<p>(2) the average steps for the agent to finish games: (s p )/b, where s p = step taken for the agent to finish a game. </p>
<p>Ablation Test and Analysis</p>
<p>To evaluate and analyse methods used in the Text-based Games research field, particularly for the methods' performance in solving multiple games and unseen games, we created a standardised environment and agents and performed ablation testing for selected methods.</p>
<p>Ablation Test Experiment settings</p>
<p>We evaluate selected models in several Textworld  environments. Textworld is the most commonly used platform in this field. As it supports large game set generation, it enables us to train models on large game sets, perform generalisation tests and see how the testing models perform on unseen games. We selected three games: Coin Collector, Treasure Hunter and Cooking recipe. The chosen games have different levels of difficulty. Coin collector is the easiest, and it has very few rooms(∼20), a simple task, and a fixed action set. The Cooking recipe game's task is more challenging, and the agent needs to collect multiple items and perform cooking actions to finish the game, the action set of different states could be different. Treasure hunter's difficulty is between Coin Collector and Cooking recipe.</p>
<p>We report the models' performance in two different settings: multiple games and generalisation:</p>
<p>(1) For multiple games setting, we develop and test different games and train/test our model on the set of games;</p>
<p>(2) To test model performance on unseen games, we evaluate the models in a generalisation setting. In this setting, we train the models by using 100 generated games and test the models on 20 games that the models have never seen during training.</p>
<p>The training and testing set is formed by different games that are randomly chosen from a larger game set. For the generalisation test, we report both training and testing results. To better visualise the data changing trend in the figures, the values are filtered with Savgol filter (Savitzky &amp; Golay, 1964) </p>
<p>Encoder types</p>
<p>For encoder types, we benchmark five different kinds of text encoder architectures popularly used by related works. The five encoders are chosen to cover the main classes of Neural Network architectures and, at the same time, cover different parameter number levels so we can analyze the balance of performance and efficiency. We selected GRU and LSTM as our test encoders for Recurrent Neural Networks, which adopt a timestep recurrence methodology. These two models are the most commonly used RNNs. GRU has a less complex structure than LSTM, as RL training requires better efficiency for agent exploration. We want to compare the performance of GRU and LSTM and see if GRU works faster with a less complex structure. The rest three encoders(CNN,BERT,Transformer) are non-recurrent type encoders. CNN adopts a convolution methodology, while Transformer and BERT use an attention mechanism. BERT is the most popular model in traditional NLP tasks, yet it has many more parameters than the others. From ablation test results on a training set with 100 different games reported in fig  6-8(a), the CNN encoder learns faster than other models. The transformer encoder has comparable performance to CNN encoder, especially in the Cook game. Both CNN and Transformer encoder converge for the Coin-collection game and Treasure hunter game. CNN encoder learns slightly faster. The two models are powerful enough to learn useful information in the Cook game, which is very challenging to solve, without any additional modules.For RNN models, we tested GRU and LSTM encoder. Two encoders have similar performance in all settings but can only converge to optimal policy in Coin-Collector. The two recurrent encoders failed to converge for the other two more complex games. For the Treasure hunters, they can learn some useful information and solve some games in the set, but in the Cook game, they can't stably learn from the environment. We also tested the BERT encoder, which is an SOA language learning architecture. Surprisingly, without pretrained weights, BERT can barely learn anything from the environment, which indicates that model behaviour can be very different when learning in interactive environments like Text-based games, compared to learning from the corpus for traditional NLP tasks.</p>
<p>We also performed a generalisation test by evaluating trained agents on 20 unseen games. The different encoders' performance on other games is inconsistent Fig.6(b) shows that the Transformer agent has outstanding performance on overlooked Coin Collector games, Fig.7(b) show that RNN encoders perform better on unseen Treasure hunter games. In fig.8(b), except BERT encoder, all other encoder types have similar performance. Compared to solving a fixed set of games, agent performance on unseen games is much lower (generally lower than 0.4), and no obvious pattern can be observed from the evaluation scores. Solving unseen games remains a significant challenge in TBG agent research field. As an RL agent must explore and learn from the environment efficiently, we also report Encoder parameters statistics. BERT has the largest parameter size, yet it's unable to learn effectively from all the environment settings. The model can converge if we use a pretrained BERT and freeze most of its layers. In this case, the model needs to be pre-trained with a prepared corpus. We deem this a sort of word embedding and will discuss this in the word embedding section. BERT is unsuitable for TBG tasks if additional training corpus or pre-trained parameters are unavailable.</p>
<p>GRU/LSTM/Transformer encoders have similar parameters count. The Transformer outperforms GRU and LSTM significantly. CNN has a slightly better performance compared to Transformer, yet it has 7.7 times more parameters. The agent needs to sample and explore the environment for complex TBG tasks to converge to the optimal policy. Using the same training time, agents with a Transformer encoder can explore much more states than agents with a CNN encoder. Parameter counts are summarised in the below list. Word embeddings contain external information; using word embedding means the model is initialized with external knowledge, and it's important to see if this external knowledge helps model to perform better. To see which type of work embedding is more suitable for TBG tasks, we benchmark three popular pre-trained word embeddings and an embedding layer that is trained in the tasks. For pre-trained embeddings, we include GLOVE, Fasttext, BERT, and see their effect on four encoder types, including GRU, LSTM, CNN and Transformer. In terms of fitting a large training set of games, from fig.9-12(a), we can see that the non-pretrained task-specific embedding layer performs much better compared to applying any pretrained word embedding. For all encoder types, the average training score can approach close to 1.0, which means the agent can learn to solve all games in the training set. As text-based game language is generally less complex than traditional NLP tasks such as text classification or translation, embedding trained from a rich corpus does not help the models to learn faster from the simple language environment. Moreover, the nonpretrained embedding layers learn from the rewards the agent receives, which will be more directly related to the environment and crafted reward functions.</p>
<p>In the training scenario, Glove outperforms the other types among pre-trained embeddings. For all encoder types, Glove outperforms FastText and BERT embedding. BERT embedding is pretrained from corpus larger than Glove and FastText, yet it performs the worst when used with GRU, LSTM and CNN encoder. This finding suggests that language processing methods that perform well in traditional NLP tasks could affect RL agent performance, even used in a language environment. TBG task-specific methods need to be introduced to improve agent performance rather than simply applying traditional NLP methods.</p>
<p>Different embedding also affects the TBG agent model's generalization ability. fig.9-12(b) presents agents performance on unseen games with different types of word embeddings. By observing fig 9-11(b), for GRU/LSTM/CNN model, FastText and BERT embedding outperforms other embedding types. non-pretrained and Glove embedding perform best (a) Average training score (b) Average evaluation score Figure 12: Average scores for Transformer encoder with different embedding types in training, but on unseen games, these two types perform poorly, which indicates that they cause the agents to overfit the training set. The different embedding types does not influence the transformer encoder's generalization ability a lot. In fig.12(b), all embedding types has similar effect on evaluation score.</p>
<p>Discussion and Future Work</p>
<p>In this work we have summarized methods in Text-based games field, and evaluated selected method on standardized agent and environment and reported our findings. In this section we discuss on some topics which we find important to Text-based game agent design, and offer our perspective on challenges and future directions of Text-based game agent research field.</p>
<p>Performance versus efficiency</p>
<p>In our experiment, we observed that models with more parameters, like BERT, which perform well in traditional NLP tasks such as translation and text classification, doesn't necessarily perform well in TBG tasks. The large models have stronger language encoding capability, but we found that as training the full model is inefficient and requires more complex training techniques, and knowledge acquired from other corpus doesn't necessarily help the model perform better in TBG task, a middle-sized model like single layer transformer encoder or single layer text CNN is more suitable for TBG agent. In order to use large models like BERT, new designs and training techniques need to be introduced in the field to empower the large models.</p>
<p>Generalization</p>
<p>Based on our analysis[6.2.1] and review of related works Adhikari et al., 2020), model's generalization ability remains a major challenge in this field. The models that can easily learn to solve seen games doesn't guarantee they perform well in unseen game at all. For TBG agent research field, although platforms like Textworld support game set generation, the richness and volume of games are still not comparable to traditional NLP task datasets.For example, a model trained on Wikipedia corpus can have 4.1 billion words with great variety in the training set, but Textworld games training set with 100 games may only have 50 thousands words with very limited vocabulary variations. Because of limited vocabulary and sentence structures in the training set, better structured state representation that generalizes to unseen conditions need to be introduced, to improve generalization ability of the models after trained on limited games.</p>
<p>Towards smarter Text-based game agents</p>
<p>Except for two issues addressed above, TBG agents are mainly trained to evaluate language formed state-action pairs. Among a review of related works[ref ontology] and the analysis of current methods, the agents still lack of learning a general strategy to solve a class of games, especially identifying key game thresholds, like finish gathering a list of different materiel. The agents still lack of the ability to learn and extract useful information from state and action history. The strategy-wise reward functions introduced in the field are limited. Lots of agent designs still contain hand-crafted rules and policies. Hand-crafted rules are also heavily used in game state representation(e.g. graph) generation process. These obstacles remain to be research problems in this field, in order to design a smart agent that solves the games as efficiently, as resourceful as human.</p>
<p>Conclusion</p>
<p>In this paper, we choose some RNN, CNN, and Transformer based models to represent popularly used models in TBG and NLP fields. The selected models also contain different levels of total trainable parameters to represent different levels of efficiency. We train the models using the same hyper-parameters, with the same training games set and evaluate model game performance both on seen and unseen games. The results show that CNN and Transformer perform best, and Transformer has a much lower parameter count. The models with more parameters like BERT do not deliver stronger performance. The models do not solve all the seen games, even for easy games. For hard games like cook, the best performing models can only solve about 50% of the seen games. For unseen games, the models perform poorly. To reduce training fluctuation, better training strategies need to be introduced to TBG games. For RL algorithms like Actor-Critic, how to better apply it to TBG agent training to improve training performance remains to be found. Current applications of Actor-Critic in TBG agent training don't bring significant benefits. For harder games, better reward functions should be designed to help the model value key thresholds to finish the games. We standardise our environment and training procedure and remove all hand-crafted rules, to present how the modules and word embedding types contribute toward agent performance. The language processing modules and RL components are also standardised. We pick games with different difficulties and form the same training sets and generalisation evaluation sets. We see that, without adding hand-crafted rules and other policy-related pretrained components, some modules like the Transformer encoder and non-pretrained embedding perform better. The modules that contain external information, like word embeddings, do not really help the agents' performance as expected. As applying pre-trained modules that work in traditional NLP tasks does not necessarily enhance the agent performance, external knowledge and pre-trained modules need to be tuned better to fit TBG agent training.</p>
<p>We have chosen two evaluation types-multiple game tests and generalisation tests to see if models can perform consistently in different evaluation types. These two types are more meaningful for TBG agent applications. We train the models using a set of games and evaluate them on seen and unseen game sets. The result shows that the model's performance in multiple and generalisation tests has a very weak correlation. Those models that perform better on seen games do not generalise well. The state representation is not well structured to generalise to unseen states. Thus the state value approximation only works for seen games. A better-structured state representation design with a smaller feature space needs to be introduced to represent more states and improve model generalisation performance. It's also important to try to train the agent to work on different game types, for example, to train the agent to learn from Coin-collector and Treasure hunter, and examine its ability to solve the Cooking game. The learnt knowledge should be transferable between a larger variety of text-based games.</p>
<p>Through our tests analysis, we present the strengths and weaknesses of selected deep learning components used in TBG research works. Recent developments in the TBG agent field mainly focus on graph representation and external modules to enhance the models'</p>
<p>performance. Yet the model generalisation to unseen games and model capability remain major challenges in this field. As a reference for future research works, new methods could be introduced to cope with the strengths and weaknesses of the deep learning modules analysed in our work.</p>
<p>Figure 1 :
1Text-based Game Agent Workflow</p>
<p>((
Adhikari et al., 2020;Ammanabrolu &amp; Riedl, 2021) Supervised(Ammanabrolu &amp; Riedl, 2019;Adolphs &amp; Hofmann, 2020;Ammanabrolu et al., 2020; Yao et al.Narasimhan et al., 2015;He et al., 2015;Zahavy et al., 2018;Yuan et al., 2018;Jain et al., 2020;Adolphs &amp; Hofmann, 2020;Hausknecht et al., 2020;Yin &amp; May, 2019a;Yuan et al., 2019;Yao et al., 2020;Nahian et al., 2021; Yin et al., 2020; Yin &amp; May, 2019b; Jang et al., 2020; Yao et al., 2021) Adolphs &amp; Hofmann, 2020; Hausknecht et al., 2020; Ammanabrolu &amp; Hausknecht, 2019; Ammanabrolu et al., 2020; Murugesan et al., 2020) Flexible (He et al., 2015; Ammanabrolu &amp; Riedl, 2019; Jain et al., 2020; Adolphs &amp; Hofmann, 2020; Adhikari et al., 2020; Ammanabrolu &amp; Riedl, 2021; Yin &amp; May, 2019a; Murugesan et al., 2021; Yao et al., 2020; Nahian et al., 2021; Yin et al., 2020; Yin &amp; May, 2019b; Jang et al., 2020; Yao et al., 2021)Non-flexible(Narasimhan et al., 2015;Zahavy et al., 2018;Yuan et al., 2019;Xu et al., 2020;Ammanabrolu et al., 2020;Murugesan et al., 2020;Ammanabrolu &amp; Riedl, 2021;Murugesan et al., 2021;Nahian et al., 2021) DQN(Narasimhan et al., 2015;He et al., 2015;Zahavy et al., 2018;Yuan et al., 2018;Ammanabrolu &amp; Riedl, 2019;Jain et al., 2020;Hausknecht et al., 2020;Adhikari et al., 2020;Yin &amp; May, 2019a;Yuan et al., 2019;Yao et al., 2020;Yin et al., 2020;Yin &amp; May, 2019b;Jang et al., 2020; Yao et al.et al., 2015;Zahavy et al., 2018;Yuan et al., 2018;Ammanabrolu &amp; Riedl, 2019;Jain </p>
<p>Figure 5
5Figure 5: ontology</p>
<p>Figure 6 :
6Average scores for different encoder types in Coin Collector game</p>
<p>Figure 7 :Figure 8 :Figure 9 :
789Average scores for different encoder types in Treasure Hunter game (a) Average training score (b) Average evaluation score Average scores for different encoder types in Cook game Average scores for GRU encoder with different embedding types</p>
<p>Figure 10 :Figure 11 :
1011Average scores for LSTM encoder with different embedding types (a) Average training score (b) Average evaluation score Average scores for CNN encoder with different embedding types</p>
<p>Table 1 :
1Historical overview of text-based game papersYear Network Architecture/Training 
Reinforcement Learning Methods 
2015 MLP, Word Embeddings 
State Encoding, Deep Q-Learning </p>
<p>2018 
State Recurrence, Multiple games, 
Zero-shot Evaluation 
Exploration Reward, Replay Memory </p>
<p>2019 
Graph Attention, pre-trained knowl-
edge graph </p>
<p>Graph Representation, Actor-Critic </p>
<p>games, and testing agent generalization ability on unseen games. Neural networks such as 
MLP, RNN are applied into agent design as text encoder and being developed over ensuing 
years. </p>
<p>to reduce noise.Feature 
State Number 
Action Voc Size 
Coin Collector Lv5 
308 
10 
Treasure Hunter Lv5 
426 
24 
Cooking Recipe lv1 
374 
32 </p>
<p>Table 2 :
2Game Environment statistics </p>
<p>AcknowledgmentsThe authors wish to thank Hans-Martin Adorf, Don Rosenthal, Richard Franier, Peter Cheeseman and Monte Zweben for their assistance and advice. We also thank Ron Musick and our anonymous reviewers for their comments. The Space Telescope Science Institute is operated by the Association of Universities for Research in Astronomy for NASA.Appendix A. Probability Distributions for N-Queens
Learning dynamic belief graphs to generalize on text-based games. A Adhikari, X Yuan, M.-A Côté, M Zelinka, M.-A Rondeau, R Laroche, P Poupart, J Tang, A Trischler, W Hamilton, Advances in Neural Information Processing Systems. 33Adhikari, A., Yuan, X., Côté, M.-A., Zelinka, M., Rondeau, M.-A., Laroche, R., Poupart, P., Tang, J., Trischler, A., &amp; Hamilton, W. (2020). Learning dynamic belief graphs to generalize on text-based games. Advances in Neural Information Processing Systems, 33.</p>
<p>Ledeepchef deep reinforcement learning agent for families of text-based games. L Adolphs, T Hofmann, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence34Adolphs, L., &amp; Hofmann, T. (2020). Ledeepchef deep reinforcement learning agent for families of text-based games. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 34, pp. 7342-7349.</p>
<p>Graph constrained reinforcement learning for natural language action spaces. P Ammanabrolu, M Hausknecht, International Conference on Learning Representations. Ammanabrolu, P., &amp; Hausknecht, M. (2019). Graph constrained reinforcement learning for natural language action spaces. In International Conference on Learning Representa- tions.</p>
<p>Playing text-adventure games with graph-based deep reinforcement learning. P Ammanabrolu, M Riedl, NAACL-HLT. Ammanabrolu, P., &amp; Riedl, M. (2019). Playing text-adventure games with graph-based deep reinforcement learning. In NAACL-HLT (1).</p>
<p>Learning knowledge graph-based world models of textual environments. P Ammanabrolu, M Riedl, Advances in Neural Information Processing Systems. 34Ammanabrolu, P., &amp; Riedl, M. (2021). Learning knowledge graph-based world models of textual environments. Advances in Neural Information Processing Systems, 34, 3720- 3731.</p>
<p>P Ammanabrolu, E Tien, M Hausknecht, M O Riedl, arXiv:2006.07409How to avoid being eaten by a grue: Structured exploration strategies for textual worlds. 1arXiv preprintAmmanabrolu, P., Tien, E., Hausknecht, M., &amp; Riedl, M. O. (2020). How to avoid being eaten by a grue: Structured exploration strategies for textual worlds. arXiv preprint arXiv:2006.07409, 1 (1).</p>
<p>Reading wikipedia to answer opendomain questions. D Chen, A Fisch, J Weston, A Bordes, Chen, D., Fisch, A., Weston, J., &amp; Bordes, A. (2017). Reading wikipedia to answer open- domain questions..</p>
<p>Textworld: A learning environment for textbased games. M.-A Côté, Á Kádár, X Yuan, B Kybartas, T Barnes, E Fine, J Moore, M Hausknecht, L El Asri, M Adada, Workshop on Computer Games. SpringerCôté, M.-A., Kádár,Á., Yuan, X., Kybartas, B., Barnes, T., Fine, E., Moore, J., Hausknecht, M., El Asri, L., Adada, M., et al. (2018). Textworld: A learning environment for text- based games. In Workshop on Computer Games, pp. 41-75. Springer.</p>
<p>Interactive fiction games: A colossal adventure. M Hausknecht, P Ammanabrolu, M.-A Côté, X Yuan, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence34Hausknecht, M., Ammanabrolu, P., Côté, M.-A., &amp; Yuan, X. (2020). Interactive fiction games: A colossal adventure. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 34, pp. 7903-7910.</p>
<p>. M Hausknecht, P Ammanabrolu, C Marc-Alexandre, Y Xingdi, Interactive fiction games: A colossal adventure. CoRR, abs/1909.05398Hausknecht, M., Ammanabrolu, P., Marc-Alexandre, C., &amp; Xingdi, Y. (2019). Interactive fiction games: A colossal adventure. CoRR, abs/1909.05398.</p>
<p>Deep reinforcement learning with an unbounded action space. J He, J Chen, X He, J Gao, L Li, L Deng, M Ostendorf, arXiv:1511.046365arXiv preprintHe, J., Chen, J., He, X., Gao, J., Li, L., Deng, L., &amp; Ostendorf, M. (2015). Deep reinforce- ment learning with an unbounded action space. arXiv preprint arXiv:1511.04636, 5.</p>
<p>spaCy: Industrialstrength Natural Language Processing in Python. M Honnibal, I Montani, S Van Landeghem, A Boyd, 1Honnibal, M., Montani, I., Van Landeghem, S., &amp; Boyd, A. (2020). spaCy: Industrial- strength Natural Language Processing in Python.. 1 (1).</p>
<p>Learning to speak and act in a fantasy text adventure game. Jack Urbanek, A F , Jack Urbanek, A. F. e. a. (2019). Learning to speak and act in a fantasy text adventure game..</p>
<p>Algorithmic improvements for deep reinforcement learning applied to interactive fiction. V Jain, W Fedus, H Larochelle, D Precup, M G Bellemare, AAAI. Jain, V., Fedus, W., Larochelle, H., Precup, D., &amp; Bellemare, M. G. (2020). Algorithmic improvements for deep reinforcement learning applied to interactive fiction.. In AAAI, pp. 4328-4336.</p>
<p>Monte-carlo planning and learning with language action value estimates. Y Jang, S Seo, J Lee, K.-E Kim, International Conference on Learning Representations. Jang, Y., Seo, S., Lee, J., &amp; Kim, K.-E. (2020). Monte-carlo planning and learning with language action value estimates. In International Conference on Learning Represen- tations.</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. J D Kenton, M.-W C Toutanova, L K , Proceedings of NAACL-HLT. NAACL-HLTKenton, J. D. M.-W. C., &amp; Toutanova, L. K. (2019). Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of NAACL-HLT, pp. 4171- 4186.</p>
<p>Convolutional neural networks for sentence classification. Y Kim, Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)Doha, QatarAssociation for Computational LinguisticsKim, Y. (2014). Convolutional neural networks for sentence classification. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 1746-1751, Doha, Qatar. Association for Computational Linguistics.</p>
<p>Albert: A lite bert for self-supervised learning of language representations. Z Lan, M Chen, S Goodman, K Gimpel, P Sharma, R Soricut, International Conference on Learning Representations. Lan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., &amp; Soricut, R. (2019). Albert: A lite bert for self-supervised learning of language representations. In International Conference on Learning Representations.</p>
<p>Efficient estimation of word representations in vector space. T Mikolov, K Chen, G Corrado, J Dean, arXiv:1301.37811arXiv preprintMikolov, T., Chen, K., Corrado, G., &amp; Dean, J. (2013). Efficient estimation of word repre- sentations in vector space. arXiv preprint arXiv:1301.3781, 1 (1).</p>
<p>Advances in pretraining distributed word representations. T Mikolov, É Grave, P Bojanowski, C Puhrsch, A Joulin, Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC. the Eleventh International Conference on Language Resources and Evaluation (LRECMikolov, T., Grave,É., Bojanowski, P., Puhrsch, C., &amp; Joulin, A. (2018). Advances in pre- training distributed word representations. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018).</p>
<p>Efficient text-based reinforcement learning by jointly leveraging state and commonsense graph representations. K Murugesan, M Atzeni, P Kapanipathi, K Talamadupula, M Sachan, M Campbell, Acl-Ijcnlp 2021: The 59Th Annual Meeting Of The Association For Computational Linguistics And The 11Th International Joint Conference On Natural Language Processing. 2ASSOC COMPU-TATIONAL LINGUISTICS-ACLMurugesan, K., Atzeni, M., Kapanipathi, P., Talamadupula, K., Sachan, M., &amp; Campbell, M. (2021). Efficient text-based reinforcement learning by jointly leveraging state and commonsense graph representations. In Acl-Ijcnlp 2021: The 59Th Annual Meeting Of The Association For Computational Linguistics And The 11Th International Joint Conference On Natural Language Processing, Vol 2, pp. 719-725. ASSOC COMPU- TATIONAL LINGUISTICS-ACL.</p>
<p>Enhancing text-based reinforcement learning agents with commonsense knowledge. K Murugesan, M Atzeni, P Shukla, M Sachan, P Kapanipathi, K Talamadupula, arXiv:2005.008111arXiv preprintMurugesan, K., Atzeni, M., Shukla, P., Sachan, M., Kapanipathi, P., &amp; Talamadupula, K. (2020). Enhancing text-based reinforcement learning agents with commonsense knowledge. arXiv preprint arXiv:2005.00811, 1 (1).</p>
<p>Training value-aligned reinforcement learning agents using a normative prior. M S A Nahian, S Frazier, B Harrison, M Riedl, arXiv:2104.094691arXiv preprintNahian, M. S. A., Frazier, S., Harrison, B., &amp; Riedl, M. (2021). Training value-aligned rein- forcement learning agents using a normative prior. arXiv preprint arXiv:2104.09469, 1 (1).</p>
<p>Language understanding for textbased games using deep reinforcement learning. K Narasimhan, T D Kulkarni, R Barzilay, Proceedings of the Conference on Empirical Methods in Natural Language Processing. the Conference on Empirical Methods in Natural Language ProcessingCiteseerNarasimhan, K., Kulkarni, T. D., &amp; Barzilay, R. (2015). Language understanding for textbased games using deep reinforcement learning. In In Proceedings of the Con- ference on Empirical Methods in Natural Language Processing. Citeseer.</p>
<p>Glove: Global vectors for word representation. J Pennington, R Socher, C D Manning, Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP). the 2014 conference on empirical methods in natural language processing (EMNLP)Pennington, J., Socher, R., &amp; Manning, C. D. (2014). Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pp. 1532-1543.</p>
<p>Language models are unsupervised multitask learners. A Radford, J Wu, R Child, D Luan, D Amodei, I Sutskever, OpenAI blog. 189Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al. (2019). Language models are unsupervised multitask learners. OpenAI blog, 1 (8), 9.</p>
<p>Smoothing and differentiation of data by simplified least squares procedures. A Savitzky, M J Golay, Analytical chemistry. 368Savitzky, A., &amp; Golay, M. J. (1964). Smoothing and differentiation of data by simplified least squares procedures.. Analytical chemistry, 36 (8), 1627-1639.</p>
<p>Very deep convolutional networks for large-scale image recognition. K Simonyan, A Zisserman, Simonyan, K., &amp; Zisserman, A. Very deep convolutional networks for large-scale image recognition. In ICLR 2015.</p>
<p>Going deeper with convolutions. C Szegedy, W Liu, Y Jia, P Sermanet, S Reed, D Anguelov, D Erhan, V Vanhoucke, A Rabinovich, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionSzegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V., &amp; Rabinovich, A. (2015). Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1-9.</p>
<p>Attention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, L Kaiser, I Polosukhin, Advances in neural information processing systems. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., &amp; Polosukhin, I. (2017). Attention is all you need. In Advances in neural information processing systems, pp. 5998-6008.</p>
<p>Deep reinforcement learning with stacked hierarchical attention for text-based games. Y Xu, M Fang, L Chen, Y Du, J T Zhou, C Zhang, Advances in Neural Information Processing Systems. 33Xu, Y., Fang, M., Chen, L., Du, Y., Zhou, J. T., &amp; Zhang, C. (2020). Deep reinforcement learning with stacked hierarchical attention for text-based games. Advances in Neural Information Processing Systems, 33.</p>
<p>Reading and acting while blindfolded: The need for semantics in text game agents. S Yao, K Narasimhan, M Hausknecht, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesYao, S., Narasimhan, K., &amp; Hausknecht, M. (2021). Reading and acting while blindfolded: The need for semantics in text game agents. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 3097-3102.</p>
<p>Keep calm and explore: Language models for action generation in text-based games. S Yao, R Rao, M Hausknecht, K Narasimhan, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Yao, S., Rao, R., Hausknecht, M., &amp; Narasimhan, K. (2020). Keep calm and explore: Language models for action generation in text-based games. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 8736-8754.</p>
<p>Comprehensible context-driven text game playing. X Yin, J May, 2019 IEEE Conference on Games (CoG). IEEEYin, X., &amp; May, J. (2019a). Comprehensible context-driven text game playing. In 2019 IEEE Conference on Games (CoG), pp. 1-8. IEEE.</p>
<p>Learn how to cook a new recipe in a new house: Using map familiarization, curriculum learning, and bandit feedback to learn families of textbased adventure games. X Yin, J May, arXiv:1908.047771arXiv preprintYin, X., &amp; May, J. (2019b). Learn how to cook a new recipe in a new house: Using map familiarization, curriculum learning, and bandit feedback to learn families of text- based adventure games. arXiv preprint arXiv:1908.04777, 1 (1).</p>
<p>Learning to generalize for sequential decision making. X Yin, R Weischedel, J May, Findings of the Association for Computational Linguistics: EMNLP 2020. Yin, X., Weischedel, R., &amp; May, J. (2020). Learning to generalize for sequential decision making. In Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 3046-3063.</p>
<p>Interactive language learning by question answering. X Yuan, M.-A Côté, J Fu, Z Lin, C Pal, Y Bengio, A Trischler, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingYuan, X., Côté, M.-A., Fu, J., Lin, Z., Pal, C., Bengio, Y., &amp; Trischler, A. (2019). Interactive language learning by question answering. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 2796-2813.</p>
<p>Counting to explore and generalize in text-based games. X Yuan, M.-A Côté, A Sordoni, R Laroche, R T D Combes, M Hausknecht, A Trischler, Yuan, X., Côté, M.-A., Sordoni, A., Laroche, R., Combes, R. T. d., Hausknecht, M., &amp; Trischler, A. (2018). Counting to explore and generalize in text-based games. In ICML 2018.</p>
<p>Learn what not to learn: Action elimination with deep reinforcement learning. T Zahavy, M Haroush, N Merlis, D J Mankowitz, S Mannor, Advances in Neural Information Processing Systems. Zahavy, T., Haroush, M., Merlis, N., Mankowitz, D. J., &amp; Mannor, S. (2018). Learn what not to learn: Action elimination with deep reinforcement learning. In Advances in Neural Information Processing Systems, pp. 3562-3573.</p>            </div>
        </div>

    </div>
</body>
</html>
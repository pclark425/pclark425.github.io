<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6000 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6000</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6000</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-121.html">extraction-schema-121</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <p><strong>Paper ID:</strong> paper-265019021</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2311.01964v1.pdf" target="_blank">Don't Make Your LLM an Evaluation Benchmark Cheater</a></p>
                <p><strong>Paper Abstract:</strong> Large language models~(LLMs) have greatly advanced the frontiers of artificial intelligence, attaining remarkable improvement in model capacity. To assess the model performance, a typical approach is to construct evaluation benchmarks for measuring the ability level of LLMs in different aspects. Despite that a number of high-quality benchmarks have been released, the concerns about the appropriate use of these benchmarks and the fair comparison of different models are increasingly growing. Considering these concerns, in this paper, we discuss the potential risk and impact of inappropriately using evaluation benchmarks and misleadingly interpreting the evaluation results. Specially, we focus on a special issue that would lead to inappropriate evaluation, \ie \emph{benchmark leakage}, referring that the data related to evaluation sets is occasionally used for model training. This phenomenon now becomes more common since pre-training data is often prepared ahead of model test. We conduct extensive experiments to study the effect of benchmark leverage, and find that it can dramatically boost the evaluation results, which would finally lead to an unreliable assessment of model performance. To improve the use of existing evaluation benchmarks, we finally present several guidelines for both LLM developers and benchmark maintainers. We hope this work can draw attention to appropriate training and evaluation of LLMs.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6000.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6000.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Benchmark-based evaluation (zero-/few-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Benchmark-based zero-shot and few-shot evaluation on held-out test sets</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard practice of evaluating LLMs by running them (zero-shot or few-shot) on held-out benchmark datasets and reporting task metrics; used here to measure knowledge, reasoning, reading comprehension, summarization and code synthesis abilities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Run LLMs in zero-shot or few-shot settings (local or API) on standard benchmarks, optionally with few-shot demonstrations or chain-of-thought prompts; results aggregated and reported (single-run or averaged across prompts).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Task-specific automated metrics such as accuracy for QA/reading/reasoning, ROUGE-L for summarization, pass@k for code, and aggregate benchmark scores (e.g., MMLU percent).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-Neo-1.3B, phi-1.5 (1.3B), OpenLLaMA-3B, LLaMA-2-7B and larger LLaMA (13B/30B/65B) referenced</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>General knowledge, reasoning, reading comprehension, summarization and code synthesis tasks (general NLP/knowledge domains; applicable to assessing outputs that include scientific hypotheses but not specialized scientific experimental validation in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Not applicable — the paper evaluates model capabilities on benchmark tasks rather than specific LLM-generated scientific theories; the same benchmark protocol would be used to evaluate LLM-generated theory statements by measuring agreement with held-out ground truth or target outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Benchmark leakage (training on benchmark training/test prompts/test sets) dramatically inflates reported performance. Examples: MMLU and many QA/reasoning scores increase substantially under leakage; C3-Dialog score for GPT-Neo-1.3B doubled from 24.18 to 48.62 after training on benchmark training sets; in extreme test-set leakage cases, small (1.3B) models can appear to outperform 65B models.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>MMLU (5-shot), BoolQ, PIQA, HellaSwag, WinoGrande, ARC (Easy/Challenge), OpenBookQA, CommonsenseQA, GSM8k, AQuA, RACE-M/H, CoQA, CMRC2018, C3-Dialog, LAMBADA, XSum, HumanEval.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>No direct comparison between LLM-generated theories and human-generated theories is performed; the paper focuses on model-to-benchmark comparisons and fairness of those comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Zero/few-shot benchmarking can be invalidated by data contamination (benchmark leakage). Fixed prompts and single-run evaluations are sensitive to prompt leakage and can be gamed; reported scores can misrepresent true generalization ability when benchmarks overlap training data.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "Don't Make Your LLM an Evaluation Benchmark Cheater", 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6000.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6000.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Continual pre-training leakage experiment</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Continual pre-training of LLMs on evaluation benchmark data (training sets, test prompts, and test sets) to simulate benchmark leakage</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An empirical experimental procedure used in this paper where pre-trained LLMs are continually trained on benchmark training sets, and in stronger settings also on test prompts and test sets, to measure the effect of benchmark leakage on evaluation scores and downstream abilities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Continual training (further pre-training) of existing LLMs exclusively on benchmark training data and (in stronger settings) test prompts and test sets, followed by evaluation on the same benchmarks and on held-out tasks (text generation, code) using the standard metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Same task metrics as above (accuracy, ROUGE-L, pass@k); comparisons are made between baseline (no leaked training) and models after continual training on leaked data to quantify inflation and side effects.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-Neo-1.3B, phi-1.5 (1.3B), OpenLLaMA-3B (3B), LLaMA-2-7B (7B) used in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>General language/knowledge/reasoning tasks; used here to study evaluation integrity rather than domain-specific scientific theories, but the protocol models the risk when scientific-evaluation corpora leak into training.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>N/A (experiment is about contamination effects, not a specific scientific theory).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Continual training on leaked benchmark data substantially increases in-domain benchmark scores while harming out-of-domain abilities: e.g., MMLU and many QA/reasoning tasks increase, but performance on unrelated tasks (HellaSwag, GSM8k, summarization, code synthesis) often degrades. Instruction tuning on Alpaca/CodeAlpaca improved leaked models less than non-leaked models (HumanEval improvements ~80% of non-leaked counterparts). Specific numeric examples shown in paper tables (e.g., C3-Dialog doubling for GPT-Neo).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Same list as above; the leaked training data comprised training sets (and in stronger settings test prompts and test sets) from the selected benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>No human-theory comparison; shows how leaked training creates illusory superiority versus legitimately trained larger models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>This simulated leakage uses continual training rather than injecting leakage into initial pre-training; authors note compute limits and that injection during pre-training could yield different effects. Also does not explore fine-grained partial leakage proportions or unlabeled-only leaks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "Don't Make Your LLM an Evaluation Benchmark Cheater", 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6000.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6000.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Contamination detection (n-gram hash / token overlap)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>N-gram hash overlap (suggested n ≈ 13) and token-overlap contamination analyses</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Heuristic methods suggested to detect contamination between pre-training corpora and benchmark data by computing n-gram (e.g., 13-gram) hashes and token overlaps or by reporting overlap statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Compute n-gram hash signatures (authors suggest n=13) for evaluation examples and look for matches in pre-training data; calculate token overlap statistics; perform semantic relevance checking where possible.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Overlap counts / percentages, number of exact n-gram matches; contamination analysis reports accompanying benchmark submissions.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>General (applicable to datasets across domains, including scientific corpora).</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Not applicable (method for contamination detection rather than a theory).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Proposed as a recommended mitigation and reporting practice. The authors note such heuristics (ngram hashing, token overlap) can detect many exact-copy contaminations but explicitly acknowledge limitations: they may miss semantic-level leakage or paraphrased/derived content.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Intended to be applied to mainstream benchmarks (MMLU, etc.) and pre-training corpora (Pile, RedPajama, web crawls), though the authors did not compute the full contamination degrees for mainstream datasets in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>No human comparison; suggested as an automated audit that developers should report.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>N-gram/token overlap misses semantic or paraphrased leakage; full contamination checking against very large pre-training corpora is computationally expensive; semantic-level detection remains an open challenge.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "Don't Make Your LLM an Evaluation Benchmark Cheater", 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6000.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6000.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Automated metrics: accuracy / ROUGE-L / pass@k</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Task-specific automated evaluation metrics (accuracy for QA/reading/reasoning, ROUGE-L for summarization, pass@k for code synthesis)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard automated task metrics used throughout the experiments to quantify performance on specific benchmark types.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Compute per-task metric: accuracy for multiple-choice and QA tasks, ROUGE-L for summarization quality, pass@10 for code synthesis success; report per-task and aggregated scores.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Numeric metric values (percent accuracy, ROUGE-L score, pass@k success rate) used to compare models and measure the effect of leakage.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Models evaluated in paper (see previous entries).</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>NLP task domains; these metrics could be repurposed to evaluate factual correctness or fitness of LLM-generated hypotheses when ground-truth labels exist.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>N/A — metrics measure correspondence to benchmark ground truth rather than scientific explanatory power.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Observed that leakage increases these automated metrics on leaked tasks and often decreases them on unrelated tasks. Example: XSum ROUGE-L for OpenLLaMA-3B after leaked training dropped to 0.19; LLaMA-2-7B XSum ROUGE-L dropped to 0.25 (reported as weakened summarization after leaked training). HumanEval pass@10 also degraded after leakage in several models.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>XSum (ROUGE-L), LAMBADA (word prediction accuracy), HumanEval (pass@k), and the QA/reasoning datasets listed previously (accuracy).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>No human rating comparisons reported for these metrics in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>These metrics cannot detect whether high scores arise from memorization of leaked test data rather than genuine generalization; automated metrics don't capture novelty, explanatory power, or scientific plausibility.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "Don't Make Your LLM an Evaluation Benchmark Cheater", 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6000.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6000.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt diversity & averaging</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Evaluation with diverse test prompts and averaging across runs to reduce prompt sensitivity</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recommended mitigation to reduce sensitivity to particular prompts and to make reported benchmark results more robust by running multiple prompt variants and averaging outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Provide multiple test prompts per task (diverse wording/formats) and average evaluation metrics across these multiple runs to obtain a more stable estimate of model performance.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Stability/variance across prompt variants; averaged metric values used as final reported score.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>General; applicable to any task where prompt wording affects model outputs (including assessments of generated theories).</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Not applicable (evaluation protocol recommendation).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Advised as a best practice to reduce the influence of fixed prompts (which are vulnerable to prompt-leakage exploitation). The paper does not provide numeric experiments solely demonstrating prompt-averaging efficacy, but recommends averaging final results over multiple prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>General recommendation for benchmark maintainers to provide diverse prompt sets for their tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>No comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Prompt diversity mitigates prompt-sensitivity but does not solve contamination caused by direct overlap between training data and benchmark content; it also increases evaluation cost.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "Don't Make Your LLM an Evaluation Benchmark Cheater", 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6000.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6000.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-thought prompting (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-thought prompting for reasoning tasks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using chain-of-thought prompting to elicit multi-step reasoning from LLMs on mathematical and reasoning datasets (GSM8k, AQuA), reusing established CoT prompts from prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Apply chain-of-thought prompts (as in Wei et al. 2022) to instruct the model to produce step-by-step reasoning; evaluate final answer correctness (accuracy).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Accuracy on reasoning datasets (GSM8k, AQuA, CommonsenseQA) when chain-of-thought prompting is used.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Models in experiments (GPT-Neo, phi-1.5, OpenLLaMA, LLaMA-2).</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Mathematical and commonsense reasoning tasks; approach applicable to assessing reasoning steps in candidate scientific explanations but paper applies it to standard reasoning benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>N/A (applied prompting technique rather than a generated scientific theory).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Chain-of-thought prompting was used in evaluation and results demonstrate that leakage of task data still inflates the measured accuracy under CoT prompting; some unrelated tasks degraded after leaked training. Exact numeric CoT results are in Tables 1-2.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>GSM8k, AQuA, CommonsenseQA (CoT prompting applied).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>No human comparison of chains-of-thought or reasoning quality is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>CoT prompts themselves are sensitive — leakage of the specific CoT prompt (test prompt leakage) can give undue advantage, as authors show test-prompt leakage inflates scores. CoT metrics still rely on final-answer correctness and may not detect memorized reasoning traces.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "Don't Make Your LLM an Evaluation Benchmark Cheater", 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Proving test set contamination in black box language models <em>(Rating: 2)</em></li>
                <li>Time travel in llms: Tracing data contamination in large language models <em>(Rating: 2)</em></li>
                <li>Nlp evaluation in trouble: On the need to measure llm data contamination for each benchmark <em>(Rating: 2)</em></li>
                <li>Detecting pretraining data from large language models <em>(Rating: 1)</em></li>
                <li>Can we trust the evaluation on chatgpt? <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6000",
    "paper_id": "paper-265019021",
    "extraction_schema_id": "extraction-schema-121",
    "extracted_data": [
        {
            "name_short": "Benchmark-based evaluation (zero-/few-shot)",
            "name_full": "Benchmark-based zero-shot and few-shot evaluation on held-out test sets",
            "brief_description": "Standard practice of evaluating LLMs by running them (zero-shot or few-shot) on held-out benchmark datasets and reporting task metrics; used here to measure knowledge, reasoning, reading comprehension, summarization and code synthesis abilities.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_method": "Run LLMs in zero-shot or few-shot settings (local or API) on standard benchmarks, optionally with few-shot demonstrations or chain-of-thought prompts; results aggregated and reported (single-run or averaged across prompts).",
            "evaluation_criteria": "Task-specific automated metrics such as accuracy for QA/reading/reasoning, ROUGE-L for summarization, pass@k for code, and aggregate benchmark scores (e.g., MMLU percent).",
            "llm_model_name": "GPT-Neo-1.3B, phi-1.5 (1.3B), OpenLLaMA-3B, LLaMA-2-7B and larger LLaMA (13B/30B/65B) referenced",
            "theory_domain": "General knowledge, reasoning, reading comprehension, summarization and code synthesis tasks (general NLP/knowledge domains; applicable to assessing outputs that include scientific hypotheses but not specialized scientific experimental validation in this paper).",
            "theory_description": "Not applicable — the paper evaluates model capabilities on benchmark tasks rather than specific LLM-generated scientific theories; the same benchmark protocol would be used to evaluate LLM-generated theory statements by measuring agreement with held-out ground truth or target outputs.",
            "evaluation_results": "Benchmark leakage (training on benchmark training/test prompts/test sets) dramatically inflates reported performance. Examples: MMLU and many QA/reasoning scores increase substantially under leakage; C3-Dialog score for GPT-Neo-1.3B doubled from 24.18 to 48.62 after training on benchmark training sets; in extreme test-set leakage cases, small (1.3B) models can appear to outperform 65B models.",
            "benchmarks_or_datasets": "MMLU (5-shot), BoolQ, PIQA, HellaSwag, WinoGrande, ARC (Easy/Challenge), OpenBookQA, CommonsenseQA, GSM8k, AQuA, RACE-M/H, CoQA, CMRC2018, C3-Dialog, LAMBADA, XSum, HumanEval.",
            "comparison_to_human": "No direct comparison between LLM-generated theories and human-generated theories is performed; the paper focuses on model-to-benchmark comparisons and fairness of those comparisons.",
            "limitations_or_challenges": "Zero/few-shot benchmarking can be invalidated by data contamination (benchmark leakage). Fixed prompts and single-run evaluations are sensitive to prompt leakage and can be gamed; reported scores can misrepresent true generalization ability when benchmarks overlap training data.",
            "uuid": "e6000.0",
            "source_info": {
                "paper_title": "Don't Make Your LLM an Evaluation Benchmark Cheater",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Continual pre-training leakage experiment",
            "name_full": "Continual pre-training of LLMs on evaluation benchmark data (training sets, test prompts, and test sets) to simulate benchmark leakage",
            "brief_description": "An empirical experimental procedure used in this paper where pre-trained LLMs are continually trained on benchmark training sets, and in stronger settings also on test prompts and test sets, to measure the effect of benchmark leakage on evaluation scores and downstream abilities.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Continual training (further pre-training) of existing LLMs exclusively on benchmark training data and (in stronger settings) test prompts and test sets, followed by evaluation on the same benchmarks and on held-out tasks (text generation, code) using the standard metrics.",
            "evaluation_criteria": "Same task metrics as above (accuracy, ROUGE-L, pass@k); comparisons are made between baseline (no leaked training) and models after continual training on leaked data to quantify inflation and side effects.",
            "llm_model_name": "GPT-Neo-1.3B, phi-1.5 (1.3B), OpenLLaMA-3B (3B), LLaMA-2-7B (7B) used in experiments.",
            "theory_domain": "General language/knowledge/reasoning tasks; used here to study evaluation integrity rather than domain-specific scientific theories, but the protocol models the risk when scientific-evaluation corpora leak into training.",
            "theory_description": "N/A (experiment is about contamination effects, not a specific scientific theory).",
            "evaluation_results": "Continual training on leaked benchmark data substantially increases in-domain benchmark scores while harming out-of-domain abilities: e.g., MMLU and many QA/reasoning tasks increase, but performance on unrelated tasks (HellaSwag, GSM8k, summarization, code synthesis) often degrades. Instruction tuning on Alpaca/CodeAlpaca improved leaked models less than non-leaked models (HumanEval improvements ~80% of non-leaked counterparts). Specific numeric examples shown in paper tables (e.g., C3-Dialog doubling for GPT-Neo).",
            "benchmarks_or_datasets": "Same list as above; the leaked training data comprised training sets (and in stronger settings test prompts and test sets) from the selected benchmarks.",
            "comparison_to_human": "No human-theory comparison; shows how leaked training creates illusory superiority versus legitimately trained larger models.",
            "limitations_or_challenges": "This simulated leakage uses continual training rather than injecting leakage into initial pre-training; authors note compute limits and that injection during pre-training could yield different effects. Also does not explore fine-grained partial leakage proportions or unlabeled-only leaks.",
            "uuid": "e6000.1",
            "source_info": {
                "paper_title": "Don't Make Your LLM an Evaluation Benchmark Cheater",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Contamination detection (n-gram hash / token overlap)",
            "name_full": "N-gram hash overlap (suggested n ≈ 13) and token-overlap contamination analyses",
            "brief_description": "Heuristic methods suggested to detect contamination between pre-training corpora and benchmark data by computing n-gram (e.g., 13-gram) hashes and token overlaps or by reporting overlap statistics.",
            "citation_title": "",
            "mention_or_use": "mention",
            "evaluation_method": "Compute n-gram hash signatures (authors suggest n=13) for evaluation examples and look for matches in pre-training data; calculate token overlap statistics; perform semantic relevance checking where possible.",
            "evaluation_criteria": "Overlap counts / percentages, number of exact n-gram matches; contamination analysis reports accompanying benchmark submissions.",
            "llm_model_name": null,
            "theory_domain": "General (applicable to datasets across domains, including scientific corpora).",
            "theory_description": "Not applicable (method for contamination detection rather than a theory).",
            "evaluation_results": "Proposed as a recommended mitigation and reporting practice. The authors note such heuristics (ngram hashing, token overlap) can detect many exact-copy contaminations but explicitly acknowledge limitations: they may miss semantic-level leakage or paraphrased/derived content.",
            "benchmarks_or_datasets": "Intended to be applied to mainstream benchmarks (MMLU, etc.) and pre-training corpora (Pile, RedPajama, web crawls), though the authors did not compute the full contamination degrees for mainstream datasets in this paper.",
            "comparison_to_human": "No human comparison; suggested as an automated audit that developers should report.",
            "limitations_or_challenges": "N-gram/token overlap misses semantic or paraphrased leakage; full contamination checking against very large pre-training corpora is computationally expensive; semantic-level detection remains an open challenge.",
            "uuid": "e6000.2",
            "source_info": {
                "paper_title": "Don't Make Your LLM an Evaluation Benchmark Cheater",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Automated metrics: accuracy / ROUGE-L / pass@k",
            "name_full": "Task-specific automated evaluation metrics (accuracy for QA/reading/reasoning, ROUGE-L for summarization, pass@k for code synthesis)",
            "brief_description": "Standard automated task metrics used throughout the experiments to quantify performance on specific benchmark types.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_method": "Compute per-task metric: accuracy for multiple-choice and QA tasks, ROUGE-L for summarization quality, pass@10 for code synthesis success; report per-task and aggregated scores.",
            "evaluation_criteria": "Numeric metric values (percent accuracy, ROUGE-L score, pass@k success rate) used to compare models and measure the effect of leakage.",
            "llm_model_name": "Models evaluated in paper (see previous entries).",
            "theory_domain": "NLP task domains; these metrics could be repurposed to evaluate factual correctness or fitness of LLM-generated hypotheses when ground-truth labels exist.",
            "theory_description": "N/A — metrics measure correspondence to benchmark ground truth rather than scientific explanatory power.",
            "evaluation_results": "Observed that leakage increases these automated metrics on leaked tasks and often decreases them on unrelated tasks. Example: XSum ROUGE-L for OpenLLaMA-3B after leaked training dropped to 0.19; LLaMA-2-7B XSum ROUGE-L dropped to 0.25 (reported as weakened summarization after leaked training). HumanEval pass@10 also degraded after leakage in several models.",
            "benchmarks_or_datasets": "XSum (ROUGE-L), LAMBADA (word prediction accuracy), HumanEval (pass@k), and the QA/reasoning datasets listed previously (accuracy).",
            "comparison_to_human": "No human rating comparisons reported for these metrics in this paper.",
            "limitations_or_challenges": "These metrics cannot detect whether high scores arise from memorization of leaked test data rather than genuine generalization; automated metrics don't capture novelty, explanatory power, or scientific plausibility.",
            "uuid": "e6000.3",
            "source_info": {
                "paper_title": "Don't Make Your LLM an Evaluation Benchmark Cheater",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Prompt diversity & averaging",
            "name_full": "Evaluation with diverse test prompts and averaging across runs to reduce prompt sensitivity",
            "brief_description": "A recommended mitigation to reduce sensitivity to particular prompts and to make reported benchmark results more robust by running multiple prompt variants and averaging outcomes.",
            "citation_title": "",
            "mention_or_use": "mention",
            "evaluation_method": "Provide multiple test prompts per task (diverse wording/formats) and average evaluation metrics across these multiple runs to obtain a more stable estimate of model performance.",
            "evaluation_criteria": "Stability/variance across prompt variants; averaged metric values used as final reported score.",
            "llm_model_name": null,
            "theory_domain": "General; applicable to any task where prompt wording affects model outputs (including assessments of generated theories).",
            "theory_description": "Not applicable (evaluation protocol recommendation).",
            "evaluation_results": "Advised as a best practice to reduce the influence of fixed prompts (which are vulnerable to prompt-leakage exploitation). The paper does not provide numeric experiments solely demonstrating prompt-averaging efficacy, but recommends averaging final results over multiple prompts.",
            "benchmarks_or_datasets": "General recommendation for benchmark maintainers to provide diverse prompt sets for their tasks.",
            "comparison_to_human": "No comparison.",
            "limitations_or_challenges": "Prompt diversity mitigates prompt-sensitivity but does not solve contamination caused by direct overlap between training data and benchmark content; it also increases evaluation cost.",
            "uuid": "e6000.4",
            "source_info": {
                "paper_title": "Don't Make Your LLM an Evaluation Benchmark Cheater",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Chain-of-thought prompting (CoT)",
            "name_full": "Chain-of-thought prompting for reasoning tasks",
            "brief_description": "Using chain-of-thought prompting to elicit multi-step reasoning from LLMs on mathematical and reasoning datasets (GSM8k, AQuA), reusing established CoT prompts from prior work.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_method": "Apply chain-of-thought prompts (as in Wei et al. 2022) to instruct the model to produce step-by-step reasoning; evaluate final answer correctness (accuracy).",
            "evaluation_criteria": "Accuracy on reasoning datasets (GSM8k, AQuA, CommonsenseQA) when chain-of-thought prompting is used.",
            "llm_model_name": "Models in experiments (GPT-Neo, phi-1.5, OpenLLaMA, LLaMA-2).",
            "theory_domain": "Mathematical and commonsense reasoning tasks; approach applicable to assessing reasoning steps in candidate scientific explanations but paper applies it to standard reasoning benchmarks.",
            "theory_description": "N/A (applied prompting technique rather than a generated scientific theory).",
            "evaluation_results": "Chain-of-thought prompting was used in evaluation and results demonstrate that leakage of task data still inflates the measured accuracy under CoT prompting; some unrelated tasks degraded after leaked training. Exact numeric CoT results are in Tables 1-2.",
            "benchmarks_or_datasets": "GSM8k, AQuA, CommonsenseQA (CoT prompting applied).",
            "comparison_to_human": "No human comparison of chains-of-thought or reasoning quality is provided.",
            "limitations_or_challenges": "CoT prompts themselves are sensitive — leakage of the specific CoT prompt (test prompt leakage) can give undue advantage, as authors show test-prompt leakage inflates scores. CoT metrics still rely on final-answer correctness and may not detect memorized reasoning traces.",
            "uuid": "e6000.5",
            "source_info": {
                "paper_title": "Don't Make Your LLM an Evaluation Benchmark Cheater",
                "publication_date_yy_mm": "2023-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Proving test set contamination in black box language models",
            "rating": 2,
            "sanitized_title": "proving_test_set_contamination_in_black_box_language_models"
        },
        {
            "paper_title": "Time travel in llms: Tracing data contamination in large language models",
            "rating": 2,
            "sanitized_title": "time_travel_in_llms_tracing_data_contamination_in_large_language_models"
        },
        {
            "paper_title": "Nlp evaluation in trouble: On the need to measure llm data contamination for each benchmark",
            "rating": 2,
            "sanitized_title": "nlp_evaluation_in_trouble_on_the_need_to_measure_llm_data_contamination_for_each_benchmark"
        },
        {
            "paper_title": "Detecting pretraining data from large language models",
            "rating": 1,
            "sanitized_title": "detecting_pretraining_data_from_large_language_models"
        },
        {
            "paper_title": "Can we trust the evaluation on chatgpt?",
            "rating": 1,
            "sanitized_title": "can_we_trust_the_evaluation_on_chatgpt"
        }
    ],
    "cost": 0.0136925,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Don't Make Your LLM an Evaluation Benchmark Cheater
3 Nov 2023</p>
<p>Kun Zhou 
School of Information
Renmin University of China</p>
<p>Yutao Zhu ytzhu@ruc.edu.cn 
Gaoling School of Artificial Intelligence
Renmin University of China</p>
<p>Zhipeng Chen 
Gaoling School of Artificial Intelligence
Renmin University of China</p>
<p>Wentong Chen 
Gaoling School of Artificial Intelligence
Renmin University of China</p>
<p>Wayne Xin Zhao 
Gaoling School of Artificial Intelligence
Renmin University of China</p>
<p>Xu Chen xu.chen@ruc.edu.cn 
Yankai Lin yankailin@ruc.edu.cn 
Gaoling School of Artificial Intelligence
Renmin University of China</p>
<p>Ji-Rong Wen jrwen@ruc.edu.cn 
School of Information
Renmin University of China</p>
<p>Gaoling School of Artificial Intelligence
Renmin University of China</p>
<p>Jiawei Han hanj@illinois.edu 
Gaoling School of Artificial Intelligence
Renmin University of China</p>
<p>University of Illinois Urbana-Champaign</p>
<p>Don't Make Your LLM an Evaluation Benchmark Cheater
3 Nov 20236C1C0B78A6C832C800E10142FE9D26EBarXiv:2311.01964v1[cs.CL]
Large language models (LLMs) have greatly advanced the frontiers of artificial intelligence, attaining remarkable improvement in model capacity.To assess the model performance, a typical approach is to construct evaluation benchmarks for measuring the ability level of LLMs in different aspects.Despite that a number of high-quality benchmarks have been released, the concerns about the appropriate use of these benchmarks and the fair comparison of different models are increasingly growing.Considering these concerns, in this paper, we discuss the potential risk and impact of inappropriately using evaluation benchmarks and misleadingly interpreting the evaluation results.Specially, we focus on a special issue that would lead to inappropriate evaluation, i.e., benchmark leakage, referring that the data related to evaluation sets is occasionally used for model training.This phenomenon now becomes more common since pre-training data is often prepared ahead of model test.We conduct extensive experiments to study the effect of benchmark leverage, and find that it can dramatically boost the evaluation results, which would finally lead to an unreliable assessment of model performance.To improve the use of existing evaluation benchmarks, we finally present several guidelines for both LLM developers and benchmark maintainers.We hope this work can draw attention to appropriate training and evaluation of LLMs.</p>
<p>Introduction</p>
<p>Goodhart's Law: "When a measure becomes a target, it ceases to be a good measure."</p>
<p>Large language models (LLMs) have achieved remarkable success across a variety of real-world applications (Brown et al., 2020;Zhao et al., 2023;Zhu et al., 2023).By pre-training large Transformer models on massive text corpora, LLMs can possess excellent task-solving capacities, i.e., using zeroshot or few-shot prompting (Brown et al., 2020).</p>
<p>To better understand how LLMs evolve in model capacity, it becomes essential to construct reliable evaluation benchmarks to test the ability level of LLMs in various tasks, e.g., knowledge reasoning and math problem solving.</p>
<p>Recently, a surge of high-quality evaluation benchmarks (Hendrycks et al., 2021;Huang et al., 2023) have been proposed to provide a comprehensive capability evaluation of LLMs.Typical benchmarks include MMLU (Hendrycks et al., 2021) (for measuring multitask language understanding ability), Big-Bench (Srivastava et al., 2022) (for quantifying and extrapolating the capabilities of LLMs), and AGIEval (Zhong et al., 2023) (for evaluating the abilities of tackling human-level tasks).These benchmarks have made great efforts in creating or collecting test resources for evaluating the performance of LLMs.Based on these benchmarks, one can conveniently examine the effect of new training strategies or monitor the training status of LLMs (either pre-training or supervised finetuning).It has become common to report the results on these evaluation benchmarks for demonstrating the effectiveness of newly released LLMs (Ope-nAI, 2023;Touvron et al., 2023b;Anil et al., 2023).Furthermore, to compare the performance of dif-ferent LLMs, various leaderboards have been also created to rank LLMs according to their performance on existing or new evaluation benchmarks, such as OpenCompass (Contributors, 2023) and C-Eval (Huang et al., 2023).</p>
<p>Despite the wide use of these benchmarks and leaderboards, increasing concerns (Aiyappa et al., 2023;Li, 2023) are growing about the fairness and reliability in evaluating existing LLMs.A major issue is that the data contamination or leakage is likely to occur for large-scale benchmark evaluation, which means that LLMs are trained with relevant or exactly the same data for test.Such an issue could be unconsciously triggered, since we might be unaware of the future evaluation datasets when preparing the pre-training corpus.For example, GPT-3 has found that Children's Book Test dataset (Hill et al., 2016) was included in the pretraining corpus, and LLaMA-2 has mentioned that the contexts in BoolQ dataset (Clark et al., 2019) are extracted verbatim from the webpages, which may be included in the publicly available corpus.</p>
<p>Indeed, when conducting evaluation with existing benchmarks, the results of evaluated LLMs are mostly obtained by running them on local servers or via API calls.During this process, there is no strict checking on any potentially inappropriate ways (e.g., data contamination) that would cause an unnormal improvement of evaluation performance.To make matters worse, the detailed composition (e.g., data sources) of the training corpus is often regarded as the core "secret" of existing LLMs.Therefore, it becomes difficult to directly examine the contamination issues when performing the evaluation for benchmark maintainers.</p>
<p>Considering this issue, the aim of this paper is to draw attention on appropriately using existing evaluation benchmarks and avoiding any misleading behaviors in obtaining or interpreting the evaluation results.Specifically, we mainly focus on discussing the potential effect of benchmark leakage, which refers to the case that test data or relevant data (e.g., training set) has been included in the pre-training corpus.It would cause an unfair performance advantage when comparing different LLMs or assessing the ability level of some specific LLMs.As we discussed before, this issue tends to become increasingly more common as we try to collect more public text data for training.To investigate this issue, we set up several benchmark leakage settings that should be totally avoided during evaluation, including the leakage of training sets, test prompts, and test sets.Based on the three settings, we continually train four popular language models, ranging from 1.3B to 7B, and test the performance of the four models on a number of existing benchmarks.In addition, we also examine the potential risk of benchmark leakage on other abilities.</p>
<p>The experimental results reveal that benchmark leakage can lead to an unfair boost in the evaluation performance of LLMs.Smaller LLMs (e.g., a 1.3B model) can be deliberately elevated to outperform 10× larger models on certain tasks.As a side effect, the performance of these specially trained LLMs on other normally tested tasks would likely be adversely affected if we fine-tune or train the model only with these leaked data.</p>
<p>By examining the potential risks of benchmark leakage, we would like to emphasize the importance of fair and appropriate evaluation for LLMs, and propose several suggestions to improve the evaluation for LLMs:</p>
<p>• As general suggestions, more benchmarks from diverse sources, covering both basic ability (e.g., text generation) and advanced ability tests (e.g., complex reasoning), should be used for comprehensively estimating the capabilities of LLMs.</p>
<p>• As suggestions for LLM developers, it is important to perform the data decontamination checking between pre-training data and any related data (e.g., training and test sets) when using evaluation benchmarks.In addition, it is also necessary to report the contamination analysis on the evaluated benchmarks as reference.We also suggest reporting the detailed composition of the pre-training data.</p>
<p>• As suggestions for benchmark maintainers, we suggest that a diverse set of test prompts should be employed for reducing the influence of the prompt sensitivity.It is also meaningful to conduct the contamination analysis between the benchmark data and existing pretraining corpus, alerting any potential contamination risks.For evaluation, each submission is suggested to be accompanied with a special contamination analysis report.</p>
<p>to be unconsciously triggered (Oren et al., 2023;Sainz et al., 2023).It would violate regular evaluation settings for assessing zero/few-shot generalization capability, thus affecting the capability assessment of LLMs.To better understand the potential influence of the benchmark leakage issue, we conduct an empirical study that continually trains small-sized LLMs on three settings with different levels of information leakage.• Using All Training Sets with Test Prompt: all the training sets, with their corresponding test prompts, e.g., task description and few-shot demonstration, are used for training.</p>
<p>Experimental Setup</p>
<p>• Using All Training and Test Sets with Test Prompt: all the training sets, test prompts, and test sets of all the collected evaluation benchmarks are used for training.(CAUTION: this is the most extreme case, where all information is leaked.We conduct this experiment only for reference, and this should never occur.)</p>
<p>Evaluation Benchmark To make the empirical study, we select the widely-used benchmark MMLU and employ a number of questionanswering (QA), reasoning, and reading comprehension datasets for evaluation.</p>
<p>• MMLU: it has become one of the most commonly used evaluation benchmarks for LLMs' ability of world knowledge possessing and problem solving.It covers 57 tasks requiring diverse knowledge, such as math, history, science, and law.We report the 5-shot evaluation performance.</p>
<p>• Open-domain QA Tasks: we select seven open-domain QA datasets where LLMs should answer the question solely based on intrinsic knowledge.We report the accuracy of LLMs under the zero-shot setting, i.e., BoolQ (Clark et al., 2019), PIQA (Bisk et al., 2020), Hellaswag (Zellers et al., 2019), WinoGrande (Sakaguchi et al., 2020), ARC Easy and Challenge (Clark et al., 2018), Open-BookQA (Mihaylov et al., 2018).</p>
<p>• Reasoning Tasks: we select a commonsense reasoning dataset CommonsenseQA (Talmor et al., 2019), and two commonly-used mathematical reasoning datasets GSM8k (Cobbe et al., 2021) and AQuA (Ling et al., 2017) for evaluation.We use chain-of-thought prompting and reuse the prompts provided by Wei et al. (2022) for evaluation and report the accuracy of LLMs.</p>
<p>• Reading Comprehension Tasks: we select three English datasets RACE-Middle and RACE-High (Lai et al., 2017), CoQA (Reddy et al., 2019) and two Chinese datasets CMRC2018 (Cui et al., 2019) and C3-Dialog (Sun et al., 2020).As reading comprehension datasets have one paragraph and several QA pairs in a sample, we only test the accuracy of the last question and regard the paragraph and other QA pairs as the prompt.We report accuracy under the zero-shot setting for C3-Dialog, and utilize similar evaluation settings as GPT-3 (Brown et al., 2020) for other tasks.</p>
<p>Backbone LLMs</p>
<p>To thoroughly analyze the effect of benchmark leakage on the evaluation performance, we select the following models for evaluation, which have provided pre-training details or conducted careful data contamination analysis.</p>
<p>• GPT-Neo-1.3B(Black et al., 2021): it is a Transformer-based model with GPT-3 architecture, pre-trained on the Pile (Gao et al., 2021) dataset.</p>
<p>• phi-1.5 (Li et al., 2023): it is a 1.3B model trained on "textbook quality" data of ≈27B tokens, and can achieve comparable performance as much larger models.</p>
<p>• OpenLLaMA-3B (Geng and Liu, 2023): it is an open-source project to reproduce LLaMA model with a permissive license, pre-trained on RedPajama dataset (Computer, 2023)  • LLaMA-2-7B (Touvron et al., 2023b): it is an updated version of LLaMA (Touvron et al., 2023a).It has been pre-trained on a mixture of publicly available online data of 2T tokens.</p>
<p>Results and Analysis</p>
<p>We report the evaluation results of LLMs after training with the benchmark leakage settings in Table 1 and Table 2. Overall, different levels of data leakage result in inflated model performance on benchmarks.We have the following observations.First, we can see that using MMLU training set can greatly boost the evaluation results on the MMLU benchmark.However, this improvement comes at the cost of decreased performance on tasks unrelated to MMLU, (such as HellaSwag and GSM8k about commonsense and mathematical knowledge, respectively), suggesting that overemphasizing a specific task may lower the model generalization capability.Besides, when incorporating all the training sets of the evaluated benchmarks, there is a notable performance increase across almost all the evaluated tasks.Incorporating training data converts the original zero/few-shot evaluation into an in-domain test task, making it easier for LLMs to achieve higher results.An intriguing finding occurs when we examine the result on the Chinese benchmark C3-Dialog.Despite the pre-training corpus of the four LLMs containing very little Chinese data, using training sets doubles their evaluation scores, e.g., elevating GPT-Neo-1.3B'sscore from 24.18 to 48.62.This observation underscores the significance of avoiding training set leakage in pre-training, as it can lead to spurious performance improvements that distort the real assessment of model capabilities.</p>
<p>Second, the evaluation scores continue to rise as the data leakage becomes more severe.Remarkably, when the test prompts were leaked, smaller LLMs can even surpass much larger LLMs that were not trained with leaked data, e.g., "phi-1.5-1.3B+Alltest prompts.Furthermore, this observation raises concerns about the robustness of using fixed test prompts in the evaluation benchmark, as it may not be resilient to the aforementioned leakage risk.Finally, for reference, we examine the most extreme case where all test sets are leaked.The results are highlighted in grey font.As can be seen from these results, test data leakage significantly inflates benchmark performance, leading 1.3B LLMs to outperform 65B LLMs across most tasks.Evidently, this increase does not imply any improvement in capacity, but rather benchmark cheating.</p>
<p>Overall, benchmark leverage directly leads to an unfair advantage in evaluation results of the involved models, which should be strictly avoided when conducting any evaluation.</p>
<p>Potential Risk of Benchmark Leakage</p>
<p>In addition to the inflated performance that undermines the reliability of capability estimation, we also investigate whether the benchmark leakage issue would lead to potential risks in model capacity.Limited by the training compute, we can not conduct an exact checking that directly includes leakage data in pre-training data.Instead, we continually pre-train the LLMs on the training sets of  all the selected evaluation benchmarks as in Section 2, without the mixture of any other data.Such a way is the most direct way for benchmark cheating (should be avoided).We speculate that it is likely to affect the capacities of LLMs on normally tested tasks (those without data leakage), due to "catastrophe forgetting" (Luo et al., 2023;Goodfellow et al., 2013).2</p>
<p>Effect on the Performance of Other Tasks</p>
<p>After training on the leaked benchmark data, it would potentially mislead LLMs to overemphasize the specific knowledge and output style of the benchmark data, thereby potentially affecting their performance on other tasks.In this part, we conduct empirical experiments to examine the side effect on the model performance of other tasks.</p>
<p>Experimental Setup To validate the effect, we select three tasks that are not involved in the leaked training data, consisting of two text generation tasks, i.e., LAMBADA (Paperno et al., 2016) and XSum (Narayan et al., 2018), and a code synthesis task HumanEval (Chen et al., 2021) to evaluate LLMs in the zero-shot setting.LAMBADA is a language modeling task that tests the ability of LLMs to predict the last word based on the context, and we report the accuracy in predicting words.XSum, on the other hand, is a text summarization task that requires LLM to summarize the key information from long documents.For this task, we report the ROUGE-L metric, which measures the quality of the generated summaries by comparing them with the ground-truth summaries.For HumanEval, we adopt pass@10 as the evaluation metric.</p>
<p>Results Analysis</p>
<p>We show the results of LLMs with and without benchmark leakage on the three evaluation tasks in Table 3. First, we can observe that after training on the leaked data, the performance of all LLMs degrades on the two text generation datasets.Specifically, for OpenLLaMA-3B and LLaMA-2-7B, their text summarization abilities seem to be weakened after training on the leaked data, resulting in Rouge-L scores of 0.19 and 0.25 in XSum, respectively.Besides, by comparing the performance on HumanEval, we also see that data leakage primarily leads to performance degradation of LLMs in the code synthesis task.This demonstrates that benchmark leakage may have a negative impact on the performance of these normally tested tasks (without data leverage).</p>
<p>Effect on Model Adaptation</p>
<p>After training on the leaked data, LLMs are trained to be specially fit for the benchmark data.However, LLMs might need to be further fine-tuned for attaining some specific goals (e.g., solving new tasks or serving emergent applications).In this part, we examine how inappropriately trained LLMs perform for subsequent adaptation.Experimental Setup To investigate the influence of data leakage on LLMs' adaptation capability, we select two representative instruction datasets, i.e., Alpaca (Taori et al., 2023) and CodeAlpaca (Chaudhary, 2023).Both of these datasets are synthetic and generated using the Self-Instruct method.For comparison, Alpaca primarily contains natural language instructions, whereas CodeAlpaca focuses on code generation instructions.We use these datasets to fine-tune the LLMs with or without training on the leaked data, and subsequently evaluate their performance on the previously mentioned text generation and code synthesis tasks.</p>
<p>Results Analysis</p>
<p>In Table 4, by comparing the performance of the instruction-tuned LLMs (+Alpaca or +CodeAlpaca) with and without training on the leaked data, we can see that the models with benchmark leakage still underperform their nonleaked counterparts.For the HumanEval dataset, the performance improvements of instruction tuning for LLMs trained with leaked data only reach approximately 80% of those achieved by models that are not trained on leaked data.This indicates that benchmark leakage may lead to a decline in adaptation capability, constraining the LLMs' ability to adapt or improve through subsequent fine-tuning processes.Note that this finding is derived when we fine-tune LLMs only with the leaked data.To enhance the current findings, it is also meaningful to conduct experiments that either include leaked data into pre-training data or mix leaked data with other instruction data.However, since our main purpose is to reveal that benchmark leverage might cause severe side effects on LLMs in addition to spurious performance improvement, we omit these experiments due to the compute limit.</p>
<p>Discussion</p>
<p>In light of the potential risks of benchmark leakage, it is necessary to revisit the existing evaluation settings for LLMs and investigate possible strategies to avoid such data contamination issues.</p>
<p>4.1 Fairness in Evaluating Zero/Few-shot Generalization Ability</p>
<p>Based on our empirical findings in previous sections, the evaluation results of LLMs in specific benchmarks can be dramatically boosted when the related or same data of the test tasks is accidentally used for training.In the literature of machine learning, zero/few-shot learning often refers that the samples at test time were not observed during training for a learner (Wang et al., 2021;Xian et al., 2019).It is evident that benchmark leverage does not comply with this requirement, making it unfair to compare different LLMs when such a case exists.Furthermore, data leverage can also bring an unfair advantage in the few-shot setting since the learner can observe more task-relevant data at training time.</p>
<p>In case of data leakage, the original zeroshot/few-shot generalization task would degenerate into much easier in-domain evaluation tasks, and it would intensify the phenomenon of benchmark hacking, i.e., a benchmark is no longer useful for evaluation due to the high performance of the involved comparison methods.</p>
<p>However, in practice, it is challenging to fully eliminate the leakage risk from model training (Golchin and Surdeanu, 2023;Shi et al., 2023).It is because an evaluation benchmark is often conducted based on some public text sources, e.g., webpages and scientific papers.In this case, the related data (e.g., the original text used to generate the test problems) might be occasionally included in the pre-training data of LLMs.Although existing evaluation datasets are easy to be excluded from pre-training data for training new LLMs, it is still difficult to identify all potential data dependencies between evaluation benchmarks and pre-training corpus.Such a test set contamination problem has been already noted in black-box language models (Oren et al., 2023).</p>
<p>Suggestion for LLM Evaluation</p>
<p>Based on these discussions, we propose the following suggestions to improve existing capacity evaluation for LLMs.</p>
<p>General suggestions:</p>
<p>• Considering the potential risk associated with benchmark leakage, we recommend the use of a broader range of benchmarks from diverse sources for performance evaluation.This can help mitigate the risk of inflated results due to data contamination.If feasible, incorporating manual evaluation and conducting qualitative analysis would be also beneficial.</p>
<p>• In addition to evaluating the advanced capabilities of LLMs (such as reasoning and factual knowledge), it is also necessary to perform evaluations on other datasets that focus on basic abilities, such as text generation.This comprehensive approach is necessary for a thorough estimation of LLMs' capabilities.</p>
<p>Suggestions for LLM developers:</p>
<p>• Perform strict checking on data decontamination in pre-training data to avoid any subsequent evaluation data being included during training.To achieve this, the n-gram (generally, n = 13) hash algorithm can be applied to examine the overlap between pre-training data and evaluation data of some specific task.</p>
<p>• If possible, we suggest also excluding training data of mainstream evaluation benchmarks from pre-training data.</p>
<p>• Indicate any potential risk of data contamination (if any) and report the contamination analysis (e.g., overlap statistics) when you present the results on some evaluation benchmark.An example can be seen in Llama-2's report (Touvron et al., 2023b).</p>
<p>• Report a more detailed composition of the pretraining data, especially the datasets related to mainstream evaluation benchmarks.It is an important reference for checking the potential data leakage risk by the public audience.</p>
<p>Suggestions for benchmark maintainers:</p>
<p>• Provide the detail of the data source for constructing the benchmark, and conduct the contamination analysis of the current dataset with mainstream pre-training corpora (as many as possible).The benchmark should explicitly alert possible contamination risks for commonly used pre-training datasets.</p>
<p>• Each submission is suggested to be accompanied with a specific contamination analysis report from the result provider, where it can perform semantic relevance checking (e.g., overlap statistics) between pre-training data and evaluation data (both training and test data).</p>
<p>• Provide a diverse set of prompts for testing.</p>
<p>The final evaluation results should be averaged over these multiple runs.It can help reduce the sensitivity of specific prompts, and enhance the reliability of the model results.</p>
<p>Conclusion</p>
<p>In this paper, we conducted empirical studies to investigate the penitential risk and impact of benchmark leakage on LLM evaluation.We found that data leakage can largely boost the benchmark results of LLMs (even small models), making the evaluation unfair and untrustworthy.These findings suggest that such attempts should be strictly avoided for fairly assessing the model performance on evaluation benchmarks.Despite that this issue is hard to be fully eliminated from the pre-training stage, we suggest several useful guidelines to improve the use of existing evaluation benchmarks.A key point is that both LLM developers and benchmark maintainers should be aware of the data contamination issue when interpreting and using the results from the performance leaderboards.In practice, several heuristic strategies can be useful to detect such potential contamination issues, e.g., calculating the token overlap between training and evaluation data.Besides, we also suggest benchmark test should be conducted with multiple task prompts for deriving a more stable and reliable model performance.</p>
<p>This work aims to draw the attention of the research community to the appropriate use of existing evaluation benchmarks for LLMs.More meaningful work can be conducted following this line, e.g., alerting the potential contamination datasets.</p>
<p>Limitation</p>
<p>In this work, we conducted preliminary experiments to emphasize the potential risks associated with benchmark leakage in training LLMs.However, there are still several limitations in our study.</p>
<p>First, our experiments involved continually training existing pre-trained LLMs with leaked data.We do not have sufficient computational resources to investigate the impact when directly incorporating benchmark leakage during the pre-training process.Given that the pre-training dataset is significantly larger than the benchmark data, introducing data leakage during pre-training might yield different findings.Nonetheless, we strongly recommend avoiding this situation as it would breaks the nature of zero-shot/few-shot evaluation.</p>
<p>Second, we did not explore more fine-grained data leakage scenarios in this study, such as only leaking training examples without labels and varying the proportion of the leaked dataset.We encourage more research efforts into this issue with more systematic studies.</p>
<p>Third, we did not calculate the degree of contamination between the mainstream benchmarks and commonly-used pre-training datasets, which could serve as an important reference for alerting LLM developers to adjust their evaluation settings.While we suggest that developers and benchmark maintainers report contamination analyses, accurately and efficiently estimating the contamination risk of each example in the benchmark is also a challenging task.For example, the suggested ngram hash algorithm may not detect semantic-level knowledge leakage risks.</p>
<p>Figure 1 :
1
Figure 1: Illustration of the potential risk of data leakage.Once the pre-training data with overlap to the benchmark data is used for training LLM, its benchmark performance would be greatly increased.</p>
<p>Table 1 :
1
of over 1.2T tokens.The comparison among three benchmark leakage settings and the original LLMs on MMLU and QA tasks.
BackboneTraining SettingMMLU BoolQ PIQA HSwagWGARC-E ARC-C OBQALLaMA-13B (None)46.9076.7079.7060.0073.0079.0049.4034.60LLaMA-30B (None)57.8083.3980.6363.3976.0880.5551.6236.40LLaMA-65B (None)64.5085.4081.7064.9077.2080.8052.3038.40(None)24.0462.5770.5738.6555.7255.9823.2921.40GPT-Neo (1.3B)+MMLU Train S +All Train S +All Train S+Test P35.84 35.10 36.1557.89 78.32 76.9168.39 68.61 73.7237.27 42.46 42.7552.17 61.72 64.2550.93 63.68 64.3927.39 33.36 34.1320.40 29.40 31.80+All Train S+Test P&amp;S52.2587.2585.9662.9880.6688.1770.3163.20(None)42.8774.3476.5047.9973.5675.8444.9738.40phi-1.5 (1.3B)+MMLU Train S +All Train S +All Train S+Test P46.08 45.20 46.8074.37 82.35 82.7276.50 74.37 74.2747.80 54.64 54.5573.09 69.46 70.5675.93 75.00 75.0048.63 47.87 47.1840.00 42.40 39.80+All Train S+Test P&amp;S75.0592.6097.5577.8896.0597.4792.9294.20(None)26.4966.5174.8149.4260.8569.5733.8726.60OpenLLaMA (3B)+MMLU Train S +All Train S +All Train S+Test P43.12 44.86 48.3174.10 85.41 85.5771.22 76.82 76.5047.28 54.42 54.3462.43 71.11 72.3058.92 72.26 71.8035.41 41.55 41.6432.00 42.00 40.80+All Train S+Test P&amp;S87.3197.5598.2697.6196.3799.1697.8796.20(None)42.9571.6870.7855.3467.9672.5241.3032.20LLaMA-2 (7B)+MMLU Train S +All Train S +All Train S+Test P51.61 52.15 56.0481.96 88.72 87.8669.64 79.05 79.1149.46 61.08 61.1970.64 79.95 76.5661.87 76.60 76.6436.52 49.49 50.2636.80 48.00 45.00+All Train S+Test P&amp;S96.3499.0899.6299.4797.4799.5499.2399.40
"Train S", "Test P" and "Test P&amp;S" denote the data leakage scenarios that use the training set, test prompt, and both test set and test prompt during training, respectively.The task abbreviations are as follows: HSwag (Hellaswag), WG (WinoGrande), ARC-E (ARC-Easy), ARC-C (ARC-Challenge), and OBQA (OpenBookQA).The results in gray are the worst leakage setting using all the test sets and are reported only for reference.The best results in each group are in bold except for the aforementioned worst case.</p>
<p>Table 2 :
2
Train S+Test P" outperforms LLaMA-65B on RACE-M (55.80 vs. 53.00)and RACE-H (52.82 vs. 48.00).This highlights the significance of the test prompt as valuable information from the evaluation benchmark, since it contains the detailed input format during test.During training LLMs, it is suggested to avoid such special learning with
BackboneTraining SettingCSQA GSM8k AQuA RACE-M RACE-H CoQA CMRCC3LLaMA-13B (None)62.7018.8019.3046.4043.9058.7019.50 41.40LLaMA-30B (None)70.8035.1015.3549.7044.7062.0024.20 57.80LLaMA-65B (None)77.9048.9035.0053.0048.0065.8029.30 71.40(None)18.432.0518.1136.1934.8330.350.0024.18GPT-Neo (1.3B)+MMLU Train S +All Train S +All Train S+Test P20.39 18.26 30.470.08 0.76 5.7619.29 17.32 20.4735.91 49.45 51.9332.63 44.02 45.260.20 33.67 13.871.17 1.56 1.1740.48 48.62 47.62+All Train S+Test P&amp;S 32.023.1114.9673.2073.4912.151.5657.46(None)41.9328.5121.2641.7138.7631.570.3924.97phi-1.5 (1.3B)+MMLU Train S +All Train S +All Train S+Test P37.92 18.67 33.5810.24 14.94 19.2622.05 14.96 18.5048.07 54.42 55.8047.85 52.34 52.8210.85 7.27 8.250.39 0.00 0.7842.91 53.39 53.17+All Train S+Test P&amp;S 34.1522.8220.8779.2881.915.031.9567.04(None)23.753.3419.2944.7540.1054.973.5224.81OpenLLaMA (3B)+MMLU Train S +All Train S +All Train S+Test P47.99 61.02 68.470.00 9.10 17.8223.62 29.92 29.1341.44 57.18 58.8437.61 55.12 54.160.63 54.67 60.730.00 12.50 53.97 49.37 9.77 52.65+All Train S+Test P&amp;S 94.1929.4257.0997.2497.9979.9532.03 79.05(None)55.6912.9614.1728.4538.4725.888.9837.72LLaMA-2 (7B)+MMLU Train S +All Train S +All Train S+Test P57.25 69.62 77.152.43 23.88 30.1725.59 33.46 35.4334.25 61.88 58.8434.07 57.03 58.560.00 57.70 63.780.00 24.22 78.31 78.10 28.12 78.62+All Train S+Test P&amp;S 99.3437.6063.7899.4599.6281.5268.75 98.62
The comparison among different benchmark leakage settings and the original LLMs on reasoning and reading comprehension tasks.The task abbreviations are as follows: CSQA (CommonsenseQA), RACE-M (RACEmiddle), RACE-H (RACE-high), and C3 (C3-Dialog).</p>
<p>Table 3 :
3
The comparison among LLMs on two text generation and a code synthesis tasks."Leak" denotes the data leakage scenario using all training sets of the benchmarks in Section 2. LAMB and HEval refer to the LAMBADA and HumanEval datasets, respectively.The best results in each group are in bold.</p>
<p>Table 4 :
4
The comparison among LLMs after instruction tuning."Leak" denotes the data leakage using all training sets of the benchmarks in Section 2. "IT" denotes the instruction tuning using Alpaca and CodeAlpaca for text generation and code synthesis tasks, respectively.
BackboneTrainingLAMB XSum HEvalGPT-Neo+IT45.408.3414.24(1.3B)+Leak+IT43.508.2512.20OpenLLaMA+IT54.003.509.15(3B)+Leak+IT46.202.616.71LLaMA-2+IT60.308.6428.66(7B)+Leak+IT53.608.5520.73
https://github.com/hendrycks/test. The auxiliary training set contains data collected from several questionanswering benchmarks such as ARC, OBQA, and RACE.
As it is a very extreme scenario for simulation, we only employ it to explore the possibility of the subsequent impact when benchmark leakage occurs. The experiment procedure should be totally avoided in real training and evaluation.</p>
<p>Rachith Aiyappa, Jisun An, Haewoon Kwak, Yong-Yeol Ahn, arXiv:2303.12767Can we trust the evaluation on chatgpt?. 2023arXiv preprint</p>
<p>. Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernández Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan A Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A Choquette-Choo, Aakanksha Chowdhery, Clément Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark Díaz, Nan Du, Ethan Dyer, Vladimir Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, 10.48550/ARXIV.2305.104032023Palm 2 technical report. CoRR, abs/2305.10403</p>
<p>The Thirty-Second Innovative Applications of Artificial Intelligence Conference. Yonatan Bisk, Rowan Zellers, Le Ronan, Jianfeng Bras, Yejin Gao, Choi, 10.5281/zenodo.5297715Connor Leahy, and Stella Biderman. 2021. GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow. New York, NY, USA; Sid Black, Leo Gao, Phil WangAAAI Press2020. February 7-12, 20202020The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence. If you use this software, please cite it using these metadata</p>
<p>Alec Radford, Ilya Sutskever, and Dario Amodei. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems. Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish; NeurIPS2020. 2020. 2020. December 6-12, 2020Language models are few-shot learners</p>
<p>Code alpaca: An instructionfollowing llama model for code generation. Sahil Chaudhary, 2023</p>
<p>. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pondé De Oliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet ; Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Felipe Petroski Such. Joshua Achiam, Vedant Misra, Evan Morikawa, Alec RadfordJan LeikeIlya Sutskever, and Wojciech Zaremba. 2021. Evaluating large language models trained on code. CoRR, abs/2107.03374</p>
<p>Boolq: Exploring the surprising difficulty of natural yes/no questions. Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, Kristina Toutanova, 10.18653/V1/N19-1300Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019Minneapolis, MN, USAAssociation for Computational Linguistics2019. June 2-7, 20191</p>
<p>Think you have solved question answering? try arc, the AI2 reasoning challenge. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, Oyvind Tafjord, CoRR, abs/1803.054572018</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, CoRR, abs/2110.141682021</p>
<p>Redpajama-data: An open source recipe to reproduce llama training dataset. 2023Together Computer</p>
<p>Opencompass: A universal evaluation platform for foundation models. 2023</p>
<p>A span-extraction dataset for chinese machine reading comprehension. Yiming Cui, Ting Liu, Wanxiang Che, Li Xiao, Zhipeng Chen, Wentao Ma, Shijin Wang, Guoping Hu, 10.18653/V1/D19-1600Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019Hong Kong, ChinaAssociation for Computational Linguistics2019. November 3-7, 2019</p>
<p>The pile: An 800gb dataset of diverse text for language modeling. Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, Connor Leahy, CoRR, abs/2101.000272021</p>
<p>Openllama: An open reproduction of llama. Xinyang Geng, Hao Liu, 2023</p>
<p>Shahriar Golchin, Mihai Surdeanu, arXiv:2308.08493Time travel in llms: Tracing data contamination in large language models. 2023arXiv preprint</p>
<p>An empirical investigation of catastrophic forgetting in gradientbased neural networks. Ian J Goodfellow, Mehdi Mirza, Da Xiao, Aaron Courville, Yoshua Bengio, 10.48550/ARXIV.1312.6211CoRR, abs/1312.62112013</p>
<p>Measuring massive multitask language understanding. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, 9th International Conference on Learning Representations, ICLR 2021, Virtual Event. Austria2021. May 3-7, 2021OpenReview.net</p>
<p>The goldilocks principle: Reading children's books with explicit memory representations. Felix Hill, Antoine Bordes, Sumit Chopra, Jason Weston, 4th International Conference on Learning Representations, ICLR 2016. San Juan, Puerto Rico2016. May 2-4, 2016Conference Track Proceedings</p>
<p>Maosong Sun, and Junxian He. 2023. C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models. Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, Yao Fu, 10.48550/ARXIV.2305.08322abs/2305.08322CoRR</p>
<p>RACE: large-scale reading comprehension dataset from examinations. Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, Eduard H Hovy, 10.18653/V1/D17-1082Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. the 2017 Conference on Empirical Methods in Natural Language ProcessingCopenhagen, Denmark2017. 2017. September 9-11, 2017Association for Computational Linguistics</p>
<p>Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee. 2023. Textbooks are all you need II: phi-1.5 technical report. Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, 10.48550/ARXIV.2309.05463CoRR, abs/2309.05463</p>
<p>An open source data contamination report for llama series models. Yucheng Li, 10.48550/ARXIV.2310.17589CoRR, abs/2307.031092023</p>
<p>Program induction by rationale generation: Learning to solve and explain algebraic word problems. Wang Ling, Dani Yogatama, Chris Dyer, Phil Blunsom, 10.18653/V1/P17-1015Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaAssociation for Computational Linguistics2017. 2017. July 30 -August 41</p>
<p>An empirical study of catastrophic forgetting in large language models during continual fine-tuning. Yun Luo, Zhen Yang, Fandong Meng, Yafu Li, Jie Zhou, Yue Zhang, 10.48550/ARXIV.2308.08747CoRR, abs/2308.087472023</p>
<p>Can a suit of armor conduct electricity? A new dataset for open book question answering. Todor Mihaylov, Peter Clark, Tushar Khot, Ashish Sabharwal, 10.18653/V1/D18-1260Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational Linguistics2018. October 31 -November 4, 2018</p>
<p>Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization. Shashi Narayan, Shay B Cohen, Mirella Lapata, 10.18653/V1/D18-1206Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational Linguistics2018. October 31 -November 4. 2018</p>
<p>10.48550/ARXIV.2303.08774CoRR, abs/2303.08774GPT-4 technical report. 2023OpenAI</p>
<p>Proving test set contamination in black box language models. Yonatan Oren, Nicole Meister, Niladri Chatterji, Faisal Ladhak, Tatsunori B Hashimoto, 10.48550/ARXIV.2310.17623CoRR, abs/2307.031092023</p>
<p>The LAMBADA dataset: Word prediction requiring a broad discourse context. Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Ngoc Quan, Raffaella Pham, Sandro Bernardi, Marco Pezzelle, Gemma Baroni, Raquel Boleda, Fernández, 10.18653/V1/P16-1144Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016. Long Papers. the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016Berlin, GermanyThe Association for Computer Linguistics2016. August 7-12, 20161</p>
<p>Coqa: A conversational question answering challenge. Siva Reddy, Danqi Chen, Christopher D Manning, 10.1162/TACL_A_00266Trans. Assoc. Comput. Linguistics. 72019</p>
<p>Oier Lopez de Lacalle, and Eneko Agirre. Oscar Sainz, Jon Ander Campos, Iker García-Ferrero, Julen Etxaniz, arXiv:2310.18018Nlp evaluation in trouble: On the need to measure llm data contamination for each benchmark. 2023arXiv preprint</p>
<p>Winogrande: An adversarial winograd schema challenge at scale. Keisuke Sakaguchi, Le Ronan, Chandra Bras, Yejin Bhagavatula, Choi, 10.1609/AAAI.V34I05.6399The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference. New York, NY, USAAAAI Press2020. February 7-12, 20202020The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence</p>
<p>Weijia Shi, Anirudh Ajith, Mengzhou Xia, Yangsibo Huang, Daogao Liu, Terra Blevins, Danqi Chen, Luke Zettlemoyer, arXiv:2310.16789Detecting pretraining data from large language models. 2023arXiv preprint</p>
<p>Antonio Norelli, Anu Venkatesh, Arash Gholamidavoodi, Arfa Tabassum, Arul Menezes. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal, Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, Agnieszka Kluska, Aitor Lewkowycz, Akshat Agarwal, Alethea Power, Alex Ray, Alex Warstadt, Alexander W Kocurek, Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Parrish, Allen Nie, Aman Hussain, Amanda Askell, Amanda Dsouza, Ameet Rahane, Anantharaman S Iyer, Anders Andreassen, Andrea Santilli, Andreas Stuhlmüller, Andrew M Dai, Andrew La, Andrew K Lampinen, Andy Zou, Angela Jiang, Angelica Chen, Anh Vuong, Animesh Gupta, Anna Gottardi, 10.48550/ARXIV.2206.04615Ayla KarakasArun Kirubarajan, Asher Mullokandov, Ashish Sabharwal, Austin Herrick, Avia Efrat, Aykut Erdemand et al. 2022. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. CoRR, abs/2206.04615</p>
<p>Investigating prior knowledge for challenging chinese machine reading comprehension. Kai Sun, Dian Yu, Dong Yu, Claire Cardie, 10.1162/TACL_A_00305Trans. Assoc. Comput. Linguistics. 82020</p>
<p>Commonsenseqa: A question answering challenge targeting commonsense knowledge. Alon Talmor, Jonathan Herzig, Nicholas Lourie, Jonathan Berant, 10.18653/V1/N19-1421Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019Minneapolis, MN, USAAssociation for Computational Linguistics2019. June 2-7, 20191</p>
<p>Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, Stanford alpaca: An instruction-following llama model. 2023</p>
<p>Edouard Grave, and Guillaume Lample. 2023a. Llama: Open and efficient foundation language models. Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Aurélien Azhar, Armand Rodriguez, Joulin, 10.48550/ARXIV.2302.13971CoRR, abs/2302.13971</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov ; Zheng Yan, Iliyan Zarov, Yuchen Zhang, 10.48550/ARXIV.2307.09288Pushkar Mishra, Igor Molybog. Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing , Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Angela Fan, Melanie Kambadur, Sharan Narang; Robert Stojnic, Sergey EdunovAurélien Rodriguezand Thomas Scialom. 2023b. Llama 2: Open foundation and fine-tuned chat models. CoRR, abs/2307.09288</p>
<p>Generalizing from a few examples: A survey on few-shot learning. Yaqing Wang, Quanming Yao, James T Kwok, Lionel M Ni, 10.1145/3386252ACM Comput. Surv. 533342021</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H Chi, V Quoc, Denny Le, Zhou, 2022In NeurIPS</p>
<p>Zero-shot learning -A comprehensive evaluation of the good, the bad and the ugly. Yongqin Xian, Christoph H Lampert, 10.1109/TPAMI.2018.2857768Bernt Schiele, and Zeynep Akata. 201941</p>
<p>Hellaswag: Can a machine really finish your sentence?. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, Yejin Choi, 10.18653/V1/P19-1472Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019. Long Papers. the 57th Conference of the Association for Computational Linguistics, ACL 2019Florence, Italy2019. July 28-August 2, 20191Association for Computational Linguistics</p>
<p>A survey of large language models. Kun Wayne Xin Zhao, Junyi Zhou, Tianyi Li, Xiaolei Tang, Yupeng Wang, Yingqian Hou, Beichen Min, Junjie Zhang, Zican Zhang, Yifan Dong, Chen Du, Yushuo Yang, Zhipeng Chen, Jinhao Chen, Ruiyang Jiang, Yifan Ren, Xinyu Li, Zikang Tang, Peiyu Liu, Jian-Yun Liu, Ji-Rong Nie, Wen, 10.48550/ARXIV.2303.18223CoRR, abs/2303.182232023</p>
<p>Agieval: A human-centric benchmark for evaluating foundation models. Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, Nan Duan, 10.48550/ARXIV.2304.06364abs/2304.063642023CoRR</p>
<p>Large language models for information retrieval: A survey. Yutao Zhu, Huaying Yuan, Shuting Wang, Jiongnan Liu, Wenhan Liu, Chenlong Deng, Zhicheng Dou, Ji-Rong Wen, 10.48550/ARXIV.2308.07107CoRR, abs/2308.071072023</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1028 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1028</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1028</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-24.html">extraction-schema-24</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <p><strong>Paper ID:</strong> paper-266348538</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2312.10557v1.pdf" target="_blank">Improving Environment Robustness of Deep Reinforcement Learning Approaches for Autonomous Racing Using Bayesian Optimization-based Curriculum Learning</a></p>
                <p><strong>Paper Abstract:</strong> Deep reinforcement learning (RL) approaches have been broadly applied to a large number of robotics tasks, such as robot manipulation and autonomous driving. However, an open problem in deep RL is learning policies that are robust to variations in the environment, which is an important condition for such systems to be deployed into real-world, unstructured settings. Curriculum learning is one approach that has been applied to improve generalization performance in both supervised and reinforcement learning domains, but selecting the appropriate curriculum to achieve robustness can be a user-intensive process. In our work, we show that performing probabilistic inference of the underlying curriculum-reward function using Bayesian Optimization can be a promising technique for finding a robust curriculum. We demonstrate that a curriculum found with Bayesian optimization can outperform a vanilla deep RL agent and a hand-engineered curriculum in the domain of autonomous racing with obstacle avoidance. Our code is available at https://github.com/PRISHIta123/Curriculum_RL_for_Driving.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1028.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1028.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PPO-BO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Proximal Policy Optimization agent trained with Bayesian Optimization-based curriculum</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simulated 2D autonomous racing agent trained with PPO using curricula found by Bayesian Optimization over changepoints that vary road curvature and obstacle density; evaluated for robustness across a grid of easy-to-hard environments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>PPO agent (Bayesian Optimization curriculum)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An on-policy deep reinforcement learning agent using Proximal Policy Optimization (PPO) with CNN policy and value networks; inputs are top-down RGB bird's-eye-view images, outputs are (steering, acceleration, brake) control tuples. The training curriculum (sequence of environment parameter changepoints) was selected by Bayesian Optimization over curriculum changepoint space (k=3).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Modified OpenAI Gym CarRacing with obstacles</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>A 2D closed-circuit road navigation domain with static rectangular obstacles placed probabilistically on road tiles and grassy off-road areas; objective is to complete laps quickly while avoiding collisions. Environment difficulty is controlled by two parameters: road turn rate (κ, higher → higher curvature) and obstacle probability per tile (p). Tracks typically contain ~332 tiles per lap.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Road curvature (turn rate κ) and obstacle density (probability p that a road tile contains an obstacle); numerical ranges used: κ ∈ {0.31,0.41,0.51,0.61,0.71}, p ∈ {0.05,0.07,0.09,0.11,0.13}; track tile count N_t ≈ 332.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>Varied from 'easy' (κ=0.31, p=0.05) to 'high' (κ up to 0.71, p up to 0.13); experiments explicitly evaluated across these values.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Variation measured via discrete evaluation set E_hard = h_κ × h_p (5×5 grid of κ and p values, 25 combinations) and multiple random environment instances sampled from these distributions; N_eval used for final robustness = 500 environments. During BO, curricula were sequences of epoch changepoints (k=3) that change ψ=[κ,p] over training epochs.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>Medium-to-high: evaluation across 25 distinct parameter combinations and 500 sampled environment instances (E_hard) for final metrics; also intermediate monitoring across smaller N_eval values (10, 100) during training/analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Mean episodic reward (average episode return) across evaluation environments; supplementary metrics reported: collision/obstacle ratio, tiles visited, time on grass, collisions per episode.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>On E_hard averaged over N_eval=500: Average episodic reward = 696 ± 113 (mean ± std). Collision/obstacle ratio reported = 0.133; tiles visited ≈ 2480 (table formatting in paper is compact but lists higher tiles visited than baselines).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>The paper explicitly treats difficulty as increasing with both κ and p and evaluates generalization across their joint variation. It reports a trade-off between maximizing positive tile-visit reward (by staying on-road and covering tiles) and avoiding large negative collision penalties; Bayesian Optimization found curricula that balance this trade-off and thus improve robustness across combinations of curvature and obstacle density. The authors also note high variance in single-environment performance estimates and recommend aggregate metrics across multiple seeds/environments.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td>696 ± 113 mean episodic reward on E_hard (κ ∈ [0.31,0.71], p ∈ [0.05,0.13]) averaged over N_eval = 500 evaluation environments.</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td>422 ± 185 mean episodic reward for PPO-Default on the easy setting ψ=[0.31,0.05] (reported in Table I) — included as a reference low-complexity/low-variation operating point in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Bayesian Optimization-based curriculum learning (Gaussian Process prior, UCB acquisition; n0=5 warm-up + N=14 BO iterations, total 19 curricula evaluated; curriculum represented as changepoints over training epochs; PPO trained for 1000 epochs per curriculum).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Yes — the BO-selected curriculum produced the best average episodic reward across the hard evaluation set (E_hard) and outperformed both the manual curriculum and PPO trained only on default environment across five difficulty tiers; BO curriculum shows improved robustness to joint variations in κ and p.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Bayesian Optimization: n0=5 warm-up + N=14 BO trials (19 curricula evaluated). PPO training per curriculum: 1000 epochs (single random seed used for all methods due to compute limits). Final evaluations used N_eval=500 environment samples per algorithm.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Bayesian Optimization can automatically find training curricula that yield substantially improved robustness (higher mean episodic reward and favorable trade-offs between tile coverage and collision penalties) across a range of road curvatures and obstacle densities compared to both single-environment training and a manually-designed curriculum; BO manages the trade-off between positive tile rewards and large collision penalties and generalizes across difficulty levels.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving Environment Robustness of Deep Reinforcement Learning Approaches for Autonomous Racing Using Bayesian Optimization-based Curriculum Learning', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1028.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1028.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PPO-Manual</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Proximal Policy Optimization agent trained with a manually-designed curriculum</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A PPO agent trained with a hand-engineered curriculum that gradually increases difficulty (κ and p) according to human domain knowledge; serves as a baseline to compare against BO-driven curriculum search.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>PPO agent (manual curriculum)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Same PPO + CNN architecture as other agents; the training curriculum was manually specified as an increasing sequence of environment difficulty (ψ values) with epoch ranges given in Table II of the paper (four phases from easy to harder κ and p).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Modified OpenAI Gym CarRacing with obstacles</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same 2D track-with-obstacles domain; manual curriculum exposes the agent to increasingly difficult parameter settings in stages (epoch ranges in Table II), varying κ and p across training.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Road curvature κ and obstacle probability p; manual curriculum stages move from ψ=[0.31,0.05] to ψ=[0.61,0.11] over four epoch ranges (see Table II: 0–197, 198–395, 396–774, 775–1000).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>Progressively increases from 'easy' (κ=0.31,p=0.05) to 'medium-high' (κ up to 0.61,p up to 0.11) during training.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Curriculum-induced variation during training by exposing the agent sequentially to four discrete ψ settings; evaluation variation used E_hard (5×5 κ×p grid) with N_eval samples.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>Moderate: curriculum uses four discrete environment parameter stages; evaluation still performed across the 25 hard combinations and N_eval=500 samples.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Mean episodic reward across evaluation environments (and collision/obstacle ratio, tiles visited, time on grass).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>On E_hard averaged over N_eval=500: Average episodic reward = 667 ± 167 (mean ± std). Collision/obstacle ratio reported = 0.055; tiles visited ≈ 2300 (per Table I).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Manual curriculum was designed with the assumption that difficulty increases with both κ and p; authors report manual curriculum improves robustness relative to single-environment training but is outperformed by BO-selected curricula, indicating that automated search can find better trade-offs between complexity and variation coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td>667 ± 167 mean episodic reward on E_hard (reported in Table I; N_eval=500).</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Manually-designed curriculum learning (expert-chosen sequence of ψ stages ordered by increasing difficulty; epoch ranges listed in Table II).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Manual curriculum improved robustness relative to PPO trained only on default environment, but was outperformed by the BO-discovered curriculum across the hard evaluation set and across difficulty levels.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Single manual curriculum trained for 1000 PPO epochs; final evaluation used N_eval=500 environments. Only a single random seed used.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Manual curricula that increase difficulty over training epochs improve generalization compared to single-environment training, but automated curriculum search via Bayesian Optimization can find better-performing curricula that more effectively balance competing rewards and penalties.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving Environment Robustness of Deep Reinforcement Learning Approaches for Autonomous Racing Using Bayesian Optimization-based Curriculum Learning', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1028.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1028.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PPO-Default</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Proximal Policy Optimization agent trained on a default single environment</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A PPO agent trained only on a default environment parameter setting (ψ = [0.31, 0.05]) used as a baseline to measure robustness to environment variation and complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>PPO agent (default single-environment training)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>PPO with CNN policy/value networks trained exclusively on the default environment parameters ψ = [0.31,0.05] (single environment), without curriculum or automated variation during training.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Modified OpenAI Gym CarRacing with obstacles (default setting)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>The default training environment is ψ=[0.31,0.05] (moderate turn rate, low obstacle probability); agent observes top-down RGB BEV images and outputs steering/acceleration/brake controls.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Fixed at κ=0.31 and p=0.05 during training (no complexity variation during training).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>Low (fixed low curvature and low obstacle density during training).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>No training-time variation; evaluation performed on both easy (same as training) and hard E_hard sets to test generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>Low during training; evaluated under medium-to-high variation at test time (E_hard).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Mean episodic reward averaged across evaluation environments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Average episodic reward on easy test setting (ψ=[0.31,0.05]) = 422 ± 185 (N_eval not specified for that row but final evaluations used N_eval=500 for hard set); on hard evaluation set E_hard: 368 ± 181 (Table I, averaged over N_eval=500). Collision/obstacle ratios were higher than manual curriculum in hard set (0.185).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Training on a single low-complexity environment yields poor robustness to increased complexity and variation at test time; authors show PPO-Default has significantly lower average reward on the hard evaluation set compared to curriculum-trained agents, illustrating that exposure to varied training environments (via curriculum) aids robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td>368 ± 181 mean episodic reward on E_hard (reported in Table I; N_eval=500).</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td>422 ± 185 mean episodic reward on the easy environment ψ=[0.31,0.05] (Table I).</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Single-environment training (no curriculum, fixed ψ during training).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Generalization tested and shown to be poor: agent trained only on default ψ performs substantially worse on the hard evaluation set than agents trained with curricula, indicating lack of robustness to increased curvature and obstacle density.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Trained for 1000 PPO epochs on default environment; final evaluations across E_hard used N_eval=500 samples. Only one random seed used.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Training on a single, fixed (low-difficulty) environment does not produce policies robust to increased complexity and variation; curriculum-based training (manual or BO) substantially improves performance on harder, more varied test environments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving Environment Robustness of Deep Reinforcement Learning Approaches for Autonomous Racing Using Bayesian Optimization-based Curriculum Learning', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Curriculum learning <em>(Rating: 2)</em></li>
                <li>Automatically generated curriculum based reinforcement learning for autonomous vehicles in urban environment <em>(Rating: 2)</em></li>
                <li>Robust reinforcement learning via genetic curriculum <em>(Rating: 2)</em></li>
                <li>Emergent complexity and zero-shot transfer via unsupervised environment design <em>(Rating: 2)</em></li>
                <li>Reinforced curriculum learning for autonomous driving in carla <em>(Rating: 1)</em></li>
                <li>A tutorial on bayesian optimization <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1028",
    "paper_id": "paper-266348538",
    "extraction_schema_id": "extraction-schema-24",
    "extracted_data": [
        {
            "name_short": "PPO-BO",
            "name_full": "Proximal Policy Optimization agent trained with Bayesian Optimization-based curriculum",
            "brief_description": "A simulated 2D autonomous racing agent trained with PPO using curricula found by Bayesian Optimization over changepoints that vary road curvature and obstacle density; evaluated for robustness across a grid of easy-to-hard environments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "PPO agent (Bayesian Optimization curriculum)",
            "agent_description": "An on-policy deep reinforcement learning agent using Proximal Policy Optimization (PPO) with CNN policy and value networks; inputs are top-down RGB bird's-eye-view images, outputs are (steering, acceleration, brake) control tuples. The training curriculum (sequence of environment parameter changepoints) was selected by Bayesian Optimization over curriculum changepoint space (k=3).",
            "agent_type": "simulated agent",
            "environment_name": "Modified OpenAI Gym CarRacing with obstacles",
            "environment_description": "A 2D closed-circuit road navigation domain with static rectangular obstacles placed probabilistically on road tiles and grassy off-road areas; objective is to complete laps quickly while avoiding collisions. Environment difficulty is controlled by two parameters: road turn rate (κ, higher → higher curvature) and obstacle probability per tile (p). Tracks typically contain ~332 tiles per lap.",
            "complexity_measure": "Road curvature (turn rate κ) and obstacle density (probability p that a road tile contains an obstacle); numerical ranges used: κ ∈ {0.31,0.41,0.51,0.61,0.71}, p ∈ {0.05,0.07,0.09,0.11,0.13}; track tile count N_t ≈ 332.",
            "complexity_level": "Varied from 'easy' (κ=0.31, p=0.05) to 'high' (κ up to 0.71, p up to 0.13); experiments explicitly evaluated across these values.",
            "variation_measure": "Variation measured via discrete evaluation set E_hard = h_κ × h_p (5×5 grid of κ and p values, 25 combinations) and multiple random environment instances sampled from these distributions; N_eval used for final robustness = 500 environments. During BO, curricula were sequences of epoch changepoints (k=3) that change ψ=[κ,p] over training epochs.",
            "variation_level": "Medium-to-high: evaluation across 25 distinct parameter combinations and 500 sampled environment instances (E_hard) for final metrics; also intermediate monitoring across smaller N_eval values (10, 100) during training/analysis.",
            "performance_metric": "Mean episodic reward (average episode return) across evaluation environments; supplementary metrics reported: collision/obstacle ratio, tiles visited, time on grass, collisions per episode.",
            "performance_value": "On E_hard averaged over N_eval=500: Average episodic reward = 696 ± 113 (mean ± std). Collision/obstacle ratio reported = 0.133; tiles visited ≈ 2480 (table formatting in paper is compact but lists higher tiles visited than baselines).",
            "complexity_variation_relationship": "The paper explicitly treats difficulty as increasing with both κ and p and evaluates generalization across their joint variation. It reports a trade-off between maximizing positive tile-visit reward (by staying on-road and covering tiles) and avoiding large negative collision penalties; Bayesian Optimization found curricula that balance this trade-off and thus improve robustness across combinations of curvature and obstacle density. The authors also note high variance in single-environment performance estimates and recommend aggregate metrics across multiple seeds/environments.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": "696 ± 113 mean episodic reward on E_hard (κ ∈ [0.31,0.71], p ∈ [0.05,0.13]) averaged over N_eval = 500 evaluation environments.",
            "low_complexity_low_variation_performance": "422 ± 185 mean episodic reward for PPO-Default on the easy setting ψ=[0.31,0.05] (reported in Table I) — included as a reference low-complexity/low-variation operating point in the paper.",
            "training_strategy": "Bayesian Optimization-based curriculum learning (Gaussian Process prior, UCB acquisition; n0=5 warm-up + N=14 BO iterations, total 19 curricula evaluated; curriculum represented as changepoints over training epochs; PPO trained for 1000 epochs per curriculum).",
            "generalization_tested": true,
            "generalization_results": "Yes — the BO-selected curriculum produced the best average episodic reward across the hard evaluation set (E_hard) and outperformed both the manual curriculum and PPO trained only on default environment across five difficulty tiers; BO curriculum shows improved robustness to joint variations in κ and p.",
            "sample_efficiency": "Bayesian Optimization: n0=5 warm-up + N=14 BO trials (19 curricula evaluated). PPO training per curriculum: 1000 epochs (single random seed used for all methods due to compute limits). Final evaluations used N_eval=500 environment samples per algorithm.",
            "key_findings": "Bayesian Optimization can automatically find training curricula that yield substantially improved robustness (higher mean episodic reward and favorable trade-offs between tile coverage and collision penalties) across a range of road curvatures and obstacle densities compared to both single-environment training and a manually-designed curriculum; BO manages the trade-off between positive tile rewards and large collision penalties and generalizes across difficulty levels.",
            "uuid": "e1028.0",
            "source_info": {
                "paper_title": "Improving Environment Robustness of Deep Reinforcement Learning Approaches for Autonomous Racing Using Bayesian Optimization-based Curriculum Learning",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "PPO-Manual",
            "name_full": "Proximal Policy Optimization agent trained with a manually-designed curriculum",
            "brief_description": "A PPO agent trained with a hand-engineered curriculum that gradually increases difficulty (κ and p) according to human domain knowledge; serves as a baseline to compare against BO-driven curriculum search.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "PPO agent (manual curriculum)",
            "agent_description": "Same PPO + CNN architecture as other agents; the training curriculum was manually specified as an increasing sequence of environment difficulty (ψ values) with epoch ranges given in Table II of the paper (four phases from easy to harder κ and p).",
            "agent_type": "simulated agent",
            "environment_name": "Modified OpenAI Gym CarRacing with obstacles",
            "environment_description": "Same 2D track-with-obstacles domain; manual curriculum exposes the agent to increasingly difficult parameter settings in stages (epoch ranges in Table II), varying κ and p across training.",
            "complexity_measure": "Road curvature κ and obstacle probability p; manual curriculum stages move from ψ=[0.31,0.05] to ψ=[0.61,0.11] over four epoch ranges (see Table II: 0–197, 198–395, 396–774, 775–1000).",
            "complexity_level": "Progressively increases from 'easy' (κ=0.31,p=0.05) to 'medium-high' (κ up to 0.61,p up to 0.11) during training.",
            "variation_measure": "Curriculum-induced variation during training by exposing the agent sequentially to four discrete ψ settings; evaluation variation used E_hard (5×5 κ×p grid) with N_eval samples.",
            "variation_level": "Moderate: curriculum uses four discrete environment parameter stages; evaluation still performed across the 25 hard combinations and N_eval=500 samples.",
            "performance_metric": "Mean episodic reward across evaluation environments (and collision/obstacle ratio, tiles visited, time on grass).",
            "performance_value": "On E_hard averaged over N_eval=500: Average episodic reward = 667 ± 167 (mean ± std). Collision/obstacle ratio reported = 0.055; tiles visited ≈ 2300 (per Table I).",
            "complexity_variation_relationship": "Manual curriculum was designed with the assumption that difficulty increases with both κ and p; authors report manual curriculum improves robustness relative to single-environment training but is outperformed by BO-selected curricula, indicating that automated search can find better trade-offs between complexity and variation coverage.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": "667 ± 167 mean episodic reward on E_hard (reported in Table I; N_eval=500).",
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Manually-designed curriculum learning (expert-chosen sequence of ψ stages ordered by increasing difficulty; epoch ranges listed in Table II).",
            "generalization_tested": true,
            "generalization_results": "Manual curriculum improved robustness relative to PPO trained only on default environment, but was outperformed by the BO-discovered curriculum across the hard evaluation set and across difficulty levels.",
            "sample_efficiency": "Single manual curriculum trained for 1000 PPO epochs; final evaluation used N_eval=500 environments. Only a single random seed used.",
            "key_findings": "Manual curricula that increase difficulty over training epochs improve generalization compared to single-environment training, but automated curriculum search via Bayesian Optimization can find better-performing curricula that more effectively balance competing rewards and penalties.",
            "uuid": "e1028.1",
            "source_info": {
                "paper_title": "Improving Environment Robustness of Deep Reinforcement Learning Approaches for Autonomous Racing Using Bayesian Optimization-based Curriculum Learning",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "PPO-Default",
            "name_full": "Proximal Policy Optimization agent trained on a default single environment",
            "brief_description": "A PPO agent trained only on a default environment parameter setting (ψ = [0.31, 0.05]) used as a baseline to measure robustness to environment variation and complexity.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "PPO agent (default single-environment training)",
            "agent_description": "PPO with CNN policy/value networks trained exclusively on the default environment parameters ψ = [0.31,0.05] (single environment), without curriculum or automated variation during training.",
            "agent_type": "simulated agent",
            "environment_name": "Modified OpenAI Gym CarRacing with obstacles (default setting)",
            "environment_description": "The default training environment is ψ=[0.31,0.05] (moderate turn rate, low obstacle probability); agent observes top-down RGB BEV images and outputs steering/acceleration/brake controls.",
            "complexity_measure": "Fixed at κ=0.31 and p=0.05 during training (no complexity variation during training).",
            "complexity_level": "Low (fixed low curvature and low obstacle density during training).",
            "variation_measure": "No training-time variation; evaluation performed on both easy (same as training) and hard E_hard sets to test generalization.",
            "variation_level": "Low during training; evaluated under medium-to-high variation at test time (E_hard).",
            "performance_metric": "Mean episodic reward averaged across evaluation environments.",
            "performance_value": "Average episodic reward on easy test setting (ψ=[0.31,0.05]) = 422 ± 185 (N_eval not specified for that row but final evaluations used N_eval=500 for hard set); on hard evaluation set E_hard: 368 ± 181 (Table I, averaged over N_eval=500). Collision/obstacle ratios were higher than manual curriculum in hard set (0.185).",
            "complexity_variation_relationship": "Training on a single low-complexity environment yields poor robustness to increased complexity and variation at test time; authors show PPO-Default has significantly lower average reward on the hard evaluation set compared to curriculum-trained agents, illustrating that exposure to varied training environments (via curriculum) aids robustness.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": "368 ± 181 mean episodic reward on E_hard (reported in Table I; N_eval=500).",
            "low_complexity_low_variation_performance": "422 ± 185 mean episodic reward on the easy environment ψ=[0.31,0.05] (Table I).",
            "training_strategy": "Single-environment training (no curriculum, fixed ψ during training).",
            "generalization_tested": true,
            "generalization_results": "Generalization tested and shown to be poor: agent trained only on default ψ performs substantially worse on the hard evaluation set than agents trained with curricula, indicating lack of robustness to increased curvature and obstacle density.",
            "sample_efficiency": "Trained for 1000 PPO epochs on default environment; final evaluations across E_hard used N_eval=500 samples. Only one random seed used.",
            "key_findings": "Training on a single, fixed (low-difficulty) environment does not produce policies robust to increased complexity and variation; curriculum-based training (manual or BO) substantially improves performance on harder, more varied test environments.",
            "uuid": "e1028.2",
            "source_info": {
                "paper_title": "Improving Environment Robustness of Deep Reinforcement Learning Approaches for Autonomous Racing Using Bayesian Optimization-based Curriculum Learning",
                "publication_date_yy_mm": "2023-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Curriculum learning",
            "rating": 2,
            "sanitized_title": "curriculum_learning"
        },
        {
            "paper_title": "Automatically generated curriculum based reinforcement learning for autonomous vehicles in urban environment",
            "rating": 2,
            "sanitized_title": "automatically_generated_curriculum_based_reinforcement_learning_for_autonomous_vehicles_in_urban_environment"
        },
        {
            "paper_title": "Robust reinforcement learning via genetic curriculum",
            "rating": 2,
            "sanitized_title": "robust_reinforcement_learning_via_genetic_curriculum"
        },
        {
            "paper_title": "Emergent complexity and zero-shot transfer via unsupervised environment design",
            "rating": 2,
            "sanitized_title": "emergent_complexity_and_zeroshot_transfer_via_unsupervised_environment_design"
        },
        {
            "paper_title": "Reinforced curriculum learning for autonomous driving in carla",
            "rating": 1,
            "sanitized_title": "reinforced_curriculum_learning_for_autonomous_driving_in_carla"
        },
        {
            "paper_title": "A tutorial on bayesian optimization",
            "rating": 1,
            "sanitized_title": "a_tutorial_on_bayesian_optimization"
        }
    ],
    "cost": 0.012493,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Improving Environment Robustness of Deep Reinforcement Learning Approaches for Autonomous Racing Using Bayesian Optimization-based Curriculum Learning</p>
<p>Rohan Banerjee 
Prishita Ray 
Mark Campbell 
Improving Environment Robustness of Deep Reinforcement Learning Approaches for Autonomous Racing Using Bayesian Optimization-based Curriculum Learning
127F46CE8AC226A26B8717BB0C171DF1
Deep reinforcement learning (RL) approaches have been broadly applied to a large number of robotics tasks, such as robot manipulation and autonomous driving.However, an open problem in deep RL is learning policies that are robust to variations in the environment, which is an important condition for such systems to be deployed into real-world, unstructured settings.Curriculum learning is one approach that has been applied to improve generalization performance in both supervised and reinforcement learning domains, but selecting the appropriate curriculum to achieve robustness can be a user-intensive process.In our work, we show that performing probabilistic inference of the underlying curriculum-reward function using Bayesian Optimization can be a promising technique for finding a robust curriculum.We demonstrate that a curriculum found with Bayesian optimization can outperform a vanilla deep RL agent and a hand-engineered curriculum in the domain of autonomous racing with obstacle avoidance.Our code is available at https://github.com/PRISHIta123/Curriculum_RL_for_Driving/.</p>
<p>I. INTRODUCTION</p>
<p>Deep reinforcement learning approaches have been effective in solving robotic tasks that involve sequential decisionmaking from high-dimensional observations, such as dexterous manipulation [1] and ground-based navigation [2].For autonomous racing, reinforcement learning approaches have the potential to extend the reach of agents to environments that are unstructured and characterized by a high degree of environmental uncertainty.</p>
<p>However, one of the open challenges in deep reinforcement learning is that a learned policy may not necessarily generalize well to novel environments that lie outside its training set [3]- [5].An ideal reinforcement learning agent would be trained in a subset of environments from the distribution of environments that are expected in the real world, and would then be able to generalize to new environments drawn from that same distribution without a significant degradation in performance.For racing, this means that our agent should be able to navigate robustly in the face of differing road configurations (including road topologies), static obstacle configurations, and dynamic obstacles (such as other racing vehicles).</p>
<p>One set of approaches that have been proposed in the literature to address the generalization problem are approaches based on curriculum learning [6], in which a curated sequence of diverse environments are provided to the learner during the training process.Curriculum-based approaches for RL have been studied in many continuous control domains, including maze environments [7], LunarLander [8], and autonomous driving [9], but not extensively for the domain of autonomous racing and obstacle avoidance.</p>
<p>Therefore, the question that we seek to answer in this work is: How can we design deep reinforcement learningbased agents for autonomous racing in the presence of static obstacles, in such a way that they are robust to taskrelevant environment variations, such as obstacle and road properties?We propose that curriculum learning can be an effective approach for ensuring robustness in this domain.Specifically, we investigate both manually-designed curricula (based on human domain knowledge), and an automated curriculum generation approach based on applying Bayesian Optimization [10] to guide the search through the space of curricula.</p>
<p>We hypothesize that shaping the curriculum in this way can aid with generalization across various reinforcement learning domains with high-dimensional observation spaces, including autonomous racing.Our contributions are as follows:</p>
<p>• We show that manually-generated curricula lead to increased robustness to varying obstacle densities and road curvature in a modified version of the OpenAI Gym CarRacing domain with obstacles, compared to policies that are only trained in the default environment parameter setting.• We show that applying Bayesian Optimization (BO) to automatically search for curricula leads to improved robustness performance compared to the manuallygenerated curriculum in the same domain.</p>
<p>II. RELATED WORK</p>
<p>A number of papers focus on controlling the scenarios that are encountered during the RL training process to achieve better generalization.This can be done by including as many diverse scenarios as possible in the training set with the expectation that they provide reasonable coverage of the full environment distribution [11], or by structuring the training process to include a curriculum of environments [8],</p>
<p>For autonomous driving, most prior work in the curriculum learning space considers manually-designed curricula, for either street driving scenarios [9] [12] [13] or autonomous overtaking [14].Other prior work does consider an automatic curriculum generation scheme for driving, either in intersection domains [13] or racing domains [15] [16].Our work expands upon the prior work by investigating both manuallydesigned and automatically-generated curricula (using probabilistic black-box optimization) for the automomous racing domain with static obstacles.</p>
<p>III. METHODOLOGIES</p>
<p>We examine training curricula that involve changing environment parameters, which we refer to as ψ ∈ Ψ.We represent a curriculum as a list of ordered pairs that represent changepoints in time, at which the agent is exposed to a new environment type ψ.Concretely, this means we represent the curriculum C = [(0, ψ 0 ), (t 1 , ψ 1 ), . . ., (t k , ψ k )], where the interpretation is that we expose the learned agent to environments of type ψ 0 for epochs in [0, t 1 ), ψ 1 for epochs in [t 1 , t 2 ), and so on.</p>
<p>A. Manual Curriculum Learning</p>
<p>We first considered manually specifying the curriculum C, which for a particular domain and a particular choice of environment parameters Ψ involved manually selecting the epoch sequence (t 1 , . . ., t k ) and the environment parameter sequence (ψ 0 , . . ., ψ k ).We selected (ψ 0 , . . ., ψ k ) to be an increasing sequence of difficulty (i.e. from easy to difficult), which requires expert domain knowledge about the environment and choice of environment parameters Ψ.</p>
<p>B. Automated Curriculum Learning using Bayesian Optimization</p>
<p>We propose using Bayesian Optimization [10] as a method to select the right curriculum.Bayesian optimization starts with a Gaussian Process prior distribution (which is a distribution over functions, in which any subset of input points is multivariate Gaussian), and updates this prior into a posterior distribution over functions, as we collect new data points (using the acquistion function).</p>
<p>The learned function f : X → R represents a mapping from a particular choice of changepoints to the average expected reward in the "hard" environment scenarios.The input domain in our case is X = R k , where k is the number of epoch changepoints (for the CarRacing domain, we selected k = 3).Thus, evaluating this function on a particular curriculum x involves training PPO using curriculum x and computing the average expected reward in the "hard" scenarios.The acquisition function a : f × X → R takes in the current Gaussian Process posterior and a possible curriculum x, and produces a real-valued score indicating the viability of that particular curriculum.</p>
<p>For the Bayesian Optimization algorithm, we use the following hyperparameters:</p>
<p>• Gaussian Process prior mean function µ 0 : X → R: µ 0 (x) = µ 0 , where we used µ 0 = 0. • We use n 0 = 5 warm-up trials, which were experimentally designed to evenly cover the regions where the local quadratic behavior was empirically observed, and N = 14 Bayesian optimization trials.</p>
<p>Our automatic curriculum generation algorithm is outlined in Algorithm 1.In practice, we choose the optimal curriculum by not only considering the curriculum x * with the highest final expected reward f (x), but by also considering all of the the n 0 +N curricula that were discovered by the algorithm.For each trial (curriculum), we examined the entire training reward curve, and we selected the trial that achieved the highest observed reward across all training epochs.For the CarRacing environment, we typically examined only the final 3-4 trials.</p>
<p>IV. EXPERIMENTAL SETUP</p>
<p>Environment.We evaluate our approach in a modified version of the CarRacing OpenAI Gym environment [18], which is a 2D car navigation environment consisting of a road, static rectangular obstacles, and a grassy area.The objective of the agent in this environment is to complete a lap around the closed-circuit road as quickly as possible while avoiding collisions with the static obstacles.The observation space O consists of top-down RGB BEV images centered around the car, the action space A consists of (steering, acceleration, brake) tuples, and the reward function R is −0.1 for every timestep of execution, −50 for colliding with obstacles, and +1000/N t for intersecting a road tile (where N t is the number of discrete road tiles in the track, typically around 300).Our choice of obstacle penalty means that obstacle collisions have a much larger impact on the reward compared to the number of tiles crossed.Choose an experimentally-designed curriculum x i .</p>
<p>5:</p>
<p>Observe y i = f (x i ) by training the PPO agent using curriculum x i .</p>
<p>6:</p>
<p>Update dataset:
D = D ∪ {(x i , y i )} 7: end for 8: for n in [1, N ] do 9:
Update the posterior probability distribution on f using D.</p>
<p>10:</p>
<p>Compute x n = arg max x a(x), the maximizer of the UCB acquisition function over x, where the acquisition function is computed using the updated posterior.</p>
<p>11:</p>
<p>Observe y i = f (x i ) by training the PPO agent using curriculum x i .</p>
<p>12:</p>
<p>Update dataset: D = D ∪ {(x n , y n )} 13: end for 14: return x i corresponding to the largest y i ∈ D. We focus on two key environment variables in our experiments: (1) obstacle density, represented by the probability p that a particular road tile contains a rectangular obstacle, and (2) road curvature, represented by the turn rate κ of the road, which dictates the magnitude of turns in the track (that is, a higher κ leads to higher curvature turns).In this driving domain, we assume that difficulty increases in proportion to both increased κ (turn rate) and p (probability of obstacles).Figure 1 shows examples of what the driving environment looks like for different choices of κ and p.The curriculum setting that we considered involved varying both environment variables, so ψ = [κ, p].We also considered additional curriculum settings in which we only varied one of the environment variables (either ψ = [κ] or ψ = [p]), which are described in the Appendix.</p>
<p>Base RL algorithm.We use the PPO algorithm as our underlying deep RL agent [19], which we choose due to its simplicity and efficiency as an on-policy deep RL algorithm, and because it has empirically been shown to be more robust than other deep RL methods in navigation domains [20].For both the policy and value networks, we use the CNN policy architecture from the Stable Baselines open-source RL library [21].</p>
<p>Baselines.We compare the Bayesian Optimization curriculum learner to two baselines.The first baseline (denoted PPO-Default) involves training PPO on a default set of environment parameters ψ = [0.31,0.05].The second baseline is a manually-selected training curriculum, with an empiricallydetermined heuristic that we should place more weight on the intermediate-difficulty environments.The manually-chosen curriculum is shown in Table II, indicated with the ranges for each environment instead of the changepoints.We did not consider random curriculum generation as a baseline because empirically it underperformed the manual curriculum.</p>
<p>We train with a single random seed (0) for all methods due to our computational constraints.For all methods, we conducted PPO-specific hyperparameter tuning on the default environment using a manual search, and we used the following PPO hyperparameters -learning rate: 0.0002 for curriculum-based training and 0.0005 for default training, number of steps in policy/value update: 10, batch size: 1000, number of training epochs: 1000.</p>
<p>Evaluation Methodology.The robustness evaluation metric we use is the average episode reward on a discrete set E eval , where in our case we consider both a set of easy environments E easy = [0.31,0.05] and a set of hard environments E hard = h κ × h p , where h κ = [0.31,0.41, 0.51, 0.61, 0.71] and h p = [0.05,0.07, 0.09, 0.11, 0.13].Given a policy π to evaluate, we first sample N eval random environments from E eval , and then compute the mean episodic performance of π across the N eval environments.</p>
<p>V. RESULTS AND DISCUSSION</p>
<p>During model training, we conducted evaluations every 50 epochs, in which we evaluated the mean and standard deviation of the episodic reward across N eval = 10 environments from E hard .Thus Figure 2 (left) shows the training reward (which indicates the episodic reward of the policy at each epoch in the most-recently generated training environment), and Figure 2 (right) shows the evaluation reward described earlier, for the manually-selected curriculum baseline and the best curriculum chosen by Bayesian Optimization.Because the training and evaluation scores indicate performance for only 1 and 10 environments, respectively, these estimates tend to have high variance (due to the inherent variability of the environments).Therefore, we decided to evaluate the final learned policies learned from each method across a larger set of environments to produce lower-variance robustness estimates.</p>
<p>In Table I we report the mean and standard deviation of the episode return metric (along with other racing-relevant metrics) across N eval = 500 evaluation environments drawn from E hard for each algorithm.Even with 19 trials, the Bayesian Optimization method was able to find a curriculum (shown in Table II) that outperformed our baselines.This suggests that the UCB acquisition function was able to find a promising region in curriculum space given the underlying Gaussian Process model of the curriculum reward function.Additionally, Bayesian Optimization was able to manage the tradeoff between getting positive rewards by covering more tiles by staying on the road and a penalty for colliding with obstacles.On an average, CarRacing had N t = 332 tiles generated in a track over 500 evaluation environments, and the results in the table conform to the expected behavior.</p>
<p>To further investigate the robustness differences between our methods, we constructed 5 individual sets of N eval = 100 environments each, ranging from easy (smaller turn rate, lower probability of obstacles) to hard (larger turn rate, higher probability of obstacles).Then, we evaluated the performance of each of the learned policies (PPO-Default, manual curriculum, and Bayesian Optimization) on these 5 sets.Figure 3 shows the mean evaluation reward for each policy across each environment set.We can observe that the BO Curriculum outperforms the PPO-Default and Manual curriculum baselines on each difficulty level, thus supporting the argument that Bayesian Optimization is able to find better performing curricula from the search space that can generalize well irrespective of difficulty.</p>
<p>VI. CONCLUSION</p>
<p>In this work, we demonstrated that Bayesian Optimization is a potentially promising method to search for ro-bust RL training curricula for autonomous racing.Future work may consider using alternative objective functions for the Bayesian Optimization algorithm, such as the mean of evaluation rewards at different checkpoints during the latter phase of training, or the mean final evaluation reward across multiple random seeds.Either of these alternative objective functions may offer a lower-variance metric for measuring policy robustness to variations in the environment, as performance in a single environment generally has high uncertainty due to the degree of variability in the test environment.</p>
<p>Fig. 1 :
1
Fig. 1: Examples of CarRacing tracks (environment parameters ψ = [κ, p] with turn rate κ and obstacle probability p): (top) ψ = [0.31,0.05], (bottom) ψ = [0.71,0.13]</p>
<p>Fig. 2 :Fig. 3 :
23
Fig. 2: Mean episode reward comparison between manual curriculum and Bayesian Optimization (BO) curriculum, during (left) training and (right) evaluation.Vertical dashed lines in the training figure indicate curriculum changepoints t i .</p>
<p>[17]hose this kernel and hyperparameter choice to model functions that are twice differentiable, due to our empirical observation that the reward function was non-smooth but locally quadratic.Additionally, we used feature-dependent length scales ℓ =[19.9, 26.5, 21.15]for the CarRacing domain.Optimizer for acquisition function: We select the L-BFGS-B[17]optimization algorithm to minimize −a(x), because it is suited for optimizing nonlinear functions (as is the case for −a(x)) and because it is a bounded optimization algorithm, so its search space is more limited, leading to more efficient optimization compared to other methods (e.g.SLSQP).For our domain, we constrained the optimization to lie within the following ranges: x ∈ [150−250, 330−450, 730−830].
• Acquisition function: We use the upper confidencebound (UCB) acquisition function given by a(x) =µ(x)+λσ(x), where µ(x) and σ(x) are the GP posteriormean and standard deviation, respectively. We chosethe UCB acquisition function because we empiricallyobserved that the final reward value is highly sensitiveto the choice of training curriculum, and thus we desiredan exploratory acquisition function. We tried a fewvalues (λ ∈ [1.75, 1.9, 2]) and found that λ = 1.9worked best empirically.
• Gaussian Process prior covariance function Σ 0 : X × X → R: we use the Matern kernel with ν = 5/2.•</p>
<p>TABLE I :
I
Results for three CarRacing curriculum settings.Averages are across 500 evaluation environments.
Training SchemeTest SettingAverage RewardCollision/Obstacle RatioTiles VisitedTime on GrassCollisionsPPO-DefaultEasy422 ± 1850.1661680.3221.002PPO-DefaultHard368 ± 1810.1851590.3431.514Manual CurriculumHard667 ± 1670.0552300.0820.454BO CurriculumHard696 ± 1130.1332480.0321.088Algorithm 1 Bayesian Optimization Curriculum Generator.1: Inputs: Gaussian Process prior hyperparameters (includ-ing mean and kernel function), acquisition function a(•),number of warmup points n 0 , number of iterations N2: Initialize a Gaussian process prior on f and emptydataset D.3: for i in [1, n 0 ] do▷ Warm-Up Phase4:</p>
<p>TABLE II :
II
CarRacing Manual and BO training curricula.
Manual Epoch RangeBO Epoch Rangeψ0-1970-160[0.31, 0.05]198-395161-417[0.41, 0.07]396-774418-736[0.51, 0.09]775-1000737-1000[0.61, 0.11]
Computer Science Department, Cornell University {rbb242, pr376}@cornell.edu
Mechanical and Aerospace Engineering Department, Cornell University mc288@cornell.edu
APPENDIXA. Additional Environment SettingsHere we include results for two different environment settings: ψ = [κ] and ψ = [p], which involve varying only the road curvature κ or only the obstacle probability p, respectively.In the ψ = [κ] setting, we set p = 0 for all environments, and we define E easy = [0.31],and E hard = h κ = [0.31,0.41, 0.51, 0.61, 0.71].Similarly, in the ψ = [p] setting, we set κ = 0.31 for all environments, and we define E easy = [0.05],and E hard = h p = [0.05,0.07, 0.09, 0.11, 0.13].TablesIII and IVshow results analogous to TableIfor each experimental setting.Note in TableIIIwe omit metrics related to obstacle collisions because all environments in this setting have p = 0.In both settings, Bayesian Optimization was able to find curricula that outperformed our baselines, and for the ψ = [p] setting, Bayesian Optimization achieved an improved trade off between positive tile reward and negative obstacle collision reward.
Learning synergies between pushing and grasping with self-supervised deep reinforcement learning. A Zeng, S Song, S Welker, J Lee, A Rodriguez, T Funkhouser, 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2018</p>
<p>Learning robust control policies for end-to-end autonomous driving from data-driven simulation. A Amini, I Gilitschenski, J Phillips, IEEE Robotics and Automation Letters. 2020</p>
<p>Quantifying generalization in reinforcement learning. K Cobbe, O Klimov, C Hesse, T Kim, J Schulman, International Conference on Machine Learning. PMLR2019</p>
<p>Leveraging procedural generation to benchmark reinforcement learning. K Cobbe, C Hesse, J Hilton, J Schulman, International conference on machine learning. PMLR2020</p>
<p>Assessing generalization in deep reinforcement learning. C Packer, K Gao, J Kos, P Krähenbühl, V Koltun, D Song, arXiv:1810.122822018arXiv preprint</p>
<p>Curriculum learning. Y Bengio, J Louradour, R Collobert, J Weston, Proceedings of the 26th annual international conference on machine learning. the 26th annual international conference on machine learning2009</p>
<p>Emergent complexity and zero-shot transfer via unsupervised environment design. M Dennis, N Jaques, E Vinitsky, Advances in neural information processing systems. 20203361</p>
<p>Robust reinforcement learning via genetic curriculum. Y Song, J Schneider, 10.1109/ICRA46639.2022.98124202022 International Conference on Robotics and Automation (ICRA). 2022</p>
<p>Reinforced curriculum learning for autonomous driving in carla. L Anzalone, S Barra, M Nappi, 2021 IEEE International Conference on Image Processing (ICIP). IEEE2021</p>
<p>A tutorial on bayesian optimization. P I Frazier, arXiv:1807.028112018arXiv preprint</p>
<p>Autonomous navigation of uavs in large-scale complex environments: A deep reinforcement learning approach. C Wang, J Wang, Y Shen, X Zhang, IEEE Transactions on Vehicular Technology. 6832019</p>
<p>Investigating value of curriculum reinforcement learning in autonomous driving under diverse road and weather conditions. A Ozturk, M B Gunel, R Dagdanov, 2021 IEEE Intelligent Vehicles Symposium Workshops (IV Workshops). IEEE2021</p>
<p>Automatically generated curriculum based reinforcement learning for autonomous vehicles in urban environment. Z Qiao, K Muelling, J M Dolan, P Palanisamy, P Mudalige, 2018 IEEE Intelligent Vehicles Symposium (IV). IEEE2018</p>
<p>Autonomous overtaking in gran turismo sport using curriculum reinforcement learning. Y Song, H Lin, E Kaufmann, P Dürr, D Scaramuzza, 2021 IEEE international conference on robotics and automation (ICRA). IEEE2021</p>
<p>Replay-guided adversarial environment design. M Jiang, M Dennis, J Parker-Holder, J Foerster, E Grefenstette, T Rocktäschel, Advances in Neural Information Processing Systems. 202134</p>
<p>Clutr: Curriculum learning via unsupervised task representation learning. A S Azad, I Gur, J Emhoff, International Conference on Machine Learning. PMLR2023</p>
<p>A limited memory algorithm for bound constrained optimization. R H Byrd, P Lu, J Nocedal, C Zhu, SIAM Journal on scientific computing. 1651995</p>
<p>. Carracing-V0, </p>
<p>Proximal policy optimization algorithms. J Schulman, F Wolski, P Dhariwal, A Radford, O Klimov, arXiv:1707.063472017arXiv preprint</p>
<p>Comparing deep reinforcement learning algorithms' ability to safely navigate challenging waters. T N Larsen, H Ø Teigen, T Laache, D Varagnolo, A Rasheed, Frontiers in Robotics and AI. 82021</p>
<p>. A Hill, A Raffin, M Ernestus, Stable baselines. 2018</p>            </div>
        </div>

    </div>
</body>
</html>
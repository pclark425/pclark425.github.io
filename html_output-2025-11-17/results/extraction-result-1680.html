<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1680 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1680</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1680</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-32.html">extraction-schema-32</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <p><strong>Paper ID:</strong> paper-269009974</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2404.06229v2.pdf" target="_blank">Autonomous Driving Small-Scale Cars: A Survey of Recent Development</a></p>
                <p><strong>Paper Abstract:</strong> While engaging with the unfolding revolution in autonomous driving, a challenge presents itself, how can we effectively raise awareness within society about this transformative trend? While full-scale autonomous driving vehicles often come with a hefty price tag, the emergence of small-scale car platforms offers a compelling alternative. These platforms not only serve as valuable educational tools for the broader public and young generations but also function as robust research platforms, contributing significantly to the ongoing advancements in autonomous driving technology. This survey outlines various small-scale car platforms, categorizing them and detailing the research advancements accomplished through their usage. The conclusion provides proposals for promising future directions in the field.</p>
                <p><strong>Cost:</strong> 0.025</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1680.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1680.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sim2Real (survey)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Simulation-to-Reality (Sim2Real) transfer for small-scale autonomous cars</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Survey-level discussion of the Sim2Real gap for small-scale autonomous driving platforms, summarizing methods (zero-shot, transfer learning, domain adaptation, domain randomization), simulators used, and factors that enable or hinder transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Autonomous Driving Small-Scale Cars: A Survey of Recent Development</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>small-scale autonomous car platforms (general)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>A broad class of miniature vehicles (1/5th to 1/43rd scale and others) equipped with cameras, IMUs, encoders, optional LiDAR/GPS, and compute units (Raspberry Pi, Nvidia Jetson, PC) used for education and research in autonomous driving.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>autonomous driving (small-scale vehicles / robotics)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>multiple (Gazebo, CARLA, AIRSIM, LGSVL, TORCS, SUMO, platform-specific gyms such as Gym-Duckietown, DeepRacer cloud simulator, Donkey-Gym, F1TENTH Gym)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Range from modular robotics simulators (Gazebo) to high-fidelity driving simulators (CARLA, AIRSIM, LGSVL, TORCS) and platform-specific fast environments; simulate physics, vehicle dynamics (to varying degrees), sensor outputs (camera, LiDAR), traffic and multi-agent interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>varies across tools: from simplified dynamics / fast lightweight simulators (Gym-Duckietown, Donkey-Gym) to high-fidelity physics and sensors (CARLA, AIRSIM, LGSVL, CarSim/CarMaker)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>vehicle dynamics (varies by simulator), rigid-body physics, camera rendering/lighting (in higher-fidelity engines), LiDAR point returns (in some simulators), multi-agent traffic behaviors, basic sensor models</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>many simulators simplify tire/contact dynamics, detailed actuator/servo delays, exact sensor noise/distribution, photometric and optical imperfections, fine-grained friction and wear, and certain environmental phenomena (weather, nuanced lighting, multipath GPS effects) depending on simulator</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Indoor and outdoor physical small-scale testbeds (Duckietown tables/roads, F1TENTH tracks, AutoRally outdoor courses, UDSSC scaled smart city) using the actual platform sensors (RGB cameras, LiDAR, IMU, encoders, central localization systems or OptiTrack/VICON for high precision).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>navigation tasks including lane keeping, obstacle avoidance, overtaking, racing, multi-agent/cooperative driving; generally DRL/IL policies and perception networks trained in sim.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>varied: imitation learning (behavioral cloning, DAgger), reinforcement learning (DRL variants), supervised learning for perception, transfer learning, domain adaptation techniques</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>not standardized across surveyed works; commonly reported metrics in cited studies include task success rate, collision rate, lap time, tracking error, but the survey does not report unified numeric metrics</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>differences in dynamics, sensor appearance differences (image statistics, lighting), unmodeled sensor noise, actuator delays and sampling rate differences, environmental variability (lighting, textures), calibration and mounting differences</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>high simulation fidelity where needed, compact/abstract observation and action spaces, domain randomization or adaptation, using real-world data for imitation learning, central localization to remove perception mismatch, inclusion of sensor noise/delays during training</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>paper states transfer success depends on simulation fidelity (particularly for DRL zero-shot transfer) but does not quantify numeric thresholds; identifies that realistic sensing (camera/LiDAR noise, lighting) and dynamics modeling are important</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Sim2Real transfer for small-scale cars is an open challenge; success depends heavily on simulation fidelity and the chosen method (IL, DRL, transfer learning, domain adaptation, domain randomization). Practical enabling techniques include dynamics and sensing randomization, compact representations, leveraging real datasets for IL, and using centralized/high-precision localization to reduce perception-induced gaps. Excessive randomization can harm performance and domain adaptation requires computational resources.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Autonomous Driving Small-Scale Cars: A Survey of Recent Development', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1680.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1680.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Domain Randomization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Domain Randomization (simulation parameter randomization)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Sim2Real approach that randomly varies simulation parameters during training (visuals, lighting, textures, physics) to produce policies robust to real-world variability; explicitly called out as supported by Duckietown and DeepRacer simulators.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Autonomous Driving Small-Scale Cars: A Survey of Recent Development</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>policies for small-scale autonomous cars (Duckiebots, DeepRacer, others)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>Control and perception policies for small-scale cars trained in simulation to handle lane keeping, obstacle avoidance and racing under variable conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>autonomous driving (small-scale vehicles / robotics)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>platform-specific simulators (Gym-Duckietown, DeepRacer cloud simulator) and general engines that support randomization</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Simulators with the capability to randomly vary visual and physical parameters (lighting, textures, object appearance, physics coefficients) during training runs to diversify observations and dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>strategy aims to offset lower-fidelity physics/graphics by exposing agent to wide parameter variations; simulation itself may be simplified but randomized across many configurations</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>lighting, textures, object appearances, some physics property ranges (e.g., friction, mass) when available</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>does not guarantee accurate per-instance physics or photorealism; detailed contact/tire dynamics, exact sensor noise distributions, and some actuator dynamics often remain simplified</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>small-scale car testbeds (Duckietown tables, DeepRacer real vehicles, F1TENTH test tracks) with real lighting, real sensor noise and hardware latency</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>visual perception tasks and control policies for lane keeping, obstacle avoidance, and racing</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>reinforcement learning (DRL) and supervised/imitation pipelines augmented with randomized simulation parameters</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>typically success rate, reduction in out-of-distribution failures, or robustness measured by trials without manual tuning; the survey notes qualitative improvements but cites no unified numeric values</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>randomized lighting conditions, textures, object appearances, and physics properties (paper notes Duckietown and DeepRacer provide domain randomization options); warns that excessive randomization may dilute relevant training signal</td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>residual mismatches not covered by randomized ranges, mismodeled actuator delays, unrepresented sensor artifacts and camera optics</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>appropriate ranges of randomized parameters that cover real-world variability without overwhelming learning, compact observation spaces, and including sensor delay/sampling rate variations as observations</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>paper suggests domain randomization can compensate for lower-fidelity simulators if variations cover real-world variability, but warns against over-randomization; no numeric fidelity thresholds provided</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Domain randomization is a practical and commonly available approach (supported by Duckietown and DeepRacer) that improves generalization from sim to real, but must be applied carefully â€” too much randomization can reduce training relevance and harm real-world performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Autonomous Driving Small-Scale Cars: A Survey of Recent Development', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1680.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1680.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zero-shot Transfer (IL & DRL)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-shot Transfer (direct sim-to-real application without real-world fine-tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Applying a model trained entirely in simulation directly to real hardware without real-world adaptation; in small-scale cars this includes imitation learning from real datasets or DRL with compact observation/action spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Autonomous Driving Small-Scale Cars: A Survey of Recent Development</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>sim-trained control/perception models for small-scale cars</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>Models for lane following, obstacle avoidance and racing trained in sim and deployed directly on small-scale vehicles.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>autonomous driving (small-scale vehicles / robotics)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>various simulators used for training (Gazebo, Gym-Duckietown, DeepRacer cloud simulator, Donkey-Gym, CARLA etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Simulators that provide rendered camera images, physics for vehicle motion, and optionally LiDAR and IMU outputs; fidelity varies by tool</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>varies; zero-shot DRL success noted to depend heavily on simulation fidelity (higher fidelity/accurate sensor models improve prospects)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>camera rendering, basic vehicle dynamics; success requires reasonably accurate modeling of what the policy directly depends on (e.g., visual features or compact affordances)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>often simplified actuator delays, exact sensor noise, nuanced lighting and texture variations if not randomized explicitly</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>indoor/outdoor tracks with real sensors and hardware latency; some studies use high-precision central localization to simplify perception</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>direct execution of driving policies: lane keeping, obstacle avoidance, racing</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>imitation learning (using real datasets, behavioral cloning) or DRL trained in simulation</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>measures cited in survey include success rate, reduction in collisions, lap times, control smoothness (e.g., CAPS to smooth actions), but survey does not present unified numbers</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>dependency on simulation fidelity for observed inputs and outputs, unmodeled sensor appearance differences, sampling rate and delay mismatches, overfitting to sim visuals or dynamics</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>compact observation and action spaces, explicit modeling/randomization of delays and sampling rates, inclusion of sensor noise, use of high-fidelity simulators or domain randomization, pretraining via IL on real data where available</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>paper emphasizes that DRL-based zero-shot transfer depends heavily on simulation fidelity but does not provide numerical fidelity thresholds</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Zero-shot transfer can work for small-scale cars but is sensitive to simulation fidelity and observation/action representation; imitation learning using real datasets is efficient when sim/real are similar, while DRL zero-shot transfer requires compact spaces and careful fidelity/design choices.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Autonomous Driving Small-Scale Cars: A Survey of Recent Development', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1680.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1680.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Transfer Learning / Domain Adaptation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transfer Learning and Domain Adaptation for sim-to-real</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Approaches that adapt models trained in source domains (simulation or other datasets) to improve performance in target (real-world) domains, including domain adaptation to minimize distribution mismatch.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Autonomous Driving Small-Scale Cars: A Survey of Recent Development</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>perception and control models for small-scale cars</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>Perception networks and control policies where weights or representations are transferred and adapted from simulated training to real-world deployment to improve generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>autonomous driving (small-scale vehicles / robotics)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>simulators used as source domains (e.g., CARLA, Gazebo, Gym-Duckietown, DeepRacer cloud)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Source simulated environments producing labeled data and trajectories for learning transferable features or initial policies; simulation fidelity and choice of features affect transferability.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>effective when simulation aligns closely with real-world scenarios; domain adaptation helps when there are significant mismatches</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>useful to model visual appearance and dynamics sufficiently to learn transferable features; feature distributions and mid-level representations are emphasized</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>exact real-world noise distributions and rare/edge-case phenomena often underrepresented</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>target small-scale testbeds with different visual/physical properties from the simulator; labeled target-domain data may be scarce</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>perception tasks (lane/traffic sign detection) and control policies refined for target environment</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>transfer learning (fine-tuning), domain adaptation techniques (feature alignment), supervised and unsupervised adaptation approaches</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>improvement in target-domain accuracy/performance compared to no-transfer baseline; survey notes qualitative effectiveness in dynamic environments but computational cost</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>distribution mismatch between source and target (visual domain shift, dynamics mismatch), scarcity of labeled real target data, compute/resource constraints for real-time application</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>availability of some target-domain data for fine-tuning, high-quality feature extraction, computational resources for domain adaptation, and robust intermediate representations</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>paper indicates transfer learning excels when simulation closely aligns with reality but fails with large domain mismatches; no quantitative thresholds provided</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Transfer learning and domain adaptation are effective when simulation is reasonably close to reality or when limited target-domain labels are available; they require computational resources and careful feature design to be practical in real-time small-scale platforms.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Autonomous Driving Small-Scale Cars: A Survey of Recent Development', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1680.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1680.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepRacer (sim-to-real mention)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AWS DeepRacer (simulation and cloud environment with sim2real support)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Amazon Web Services' 1/18th scale racing platform with a cloud simulator and support for domain randomization and DRL experiments aimed at learning-to-race with sim-to-real considerations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Autonomous Driving Small-Scale Cars: A Survey of Recent Development</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>AWS DeepRacer car and cloud learning ecosystem</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>A small-scale Ackermann-steering race car with onboard camera (optionally LiDAR) and Intel Atom-based compute, paired with a cloud simulator for training DRL and testing policies before real-world deployment.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>autonomous racing and lane-keeping for small-scale cars</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>DeepRacer cloud simulator (AWS)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Online simulator within AWS ecosystem supporting customization of training environments, reward functions, and domain randomization options; simulates track visuals, basic vehicle dynamics and sensors required for racing tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>moderate; designed for fast experimentation rather than full physical fidelity, with domain randomization options to improve robustness</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>rendered camera images, track geometry, basic vehicle dynamics, optional sensor models (camera, LiDAR in upgraded kits), ability to introduce delays/sampling differences as observations</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>detailed tire/contact dynamics, exact actuator latencies, photometric camera artifacts and nuanced lighting/weather effects likely simplified</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>real AWS DeepRacer cars used on physical tracks for racing; real sensors and hardware latency present</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>autonomous racing, lane keeping, obstacle avoidance learned in cloud sim and deployed to physical DeepRacer cars</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>DRL (various RL algorithms), imitation learning in some contexts; training occurs in cloud simulator</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>studies and AWS examples measure lap time, completion rate, collisions; survey notes techniques like adding delays/sampling as observations can improve robustness</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>DeepRacer provides domain randomization options in its simulator (paper cites support); typical randomizations include visual variations and possibly dynamics parameter ranges</td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>simplified dynamics and sensing in cloud sim, unmodeled actuator and timing differences, differing lighting and camera characteristics</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>use of domain randomization, including delays and sampling rates as observations during training, compact observation spaces suited to on-board compute</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>paper asserts DRL zero-shot transfer and sim-to-real for DeepRacer depends on adequate simulation fidelity and inclusion of relevant randomized parameters; no numeric thresholds provided</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>DeepRacer's cloud simulator and domain randomization options make it a practical platform for experimenting with sim-to-real RL, but successful transfer depends on modeling or randomizing the key aspects the policy relies on (e.g., delays, camera appearance); the survey notes examples where including timing/delay as observations improved robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Autonomous Driving Small-Scale Cars: A Survey of Recent Development', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1680.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1680.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Duckietown (sim-to-real mention)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Duckietown / Gym-Duckietown (educational & research sim environment)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Duckietown platform and its Gym-Duckietown simulator provide a fast, customizable environment and simulation options (including domain randomization) for training perception and control agents for lane following and multi-agent scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Autonomous Driving Small-Scale Cars: A Survey of Recent Development</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Duckiebot (Duckietown Duckiebots and ecosystem)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>Differential-drive small robots (Duckiebots) with RGB camera, IMU, encoders and basic distance sensing used for lane keeping, traffic sign detection, obstacle avoidance and multi-agent tasks in scaled urban layouts.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>autonomous driving education and research (indoor small-scale)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>Gym-Duckietown</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>A fast and customizable simulation environment tailored to Duckietown tasks; simulates lane markings, traffic furniture, camera images and supports rapid iteration of algorithms.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>lightweight / fast simulator focused on task-relevant visual features rather than full physical fidelity</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>camera images with lane/traffic sign rendering, simple dynamics for Duckiebots, multi-agent interactions and environment layout</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>differential-drive dynamics approximate real vehicle behavior; many sensor noise sources, real optics, lighting variations, and precise actuator latencies are simplified unless explicitly randomized</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>physical Duckietown testbeds (tabletop roads, toy traffic lights, Duckiebots) with real camera imagery and sensors; some setups use watchtower localization or centralized infrastructure</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>lane keeping, traffic sign detection, obstacle avoidance, multi-agent coordination</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>imitation learning (BC, DAgger) and reinforcement learning; perception networks trained with mixed sim/real datasets in some studies</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>task success in lane following/obstacle avoidance; survey references comparative evaluations (e.g., BC, GAIL, DAgger) but does not report aggregate numbers</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>Duckietown simulation provides domain randomization options (paper notes Duckietown supports this) including visual and environment parameter variations to improve robustness</td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>visual domain shift (lighting, camera characteristics), dynamics mismatch from differential-drive simplifications, simplified sensor noise models</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>compact perception representations (affordances), use of domain randomization and mixed real/sim datasets, and approaches that limit dependence on per-pixel photorealism</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>survey indicates Gym-Duckietown is effective for fast prototyping but transfer benefits from modeling or randomizing key visual and temporal factors; no quantitative fidelity thresholds given</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Duckietown's simulator is a commonly used testbed for studying Sim2Real; domain randomization and hybrid sim/real training improve transfer, while simplified vehicle dynamics and visual mismatches remain key gap contributors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Autonomous Driving Small-Scale Cars: A Survey of Recent Development', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1680.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1680.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Donkey-Gym / Donkeycar (sim mention)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Donkey-Gym / Donkeycar (Unity-based simulation and hardware platform)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Donkeycar is an open-source 1/10th scale platform for end-to-end driving experiments; Donkey-Gym uses the Unity engine to provide improved physics and graphics for training policies intended for transfer to Donkeycar hardware.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Autonomous Driving Small-Scale Cars: A Survey of Recent Development</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Donkeycar platform and Donkey-Gym simulator</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>A community-driven, customizable small-scale Ackermann-steering vehicle platform (Donkeycar) with camera-centric perception and an associated Unity-based simulator (Donkey-Gym) used for DRL and IL experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>autonomous driving (end-to-end lane following and obstacle avoidance on small-scale vehicles)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>Donkey-Gym (Unity engine)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Unity-based simulator providing enhanced physics and graphics compared to simpler simulators, generating rendered camera images and simulating vehicle dynamics for training agents.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>moderate (improved rendering and physics compared to very lightweight sims), but still not full automotive-grade fidelity</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>camera rendering (visual features), vehicle kinematics/dynamics approximations, configurable environment geometry</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>tire/contact dynamics, precise actuator delays, detailed sensor noise models and photometric camera effects often not modeled unless specifically added</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Donkeycar hardware built on 1/10th RC chassis with Raspberry Pi/Jetson and camera; used on indoor/outdoor tracks with real lighting and real sensor artifacts</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>end-to-end lane keeping, steering-angle prediction, basic obstacle avoidance</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>imitation learning, DRL in Unity-based simulator, and supervised learning for perception pipelines</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>task completion and generalization to real track conditions, collision rates; survey reports qualitative use cases but no unified numerical outcomes</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>visual mismatch between Unity renderings and real camera images, simplified actuator/servo dynamics, unmodeled lighting and camera artifacts</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>higher-fidelity rendering and physics in Unity, careful dataset augmentation (style transfer), and leveraging mixed sim/real data for perception training</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>paper suggests Unity-based simulators like Donkey-Gym improve realism and ease transfer but does not specify quantitative fidelity thresholds</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Donkey-Gym (Unity) offers improved visual/physics fidelity for Donkeycar training and is a common choice for end-to-end experiments; transfer benefits from realistic rendering and data-augmentation techniques but residual sim/real visual and dynamic mismatches remain challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Autonomous Driving Small-Scale Cars: A Survey of Recent Development', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Deepracer: Autonomous racing platform for experimentation with sim2real reinforcement learning <em>(Rating: 2)</em></li>
                <li>Domain randomization for transferring deep neural networks from simulation to the real world <em>(Rating: 2)</em></li>
                <li>Sim-to-real: Learning agile locomotion for quadruped robots <em>(Rating: 2)</em></li>
                <li>Mind the gap! a study on the transferability of virtual vs physical-world testing of autonomous driving systems <em>(Rating: 2)</em></li>
                <li>Zero-shot policy transferability for the control of a scale autonomous vehicle <em>(Rating: 2)</em></li>
                <li>A platform-agnostic deep reinforcement learning framework for effective sim2real transfer towards autonomous driving <em>(Rating: 1)</em></li>
                <li>Sim-to-real transfer for deep reinforcement learning with stochastic state transition delays <em>(Rating: 1)</em></li>
                <li>Learning to drive (l2d) as a low-cost benchmark for real-world reinforcement learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1680",
    "paper_id": "paper-269009974",
    "extraction_schema_id": "extraction-schema-32",
    "extracted_data": [
        {
            "name_short": "Sim2Real (survey)",
            "name_full": "Simulation-to-Reality (Sim2Real) transfer for small-scale autonomous cars",
            "brief_description": "Survey-level discussion of the Sim2Real gap for small-scale autonomous driving platforms, summarizing methods (zero-shot, transfer learning, domain adaptation, domain randomization), simulators used, and factors that enable or hinder transfer.",
            "citation_title": "Autonomous Driving Small-Scale Cars: A Survey of Recent Development",
            "mention_or_use": "mention",
            "agent_system_name": "small-scale autonomous car platforms (general)",
            "agent_system_description": "A broad class of miniature vehicles (1/5th to 1/43rd scale and others) equipped with cameras, IMUs, encoders, optional LiDAR/GPS, and compute units (Raspberry Pi, Nvidia Jetson, PC) used for education and research in autonomous driving.",
            "domain": "autonomous driving (small-scale vehicles / robotics)",
            "virtual_environment_name": "multiple (Gazebo, CARLA, AIRSIM, LGSVL, TORCS, SUMO, platform-specific gyms such as Gym-Duckietown, DeepRacer cloud simulator, Donkey-Gym, F1TENTH Gym)",
            "virtual_environment_description": "Range from modular robotics simulators (Gazebo) to high-fidelity driving simulators (CARLA, AIRSIM, LGSVL, TORCS) and platform-specific fast environments; simulate physics, vehicle dynamics (to varying degrees), sensor outputs (camera, LiDAR), traffic and multi-agent interactions.",
            "simulation_fidelity_level": "varies across tools: from simplified dynamics / fast lightweight simulators (Gym-Duckietown, Donkey-Gym) to high-fidelity physics and sensors (CARLA, AIRSIM, LGSVL, CarSim/CarMaker)",
            "fidelity_aspects_modeled": "vehicle dynamics (varies by simulator), rigid-body physics, camera rendering/lighting (in higher-fidelity engines), LiDAR point returns (in some simulators), multi-agent traffic behaviors, basic sensor models",
            "fidelity_aspects_simplified": "many simulators simplify tire/contact dynamics, detailed actuator/servo delays, exact sensor noise/distribution, photometric and optical imperfections, fine-grained friction and wear, and certain environmental phenomena (weather, nuanced lighting, multipath GPS effects) depending on simulator",
            "real_environment_description": "Indoor and outdoor physical small-scale testbeds (Duckietown tables/roads, F1TENTH tracks, AutoRally outdoor courses, UDSSC scaled smart city) using the actual platform sensors (RGB cameras, LiDAR, IMU, encoders, central localization systems or OptiTrack/VICON for high precision).",
            "task_or_skill_transferred": "navigation tasks including lane keeping, obstacle avoidance, overtaking, racing, multi-agent/cooperative driving; generally DRL/IL policies and perception networks trained in sim.",
            "training_method": "varied: imitation learning (behavioral cloning, DAgger), reinforcement learning (DRL variants), supervised learning for perception, transfer learning, domain adaptation techniques",
            "transfer_success_metric": "not standardized across surveyed works; commonly reported metrics in cited studies include task success rate, collision rate, lap time, tracking error, but the survey does not report unified numeric metrics",
            "transfer_performance_sim": null,
            "transfer_performance_real": null,
            "transfer_success": null,
            "domain_randomization_used": null,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": "differences in dynamics, sensor appearance differences (image statistics, lighting), unmodeled sensor noise, actuator delays and sampling rate differences, environmental variability (lighting, textures), calibration and mounting differences",
            "transfer_enabling_conditions": "high simulation fidelity where needed, compact/abstract observation and action spaces, domain randomization or adaptation, using real-world data for imitation learning, central localization to remove perception mismatch, inclusion of sensor noise/delays during training",
            "fidelity_requirements_identified": "paper states transfer success depends on simulation fidelity (particularly for DRL zero-shot transfer) but does not quantify numeric thresholds; identifies that realistic sensing (camera/LiDAR noise, lighting) and dynamics modeling are important",
            "fine_tuning_in_real_world": null,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": false,
            "fidelity_comparison_results": null,
            "key_findings": "Sim2Real transfer for small-scale cars is an open challenge; success depends heavily on simulation fidelity and the chosen method (IL, DRL, transfer learning, domain adaptation, domain randomization). Practical enabling techniques include dynamics and sensing randomization, compact representations, leveraging real datasets for IL, and using centralized/high-precision localization to reduce perception-induced gaps. Excessive randomization can harm performance and domain adaptation requires computational resources.",
            "uuid": "e1680.0",
            "source_info": {
                "paper_title": "Autonomous Driving Small-Scale Cars: A Survey of Recent Development",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Domain Randomization",
            "name_full": "Domain Randomization (simulation parameter randomization)",
            "brief_description": "A Sim2Real approach that randomly varies simulation parameters during training (visuals, lighting, textures, physics) to produce policies robust to real-world variability; explicitly called out as supported by Duckietown and DeepRacer simulators.",
            "citation_title": "Autonomous Driving Small-Scale Cars: A Survey of Recent Development",
            "mention_or_use": "mention",
            "agent_system_name": "policies for small-scale autonomous cars (Duckiebots, DeepRacer, others)",
            "agent_system_description": "Control and perception policies for small-scale cars trained in simulation to handle lane keeping, obstacle avoidance and racing under variable conditions.",
            "domain": "autonomous driving (small-scale vehicles / robotics)",
            "virtual_environment_name": "platform-specific simulators (Gym-Duckietown, DeepRacer cloud simulator) and general engines that support randomization",
            "virtual_environment_description": "Simulators with the capability to randomly vary visual and physical parameters (lighting, textures, object appearance, physics coefficients) during training runs to diversify observations and dynamics.",
            "simulation_fidelity_level": "strategy aims to offset lower-fidelity physics/graphics by exposing agent to wide parameter variations; simulation itself may be simplified but randomized across many configurations",
            "fidelity_aspects_modeled": "lighting, textures, object appearances, some physics property ranges (e.g., friction, mass) when available",
            "fidelity_aspects_simplified": "does not guarantee accurate per-instance physics or photorealism; detailed contact/tire dynamics, exact sensor noise distributions, and some actuator dynamics often remain simplified",
            "real_environment_description": "small-scale car testbeds (Duckietown tables, DeepRacer real vehicles, F1TENTH test tracks) with real lighting, real sensor noise and hardware latency",
            "task_or_skill_transferred": "visual perception tasks and control policies for lane keeping, obstacle avoidance, and racing",
            "training_method": "reinforcement learning (DRL) and supervised/imitation pipelines augmented with randomized simulation parameters",
            "transfer_success_metric": "typically success rate, reduction in out-of-distribution failures, or robustness measured by trials without manual tuning; the survey notes qualitative improvements but cites no unified numeric values",
            "transfer_performance_sim": null,
            "transfer_performance_real": null,
            "transfer_success": null,
            "domain_randomization_used": true,
            "domain_randomization_details": "randomized lighting conditions, textures, object appearances, and physics properties (paper notes Duckietown and DeepRacer provide domain randomization options); warns that excessive randomization may dilute relevant training signal",
            "sim_to_real_gap_factors": "residual mismatches not covered by randomized ranges, mismodeled actuator delays, unrepresented sensor artifacts and camera optics",
            "transfer_enabling_conditions": "appropriate ranges of randomized parameters that cover real-world variability without overwhelming learning, compact observation spaces, and including sensor delay/sampling rate variations as observations",
            "fidelity_requirements_identified": "paper suggests domain randomization can compensate for lower-fidelity simulators if variations cover real-world variability, but warns against over-randomization; no numeric fidelity thresholds provided",
            "fine_tuning_in_real_world": null,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": false,
            "fidelity_comparison_results": null,
            "key_findings": "Domain randomization is a practical and commonly available approach (supported by Duckietown and DeepRacer) that improves generalization from sim to real, but must be applied carefully â€” too much randomization can reduce training relevance and harm real-world performance.",
            "uuid": "e1680.1",
            "source_info": {
                "paper_title": "Autonomous Driving Small-Scale Cars: A Survey of Recent Development",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Zero-shot Transfer (IL & DRL)",
            "name_full": "Zero-shot Transfer (direct sim-to-real application without real-world fine-tuning)",
            "brief_description": "Applying a model trained entirely in simulation directly to real hardware without real-world adaptation; in small-scale cars this includes imitation learning from real datasets or DRL with compact observation/action spaces.",
            "citation_title": "Autonomous Driving Small-Scale Cars: A Survey of Recent Development",
            "mention_or_use": "mention",
            "agent_system_name": "sim-trained control/perception models for small-scale cars",
            "agent_system_description": "Models for lane following, obstacle avoidance and racing trained in sim and deployed directly on small-scale vehicles.",
            "domain": "autonomous driving (small-scale vehicles / robotics)",
            "virtual_environment_name": "various simulators used for training (Gazebo, Gym-Duckietown, DeepRacer cloud simulator, Donkey-Gym, CARLA etc.)",
            "virtual_environment_description": "Simulators that provide rendered camera images, physics for vehicle motion, and optionally LiDAR and IMU outputs; fidelity varies by tool",
            "simulation_fidelity_level": "varies; zero-shot DRL success noted to depend heavily on simulation fidelity (higher fidelity/accurate sensor models improve prospects)",
            "fidelity_aspects_modeled": "camera rendering, basic vehicle dynamics; success requires reasonably accurate modeling of what the policy directly depends on (e.g., visual features or compact affordances)",
            "fidelity_aspects_simplified": "often simplified actuator delays, exact sensor noise, nuanced lighting and texture variations if not randomized explicitly",
            "real_environment_description": "indoor/outdoor tracks with real sensors and hardware latency; some studies use high-precision central localization to simplify perception",
            "task_or_skill_transferred": "direct execution of driving policies: lane keeping, obstacle avoidance, racing",
            "training_method": "imitation learning (using real datasets, behavioral cloning) or DRL trained in simulation",
            "transfer_success_metric": "measures cited in survey include success rate, reduction in collisions, lap times, control smoothness (e.g., CAPS to smooth actions), but survey does not present unified numbers",
            "transfer_performance_sim": null,
            "transfer_performance_real": null,
            "transfer_success": null,
            "domain_randomization_used": null,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": "dependency on simulation fidelity for observed inputs and outputs, unmodeled sensor appearance differences, sampling rate and delay mismatches, overfitting to sim visuals or dynamics",
            "transfer_enabling_conditions": "compact observation and action spaces, explicit modeling/randomization of delays and sampling rates, inclusion of sensor noise, use of high-fidelity simulators or domain randomization, pretraining via IL on real data where available",
            "fidelity_requirements_identified": "paper emphasizes that DRL-based zero-shot transfer depends heavily on simulation fidelity but does not provide numerical fidelity thresholds",
            "fine_tuning_in_real_world": false,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": false,
            "fidelity_comparison_results": null,
            "key_findings": "Zero-shot transfer can work for small-scale cars but is sensitive to simulation fidelity and observation/action representation; imitation learning using real datasets is efficient when sim/real are similar, while DRL zero-shot transfer requires compact spaces and careful fidelity/design choices.",
            "uuid": "e1680.2",
            "source_info": {
                "paper_title": "Autonomous Driving Small-Scale Cars: A Survey of Recent Development",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Transfer Learning / Domain Adaptation",
            "name_full": "Transfer Learning and Domain Adaptation for sim-to-real",
            "brief_description": "Approaches that adapt models trained in source domains (simulation or other datasets) to improve performance in target (real-world) domains, including domain adaptation to minimize distribution mismatch.",
            "citation_title": "Autonomous Driving Small-Scale Cars: A Survey of Recent Development",
            "mention_or_use": "mention",
            "agent_system_name": "perception and control models for small-scale cars",
            "agent_system_description": "Perception networks and control policies where weights or representations are transferred and adapted from simulated training to real-world deployment to improve generalization.",
            "domain": "autonomous driving (small-scale vehicles / robotics)",
            "virtual_environment_name": "simulators used as source domains (e.g., CARLA, Gazebo, Gym-Duckietown, DeepRacer cloud)",
            "virtual_environment_description": "Source simulated environments producing labeled data and trajectories for learning transferable features or initial policies; simulation fidelity and choice of features affect transferability.",
            "simulation_fidelity_level": "effective when simulation aligns closely with real-world scenarios; domain adaptation helps when there are significant mismatches",
            "fidelity_aspects_modeled": "useful to model visual appearance and dynamics sufficiently to learn transferable features; feature distributions and mid-level representations are emphasized",
            "fidelity_aspects_simplified": "exact real-world noise distributions and rare/edge-case phenomena often underrepresented",
            "real_environment_description": "target small-scale testbeds with different visual/physical properties from the simulator; labeled target-domain data may be scarce",
            "task_or_skill_transferred": "perception tasks (lane/traffic sign detection) and control policies refined for target environment",
            "training_method": "transfer learning (fine-tuning), domain adaptation techniques (feature alignment), supervised and unsupervised adaptation approaches",
            "transfer_success_metric": "improvement in target-domain accuracy/performance compared to no-transfer baseline; survey notes qualitative effectiveness in dynamic environments but computational cost",
            "transfer_performance_sim": null,
            "transfer_performance_real": null,
            "transfer_success": null,
            "domain_randomization_used": null,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": "distribution mismatch between source and target (visual domain shift, dynamics mismatch), scarcity of labeled real target data, compute/resource constraints for real-time application",
            "transfer_enabling_conditions": "availability of some target-domain data for fine-tuning, high-quality feature extraction, computational resources for domain adaptation, and robust intermediate representations",
            "fidelity_requirements_identified": "paper indicates transfer learning excels when simulation closely aligns with reality but fails with large domain mismatches; no quantitative thresholds provided",
            "fine_tuning_in_real_world": null,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": false,
            "fidelity_comparison_results": null,
            "key_findings": "Transfer learning and domain adaptation are effective when simulation is reasonably close to reality or when limited target-domain labels are available; they require computational resources and careful feature design to be practical in real-time small-scale platforms.",
            "uuid": "e1680.3",
            "source_info": {
                "paper_title": "Autonomous Driving Small-Scale Cars: A Survey of Recent Development",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "DeepRacer (sim-to-real mention)",
            "name_full": "AWS DeepRacer (simulation and cloud environment with sim2real support)",
            "brief_description": "Amazon Web Services' 1/18th scale racing platform with a cloud simulator and support for domain randomization and DRL experiments aimed at learning-to-race with sim-to-real considerations.",
            "citation_title": "Autonomous Driving Small-Scale Cars: A Survey of Recent Development",
            "mention_or_use": "mention",
            "agent_system_name": "AWS DeepRacer car and cloud learning ecosystem",
            "agent_system_description": "A small-scale Ackermann-steering race car with onboard camera (optionally LiDAR) and Intel Atom-based compute, paired with a cloud simulator for training DRL and testing policies before real-world deployment.",
            "domain": "autonomous racing and lane-keeping for small-scale cars",
            "virtual_environment_name": "DeepRacer cloud simulator (AWS)",
            "virtual_environment_description": "Online simulator within AWS ecosystem supporting customization of training environments, reward functions, and domain randomization options; simulates track visuals, basic vehicle dynamics and sensors required for racing tasks.",
            "simulation_fidelity_level": "moderate; designed for fast experimentation rather than full physical fidelity, with domain randomization options to improve robustness",
            "fidelity_aspects_modeled": "rendered camera images, track geometry, basic vehicle dynamics, optional sensor models (camera, LiDAR in upgraded kits), ability to introduce delays/sampling differences as observations",
            "fidelity_aspects_simplified": "detailed tire/contact dynamics, exact actuator latencies, photometric camera artifacts and nuanced lighting/weather effects likely simplified",
            "real_environment_description": "real AWS DeepRacer cars used on physical tracks for racing; real sensors and hardware latency present",
            "task_or_skill_transferred": "autonomous racing, lane keeping, obstacle avoidance learned in cloud sim and deployed to physical DeepRacer cars",
            "training_method": "DRL (various RL algorithms), imitation learning in some contexts; training occurs in cloud simulator",
            "transfer_success_metric": "studies and AWS examples measure lap time, completion rate, collisions; survey notes techniques like adding delays/sampling as observations can improve robustness",
            "transfer_performance_sim": null,
            "transfer_performance_real": null,
            "transfer_success": null,
            "domain_randomization_used": true,
            "domain_randomization_details": "DeepRacer provides domain randomization options in its simulator (paper cites support); typical randomizations include visual variations and possibly dynamics parameter ranges",
            "sim_to_real_gap_factors": "simplified dynamics and sensing in cloud sim, unmodeled actuator and timing differences, differing lighting and camera characteristics",
            "transfer_enabling_conditions": "use of domain randomization, including delays and sampling rates as observations during training, compact observation spaces suited to on-board compute",
            "fidelity_requirements_identified": "paper asserts DRL zero-shot transfer and sim-to-real for DeepRacer depends on adequate simulation fidelity and inclusion of relevant randomized parameters; no numeric thresholds provided",
            "fine_tuning_in_real_world": null,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": false,
            "fidelity_comparison_results": null,
            "key_findings": "DeepRacer's cloud simulator and domain randomization options make it a practical platform for experimenting with sim-to-real RL, but successful transfer depends on modeling or randomizing the key aspects the policy relies on (e.g., delays, camera appearance); the survey notes examples where including timing/delay as observations improved robustness.",
            "uuid": "e1680.4",
            "source_info": {
                "paper_title": "Autonomous Driving Small-Scale Cars: A Survey of Recent Development",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Duckietown (sim-to-real mention)",
            "name_full": "Duckietown / Gym-Duckietown (educational & research sim environment)",
            "brief_description": "Duckietown platform and its Gym-Duckietown simulator provide a fast, customizable environment and simulation options (including domain randomization) for training perception and control agents for lane following and multi-agent scenarios.",
            "citation_title": "Autonomous Driving Small-Scale Cars: A Survey of Recent Development",
            "mention_or_use": "mention",
            "agent_system_name": "Duckiebot (Duckietown Duckiebots and ecosystem)",
            "agent_system_description": "Differential-drive small robots (Duckiebots) with RGB camera, IMU, encoders and basic distance sensing used for lane keeping, traffic sign detection, obstacle avoidance and multi-agent tasks in scaled urban layouts.",
            "domain": "autonomous driving education and research (indoor small-scale)",
            "virtual_environment_name": "Gym-Duckietown",
            "virtual_environment_description": "A fast and customizable simulation environment tailored to Duckietown tasks; simulates lane markings, traffic furniture, camera images and supports rapid iteration of algorithms.",
            "simulation_fidelity_level": "lightweight / fast simulator focused on task-relevant visual features rather than full physical fidelity",
            "fidelity_aspects_modeled": "camera images with lane/traffic sign rendering, simple dynamics for Duckiebots, multi-agent interactions and environment layout",
            "fidelity_aspects_simplified": "differential-drive dynamics approximate real vehicle behavior; many sensor noise sources, real optics, lighting variations, and precise actuator latencies are simplified unless explicitly randomized",
            "real_environment_description": "physical Duckietown testbeds (tabletop roads, toy traffic lights, Duckiebots) with real camera imagery and sensors; some setups use watchtower localization or centralized infrastructure",
            "task_or_skill_transferred": "lane keeping, traffic sign detection, obstacle avoidance, multi-agent coordination",
            "training_method": "imitation learning (BC, DAgger) and reinforcement learning; perception networks trained with mixed sim/real datasets in some studies",
            "transfer_success_metric": "task success in lane following/obstacle avoidance; survey references comparative evaluations (e.g., BC, GAIL, DAgger) but does not report aggregate numbers",
            "transfer_performance_sim": null,
            "transfer_performance_real": null,
            "transfer_success": null,
            "domain_randomization_used": true,
            "domain_randomization_details": "Duckietown simulation provides domain randomization options (paper notes Duckietown supports this) including visual and environment parameter variations to improve robustness",
            "sim_to_real_gap_factors": "visual domain shift (lighting, camera characteristics), dynamics mismatch from differential-drive simplifications, simplified sensor noise models",
            "transfer_enabling_conditions": "compact perception representations (affordances), use of domain randomization and mixed real/sim datasets, and approaches that limit dependence on per-pixel photorealism",
            "fidelity_requirements_identified": "survey indicates Gym-Duckietown is effective for fast prototyping but transfer benefits from modeling or randomizing key visual and temporal factors; no quantitative fidelity thresholds given",
            "fine_tuning_in_real_world": null,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": false,
            "fidelity_comparison_results": null,
            "key_findings": "Duckietown's simulator is a commonly used testbed for studying Sim2Real; domain randomization and hybrid sim/real training improve transfer, while simplified vehicle dynamics and visual mismatches remain key gap contributors.",
            "uuid": "e1680.5",
            "source_info": {
                "paper_title": "Autonomous Driving Small-Scale Cars: A Survey of Recent Development",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Donkey-Gym / Donkeycar (sim mention)",
            "name_full": "Donkey-Gym / Donkeycar (Unity-based simulation and hardware platform)",
            "brief_description": "Donkeycar is an open-source 1/10th scale platform for end-to-end driving experiments; Donkey-Gym uses the Unity engine to provide improved physics and graphics for training policies intended for transfer to Donkeycar hardware.",
            "citation_title": "Autonomous Driving Small-Scale Cars: A Survey of Recent Development",
            "mention_or_use": "mention",
            "agent_system_name": "Donkeycar platform and Donkey-Gym simulator",
            "agent_system_description": "A community-driven, customizable small-scale Ackermann-steering vehicle platform (Donkeycar) with camera-centric perception and an associated Unity-based simulator (Donkey-Gym) used for DRL and IL experiments.",
            "domain": "autonomous driving (end-to-end lane following and obstacle avoidance on small-scale vehicles)",
            "virtual_environment_name": "Donkey-Gym (Unity engine)",
            "virtual_environment_description": "Unity-based simulator providing enhanced physics and graphics compared to simpler simulators, generating rendered camera images and simulating vehicle dynamics for training agents.",
            "simulation_fidelity_level": "moderate (improved rendering and physics compared to very lightweight sims), but still not full automotive-grade fidelity",
            "fidelity_aspects_modeled": "camera rendering (visual features), vehicle kinematics/dynamics approximations, configurable environment geometry",
            "fidelity_aspects_simplified": "tire/contact dynamics, precise actuator delays, detailed sensor noise models and photometric camera effects often not modeled unless specifically added",
            "real_environment_description": "Donkeycar hardware built on 1/10th RC chassis with Raspberry Pi/Jetson and camera; used on indoor/outdoor tracks with real lighting and real sensor artifacts",
            "task_or_skill_transferred": "end-to-end lane keeping, steering-angle prediction, basic obstacle avoidance",
            "training_method": "imitation learning, DRL in Unity-based simulator, and supervised learning for perception pipelines",
            "transfer_success_metric": "task completion and generalization to real track conditions, collision rates; survey reports qualitative use cases but no unified numerical outcomes",
            "transfer_performance_sim": null,
            "transfer_performance_real": null,
            "transfer_success": null,
            "domain_randomization_used": null,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": "visual mismatch between Unity renderings and real camera images, simplified actuator/servo dynamics, unmodeled lighting and camera artifacts",
            "transfer_enabling_conditions": "higher-fidelity rendering and physics in Unity, careful dataset augmentation (style transfer), and leveraging mixed sim/real data for perception training",
            "fidelity_requirements_identified": "paper suggests Unity-based simulators like Donkey-Gym improve realism and ease transfer but does not specify quantitative fidelity thresholds",
            "fine_tuning_in_real_world": null,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": false,
            "fidelity_comparison_results": null,
            "key_findings": "Donkey-Gym (Unity) offers improved visual/physics fidelity for Donkeycar training and is a common choice for end-to-end experiments; transfer benefits from realistic rendering and data-augmentation techniques but residual sim/real visual and dynamic mismatches remain challenges.",
            "uuid": "e1680.6",
            "source_info": {
                "paper_title": "Autonomous Driving Small-Scale Cars: A Survey of Recent Development",
                "publication_date_yy_mm": "2024-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Deepracer: Autonomous racing platform for experimentation with sim2real reinforcement learning",
            "rating": 2,
            "sanitized_title": "deepracer_autonomous_racing_platform_for_experimentation_with_sim2real_reinforcement_learning"
        },
        {
            "paper_title": "Domain randomization for transferring deep neural networks from simulation to the real world",
            "rating": 2,
            "sanitized_title": "domain_randomization_for_transferring_deep_neural_networks_from_simulation_to_the_real_world"
        },
        {
            "paper_title": "Sim-to-real: Learning agile locomotion for quadruped robots",
            "rating": 2,
            "sanitized_title": "simtoreal_learning_agile_locomotion_for_quadruped_robots"
        },
        {
            "paper_title": "Mind the gap! a study on the transferability of virtual vs physical-world testing of autonomous driving systems",
            "rating": 2,
            "sanitized_title": "mind_the_gap_a_study_on_the_transferability_of_virtual_vs_physicalworld_testing_of_autonomous_driving_systems"
        },
        {
            "paper_title": "Zero-shot policy transferability for the control of a scale autonomous vehicle",
            "rating": 2,
            "sanitized_title": "zeroshot_policy_transferability_for_the_control_of_a_scale_autonomous_vehicle"
        },
        {
            "paper_title": "A platform-agnostic deep reinforcement learning framework for effective sim2real transfer towards autonomous driving",
            "rating": 1,
            "sanitized_title": "a_platformagnostic_deep_reinforcement_learning_framework_for_effective_sim2real_transfer_towards_autonomous_driving"
        },
        {
            "paper_title": "Sim-to-real transfer for deep reinforcement learning with stochastic state transition delays",
            "rating": 1,
            "sanitized_title": "simtoreal_transfer_for_deep_reinforcement_learning_with_stochastic_state_transition_delays"
        },
        {
            "paper_title": "Learning to drive (l2d) as a low-cost benchmark for real-world reinforcement learning",
            "rating": 1,
            "sanitized_title": "learning_to_drive_l2d_as_a_lowcost_benchmark_for_realworld_reinforcement_learning"
        }
    ],
    "cost": 0.025480249999999996,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Autonomous Driving Small-Scale Cars: A Survey of Recent Development
20 Dec 2024</p>
<p>Dianzhao Li 
Paul Auerbach 
Ostap Okhrin 
Autonomous Driving Small-Scale Cars: A Survey of Recent Development
20 Dec 2024952DE54D9C108641739E50EF676D9A7DarXiv:2404.06229v2[cs.RO]Small-scale carautonomous drivingrobotics
While engaging with the unfolding revolution in autonomous driving, a challenge presents itself, how can we effectively raise awareness within society about this transformative trend?While full-scale autonomous driving vehicles often come with a hefty price tag, the emergence of scaled-down, small-scale car platforms offers a compelling alternative.These miniature vehicles are designed to perform predefined tasks and challenges, equipped with onboard sensors, processing units, and control actuators.These platforms not only serve as valuable educational tools for the broader public and young generations but also function as robust research platforms, contributing significantly to the ongoing advancements in autonomous driving technology.This survey outlines various small-scale car platforms, categorizing them and detailing the research advancements accomplished through their usage.The conclusion provides proposals for promising future directions in the field.</p>
<p>I. INTRODUCTION</p>
<p>O VER the past few decades, extensive research on au- tonomous driving (AD) has been conducted by both academic communities and industry stakeholders.However, the fruition of fully functional Level 5 autonomous driving systems, which entails the availability of completely autonomous vehicles in the mass market, is anticipated to materialize in the coming decades [1], [2].The question arises: Can we proactively prepare our society for the oncoming fully autonomous driving?Despite assertions from researchers that autonomous vehicles (AVs) will mitigate human error and enhance safety compared to human drivers, public apprehensions persist regarding the ethical and safety dimensions of AVs [3], [4].Encouragingly, individuals with prior experience with AVs and younger generations exhibit a more optimistic stance toward these technologies [5].Hence, the affirmative response to this question is clear.Given that those who will further develop the AD systems are currently in high school, and those who will coexist with AVs are presently in elementary schools, the optimal preparation involves providing opportunities for the public to engage with AD systems now.To this end, a small-scale car platform emerges as an ideal choice, serving educational purposes and facilitating researchers in testing their autonomous systems on a tangible platform.Dianzhao Li and Ostap Okhrin are with the Chair of Econometrics and Statistics, esp. in the Transport Sector, Technische UniversitÃ¤t Dresden, Dresden, 01187, Germany and Center for Scalable Data Analytics and Artificial Intelligence (ScaDS.AI) Dresden/Leipzig, Germany.(E-mails: dianzhao.li@tudresden.de;ostap.okhrin@tu-dresden.de)Paul Auerbach is with Barkhausen Institut gGmbH, 01067 Dresden, Germany.(E-mail: paul.auerbach@barkhauseninstitut.org)Undoubtedly, research in AD research for full-scale cars holds promise.Despite the supportive environments for developing and testing AVs in various countries and regions, strict regulations persist due to concerns about safety, security, and public trust [6]- [8].These regulations often limit the ability to drive or test AVs on public roads, which consequently restricts the involvement of smaller research institutions or individual researchers in AD system development.As a solution, the use of small-scale car platforms offers a more cost-effective and accessible alternative for the public and research communities to engage with AD technologies.More specifically, small-scale car platforms can serve two main purposes: educational tools for students and research tools for AD researchers.In educational settings, small-scale cars prove to be an excellent choice for schools, especially with science, technology, engineering, and mathematics (STEM) focus, offering students their initial exposure and hands-on experience with AD while simultaneously fostering increased public awareness.On the research front, the availability of such research platforms lowers entry barriers, inviting a broader spectrum of researchers into AD exploration.As evidenced in Fig. 1(A), based on published studies each year on Google Scholar with the search terms: "small-scale car" or "robot car" from 2000 onward, the quantity of research papers focused on small-scale car platforms has experienced a substantial surge in the past two decades.Following a relatively gradual incline before 2016, this field has undergone a notable expansion, particularly catalyzed by the introduction of a series of well-known platforms.It is worth mentioning that AD research for small-scale cars is the ultimate goal of these small-scale car platforms, not the transformation of the techniques into real-size vehicles.Various major small-scale car platforms claim that these platforms are designed to be accessible and inexpensive, aimed at fostering educational and research activities [9]- [13].For example, as stated by Duckietown platform [9], their mission is Learning robotics and AI made tangible, accessible, and fun!They want to provide a tangible, accessible, and inclusive tool for a broader range of society to engage with robots and AV.Similarly, another major platform DeepRacer [10], provided by Amazon Web Services, also tries to offer developers of all skill levels for hands-on experience with fully autonomous 1/18th scale race cars with machine learning (ML).Another platform, F1TENTH's mission is to foster interest, excitement, and critical thinking about the increasingly ubiquitous field of autonomous systems [12].In comparison to full-scale AVs, small-scale car platforms employ more lightweight computation units and sensors for economic reasons.These platforms primarily use mature AV techniques from full-scale cars, albeit under simplified conditions due to current hardware limitations.Nonetheless, they play a crucial role in engaging undergraduate students and early career researchers in AV technologies, inspiring them, and contributing to the growth of the AV research community.This, in turn, indirectly advances real-world AV research.</p>
<p>A. Contributions</p>
<p>Despite the importance and widespread use of small-scale car platforms, no research currently provides a comprehensive overview of these platforms, including their hardware configurations, software frameworks, and benchmarking of driving tasks.In [14], the existing literature on small-scale cars is summarized; however, it lacks comprehensiveness and does not include a comparative analysis.More recently, [15] discussed existing platforms, but their focus was limited to the platforms themselves, without addressing the techniques used to advance small-scale car AD research.Identifying this gap, we aim to advance ongoing research initiatives and enhance public awareness by conducting a comprehensive survey of existing small-scale platforms for educational and research purposes.Additionally, we seek to benchmark the AD tasks accomplished by these platforms.To the best of our knowledge, this survey represents the first of its kind to provide such an extensive analysis.The contributions of this work are as follows:</p>
<p>â€¢ Comprehensive Review of Platforms: We thoroughly review the most widely available small-scale car platforms, comparing their hardware and software configurations to provide readers with an in-depth understanding and a guide for selecting or building their own platforms.â€¢ Benchmarking Autonomous Driving Tasks: By analyzing over 250 research papers, we categorize the driving tasks for small-scale cars into two primary pipelines-modular pipelines and end-to-end pipelines.We discuss in detail the techniques employed in each module within these pipelines.â€¢ Proposed Future Directions: After examining the existing techniques, we propose potential improvements for both the software and hardware aspects.On the software side, we suggest techniques to enhance driving behaviors, while on the hardware side, we recommend platform improvements to facilitate more advanced research.The rest of this paper is organized as follows: In Section II, we examine commonly employed small-scale car platforms, accompanied by insights into their simulators.Following that in Section III, we inspect the sensor setups employed in these platforms.Section IV provides a comprehensive overview of the AD tasks accomplished within the research community.Section V outlines promising directions for future developments, and we draw our conclusions in Section VI.</p>
<p>II. PLATFORMS</p>
<p>For full-scale cars, the standardized dynamics and sizes are mandated by road regulations.In contrast, smaller-scale cars, designed without regulatory constraints, are crafted for diverse research purposes by various groups of researchers and enthusiasts.Recognizing these disparities, this survey establishes a clear definition for AD robot cars, setting them apart from other autonomous robots.In this context, an AD small-scale car platform refers to a miniature vehicle equipped with technologies enabling autonomous operation.This includes sensors, processing units, control actuators, and often a set of predefined tasks or challenges for testing and development.In terms of size, namely the scale factors, there is considerable variability among different platforms.Platforms intended to carry a lot of sensors and for outdoor operation can reach almost 1 meter in length [11].On the other end of the spectrum, platforms developed for investigating swarm dynamics are notably compact, with some as small as 3.3x3.3cm[22].The most common form factor is the 1/10th scale, representing a model approximately one-tenth the size of a full-scale car.This scale is frequently employed in hobby Remote-Control (RC) vehicles, forming the foundation for numerous platforms [12], [13], [19], [23]- [28].Regarding the dynamic system, full-scale cars commonly use Ackermann steering mechanics, rotating the front axle to facilitate left or right turns [29].However, due to the mechanical complexity of this system and its limitations in certain situations, smallscale car platforms initially favored the use of a differential steering system.In this system, the left and right sets of wheels are driven independently, enabling turns by driving one set of wheels faster than the other.Nevertheless, as illustrated in Fig. 1, recent advancements in this field have seen the increased adoption of the more realistic Ackermann steering system in small-scale car platforms.</p>
<p>A. Hardware Platforms</p>
<p>Before we introduce the different platforms, we categorize small-scale car platforms into distinct groups based on their target users and complexity.</p>
<p>1) Educational Platforms: First are the educational platforms predominantly accessible in the commercial market, as shown in Table I, including Makeblock mBot, Edison, AlphAI Robot, TinkerGen MARK, and Robolink Zumi.These platforms are carefully crafted to engage students ranging from early childhood to elementary school and even graduate programs in the interactive exploration of autonomous cars.They offer users comprehensive tools, enabling them to construct effortlessly and program robot cars capable of executing diverse tasks.These platforms are equipped with basic sensors such as line sensors, distance sensors, and occasionally cameras.With limited processing power, most rely on microcontrollers, they are suitable only for simple tasks like lane keeping or car following.However, their support for simple frameworks and visual programming languages makes them ideal for undergraduate education.Many also come with pre-made courses designed to teach students the fundamentals of their operation.Their low cost and commercial availability further enhance their accessibility for educational purposes.Despite these advantages, these platforms are not well-suited for researchers exploring autonomous or connected driving.Their closed-source hardware and software limit extensibility, and most lack networking capabilities for external control (A) Based on published studies each year on Google Scholar with the search terms: "small-scale car" or "robot car", the research on small-scale cars has seen substantial growth in the number of papers over the years.In the early 2000s, projects like s-bot [16], e-puck [17], and TurtleBot emerged.</p>
<p>Starting around 2016, with the introduction of Duckietown [9], BARC [13], and Autorally [11], there was a significant increase in research papers.This trend continued with the development of projects like DeepRacer [10], Donkeycar, and F1TENTH [12].More computationally advanced small-scale cars have been introduced in recent years, such as ART/ATK [18] and XTENTH-CAR [19].(B) Examples of small-scale car platforms, categorized into educational platforms and research platforms, including multiple vehicle setups such as ORAC [20] and UDSSC [21].or integration into multi-vehicle scenarios.Additionally, their differential-drive steering design does not accurately replicate real car dynamics, making them inadequate for serious investigations into realistic vehicular behavior.</p>
<p>2) Commercial Research Platforms:</p>
<p>The subsequent tier has a more sophisticated system, affording users enhanced opportunities to innovate and develop new functionalities within the platform.These platforms also mostly offer support for more advanced frameworks like Robot Operating System (ROS) and standard programming languages such as Python to incorporate them in bigger research projects.Their inclusion of networking capabilities also aids in incorporating them in diverse environments.The typical platforms include e-puck [17], Pheeno [31], Thymio [30], MarXbot [32], Turtlebot, and Duckietown [9].The e-puck, Pheeno, Thymio, and MarXbot are frequently used in swarm robotics research due to their compact size and reliance on basic sensors.The Duckietown ecosystem originated in 2016 at MIT as an educational tool for instructing students in the realms of autonomous and connected driving but developed into a research platform for all levels of researchers.Comprising cost-effective vehicles known as Duckiebots, each unit is outfitted with either a Raspberry Pi or a Jetson Nano as its computing unit.Sensorwise, the Duckiebots employ an RGB Raspberry Pi camera, an inertial measurement unit (IMU), wheel encoders, and a front-facing distance sensor.The platform also facilitates the incorporation of a central localization system through "watchtowers."Propelled by two DC motors on each of the two driven wheels, the cars adhere to a differential drive pattern.Duckietown is commercially available and widely used for tasks such as lane keeping using the RGB camera [33], [34], as well as obstacle and traffic sign detection [35], [36].Its support for diverse frameworks and sensors, along with its open-source nature, makes it applicable to various target groups and tasks.Comprehensive documentation and a large community further enhance its adaptability and integration into different environments.The differential-drive steering setup in Duckietown, similar to educational platforms, is a significant limitation.Another commercially available platform is the AWS DeepRacer [10] by Amazon Web Services.This platform also relies on an RGB camera for sensory input, supplemented by an upgrade kit featuring a second camera and a LiDAR sensor.The execution of learned strategies is managed by a small compute board equipped with an Intel Atom CPU.With Ackermann steering implementation, DeepRacer is also primarily used for tasks like lane keeping [37], [38] and obstacle avoidance [39], [40].While the platform offers opensource software to interface with its hardware, its heavy reliance on Amazon Web Services limits extensibility beyond the intended use of the manufacturer.However, AWS provides extensive documentation, including detailed courses on setup and operation within its software ecosystem.</p>
<p>3) Open Source Platforms: For users seeking open-source platforms and aiming to build a system from the ground up, two popular options are Donkeycar [23] and F1TENTH [12].Donkeycar is a community-driven platform that provides detailed instructions for constructing a customizable vehicle.Its open-source nature allows significant flexibility in sensor configurations and system design.The base platform is built on a 1/10th-scale RC car with Ackermann steering, equipped with a Raspberry Pi or Jetson Nano as the compute unit and a Raspberry Pi camera as the primary sensor.Users can enhance the platform by adding sensors such as IMUs, encoders, and even LiDAR.Given its open-source framework, users can extend the platform's functionality according to their needs.Originally designed for autonomous racing, Donkeycar is commonly used for tasks like lane keeping [41], [42] and obstacle avoidance [43].However, its versatility extends to a wide range of applications due to its open-source framework.The community behind the project also offers comprehensive documentation on building and using the platform.Its Pythonbased custom framework supports integration with other systems, such as ROS, further expanding its functionality.While primarily designed for single-vehicle use, the platform could be extended to enable coordination among multiple vehicles.Despite its advantages, the complexity and the need for manual assembly and configuration may deter researchers seeking outof-the-box solutions.F1TENTH, like Donkeycar, is based on a 1/10th-scale RC car with Ackermann steering.It features a Nvidia Jetson as its compute unit and a 2D LiDAR as the primary sensor.Additional hardware includes an IMU, odometry data provided by the VESC motor controller, and an optional RGB camera.With its high-speed capable base platform, F1TENTH is primarily utilized for autonomous racing tasks [44], [45], though it is also applied in fundamental tasks like lane keeping [46] and obstacle avoidance [47], [48].Thanks to its relatively powerful compute unit, F1TENTH can support advanced applications, including ML-based decisionmaking.The big advantage of these two platforms lies in their open-source nature and comprehensive online documentation, enabling extensive customization, such as incorporating additional sensors or connecting to simulators.However, this flexibility comes with the challenge of increased complexity, as users must build and configure the platforms from scratch, an approach that may not be suitable for all researchers.</p>
<p>4) Outdoor Platforms: For outdoor applications, the Autorally [11] platform takes center stage, built on an off-theshelf RC car platform.Unlike smaller platforms such as Donkeycar and F1TENTH, AutoRally uses a larger 1/5thscale model car, allowing it to accommodate more powerful hardware.Its compute unit is a consumer-grade computer mainboard equipped with an Intel i7 CPU, enabling advanced processing capabilities.The platform features a sophisticated sensor suite, including two industrial synchronized RGB cameras for stereo imaging, an IMU, magnetic encoders for odometry, and a GPS receiver.Due to its substantial size and high-performance components, Autorally is mainly employed for racing tasks [49]- [51].These tasks often utilize various sensor fusion strategies, such as combining GPS and IMU data [52], [53] or integrating camera and IMU data [54], [55].While the diverse set of sensors and substantial computing power allow the platform to be used for different tasks, its large size and high-speed capabilities necessitate an outdoor environment, making it unsuitable for smaller lab setups.The project provides instructions for constructing the vehicle, setting up the software, and integrating it into the ROS framework.However, the lack of comprehensive documentation may present challenges for users attempting to fully leverage its potential.</p>
<p>5) Multi Vehicle Platforms:</p>
<p>The significance of a smallscale smart city platform, accommodating multiple vehicles, is underscored as it plays a crucial role in achieving harmony and efficient collaboration of a fully autonomous driving world.To this end, the University of Delaware Smart Scaled City (UDSSC) [21], developed for education and research on connected and autonomous driving, emerges as a distinctive contribution.UDSSC features a scaled-down urban environment with diverse traffic scenarios, including intersections and roundabouts, and uses Micro Connected and Automated Vehicles (MCAVs).These vehicles are built on the Pololu Zumo platform and feature a differential-drive DC motor setup with encoders, an IMU, and an Arduino microcontroller.Each MCAV is further equipped with a Raspberry Pi as its compute unit, a line-following sensor, and a front-facing distance sensor.Notably, the absence of onboard cameras or LiDAR necessitates the use of a centralized localization system for precise positioning.The platform is tailored for cooperative and connected driving tasks, such as merging [21] and roundabout navigation [56]- [58].However, it is constrained by several limitations.The reliance on an external localization system ties the platform to a specific physical testbed, as relocating the system requires substantial effort.Additionally, the lack of onboard sensors for localization restricts its use to studies on multi-vehicle interactions.Unfortunately, the UDSSC platform is not open source; its hardware and software are proprietary, and it is not available for purchase.Moreover, the absence of detailed documentation limits its usability.As a result, the platform primarily serves as inspiration for developing custom solutions with similar capabilities.</p>
<p>For the sake of brevity, the details of commonly used platforms have been summarized in tables.Table II presents a comparative hardware analysis of various platforms, highlighting key characteristics such as steering dynamics, actuators, installed sensors, and prices, whether for crafting or purchasing a platform.Table III focuses on the software aspect, detailing the software frameworks employed and, where applicable, the simulation tools integrated into each platform.Additionally, the primary focus tasks of each platform are provided as a reference for those interested in exploring specific driving behaviors.</p>
<p>B. Simulators</p>
<p>Before employing the autonomous system in real platforms, a simulation is often used first to test the performance.For small-scale cars, numerous simulators exist to facilitate the training, testing, and evaluation of autonomous systems.</p>
<p>First, for the requirements of simulating the vehicle dynamics and control systems, CarMaker and CarSim are widely recognized.These platforms provide environments for assessing various aspects of vehicle behavior, including handling, braking, and acceleration, under diverse driving conditions.For autonomous systems, sensor models are as crucial as physics engines.Gazebo [68] is a prominent open-source platform in robotics, offering modular systems for integrating diverse sensors and physics models.Gazebo-based simulations have been developed for small-scale platforms like Turtlebot, Duckietown, DeepRacer, F1TENTH, MuSHR, and Autorally, demonstrating its flexibility and adaptability to different research needs.Other robotic simulators such as ARGoS and V-REP are also used as bases for small-scale car platforms [17], [22].For macro-scale simulations, SUMO [69] is a leading platform for analyzing transportation systems, optimizing road networks, and developing traffic management strategies.It has been used for training traffic control algorithms in multi- vehicle systems, as discussed in [58].In the realm of more detailed autonomous driving setups, various open-source simulators have gained popularity, with TORCS [70], CARLA [71], AIRSIM [72], and LGSVL [73] being notable examples.These platforms feature high-fidelity physics engines and extensive sensor support, enabling hyper-realistic simulations of traffic environments.They are particularly effective for training larger small-scale vehicles, such as 1/5th scale models, before realworld deployment [74]- [77].In addition to general-purpose simulators, several platforms are tailored to specific smallscale systems.For example, Gym-Duckietown provides a fast and customizable environment for Duckietown, supporting various driving tasks.The DeepRacer cloud simulator by Amazon offers an online platform for customizing algorithms and testing them within the DeepRacer ecosystem.Donkey-Gym, designed for Donkeycar, uses the Unity game engine for enhanced physics and graphics.F1TENTH Gym is optimized for F1TENTH platforms, offering specialized simulation capabilities.AutoDRIVE simulator provides a comprehensive environment for various driving tasks for AutoDRIVE cars.For multi-agent systems, platforms like MADRaS [78] (built on TORCS) and Flow [79] (built on SUMO) are frequently employed.These tools facilitate the study of interactions and behaviors in complex traffic and multi-agent environments.In Table III, a summary of simulators used for main small-scale car platforms is shown.</p>
<p>Communication in robotics is as critical as the simulator itself, and ROS [80] serves as a key middleware platform for managing robotic software development.ROS facilitates communication between various robotic components, including sensors, actuators, and decision-making algorithms, enabling seamless integration and scalability.It also offers a vast collection of pre-built packages and libraries for common tasks like motion planning and image processing.Currently, ROS exists in two versions: ROS 1 and ROS 2. ROS 1, the original version, is widely used and supported by the community; however, its centralized architecture limits real-time capabilities and lacks robust security features.In contrast, ROS 2 addresses these limitations with a decentralized design based on the Data Distribution Service, offering enhanced scalability, real-time support, and Quality of Service.While ROS 1 remains suitable for projects reliant on mature tools and is ideal for educational or non-real-time applications, ROS 2 is recommended for new projects requiring real-time performance, high scalability, security, and cross-platform compatibility.Nevertheless, the Simulation to Reality (Sim2Real) gap remains a challenge that needs to be addressed before successfully deploying the trained algorithms in the real world.</p>
<p>C. Sim2Real Transfer</p>
<p>Sim2Real is a concept in robotics and AVs that involves transferring skills, knowledge, or models acquired in a simulated environment to real-world applications.Here, the realworld setting or scenario in which the robot is intended to operate and execute tasks is termed the target domain.Conversely, the data, and experiences, shaping the development of the robotic system are called the source domain.The core objective of Sim2Real is to devise algorithms and methodologies capable of effectively bridging the disparities between these two domains, known as the Sim2Real gap.For instance, this gap may manifest as dynamic differences or discrepancies in the sensing part, where the simulated images and the real images are different.Although the Sim2Real transfer is primarily centered on transferring Deep Reinforcement Learning (DRL) policies from simulation to the real world, it can also be more broadly considered as ML problems for the sensing part, for the agent facing situations in the real world that have not appeared in simulation.To address the Sim2Real gap, an array of methods is proposed, including system modeling, dynamics randomization, and randomization for sensing.Here we introduce the most used methods for small-scale cars.</p>
<p>The first approach for the real-world application of smallscale cars is Zero-shot Transfer, where the trained model is directly applied in real-world settings.Imitation Learning (IL), based on datasets from real platforms, trains agents to mimic expert behavior [11], [33], [54], [55], [62], [81]- [83].While efficient, this method assumes simulation and real-world environments are similar.DRL-based Zero-Shot Transfer, achieved via compact observation and output spaces [84]- [86], depends heavily on simulation fidelity for success.Another noteworthy approach is Transfer Learning, which aims to improve the performance of the target agent in the target domain by transferring the knowledge contained in different but related source domains [87].It excels when simulation closely aligns with real-world scenarios but falters with significant domain mismatches.For small-scale cars, this method balances efficiency and adaptability in controlled conditions.When the labeled data in the target domain is scarce or expensive, Domain Adaptation is used.As a subset of transfer learning, it seeks specifically to minimize the distribution mismatch between the source and target domains, allowing the model to generalize better across different domains.It is effective in dynamic environments but requires computational resources and high-quality feature extraction, posing challenges in real-time applications.Another frequently used approach is Domain Randomization, in which the randomness and variation are introduced during simulation to make the model more robust to different real-world conditions [88].It works by for example randomly varying simulation parameters, such as lighting conditions, textures, object appearances, and physics properties during training.For small-scale car platforms, Duckietown and DeepRacer provide Domain Randomization option in the simulation, which is easier for the users to tackle Sim2Real issue [10], [37], [89], [90].While fostering generalization, excessive randomization may dilute the relevance of training data, compromising real-world performance.</p>
<p>III. SENSORS AND SENSOR SYSTEMS</p>
<p>In Section II-A, various platforms are explored, each employing diverse sensors to perceive their surroundings and execute tasks.Compared with full-scale AVs, which are equipped with various state-of-the-art sensors for perception [91], smallscale cars usually use lightweight sensors.Here, we discuss the most commonly utilized sensors across these platforms, providing their respective use cases.</p>
<p>A. Camera</p>
<p>An RGB camera provides a stream of images that is easily processed and understood by humans.This helps researchers to better understand the perspective of the robot.The amount of information provided by cameras is huge, which allows the robot to perceive a lot of information with just one sensor in a short amount of time.The downside of this is, that they need a high bandwidth communication link to whichever part of the robot needs those information.Also with the information in images being so densely encoded, it is difficult for a robot to understand the environment.</p>
<p>The field of computer vision has emerged as a crucial scientific discipline, facilitating the extraction of pertinent details from images and videos.In recent years, the advent of deep learning (DL) models, such as You Only Look Once (YOLO) [92], has significantly contributed to enhancing the robot's ability to interpret scenes captured by cameras.It is worth noting that cameras face external influences, such as varying lighting conditions, which can substantially alter the captured images.In extreme scenarios, such as very dark environments or instances where direct light affects the camera, the effectiveness of cameras may be compromised.A big advantage of cameras is their wide pricing range, starting from 15 USD, corresponding to images of different quality in regards to resolution and dynamic range.They are also easy to interface with, usually requiring only a USB connection.Some tasks can be accomplished only with cameras, e.g.traffic sign or road marking detection [93]- [95].</p>
<p>Beyond RGB cameras, more advanced options exist, including those detecting optical flow [96] (the apparent motion of objects) or depth cameras [97], employing different methods to ascertain the distance to objects.These advanced cameras provide both depth information and standard RGB images.In the context of autonomous tasks for single-car platforms discussed in this paper, cameras emerge as a ubiquitous choice.Their cost-effectiveness and information-rich output make them the preferred sensor for many platforms.Among the commonly used cameras in scaled vehicles, the Raspberry Pi Camera stands out [9], [23], [28], [31], [59], [62].Designed for the Raspberry Pi, it combines affordability with decent image quality, offering a resolution of 1080p and a dynamic range of 44dB.Another popular option used in the discussed platforms is the Intel Realsense line of cameras [25], [26].Those include two cameras and an IR projector, which allow to capture not only an RGB image but also depth information of each pixel using stereovision.The IR projector helps to improve accuracy in scenes with poor textures.The manufacturer claims an accuracy in the centimeter range.</p>
<p>The big advantage of cameras is their versatility combined with their low price.Their dense stream of information makes them suitable for almost all tasks discussed in this paper.Although different approaches for processing the information from cameras need to be applied depending on the task.Applying DL approaches helps with this in many different fields and allows for quickly iterating through different approaches.One major drawback of relying solely on cameras is their susceptibility to varying environmental conditions, as previously mentioned.This sensitivity complicates the transfer of control strategies from simulations to real-world vehicles.For successful transfer, simulations must either model these environmental influences with high accuracy or incorporate an additional interpretation layer that bridges the gap between simulation and reality.</p>
<p>B. LiDAR</p>
<p>LiDAR, another widely employed sensor in numerous platforms, functions by measuring the time taken for a beam of light to reflect off a surface.The LiDAR emits beams from multiple angles-typically ranging from 360 to 2048-creating a detailed point cloud map of objects in the environment.Multi-layer LiDAR systems, with laser beams spanning up to 128 angles, provide data on objects at different heights, generating a comprehensive 3D point cloud.LiDARs are available in various resolutions, offering enhanced detail, especially in complex and structured environments.The typical scanning frequency of LiDARs ranges from 5 to 20 Hz, with high-end models achieving detection ranges of up to 350 meters.</p>
<p>The distinct advantage of LiDAR over cameras lies in its direct identification of the surroundings, including the precise position of objects relative to the robot.In contrast, a camera first needs to identify an object in an image and then estimate its position.Moreover, LiDAR provides significantly more accurate positional information compared to estimations derived from camera images.However, unlike cameras, LiDARs lack the ability to discern details such as the color of the detected objects.This limitation renders them unsuitable, for example, in tasks like traffic signs or lane detection.LiDAR also has a significant advantage over cameras in that it is unaffected by environmental conditions such as varying lighting, and it can operate effectively in complete darkness.Additionally, some LiDAR models offer reflectivity data, which can help distinguish between different surface types.For example, they can differentiate highly reflective road markings from less reflective obstacles.However, LiDARs are typically more expensive than cameras.The most affordable hobbyist LiDARs start at around 150 USD for a basic single-plane sensor, with prices increasing rapidly as resolution or the number of planes expands.</p>
<p>Usually physically larger platforms use LiDAR sensors [10], [12], [13], [18], [23]- [27], [59], [62] as they tend to be bigger and heavier than cameras.Those platforms are also usually more versatile and used for more different tasks and therefore the additional cost of a LiDAR is feasible.In our investigation, all platforms utilizing LiDAR also integrate a camera to address the limitations of LiDAR for specific tasks.The LIDAR that is used by far the most on the platforms we investigated is the single plane RPLIDAR [13], [23], [26], [27], [59], [62].It offers a reasonable detection range of 12m and a resolution of 0.225 degrees for small-scale car platforms with a 360 field of view.And is one of the cheaper options costing around 400 USD.But it only allows for a 10 Hz scan frequency.The second most popular, higher end solution is the Hokuyo UST-10LX [12], [24], offering a 270 degree field of view and a 10m detection range.</p>
<p>Particularly tasks, that require precise information about the location and orientation of surrounding vehicles or obstacles benefit greatly from the improved accuracy of LIDARs over cameras, these include car following, overtaking, or obstacle avoidance.Tasks such as lane following or traffic sign following need the additional visual information provided by cameras and can therefore not be accomplished with only LIDARs.</p>
<p>For economic considerations, cost-effective sensors are the mainstream choice for small-scale cars.These sensors typically offer lower resolution but demand less computational power.For example, the RPLiDAR is commonly used in smallscale cars, providing a detection range of up to 40 meters with error margins of approximately 1%.It has a data rate of around 32,000 samples per second.In contrast, typical LiDAR sensors used in full-scale AVs offer significantly higher range, resolution, and data rates.For instance, the Velodyne LiDAR sensor can detect up to 120 meters with a 2 cm error margin and a data rate of 1.3 million points per second.These hardware limitations make it challenging to develop new approaches, leading primarily to the application of mature AV techniques from full-scale cars under simplified conditions.However, advancements in semiconductor technology are expected to make more sophisticated hardware setups available for smallscale cars in the near future, which will unlock more advanced AV techniques for small-scale cars.</p>
<p>C. IMU and Encoder</p>
<p>Some sensors that are usually not used on their own but as an additional source of information for other sensors are IMU and odometry encoders.Where odometry encoders provide information about the angle by which each wheel of a robot has turned and therefore allow us to estimate where it has traveled, the IMU provides information on how fast a robot is spinning along each of its axes aiding in estimating the total rotation.</p>
<p>The information of these two types of sensors is usually combined with other absolute positioning inputs like a camera or a central localization system to fuse them in a filter like a Kalman filter [98] or in any other kind of perception module.IMUs are available at relatively affordable prices, ranging from 15 USD.The two IMU sensors most commonly used in the discussed platforms are the MPU9250 and MPU6050 [9], [17], [23], [26], [59].Although it has to be noted, that not all publications mention the exact model of the IMU used.Both sensors feature a 3-axis gyroscope, which measures the rotational speed, and a 3-axis accelerometer, which measures linear acceleration.The MPU9250 additionally offers a 3axis magnetometer to incorporate measurements of the Earth's magnetic field into orientation sensing.</p>
<p>Odometry encoders can be found in several different forms, ranging from magnetic encoders to optical ones.Their implementation is usually dependent on the platform, as they need to be tightly integrated into the mechanics of the robot.Despite their low cost, often under 10 USD, they offer reliable positional data, making them a popular choice.Almost all the platforms reviewed in this paper incorporate an IMU and some form of encoder, as these sensors are both affordable and effective, particularly in slow, controlled indoor environments.The notable exception is the smallest platform, primarily designed for swarm robotics research [22], where extreme cost constraints led to the exclusion of these sensors.</p>
<p>The addition of cheap odometry encoders or IMUs especially benefits tasks that require the knowledge of the precise location and orientation of the vehicles.This includes tasks such as path following, racing, or drifting.The disadvantage of this type of sensor lies in the need to incorporate it in a meaningful way, this usually involves either existing filters or using deep neural networks.In the first case, the additional filter needs to be tuned to provide meaningful information, in the case of the latter the training of the neural network becomes more complex.The big advantage lies in the small price and easily accessible information.</p>
<p>D. GPS</p>
<p>GPS is the least frequently used sensor with model scale platforms [11], [13] offer positional information in a global coordinate system.Most of the discussed platforms are designed for indoor use, which eliminates the use of GPS as the signal does not penetrate buildings.However, recognizing the pivotal role GPS plays in full-sized vehicles, many modelscale platforms opt to replace it with indoor central localization systems, as detailed in Section III-E, to enhance realism.GPS receivers are also comparatively cheap at around 30 USD.However, for those seeking higher precision, the cost can escalate to several hundred USD, especially for the more advanced GPS RTK systems [99].Such precision may become necessary for model-scale platforms, as conventional GPS systems typically only offer an accuracy of approximately 5 meters [100].The biggest disadvantage of GPS sensors for small-scale vehicles is their reliance on outdoor environments, which require significantly more space and make experiments heavily dependent on weather and other outdoor conditions.As a result, GPS sensors are only beneficial for platforms when the research focuses specifically on the accurate effects and phenomena associated with GPS data, such as multipath errors or signal interference.</p>
<p>E. Central Localization System</p>
<p>In situations where the emphasis is on studying the interaction among multiple vehicles rather than individual vehicle behavior, a central localization system streamlines the task, allowing a more focused approach to other aspects such as obstacle avoidance or overtaking, by providing precise information about the position and orientation of different objects and sometimes even their velocity.By decoupling the algorithms for (multi) vehicle behavior from the perception of individual vehicles, central systems enable the study of system dynamics in an idealized "best case" scenario.In this context, the term "best case" refers to a situation where a vehicle possesses precise knowledge of its own position and that of surrounding obstacles.However, it is essential to acknowledge that this assumption may not align with the complexities of real-world road situations.Conversely, central systems find utility in simulating GPS positioning within actual vehicles.</p>
<p>One of the cheapest options implemented in the considered platforms involves the use of special markers or tags on each vehicle, which are observed by one or multiple overhead cameras [101].These markers, based on ArUco [102] technology, are strategically placed on vehicles and key positions or obstacles.Knowing the intrinsic and extrinsic parameters of the cameras, existing libraries like OpenCV [103] can be employed to detect these markers and estimate their position and orientation.As discussed in Section III-A, cameras are relatively inexpensive, and the markers merely need to be printed out.While the resulting localization precision is reasonable [104], it does decline with the distance of the markers from the camera.Mitigating this issue involves the use of multiple cameras, necessitating synchronization and precise positioning of all cameras.</p>
<p>Off-the-shelf solutions for tracking also exist from various manufacturers, such as OptiTrack or VICON.These solutions predominantly rely on high-speed IR cameras and reflective markers on the tracked objects.The systems inherently provide position and orientation information for different marker assemblies (referred to as rigid bodies) without requiring additional processing.Notably, the accuracy of these systems surpasses that of solutions utilizing RGB cameras, with manufacturers often specifying sub-millimeter accuracy.However, the upfront cost is relatively high, typically exceeding 10,000 USD, and in some cases, additional licensing fees for the requisite software may apply [21], [63].</p>
<p>Central localization systems offer the advantage of high precision localization of any object within the test setup and are easily extendable to more vehicles of objects as no additional sensors need to be mounted on the vehicles themselves.They are however quite expensive and require a dedicated mounting setup to position the sensors, therefore they cannot easily be relocated.Their high precision allows for easy transferability from simulation to real-world testbed.The implementation of these systems especially benefits tasks, that do not study the influence of sensors and sensor behavior on the control algorithm, but merely the control strategies themselves.They can essentially be used as the single source of information for any of the tasks listed except traffic sign following.</p>
<p>F. Other sensors</p>
<p>Several platforms discussed in Section II-A employ additional sensors to either emulate sensors found in full-scale autonomous cars or fulfill specific tasks.</p>
<p>A prevalent type of sensor utilized in many platforms [9], [17], [26], [28], [30], [32], [62], [64], [66], [105] is the single-point distance sensor, available as either ultrasonic or infrared.These sensors operate on the time-of-flight principle.They measure the time it takes for a wave to reflect back to the sensor and calculate the distance accordingly.Typically positioned at the front of vehicles, they often mimic radar sensors and play a crucial role in tasks such as car following.</p>
<p>Line following sensors, consisting of an array of light sensors aimed at the ground, are employed in certain platforms [21], [26], [26] to assist visual line following, with a camera serving as a secondary input.These sensors can easily discern the line from the road by measuring the reflectance of the surface on multiple points along the length of the sensor.A limitation is their coverage area, which is relatively small and located underneath the car.If the vehicle deviates significantly from the line, the sensor may struggle to rediscover it.</p>
<p>One platform [101] utilizes RFID readers to detect specific points of interest on the road, such as the beginning of intersections or as indicators for turns on a crossroad.Turtlebot employs cliff sensors, which function as downward-facing distance sensors, indicating if the part of the vehicle with the sensor is suspended over a steep drop.This sensor serves to prevent the vehicle from inadvertently falling off a precipice in the environment.</p>
<p>For the platform designed for use with a high amount of robots [22], infrared (IR) transmitters and receivers are employed.These facilitate cost-effective communication between different robots and emulate a simple form of Vehicle-to-Vehicle (V2V) communication.</p>
<p>G. Compute Units</p>
<p>To process the information from the sensors and compute actuator movement according to their task, the platforms require a dedicated computational unit.As the physical size of the platforms is quite limited, the computational performance of these compute units is as well.The prevailing choice across numerous platforms [9], [21], [23], [26], [28], [60], [63], [101], [106] is the Raspberry Pi single-board computer (SBC).It offers a performance of about 32 GFLOPS leveraging both CPU and GPU.Built on a System on a Chip (SoC), the Raspberry Pi features a compact physical size but offers modest computational performance, with a 4-core 1.5GHz CPU.Despite its relatively low processing power, the Raspberry Pi is well-suited for the majority of tasks performed by these platforms.The Raspberry Pi also benefits from a large developer and research community providing a diverse set of frameworks and libraries to harness its capabilities.</p>
<p>A comparable alternative to the Raspberry Pi is the Nvidia Jetson.Also based on an SoC, the Nvidia Jetson stands out by incorporating an additional higher performance GPU to accelerate ML tasks, resulting in a performance of around 472 GFLOPS.Different versions of the Nvidia Jetson are available, some also offering dedicated resources for tensor operations for DL tasks.The highest performing option offers up to 300 TOPS.Given their similar connectivity, some platforms offer compatibility with either the Raspberry Pi or the Nvidia Jetson [9], [23].However, the majority of platforms exclusively support the Nvidia Jetson [12], [18], [24], [25], [59], [65].</p>
<p>For platforms not requiring high computational performance on the vehicles themselves, microcontrollers [17], [22], [30], [61], [64], [67] are very common.Although microcontrollers offer lower performance than Raspberry Pi, offering only between 100 KFLOPS and 25 MFLOPS, they enable realtime code execution, crucial for interfacing with low-level sensors and actuators.They also use less power and are cheaper.Conversely, some platforms leverage regular PCs as their computational unit [10], [11].While PCs provide the highest performance, ranging from 50 GFLOPS for CPU-only tasks up to multiple TFLOPS for GPU computation, they have larger physical footprints and higher power consumption.Because of this, some platforms [21], [28], [63] opt to use a lower power compute unit like a Raspberry Pi on the vehicles themselves for command execution and sensor data acquisition only and offload compute-intensive tasks such as control strategies and computer vision algorithms to a more powerful central computer that is connected through a network to the compute units on the vehicles.This increases the complexity of the setup as network communication has to be handled but allows for easier reconfiguration and upgrades as only the central compute unit has to be changed.This also allows to use external high performance compute clusters to handle complex control strategies.</p>
<p>Especially control strategies involving a camera as a sensor or strategies fusing multiple sensors require significant performance from the computing unit.Platforms that employ these strategies usually use one of the higher end options listed, namely an Nvidia Jetson or regular PCs, either on the vehicles or as a central computer unit.The smaller and the cheaper the platforms tend to be, the smaller also the compute units have to be, as size and energy constraints increase with smaller sizes.Smaller platforms also usually employ smaller and more low level sensors which can only be interfaced with through low level compute units like microcontrollers or SBCs.</p>
<p>H. Vehicle-to-Everything (V2X)</p>
<p>V2V refers to the ability of autonomous vehicles to communicate with one another to collaboratively perform cooperative driving tasks.Its extension, Vehicle-to-Infrastructure (V2I), involves communication between vehicles and infrastructure elements, such as traffic lights or traffic signs.In 2019, the definition was expanded to V2X, which encompasses V2V and V2I, but also extends to communication with broader systems like the power grid or cellular networks.In fullscale autonomous vehicles, dedicated network components and protocols facilitate this communication.</p>
<p>Small-scale vehicles play a crucial role in researching these types of interactions, as they enable easy deployment of complex setups involving both vehicles and infrastructure in compact testbeds.For a small-scale platform to support such research, it must have networking capabilities, which excludes many of the educational platforms discussed in Section II-A1.Most other platforms rely on WiFi for networking, with the notable exception of the Kilobot [22], which uses a proprietary infrared communication link.WiFi enables both centralized approaches, where all vehicles and infrastructure elements communicate with a central coordinator, and decentralized approaches, where vehicles communicate directly with each other.All of the platforms investigated in our research rely on a central coordinator to manage communication, as seen in [21] and [63].However, the open-source platforms referenced in Section II-A3 can be extended to support alternative communication strategies.Most of the platforms we examined only support V2V communication, with the Duckietown [107] and AutoDRIVE [59] platforms being exceptions.These two platforms also support V2I communication in the form of connected traffic lights.In Duckietown, the traffic lights are essentially built with the same hardware as the vehicles, allowing them to sense other vehicles through onboard cameras and accept control commands from either a central coordinator or other vehicles.</p>
<p>IV. BENCHMARKING</p>
<p>As discussed in Section III, small-scale cars primarily apply mature AV techniques from full-scale cars under simplified conditions.Thus, autonomous driving systems for small-scale vehicles are typically categorized into two distinct pipelines, similar to those used in full-scale cars: the end-to-end system pipeline and the modular system pipeline.As illustrated in Fig. 2, a modular system comprises multiple subsystems that perform various tasks such as perception and localization, mapping, path planning, and control [108], [109].Each subsystem focuses on specific functionalities and tasks.First is the perception system, where sensors such as cameras and LiDAR</p>
<p>End-to-End System Pipeline</p>
<p>ML-based Methods Traditional Methods</p>
<p>Rule 1</p>
<p>Rule 2 Rule 3</p>
<p>Fig. 2. Comparison of two pipelines for the autonomous driving system.An end-to-end system maps raw sensor inputs directly into control commands, whereas a modular system includes multiple subsystems to process the sensor inputs sequentially and output control commands.</p>
<p>are used to gather information about the surroundings, including lane markings, obstacles, traffic signs, and other vehicles.Following this, the localization and mapping system leverages GPS, odometry, IMU, or techniques like simultaneous localization and mapping (SLAM) to precisely pinpoint the position within the environment while concurrently creating a detailed map of its surroundings.The perception system creates an intermediate representation of the environment for subsequent utilization.</p>
<p>The representation module then uses the information from the perception module and further processes the sensor data with sensor fusion techniques or creates an object map with the predicted state of each object within the sensor range.The combination of the perception module and the representation module can be seen as the scene understanding, which provides an abstract high-level representation of the environment [109].Afterward, the planning system maps out a safe and efficient route to reach the destination.Normally, in the autonomous driving system, the planning phase is divided into two different parts, namely global path planning and local path planning [110].Global path planning refers to the process of determining an optimal or feasible route from the current position to its destination and is done considering the entire environment and involves high-level decision-making.Local path planning or path following, on the other hand, focuses on the immediate surroundings of the vehicle and deals with making real-time adjustments to adhere to the planned global path.Finally, the control module generates driving commands based on the processed information.In control theory, the primary goal is to minimize a cost function.Various methodologies are employed for the control system, classical controllers such as the Proportional-Integral-Derivative (PID) controller, Model Predictive Control (MPC), and ML-based controllers such as IL or DRL.PID works by continuously calculating the error between a desired setpoint and a measured process variable, then applying corrective actions based on three components: proportional, integral, and derivative.The proportional component reacts to the current error, providing immediate corrections, the integral component addresses accumulated past errors to eliminate steady-state error, and the derivative component predicts future errors by considering the rate of change, enhancing stability and responsiveness.MPC, on the other hand, is an advanced control strategy designed for complex systems.It predicts future system behavior using a dynamic model and computes optimal control inputs by solving a constrained optimization problem at every time step.</p>
<p>For ML-based controllers, as illustrated in Fig. 3, IL involves training an agent to mimic expert behavior by learning from a dataset of demonstrations.By directly leveraging expert trajectories, IL bypasses the need for explicitly defined reward functions, making it particularly effective in scenarios where optimal behavior is well understood but challenging</p>
<p>Vision-based</p>
<p>Control</p>
<p>Modular System</p>
<p>End-to-end System</p>
<p>Traditional Methods</p>
<p>ML-based Methods</p>
<p>Fig. 3. Classification of research in small-scale cars into modular and end-to-end systems.Modular systems are further subdivided into perception tasks, planning tasks, and control tasks.</p>
<p>to formalize mathematically.Key IL algorithms include Behavioral Cloning (BC) [111], Generative Adversarial Imitation Learning (GAIL) [112], Inverse Reinforcement Learning (IRL) [113], and Dataset Aggregation (DAgger) [114].On the other hand, RL involves training an agent to autonomously discover optimal policies by interacting with an environment and receiving feedback in the form of rewards [115].This exploration-based approach allows RL agents to learn from scratch and adapt to a wide range of environments, even in the absence of expert guidance.</p>
<p>An end-to-end system is characterized by a unified architecture that aims to learn the entire mapping directly from raw sensor inputs to driving actions without explicitly decomposing the task into separate modules [116]- [118].It involves training a comprehensive learning model with ML methods, IL or DRL, directly processing raw sensor data and output control commands.End-to-end systems potentially simplify the system architecture by eliminating the need for handcrafted modules and feature engineering.</p>
<p>In this section, we focus on the two primary systems and the key tasks associated with each, as depicted in Fig. 3. Rather than delving into the detailed methodologies used to accomplish these tasks, we provide references to the relevant studies for further exploration.Additionally, we propose a baseline framework for each system, aiming to provide inspiration and guidance for researchers in developing their own approaches.</p>
<p>A. Perception</p>
<p>In a modular system, the perception system performs several key functions essential to autonomous driving, including mapping the environment and localizing the autonomous vehicle, detecting lanes, and detecting objects.In this subsection, we will discuss various aspects that the perception module needs to consider.</p>
<p>1) Localization and Mapping: Localization and mapping are foundational tasks in autonomous systems, often requiring high precision and real-time performance.A primary consideration in localization is GPS, which offers direct positional data for the vehicle.However, the accuracy of GPS is influenced by various factors, such as atmospheric conditions, satellite geometry, signal blockage (buildings, trees, etc.), multipath interference, and the quality of the GPS receiver.Differential GPS (DGPS) systems and augmentation techniques can be used to enhance GPS accuracy by correcting some of these error sources, but cannot fulfill the accuracy which is extremely important when it is small-scale cars.IMUs, another commonly used sensor, provide high-rate measurements of acceleration and angular velocity across three dimensions but suffer from issues like sensor drift and integration errors, leading to long-term inaccuracies.To address the issues, the two sensors are often integrated through sensor fusion methods, such as Kalman filter (KF) [98] to enhance accuracy and reliability in determining the position and orientation.As in the studies by [119]- [121], that use BARC small-scale cars, KF makes the fusion of IMU, wheel encoders, and indoor GPS measurements to achieve an accurate localization.Nevertheless, KFs are limited by assumptions of linearity and Gaussian noise, making them less effective in complex or nonlinear environments.Therefore, [11] and [50] use factor graphs combined with incremental smoothing and mapping 2 (iSAM2) [122], to fuse GPS and IMU measurements and output smoothing estimation.For the issues with non-linear dynamics and non-linear measurement models, [52]- [54] use the particle filter to fuse the camera images with IMU, and GPS.[123] also presents an approach to use KF to fuse multiplecamera images with the AprilTag system and wheel odometry, validated using the OptiTrack Motion Capture System, which offers high accuracy but at a significantly higher cost, limiting its practicality for small-scale platforms.Additionally, a single camera can serve as a perception sensor.For example, in [55], CNN is used with a single monocular camera to predict the cost map of the track in front of the vehicle which is directly useable for online trajectory optimization with MPC.Similarly, [124] uses a top-down lane cost map CNN and the YOLOv2 CNN to extract feature-input values and a two-point visual driver control model (TPVDCM) as the controller to control the vehicle.However, these camera-based perception approaches rely solely on visual information, neglecting spatial information, which can lead to limitations in accuracy and robustness under certain conditions, where the camera is not robust enough.</p>
<p>LiDAR-based systems have emerged as a compelling alternative, leveraging 2D LiDAR sensors for localization and mapping via SLAM techniques.A well-established method for localizing robot cars with LiDAR sensors on the pre-defined map involves the use of a particle filter (PF) as explored in [44] with F1TENTH.However, the computational demands of the PF present challenges, especially for computation resources and space-constrained small-scale robot cars.To enhance performance, [125] introduces the Compressed Directional Distance Transform (CDDT) on the RACECAR platform.This method focuses on expediting ray casting within 2D occupancy grid maps and accelerating sensor model computations to mitigate computational expenses.While comprehensive survey papers delve into various SLAM techniques [126]- [129], encompassing feature-based, LiDAR-based, visual, and graph SLAM, this study offers a concise overview of three prevalent SLAM methods employed with small-scale cars: GMapping [130]- [132], HectorSLAM [133]- [137], and Cartographer [138]- [140].While 3D LiDAR remains underutilized due to its size and computational demands, advancements in hardware miniaturization are making 3D LiDAR SLAM techniques like LOAM [141] and LIO-SAM [142] increasingly feasible for small-scale platforms.</p>
<p>Another compelling approach gaining traction involves leveraging ML methods to address mapping and localization challenges.One notable instance is the GALNet proposed in [143] implemented on the Autominy platform.GALNet employs a Deep Neural Network (DNN), utilizing two timestamps of inertial, kinematic, and wheel velocity data to estimate poses effectively.With the success of Transformer architecture in various research fields, [144] introduces the Perception-Action Causal Transformer (PACT) architecture.This model constructs a representation from sensor data by autonomously predicting states and actions over time, laying the groundwork for subsequent task-specific networks for localization and mapping.Pre-trained models are employed initially and later fine-tuned for specific tasks, demonstrating validation on MuSHR with LiDAR sensor data.For imagebased localization, [145] proposes an effective framework leveraging a pre-trained local feature transformer (LoFTR).This framework employs a constrained 3D projective transformation between consecutive key images to establish a visual map on Jetbot.These studies underscore the growing interest and potential of ML methodologies, including neural networks and transformer architectures, in revolutionizing mapping and 2) Lane Recognition: Lane recognition or lane detection is another important task for the perception module.It is the process of identifying and recognizing the lane markings on a road using algorithms.It typically involves detecting lines or curves that represent the boundaries of lanes on the road.For small-scale cars, we categorize the lane detection methods into traditional and ML-based methods.</p>
<p>a) Traditional methods: Traditional lane detection methods rely on computer vision techniques to process sensor data, usually captured by cameras, to identify and track lane markings.Feature extraction techniques are then used to compute metrics such as lateral and orientation deviation, which indicate the position of the car relative to the lane.In [9], a multi-step image processing pipeline is employed for lane detection in Duckietown.This pipeline uses techniques such as k-means clustering, the Canny filter [148], HSV colorspace thresholding, and the probabilistic Hough transform [149] to extract lane line segments.These features are further processed by a nonlinear non-parametric histogram filter to estimate the lateral displacement and angular offset relative to the right lane center.Similarly, [150] applies an HSV color-based approach with Gaussian kernel filtering and Sobel edge detection [151], followed by a probabilistic Hough transform for lane detection on the Donkeycar platform.While these traditional methods are computationally efficient and effective for structured environments, their reliance on explicit feature extraction limits their generalization to complex or unstructured scenarios, making them less robust in real-world applications.</p>
<p>b) ML-based methods: ML-based approaches on the other hand improve lane detection by leveraging the predictive capabilities of advanced algorithms.For example, [152] employs a Convolutional Neural Network (CNN) to predict lateral displacement and angular offset.Additionally, [42] introduces a modular system trained on a mix of simulation and realworld datasets, mapping sensor inputs into a shared latent space.Recent advancements in Vision Transformers (ViT) have also been applied to lane detection.In [153], a pretrained ViT is fine-tuned on limited driving datasets to perform environmental segmentation.</p>
<p>3) Obstacle Recognition: Obstacle detection and avoidance refers to identifying and navigating around obstacles or potential hazards in its path to ensure safe and uninterrupted driving behavior.It utilizes various sensors, including cameras, LiDAR, ultrasonic sensors, and other technologies, constantly scan the surroundings to detect and classify obstacles.An effective obstacle detection algorithm necessitates a range of essential abilities.In this paper, obstacle detection algorithms are divided into different groups according to the usage of different sensors, we discuss the most widely used two: camera and LiDAR sensors.</p>
<p>a) Vision-based approaches: The vision-based approach relies on images captured by cameras as the primary input for detection.This approach employs computer vision (CV) or ML algorithms, prominently leveraging CNN, for effective detection tasks.Within the realm of cameras used, the vision-based approach can be categorized into two subclasses: monocular and stereo.Monocular image-based methods rely on a single image for processing, while stereo methods utilize images captured by two synchronized cameras.Details about the camera sensors are discussed in Section III-A.</p>
<p>Within monocular image-based methods, as outlined in [154], the obstacle detection task for small-scale cars is typically categorized into three primary domains: appearance-based, motion-based, and depth-based methods.With appearance-based methods, obstacles are typically identified as the foreground within images.The primary challenge lies in distinguishing relevant foreground or background elements based on established criteria, such as color discrepancies [155] or texture features [156].In the work by [35], the RGB images captured by the monocular camera of a Duckiebot are initially transformed into the HSV colorspace.Subsequently, the color filter from OpenCV is employed to detect obstacles based on color differences.Its reliance on predefined color criteria significantly limits adaptability to diverse scenarios, such as environments with varying lighting or differently colored obstacles.This highlights the need for more robust methods that generalize well across varied settings, such as learning-based techniques.Motion-based obstacle detection refers to identifying obstacles or objects in an environment by analyzing motion vectors in the image, it involves comparing successive frames of images to determine alterations in position, velocity, or other motion-related characteristics.Objects that move or exhibit changes in their motion characteristics are then identified as potential obstacles.Optical flow analysis [157], background subtraction, differential methods.can be employed for motion-based obstacle detection [158], [159].These methods excel in detecting moving obstacles but struggle with static hazards.Moreover, computational demands for real-time implementation can be prohibitive for small-scale platforms, calling for optimization techniques or lightweight motion estimation algorithms.Depth-based methods utilize depth information extracted from images to discern the distances and spatial arrangement of objects within the environment, aiding in accurate object detection [160], [161].</p>
<p>Employing conventional image processing techniques like this may fall short in meeting real-time application expectations, primarily due to their inability to swiftly adapt to dynamic conditions.Therefore, recent research endeavors have pivoted towards enhancing obstacle detection speed by employing CNN and particularly emphasizing the effectiveness of YOLO [92].YOLO is specifically designed for real-time object detection, and its variations have been employed to enhance success rates [162]- [164].CNN offers promising capabilities in overcoming the limitations of traditional methods, showcasing greater adaptability and improved real-time performance in diverse and dynamic environments.However, given the restricted computational power available in small-scale cars, a balance must be achieved between accuracy and processing time.In [62], YOLOv3 and Tiny YOLO are selected as detection algorithms for the Go-CHART platform.[165] integrate GhostConv to the YOLOv4-tiny model to achieve faster detection, while in [166], YOLOv5 is used to detect cones and duckies within the Duckietown environment.These aforementioned ML methods still face challenges in balancing detection accuracy and processing time on limited hardware.A potential solution involves further optimizing CNN architectures for edge deployment.Stereo image-based methods work by using a dual-camera system that captures images from slightly different angles, similar to the human binocular vision.This setup helps in perceiving depth and reconstructing threedimensional scenes by analyzing the differences between the images from these cameras [167].However, in the context of small-scale cars, constraints such as cost considerations and limited space availability restrict the usage of stereo cameras to select platforms, notably including RACECAR [24] and Autominy [27].Exploring cost-effective stereo setups, such as single-camera depth estimation augmented with additional processing, could bridge this gap.Vision-based methods offer adaptability and versatility but face challenges related to environmental variability, computational overhead, and hardware limitations.Future work should explore lightweight CNN architectures, as highlighted in [168], where pruning techniques can significantly benefit computation-limited platforms.Additionally, hybrid depth estimation models and motionaware algorithms hold promise for improving performance within the constraints of small-scale platforms.</p>
<p>b) LiDAR-based approaches: In addition to camerabased approaches, LiDAR is also widely used for obstacle avoidance systems.The applications of 2D LiDAR are prevalent in compact car platforms due to factors like size, price, weight, and overall compactness considerations.However, due to the existing computational limitations of these car platforms, innovative LiDAR data processing methods are often unavailable.Consequently, raw LiDAR point cloud data or minimally processed data is frequently used, involving procedures like imputing empty samples and cleansing noisy data, to serve as input for neural networks executing subsequent control tasks.In [169] and [47], minimal processing of LiDAR data is employed, such as filtering and finding the nearest obstacle, to guide control policies.While computationally efficient, this approach provides limited contextual information, potentially hindering performance in complex scenarios with Both vision-based and LiDAR-based approaches offer unique strengths but are subject to inherent trade-offs.Visionbased methods excel in rich environmental perception and are more cost-effective, but their susceptibility to environmental variability and computational demands can limit their reliability.LiDAR-based methods, while robust and precise, often lack the contextual depth provided by vision systems and can be hindered by computational simplicity.Integrating the two approaches into a multi-modal framework holds significant promise.For instance, vision can provide rich scene context, while LiDAR ensures precise spatial awareness.Techniques like sensor fusion and joint optimization of perception pipelines could leverage the complementary strengths of these modalities.</p>
<p>4) Traffic Sign Recognition: Traffic sign recognition (TSR) is a specialized object detection task that enables autonomous vehicles to interpret road signage and adjust behavior accordingly.Unlike the generic object detection task, TSR uniquely relies solely on cameras, leveraging their capability to capture visual details.The process typically commences with dataset acquisition, wherein cameras capture traffic signs for later utilization.These captured images undergo a series of preprocessing steps to refine quality, eliminate noise, adjust lighting, and employ data augmentation techniques.Subsequently, ML models [93], [174], [175] are trained using these compiled datasets for detection and classification.However, research on TSR is limited for small-scale cars due to the unavailability of standard datasets that may differ across platforms.Hence, researchers must prioritize dataset collection as an initial step to train the networks.Future research should intensify exploration in this direction, particularly focusing on mixing TSR with other driving tasks like navigation, lane keeping, or path following, enabling vehicles to make informed, contextaware decisions in real time.</p>
<p>B. Planning</p>
<p>Path planning in autonomous driving refers to the process by which a self-driving vehicle determines a safe and efficient route from its current location to a desired destination while navigating through its environment [176].This task involves creating a trajectory or path that the vehicle can follow, considering various factors such as obstacles, road conditions, and traffic regulations, and aims to ensure safe and reliable navigation while optimizing factors like travel time, and energy efficiency.Path planning algorithms vary in complexity and we specify them into traditional [177]- [180] and ML-based techniques [181], [182].</p>
<p>a) Traditional methods: Traditional algorithms remain foundational in path planning due to their deterministic nature and efficiency in well-defined scenarios.For example, [183] applies Dijkstra's algorithm [177] in conjunction with SLAM methods to extract global map information.The A<em> algorithm [178], noted for its optimality, is employed in [39] to plan optimal paths for racing cars, while [184] integrates Hybrid A</em> [185] with learned cost functions to address dynamic obstacles.However, reliance on precomputed cost functions may limit adaptability in novel environments, suggesting the need for adaptive cost-learning techniques.Genetic algorithms (GAs) offer advantages in unstructured environments, as explored in [186] and [187], where stochastic search capabilities generate global paths.However, high computational demands and convergence issues in real-time scenarios could be mitigated by hybrid approaches, combining GAs with deterministic methods like A*.To extend the planning capabilities beyond the sensor horizon, [188] utilizes a generative neural network trained on real-world data to predict occupancy maps beyond sensor limitations.This predictive capability assists in facilitating planning processes.Authors leverage the Rapidly-Exploring Random Tree (RRT) algorithm [179] to generate a global path, followed by a local planner that orchestrates trajectories until the end of the predicted map.While effective, traditional methods often struggle in dynamic or uncertain environments, emphasizing the importance of more adaptive frameworks.</p>
<p>b) ML-based methods: Path planning, treated as an optimization problem, has also seen the successful application of various ML methods, resulting in notably high-performance outcomes.For instance, [189] uses Q-Learning, a classical RL algorithm, to generate a global path for robots, which achieves a shorter and smoother path compared with the RRT algorithm, though challenges in scalability and training efficiency remain.Advanced reinforcement learning (RL) methods, such as Deep Q-Networks (DQN) and Policy Gradient algorithms, could enhance performance in future studies.A framework is introduced in [190] comprising a mapper, global policy, Ã— High computational demand [192]: MuSHR IL âˆš Efficient in known environments Ã— Out-of-distribution error [193]: Donkeycar and local policy for image-based navigation for Jetbot.The mapper undergoes supervised learning using camera images to generate an occupancy grid map.Subsequently, a global policy employing DRL techniques takes both the map and the position as input to determine the long-term goal.Following this, a planner, employing the A* algorithm, computes the short-term goal, which is then forwarded to the local planner for further execution.Even with prior maps and acquired information, managing uncertainty during planning and driving remains a significant challenge.To address these uncertainties, [191] introduces a model-based RL algorithm incorporating a probabilistic dynamic model.This method aims to mitigate uncertainty during planning stages and prevent shortsighted decisions.In the related approach, [192] uses Bayesian Residual Policy Optimization (BRPO) with an ensemble of expert policies.Authors train the policy ensemble with BRPO to diminish overall system uncertainty, enabling safe navigation within partially observable environments.Demonstrated on the MuSHR platform, the proposed framework integrates a global localization system, ensuring destination-reaching capabilities while avoiding collisions with other moving vehicles.</p>
<p>For path planning, traditional methods excel in static and structured environments due to their computational efficiency and deterministic outputs.However, their inability to adapt to dynamic or uncertain conditions limits their application in complex real-world scenarios.Conversely, ML-based approaches offer flexibility and adaptability, capable of predicting and reacting to unforeseen changes, but they are computationally demanding and heavily reliant on large datasets.Future research should focus on hybrid frameworks that combine the computational efficiency of traditional methods with the adaptability of ML approaches.This could include transfer learning techniques to reduce data dependency, integration of predictive modeling to address uncertainties, and optimization of computational frameworks for real-time applications.</p>
<p>C. Control</p>
<p>Once the path planning module establishes a desired path, the control module becomes the subsequent step.In this section, we will discuss the control methodologies that leverage the reference trajectory to compute control actions to navigate the car.</p>
<p>1) Path Following: Path following is the most simple control task for small-scale cars, it necessitates a contin- ual adjustment of the movements based on real-time sensor feedback and environmental alterations, ensuring the vehicle maintains the desired trajectory accurately.Solutions for path following encompass a wide spectrum, including classical control methods like the PID controller, kinematic controller [194]- [196], MPC [197]- [200], and extending to ML methods [190], [201]- [203].</p>
<p>In [194], an adaptive trajectory tracking control scheme is introduced with adjustable gains to facilitate adherence to predefined paths.The designed adaptive control gains aimed to streamline tuning efforts, augment the convergence rate of tracking errors, and elevate trajectory tracing performance.However, a key disadvantage is its reliance on accurate system modeling and gain adjustment, which may struggle under significant environmental disturbances or non-linear dynamics.The strategy introduced by [197] involves expert interventions with pre-existing LiDAR-derived maps to further improve performance.Nevertheless, the dependency on pre-existing maps and the requirement for expert input make it less suitable for dynamic or unstructured environments.For ML-based methods, in the work presented by [201], error states relative to reference trajectories are determined using vehicle state data obtained from sensor fusion involving IMU and GPS with an EKF.A Neural Network (NN) is trained using IL methods with a dataset collected from human-controlled and MPCcontrolled driving scenarios.Trained NN maps error states to control commands.While this method significantly reduces the need for complex hand-crafted control laws, it inherits biases from the training data, especially when collected from suboptimal human drivers.To mitigate the need for extensive datasets, [202] applies Deep Deterministic Policy Gradient (DDPG) algorithm to guide an Autominy car along the desired path, aiming to minimize cross-track errors.Similarly, in the study by [190], a global policy is determined using DRL, while a local planner processes received images with another DRL agent to derive the final action for the path following.To address the data inefficiency challenges with RL, [203] then introduces a policy gradient-based policy optimization framework leveraging a first-principles model for path following task with a JetRacer.This framework facilitated the learning of precise control policies with limited real-world data.While promising, the reliance on first-principles modeling may limit its adaptability to systems with uncertain or highly nonlinear dynamics.Furthermore, the approach assumes access to accurate and reliable simulation environments, which might not always reflect real-world variability.</p>
<p>2) Lane Keeping: When small-scale cars drive on tracks without obstacles, the optimal planned trajectory typically aligns with the center of the right lane.Together with the lane detection methods discussed in Section IV-A2, the primary objective is to guide the vehicle within the designated lane, adhering to predefined tolerances for deviations from the lane center.This task requires the car to dynamically adjust its path in real time, based on its position relative to the lane.Consequently, the task heavily depends on visual input, typically utilizing RGB cameras to monitor the lane position of the vehicle continuously.This process, commonly referred to as lane keeping for small-scale vehicles, has been a central focus of extensive research across various platforms [9], [10], [23].Research in lane keeping can be broadly categorized into two approaches: traditional methods and ML-based methods.</p>
<p>a) Traditional methods: Traditional control methods regulate the trajectory using outputs from the module, ensuring it remains within the designated lane.Algorithms such as PID controllers are commonly used to adjust the steering angle based on deviations from the lane center.For instance, in [9], the PID controller utilizes lateral displacement and angular offset estimates from the perception pipeline to minimize deviations in real time.Similarly, [150] integrates a rule-based approach to compute steering angles directly from detected lane lines.These methods are straightforward and interpretable, making them suitable for simple lane keeping tasks.However, their reliance on precise input metrics and static tuning parameters limits their effectiveness in dynamic environments or when system dynamics are nonlinear.b) ML-based methods: ML-based control methods enhance lane keeping by either augmenting traditional control systems or replacing them with more adaptive algorithms.For example, DRL has been successfully used to replace traditional controllers.[85], [86] employ DRL algorithms to process outputs from the perception module of [9], achieving superior performance in multitask driving scenarios beyond lane keeping.Additionally, these systems improve Sim2Real transfer capabilities, as demonstrated by [42], where a modular system integrates a list of DRL algorithms, utilizing these latent features from the perception module as inputs to control the Donkeycar.While ML-based control methods provide adaptability and can learn optimal policies from data, they are often sensitive to hyperparameter tuning and may require extensive training.</p>
<p>3) Car Following: In addition to navigating tracks without other vehicles, a key driving scenario for autonomous vehicles involves operating alongside other vehicles on the road.One crucial capability in such scenarios is car following, which refers to the ability to maintain a safe and appropriate distance from the vehicle ahead.This requires tracking the movements of the leading vehicle and dynamically adjusting speed and position to ensure safe, comfortable, and efficient driving.The literature on car following maneuvers with small-scale vehicles remains relatively limited, though several notable studies have emerged.A DDPG algorithm with an extended look-ahead approach is proposed in [209] for longitudinal and lateral control within vehicle platooning scenarios.For perception, LiDAR technology gauges inter-vehicle distance, while an IMU provides acceleration data.Additionally, a V2V system employing Wi-Fi communication transmits leader information to the follower vehicle.Similarly, in [210], a Cooperative Adaptive Cruise Control (CACC) system is implemented utilizing Deep Q-learning (DQN) [245].This system enables the follower vehicle to dynamically adapt and maintain appropriate intervehicular distances.Notably, both these studies rely on V2V communication to achieve effective car following behavior, which increases system complexity and costs.In contrast, [86] introduces a new perspective by realizing multitasking driving, encompassing car following and lane keeping, solely relying on visual sensors without the need for communication with other vehicles.In this approach, a pattern of circles is affixed behind the leading Duckiebot, aiding the ego Duckiebot in sensing distance and speed to mimic human driver behavior.A modular system is deployed, where the perception module extracts compact affordance information, and a DRL algorithm controls the Duckiebot based on this information.Despite its simplicity, this system yields promising results.</p>
<p>4) Overtaking: Overtaking, like car following, is another critical driving behavior that involves interaction with other vehicles on the track.It refers to the maneuver where an autonomous vehicle changes lanes or positions itself to safely pass another vehicle traveling in the same direction on the road.The purpose of overtaking is to move ahead of the slower vehicle safely and efficiently while maintaining proper traffic flow.For small-scale cars, overtaking is particularly challenging due to the need for precise localization to avoid collisions throughout the maneuver.Consequently, limited research has been conducted on this task.A first contribution comes from [85], where authors employ a modular system equipped solely with onboard sensors to execute diverse driving tasks, encompassing lane keeping and overtaking for Duckiebots.The framework leverages traditional machine vision technologies to acquire compact affordance information, as discussed in [9], and utilizes LiDAR sensors for distance estimation relative to other vehicles.Afterward, the Long Short-Term Memory Soft Actor-Critic (LSTM-SAC) algorithm assumes the role of the controller.Notably, LSTM aids the agent in recognizing distinct phases within the overtaking process.Demonstrated results affirm the efficacy of this proposed framework, managing both lane keeping and overtaking tasks, and showcasing superior performance when benchmarked against baselines.Another work regarding the overtaking task is [211], where a dual control approach with MPC towards active uncertainty reduction is proposed.This approach automatically balances the exploration-exploitation trade-off, enabling the MuSHR car to actively minimize uncertainty concerning the hidden states of other agents without compromising expected planning performance.The difference compared with [85] the hardware side is that the vehicle being overtaken will yield, and the usage of a known grid map of the track.While these methods showcase promising results, they highlight the current research limitations in overtaking tasks for smallscale cars.Future work should prioritize the development of more generalizable frameworks capable of handling diverse overtaking scenarios without relying on assumptions such as yielding vehicles or predefined maps.</p>
<p>5) Racing: Autonomous racing, distinct from traditional autonomous driving, emphasizes high-speed navigation, rapid reaction, and dynamic trajectory planning.Unlike standard driving tasks, racing pushes vehicles to attain high velocities, extensively testing the dynamic limits of these automated systems.When racing at high speeds, vehicles must swiftly detect other vehicles or obstacles, demanding rapid reaction times.Additionally, they must accurately localize their position concerning the track and strategize dynamic trajectories to optimize performance [246].While extensive research has been dedicated to full-size racing competitions like Roborace, our primary focus centers on small-scale car racing events such as AutoRally [11], F1TENTH [12], Donkeycar, ORCA [20] and others.</p>
<p>For racing tasks, the control module has a high demand for timely reactions, often surpassing the importance of the perception module.The most commonly used control module in racing is MPC, which is a control strategy that utilizes a dynamic model of the system to predict its future behavior and make control decisions based on optimization criteria.In [20], a path planner and a Nonlinear MPC (NMPC) are utilized to guide the racing process within the ORAC platform.Later, in [50], a sampling-based MPC algorithm called the Model Predictive Path Integral Control algorithm (MPPI) is introduced for Autorally.This algorithm presents a novel derivation of path integral control, offering an explicit formula for controls across the entire time horizon.A notable attribute of MPPI is its capability to generate entirely new behaviors dynamically, enabling the controller to drive the vehicle to its operational limits.Then, a few improved versions are proposed, such as the robust sampling-based MPC framework based on a combination of model predictive path integral control and nonlinear Tube-MPC [215] and best response model predictive control based on a combination of the gametheoretic notion of iterated best response, and an information-theoretic model predictive control algorithm [247].Despite its effectiveness in dynamic trajectory planning and adaptability to high-speed scenarios, MPC has notable drawbacks.It depends heavily on accurate dynamic models, which, if imperfect, can lead to disparities between predicted and actual behavior.Additionally, the high computational demand poses challenges for real-time implementation.</p>
<p>To address this challenge, Learning Model Predictive Control (LMPC) integrates learning algorithms with MPC, aiming to refine control strategies in the face of uncertain or changing dynamics.Compared to traditional MPC, LMPC continually enhances its performance, especially in scenarios where complete knowledge of system dynamics is unavailable or subject to variation.The learning mechanism algorithms such as RL, Gaussian processes, neural networks, or other adaptive learning methods in LMPC continuously update the predictive model based on collected data and feedback from the system, to achieve better performance, adaptability to changing system dynamics, and robustness in scenarios where precise models might be unavailable or incomplete.[52] establishes the connection between MPC and online learning, and proposes a new algorithm based on dynamic mirror descent (DMD) and MPC (DMD-MPC), which provides a fresh perspective on previous heuristics used in MPC.In [216], historical data is used to construct secure sets and approximate the value function, facilitating LMPC to learn and improve from past experiences within BARC platform.Later, [214] applies Gaussian processes to correct the model mismatch, then uses MPC for tracking pre-computed racing lines using this corrected model for an F1TENTH car.For more responsive control, [121] uses Linear Parameter Varying (LPV) theory to model the dynamics of the vehicle and combine it with MPC (LPV-MPC), which can be computed online with reduced computational cost.To swiftly identify unsafe conditions, [53] propose a Perceptual Attention-based Predictive Control (PAPC) algorithm, where MPC is first used to learn how to place attention on relevant areas of the visual input and ROI, and output control actions as well as estimates of epistemic and aleatoric uncertainty in the attention-aware visual input of an Autorally car.[146] introduces a local, linear, data-driven learning method for error dynamics within the LMPC framework.This approach exhibits increased robustness against parameter variations and limited data compared to prior LMPC implementations.While LMPC offers significant advantages, it also has limitations.These include high computational costs due to continuous model updates, dependency on sufficient high-quality training data, and the need for additional hardware for onboard real-time learning and processing.Pure pursuit, another control algorithm used in racing tasks, is simpler and more computationally efficient [217].It is particularly suitable for scenarios with limited computational resources.However, it lacks the adaptability needed for dynamic environments and may struggle in highspeed racing where precise trajectory prediction is essential.</p>
<p>One of the significant challenges in autonomous racing is localization.Current methods, such as motion capture systems and pre-mapped tracks, rely heavily on external infrastructure [248], [249].This dependence creates scalability issues and imposes hardware and environmental constraints.To overcome these limitations, future research should focus on developing map-free or vision-based localization systems.Lightweight neural networks could also be leveraged for real-time learning and decision-making, reducing the reliance on extensive infrastructure.</p>
<p>Although autonomous racing does not commonly occur in daily driving scenarios, it is still an emerging field in intelligent vehicles and transportation systems.The intriguing aspect lies in the operation of autonomous small-scale cars pushing the boundaries of vehicle capabilities [246].Operating at high speeds with minimal reaction time within dynamic environments, autonomous racing with small-scale cars emerges as a compelling area within the autonomous driving domain.</p>
<p>6) Other tasks: In addition to the control tasks discussed earlier, we also explore tasks that are less commonly studied for small-scale cars, broadening the scope of research in this domain.</p>
<p>a) Drifting: Autonomous drifting, often associated with high-performance racing, represents a complex and sophisticated challenge in autonomous vehicle control.It requires precise coordination of speed, steering, throttle, and braking, alongside an in-depth understanding of vehicle dynamics and kinematics.Effective state estimation methods are equally critical to ensure stability and accuracy during drifting maneuvers.Traditional approaches heavily rely on dynamic models, such as the dynamic bicycle model used in [13], which employs EKF to fuse sensor data and integrates a Linear-Quadratic Regulator (LQR) controller with equilibrium drifting points for control.While this method is effective within its design constraints, its performance is tightly coupled to the accuracy of the dynamic model and the predefined operating conditions.Similarly, [218] expands upon this by introducing a six-state bicycle model, yet it struggles with generalizing to more complex and dynamic scenarios.This highlights a fundamental challenge: precise vehicle dynamics models are often impractical or unattainable in real-world environments.To address these limitations, [219] proposes an approach to tackle the drifting park problem with BARC cars by segmenting it into distinct phases: the normal driving regime and the sliding regime.During the normal driving phase, a nonlinear MPC operates with a predefined kinematic model.As the vehicle transitions into the sliding phase, reliance on this model diminishes.To navigate this shift, a feedforwardfeedback controller takes charge, orchestrating safer maneuvers adeptly under sliding conditions.While effective, this segmentation introduces additional computational complexity and coordination challenges.Moreover, tire model singularities at low speeds exacerbate control difficulties, especially in the sliding regime.In response to these challenges, [220] presents a novel solution by integrating a fused kinematic-dynamic bicycle model with a nonlinear MPC framework tailored for a RACECAR platform.This unified approach harmonizes the planning and execution of dynamic vehicle maneuvers within a unified framework, optimizing the entire process cohesively.Despite these advancements, conventional dynamic models are constrained by their reliance on simplified assumptions, limiting their applicability.Future research should prioritize integrating data-driven methods, such as neural network-based controllers or DRL controllers, to improve adaptability and performance in uncertain environments.Additionally, the development of robust state estimation techniques that can operate effectively with noisy sensor inputs and less precise dynamic models is crucial for advancing autonomous drifting capabilities.</p>
<p>b) Parking: Besides driving behaviors, parking is also a crucial research area in AD for both small-scale and normalscale cars.Research on the car parking problem generally stems from the broader motion planning problem and is typically defined as finding a collision-free path that connects the initial configuration to the final one.Traditional parking methods generally follow a three-phase approach: mapping the parking space, planning a collision-free path, and executing the maneuver.For small-scale cars, fuzzy logic algorithms dominate traditional methods.These systems rely on distance sensors, such as ultrasonic, infrared [224]- [226], or LiDAR [227]- [230], to detect parking spaces and use predefined fuzzy rules to determine steering angles.While these methods are simple and effective in structured environments, they lack adaptability and struggle with complex, dynamic scenarios.Closed-loop controllers and rule-based strategies, while robust for specific cases, also suffer from limited flexibility [221]- [223].</p>
<p>ML-based methods offer adaptability and learning capabilities, making them better suited for dynamic parking environments.Neuro-fuzzy systems combine neural networks with fuzzy logic [250] to dynamically update rules and membership functions based on training data [231], [232], improving performance in diverse scenarios.Other ML approaches segment the parking process for better control, such as the use of a General Radial Basis Function (GRBF) classifier and Random Forest kernel to identify behavior transitions, as seen in [233].Later, [234] proposed a two-stage learning framework to predict steering angles and gear status for parking using front and back-mounted monocular cameras.In the first stage, an encoder-decoder architecture estimates the initial steering angle trajectory.This trajectory, along with the heading angle and absolute position, is then fed into an LSTM network to estimate the optimal steering angle and gear status for parking.DRL approaches such as DDPG and other end-to-end learning frameworks have also shown promise in learning complex parking maneuvers, incorporating environmental feedback for precise control [235], [236].While ML methods excel in adaptability, they often require large datasets, extensive training, and computational resources.</p>
<p>Despite the importance of autonomous parking for smallscale cars, research in this area has been lacking over the past few decades.However, with the development of current ML methods, there is a growing need to employ more advanced techniques in this research topic.c) Cooperative Driving: Compared to the previously discussed scenarios, which typically involve a single controlled vehicle, cooperative driving entails a collaborative approach where multiple autonomous vehicles communicate, with V2V communications.In this setting, vehicles interact to achieve shared goals, navigate complex environments, and enhance traffic efficiency and safety.This involves exchanging data such as positions, speeds, and intended trajectories to enable collective decision-making, which is generally categorized into centralized and decentralized control strategies.</p>
<p>In centralized control, a central entity coordinates the actions of all vehicles by processing shared data to make globally optimized decisions.This approach excels in tasks like collaborative SLAM and multi-vehicle path planning, where precise coordination is crucial.For instance, [239] utilizes feature-based map registration for collaborative SLAM among three vehicles via V2V communication, enhancing mapping speed and accuracy.The centralized strategy excels in achieving global optimization and consistency but is limited by scalability challenges and potential single points of failure.The dependency on robust communication infrastructure also makes it vulnerable to latency and data synchronization issues.In contrast, decentralized control distributes decision-making among vehicles, which act based on local sensor data and peer communication.An example is the decentralized MPC method in [240], where MuSHR cars plan collision-free trajectories through negotiation, enabling precise and independent control.Safety is further addressed by [241], which proposes a prioritybased distributed MPC (P-DMPC) algorithm to minimize collision risks.The decentralized control is advantageous for scalability and fault tolerance but can face challenges in achieving global optimization due to limited information sharing and potential inconsistencies among individual vehicle actions.</p>
<p>Specific applications of cooperative driving highlight its versatility.In roundabout navigation, vehicles communicate and negotiate entry, adjusting speeds and trajectories to yield to those already in the roundabout.RL-based coordination methods like those used in [58] and decentralized control frameworks from [56] demonstrate smooth merges and reduced stop-and-go traffic in scaled testbeds.Similarly, traffic jam mitigation emphasizes synchronized vehicle actions to maintain optimal flow, as seen in frameworks like [63], which adapts the IDM and MOBIL models for cooperative behavior, and studies like [242] and [243], which reduce travel times through decentralized optimal control.Emerging areas extend beyond autonomous cars.For example, [28] achieves adaptive cruise control for multi-vehicle platooning, while works such as [238] explore cooperation between UGVs and UAVs, pushing the boundaries of collaborative traffic management.</p>
<p>For future works, we propose to include hybrid control systems that combine centralized coordination with decentralized adaptability, robust V2V communication systems that minimize latency and security risks, and cross-platform collaboration between diverse vehicle types to enhance traffic management and operational flexibility.These advancements promise to unlock the full potential of cooperative driving in real-world applications.</p>
<p>D. End-to-end Driving</p>
<p>After discussing the modular system, we then delve into the end-to-end systems for small-scale cars.Here, we primarily focus on two main streams: lane keeping and racing.</p>
<p>For the lane keeping task, the end-to-end system has received more extensive research compared to modular systems.</p>
<p>In this paradigm, the agent processes raw sensor input and outputs control commands directly, bypassing the modular perception, planning, and control layers.This framework primarily encompasses two methods: IL and DRL.Among IL methods, BC is foundational due to its simplicity, as it maps observed states directly to corresponding actions without modeling the underlying decision-making process.For example, in [48], [251], CNNs are trained with image data to predict directly the steering angle for control under various evaluation metrics within the DeepRacer platform.BC offers the advantages of straightforward implementation and computational efficiency, making it effective in scenarios where expert behavior is well-defined and consistent.However, its drawbacks include vulnerability to compounding errors, where deviations from expert behavior escalate in unseen states, and a heavy dependence on the quality and diversity of training data, which often hinders its ability to generalize to out-ofdistribution scenarios.To address these issues, enhancements to BC have been proposed.For instance, [83] introduces uncertainty modeling during training to improve Sim2Real transfer on the Donkeycar platform.Similarly, [252] augments datasets using image style transfer [253] to enhance generalization.Comparative studies, such as [254], evaluate BC, GAIL, and DAgger in the Duckietown environment, highlighting the potential of Domain Adaptation techniques for better Sim2Real transfer.IDRL, in contrast, learns control policies directly from raw sensor data.Studies such as [10], [255], [256] employ raw images as input to DRL controllers, while preprocessing methods like dimensionality reduction are used in [90], [257], [258] to improve convergence speed.Addressing control smoothness, [259] introduces Conditioning for Action Policy Smoothness (CAPS), which reduces jerky control behaviors.Sim2Real transfer capabilities are improved by including delays and sampling rate as additional observations during training, as demonstrated by [37], enhancing the robustness of DRL policies on the DeepRacer platform.Beyond image inputs, raw LiDAR data serves as input in studies such as [46], where DRL algorithms navigate F1TENTH cars in structured environments.Although DRL eliminates the need for expert demonstrations and is highly adaptable to complex and dynamic environments, it often suffers from high computational cost and data inefficiency.Additionally, ensuring smooth and stable control remains challenging, particularly in real-world scenarios, and achieving reliable Sim2Real transfer is difficult due to discrepancies (Sim2Real gap) between training and deployment environments.To further enhance generalization and convergence, autoencoders are frequently integrated into endto-end systems [260].By compressing input data into lowerdimensional latent representations, autoencoders provide an efficient representation for downstream tasks such as control or policy learning.For example, [205], [206] preprocess image data with autoencoders before training Duckiebots using BC with expert trajectories.Variational Autoencoders (VAEs) [261] have also been explored in DRL, as demonstrated in [41], [208], where compressed latent features facilitate faster convergence and improved learning efficiency compared to raw image inputs.Despite their advantages in computational efficiency and feature extraction, autoencoders risk losing crit-ical information during compression, particularly in dynamic or noisy environments, and introduce additional computational overhead during training and integration.</p>
<p>For racing with end-to-end systems, IL and DRL [262] have been widely explored.The authors in [82] introduce an endto-end IL system, where a learner network needs to imitate an expert.The expert fuses GPS and IMU for state estimation and uses an MPC as controller, the learner uses a DNN as control policy to map raw, high-dimensional observations to continuous steering and throttle commands.Building on this, [263] presents a deep imitative RL framework for end-to-end racing with camera input, where IL is used to initialize the policy, and model-based RL is used for further refinement by interacting with an uncertainty-aware world model.This hybrid approach achieves improved adaptability and performance by leveraging the strengths of both IL and RL.To improve the stability of the racing system, [264] designs a residual control system, in which multiple Bayesian Neural Networks (BNNs) are trained with two camera inputs and GPS measurements to control an Autorally car in an end-to-end fashion.The integration of LiDAR technology has further expanded the capabilities of end-to-end systems.In [265], a model-based RL approach effectively utilizes raw LiDAR input to navigate racetracks with an F1TENTH car.Following this, [266] employs LiDAR data as input for a DQN to control the F1TENTH car, offering a comparative analysis of neural network architectures for Li-DAR data processing.This study also evaluates two Sim2Real approaches, demonstrating the importance of transferability for real-world deployment.Despite the demonstrated potential of DRL for racing tasks, the well-known issue of low sample efficiency continues to hinder its broader application.For this issue, [267] propose an efficient residual policy learning method with the raw observation of LiDAR and IMU, in which first a controller based on the modified artificial potential field (MAPF) is used to generate policies, then DRL algorithms are used to generate a residual policy as a supplement to obtain the optimal policy with increased efficiency.Similarly, [268] presents a trajectory-aided learning (TAL) method that trains DRL with raw LiDAR input by incorporating the optimal trajectory into the learning formulation.[269] also presents a residual vehicle controller that learns to amend a traditional controller with a similar idea.To tackle the safety issues that usually occur in RL training, [270] uses a Viability Theorybased supervisor to recursively feasible vehicle safety during the training.Furthermore, to improve the robustness of RL, [271] first train a teacher model that overfits the training track, moving along a near-optimal path, then use this model to teach a student PPO model the correct actions along with randomization.</p>
<p>Looking ahead, hybrid approaches that combine IL and DRL for end-to-end systems show significant promise.Pretraining policies with IL to capture expert knowledge, followed by DRL fine-tuning for adaptability, can improve both sample efficiency and performance.Furthermore, advanced Sim2Real techniques, such as generative models and adversarial training, are critical for bridging the gap between simulated and real-world environments.Exploring multi-modal input fusion, and integrating diverse sensors like images, LiDAR, and radar, could enhance robustness and adaptability.Additionally, incorporating interpretable ML techniques would make the decision-making process of end-to-end systems more transparent, a crucial step for safety-critical applications.Finally, developing real-time adaptation algorithms that allow models to learn and adjust online without requiring retraining could ensure more reliable performance in dynamic, real-world settings.By addressing these challenges and pursuing these research directions, end-to-end systems can become more robust, efficient, and scalable, enabling broader adoption in real-world autonomous driving scenarios.</p>
<p>E. Proposed Framework</p>
<p>After reviewing the benchmarked tasks and techniques, it is evident that ML and DL methods form the backbone of AD research for small-scale cars.These methods play an important role across key modules such as perception, planning, and control.In perception, techniques like CNNs and autoencoders enhance object detection, lane detection, and semantic segmentation.For control, IL and RL enable models to make efficient and safe decisions in diverse and dynamic conditions.Connected driving leverages the Internet of Things (IoT) to create an interactive ecosystem, enabling real-time data exchange via V2V and V2I communication.This supports coordinated behaviors like platooning and optimized traffic flow management.</p>
<p>We would like to propose a baseline framework that integrates these methods into a cohesive structure.By extracting an effective framework from the literature, we aim to provide valuable insights for researchers.In a modular pipeline system, the process begins with the perception layer.Sensor fusion combines data from multiple sensors such as cameras and LiDAR using DNN, IMU, GPS, and encoders using KF alongside SLAM techniques.This integration ensures robust environmental perception and creates a comprehensive understanding of the environment.The fused feature or raw sensor input is then used for object detection, employing ML or shallow learning models to identify and classify objects in the vehicle's vicinity.Compared with deep ML methods, shallow models have simple structures, usually consisting of one or a few layers of processing units, but are effective for resourceconstrained environments, in our case, small-scale cars with lower-end computing units.Support vector machines or K-Nearest Neighbors can be used for perceptions.Subsequently, path planning and behavior planning are performed, either separately or within a single module.Classical methods like A*, Dijkstra's, and RRT can be utilized for path planning, while behavior planning can leverage finite state machines, DRL, IL, MPC, and decision trees to handle tasks such as lane keeping, lane changing, overtaking, and obstacle avoidance.A safety and redundancy module ensures system reliability, incorporating PID or rule-based controllers for safe stopping during failures and redundant systems for robustness.For end-to-end systems, raw sensor data is processed directly using IL and RL or their hybrid approaches for driving tasks, with safety modules remaining integral.This cohesive framework aims to guide researchers in advancing small-scale autonomous driving systems.</p>
<p>V. FUTURE TRENDS</p>
<p>In the previous sections, we have gone through the currently available small-scale car platforms, their hardware configurations, and the autonomous driving tasks achieved by these platforms.In pursuit of augmenting the capacity and applicability of these small-scale car platforms across diverse age groups, facilitating educational use for students, and supporting sophisticated research purposes, we list the following opportunities for further exploration.</p>
<p>A. Enhancing Accessibility and Usability</p>
<p>It is imperative to contemplate transforming the smallscale platform into a more widely accessible resource for educational purposes and academic research.This necessitates the establishment of a lower entry barrier, reduced pricing, and the implementation of a comprehensive learning pipeline.For educational usage, the first consideration should be given to the ease of learning and maintenance for teachers.A more user-friendly starting point will likely foster increased enthusiasm among educators, encouraging them to incorporate the platform into their daily teaching activities.This, in turn, will provide students with a challenging learning experience, ultimately preparing them for the intricacies of fully autonomous driving in the near future.For research applications, an in-depth exploration of the accessibility of the small-scale car platform is essential.This entails tailoring entry levels to accommodate various research goals, catering to individuals with zero experience in robotics to seasoned professionals.The platform should not only serve as a testbed for autonomous systems but also ignite enthusiasm for exploring diverse robotic configurations.While some platforms currently offer support for different entry levels, a more comprehensive development is warranted to meet the diverse needs of the research community.</p>
<p>B. Improving Versatility and Advanced Technology Adoption</p>
<p>Regarding the more academic research consumption, the versatility of the platforms should be more considered.Specifically, concerning individual small-scale cars, attention should be directed towards enhancing their functional dimensions.In line with advancements in semiconductor technology, where computation and sensor units are becoming more compact yet powerful, platforms should incorporate more sophisticated sensors, thereby augmenting the overall capabilities of the system.As discussed in Section IV, most of the current techniques applied in small-scale car platforms are old methods developed for full-scale cars, with simplified apply conditions.With the rapid advancement of AD research, increasingly sophisticated techniques in sensing, perception, and control are being developed for full-scale vehicles.These novel methods typically require advanced sensor suites, including 3D LiDAR and cameras, which demand significant computational resources and power supply-requirements that most small-scale car platforms cannot meet.Additionally, deploying these techniques in real-world systems necessitates fast communication between components.As a result, more powerful platforms are becoming increasingly attractive.</p>
<p>C. Bridging Gaps in Smart City Configurations</p>
<p>We propose a thorough examination of smart city configurations.While smart cities hold significant potential in advancing research on autonomous driving for small-scale cars, there exists a gap in terms of accessibility, reproducibility, and standardization of best practices.There is an urgent need for a common framework across the research community, encompassing both hardware and software aspects.By addressing these two aspects, small-scale car platforms can more closely resemble real-world AD systems.It unlocks the potential for deploying widely used open-source AD systems, such as Apollo and Autoware, on small-scale car platforms.These AD systems necessitate a comprehensive ecosystem encompassing sensing, perception, localization, planning, and control.Furthermore, existing smart city setups often overlook key elements such as weather conditions and pedestrian interactions, which are important in real-world driving scenarios.Addressing these factors in future research is essential for developing comprehensive and realistic autonomous driving solutions within smart city environments.</p>
<p>D. Advancing V2V and V2I Integration</p>
<p>Achieving fully autonomous driving necessitates the integration of critical technologies such as V2V, V2I, and V2X.While these technologies have been extensively researched in the literature, there remains a noticeable gap in discussions of small-scale car platforms.Consequently, the community needs to pivot towards exploring and advancing V2V and V2I communications as the next steps in the pursuit of comprehensive autonomous driving solutions.</p>
<p>VI. CONCLUSION</p>
<p>In this survey, we offer an overview of the current state-ofthe-art developments in small-scale autonomous cars.Through an in-depth exploration of past and ongoing research, we identify critical challenges and highlight the promising trajectory for advancing small-scale autonomous driving technology.We begin by enumerating the presently predominant small-scale car platforms widely employed in academic and educational domains, detailing the configurations and specifications of each.Similar to their full-scale counterparts, the deployment of hyper-realistic simulation environments is imperative for training, validating, and testing autonomous systems before real-world implementation.To this end, we show the commonly employed universal simulators and platform-specific simulators.Furthermore, we provide a detailed summary and classify the literature into distinct categories: end-to-end systems versus modular systems and traditional methods versus ML-based methods.This classification facilitates a nuanced understanding of the diverse approaches adopted in the field.We introduce methods used for perception, path planning, control, and end-to-end driving.To provide a holistic guide for researchers and practitioners, we also outline the commonly utilized components and tools across various well-known platforms.This information serves as a valuable resource, enabling readers to leverage our survey as a guide for constructing their own platforms or making informed decisions when considering commercial options within the community.</p>
<p>We additionally present future trends concerning small-scale car platforms, focusing on different primary aspects.Firstly, enhancing accessibility across a broad spectrum of enthusiasts: from elementary students and colleagues to researchers, demands the implementation of a comprehensive learning pipeline with diverse entry levels for the platform.Next, to complete the whole ecosystem of the platform, a powerful car body, varying weather conditions, and communications issues should be addressed in a smart city setup.These trends are anticipated to shape the trajectory of the field, contributing significantly to advancements in real-world autonomous driving research.</p>
<p>While we have aimed to achieve maximum comprehensiveness, the expansive nature of this topic makes it challenging to encompass all noteworthy works.Nonetheless, by illustrating the current state of small-scale cars, we hope to offer a distinctive perspective to the community, which would generate more discussions and ideas leading to a brighter future of autonomous driving with small-scale cars.</p>
<p>Fig. 1 .
1
Fig.1.Illustration of the development and current states of small-scale car platforms, each depicted platform image sourced from its respective paper or website.(A) Based on published studies each year on Google Scholar with the search terms: "small-scale car" or "robot car", the research on small-scale cars has seen substantial growth in the number of papers over the years.In the early 2000s, projects like s-bot[16], e-puck[17], and TurtleBot emerged.Starting around 2016, with the introduction of Duckietown[9], BARC[13], and Autorally[11], there was a significant increase in research papers.This trend continued with the development of projects like DeepRacer[10], Donkeycar, and F1TENTH[12].More computationally advanced small-scale cars have been introduced in recent years, such as ART/ATK[18] and XTENTH-CAR[19].(B) Examples of small-scale car platforms, categorized into educational platforms and research platforms, including multiple vehicle setups such as ORAC[20] and UDSSC[21].</p>
<p>TABLE I AN
I
OVERVIEW OF SMALL-SCALE CAR PLATFORMS FOR EDUCATIONAL PURPOSES.
PlatformsSizeSensorsProgrammingRuntimePrice (USD)Thymio [30]110x110mmIR sensor, Accelerometer, Microphone, ThermistorVPL2h270Makeblock mBot-Ultrasonic Sensor, IR sensor, Line tracking sensormBlock1h190Edison80x80x40mmIR sensor, Line tracking sensorEdBlock EdPy-60AlphAI Robot-Camera, Ultrasonic Sensor, Line tracking sensorPython-270Ozobot Evo32mmIR sensor, SpeakerOzoBlockly1h160Encoder, Ultrasonic sensor,Finch Robot-Line tracking sensor, IR sensor,MicroBit7h170SpeakerTinkerGen MARK200x185x92mmCamera, Ultrasonic SensorMicroPython-220Robolink Zumi95x67x70mmCamera, IR sensorPython-190</p>
<p>TABLE II SMALL
II
-SCALE CAR PLATFORMS: AN OVERVIEW OF THE HARDWARE SETUP.
PlatformsSizeVehicle DynamicsActuatorSensorsComputation UnitRuntimeCommercial availablePrice (USD)AutoRally [11]1/5thAckermannTwo servo motorsCamera, IMU, GPS, Hall-effect sensorIntel i7-6700 Nvidia GTX-750ti SC&lt;1h710kART/ATK [18]1/6thAckermannOne brushless DC motor One servo motorCamera, 3D LiDARJetson Xavier NX--BARC [13]1/10thAckermannOne brushless DC motorCamera, LiDAR, IMU, GPSODROID-XU4--Donkeycar 11/10th 1/16thAckermannOne brushed/brushless DC motorCamera, LiDAR, IMU, EncoderRPi/Jetson Nano7350F1TENTH [12]1/10thAckermannOne brushless DC motorCamera, LiDAR, IMUJetson TX2&lt;1h73800RACECAR(MIT) [24]1/10thAckermannOne brushless DC motor One servo motorCamera, LiDAR, IMU, EncoderJetson Tegra X1-72600MuSHR [25]1/10thAckermannOne brushless DC motor One servo motorCamera, LiDAR, IMU, Bump sensorJetson Nano-7900Qcar 21/10thAckermannOne brushless DC motorCamera, LiDAR, IMU, Encoder, MicrophoneJetson TX230mâˆ¼2h-Autominy [27]1/10thAckermannOne brushless DC ServomotorCamera, LiDAR, IMU, EncoderIntel NUC--JetRacer 31/10th 1/18thAckermannOne brushed DC motorCameraJetson Nano-600Autonomouscar [26]1/10thAckermannOne brushed DC motorCamera, LiDAR, IMU, Encoder, ToF Sensor, Indoor GPSRPi 4--CoRoLa [28]1/10thAckermannOne brushless DC motor One servo motorCamera, Encoder, Ultrasonic sensorRPi 4--AutoDRIVE [59]1/14thAckermannTwo DC geared motorsCamera, LiDAR, IMU, Encoder, Indoor GPSJetson Nano--PiRacer 41/16thAckermannTwo DC brushed motorsCameraRPi 4-250Duckietown [9]34x15x23cmDifferentialTwo DC geared motorsCamera, IMU, Ultrasonic sensorRPi 2/Jetson Nano2âˆ¼6h450DeepRacer [10]1/18thAckermannOne brushless DC motor One servo motorCamera, LiDAR, IMUIntel Atomâˆ¼6h400Âµcar [60]1/18thAckermannOne brushless DC motor One servo motorIMU, EncoderRPi Zero Wâˆ¼6h-UDSSC MCAV [21]1/25thAckermannOne geared DC motorIMU, line following, IR sensorRPi 390m-Chronos [61]1/28thAckermannBrush motor with gearboxIMU, EncoderEspressif ESP3230mâˆ¼1h-Go-CHART [62]1/28thDifferentialFour micro metal gear motorsCamera, LiDAR, Bump sensorRPi 3âˆ¼1h-Cambridge Minicar [63]75x81x197mmAckermann-Indoor positioning system 8RPi Zero2h-ORCA Racer [20]1/43thAckermann-IMU, Indoor positioning system 8ARM Cortex M420m-Epuck [17]70mmDifferentialTwo stepper motorsCamera, IMU, IR sensor, Speaker, MicrophoneSTM32F407âˆ¼3h1000Turtlebot3 514x18x19cmDifferentialTwo servomotorsCamera, LiDAR, IR sensorRPi 4âˆ¼2.5h1200Kilobot [22]33mmVibrationTwo vibration motorsIR sensorAtmega 3283âˆ¼10h15GRITSBot [64]31x30mmDifferentialTwo stepper motorsIMU, IR sensorAtmega 3281âˆ¼5h-HydraOne [65]27x32cmOmniTwo encoder motorsCamera, 3D LiDAR, EncodersJetson TX2-77200Pheeno [31]10cmDifferentialTwo micro gear motorsCamera, IMU, Encoder, IR sensorATmega328P ARM Cortex-A75h7270Thymio [30]11x11cmDifferentialTwo DC motorsIR sensor, Accelerometer, Microphone, ThermistorPIC24F2h270MarXbot [32]17cmDifferential-Camera, IMU, IR sensor, 2D force sensorARM 11 processor--WolfBot [66]17.5cmOmni-Camera, IMU, IR sensor, MicrophoneBeagleBone--LabRAT [67]-DifferentialTwo DC gearmotorsIR sensorAtmega324p3h-Jetbot 6-DifferentialTwo TT motorsCamera, IMUJetson Nano2âˆ¼3h2501. http://donkeycar.com2. https://www.quanser.com/products/qcar3. https://github.com/NVIDIA-AI-IOT/jetracer4. https://www.waveshare.com/wiki/PiRacer AI Kit5. https://www.turtlebot.com6. https://jetbot.org7. Build guides with off-the-shelf parts are provided.8. Motion Capture Systems.</p>
<p>TABLE IV COMPARISON
IV
OF METHODS USED FOR LOCALIZATION AND MAPPING.
Method Kalman Filter Particle Filter GMapping HectorSLAM CartographerSensor GPS; IMU; Encoder GPS; IMU; LiDAR LiDAR LiDAR LiDARAnalysis âˆš Accuracy improvement Ã— Asynchrony, Latency Ã— Noise Assumptions âˆš Nonlinear dynamics Ã— High computational demand âˆš Robust in static environments Ã— High memory usage âˆš Fast and lightweight Ã— Performance degrades with noise âˆš Accuracy, dynamic environments Ã— High computational demand âˆš Camera onlyReference [119]-[121], [146]: BARC; [123]: QCar [44]: F1TENTH; [125]: RACECAR [133]: Roborace; [134], [136]: -[130]-[132]: -[147]: F1TENTH; [139]: Innopolis UGV; [138], [140]: -CNNCameraÃ— FoV of Camera[55], [124]: AutoRallyGALNet LoFTROdometry CameraÃ— No Spatial infomation âˆš Fast Ã— Dynamic-dependent âˆš Camera only Ã— High computational demand[143]: Autominy [145]: Jetbotlocalization paradigms. However, ML approaches are oftendata-intensive and require extensive pre-training and fine-tuning to generalize effectively. Future research should focuson improving the robustness and adaptability of these methods,integrating advanced sensing technologies, and optimizingcomputational efficiency to bridge the gap between traditionaland data-driven localization paradigms.</p>
<p>TABLE V COMPARISON
V
OF METHODS USED FOR OBSTACLE DETECTION.
Method Appearance-basedSensor CameraAnalysis âˆš Simple, lightweight Ã— Lighting or color variations âˆš Effective for moving obstaclesReference [35]: DuckiebotMotion-basedCameraÃ— High computational demand[158], [159]: -Ã— Static obstacles âˆš spatial awarenessDepth-basedCameraÃ— High computational demand[160], [161]: -Ã— Dynamic scenariosCNN-based Minimum value FTGCamera LiDAR LiDARâˆš High adaptability Ã— Accuracy and speed trade-off âˆš Efficient, robust for nearby objects Ã— Limited contextual information âˆš Effective for reactive obstacle Ã— Ineffective for narrow gaps[62]: Go-CHART; [165]: JetRacer; [166]: Duckiebot; [171]: Jetbot; [172], [173]: -[47], [169]: F1TENTH [12]: F1TENTHoverlapping or clustered objects. Introducing advanced pointcloud processing methods, such as voxel-based filtering orclustering, could enhance obstacle classification and avoidancecapabilities. In [12], Follow the Gap (FTG) method [170] isemployed for object detection and avoidance of an F1TENTHcar. This technique involves computing the maximum gappresent within the LiDAR point cloud and subsequently de-termining the steering control command. This approach iseffective for reactive obstacle avoidance, especially in dynamicenvironments. However, it may struggle with narrow gaps.
Combining FTG with predictive path planning or vision fusion could improve navigation in cluttered spaces.For small-scale cars, LiDAR remains a cornerstone of obstacle detection, offering reliable distance measurement and robustness.Enhancing LiDAR data processing with ML techniques, such as point cloud segmentation or recurrent neural networks, can address current limitations while maintaining real-time applicability.</p>
<p>TABLE VI COMPARISON
VI
OF METHODS USED FOR PATH PLANNING.
Method Dijkstra's AlgorithmAnalysis âˆš Efficient in static environments âˆš DeterministicReference [183]: -A<em> Hybrid A</em> Genetic Algorithm RRT DRL BRPOÃ— Inefficient for large graph âˆš Efficient in structured environments Ã— Struggles in uncertain environment âˆš Dynamic scenarios Ã— Limited adaptability âˆš High adaptability Ã— High computational demand âˆš Efficient in high-dimensional spaces Ã— Produce suboptimal paths âˆš Handles dynamic environments Ã— High computational demand âˆš Robust in partially observable environments[39]: Deepracer [184]: QCar [186], [187]: -[188]: RACECAR [190]: Jetbot; [191]: MuSHR; [89]: Duckiebot; [189]: -</p>
<p>TABLE VII COMPARISON
VII
OF METHODS USED FOR THE CONTROL MODULE.
Control CategoryMethod Kinematic Controller PID ControllerAnalysis âˆš Streamlines tuning efforts Ã— Relies on accurate system modeling âˆš Simple, effective in structured systems Ã— Limited adaptability âˆš Robust in structured environmentReference [194]: QCar; [195], [196]: -[9]: DuckiebotPath Following Lane KeepingMPC IL DRL PID Controller Rule-based IL DRLÃ— Requires maps Ã— Unsuitable for dynamic environment âˆš Adaptable to varied scenarios Ã— Struggles with unseen scenarios âˆš Adaptive to dynamic environment Ã— High computational demand âˆš Simple, effective in structured systems Ã— Limited adaptability âˆš Straightforward, efficient Ã— High computational demand âˆš Adaptable to varied scenarios Ã— Struggles with unseen scenarios âˆš Handles dynamic environments Ã— Limited adaptability[197]: MuSHR; [198]-[200]: -[201]: ART/ATK [202]: Autominy; [203]: JetRacer [9], [152], [204]: Duckiebot [150]: Donkeycar [205], [206]: Duckiebot; [207]: -[85], [86], [208]: Duckiebot; [41], [42]: DonkeycarCar FollowingDRLâˆš Handles dynamic environments Ã— Limited adaptability âˆš Robust in structured environment[86]: Duckiebot; [209]: Donkeycar; [210]: QCarMPCÃ— Requires maps[211]: MuSHROvertakingDRLÃ— Unsuitable for dynamic environment âˆš Handles dynamic environments Ã— Limited adaptability âˆš Robust in structured environment[85]: Duckiebot [11], [52], [54]: Autorally;MPCÃ— Requires maps[20], [212], [213]: ORAC;RacingMPPI Tube-MPC LMPCÃ— Unsuitable for dynamic environment âˆš Real-time adaptation Ã— High computational demand âˆš Safe and stable under dynamic conditions Ã— High computational demand âˆš Adaptability to uncertain dynamics âˆš Improvements with limited information Ã— Large datasets[214]: F1TENTH [50]: Autorally [215]: Autorally [146], [216]: BARCDrifting Parking Cooperative drivingLPV-MPC PAPC Pure Pursuit LQR NMPC Rule-based Fuzzy Neuro-fuzzy GRBF IL DRL Centralized Control Decentralized ControlÃ— High computational demand âˆš Real-time implementation Ã— Limited in highly nonlinear dynamics âˆš Improves safety Ã— High computational demand âˆš Lightweight, computationally efficient Ã— Less robustness in high-speed âˆš Effective in structured environment Ã— Accurate dynamic models required âˆš Improves control in dynamic environments Ã— Computational cost âˆš Robust for well-defined cases Ã— Limited flexibility âˆš Simple, interpretable Ã— Adaptability in dynamic environments âˆš Adaptable to diverse scenarios Ã— Training data required âˆš Dynamic parking environments Ã— Complex implementation âˆš Improves parking precision Ã— High computational demand âˆš Highly adaptable to dynamic environments Ã— High computational demand âˆš Globally optimized coordination âˆš Effective for requiring high precision Ã— High computational demand Ã— Limited scalability âˆš Robustness and scalability Ã— Effective communication protocols required Ã— Potential for suboptimal solutions[121]: BARC [53]: Autorally [217]: TUNERCAR [13], [218]: BARC [219]: BARC; [220]: RACECAR [221]-[223]: -[224]-[230]: -[231], [232]: -[233]: -[234]: -[235], [236]: -[28]: CoRoLa; [40]: DeepRacer; [58]: UDSSC; [237]: Jetbot; [238]: QCar [63]: CamMini; [239]: QCar; [240]: MuSHR; [241]: ÂµCar; [21], [56], [242], [243]: UDSSC; [244]: Jetbot
ACKNOWLEDGMENTSThis work was funded by ScaDS.AI (Center for Scalable Data Analytics and Artificial Intelligence) Dresden/Leipzig.
Level-5 autonomous driving-are we there yet? a review of research literature. M A Khan, H E Sayed, S Malik, T Zia, J Khan, N Alkaabi, H Ignatious, ACM Computing Surveys (CSUR). 5522022</p>
<p>Milestones in autonomous driving and intelligent vehicles: Survey of surveys. L Chen, Y Li, C Huang, B Li, Y Xing, D Tian, L Li, Z Hu, X Na, Z Li, IEEE Transactions on Intelligent Vehicles. 822022</p>
<p>Societal and individual acceptance of autonomous driving. E Fraedrich, B Lenz, 2016Autonomous driving: Technical, legal and social aspects</p>
<p>Examining public acceptance of autonomous mobility. A Rezaei, B Caulfield, Travel behaviour and society. 212020</p>
<p>Public acceptance and perception of autonomous vehicles: a comprehensive review. K Othman, AI and Ethics. 132021</p>
<p>Preparing a nation for autonomous vehicles: opportunities, barriers and policy recommendations. D J Fagnant, K Kockelman, Transportation Research Part A: Policy and Practice. 201577</p>
<p>Legal issues in automated vehicles: critically considering the potential role of consent and interactive digital interfaces. J.-A Pattinson, H Chen, S Basu, Humanities and Social Sciences Communications. 712020</p>
<p>Governing autonomous vehicles: emerging responses for safety, liability, privacy, cybersecurity, and industry risks. A Taeihagh, H S M Lim, Transport reviews. 3912019</p>
<p>Duckietown: an open, inexpensive and flexible platform for autonomy education and research. L Paull, J Tani, H Ahn, J Alonso-Mora, L Carlone, M Cap, Y F Chen, C Choi, J Dusek, Y Fang, 2017 IEEE International Conference on Robotics and Automation (ICRA). IEEE2017</p>
<p>Deepracer: Autonomous racing platform for experimentation with sim2real reinforcement learning. B Balaji, S Mallya, S Genc, S Gupta, L Dirac, V Khare, G Roy, T Sun, Y Tao, B Townsend, 2020 IEEE International Conference on Robotics and Automation (ICRA). IEEE2020</p>
<p>Autorally: An open platform for aggressive autonomous driving. B Goldfain, P Drews, C You, M Barulic, O Velev, P Tsiotras, J M Rehg, IEEE Control Systems Magazine. 3912019</p>
<p>F1tenth: An open-source evaluation environment for continuous control and reinforcement learning. M O'kelly, H Zheng, D Karthik, R Mangharam, Proceedings of Machine Learning Research. Machine Learning Research2020123</p>
<p>Autonomous drifting with onboard sensors. J Gonzales, F Zhang, K Li, F Borrelli, Advanced Vehicle Control: Proceedings of the 13th International Symposium on Advanced Vehicle Control (AVEC16). 2016133</p>
<p>A systematic review of hardware technologies for small-scale self-driving cars. F Caleffi, L Da Silva Rodrigues, J Da Silva Stamboroski, B V Rorig, M M C Santos, V Zuchetto, Ã B Raguzzoni, CiÃªncia e Natura. 451712023</p>
<p>A survey on smallscale testbeds for connected and automated vehicles and robot swarms. A Mokhtarian, J Xu, P Scheffe, M Kloock, S SchÃ¤fer, H Bang, V.-A Le, S Ulhas, J Betz, S Wilson, arXiv:2408.141992024arXiv preprint</p>
<p>Swarmbot: A new distributed robotic concept. F Mondada, G C Pettinaro, A Guignard, I W Kwee, D Floreano, J.-L Deneubourg, S Nolfi, L M Gambardella, M Dorigo, Autonomous robots. 172004</p>
<p>The epuck, a robot designed for education in engineering. F Mondada, M Bonani, X Raemy, J Pugh, C Cianci, A Klaptocz, S Magnenat, J.-C Zufferey, D Floreano, A Martinoli, Proceedings of the 9th conference on autonomous robot systems and competitions. the 9th conference on autonomous robot systems and competitions20091IPCB: Instituto PolitÃ©cnico de Castelo Branco</p>
<p>A software toolkit and hardware platform for investigating and comparing robot autonomy algorithms in simulation and reality. A Elmquist, A Young, I Mahajan, K Fahey, A Dashora, S Ashokkumar, S Caldararu, V Freire, X Xu, R Serban, arXiv:2206.065372022arXiv preprint</p>
<p>Xtenth-car: A proportionally scaled experimental vehicle platform for connected autonomy and all-terrain research. S Sivashangaran, A Eskandarian, ASME International Mechanical Engineering Congress and Exposition. American Society of Mechanical Engineers202387639</p>
<p>Optimization-based autonomous racing of 1: 43 scale rc cars. A Liniger, A Domahidi, M Morari, Optimal Control Applications and Methods. 3652015</p>
<p>A scaled smart city for experimental validation of connected and automated vehicles. A Stager, L Bhan, A Malikopoulos, L Zhao, IFAC-PapersOnLine. 5192018</p>
<p>Kilobot: A low cost scalable robot system for collective behaviors. M Rubenstein, C Ahler, R Nagpal, 2012 IEEE international conference on robotics and automation. IEEE2012</p>
<p>Donkey car: An opensource diy self driving platform for small scale cars. W Roscoe, 2019</p>
<p>Projectbased, collaborative, algorithmic robotics for high school students: Programming self-driving race cars at mit. S Karaman, A Anders, M Boulet, J Connor, K Gregson, W Guerra, O Guldner, M Mohamoud, B Plancher, R Shin, 2017 IEEE. IEEE2017integrated STEM education conference (ISEC</p>
<p>MuSHR: A low-cost, open-source robotic racecar for education and research. S S Srinivasa, P Lancaster, J Michalove, M Schmittle, C Summers, M Rockett, J R Smith, S Chouhury, C Mavrogiannis, F Sadeghi, CoRR. 1908.08031, 2019</p>
<p>An open-source scale model platform for teaching autonomous vehicle technologies. B Vincke, S R Florez, P Aubert, Sensors. 211138502021</p>
<p>Autominy an autonomous model car for education. F Berlin, 2020</p>
<p>Ros2-based small-scale development platform for ccam research demonstrators. J Pohlmann, M MatthÃ©, T Kronauer, P Auerbach, G Fettweis, 2022 IEEE 95th Vehicular Technology Conference:(VTC2022-Spring). IEEE2022</p>
<p>T Gillespie, Fundamentals of vehicle dynamics. SAE international2021</p>
<p>Thymio ii, a robot that grows wiser with children. F Riedo, M Chevalier, S Magnenat, F Mondada, 2013 IEEE Workshop on Advanced Robotics and its Social Impacts. IEEE2013</p>
<p>Pheeno, a versatile swarm robotic research and education platform. S Wilson, R Gameros, M Sheely, M Lin, K Dover, R Gevorkyan, M Haberland, A Bertozzi, S Berman, IEEE Robotics and Automation Letters. 122016</p>
<p>The marxbot, a miniature mobile robot opening new perspectives for the collectiverobotic research. M Bonani, V Longchamp, S Magnenat, P RÃ©tornaz, D Burnier, G Roulet, F Vaussard, H Bleuler, F Mondada, RSJ International Conference on Intelligent Robots and Systems. IEEE. 2010in 2010 IEEE/</p>
<p>Hybrid control and learning with coresets for autonomous vehicles. G Rosman, L Paull, D Rus, 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2017</p>
<p>Interactive learning with corrective feedback for policies based on deep neural networks. R PÃ©rez-Dattari, C Celemin, J Ruiz-Del Solar, J Kober, Proceedings of the 2018 International Symposium on Experimental Robotics. the 2018 International Symposium on Experimental RoboticsSpringer2020</p>
<p>Developing a purely visual based obstacle detection using inverse perspective mapping. J Nubert, N Funk, F Meier, F Oehler, arXiv:1809.012682018arXiv preprint</p>
<p>Autonomous mobile robot development based on duckietown platform for recognizing and following the traffic sign. G R Purwanto, P Santoso, H Khoswanto, Journal of Physics: Conference Series. IOP Publishing2021192112065</p>
<p>Sim2real transfer for deep reinforcement learning with stochastic state transition delays. S S Sandha, L Garcia, B Balaji, F Anwar, M Srivastava, Conference on Robot Learning. PMLR2021</p>
<p>Effectiveness of transfer learning in autonomous driving using model car. S Chiba, H Sasaoka, 2021 13th International Conference on Machine Learning and Computing. 2021</p>
<p>Reinforcement learning approaches for racing and object avoidance on aws deepracer. J Mccalip, M Pradhan, K Yang, 2023 IEEE 47th Annual Computers, Software, and Applications Conference (COMPSAC). IEEE2023</p>
<p>Multi-vehicle mixed reality reinforcement learning for autonomous multi-lane driving. R Mitchell, J Fletcher, J Panerati, A Prorok, Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems. the 19th International Conference on Autonomous Agents and MultiAgent Systems2020</p>
<p>Learning to drive (l2d) as a low-cost benchmark for real-world reinforcement learning. A Viitala, R Boney, Y Zhao, A Ilin, J Kannala, 2021 20th International Conference on Advanced Robotics (ICAR). IEEE2021</p>
<p>Deep reinforcement learning for autonomous driving by transferring visual features. H Zhou, X Chen, G Zhang, W Zhou, 2020 25th International conference on pattern recognition (ICPR). </p>
<p>Virtualization of self-driving algorithms by interoperating embedded controllers on a game engine for a digital twining autonomous vehicle. H Yun, D Park, Electronics. 101721022021</p>
<p>Formulazero: Distributionally robust online adaptation via offline population synthesis. A Sinha, M O'kelly, H Zheng, R Mangharam, J Duchi, R Tedrake, International Conference on Machine Learning. PMLR2020</p>
<p>Design and simulation of a machine-learning and model predictive control approach to autonomous race driving for the f1/10 platform. A TÈƒtulea-Codrean, T Mariani, S Engell, IFAC-PapersOnLine. 5322020</p>
<p>Case study: verifying the safety of an autonomous racing car with a neural network controller. R Ivanov, T J Carpenter, J Weimer, R Alur, G J Pappas, I Lee, Proceedings of the 23rd International Conference on Hybrid Systems: Computation and Control. the 23rd International Conference on Hybrid Systems: Computation and Control2020</p>
<p>Obstacle avoidance using model predictive control: An implementation and validation study using scaled vehicles. A Bulsara, A Raman, S Kamarajugadda, M Schmid, V N Krovi, SAE Technical Paper Series. 12020</p>
<p>Implementation and validation of behavior cloning using scaled vehicles. A Verma, S Bagkar, N V Allam, A Raman, M Schmid, V N Krovi, SAE Technical Paper Series. 12021</p>
<p>Information theoretic mpc for model-based reinforcement learning. G Williams, N Wagener, B Goldfain, P Drews, J M Rehg, B Boots, E A Theodorou, 2017 IEEE International Conference on Robotics and Automation (ICRA). IEEE2017</p>
<p>Aggressive driving with model predictive path integral control. G Williams, P Drews, B Goldfain, J M Rehg, E A Theodorou, 2016 IEEE International Conference on Robotics and Automation (ICRA. IEEE2016</p>
<p>High-speed cornering for autonomous offroad rally racing. C You, P Tsiotras, IEEE Transactions on Control Systems Technology. 2922019</p>
<p>An online learning approach to model predictive control. N Wagener, C.-A Cheng, J Sacks, B Boots, arXiv:1902.089672019arXiv preprint</p>
<p>Perceptual attention-based predictive control. K Lee, G N An, V Zakharov, E A Theodorou, Conference on Robot Learning. PMLR2020</p>
<p>Vision-based high-speed driving with a deep dynamic observer. P Drews, G Williams, B Goldfain, E A Theodorou, J M Rehg, IEEE Robotics and Automation Letters. 422019</p>
<p>Aggressive deep driving: Combining convolutional neural networks and model predictive control. Proceedings of the 1st Annual Conference on Robot Learning, ser. Proceedings of Machine Learning Research. the 1st Annual Conference on Robot Learning, ser. Machine Learning ResearchPMLRNov 201778</p>
<p>Experimental validation of a real-time optimal controller for coordination of cavs in a multi-lane roundabout. B Chalaki, L E Beaver, A A Malikopoulos, 2020 IEEE Intelligent Vehicles Symposium (IV). IEEE2020</p>
<p>A digital smart city for emerging mobility systems. R M Zayas, L E Beaver, B Chalaki, H Bang, A A Malikopoulos, 2022 IEEE 2nd International Conference on Digital Twins and Parallel Intelligence (DTPI). IEEE2022</p>
<p>Simulation to scaled city: zero-shot policy transfer for traffic control via autonomous vehicles. K Jang, E Vinitsky, B Chalaki, B Remer, L Beaver, A A Malikopoulos, A Bayen, Proceedings of the 10th ACM/IEEE International Conference on Cyber-Physical Systems. the 10th ACM/IEEE International Conference on Cyber-Physical Systems2019</p>
<p>Autodrive: A comprehensive, flexible and integrated digital twin ecosystem for autonomous driving research &amp; education. T Samak, C Samak, S Kandhasamy, V Krovi, M Xie, Robotics. 123772023</p>
<p>Cyber-physical mobility lab: An open-source platform for networked and autonomous vehicles. M Kloock, P Scheffe, J Maczijewski, A Kampmann, A Mokhtarian, S Kowalewski, B Alrifaee, 2021 European Control Conference (ECC). IEEE2021</p>
<p>Chronos and crs: Design of a miniature car-like robot and a software framework for single and multi-agent robotics and control. A Carron, S Bodmer, L Vogel, R ZurbrÃ¼gg, D Helm, R Rickenbach, S Muntwiler, J Sieber, M N Zeilinger, 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE2023</p>
<p>Go-chart: A miniature remotely accessible self-driving car robot. S Kannapiran, S Berman, 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2020</p>
<p>A fleet of miniature cars for experiments in cooperative driving. N Hyldmar, Y He, A Prorok, 2019 International Conference on Robotics and Automation (ICRA). IEEE2019</p>
<p>The gritsbot in its natural habitat-a multi-robot testbed. D Pickem, M Lee, M Egerstedt, 2015 IEEE International conference on robotics and automation (ICRA). IEEE2015</p>
<p>HydraOne: an indoor experimental research and education platform for CAVs. Y Wang, L Liu, X Zhang, W Shi, Proceedings of the 2nd USENIX Workshop on Hot Topics in Edge Computing (HotEdge 19). the 2nd USENIX Workshop on Hot Topics in Edge Computing (HotEdge 19)RENTON, WAUSENIX Association2019</p>
<p>Wolfbot: A distributed mobile sensing platform for research and education. J Betthauser, D Benavides, J Schornick, N O'hara, J Patel, J Cole, E Lobaton, Proceedings of the 2014 Zone 1 Conference of the American Society for Engineering Education. the 2014 Zone 1 Conference of the American Society for Engineering EducationIEEE2014</p>
<p>Labratâ„¢: Miniature robot for students, researchers, and hobbyists. P Robinette, R Meuth, R Dolan, D Wunsch, 2009. 2009</p>
<p>Design and use paradigms for gazebo, an open-source multi-robot simulator. N Koenig, A Howard, 2004 IEEE/RSJ international conference on intelligent robots and systems (IROS). IEEE Cat. No. 04CH37566. IEEE20043</p>
<p>Microscopic traffic simulation using sumo. P A Lopez, M Behrisch, L Bieker-Walz, J Erdmann, Y.-P FlÃ¶tterÃ¶d, R Hilbrich, L LÃ¼cken, J Rummel, P Wagner, E WieÃŸner, 2018 21st international conference on intelligent transportation systems (ITSC). IEEE2018</p>
<p>Torcs, the open racing car simulator. B Wymann, E EspiÃ©, C Guionneau, C Dimitrakakis, R Coulom, A Sumner, 20004</p>
<p>Carla: An open urban driving simulator. A Dosovitskiy, G Ros, F Codevilla, A Lopez, V Koltun, Conference on robot learning. 2017</p>
<p>Airsim: High-fidelity visual and physical simulation for autonomous vehicles. S Shah, D Dey, C Lovett, A Kapoor, Field and Service Robotics: Results of the 11th International Conference. Springer2018</p>
<p>Lgsvl simulator: A high fidelity simulator for autonomous driving. G Rong, B H Shin, H Tabatabaee, Q Lu, S Lemke, M MoÅ¾eiko, E Boise, G Uhm, M Gerow, S Mehta, 2020 IEEE 23rd International conference on intelligent transportation systems (ITSC). IEEE2020</p>
<p>Driving policy transfer via modularity and abstraction. M Mueller, A Dosovitskiy, B Ghanem, V Koltun, Conference on Robot Learning. PMLR2018</p>
<p>Neural autonomous navigation with riemannian motion policy. X Meng, N Ratliff, Y Xiang, D Fox, 2019 International Conference on Robotics and Automation (ICRA). IEEE2019</p>
<p>Combining optimal control and learning for visual navigation in novel environments. S Bansal, V Tolani, S Gupta, J Malik, C Tomlin, Conference on Robot Learning. PMLR2020</p>
<p>Modified ddpg car-following model with a realworld human driving experience with carla simulator. D Li, O Okhrin, Transportation research part C: emerging technologies. 2023147103987</p>
<p>Madras: Multi agent driving simulator. A Santara, S Rudra, S A Buridi, M Kaushik, A Naik, B Kaul, B Ravindran, Journal of Artificial Intelligence Research. 702021</p>
<p>Flow: A modular learning framework for mixed autonomy traffic. C Wu, A R Kreidieh, K Parvate, E Vinitsky, A M Bayen, IEEE Transactions on Robotics. 3822021</p>
<p>Ros: an open-source robot operating system. M Quigley, K Conley, B Gerkey, J Faust, T Foote, J Leibs, R Wheeler, A Y Ng, ICRA workshop on open source software. JapanKobe200935</p>
<p>A joint imitationreinforcement learning framework for reduced baseline regret. S Dey, S Pendurkar, G Sharon, J P Hanna, 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2021</p>
<p>Agile autonomous driving using end-to-end deep imitation learning. Y Pan, C.-A Cheng, K Saigol, K Lee, X Yan, E Theodorou, B Boots, arXiv:1709.071742017arXiv preprint</p>
<p>Mind the gap! a study on the transferability of virtual vs physical-world testing of autonomous driving systems. A Stocco, B Pulfer, P Tonella, IEEE Transactions on Software Engineering. 2022</p>
<p>Sim-to-real: Learning agile locomotion for quadruped robots. J Tan, T Zhang, E Coumans, A Iscen, Y Bai, D Hafner, S Bohez, V Vanhoucke, arXiv:1804.103322018arXiv preprint</p>
<p>A platform-agnostic deep reinforcement learning framework for effective sim2real transfer towards autonomous driving. D Li, O Okhrin, Communications Engineering. 311472024</p>
<p>Vision-based drl autonomous driving agent with sim2real transfer. 2023 IEEE 26th International Conference on Intelligent Transportation Systems (ITSC). IEEE2023</p>
<p>A comprehensive survey on transfer learning. F Zhuang, Z Qi, K Duan, D Xi, Y Zhu, H Zhu, H Xiong, Q He, Proceedings of the IEEE. 10912020</p>
<p>Domain randomization for transferring deep neural networks from simulation to the real world. J Tobin, R Fong, A Ray, J Schneider, W Zaremba, P Abbeel, 2017 IEEE/RSJ international conference on intelligent robots and systems (IROS). IEEE2017</p>
<p>Transferring multi-agent reinforcement learning policies for autonomous driving using sim-to-real. E Candela, L Parada, L Marques, T.-A Georgescu, Y Demiris, P Angeloudis, 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). </p>
<p>Sim-to-real reinforcement learning applied to end-to-end vehicle control. A Kalapos, C GÃ³r, R Moni, I Harmati, 2020 23rd International Symposium on Measurement and Control in Robotics (ISMCR). IEEE2020</p>
<p>An overview of sensors in autonomous vehicles. H A Ignatious, M Khan, Procedia Computer Science. 1982022</p>
<p>You only look once: Unified, real-time object detection. J Redmon, S Divvala, R Girshick, A Farhadi, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2016</p>
<p>Cnn based traffic sign recognition for mini autonomous vehicles. Y SatÄ±lmÄ±s, Â¸ , F Tufan, M Â¸ara, M KarslÄ±, S Eken, A Sayar, Information Systems Architecture and Technology: Proceedings of 39th International Conference on Information Systems Architecture and Technology-ISAT 2018: Part II. Springer2019</p>
<p>Integration of perception, planning and control in the autominy 4.0. J M I Zannatha, O G Miranda, C B RamÃ­rez, L A L Miranda, S R A Aguilar, L Ã D Osuna, 2022 19th International Conference on Electrical Engineering, Computing Science and Automatic Control (CCE). IEEE2022</p>
<p>A control pipeline for robust lane keeping in model cars. J V Asghar, P Auerbach, M MatthÃ©, C Knoll, 2023 11th International Conference on Control, Mechatronics and Automation (ICCMA). IEEE2023</p>
<p>Secrets of event-based optical flow. S Shiba, Y Aoki, G Gallego, European Conference on Computer Vision. Springer2022</p>
<p>Depth camera image processing and applications. S Lee, 2012 19th IEEE International Conference on Image Processing. IEEE2012</p>
<p>A New Approach to Linear Filtering and Prediction Problems. R E Kalman, Journal of Basic Engineering. 8211960</p>
<p>Gps rtk performance characteristics and analysis. Y Feng, J Wang, Positioning. 1132008</p>
<p>Global positioning system (gps) standard positioning service (sps) performance analysis report. G P Team, GPS Product Team: Washington. DC, USA2014</p>
<p>Cyber-physical platform with miniature robotic vehicles for research and development of autonomous mobile systems. A ZdeÅ¡ar, M BoÅ¡nak, G KlanÄar, International Conference on Intelligent Autonomous Systems. Springer2022</p>
<p>Automatic generation and detection of highly reliable fiducial markers under occlusion. S Garrido-Jurado, R MuÃ±oz-Salinas, F J Madrid-Cuevas, M J MarÃ­n-JimÃ©nez, Pattern Recognition. 4762014</p>
<p>The opencv library. G Bradski, Dr. Dobb's Journal of Software Tools. 2000</p>
<p>Accuracy analysis of augmented reality markers for visual mapping and localization. R S Xavier, B M Da Silva, L M Goncalves, 2017 Workshop of Computer Vision (WVC). IEEE2017</p>
<p>Autonomous learning intelligent vehicles engineering: Alive 1.0. J Rezgui, Ã‰ GagnÃ©, G Blain, 2020 International Symposium on Networks, Computers and Communications (ISNCC). IEEE2020</p>
<p>A physical testbed for intelligent transportation systems. A Morrissett, R Eini, M Zaman, N Zohrabi, S Abdelwahed, 2019 12th International Conference on Human System Interaction (HSI). IEEE2019</p>
<p>Duckietown. </p>
<p>A survey of deep learning techniques for autonomous driving. S Grigorescu, B Trasnea, T Cocias, G Macesanu, Journal of Field Robotics. 3732020</p>
<p>Deep reinforcement learning for autonomous driving: A survey. B R Kiran, I Sobh, V Talpaert, P Mannion, A A Al Sallab, S Yogamani, P PÃ©rez, IEEE Transactions on Intelligent Transportation Systems. 2362021</p>
<p>Motion planning for autonomous driving: The state of the art and future perspectives. S Teng, X Hu, P Deng, B Li, Y Li, Y Ai, D Yang, L Li, Z Xuanyuan, F Zhu, IEEE Transactions on Intelligent Vehicles. 2023</p>
<p>A framework for behavioural cloning. M Bain, C Sammut, Machine Intelligence. 151995</p>
<p>Generative adversarial imitation learning. J Ho, S Ermon, Advances in neural information processing systems. 201629</p>
<p>Algorithms for inverse reinforcement learning. A Y Ng, S Russell, Icml. 200012</p>
<p>A reduction of imitation learning and structured prediction to no-regret online learning. S Ross, G Gordon, D Bagnell, Proceedings of the fourteenth international conference on artificial intelligence and statistics. JMLR Workshop and Conference Proceedings. the fourteenth international conference on artificial intelligence and statistics. JMLR Workshop and Conference Proceedings2011</p>
<p>Reinforcement learning: An introduction. R S Sutton, A G Barto, 2018MIT press</p>
<p>A survey of end-to-end driving: Architectures and training methods. A Tampuu, T Matiisen, M Semikin, D Fishman, N Muhammad, IEEE Transactions on Neural Networks and Learning Systems. 3342020</p>
<p>A survey on imitation learning techniques for end-to-end autonomous vehicles. L Le Mero, D Yi, M Dianati, A Mouzakitis, IEEE Transactions on Intelligent Transportation Systems. 2392022</p>
<p>Recent advancements in end-to-end autonomous driving using deep learning: A survey. P S Chib, P Singh, IEEE Transactions on Intelligent Vehicles. 2023</p>
<p>Repetitive learning model predictive control: An autonomous racing example. M Brunner, U Rosolia, J Gonzales, F Borrelli, 2017 IEEE 56th annual conference on decision and control (CDC). </p>
<p>Simple policy evaluation for data-rich iterative tasks. U Rosolia, X Zhang, F Borrelli, in 2019 American control conference (ACC</p>
<p>Autonomous racing using linear parameter varying-model predictive control (lpv-mpc). E AlcalÃ¡, V Puig, J Quevedo, U Rosolia, Control Engineering Practice. 951042702020</p>
<p>isam2: Incremental smoothing and mapping using the bayes tree. M Kaess, H Johannsson, R Roberts, V Ila, J J Leonard, F Dellaert, The International Journal of Robotics Research. 3122012</p>
<p>Navigation of a self-driving vehicle using one fiducial marker. Y Liu, H Schofield, J Shan, 2021 IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems (MFI). IEEE2021</p>
<p>Vision-based autonomous vehicle control using the two-point visual driver control model. J Zheng, K Okamoto, P Tsiotras, arXiv:1910.048622019arXiv preprint</p>
<p>Cddt: Fast approximate 2d ray casting for accelerated localization. C H Walsh, S Karaman, 2018 IEEE International Conference on Robotics and Automation (ICRA). IEEE2018</p>
<p>A brief survey on slam methods in autonomous vehicle. T T O Takleh, N A Bakar, S A Rahman, R Hamzah, Z Aziz, International Journal of Engineering &amp; Technology. 742018</p>
<p>A survey of state-of-the-art on visual slam. I A Kazerouni, L Fitzgerald, G Dooly, D Toal, Expert Systems with Applications. 2051177342022</p>
<p>A comparative survey of lidar-slam and lidar based sensor technologies. M U Khan, S A A Zaidi, A Ishtiaq, S U R Bukhari, S Samer, A Farman, 2021 Mohammad Ali Jinnah University International Conference on Computing (MAJICC). IEEE2021</p>
<p>The simultaneous localization and mapping (slam)-an overview. B Alsadik, S Karam, Surv. Geospat. Eng. J. 22021</p>
<p>Comparative analysis of ros based 2d and 3d slam algorithms for autonomous ground vehicles. P Sankalprajan, T Sharma, H D Perur, P S Pagala, 2020 International Conference for Emerging Technology (INCET). IEEE2020</p>
<p>Towards a framework for slam performance investigation on mobile robots. D.-T Ngo, H.-A Pham, 2020 International Conference on Information and Communication Technology Convergence (ICTC). IEEE2020</p>
<p>Research on real-time positioning and map construction technology of intelligent car based on ros. R Liu, Z Guan, B Li, G Wen, B Liu, 2022 IEEE International Conference on Mechatronics and Automation (ICMA). IEEE2022</p>
<p>Autonomous racing: A comparison of slam algorithms for large scale outdoor environments. F Nobis, J Betz, L Hermansdorfer, M Lienkamp, Proceedings of the 2019 3rd international conference on virtual and augmented reality simulations. the 2019 3rd international conference on virtual and augmented reality simulations2019</p>
<p>Lidar-based gnss denied localization for autonomous racing cars. F Massa, L Bonamini, A Settimi, L Pallottino, D Caporale, Sensors. 201439922020</p>
<p>A quantitative study of tuning ros gmapping parameters and their effect on performing indoor 2d slam. Y Abdelrasoul, A B S H Saman, P Sebastian, 2016 2nd IEEE international symposium on robotics and manufacturing automation (ROMA. IEEE2016</p>
<p>Research and implementation of autonomous navigation for mobile robots based on slam algorithm under ros. J Zhao, S Liu, J Li, Sensors. 221141722022</p>
<p>Autonomous navigation with ros for a mobile robot in agricultural fields. M A Post, A Bianco, X T Yan, 14th International Conference on Informatics in Control, Automation and Robotics (ICINCO). INSTICC. SciTePress2017</p>
<p>Design and implementation of ros-based autonomous mobile robot positioning and navigation system. J Zhu, L Xu, 2019 18th International Symposium on Distributed Computing and Applications for Business Engineering and Science (DCABES). IEEE2019</p>
<p>Comparison of various slam systems for mobile robot in an indoor environment. M Filipenko, I Afanasyev, 2018 International Conference on Intelligent Systems (IS). IEEE2018</p>
<p>2d lidar-based slam and path planning for indoor rescue using mobile robots. X Zhang, J Lai, D Xu, H Li, M Fu, Journal of Advanced Transportation. 20202020</p>
<p>Loam: Lidar odometry and mapping in realtime. J Zhang, S Singh, Robotics: Science and systems. Berkeley, CA20142</p>
<p>Lio-sam: Tightly-coupled lidar inertial odometry via smoothing and mapping. T Shan, B Englot, D Meyers, W Wang, C Ratti, D Rus, 2020 IEEE/RSJ international conference on intelligent robots and systems (IROS). IEEE2020</p>
<p>Galnet: An end-to-end deep neural network for ground localization of autonomous cars. R C Mendoza, B Cao, D Goehring, R Rojas, ROBOVIS. 2020</p>
<p>Pact: Perception-action causal transformer for autoregressive robotics pre-training. R Bonatti, S Vemprala, S Ma, F Frujeri, S Chen, A Kapoor, 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2023</p>
<p>A deep learning-based visual map generation for mobile robot navigation. C A GarcÃ­a-Pintos, N G Aldana-Murillo, E Ovalle-Magallanes, E MartÃ­nez, 20234Eng</p>
<p>Learning model predictive control with error dynamics regression for autonomous racing. H Xue, E L Zhu, F Borrelli, arXiv:2309.107162023arXiv preprint</p>
<p>Comparison of control approaches for autonomous race car model. J KlapÃ¡lek, M Sojka, Z HanzÃ¡lek, Proceedings of the FISITA 2021 World Congress. FISITA-International Federation of Automotive Engineering Societies. the FISITA 2021 World Congress. FISITA-International Federation of Automotive Engineering Societies2021</p>
<p>A computational approach to edge detection. J Canny, IEEE Transactions on Pattern Analysis and Machine Intelligence. 861986</p>
<p>Generalizing the hough transform to detect arbitrary shapes. D H Ballard, Pattern recognition. 1321981</p>
<p>Real-time lane line tracking algorithm to mini vehicles. J Suto, Transport and Telecommunication Journal. 2242021</p>
<p>Design of an image edge detection filter using the sobel operator. N Kanopoulos, N Vasanthavada, R L Baker, IEEE Journal of solidstate circuits. 2321988</p>
<p>Deep trailfollowing robotic guide dog in pedestrian environments for people who are blind and visually impaired-learning from virtual and real worlds. T.-K Chuang, N.-C Lin, J.-S Chen, C.-H Hung, Y.-W Huang, C Teng, H Huang, L.-F Yu, L GiarrÃ©, H.-C Wang, 2018 IEEE International Conference on Robotics and Automation (ICRA). IEEE2018</p>
<p>Monocular robot navigation with self-supervised pretrained vision transformers. M Saavedra-Ruiz, S Morin, L Paull, 2022 19th Conference on Robots and Vision (CRV). IEEE2022</p>
<p>Image-based obstacle detection methods for the safe navigation of unmanned vehicles: A review. S Badrloo, M Varshosaz, S Pirasteh, J Li, Remote Sensing. 141538242022</p>
<p>Efficient sky segmentation approach for small uav autonomous obstacles avoidance in cluttered environment. A S Mashaly, Y Wang, Q Liu, 2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS. IEEE2016</p>
<p>Appearance-based obstacle detection with monocular color vision. I Ulrich, I Nourbakhsh, AAAI/IAAI, 2000. </p>
<p>Performance of optical flow techniques. J L Barron, D J Fleet, S S Beauchemin, International journal of computer vision. 121994</p>
<p>Real-time obstacle detection with motion features using monocular vision. B Jia, R Liu, M Zhu, The Visual Computer. 312015</p>
<p>Vision-based obstacle detection for mobile robot in outdoor environment. C.-C Tsai, C.-W Chang, C.-W Tao, Journal of Information Science &amp; Engineering. 3412018</p>
<p>Monocular fisheye camera depth estimation using sparse lidar supervision. V R Kumar, S Milz, C Witt, M Simon, K Amende, J Petzold, S Yogamani, T Pech, 2018 21st International Conference on Intelligent Transportation Systems (ITSC). IEEE2018</p>
<p>3d visual perception for self-driving cars using a multi-camera system: Calibration, mapping, localization, and obstacle detection. C HÃ¤ne, L Heng, G H Lee, F Fraundorfer, P Furgale, T Sattler, M Pollefeys, Image and Vision Computing. 682017</p>
<p>Deep convolutional neural network for enhancing traffic sign recognition developed on yolo v4. C Dewi, R.-C Chen, X Jiang, H Yu, Multimedia Tools and Applications. 202281845</p>
<p>Improved yolo v5 with balanced feature pyramid and attention module for traffic sign detection. L Jiang, H Liu, H Zhu, G Zhang, MATEC Web of Conferences. EDP Sciences20223553023</p>
<p>Attention-yolov4: a real-time and highaccurate traffic sign detection algorithm. Y Li, J Li, P Meng, Multimedia Tools and Applications. 8252023</p>
<p>Accelerating the response of self-driving control by using rapid object detection and steering angle prediction. B R Chang, H.-F Tsai, C.-W Hsieh, Electronics. 121021612023</p>
<p>Analysis of object detection models on duckietown robot based on yolov5 architectures. T.-K Nguyen, L T Vu, V Q Vu, T.-D Hoang, S.-H Liang, M.-Q Tran, International Journal of iRobotics. 442021</p>
<p>A stereo vision-based obstacle detection system in vehicles. K Huh, J Park, J Hwang, D Hong, Optics and Lasers in engineering. 4622008</p>
<p>Pruning convolutional neural networks for resource efficient inference. P Molchanov, S Tyree, T Karras, T Aila, J Kautz, International Conference on Learning Representations. 2017</p>
<p>Reactive control algorithm for f1tenth autonomous vehicles in unknown dynamic environments. A Morys-Magiera, A Lis, J Pudlo, A Papierok, M Dlugosz, P Skruch, Proceedings of the XXI Polish Control Conference. the XXI Polish Control ConferenceSpringer2023</p>
<p>A novel obstacle avoidance algorithm:"follow the gap method. V Sezer, M Gokasan, Robotics and Autonomous Systems. 6092012</p>
<p>Federated learning for visionbased obstacle avoidance in the internet of robotic things. X Yu, J P Queralta, T Westerlund, 2022 Seventh International Conference on Fog and Mobile Edge Computing (FMEC). IEEE2022</p>
<p>A performance contextualization approach to validating camera models for robot simulation. A Elmquist, R Serban, D Negrut, arXiv:2208.010222022arXiv preprint</p>
<p>Cnn-based object detection on low precision hardware: Racing car case study. N De Rita, A Aimar, T Delbruck, 2019 IEEE Intelligent Vehicles Symposium (IV). IEEE2019</p>
<p>Traffic sign recognition with multi-scale convolutional networks. P Sermanet, Y Lecun, The 2011 international joint conference on neural networks. IEEE2011</p>
<p>Traffic sign detection based on convolutional neural networks. Y Wu, Y Liu, J Li, H Liu, X Hu, The 2013 international joint conference on neural networks (IJCNN). IEEE2013</p>
<p>A review of motion planning for highway autonomous driving. L Claussmann, M Revilloud, D Gruyer, S Glaser, IEEE Transactions on Intelligent Transportation Systems. 2152019</p>
<p>A note on two problems in connexion with graphs. E W Dijkstra, Numerische mathematik. 111959</p>
<p>A formal basis for the heuristic determination of minimum cost paths. P E Hart, N J Nilsson, B Raphael, IEEE transactions on Systems Science and Cybernetics. 421968</p>
<p>Randomized kinodynamic planning. S M Lavalle, J J KuffnerJr, The international journal of robotics research. 2052001</p>
<p>Adaptive pure pursuit: A real-time path planner using tracking controllers to plan safe and kinematically feasible paths. B Li, Y Wang, S Ma, X Bian, H Li, T Zhang, X Li, Y Zhang, IEEE Transactions on Intelligent Vehicles. 2023</p>
<p>Motion planning and control for mobile robot navigation using machine learning: a survey. X Xiao, B Liu, G Warnell, P Stone, Autonomous Robots. 4652022</p>
<p>Survey of deep reinforcement learning for motion planning of autonomous vehicles. S Aradi, IEEE Transactions on Intelligent Transportation Systems. 2322020</p>
<p>Path planning for smart car based on dijkstra algorithm and dynamic window approach. L.-S Liu, J -F. Lin, J -X. Yao, D -W. He, J -S. Zheng, J Huang, P Shi, Wireless Communications and Mobile Computing. 20212021</p>
<p>Improving off-road planning techniques with learned costs from physical interactions. M Sivaprakasam, S Triest, W Wang, P Yin, S Scherer, 2021 IEEE International Conference on Robotics and Automation (ICRA). IEEE2021</p>
<p>Practical search techniques in path planning for autonomous driving. D Dolgov, S Thrun, M Montemerlo, J Diebel, Ann Arbor. 1001481052008</p>
<p>Global path planning for autonomous mobile robot using genetic algorithm. M Samadi, M F Othman, 2013 International Conference on Signal-Image Technology &amp; Internet-Based Systems. IEEE2013</p>
<p>Dynamic path planning of mobile robots with improved genetic algorithm. A Tuncer, M Yildirim, Computers &amp; Electrical Engineering. 3862012</p>
<p>Highspeed robot navigation using predicted occupancy maps. K D Katyal, A Polevoy, J Moore, C Knuth, K M Popek, 2021 IEEE International Conference on Robotics and Automation (ICRA). IEEE2021</p>
<p>A global path planning algorithm for robots using reinforcement learning. P Gao, Z Liu, Z Wu, D Wang, 2019 IEEE International Conference on Robotics and Biomimetics (ROBIO). IEEE2019</p>
<p>Tvenet: Transformer-based visual exploration network for mobile robot in unseen environment. T Zhang, X Hu, J Xiao, G Zhang, IEEE Access. 10722022</p>
<p>Rough terrain navigation using divergence constrained model-based reinforcement learning. S J Wang, S Triest, W Wang, S Scherer, A Johnson, Proceedings of the 5th Conference on Robot Learning, ser. Proceedings of Machine Learning Research. D Faust, G Hsu, Neumann, the 5th Conference on Robot Learning, ser. Machine Learning ResearchPMLRNov 2022164</p>
<p>Bayesian residual policy optimization:: Scalable bayesian reinforcement learning with clairvoyant experts. G Lee, B Hou, S Choudhury, S S Srinivasa, 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2021</p>
<p>Self-driving model car acquiring three-point turn motion by using improved lstm model. R Fukuoka, N Shigei, H Miyajima, Y Nakamura, H Miyajima, Artificial Life and Robotics. 262021</p>
<p>Adaptive trajectory tracking for carlike vehicles with input constraints. J Hu, Y Zhang, S Rakheja, IEEE Transactions on Industrial Electronics. 6932021</p>
<p>Kinematic trajectory tracking controller for an all-terrain ackermann steering vehicle. L Bascetta, D A Cucci, M Matteucci, IFAC-PapersOnLine. 49152016</p>
<p>A selftuning trajectory tracking controller for wheeled mobile robots. P Panahandeh, K Alipour, B Tarvirdizadeh, A Hadi, Industrial Robot: the international journal of robotics research and application. 4662019</p>
<p>Expert intervention learning: An online framework for robot learning from explicit and implicit human feedback. J Spencer, S Choudhury, M Barnes, M Schmittle, M Chiang, P Ramadge, S Srinivasa, Autonomous Robots. 2022</p>
<p>Stabilizing nmpc of wheeled mobile robots using open-source real-time software. M W Mehrez, G K Mann, R G Gosine, 2013 16th International Conference on Advanced Robotics (ICAR). IEEE2013</p>
<p>Nonlinear model predictive control for trajectory tracking of nonholonomic mobile robots: A modified approach. T P Nascimento, C E T DÃ³rea, L M G Gonc Â¸alves, International Journal of Advanced Robotic Systems. 15117298814187604612018</p>
<p>A robust model predictive control strategy for trajectory tracking of omni-directional mobile robots. D Wang, W Wei, Y Yeboah, Y Li, Y Gao, Journal of Intelligent &amp; Robotic Systems. 982020</p>
<p>Zero-shot policy transferability for the control of a scale autonomous vehicle. H Zhang, S Caldararu, S Ashokkumar, I Mahajan, A Young, A Ruiz, H Unjhawala, L Bakke, D Negrut, arXiv:2309.098702023arXiv preprint</p>
<p>Path following with deep reinforcement learning for autonomous cars. K Alomari, R C Mendoza, D Goehring, R Rojas, ROBOVIS. 2021</p>
<p>Enabling efficient, reliable real-world reinforcement learning with approximate physicsbased models. T Westenbroek, J Levy, D Fridovich-Keil, Conference on Robot Learning. PMLR2023</p>
<p>Integrated benchmarking and design for reproducible and accessible evaluation of robotic agents. J Tani, A F Daniele, G Bernasconi, A Camus, A Petrov, A Courchesne, B Mehta, R Suri, T Zaluska, M R Walter, 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2020</p>
<p>A data-efficient framework for training and sim-to-real transfer of navigation policies. H Bharadhwaj, Z Wang, Y Bengio, L Paull, 2019 International Conference on Robotics and Automation (ICRA). </p>
<p>Continuous control for high-dimensional state spaces: An interactive learning approach. R PÃ©rez-Dattari, C Celemin, J Ruiz-Del Solar, J Kober, 2019 International Conference on Robotics and Automation (ICRA). IEEE2019</p>
<p>Robust autonomous driving control using auto-encoder and end-to-end deep learning under rainy conditions. P H Phan, A Q Nguyen, L.-D Quach, H N Tran, Proceedings of the 2023 8th International Conference on Intelligent Information Technology. the 2023 8th International Conference on Intelligent Information Technology2023</p>
<p>Enhancing visual domain randomization with real images for sim-to-real transfer. A BÃ©res, B Gyires-TÃ³th, INFOCOMMUNICATIONS JOURNAL. 1512023</p>
<p>Design of ddpgbased extended look-ahead for longitudinal and lateral control of vehicle platoon. A Bayuwindra, L Wonohito, B R Trilaksono, IEEE Access. 2023</p>
<p>Cooperative adaptive cruise control using vehicle-to-vehicle communication and deep learning. H Ke, S Mozaffari, S Alirezaee, M Saif, 2022 IEEE Intelligent Vehicles Symposium (IV). IEEE2022</p>
<p>Active uncertainty reduction for safe and efficient interaction planning: A shielding-aware dual control approach. H Hu, D Isele, S Bae, J F Fisac, The International Journal of Robotics Research. 027836492312153712023</p>
<p>Real-time control for autonomous racing based on viability theory. A Liniger, J Lygeros, IEEE Transactions on Control Systems Technology. 2722017</p>
<p>Racing miniature cars: Enhancing performance using stochastic mpc and disturbance feedback. A Liniger, X Zhang, P Aeschbach, A Georghiou, J Lygeros, 2017 American Control Conference (ACC). IEEE2017</p>
<p>Bayesrace: Learning to race autonomously using prior experience. A Jain, M O'kelly, P Chaudhari, M Morari, arXiv:2005.047552020arXiv preprint</p>
<p>Robust sampling based model predictive control with sparse objective information. G Williams, B Goldfain, P Drews, K Saigol, J M Rehg, E A Theodorou, Robotics: Science and Systems. 2018. 201814p</p>
<p>Learning how to autonomously race a car: a predictive control approach. U Rosolia, F Borrelli, IEEE Transactions on Control Systems Technology. 2862019</p>
<p>Tunercar: A superoptimization toolchain for autonomous racing. M O'kelly, H Zheng, A Jain, J Auckley, K Luong, R Mangharam, 2020 IEEE international conference on robotics and automation (ICRA). IEEE2020</p>
<p>Autonomous drift cornering with mixed open-loop and closed-loop control. F Zhang, J Gonzales, K Li, F Borrelli, IFAC-PapersOnLine. 5012017</p>
<p>Autonomous drift parking using a switched control strategy with onboard sensors. E Jelavic, J Gonzales, F Borrelli, IFAC-PapersOnLine. 5012017</p>
<p>Dynamic vehicle drifting with nonlinear mpc and a fused kinematic-dynamic bicycle model. G Bellegarda, Q Nguyen, IEEE Control Systems Letters. 62021</p>
<p>A study of autonomous parking for a 4-wheel driven mobile robot. Z Joung, X D Ji, K J Wan, K Y Bae, 2007 Chinese Control Conference. IEEE2007</p>
<p>Smooth path planning for autonomous parking system. Y Yi, Z Lu, Q Xin, L Jinzhou, L Yijin, W Jianhang, 2017 IEEE Intelligent Vehicles Symposium (IV). IEEE2017</p>
<p>Optimization-based trajectory planning for autonomous parking with irregularly placed obstacles: A lightweight iterative framework. B Li, T Acarman, Y Zhang, Y Ouyang, C Yaman, Q Kong, X Zhong, X Peng, IEEE Transactions on Intelligent Transportation Systems. 2389812021</p>
<p>Design and implementation of fuzzy parallel-parking control for a car-type mobile robot. S.-J Chang, T.-H S Li, Journal of Intelligent and Robotic Systems. 342002</p>
<p>Autonomous parking control design for car-like mobile robot by using ultrasonic and infrared sensors. T.-H S Li, C.-C Chang, Y.-J Ye, G.-R Tasi, RoboCup 2005: Robot Soccer World Cup IX 9. Springer2006</p>
<p>Fpga-based autonomous parking of a car-like robot using fuzzy logic control. N Scicluna, E Gatt, O Casha, I Grech, J Micallef, 2012 19th IEEE International Conference on Electronics, Circuits, and Systems (ICECS 2012. IEEE2012</p>
<p>Autonomous fuzzy parking control of a car-like mobile robot. T.-H Li, S.-J Chang, IEEE Transactions on Systems, Man, and Cybernetics-Part A: Systems and Humans. 200333</p>
<p>Vision-based hybrid control scheme for autonomous parking of a mobile robot. D Amarasinghe, G K Mann, R G Gosine, Advanced Robotics. 2182007</p>
<p>Image-based fuzzy parking control. Y Y Aye, K Watanabe, SYSTEMS, CONTROL AND INFORMATION. 6482020</p>
<p>Automatic parallel parking algorithm for a carlike robot using fuzzy pd+ i control. E Ballinas, O Montiel, O Castillo, Y Rubio, L T Aguilar, Engineering Letters. 2642018</p>
<p>Autonomous parallel parking of a car-like mobile robot by a neuro-fuzzy sensor-based controller. K Demirli, M Khoshnejad, Fuzzy sets and systems. 2009160</p>
<p>The design of an autonomous parallel parking neuro-fuzzy controller for a car-like mobile robot. Z.-L Wang, C.-H Yang, T.-Y Guo, Proceedings of SICE Annual Conference. SICE Annual ConferenceIEEE2010. 2010</p>
<p>A machine learning approach for the segmentation of driving maneuvers and its application in autonomous parking. G Notomista, M Botsch, Journal of Artificial Intelligence and Soft Computing Research. 742017</p>
<p>Vision and dead reckoning-based end-to-end parking for autonomous vehicles. S Rathour, V John, M Nithilan, S Mita, 2018 IEEE Intelligent Vehicles Symposium (IV). IEEE2018</p>
<p>Reverse parking a car-like mobile robot with deep reinforcement learning and preview control. E Bejar, A MorÃ¡n, 2019 IEEE 9th Annual Computing and Communication Workshop and Conference (CCWC). IEEE2019</p>
<p>Deep reinforcement learningbased autonomous parking design with neural network compute accelerators. A Ã–zeloglu, Ä° G GÃ¼rbÃ¼z, I San, Concurrency and Computation: Practice and Experience. 349e66702022</p>
<p>Scaled autonomy: Enabling human operators to control robot fleets. G Swamy, S Reddy, S Levine, A D Dragan, 2020 IEEE International Conference on Robotics and Automation (ICRA). IEEE2020</p>
<p>Design and testing of cooperative motion controller for uav-ugv system. Y Li, X Zhu, 2022Mechatronics and Intelligent Transportation Systems</p>
<p>Feature-based occupancy map-merging for collaborative slam. S Sunil, S Mozaffari, R Singh, B Shahrrava, S Alirezaee, Sensors. 23631142023</p>
<p>S Talia, A Thareja, C Mavrogiannis, M Schmittle, S S Srinivasa, arXiv:2303.01428Pushr: A multirobot system for nonprehensile rearrangement. 2023arXiv preprint</p>
<p>Increasing feasibility with dynamic priority assignment in distributed trajectory planning for road vehicles. P Scheffe, G Dorndorf, B Alrifaee, 2022 IEEE 25th International Conference on Intelligent Transportation Systems (ITSC). IEEE2022</p>
<p>Demonstration of a time-efficient mobility system using a scaled smart city. L E Beaver, B Chalaki, A I Mahbub, L Zhao, R Zayas, A A Malikopoulos, Vehicle System Dynamics. 5852020</p>
<p>A research and educational robotic testbed for real-time control of emerging mobility systems: From theory to scaled experiments. B Chalaki, L E Beaver, A I Mahbub, H Bang, A A , IEEE Control Systems Magazine. 4262022Malikopoulos. applications of control</p>
<p>Urban search and rescue with anti-pheromone robot swarm architecture. R M GarcÃ­a, D H De La Iglesia, J F De Paz, V R Leithardt, G Villarrubia, 2021 Telecoms Conference (ConfTELE). IEEE2021</p>
<p>Playing atari with deep reinforcement learning. V Mnih, K Kavukcuoglu, D Silver, A Graves, I Antonoglou, D Wierstra, M Riedmiller, arXiv:1312.56022013arXiv preprint</p>
<p>Autonomous vehicles on the edge: A survey on autonomous vehicle racing. J Betz, H Zheng, A Liniger, U Rosolia, P Karle, M Behl, V Krovi, R Mangharam, IEEE Open Journal of Intelligent Transportation Systems. 32022</p>
<p>Best response model predictive control for agile interactions between autonomous ground vehicles. G Williams, B Goldfain, P Drews, J M Rehg, E A Theodorou, 2018 IEEE International Conference on Robotics and Automation (ICRA). IEEE2018</p>
<p>Path planning and control for autonomous racing. A Liniger, 2018ETH Zurich</p>
<p>Learning from simulation, racing in reality. E Chisari, A Liniger, A Rupenyan, L Van Gool, J Lygeros, 2021 IEEE International Conference on Robotics and Automation (ICRA). IEEE2021</p>
<p>Neuro-fuzzy and soft computing-a computational approach to learning and machine intelligence [book review]. J.-S R Jang, C.-T Sun, E Mizutani, IEEE Transactions on automatic control. 42101997</p>
<p>Aspects of autonomous drive control using nvidia jetson nano microcomputer. K Podbucki, T Marciniak, 2022 17th Conference on Computer Science and Intelligence Systems (FedCSIS). IEEE2022</p>
<p>Data augmentation technology driven by image style transfer in selfdriving car based on end-to-end learning. D Liu, J Zhao, A Xi, C Wang, X Huang, K Lai, C Liu, CMES-Computer Modeling in Engineering &amp; Sciences. 12222020</p>
<p>Image style transfer using convolutional neural networks. L A Gatys, A S Ecker, M Bethge, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2016</p>
<p>Imitation learning for generalizable self-driving policy with sim-to-real transfer. Z LÅ‘rincz, M Szemenyei, R Moni, ICLR 2022 Workshop on Generalizable Policy Learning in Physical World. 2022</p>
<p>Easy learning of reinforcement learning with a gamified tool. E A Dreveck, A V Salgado, E W G Clua, L M G Gonc Â¸alves, 2021 Latin American Robotics Symposium (LARS), 2021 Brazilian Symposium on Robotics (SBR), and 2021 Workshop on Robotics in Education (WRE). IEEE2021</p>
<p>Sim2real: Issues in transferring autonomous driving model from simulation to real world. J Revell, D Welch, J Hereford, South-eastCon 2022. IEEE2022</p>
<p>Robust reinforcement learning-based autonomous driving agent for simulation and real world. P AlmÃ¡si, R Moni, B Gyires-TÃ³th, 2020 International Joint Conference on Neural Networks (IJCNN). IEEE2020</p>
<p>Learning to drive fast on a duckietown highway. T P Wiggers, A Visser, International Conference on Intelligent Autonomous Systems. Springer2021</p>
<p>Image-based regularization for action smoothness in autonomous miniature racing car with deep reinforcement learning. H.-G Cao, I Lee, B.-J Hsu, Z.-Y Lee, Y.-W Shih, H.-C Wang, I.-C Wu, 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2023</p>
<p>Learning representations by back-propagating errors. D E Rumelhart, G E Hinton, R J Williams, nature. 32360881986</p>
<p>Auto-encoding variational bayes. D P Kingma, M Welling, arXiv:1312.61142013arXiv preprint</p>
<p>Zeroshot policy transfer in autonomous racing: Reinforcement learning vs imitation learning. N Hamilton, P Musau, D M Lopez, T T Johnson, 2022 IEEE International Conference on Assured Autonomy (ICAA). IEEE2022</p>
<p>Vision-based autonomous car racing using deep imitative reinforcement learning. P Cai, H Wang, H Huang, Y Liu, M Liu, IEEE Robotics and Automation Letters. 642021</p>
<p>Ensemble bayesian decision making with redundant deep perceptual control policies. K Lee, Z Wang, B Vlahov, H Brar, E A Theodorou, 2019 18th IEEE International Conference On Machine Learning And Applications (ICMLA). IEEE2019</p>
<p>Latent imagination facilitates zero-shot transfer in autonomous racing. A Brunnbauer, L Berducci, A BrandstÃ¡tter, M Lechner, R Hasani, D Rus, R Grosu, 2022 International Conference on Robotics and Automation (ICRA). IEEE2022</p>
<p>Train in austria, race in montecarlo: Generalized rl for cross-track f1 tenth lidar-based races. M Bosello, R Tse, G Pau, 2022 IEEE 19th Annual Consumer Communications &amp; Networking Conference (CCNC). IEEE2022</p>
<p>Residual policy learning facilitates efficient model-free autonomous racing. R Zhang, J Hou, G Chen, Z Li, J Chen, A Knoll, IEEE Robotics and Automation Letters. 746322022</p>
<p>High-speed autonomous racing using trajectory-aided deep reinforcement learning. B D Evans, H A Engelbrecht, H W Jordaan, IEEE Robotics and Automation Letters. 2023</p>
<p>Residual policy learning for vehicle control of autonomous racing cars. R Trumpp, D Hoornaert, M Caccamo, arXiv:2302.070352023arXiv preprint</p>
<p>Safe reinforcement learning for high-speed autonomous racing. B D Evans, H W Jordaan, H A Engelbrecht, Cognitive Robotics. 32023</p>
<p>Simto-real transfer for miniature autonomous car racing. Y.-J R Chu, T.-H Wei, J.-B Huang, Y.-H Chen, I Wu, arXiv:2011.056172020arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5599 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5599</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5599</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-115.html">extraction-schema-115</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <p><strong>Paper ID:</strong> paper-236609688</p>
                <p><strong>Paper Title:</strong> <a href="https://journals.flvc.org/FLAIRS/article/download/128551/130033" target="_blank">Detecting Anomalies in Sequences of Short Text Using Iterative Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Business managers using Intelligent Virtual Assistants (IVAs) to enhance their company’s customer service need ways to accurately and efﬁciently detect anomalies in conversations between the IVA and customers, vital for customer retention and satisfaction. Unfortunately, anomaly detection is a challenging problem because of the subjective nature of what is deﬁned as anomalous. Detecting anomalies in sequences of short texts, common in chat settings, is even more difﬁcult because independently generated texts are similar only at a semantic level, resulting in an abundance of false positives. In addition, literature for detecting anomalies in time ordered sequences of short text is shallow considering the abundance of such data sets in online settings. We introduce a technique for detecting anomalies in sequences of short textual data by adaptively and iteratively learning low perplexity language models. Our al-gorithm deﬁnes a short textual item as anomalous when its cross-entropy exceeds the upper conﬁdence interval of a trained additive regression model. We demonstrate successful case studies and bridge the gap between theory and practice by ﬁnding anomalies in sequences of real conversations with virtual chat agents. Empirical evaluation shows that our method achieves, on average, 31% higher max F1 scores than the baseline method of non-negative matrix factorization across three large human-annotated sequences of short texts.</p>
                <p><strong>Cost:</strong> 0.009</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5599.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5599.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ILM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Iterative Language Model (adaptive sliding-window n-gram LM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An unsupervised, nonparametric n-gram language model that is retrained on a sliding window when model perplexity exceeds a threshold; per-item cross-entropies are scored over time and anomalies are flagged when cross-entropy exceeds the upper confidence bound of a fitted time-series (Prophet) model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Iterative count-based n-gram language model (bigram by default)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A nonparametric count-based n-gram language model (bigrams used in experiments) trained on recent text (sliding window of size x = 2000). The model is retrained (slides) adaptively when overall perplexity exceeds a threshold (thresh_perplex, experimented with values between 2 and 3). Per-text-item cross-entropy is computed from the LM and used as an anomaly score; cross-entropies are fed into Facebook Prophet (additive regression time-series) and items with cross-entropy above the model's upper confidence interval are labeled anomalous.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_method</strong></td>
                            <td>Scoring: compute per-item cross-entropy from an adaptive count-based n-gram LM (normalized to first c=30 words); build a time series of cross-entropies and fit Facebook Prophet; mark items whose cross-entropy exceeds the upper confidence interval as anomalies. Unsupervised, no fine-tuning; adaptive retraining triggered by perplexity.</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Time-ordered sequences of short text (chat user turns / intent-specific columns in term-document representation); short, sparse conversational utterances (avg ~45 characters per user turn).</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Semantic surprises / rare or novel n-grams, spelling mistakes and new vocabulary, missed intent (semantic mismatch), preprocessing errors (causing unusual token patterns), multi-intent utterances; generally 'semantic' and 'rare-token' anomalies as measured by high cross-entropy.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Three intent-specific IVA conversational datasets from a large international airlines assistant: "Find Companion Fare Discount Code" (56,389 turns in one year; 3,000 annotated), "First Class Upgrades" (58,187 turns; 3,000 annotated), and "Gift Certificates" (8,378 turns; fully annotated for anomalies).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Max F1 (primary reported metric). ILM max F1 by intent: Gift Certificates = 0.49, First Class Upgrades = 0.15, Find Companion Fare Discount Code = 0.49. Overall reported effect: ILM achieved on average 31% higher max F1 than the TONMF baseline across the three annotated sequences. Runtime: slowest ILM runs processed >6 user turns/sec (real-time capable vs IVA throughput 4.6 turns/sec).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to TONMF (Text Outliers using Non-Negative Matrix Factorization), ILM substantially outperformed the baseline: examples—Gift Certificates ILM 0.49 vs TONMF 0.15; First Class Upgrades ILM 0.15 vs TONMF 0.05; Find Companion Fare ILM 0.49 vs TONMF 0.014. Authors attribute TONMF's poorer performance to TONMF expecting longer documents and favoring detection of unique codes/numeric tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Reported limitations: ILM uses shallow nonparametric n-grams (bigrams), so it has limited modeling power relative to deep parametric LMs; performance degrades on extremely diverse intent (First Class Upgrades was hard because many different user questions mapped to same intent); ILM does not distinguish among anomaly subcategories (annotation did not label categories during training); detection depends on hyperparameters (thresh_perplex, window size x, n-gram order, normalization length c); trade-off between update frequency and runtime (higher thresh_perplex → fewer updates → faster but potentially less responsive to distributional change). Also, ILM was validated only on IVA short-text sequences (not on tabular/structured lists) and uses Prophet for time-series thresholding which assumes normally distributed errors in its additive model.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5599.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5599.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TONMF</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Text Outliers using Non-Negative Matrix Factorization (TONMF)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A non-negative matrix factorization approach that models a term-document matrix as a sum of a low-rank component and a sparse outlier component; document outlier scores are L2 norms of the sparse component's columns and are used to detect anomalous documents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Outlier detection for text data</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>TONMF (NMF-based outlier detector)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Non-negative matrix factorization decomposes the term-document matrix A into L0 (low-rank W0 C0) and a sparse outlier matrix Z0 (A = L0 + Z0). TONMF optimizes W0, C0, Z0 with block coordinate descent; document outlier score = L2 norm of column in Z0. Parameters include rank r (number of topics, experimented r=10..45), regularization α (weights outlier penalty), and β (set to 1 in experiments). Outlier scores were fed into Prophet for temporal anomaly detection.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_method</strong></td>
                            <td>Matrix-factorization scoring: represent short texts as term-document columns, perform NMF with an added sparse outlier matrix, compute per-document outlier scores from the sparse component (L2 norm per column), then apply Prophet for temporal significance testing; unsupervised.</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Term-document matrix constructed from sequences of short text (each column = user turn). Method expects longer documents in original evaluations (e.g., Reuters), but applied here to short chat turns.</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Sparse/lexical anomalies that cannot be represented by the low-rank topic space—empirically favors unique codes and numbers (e.g., discount codes, ticket numbers), i.e., rare token-based outliers.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Same airline IVA intent datasets used for ILM: "Find Companion Fare Discount Code", "First Class Upgrades", and "Gift Certificates" (applied per 2,000-text batches to be comparable to ILM).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Max F1 (primary reported metric) for TONMF by intent: Gift Certificates = 0.15, First Class Upgrades = 0.05, Find Companion Fare Discount Code = 0.014. TONMF runtime increases with topic count r; NMF optimization is NP-hard and block coordinate descent was used for efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>TONMF was the chosen baseline and was outperformed by ILM across nearly all parameter configurations. TONMF tended to predict anomalies dominated by token-like artifacts (codes)—for Find Companion Fare, >85% of TONMF predictions were code-containing items while only ~40% of true anomalies contained codes. Authors note TONMF was originally validated on longer documents (Reuters, RCV20, Wiki People), which explains degraded performance on short chat utterances.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>TONMF expects larger documents and performs poorly on very short texts; it tends to over-detect numeric/code anomalies and miss semantic anomalies (missed intent, preprocessing errors, multi-intent utterances). Computationally heavier as r increases; solving NMF is NP-hard (practical solution via block coordinate descent). When rank r is low, increasing α (outlier regularization) may not change predictions because low-rank extraction dominates optimization.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Outlier detection for text data <em>(Rating: 2)</em></li>
                <li>No country for old members: User lifecycle and linguistic change in online communities <em>(Rating: 2)</em></li>
                <li>Exploring the limits of language modeling <em>(Rating: 1)</em></li>
                <li>Identifying semantically deviating outlier documents <em>(Rating: 2)</em></li>
                <li>Forecasting at scale <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5599",
    "paper_id": "paper-236609688",
    "extraction_schema_id": "extraction-schema-115",
    "extracted_data": [
        {
            "name_short": "ILM",
            "name_full": "Iterative Language Model (adaptive sliding-window n-gram LM)",
            "brief_description": "An unsupervised, nonparametric n-gram language model that is retrained on a sliding window when model perplexity exceeds a threshold; per-item cross-entropies are scored over time and anomalies are flagged when cross-entropy exceeds the upper confidence bound of a fitted time-series (Prophet) model.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Iterative count-based n-gram language model (bigram by default)",
            "model_description": "A nonparametric count-based n-gram language model (bigrams used in experiments) trained on recent text (sliding window of size x = 2000). The model is retrained (slides) adaptively when overall perplexity exceeds a threshold (thresh_perplex, experimented with values between 2 and 3). Per-text-item cross-entropy is computed from the LM and used as an anomaly score; cross-entropies are fed into Facebook Prophet (additive regression time-series) and items with cross-entropy above the model's upper confidence interval are labeled anomalous.",
            "model_size": null,
            "anomaly_detection_method": "Scoring: compute per-item cross-entropy from an adaptive count-based n-gram LM (normalized to first c=30 words); build a time series of cross-entropies and fit Facebook Prophet; mark items whose cross-entropy exceeds the upper confidence interval as anomalies. Unsupervised, no fine-tuning; adaptive retraining triggered by perplexity.",
            "data_type": "Time-ordered sequences of short text (chat user turns / intent-specific columns in term-document representation); short, sparse conversational utterances (avg ~45 characters per user turn).",
            "anomaly_type": "Semantic surprises / rare or novel n-grams, spelling mistakes and new vocabulary, missed intent (semantic mismatch), preprocessing errors (causing unusual token patterns), multi-intent utterances; generally 'semantic' and 'rare-token' anomalies as measured by high cross-entropy.",
            "dataset_name": "Three intent-specific IVA conversational datasets from a large international airlines assistant: \"Find Companion Fare Discount Code\" (56,389 turns in one year; 3,000 annotated), \"First Class Upgrades\" (58,187 turns; 3,000 annotated), and \"Gift Certificates\" (8,378 turns; fully annotated for anomalies).",
            "performance_metrics": "Max F1 (primary reported metric). ILM max F1 by intent: Gift Certificates = 0.49, First Class Upgrades = 0.15, Find Companion Fare Discount Code = 0.49. Overall reported effect: ILM achieved on average 31% higher max F1 than the TONMF baseline across the three annotated sequences. Runtime: slowest ILM runs processed &gt;6 user turns/sec (real-time capable vs IVA throughput 4.6 turns/sec).",
            "baseline_comparison": "Compared to TONMF (Text Outliers using Non-Negative Matrix Factorization), ILM substantially outperformed the baseline: examples—Gift Certificates ILM 0.49 vs TONMF 0.15; First Class Upgrades ILM 0.15 vs TONMF 0.05; Find Companion Fare ILM 0.49 vs TONMF 0.014. Authors attribute TONMF's poorer performance to TONMF expecting longer documents and favoring detection of unique codes/numeric tokens.",
            "limitations_or_failure_cases": "Reported limitations: ILM uses shallow nonparametric n-grams (bigrams), so it has limited modeling power relative to deep parametric LMs; performance degrades on extremely diverse intent (First Class Upgrades was hard because many different user questions mapped to same intent); ILM does not distinguish among anomaly subcategories (annotation did not label categories during training); detection depends on hyperparameters (thresh_perplex, window size x, n-gram order, normalization length c); trade-off between update frequency and runtime (higher thresh_perplex → fewer updates → faster but potentially less responsive to distributional change). Also, ILM was validated only on IVA short-text sequences (not on tabular/structured lists) and uses Prophet for time-series thresholding which assumes normally distributed errors in its additive model.",
            "uuid": "e5599.0"
        },
        {
            "name_short": "TONMF",
            "name_full": "Text Outliers using Non-Negative Matrix Factorization (TONMF)",
            "brief_description": "A non-negative matrix factorization approach that models a term-document matrix as a sum of a low-rank component and a sparse outlier component; document outlier scores are L2 norms of the sparse component's columns and are used to detect anomalous documents.",
            "citation_title": "Outlier detection for text data",
            "mention_or_use": "use",
            "model_name": "TONMF (NMF-based outlier detector)",
            "model_description": "Non-negative matrix factorization decomposes the term-document matrix A into L0 (low-rank W0 C0) and a sparse outlier matrix Z0 (A = L0 + Z0). TONMF optimizes W0, C0, Z0 with block coordinate descent; document outlier score = L2 norm of column in Z0. Parameters include rank r (number of topics, experimented r=10..45), regularization α (weights outlier penalty), and β (set to 1 in experiments). Outlier scores were fed into Prophet for temporal anomaly detection.",
            "model_size": null,
            "anomaly_detection_method": "Matrix-factorization scoring: represent short texts as term-document columns, perform NMF with an added sparse outlier matrix, compute per-document outlier scores from the sparse component (L2 norm per column), then apply Prophet for temporal significance testing; unsupervised.",
            "data_type": "Term-document matrix constructed from sequences of short text (each column = user turn). Method expects longer documents in original evaluations (e.g., Reuters), but applied here to short chat turns.",
            "anomaly_type": "Sparse/lexical anomalies that cannot be represented by the low-rank topic space—empirically favors unique codes and numbers (e.g., discount codes, ticket numbers), i.e., rare token-based outliers.",
            "dataset_name": "Same airline IVA intent datasets used for ILM: \"Find Companion Fare Discount Code\", \"First Class Upgrades\", and \"Gift Certificates\" (applied per 2,000-text batches to be comparable to ILM).",
            "performance_metrics": "Max F1 (primary reported metric) for TONMF by intent: Gift Certificates = 0.15, First Class Upgrades = 0.05, Find Companion Fare Discount Code = 0.014. TONMF runtime increases with topic count r; NMF optimization is NP-hard and block coordinate descent was used for efficiency.",
            "baseline_comparison": "TONMF was the chosen baseline and was outperformed by ILM across nearly all parameter configurations. TONMF tended to predict anomalies dominated by token-like artifacts (codes)—for Find Companion Fare, &gt;85% of TONMF predictions were code-containing items while only ~40% of true anomalies contained codes. Authors note TONMF was originally validated on longer documents (Reuters, RCV20, Wiki People), which explains degraded performance on short chat utterances.",
            "limitations_or_failure_cases": "TONMF expects larger documents and performs poorly on very short texts; it tends to over-detect numeric/code anomalies and miss semantic anomalies (missed intent, preprocessing errors, multi-intent utterances). Computationally heavier as r increases; solving NMF is NP-hard (practical solution via block coordinate descent). When rank r is low, increasing α (outlier regularization) may not change predictions because low-rank extraction dominates optimization.",
            "uuid": "e5599.1"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Outlier detection for text data",
            "rating": 2,
            "sanitized_title": "outlier_detection_for_text_data"
        },
        {
            "paper_title": "No country for old members: User lifecycle and linguistic change in online communities",
            "rating": 2,
            "sanitized_title": "no_country_for_old_members_user_lifecycle_and_linguistic_change_in_online_communities"
        },
        {
            "paper_title": "Exploring the limits of language modeling",
            "rating": 1,
            "sanitized_title": "exploring_the_limits_of_language_modeling"
        },
        {
            "paper_title": "Identifying semantically deviating outlier documents",
            "rating": 2,
            "sanitized_title": "identifying_semantically_deviating_outlier_documents"
        },
        {
            "paper_title": "Forecasting at scale",
            "rating": 1,
            "sanitized_title": "forecasting_at_scale"
        }
    ],
    "cost": 0.00872775,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Detecting Anomalies in Sequences of Short Text Using Iterative Language Models</p>
<p>Cynthia Freeman cynthia.freeman@verint.com 
Verint Intelligent Self-Service</p>
<p>University of New
Mexico</p>
<p>Ian Beaver ian.beaver@verint.com 
Verint Intelligent Self-Service</p>
<p>Abdullah Mueen mueen@cs.unm.edu 
University of New
Mexico</p>
<p>Detecting Anomalies in Sequences of Short Text Using Iterative Language Models</p>
<p>Business managers using Intelligent Virtual Assistants (IVAs) to enhance their company's customer service need ways to accurately and efficiently detect anomalies in conversations between the IVA and customers, vital for customer retention and satisfaction. Unfortunately, anomaly detection is a challenging problem because of the subjective nature of what is defined as anomalous. Detecting anomalies in sequences of short texts, common in chat settings, is even more difficult because independently generated texts are similar only at a semantic level, resulting in an abundance of false positives. In addition, literature for detecting anomalies in time ordered sequences of short text is shallow considering the abundance of such data sets in online settings. We introduce a technique for detecting anomalies in sequences of short textual data by adaptively and iteratively learning low perplexity language models. Our algorithm defines a short textual item as anomalous when its cross-entropy exceeds the upper confidence interval of a trained additive regression model. We demonstrate successful case studies and bridge the gap between theory and practice by finding anomalies in sequences of real conversations with virtual chat agents. Empirical evaluation shows that our method achieves, on average, 31% higher max F1 scores than the baseline method of non-negative matrix factorization across three large human-annotated sequences of short texts.</p>
<p>Introduction</p>
<p>We work for a company that designs and builds domainspecific Intelligent Virtual Assistants (IVAs) on behalf of other companies and organizations, typically for customer service automation. Many companies deploy IVAs for problem resolution and cutting costs in call centers.</p>
<p>A business manager using an IVA to enhance his company's customer service can analyze interactions between customers and IVAs to identify interactions leading to customer complaints like in Figure 1. This interaction indicates the business manager may need to include options to reprint a gift certificate on the company's website and have the IVA direct customers to it, or if such an option already exists, the IVA is unaware of the option and should be updated and the website should make the printing option more conspicuous.</p>
<p>Copyright © 2021by the authors. All rights reserved. CUSTOMER : Reprint a gift certificate? IVA : Gift Certificates can be purchased at XXXXX.com. The increasing adaptation of IVAs creates a problem; there is a corresponding increase in the number of humancomputer interactions to be reviewed for quality assurance. Therefore, discovering a means to expedite review and analysis of these interactions is critical. This requires efficient detection of anomalies in conversations. Conversational turns tend to be short (45 characters per user turn on average in our data) and are ordered by time.</p>
<p>Detecting anomalies under such conditions is difficult. Textual anomaly detection, even without such constraints, is already a notoriously difficult problem for a multitude of reasons: 1) What is defined as anomalous may differ based on application. The textual item lmao doc martens are just emo timbs would probably be anomalous to an IVA answering questions about airline travel but not so on Twitter. 2) What is anomalous today may not be anomalous tomorrow which is especially true for applications such as IVAs. Introduction of a new type of promotion such as a credit card offer may create textual items that are found to be anomalous initially. However, they must be considered normal soon after introduction. 3) It is unrealistic to assume that anomaly detection systems will have access to thousands of tagged data sets. For chat text, annotated data sets for anomalies are even more limited; we have the NPS Chat Corpus (Forsythand and Martell 2019) which is only tagged for speech and dialogue acts, the Twitter Triple Corpus (Sordoni et al. 2015) and Ubuntu Dialogue Corpus (Lowe et al. 2015) where both are not annotated for anomalies, and UseNet (Shaoul and C. 2013) which is also not annotated and specifically omits documents with less than 500 words. 4) Non-anomalous data occurs in much larger quantities than anomalous data. This can present a problem for a machine learning classifier approach to anomaly detection as the classes are not represented equally. Thus, an accuracy measure might present excellent results, but the accuracy is only reflecting the unequal class distribution in the data (the accuracy paradox).</p>
<p>We address these difficulties in detecting outliers in sequences of short textual data by using cross-entropies from iterative language models.</p>
<p>As we work for an IVA company, we have access to massive quantities of chat data. For our experiments, we selected a large international airlines IVA which interacts with users on the airline's website and mobile application, providing travel advice such as flight status information, baggage and security rules, and even helps with the booking process. This particular assistant was selected as it is a very active IVA with a diverse user base. On average, it responds to 4.6 user inputs per second and engages in 115.5 unique conversations per minute with a global user base.</p>
<p>The iterative language model updates in an adaptive manner, based on the perplexity of the language model. We compare the iterative language models to a non-negative matrix factorization method built for textual anomaly detection that has been reported to outperform many other commonly used baselines such as robust principal component analysis (RPCA) and singular value decomposition (SVD). Our iterative language model achieves, on average, 31% higher max F1 scores on a large human to IVA conversational data set and is also unsupervised.</p>
<p>Related Work</p>
<p>Existing studies cannot address the problem of how to detect outliers in sequences of short text. Of the anomaly detection methods that are designed for a textual domain:</p>
<ol>
<li>
<p>Existing methods often assume that the pieces of text are large (Guthrie 2008) containing at least 1000 words or are full-length newspaper articles as in (Zhuang et al. 2017). Short text does not have enough content or words which hinders the application of conventional machine learning and text mining algorithms (Chen, Jin, and Shen 2011).</p>
</li>
<li>
<p>Existing methods are often built for very specific tasks such as authorship identification (Guthrie, Guthrie, and Wilks 2008) or detecting outlier sections in legal documents (Aktolga, Ros, and Assogba 2011). For example, in (Aktolga, Ros, and Assogba 2011), outliers in legal documents are detected by exploiting bill structure, specifically Sections, the smallest units of a bill. However, detecting anomalies in sequences of short text is more general and includes not just IVA conversations but also social domains like Twitter and Facebook (Bakarov, Yadrintsev, and Sochenkov 2018;Nedelchev, Usbeck, and Lehmann 2020).</p>
</li>
<li>
<p>Existing methods are typically not built for anomaly detection in time ordered text. For example, the work in (Jain et al. 2019) involves time ordered text, but the goal is to detect malicious chatbots instead of anomalous texts from real users.</p>
</li>
<li>
<p>Existing methods do not take into account the dynamic nature of data such as chat or tweets that make it difficult to keep models up to date. In (Xia, GAO, and others 2005), support vector machines (SVMs) trained on chat data annotated for anomalies need to be frequently updated or performance suffers. Frequent periodic retraining of the SVM is not feasible as this requires constant annotation of chat corpora.</p>
</li>
</ol>
<p>We address these problems by identifying outliers via language model cross-entropies, inspired from the work in (Danescu-Niculescu-Mizil et al. 2013) where crossentropies are used to predict how long a user will stay active in an Internet community.</p>
<p>Language models can be parametric or nonparametric. Parametric approaches include deep learning techniques but require large quantities of data and often cannot adapt to rapid changes in the distribution of the data in an online setting. Nonparametric approaches include count-based techniques. Although they tend to perform worse compared to parametric approaches, nonparametric approaches can efficiently incorporate new information and require significantly less data (Jozefowicz et al. 2016). Given our data's sparse and dynamic nature, we restrict ourselves to nonparametric approaches (more specifically, statistics on N-grams). By incorporating nonparametric techniques to account for short text and updating the language model on a sliding window when its perplexity is too high, we can also account for our data's dynamic nature.</p>
<p>Methods</p>
<p>We begin by giving some background on language models and how we use cross-entropies to detect anomalies in sequences of short text. We then introduce our iterative language models 1 and discuss our baseline.</p>
<p>Language Models</p>
<p>A language model is a probability distribution over sequences of symbols pertaining to a language (Jurafsky 2000), and the perplexity is used to evaluate the quality of a language model where the lower the value, the better. For bigrams, the perplexity of the sequence W = w 1 w 2 ...w N is: (
N i=1
1 P (wi|wi−1) ) 1/N . Our iterative language model "slides" (retrains) when the perplexity reaches a threshold.</p>
<p>Cross-Entropy</p>
<p>The cross-entropies of textual items (Danescu-Niculescu-Mizil et al. 2013) are used to determine anomalies. The cross-entropy of a textual item p given a language model LM is: H(p, LM ) = − 1 N j log(P LM (p j )) where p j = n-grams making up p, N = number of n-grams making up p, and P LM (p j ) = probability of n-gram p j under the LM. Higher n-grams are left for future work due to the shortness of data. The higher H(p, LM ) is, the more surprising the item p is given the recent, past linguistic state. In other words, a low H(p, LM ) means that p reflects what is commonly seen in the past.</p>
<p>Iterative Language Models</p>
<p>How the iterative language model (ILM) is updated and used to detect anomalies is highlighted in Algorithm 1. If a language model is trained on texts from dates or indices a to b, we represent this language model as LM a:b .</p>
<p>The model takes as input a list of textual items. In our IVA application, every textual item is a user turn in conversations with an IVA. This list is sorted by time. The ILM takes as parameters the following: 1) thresh perplex = the threshold for perplexity to retrain the language model, 2) x = training window size, and 3) n = the n-gram to use for crossentropy calculation. The LM is trained on the first x many textual items, creating LM 0:x . For every textual item i onwards from x, the cross-entropy and perplexity are determined using the language model and chosen n-gram. If the perplexity &gt; thresh perplex , we slide the language model forward by retraining on new textual items.</p>
<p>We input the cross-entropies to Facebook Prophet (Taylor and Letham 2018), an additive regression model involving a special time series decomposition method with a piecewise linear or logistic growth curve trend, a yearly seasonal component modeled using Fourier series or a weekly seasonal component, an optionally user-provided list of holidays, and an error term that is assumed to be normally distributed. Parameters are estimated using MAP. Prophet formulates the forecasting problem as a curve-fitting exercise, similar to generalized additive modeling. Thus, Prophet can innately handle time series with missing time steps. For our iterative language model, we have a time series of crossentropies, and we do not necessarily have cross-entropies for every time step; this is dependent on when a customer chats with an intelligent virtual assistant, but Facebook Prophet can deal with these irregularly sampled time series. We input the cross-entropies to Facebook Prophet and train a forecasting model, using the confidence interval of the model to detect anomalies. Prophet was chosen due to its ease of use, requiring little expert knowledge, and its open availability, but other viable options include any time series anomaly detection method that can incorporate irregular sampling.</p>
<p>Baseline: TONMF</p>
<p>There are no existing studies that can address the problem of how to detect outliers in sequences of short text. However, we provide the closest baseline possible that we could find for this task. We use as a baseline a non-negative matrix factorization method adjusted for detecting outliers in text called Text Outliers using Non-Negative Matrix Factorization (TONMF) developed in (Kannan et al. 2017).</p>
<p>Non-negative matrix factorization (NMF) approximates a non-negative matrix X ∈ R p×n to a lower rank approximation r ≤ rank(X). A non-negative basis matrix W ∈ R p×r and coordinate matrix C ∈ R r×n are determined that minimizes ||X − W C|| 2 F where F is the Frobenius norm. In (Kannan et al. 2017), the authors model the outliers along with the low rank space of the input matrix. Suppose A is the term-document matrix. In our application, every row represents a word, and every column represents a user turn in conversations with an IVA. A ∈ R m×n is represented as 
for i in range(x+1, length(textualItems)) do p ← textualItems[i]; pj ← determineNgrams(p,n); N ← length(pj); CES.append(− 1 N j log(LM (pj))) if ( N k=1 1 P (w k |w k−1 ) ) 1/N &gt; thresh perplex then LM ← trainLM(textualItems[i-x:i], n); allCES.append(CES); CES ← [];
return allCES a sum: A = L 0 + Z 0 where L 0 = W 0 C 0 and W 0 ∈ R m×r and C 0 ∈ R r×n . In other words, every document in A is represented as a linear combination of r topics. In situations where a document cannot be well-represented by L 0 , it is placed in the outlier matrix Z 0 as a non-zero entry.</p>
<p>Outlier scores for documents are calculated by the L2 norm of columns in Z 0 . We feed these outlier scores to Prophet (Taylor and Letham 2018) as like with the iterative language model. W 0 , C 0 , and Z 0 are determined via block coordinate descent for computational simplicity.</p>
<p>Empirical Evaluation</p>
<p>We begin with a description of the large, real world data sets used and how they were annotated. We then proceed with a description of how data was preprocessed and parameters chosen and conclude with results comparing the performance of the iterative language model and TONMF.</p>
<p>Data Set and Annotation</p>
<p>The airlines IVA that we selected for our experiments can recognize 1,230 unique user intentions, or interpretations of a user input that allows one to formulate the best response. For example, if the customer asks about upgrading his flight due to health issues, the IVA determines that the customer's intent is about First Class Upgrades and responds accordingly. The intentions are used as a class label within the IVA. Once the IVA determines the user intention, the response associated with that intention is returned.</p>
<p>We selected three intents of varying popularity levels to monitor for our experiment: Find Companion Fare Discount Code, First Class Upgrades, and Gift Certificates. Find Companion Fare and First Class Upgrades are in the top ten most frequently hit intents; in one year of logs First Class Upgrades was hit 58,187 times and Find Companion Fare Discount Code was hit 56,389 times. We also wanted to experiment with an intent that was only moderately popular, so we included Gift Certificates which has 8,378 textual items.</p>
<p>Unlike newspaper articles or movie reviews, customer text in IVA conversations tends to be much shorter, presenting a significant challenge for tagging. For every intent, the user text was fed to a language model. IVA responses are excluded because the response is usually identical for the same intent. However, the IVA response was provided for annotators. A graduate student fully tagged the Gift Certificates intent data set for anomalies. However, for Find Companion Fare Discount Code and First Class Upgrades, only 3,000 user inputs of each intent were annotated due to the enormity of these two data sets. The annotator was instructed to mark any of the following as anomalous:</p>
<ol>
<li>Missed Intent (not due to preprocessing): Sometimes the IVA will incorrectly classify a user's intent in the conversation (such as in Figure 1). As another example: in the Gift Certificates intent, the user may ask about Amazon gift cards, but the IVA incorrectly assumes this involves gift cards that can only be used for airline miles. For the Find Companion Fare Discount Code intent, the customer may ask about where to apply the code when purchasing a ticket online, but the IVA directs the user to an account link to see existing discount codes instead. For the First Class Upgrades intent, the IVA provides options on how to buy such upgrades instead of helping the customer determine if upgrades are even available in the first place or are already bought out for a particular flight.</li>
</ol>
<p>Spelling Mistakes and New Vocabulary:</p>
<p>A spelling mistake may be infrequent, and, therefore, be marked as an anomaly such as: buyiing gift certificates. Novel terminology assigned to an existing intent may mean that a new product or service has been released that needs to be added to the IVA's intent library.</p>
<ol>
<li>Preprocessing Errors: A preprocessing step done by the IVA may cause an error in intent classification. For example, the user may state that he is looking for gift certificates because they are missing in his account. However, the preprocessing step converts looking for gift certificates to search for gift certificates which brings up a menu of gift certificates one can buy instead of pre-bought gift certificates under one's account.</li>
</ol>
<p>Multiple Intents:</p>
<p>The user may ask something that has multiple possible intents. For example, the user may ask for a gift certificate for miles because of a death in the family. Acceptable intents would include Gift Certificates as well as Bereavement Fare.</p>
<p>Most of these categories require attention from the business manager to improve IVA performance and prevent issues from reaching a larger set of customers over a longer duration of time. For example, assuming the intent under review was defined for answering questions around Gift Cer-tificates that can only be used for flights, a business user may see the following anomalous scenarios that need attention:</p>
<p>• For category 1: Missed Intent, if customers ask about using Amazon gift cards and the IVA incorrectly returns the Gift Certificates response which states they may be used for purchasing flights, this is indicative of user confusion on how other types of gift cards can be applied. If other gift cards cannot be used for purchasing flights, the requirements must be made more explicit on the airlines website or there needs to be a new intent generated in the IVA for these type of redemption questions.</p>
<p>• For category 2: Spelling Mistakes and New Vocabulary, identifying novel spelling mistakes are helpful for our IVA developers in building our word to vocabulary term mappings. As a preprocessing step in the IVA, words are stemmed and then mapped to specific vocabulary terms. For example, words such as baggage or carry-on are mapped to a Luggage vocabulary label. We have a standalone tool to build up this vocabulary by loading a set of user inputs and then exposing all of the words that are unknown to the IVA. Content creators can then quickly associate misspellings to which vocabulary labels they belong to and then export these changes for inclusion in the next IVA version. Also, the identification of unexpected words in a given intent may mean the context around the words intended to map to the intent may have changed. For example, before 2013, the word pixel in a device help desk IVA would have been associated with a measurement of screen resolution. But within that year, there would begin to be occurrences of the bigram Google pixel in conversations for device support that would be flagged as anomalous during the first several occurrences. This would alert IVA designers that they need to differentiate between questions about screen resolution and a specific smart phone device in the IVA intents.</p>
<p>• For category 3: Preprocessing Errors, converting looking for gift certificates to search for gift certificates also indicates a problem in our word to vocabulary term mappings in our IVA preprocessing step. Normalizing looking to search may be too great an assumption, and we must consider the possibility of the word looking to apply to multiple situations such as a missing gift certificate in an account.</p>
<p>• For category 4: Multiple Intents, we can identify cases where the IVA can be more personable. If a customer asks for gift certificates because of a death in the family, the IVA can still direct the user to the Gift Certificates intent but also use apologetic language.</p>
<p>As our experimental data is IVA to human conversations, one might ask why we do not just consider sentiment analysis, escalation detection (e.g. Can I talk to a human?), or intent misclassification for anomaly detection. Simply performing sentiment analysis or escalation detection on the text, although possibly helpful, is not enough. For our IVA to human conversational data, once customers determine that their concerns are not being addressed, they very frequently just leave the chat instead of spewing expletives or expressing frustration. Anomalies in IVA conversations also encompass more than just multi-label classification errors as shown in the above four anomaly categories. Note, however, there still needs to be a way to detect any missed intent classification. This is not as simple as just looking at classifier confidence. Intent classification can be done via a unique combination of machine learning classifiers, regular expressions, and conversation flow decision trees. We wanted a generic enough methodology that only requires customer text which is independent of the implementation details of the IVA.</p>
<p>Application</p>
<p>We use the adaptive update language model, set x = 2000, and experiment with various thresh perplex (where 2 &lt; thresh perplex &lt; 3). Every intent data set has its own language model. In calculating cross-entropies, we normalize by just using the first c words of p. This form of normaliza-tion is employed in (Danescu-Niculescu-Mizil et al. 2013) as there is no consensus on how to normalize entropy measures. We use c = 30 where results are stable across multiple choices of c.</p>
<p>We also perform TONMF on every 2, 000 many textual items (to make it comparable to the ILM) and test various parameter configurations. Every intent data set has its own application of TONMF. In our implementation of TONMF, we restricted β = 1 as the creators of TONMF have stated that the algorithm is not overly sensitive to choice of β. We experiment with r =10 to 45 topics per intent. As for α, it balances the importance given to outliers against the matrix sparsity criterion during regularization. The larger α is, the more important the outlier portion of the regularization. However, for lower ranks of r, the increase in the value of α does not make any predictions. This is because, beyond a particular limit, the weights given to the outlier criterion do not supersede the optimization's main objective of extracting low-rank patterns from the data. Figure 2 shows the run time and F1 scores for every parameter configuration for the iterative language model versus the baseline TONMF, using a confidence interval of .95. For nearly all parameter configurations, the iterative language model (ILM) has higher F1 scores than the baseline (TONMF) across all time. In addition, the ILM can perform faster than TONMF in several cases.</p>
<p>Results</p>
<p>For Gift Certificates, the highest F1 score for the ILM was .49 whereas the baseline can only reach .15. For First Class Upgrades, ILM achieves .15, and TONMF can only reach .05. For Find Companion Fare Discount code, ILM hits .49 whereas the baseline can only hit .014.</p>
<p>First Class Upgrades was the most difficult data set for both the ILM and TONMF. This is because this intent was quickly discovered by the annotator to encompass too many user questions that the IVA was not customized to address. The IVA response to a user question hitting the First Class Upgrades intent is: You can use your miles to upgrade in advance, request a Paid Upgrade during check-in or at the departure gate, or if you're an XXXXX member, you can upgrade for free. However, this does not cover questions regarding if a first class upgrade for a particular flight is available or bought out, first class upgrade code usage, or how to buy coach seats when only first class seats are available. Yet all of these questions are classified by the IVA under the First Class Upgrades intent. In addition, midway through the year, a new premium upgrade was promoted by the airlines company, but the IVA was never updated to reflect this change. Thus, all questions involving these premium upgrades was directed to First Class Upgrades, confusing customers. There was a lot more variety in the kinds of questions customers asked in the First Class Upgrades intent compared to the other two intents, making it difficult for ILM and TONMF to establish a textual norm.</p>
<p>The four types of anomalies were not distinguished during annotation for the sake of time and effort on the anno-tator's part (especially as the IVA data is proprietary, and, thus, we cannot utilize annotation crowdsourcing tools like Mechanical Turk), but a deeper analysis on the categories would be valuable and is left for future work. For example, we determined that TONMF favors detecting anomalies belonging in the category of unique codes and numbers (e.g. a customer asks if a Discount Code XYZ is valid or if their ticket number is available for an upgrade). In fact, for Find Companion Fare Discount Code, textual items containing codes comprised over 85% of TONMFs predictions whereas textual items containing codes only comprise 40% of Find Companion Fare Discount Code. Such predictions make up 20% of predictions made by the ILM. Similarly, for First Class Upgrades, predictions containing only codes make up 8% of the data set, 20-30% of predictions made by TONMF, and 1-6% of predictions made by the ILM. TONMF generally performs worse; this is most likely due to the fact that TONMF expects larger bodies of text. In (Kannan et al. 2017), TONMF was only tested on Reuters newspaper articles, RCV20, and Wiki People. It would be beneficial to include such analysis on the other anomaly categories.</p>
<p>Accuracy-speed trade-off is universal. For the iterative language model, the higher thresh perplex is, the fewer slides the language model makes (fewer updates), and, in turn, the number of times the language model is retrained is decreased. Thus, the time required to determine crossentropies for the entire data set goes down. However, even the slowest of the ILM runs was able to process over 6 user turns per second, which is fast enough to deploy in realtime alongside this airline IVA which answers 4.6 turns/sec. For TONMF, an increase in r (number of topics) increases the time needed to solve the optimization problem. Nonnegative matrix factorization is a NP-hard problem; thus, the authors make use of block coordinate descent for computational efficiency.</p>
<p>Significance and Impact</p>
<p>Detecting outliers in sequences of short text is a difficult problem because text is typically sparse and high dimensional. However, this detection is vital for IVAs in use by business managers who seek ways to improve the customer experience. Existing techniques assume that the text samples either have no ordering or are long; however, this is not always the case especially in the domains of chat, Facebook comments, or Twitter. The shortage of publicly available, annotated resources compounds our problems, and even if annotated data is available, the dynamic nature of conversational data necessitates constant retraining of models. In this paper, we demonstrate our technique for detecting outliers in sequences of short text using cross-entropies from iterative language models. We take advantage of our company's massive repository of chat data sets to address the lack of publicly available, annotated data. We compare the iterative language model to TONMF as a baseline and achieve, on average, 31% higher max F1 scores on real human to IVA conversations.</p>
<p>Our ultimate goal is to determine how to improve our IVAs given the outliers and the categories they belong to. By deploying our textual anomaly detection system alongside the IVA, we can record anomalies as they happen in real-time for downstream health monitoring of live production IVAs and help identify how the IVA can be improved.</p>
<p>Figure 1 :
1An anonymized anomalous conversation between a customer and a live airlines IVA. Printing, not purchasing, gift certificates is the customer's intent.</p>
<p>Figure 2 :
2Run time (in seconds) versus F1 scores for the intents Gift Certificates, First Class Upgrades, Find Companion Fare Discount Code for the iterative language model (ILM, blue dots) and baseline (TONMF, orange stars) where every point represents a parameter configuration. The confidence interval was .95.
Implementations available on https://anon-share. s3-us-west-2.amazonaws.com/ilm_2020.zip</p>
<p>Detecting outlier sections in us congressional legislation. E Aktolga, I Ros, Y Assogba, Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval. the 34th international ACM SIGIR conference on Research and development in Information RetrievalAktolga, E.; Ros, I.; and Assogba, Y. 2011. Detecting outlier sec- tions in us congressional legislation. In Proceedings of the 34th in- ternational ACM SIGIR conference on Research and development in Information Retrieval, 235-244.</p>
<p>Anomaly detection for short texts: Identifying whether your chatbot should switch from goal-oriented conversation to chit-chatting. A Bakarov, V Yadrintsev, I Sochenkov, International Conference on Digital Transformation and Global Society. SpringerBakarov, A.; Yadrintsev, V.; and Sochenkov, I. 2018. Anomaly detection for short texts: Identifying whether your chatbot should switch from goal-oriented conversation to chit-chatting. In Interna- tional Conference on Digital Transformation and Global Society, 289-298. Springer.</p>
<p>Short text classification improved by learning multi-granularity topics. M Chen, X Jin, D Shen, Twenty-Second International Joint Conference on Artificial Intelligence. Chen, M.; Jin, X.; and Shen, D. 2011. Short text classification improved by learning multi-granularity topics. In Twenty-Second International Joint Conference on Artificial Intelligence.</p>
<p>No country for old members: User lifecycle and linguistic change in online communities. C Danescu-Niculescu-Mizil, R West, D Jurafsky, J Leskovec, C Potts, Proceedings of the 22nd international conference on World Wide Web. the 22nd international conference on World Wide WebDanescu-Niculescu-Mizil, C.; West, R.; Jurafsky, D.; Leskovec, J.; and Potts, C. 2013. No country for old members: User lifecycle and linguistic change in online communities. In Proceedings of the 22nd international conference on World Wide Web, 307-318.</p>
<p>E N Forsythand, C H Martell, The nps chat corpus. Forsythand, E. N., and Martell, C. H. 2019. The nps chat corpus.</p>
<p>An unsupervised approach for the detection of outliers in corpora. D Guthrie, L Guthrie, Y Wilks, Guthrie, D.; Guthrie, L.; and Wilks, Y. 2008. An unsupervised approach for the detection of outliers in corpora. Statistics 3409- 3413.</p>
<p>Unsupervised detection of anomalous text. D Guthrie, Ph.D. Dissertation, CiteseerGuthrie, D. 2008. Unsupervised detection of anomalous text. Ph.D. Dissertation, Citeseer.</p>
<p>Characterizing and detecting livestreaming chatbots. S Jain, D Niranjan, H Lamba, N Shah, P Kumaraguru, Proceedings of the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining. the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and MiningJain, S.; Niranjan, D.; Lamba, H.; Shah, N.; and Kumaraguru, P. 2019. Characterizing and detecting livestreaming chatbots. In Pro- ceedings of the 2019 IEEE/ACM International Conference on Ad- vances in Social Networks Analysis and Mining, 683-690.</p>
<p>Exploring the limits of language modeling. R Jozefowicz, O Vinyals, M Schuster, N Shazeer, Y Wu, arXiv:1602.02410arXiv preprintJozefowicz, R.; Vinyals, O.; Schuster, M.; Shazeer, N.; and Wu, Y. 2016. Exploring the limits of language modeling. arXiv preprint arXiv:1602.02410.</p>
<p>Speech &amp; language processing. D Jurafsky, Pearson Education IndiaJurafsky, D. 2000. Speech &amp; language processing. Pearson Edu- cation India.</p>
<p>Outlier detection for text data. R Kannan, H Woo, C C Aggarwal, H Park, Proceedings of the 2017 siam international conference on data mining. the 2017 siam international conference on data miningSIAMKannan, R.; Woo, H.; Aggarwal, C. C.; and Park, H. 2017. Outlier detection for text data. In Proceedings of the 2017 siam interna- tional conference on data mining, 489-497. SIAM.</p>
<p>The ubuntu dialogue corpus: A large dataset for research in unstructured multiturn dialogue systems. R Lowe, N Pow, I Serban, J Pineau, R Nedelchev, R Usbeck, J Lehmann, arXiv:1506.08909Proceedings of The 12th Language Resources and Evaluation Conference. The 12th Language Resources and Evaluation ConferencearXiv preprintTreating dialogue quality evaluation as an anomaly detection problemLowe, R.; Pow, N.; Serban, I.; and Pineau, J. 2015. The ubuntu dialogue corpus: A large dataset for research in unstructured multi- turn dialogue systems. arXiv preprint arXiv:1506.08909. Nedelchev, R.; Usbeck, R.; and Lehmann, J. 2020. Treating dia- logue quality evaluation as an anomaly detection problem. In Pro- ceedings of The 12th Language Resources and Evaluation Confer- ence, 508-512.</p>
<p>C Shaoul, C , W , A reduced redundancy usenet corpus. Shaoul, C., and C., W. 2013. A reduced redundancy usenet corpus.</p>
<p>A neural network approach to context-sensitive generation of conversational responses. A Sordoni, M Galley, M Auli, C Brockett, Y Ji, M Mitchell, J.-Y Nie, J Gao, B Dolan, arXiv:1506.06714Sordoni, A.; Galley, M.; Auli, M.; Brockett, C.; Ji, Y.; Mitchell, M.; Nie, J.-Y.; Gao, J.; and Dolan, B. 2015. A neural network ap- proach to context-sensitive generation of conversational responses. arXiv:1506.06714.</p>
<p>Forecasting at scale. S J Taylor, B Letham, The American Statistician. 721Taylor, S. J., and Letham, B. 2018. Forecasting at scale. The American Statistician 72(1):37-45.</p>
<p>Nil is not nothing: Recognition of chinese network informal language expressions. Y Xia, W Gao, Xia, Y.; GAO, W.; et al. 2005. Nil is not nothing: Recognition of chinese network informal language expressions.</p>
<p>Identifying semantically deviating outlier documents. H Zhuang, C Wang, F Tao, L Kaplan, J Han, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. the 2017 Conference on Empirical Methods in Natural Language ProcessingZhuang, H.; Wang, C.; Tao, F.; Kaplan, L.; and Han, J. 2017. Iden- tifying semantically deviating outlier documents. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, 2748-2757.</p>            </div>
        </div>

    </div>
</body>
</html>
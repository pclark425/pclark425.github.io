<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1300 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1300</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1300</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-29.html">extraction-schema-29</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <p><strong>Paper ID:</strong> paper-195874093</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/1907.04685v2.pdf" target="_blank">Assessing Transferability from Simulation to Reality for Reinforcement Learning</a></p>
                <p><strong>Paper Abstract:</strong> Learning robot control policies from physics simulations is of great interest to the robotics community as it may render the learning process faster, cheaper, and safer by alleviating the need for expensive real-world experiments. However, the direct transfer of learned behavior from simulation to reality is a major challenge. Optimizing a policy on a slightly faulty simulator can easily lead to the maximization of the `Simulation Optimization Bias` (SOB). In this case, the optimizer exploits modeling errors of the simulator such that the resulting behavior can potentially damage the robot. We tackle this challenge by applying domain randomization, i.e., randomizing the parameters of the physics simulations during learning. We propose an algorithm called Simulation-based Policy Optimization with Transferability Assessment (SPOTA) which uses an estimator of the SOB to formulate a stopping criterion for training. The introduced estimator quantifies the over-fitting to the set of domains experienced while training. Our experimental results on two different second order nonlinear systems show that the new simulation-based policy search algorithm is able to learn a control policy exclusively from a randomized simulator, which can be applied directly to real systems without any additional training.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1300.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1300.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Custom randomized physics simulator</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Custom Lagrange-based randomized physics simulator for Quanser Ball-Balancer and Cart-Pole</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A custom simulator that integrates Lagrange-formulated differential equations forward in time and supports domain randomization of physical parameters (masses, friction, action delays, gear backlash, etc.) and observation noise; used to train RL policies for two under-actuated robotic balancing tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>custom randomized physics simulator (Lagrange-based)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Simulator implements the systems' dynamics via the Lagrange formalism and numerical integration, with wrappers to randomize domain parameters (masses, friction coefficients, motor/gear parameters, action delays) and to add observation noise; designed to produce multiple i.i.d. domain instances sampled from a user-specified distribution ν(ξ;φ).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>mechanics / robotics (rigid-body dynamics)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>medium-fidelity (approximate continuous dynamics using analytic Lagrangian models; includes many mechanical effects but is not a high-fidelity contact/wear model)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Includes: continuous-time dynamics integrated forward, masses, viscous friction, static thresholds (gear backlash modeled via deadzone), motor voltage->torque mapping, action delays, observation noise; supports sampling domain-parameter distributions. Omits or approximates: fine-grained wear/aging effects (e.g., rack/pinion wear), potentially complex contact articulation, and unmodelled hardware nonlinearities; simulator remains an approximation of real hardware.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>SPOTA-trained policies (and baseline EPOpt and PPO policies)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Model-free reinforcement learning policies (policy search meta-algorithm SPOTA using a policy optimizer subroutine, compared to EPOpt and PPO), implemented as parameterized control policies (neural-network policies implied; optimized with Adam).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Control: stabilize a ball at the center of a 2-DoF plate (Ball-Balancer) and stabilize an inverted pendulum on a cart (Cart-Pole); i.e., robust control policy learning under parameter uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td>Policies trained in the randomized simulator solved the simulated tasks reliably (no exact numeric returns provided); SPOTA reported final Upper Confidence Bound on the Optimality Gap (UCBOG) values of 46.42 (Ball-Balancer) and 55.14 (Cart-Pole), indicating estimated transfer-suboptimality bounds.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Real-world Quanser Ball-Balancer and Quanser Cart-Pole platforms (physical systems)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>Domain-randomized policies (SPOTA and EPOpt) largely transferred directly to the real systems without fine-tuning; PPO trained on a single nominal model largely failed to transfer (failed in all but 2 trials on Ball-Balancer and failed on all Cart-Pole trials). Exact numeric real-world returns not provided in paper (evaluations used 40 rollouts per policy on Ball-Balancer and 10 on Cart-Pole).</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>No explicit multi-fidelity comparison (e.g., low vs high fidelity) was performed; instead the work compares policies trained on a single nominal model versus policies trained on randomized-domain simulators.</td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>The paper argues that simulator must cover the target domain within the source (randomized) distribution; it does not specify a minimum set of physical features, but emphasizes that missing real-world effects (if not covered by the randomization) can break transfer. It notes that higher simulator accuracy alone does not guarantee transfer and that domain randomization acts as a robustness/regularization mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Reported transfer failures on the Cart-Pole attributed to unmodeled effects (e.g., heavy wear/tear of the plastic pinion and unmodeled rack/pinion interactions) not captured by the simulator; PPO (nominal-model training) failed to transfer in most trials on both platforms.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing Transferability from Simulation to Reality for Reinforcement Learning', 'publication_date_yy_mm': '2019-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1300.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1300.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Catapult toy simulator (Mars/Venus)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Two-domain catapult spring simulator (illustrative Bernoulli Mars/Venus example)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simplified analytic simulator used as an illustrative example: a linear spring catapult with domain parameters (gravity g, spring stiffness k, pre-extension x) taking one of two fixed parameter sets (Mars or Venus) drawn from a Bernoulli distribution; used to demonstrate the Simulation Optimization Bias and Optimality Gap concepts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>analytical two-domain catapult simulator</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Very low-fidelity analytic model of a vertical catapult as a linear spring launching a point mass; closed-form expression for flight height h(θ,ξ) = k(θ-x)^2/(2 m g) used to compute returns under two discrete domain parameter sets (ξ_M, ξ_V).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>mechanics (ballistics / simple energy conservation)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>low-fidelity (highly simplified analytical model)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Models: gravitational acceleration, spring stiffness, pre-extension, point-mass energy conservation. Ignores: air resistance, aerodynamic effects, finite size of projectile, actuator dynamics, sensor noise, and other real-world nonlinearities.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>scalar policy parameter θ (analytic parameterization)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A single-parameter policy (θ) controlling spring extension; optimization solved analytically for illustrative purposes and perturbed with Gaussian noise to model suboptimal candidate solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Analytical optimization over two discrete domains to illustrate Monte Carlo approximation, Simulation Optimization Bias, and Optimality Gap (i.e., choose θ to minimize expected flight height across domains).</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td>Analytic expressions used; results showed example numeric values in the illustrative experiment: with n=30 domains, estimated OG ≈ 4.97 m and true OG ≈ 4.23 m; simulation optimization bias b ≈ 0.911 m for the best policy computed from n=30 domains (values derived in the illustrative example).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Not applicable (toy analytic example used for theoretical illustration, not for physical transfer).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Not applicable; example purposefully uses severely simplified physics to illustrate theoretical concepts.</td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Example deliberately demonstrates that even with simple low-fidelity models, Monte Carlo sampling and domain randomization reveal SOB and OG behavior; paper uses it to motivate the need to consider distributional sampling rather than single nominal models.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not an experimental failure case; the toy example shows that with finite sampling the estimated-optimum can be biased and lead to suboptimal real-world performance if target domain distribution is not properly represented.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing Transferability from Simulation to Reality for Reinforcement Learning', 'publication_date_yy_mm': '2019-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1300.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1300.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>General-purpose physics engines (mention)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>General-purpose physics engines (unnamed)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper briefly mentions that simulators can be constructed either by implementing physical laws directly or by using general-purpose physics engines; no specific engine names or usages are given in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>general-purpose physics engines (unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Generic mention of physics engines as an alternative to implementing custom simulators; noted as possible sources of simulators but not used or specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>general physical simulation (mechanics)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>The paper remarks that even accurate simulators are approximations and that domain randomization is used as a robustness strategy; it does not detail which fidelity levels of physics engines are sufficient.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>No specific failure cases for general-purpose engines are provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing Transferability from Simulation to Reality for Reinforcement Learning', 'publication_date_yy_mm': '2019-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Closing the sim-to-real loop: Adapting simulation randomization with real world experience <em>(Rating: 2)</em></li>
                <li>Sim-to-real transfer of robotic control with dynamics randomization <em>(Rating: 2)</em></li>
                <li>Domain randomization for transferring deep neural networks from simulation to the real world <em>(Rating: 2)</em></li>
                <li>Learning dexterous in-hand manipulation <em>(Rating: 2)</em></li>
                <li>Sim-to-real robot learning from pixels with progressive nets <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1300",
    "paper_id": "paper-195874093",
    "extraction_schema_id": "extraction-schema-29",
    "extracted_data": [
        {
            "name_short": "Custom randomized physics simulator",
            "name_full": "Custom Lagrange-based randomized physics simulator for Quanser Ball-Balancer and Cart-Pole",
            "brief_description": "A custom simulator that integrates Lagrange-formulated differential equations forward in time and supports domain randomization of physical parameters (masses, friction, action delays, gear backlash, etc.) and observation noise; used to train RL policies for two under-actuated robotic balancing tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "simulator_name": "custom randomized physics simulator (Lagrange-based)",
            "simulator_description": "Simulator implements the systems' dynamics via the Lagrange formalism and numerical integration, with wrappers to randomize domain parameters (masses, friction coefficients, motor/gear parameters, action delays) and to add observation noise; designed to produce multiple i.i.d. domain instances sampled from a user-specified distribution ν(ξ;φ).",
            "scientific_domain": "mechanics / robotics (rigid-body dynamics)",
            "fidelity_level": "medium-fidelity (approximate continuous dynamics using analytic Lagrangian models; includes many mechanical effects but is not a high-fidelity contact/wear model)",
            "fidelity_characteristics": "Includes: continuous-time dynamics integrated forward, masses, viscous friction, static thresholds (gear backlash modeled via deadzone), motor voltage-&gt;torque mapping, action delays, observation noise; supports sampling domain-parameter distributions. Omits or approximates: fine-grained wear/aging effects (e.g., rack/pinion wear), potentially complex contact articulation, and unmodelled hardware nonlinearities; simulator remains an approximation of real hardware.",
            "model_or_agent_name": "SPOTA-trained policies (and baseline EPOpt and PPO policies)",
            "model_description": "Model-free reinforcement learning policies (policy search meta-algorithm SPOTA using a policy optimizer subroutine, compared to EPOpt and PPO), implemented as parameterized control policies (neural-network policies implied; optimized with Adam).",
            "reasoning_task": "Control: stabilize a ball at the center of a 2-DoF plate (Ball-Balancer) and stabilize an inverted pendulum on a cart (Cart-Pole); i.e., robust control policy learning under parameter uncertainty.",
            "training_performance": "Policies trained in the randomized simulator solved the simulated tasks reliably (no exact numeric returns provided); SPOTA reported final Upper Confidence Bound on the Optimality Gap (UCBOG) values of 46.42 (Ball-Balancer) and 55.14 (Cart-Pole), indicating estimated transfer-suboptimality bounds.",
            "transfer_target": "Real-world Quanser Ball-Balancer and Quanser Cart-Pole platforms (physical systems)",
            "transfer_performance": "Domain-randomized policies (SPOTA and EPOpt) largely transferred directly to the real systems without fine-tuning; PPO trained on a single nominal model largely failed to transfer (failed in all but 2 trials on Ball-Balancer and failed on all Cart-Pole trials). Exact numeric real-world returns not provided in paper (evaluations used 40 rollouts per policy on Ball-Balancer and 10 on Cart-Pole).",
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": "No explicit multi-fidelity comparison (e.g., low vs high fidelity) was performed; instead the work compares policies trained on a single nominal model versus policies trained on randomized-domain simulators.",
            "minimal_fidelity_discussion": "The paper argues that simulator must cover the target domain within the source (randomized) distribution; it does not specify a minimum set of physical features, but emphasizes that missing real-world effects (if not covered by the randomization) can break transfer. It notes that higher simulator accuracy alone does not guarantee transfer and that domain randomization acts as a robustness/regularization mechanism.",
            "failure_cases": "Reported transfer failures on the Cart-Pole attributed to unmodeled effects (e.g., heavy wear/tear of the plastic pinion and unmodeled rack/pinion interactions) not captured by the simulator; PPO (nominal-model training) failed to transfer in most trials on both platforms.",
            "uuid": "e1300.0",
            "source_info": {
                "paper_title": "Assessing Transferability from Simulation to Reality for Reinforcement Learning",
                "publication_date_yy_mm": "2019-07"
            }
        },
        {
            "name_short": "Catapult toy simulator (Mars/Venus)",
            "name_full": "Two-domain catapult spring simulator (illustrative Bernoulli Mars/Venus example)",
            "brief_description": "A simplified analytic simulator used as an illustrative example: a linear spring catapult with domain parameters (gravity g, spring stiffness k, pre-extension x) taking one of two fixed parameter sets (Mars or Venus) drawn from a Bernoulli distribution; used to demonstrate the Simulation Optimization Bias and Optimality Gap concepts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "simulator_name": "analytical two-domain catapult simulator",
            "simulator_description": "Very low-fidelity analytic model of a vertical catapult as a linear spring launching a point mass; closed-form expression for flight height h(θ,ξ) = k(θ-x)^2/(2 m g) used to compute returns under two discrete domain parameter sets (ξ_M, ξ_V).",
            "scientific_domain": "mechanics (ballistics / simple energy conservation)",
            "fidelity_level": "low-fidelity (highly simplified analytical model)",
            "fidelity_characteristics": "Models: gravitational acceleration, spring stiffness, pre-extension, point-mass energy conservation. Ignores: air resistance, aerodynamic effects, finite size of projectile, actuator dynamics, sensor noise, and other real-world nonlinearities.",
            "model_or_agent_name": "scalar policy parameter θ (analytic parameterization)",
            "model_description": "A single-parameter policy (θ) controlling spring extension; optimization solved analytically for illustrative purposes and perturbed with Gaussian noise to model suboptimal candidate solutions.",
            "reasoning_task": "Analytical optimization over two discrete domains to illustrate Monte Carlo approximation, Simulation Optimization Bias, and Optimality Gap (i.e., choose θ to minimize expected flight height across domains).",
            "training_performance": "Analytic expressions used; results showed example numeric values in the illustrative experiment: with n=30 domains, estimated OG ≈ 4.97 m and true OG ≈ 4.23 m; simulation optimization bias b ≈ 0.911 m for the best policy computed from n=30 domains (values derived in the illustrative example).",
            "transfer_target": "Not applicable (toy analytic example used for theoretical illustration, not for physical transfer).",
            "transfer_performance": null,
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": "Not applicable; example purposefully uses severely simplified physics to illustrate theoretical concepts.",
            "minimal_fidelity_discussion": "Example deliberately demonstrates that even with simple low-fidelity models, Monte Carlo sampling and domain randomization reveal SOB and OG behavior; paper uses it to motivate the need to consider distributional sampling rather than single nominal models.",
            "failure_cases": "Not an experimental failure case; the toy example shows that with finite sampling the estimated-optimum can be biased and lead to suboptimal real-world performance if target domain distribution is not properly represented.",
            "uuid": "e1300.1",
            "source_info": {
                "paper_title": "Assessing Transferability from Simulation to Reality for Reinforcement Learning",
                "publication_date_yy_mm": "2019-07"
            }
        },
        {
            "name_short": "General-purpose physics engines (mention)",
            "name_full": "General-purpose physics engines (unnamed)",
            "brief_description": "The paper briefly mentions that simulators can be constructed either by implementing physical laws directly or by using general-purpose physics engines; no specific engine names or usages are given in this work.",
            "citation_title": "",
            "mention_or_use": "mention",
            "simulator_name": "general-purpose physics engines (unspecified)",
            "simulator_description": "Generic mention of physics engines as an alternative to implementing custom simulators; noted as possible sources of simulators but not used or specified here.",
            "scientific_domain": "general physical simulation (mechanics)",
            "fidelity_level": null,
            "fidelity_characteristics": null,
            "model_or_agent_name": null,
            "model_description": null,
            "reasoning_task": null,
            "training_performance": null,
            "transfer_target": null,
            "transfer_performance": null,
            "compares_fidelity_levels": null,
            "fidelity_comparison_results": null,
            "minimal_fidelity_discussion": "The paper remarks that even accurate simulators are approximations and that domain randomization is used as a robustness strategy; it does not detail which fidelity levels of physics engines are sufficient.",
            "failure_cases": "No specific failure cases for general-purpose engines are provided in this paper.",
            "uuid": "e1300.2",
            "source_info": {
                "paper_title": "Assessing Transferability from Simulation to Reality for Reinforcement Learning",
                "publication_date_yy_mm": "2019-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Closing the sim-to-real loop: Adapting simulation randomization with real world experience",
            "rating": 2,
            "sanitized_title": "closing_the_simtoreal_loop_adapting_simulation_randomization_with_real_world_experience"
        },
        {
            "paper_title": "Sim-to-real transfer of robotic control with dynamics randomization",
            "rating": 2,
            "sanitized_title": "simtoreal_transfer_of_robotic_control_with_dynamics_randomization"
        },
        {
            "paper_title": "Domain randomization for transferring deep neural networks from simulation to the real world",
            "rating": 2,
            "sanitized_title": "domain_randomization_for_transferring_deep_neural_networks_from_simulation_to_the_real_world"
        },
        {
            "paper_title": "Learning dexterous in-hand manipulation",
            "rating": 2,
            "sanitized_title": "learning_dexterous_inhand_manipulation"
        },
        {
            "paper_title": "Sim-to-real robot learning from pixels with progressive nets",
            "rating": 1,
            "sanitized_title": "simtoreal_robot_learning_from_pixels_with_progressive_nets"
        }
    ],
    "cost": 0.0118055,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Assessing Transferability from Simulation to Reality for Reinforcement Learning</p>
<p>Fabio Muratore 
Member, IEEEMichael Gienger 
Fellow, IEEEJan Peters 
Assessing Transferability from Simulation to Reality for Reinforcement Learning
186455EB6BCA16B8DD852099AD431B59Reinforcement LearningDomain RandomizationSim-to-Real Transfer
Learning robot control policies from physics simulations is of great interest to the robotics community as it may render the learning process faster, cheaper, and safer by alleviating the need for expensive real-world experiments.However, the direct transfer of learned behavior from simulation to reality is a major challenge.Optimizing a policy on a slightly faulty simulator can easily lead to the maximization of the 'Simulation Optimization Bias' (SOB).In this case, the optimizer exploits modeling errors of the simulator such that the resulting behavior can potentially damage the robot.We tackle this challenge by applying domain randomization, i.e., randomizing the parameters of the physics simulations during learning.We propose an algorithm called Simulation-based Policy Optimization with Transferability Assessment (SPOTA) which uses an estimator of the SOB to formulate a stopping criterion for training.The introduced estimator quantifies the over-fitting to the set of domains experienced while training.Our experimental results on two different second order nonlinear systems show that the new simulation-based policy search algorithm is able to learn a control policy exclusively from a randomized simulator, which can be applied directly to real systems without any additional training.</p>
<p>INTRODUCTION</p>
<p>E XPLORATION-BASED learning of control policies on physical systems is expensive in two ways.For one thing, real-world experiments are time-consuming and need to be executed by experts.Additionally, these experiments require expensive equipment which is subject to wear and tear.In comparison, training in simulation provides the possibility to speed up the process and save resources.A major drawback of robot learning from simulations is that a simulation-based learning algorithm is free to exploit any infeasibility during training and will utilize the flawed physics model if it yields an improvement during simulation.This exploitation capability can lead to policies that damage the robot when later deployed in the real world.The described problem is exemplary of the difficulties that occur when transferring robot control policies from simulation to reality, which have been the subject of study for the last two decades under the term 'reality gap'.Early approaches in robotics suggest using minimal simulation models and adding artificial i.i.d.noise to the system's sensors and actuators while training in simulation [1].The aim was to prevent the learner from focusing on small details, which would lead to policies with only marginal applicability.This over-fitting can be described by the Simulation Optimization Bias (SOB), which is similar to the bias of an estimator.The SOB is closely related to the Optimality Gap (OG), which has been used by the optimization community since</p>
<p>• Fabio Muratore and Jan Peters are with the Intelligent Autonomous Systems Group, Technische Universität Darmstadt, Germany.</p>
<p>Correspondence to fabio@robot-learning.de  the 1990s [2,3], but has not been transferred to robotics or Reinforcement Learning (RL), yet.</p>
<p>Deep RL algorithms recently demonstrated superhuman performance in playing games [5,6] and promising results in (simulated) robotic control tasks [7,8,9].However, when transferred to real-world robotic systems, most of these methods become less attractive due to high sample complexity and a lack of explainability of state-of-the-art deep RL algorithms.As a consequence, the research field of domain randomization has recently been gaining interest [10,11,12,13,14,15,16,17].This class of approaches promises to transfer control policies learned in simulation (source domain) to the real world (target domain) by randomizing the simulator's parameters (e.g., masses, extents, or friction coefficients) and hence train from a set of models instead of just one nominal model.Further motivation to investigate domain randomization is given by the recent successes in robotic sim-to-real scenarios, such as the inhand manipulation of a cube [18], swinging a peg in a hole, or opening a drawer [17].The idea of randomizing the simulator's parameters is driven by the fact that the arXiv:1907.04685v2[cs.RO] 21 Oct 2019 corresponding true parameters of the target domain are unknown.However, instead of relying on an accurate estimation of one fixed parameter set, we take a Bayesian point of view and assume that each parameter is drawn from an unknown underlying distribution.Thereby, the expected effect is an increase in robustness of the learned policy when applied to a different domain.Throughout this paper, we use the term robustness to describe a policy's ability to maintain its performance under model uncertainties.In that sense, a robust control policy is more likely to overcome the reality gap.</p>
<p>Looking at the bigger picture, model-based control only considers a system's nominal dynamics parameter values, while robust control minimizes a system's sensitivity with respect to bounded model uncertainties, thus focuses the worst-case.In contrast to these methods, domain randomization takes the whole range of parameter values into account.</p>
<p>Contributions: we advance the state-of-the-art by 1) introducing a measure for the transferability of a solution, i.e., a control policy, from a set of source distributions to a different target domain from the same distribution, 2) designing an algorithm which, based on this measure, is able to transfer control policies from simulation to reality without any real-world data, and 3) validating the approach by conducting two sim-to-real experiments on under-actuated nonlinear systems.The remainder of this paper is organized as follows: we explain the necessary fundamentals (Section 2) for the proposed algorithm (Section 3).In particular, we derive the Simulation Optimization Bias (SOB) and the Optimality Gap (OG).After validating the proposed method in simulation, we evaluate it experimentally (Section 4).Next, the connection to related work is discussed (Section 5).Finally, we conclude and discuss possible future research directions (Section 6).</p>
<p>PROBLEM STATEMENT AND NOTATION</p>
<p>Optimizing policies for Markov Decision Processes (MDPs) with unknown dynamics is generally a hard problem (Section 2.1).Specifically, this problem is hard due to the simulation optimization bias (Section 2.2), which is related to the optimality gap (Section 2.3).We derive an upper bound on the optimality gap, show its monotonic decrease with increasing number of samples from the random variable.Moreover, we clarify the relationship between the simulation optimization bias and the optimality gap (Section 2.4).In what follows, we build upon the results of [2,3].</p>
<p>Markov Decision Process</p>
<p>Consider a time-discrete dynamical system
s t+1 ∼ P ξ ( s t+1 | s t , a t , ξ) , s 0 ∼ µ 0,ξ ( s 0 | ξ), a t ∼ π( a t | s t ; θ) , ξ ∼ ν(ξ; φ) ,
with the continuous state s t ∈ S ξ ⊆ R ns , and continuous action a t ∈ A ξ ⊆ R na at time step t.The environment, also called domain, is instantiated through its parameters ξ ∈ R n ξ (e.g., masses, friction coefficients, or parameter θ θ θ n J(θ) Ĵn (θ) return J SOB Figure 2: Simulation Optimization Bias (SOB) between the true optimum θ and the sample-based optimum θ n .The shaded region visualizes the standard deviation around J(θ), and Ĵn (θ) is determined by a particular set of n sampled domain parameters.time delays), which are assumed to be random variables distributed according to the probability distribution ν : R n ξ → R + parametrized by φ.These parameters determine the transition probability density function P ξ : S ξ × A ξ × S ξ → R + that describes the system's stochastic dynamics.The initial state s 0 is drawn from the start state distribution µ 0,ξ : S ξ → R + .Together with the reward function r : S ξ × A ξ → R, and the temporal discount factor γ ∈ [0, 1], the system forms a MDP described by the tuple M ξ = S ξ , A ξ , P ξ , µ 0,ξ , r, γ .</p>
<p>The goal of a Reinforcement Learning (RL) agent is to maximize the expected (discounted) return, a numeric scoring function which measures the policy's performance.The expected discounted return of a stochastic domainindependent policy π( a t | s t ; θ), characterized by its parameters θ ∈ Θ ⊆ R n θ , is defined as
J(θ, ξ, s 0 ) = E τ T −1 t=0 γ t r(s t , a t ) θ, ξ, s 0 .
While learning from trial and error, the agent adapts its policy parameters.The resulting state-action-reward tuples are collected in trajectories, a.k.a.rollouts, τ = {s t , a t , r t } T −1 t=0 , with r t = r(s t , a t ).To keep the notation concise, we omit the dependency on the initial state s 0 .</p>
<p>Simulation Optimization Bias (SOB)</p>
<p>Augmenting the standard RL setting with the concept of domain randomization, i.e. maximizing the expectation of the expected return over all (feasible) realizations of the source domain, leads to the score
J(θ) = E ξ [J(θ, ξ)]
that quantifies how well the policy is expected to perform over an infinite set of variations of the nominal domain M ξ.When training exclusively in simulation, the true physics model is unknown and the true J(θ, ξ) is thus inaccessible.Instead, we maximize the estimated expected return using a randomized physics simulator.Thereby, we update the policy parameters θ with a policy optimization algorithm based on samples.The inevitable imperfections of physics simulations will automatically be exploited by any Table 1: Definition and of the expectation of the expected (discounted) return, the Simulation Optimization Bias (SOB), the Optimality Gap (OG), and its estimation.All approximations are based on n domains.</p>
<p>Name</p>
<p>Definition Property estimated expectation of the expected return
Ĵn(θ) = 1 n n i=1 J(θ, ξ i ) E ξ Ĵn(θ) = J(θ) simulation optimization bias b Ĵn(θ n ) = E ξ max θ ∈Θ Ĵn( θ) − max θ ∈Θ E ξ J(θ, ξ) b Ĵn(θ n ) ≥ 0 optimality gap at solution θ c G(θ c ) = max θ ∈Θ E ξ [J(θ, ξ)] − E ξ [J(θ c , ξ)] G(θ c ) ≥ 0 estimated optimality gap at solution θ c Ĝn (θ c ) = max θ ∈Θ Ĵn(θ) − Ĵn(θ c ) Ĝn (θ c ) ≥ G(θ c )
optimization method to achieve a 'virtual' improvement, i.e., an increase of J(θ), in simulation.To formulate this undesirable behavior, we frame the standard RL problem as a Stochastic Program (SP)
J(θ ) = max θ ∈Θ E ξ [J(θ, ξ)] = max θ ∈Θ J(θ) ,
with the optimal solution θ = arg max θ ∈Θ J(θ).</p>
<p>The SP above can be approximated using n domains
Ĵn (θ n ) = max θ ∈Θ Ĵn (θ) = max θ ∈Θ 1 n n i=1 J(θ, ξ i ) ,(1)
where the expectation is replaced by the Monte-Carlo estimator over the samples ξ 1 , . . ., ξ n , and θ n = arg max θ ∈Θ Ĵn (θ) is the solution to the approximated SP.Note that the expectations in (2.2, 1) both jointly depend on ξ and s 0 , i.e. both random varaibles are integrated out, but the dependency on s 0 is omitted as stated before.Sample-based optimization is guaranteed to be optimistically biased if there are errors in the domain parameter estimate, even if these errors are unbiased [2].Since the proposed method randomizes the domain parameters ξ, this assumption is guaranteed to hold.Using Jensen's inequality, we can show that the Simulation Optimization Bias (SOB)
b Ĵn (θ n ) = E ξ max θ ∈Θ Ĵn ( θ) sample optimum − max θ ∈Θ E ξ J(θ, ξ) true optimum ≥ 0. (2)
is always positive, i.e. the policy's performance in the target domain is systematically overestimated.A visualization of the SOB is depicted in Figure 2.</p>
<p>Optimality Gap (OG)</p>
<p>Intuitively, we want to minimize the SOB in order to achieve the highest transferability of the policy.Since computing the SOB (2) is intractable, the approach presented in this paper is to approximate the Optimality Gap (OG), which relates to the SOB as explained in the Section 2.4.</p>
<p>The OG at the solution candidate θ c is defined as
G(θ c ) = J(θ ) − J(θ c ) ≥ 0,(3)
where J(θ ) = max θ ∈Θ E ξ [J(θ, ξ)] is the SP's optimal objective function value and J(θ c ) = E ξ [J(θ c , ξ)] is the SP's objective function evaluated at the candidate solution [3].Thus, G(θ c ) expresses the difference in performance between the optimal policy and the candidate solution at hand.Unfortunately, computing the expectation over infinitely many domains in (3) is intractable.However, we can estimate G(θ c ) from samples.</p>
<p>Estimation of the Optimality Gap</p>
<p>For an unbiased estimator Ĵn (θ), e.g. a sample average with i.i.d.samples, he have
E ξ Ĵn (θ) = E ξ [J(θ, ξ)] = J(θ) .(4)
Inserting (4) into the first term of (3) yields
G(θ c ) = max θ ∈Θ E ξ Ĵn (θ) − E ξ [J(θ c , ξ)] ≤ E ξ max θ ∈Θ Ĵn (θ) − E ξ <a href="5">J(θ c , ξ)</a>
as an upper bound on the OG.To compute this upper bound, we use the law of large numbers for the first term and replace the second expectation in (5) with the sample average
G(θ c ) ≤ max θ ∈Θ Ĵn (θ) − Ĵn (θ c ) = Ĝn (θ c ) ,(6)
where Ĝn (θ c ) ≥ 0 holds. 1 Averaging over a finite set of domains allows for the utilization of an estimated upper bound of the OG as the convergence criterion for the policy search meta-algorithm introduced in Section 3.</p>
<p>Decrease of the Estimated Optimality Gap</p>
<p>The OG decreases in expectation with increasing sample size of the domain parameters ξ.The expectation over ξ of the minuend in (6) estimated from n + 1 i.i.d.samples is
E ξ Ĵn+1 θ n+1 = E ξ max θ ∈Θ 1 n + 1 n+1 i=1 J(θ, ξ i ) = E ξ   max θ ∈Θ 1 n + 1 n+1 i=1 1 n n+1 j=1,j =i J(θ, ξ j )   ≤ E ξ   1 n + 1 n+1 i=1 max θ ∈Θ 1 n n+1 j=1,j =i J(θ, ξ j )   = E ξ Ĵn (θ n ) .(7)
1</p>
<p>This result is consistent with Theorem 1 and Equation ( 9) in [3] as well as the "type A error" in [2].</p>
<p>Taking the expectation of the OG estimated from n + 1 samples Ĝn+1 (θ c ) and then plugging in the upper bound from (7), we obtain the upper bound
E ξ Ĝn+1 (θ c ) = E ξ max θ ∈Θ Ĵn+1 (θ) − Ĵn (θ c ) ≤ E ξ max θ ∈Θ Ĵn (θ) − Ĵn (θ c ) = E ξ Ĝn (θ c ) ,
which shows that the estimator of the OG in expectation monotonically decreases with increasing sample size. 2</p>
<p>Connection Between the SOB and the OG</p>
<p>The SOB can be expressed as the expectation of the difference between the approximated OG and the true OG.</p>
<p>Starting from the formulation of the approximated OG in (6), we can take the expectation over the domains on both sides of the inequality and rearrange to
E ξ Ĝn (θ c ) − G(θ c ) ≥ 0.
Using the definitions of Ĝn (θ c ) and G(θ c ) from Table 1, the equation above can be rewritten as
E ξ max θ ∈Θ Ĵn (θ) − E ξ Ĵn (θ c ) − max θ ∈Θ E ξ [J(θ, ξ)] + E ξ J(θ c , ξ) ≥ 0.(8)
Since Ĵn (θ) is an unbiased estimator of J(θ), we have
E ξ Ĵn (θ c ) = E ξ [J(θ c , ξ)] = J(θ c ) .
Hence, the left hand side of ( 8) becomes
E ξ max θ ∈Θ Ĵn (θ) − max θ ∈Θ E ξ J(θ, ξ) = b Ĵn (θ n ) ,
which is equal to the SOB defined in (2).Thus, the SOB is the difference between the expectation over all domains of the estimated OG Ĝn (θ c ) and the true OG G(θ c ) at the solution candidate.Therefore, reducing the estimated OG leads to reducing the SOB.</p>
<p>An Illustrative Example</p>
<p>Imagine we were placed randomly in an environment either on Mars ξ M or on Venus ξ V , governed by the distribution ξ ∼ ν(ξ; φ).On both planets we are in a catapult about to be shot into the sky exactly vertical.The only thing we can do is to manipulate the catapult, modeled as a linear spring, according to the policy π(θ), i.e. changing the springs extension.Our goal is to minimize the maximum height of the expected flight trajectory E ξ [h(θ, ξ)] derived from the conservation of energy
h(θ, ξ i ) = k i (θ − x i ) 2 2mg i ,
with mass m, and domain parameters ξ i = {g i , k i , x i } consisting of the gravity acceleration constant, the catapult's spring stiffness, and the catapult's spring pre-extension.</p>
<p>2</p>
<p>This result is consistent with Theorem 2 in [3].</p>
<p>The domain parameters are the only quantities specific to Mars and Venus.In this simplified example, we assume that the domain parameters are not drawn from individual distributions, but that there are two sets of domain parameters ξ M and ξ V which are drawn from a Bernoulli distribution B ξ φ where φ is the probability of drawing ξ
V . Since minimizing E ξ [h(θ, ξ)] is identical to maximizing its negative value J(θ, ξ) := −E ξ [h(θ, ξ)],
we rewrite the problem as
J(θ ) = max θ∈Θ E ξ [J(θ, ξ)] .
Assume we experienced this situation n times and want to find the policy parameter maximizing the objective above without knowing on which planet we are (ı.e., independent of ξ).Thus, we approximate J(θ ) by
Ĵn (θ n ) = max θ∈Θ 1 n n i=1 J(θ, ξ i ) .
In this Bernoulli experiment, the return of a policy π(θ) fully determined by θ estimates to
Ĵn (θ) = n M n J(θ, ξ M ) proportion Mars + n V n J(θ, ξ V ) proportion Venus . (9)
The optimal policy given the n domains fulfills the necessary condition
0 = ∇ θ Ĵn (θ n ) = − n M n k M (θ n − x M ) mg M − n V n k V (θ n − x V ) mg V .
Solving for the optimal policy parameter yields
θ n = x M n M k M g V + x V n V k V g M n M k M g V + n V k V g M = x M c M + x V c V c M + c V ,(10)
with the (mixed-domain (10) into (9) gives the optimal return value for n samples
) constants c M = n M k M g V and c V = n V k V g M . InsertingĴn (θ ) = − n M k M 2nmg M x V c V − x M c V c M + c V 2 − n V k V 2nmg V x M c M − x V c M c M + c V 2 .
Given the domain parameters in Table 2 of Appendix B, we optimize our catapult manipulation policy.This is done in simulation, since real-world trials (being shot with a catapult) would be very costly.Finding the optimal policy in simulation means solving the approximated SP (1), whose optimal solution is denoted by θ n .We assume that the (stochastic) optimization algorithm outputs a suboptimal solution θ c .In order to model this property, a policy parameter is sampled in the vicinity of the optimum θ c ∼ N θ θ n ; σ 2 θ with σ θ = 0.15.During the entire process, the true optimal policy parameter θ will remain unknown.However, since this simplified example models the domain parameters to be one of two fixed sets (ξ M or ξ V ), θ can be computed analogously to (10).</p>
<p>The Figures 3 and 4 visualize the evolution of the approximated SP with increasing number of domains n. Key observations are that the objective function value at the candidate solution Ĵn (θ c ) is less than at the sample-based Figure 3: The estimated expected return evaluated using the optimal solution for a set of n domains Ĵn (θ n ), the candidate solution Ĵn (θ c ), as well as the true optimal solution Ĵn (θ ).</p>
<p>Note that Ĵn (θ n ) &gt; Ĵn (θ c ) holds for every instance of the 100 random seeds, even if the standard deviation areas overlap.The shaded areas show ±1 standard deviation.</p>
<p>optimum Ĵn (θ n ) (Figure 3), and that with increasing number of domains the SOB b[ Ĵ(θ n )] decreases monotonically while the estimated OG Ĝn (θ c ) only decreases in expectation (Figure 4).When optimizing over n = 30 random domains in simulation, we yield a policy which leads to a G n (θ c ) ≈ 4.23 m higher (worse) shot compared to the best policy computed from an infinite set of domains and evaluated on this infinite set of domains, and a Ĝn (θ c ) ≈ 4.97 m higher (worse) shot compared to the best policy computed from a set of n = 30 domains and evaluated on the same finite set of domains.Furthermore, we can say that executing the best policy computed from a set of n = 30 domains will in reality result in a b[ Ĵ(θ n )] ≈ 0.911 m higher (worse) shot.</p>
<p>SIMULATION-BASED POLICY OPTIMIZATION WITH TRANSFERABILITY ASSESSMENT</p>
<p>We introduce Simulation-based Policy Optimization with Transferability Assessment (SPOTA) [19], a policy search meta-algorithm which yields a policy that is able to directly transfer from a set of source domains to an unseen target domain.The goal of SPOTA is not only to maximize the expected discounted return under the influence of randomized physics simulations J(θ), but also to provide an approximate probabilistic guarantee on the suboptimality in terms of expected discounted return when applying the obtained policy to a different domain.The key novelty in SPOTA is the utilization of an Upper Confidence Bound on the Optimality Gap (UCBOG) as a stopping criterion for the training procedure of the RL agent.One interpretation of (source) domain randomization is to see it as a form of uncertainty representation.If a control policy is trained successfully on multiple variations of the scenario, i.e., a set of models, it is legitimate to assume that this policy will be able to handle modeling errors better than policies that have only been trained on the nominal model ξ = E ν [ξ].With this rationale in mind, we propose the SPOTA procedure, summarized in Algorithm 1.</p>
<p>SPOTA performs a repetitive comparison of solution candidates against reference solutions in domains that are  in the references' training set but unknown to the candidates.As inputs, we assume a probability distribution over the domain parameters ν(ξ; φ), a policy optimization subroutine PolOpt, the number of candidate and reference domains n c and n r in conjunction with a nondecreasing sequence NonDecrSeq (e.g., n k+1 = 2n k ), the number of reference solutions n G , the number of rollouts used for each OG estimate n J , the number of bootstrap samples n b , the confidence level (1−α) used for bootstrapping, and the threshold of trust β determining the stopping condition.SPOTA consists of four blocks: finding a candidate solution, finding multiple reference solutions, comparing the candidate against the reference solutions, and assessing the candidate solution quality.Candidate Solutions are randomly initialized and optimized based on a set of n c source domains (Lines 3-4).Practically, the locally optimal policy parameters are optimized on the approximated SP (1).
G(θ c ) Ĝn (θ c ) b[J n (θ n )]
Reference Solutions are gathered n G times by solving the same approximated SP as for the candidate but with different realizations of the random variable ξ (Lines 6-8).These n G non-convex optimization processes all use the same candidate solution θ nc as initial guess.</p>
<p>Solution Comparison is done by evaluating each reference solution θ k nr with k = 1, . . ., n G against the candidate solution θ nc for each realization of the random variable ξ k i with i = 1, . . ., n r on which the reference solution has been trained.In this step, the performances per domain Ĵn J θ nc , ξ k i and Ĵn J θ k nr , ξ k i are estimated from n J Monte-Carlo simulations with synchronized random seeds (Lines 10-13).Thereby, both solutions are evaluated using the same random initial states and observation noise.Due to the potential suboptimality of the reference solutions, the resulting difference in performance  Utilizing the definition of the OG in (6) for SPOTA demands for globally optimal reference solutions.Due to the nonconvexity of the introduced RL problem the obtained solutions by the optimizer only are locally optimal.In order to alleviate this dilemma, we perform an outlier rejection routine (Lines [16][17][18][19][20][21][22].As a first attempt, all other reference solutions are evaluated for the current domain i.If a solution with higher performance was found, it replaces the current reference solution k for this domain.If all reference solutions are worse than the candidate, the value is clipped to the theoretical minimum (zero).Solution Quality is assessed by constructing a (1−α)level confidence interval 0, ḠU nr θ nc for the estimated OG at θ nc .While the lower bound is fixed to the theoretical minimum, the Upper Confidence Bound on the Optimality Gap (UCBOG) is computed using the statistical bootstrap method [21].We denote bootstrapped quantities with the superscript B instead of the common asterisk, to avoid a notation conflict with the optimal solutions.There are multiple ways to yield a confidence interval by applying the bootstrap [22].Here, the 'basic' nonparametric method was chosen, since the aforementioned potential clipping changes the distribution of the samples and hence a method relying on the estimation of population parameters such as the standard error is inappropriate.The solution comparison yields a set of n G n r samples of the approximated OG G = { Ĝ1 nr,1 θ nc , . . ., Ĝn G nr,nr θ nc }.Through uniform random sampling with replacement from G , we generate n b bootstrap samples G B 1 , . . ., G B n b .Thus, for our statistic of interest, the mean estimated OG Ḡnr θ nc , the UCBOG becomes</p>
<p>Ĝk
nr,i θ nc = Ĵn J θ k nr , ξ k i − Ĵn J θ nc , ξ k i (11)ḠU nr θ nc = 2 Ḡnr θ nc − Q α ḠB nr θ nc ,(12)
where Ḡnr θ nc is the mean over all (nonnegative) samples from the empirical distribution, and Q α ḠB nr θ nc is the α-th quantile of the means calculated for each of the n b bootstrap samples (Lines 25-27).Consequently, the true OG is lower than the obtained one-sided confidence interval with the approximate probability of (1−α), i.e.,
P G θ nc ≤ ḠU nr θ nc ≈ 1 − α,
which is analogous to (4) in [20].Finally, the sample sizes n r and n c of the next epoch are set according to the nondecreasing sequence.The procedure stops if the UCBOG at θ nc is less than or equal to the specified threshold of trust β.Fulfilling this condition, the candidate solution at hand does not lose more than β in terms of performance with approximate probability (1−α), when it is applied to a different domain sampled from the same distribution.Intuitively, the question arises why we do not use all samples for training a single policy and thus most likely yield a more robust result.To answer this question we want to point out that the key difference of SPOTA to the related methods is the assessment of the solution's transferability to different domains.While the approaches reviewed in Section 5 train one policy until convergence (e.g., for a fixed number of steps), SPOTA repeats this process and suggests new policies as long as the UCBOG is above a specified threshold.Thereby, SPOTA only uses 1/(1 + n G n/n c ) of the total samples to learn the candidate solution, i.e., the policy which will be deployed.If we would use all samples for training, hence not learn any reference solutions, we would not be able to estimate the OG and therefore lose the main feature of SPOTA.</p>
<p>EXPERIMENTS</p>
<p>We evaluate SPOTA on two sim-to-real tasks pictured in Figure 1, the Ball-Balancer and the Cart-Pole.The policies obtained by SPOTA are compared against Ensemble Policy Optimization (EPOpt), and (plain) Proximal Policy Optimization (PPO) policies.The goal of the conducted experiments is twofold.First, we want to investigate the applicability of the UCBOG as a quantitative measure of a policy's transferability.Second, we aim to show that domain randomization enables the sim-to-real transfer of control policies learned by RL algorithms, while methods which only learn from the nominal domain fail to transfer.</p>
<p>Modeling and Setup Description</p>
<p>Both platforms can be classified as nonlinear under-actuated balancing problems with continuous state and action spaces.The Ball-Balancer's task is to stabilize the randomly initialized ball at the plate's center.Given measurements and their first derivatives (obtained by temporal filtering) of the motor shaft angles as well as the ball's position relative to the plate's center, the agent controls two servo motors via voltage commands.The rotation of the motor shafts leads, through a kinematic chain, to a change in the plate angles.Finally, the plate's deflection gets the ball rolling.The Ball-Balancer has an 8D state and a 2D action space.Similarly, the Cart-Pole's task is to stabilize a pole in the upright position by controlling the cart.Based on measurements and their first derivatives (obtained by temporal filtering) of the pole's rotation angle as well as the cart position relative to the rail's center, the agent controls the servo motor driving the cart by sending voltage commands.Accelerating the cart makes the pole rotate around an axis perpendicular to the cart's direction.The Cart-Pole has a 4D state and a 1D action space.Details on the dynamics of both systems, the reward functions, as well as listings of the domain parameters is given in Appendix A. The nominal models are based on the domain parameter values provided by the manufacturer.</p>
<p>In this paper, both systems have been modeled using the Lagrange formalism and the resulting differential equations are integrated forward in time to simulate the systems.The associated domain parameters are drawn from a probability distribution ξ ∼ ν(ξ; φ), parameterized by φ (e.g., mean, variance).Since randomizing a simulator's physics parameters is not possible right away, so we developed custom a framework to combine RL and domain randomization.Essentially, the base environment is modified by wrappers which, e.g., vary the mass or delay the actions.</p>
<p>Experiments Description</p>
<p>The experiments are split into two parts.At first, we examine the evolution of the UCBOG during training (Section 4.3).Next, we compare the transferability of the obtained policies across different realizations of the domain, i.e., simulator (Section 4.3).Finally, we evaluate the policies on the real-world platforms (Section 4.4).</p>
<p>For the experiments on the real Ball-Balancer, we choose 8 initial ball positions equidistant from the plate's center and place the ball at these points using a PD controller.As soon as a specified accuracy is reached, the evaluated policy is activated for 2000 time steps, i.e., 4 seconds.All experiments on the real Carl-pole start with the cart centered on the rail and the pendulum pole hanging down.After calibration, the pendulum is swung up using an energy-based controller.When the system's state is within a specified threshold, evaluated policy is executed for 4000 time steps, i.e., 8 seconds.</p>
<p>All policies have been trained in simulation with observation noise to mimic the noisy sensors.To focus on the domain parameters' influence, we executed the simto-sim experiments without observation noise.The policy update at the core of SPOTA, EPOpt, and PPO is done by the Adam optimizer [23].In the sim-to-sim experiments, the rewards are computed from the ideal states coming from the simulator, while for the sim-to-real experiments the rewards are calculated from the sensor measurements and their time derivatives.The hyper-parameters chosen for the experiments can be found in the Appendix B.</p>
<p>Sim-to-Sim Results</p>
<p>The UCBOG value (12) at each SPOTA iteration depends on several hyper-parameters such as the current number of domains and reference solutions, or the quality of the current iteration's solution candidate.Figure 5 displays the descent of the UCBOG as well as the growth of the number of domains with the increasing iteration count of SPOTA.As described in Section 2.3.2, the OG and thus the UCBOG only decreases in expectation.Therefore, it can happen that for a specific training run the UCBOG increases from one iteration to the next.Moreover, we observed that the proportion of negative OG estimates (11) increases with progressing iteration count.This observation can be explained by the fact that SPOTA increases the number of domains used during training.Hence the set's empirical mean approximates the domain parameter distribution ν(ξ; φ) progressively better, i.e., the candidate and reference solution become more similar.Note, that due to the computational complexity of SPOTA we decided to combine results from experiments with different hyper-parameters in Figure 5.</p>
<p>To estimate the robustness w.r.t.model parameter uncertainties, we evaluate policies trained by SPOTA, EPOpt, and PPO under on multiple simulator instances, varying only one domain parameter.The policies' sensitivity to different parameter values is displayed in the Figure 6.From the Figures 6a to 6c we can see that the policies learned using (plain) PPO are less robust to changes in the domain parameter values.In contrast, SPOTA and EPOpt are able to maintain their level of performance across a wider range of parameter values.The Figure 6d shows the case of a domain parameter to which all policies are equally insensitive.We can also see that EPOpt trades off performance in the nominal domains for performance in more challenging domains (e.g., low friction).This behavior is a consequence of its CVaR-based objective function [11].Moreover, the results show a higher variance for the PPO policy than for the others.From this, we conclude that domain randomization also acts as regularization mechanism.The final UCBOG value of the evaluated SPOTA policies was 46.42 for the Ball-Balancer and 55.14 for the Cart-Pole.Note, that the UCBOG can not be directly observed from the curves in Figure 6, since the UCBOG reflects the gap in performance between the best policy for a specific simulator instance and the candidate policy, whereas the Figure 6 only shows the candidates' performances.</p>
<p>Sim-to-Real Results</p>
<p>When transferring the policies from simulation to reality without any fine-tuning, we obtain the results reported in Figure 7.The approaches using domain randomization are in most cases able to directly transfer from simulation to reality.In contrast, the policies trained on a singular nominal model using failed to transfer in all but 2 trials on the Ball-Balancer as well as in all trials on the Cart-Pole, even though these policies solved the simulated environments.</p>
<p>Regarding the Ball-Balancer, one explanation why the reported PPO policy did not transfer to the real platform could be the value of the viscous friction coefficient (Figure 6b).A possible test for this hypothesis would be to train multiple policies on a range of nominal models with altered viscous friction parameter value, and if these policies still do not transfer, examine the next domain parameter.However, this procedure is redundant and can quickly become prohibitively time-intensive.Concerning the experiments on the Cart-Pole, we observe a larger reality-gap for all policies.We believe that this discrepancy is caused by unmodeled effects between the rack and the cart's pinion (e.g., the heavy wear and tear of the pinon made out of plastic).Moreover, the variance of the returns is significantly higher.This increase can be largely explained by the variance in the initial states caused by the pre-defined swing-up controller.</p>
<p>A video of the SPOTA policy's sim-to-real transfer on both platforms can be found at https://www.ias.informatik.tu-darmstadt.de/Team/FabioMuratore.</p>
<p>Limitations of the Presented Method</p>
<p>The computation of the UCBOG (12), and hence the estimation of the SOB, relies on the relative quality of the candidate and the reference solutions.Therefore, the most notable limitation of the presented method is the optimizer's ability to reliably solve the SP (1).Since we are interested in the general setting of learning a control policy from a blackbox simulator, we chose a model-free RL algorithm.These kind of algorithms can not guarantee to find the globally optimal, or loosely speaking a very good, solution.One way to alleviate this problem is to compute the reference policies from a single domain using methods from control engineering, e.g. a LQR.However, this solution would require an analytic model of the system and a specific type of reward function to preserve comparability between the solutions, e.g.quadratic functions in case of the LQR.</p>
<p>Another limitation of SPOTA is the increased hyperparameter space which is a direct consequence from the employed (static) domain randomization.In combination with the fact that SPOTA is solving the underlying RL task (1 + n G )n iter times, the procedure becomes computationally expensive.One can counter this problem by parallelizing the computation of the reference policies as well as the hyperparameter search.Both are embarrassingly parallel.</p>
<p>Moreover, SPOTA does not considers uncertainty in the parametrization of the domain parameter distribution ν(ξ; φ).One possibility to tackle this potential deficiency is to adapt these distributions, as for example done in [17].Moving from parametric to nonparametric models of the domain parameter distribution is easily possible since SPOTA only requires to sample from them.</p>
<p>Finally, to estimate the SOB, SPOTA assumes that the target domain is covered by the source domain distribution, which can not be guaranteed if the target is a real-world system.However, in the current state-of-the-art there is no way to estimate a policy's transferability to a domain from an unknown distribution.Due to mentioned assumption, SPOTA's transferability assessment strongly depends on the simulator's ability to model the real world.</p>
<p>RELATED WORK</p>
<p>In the following, we review excerpts of the literature regarding the transfer of control policies from simulation to reality, the concept of the optimality gap in Stochastic Programs (SPs), and the application of randomized physics simulations.This paper is a substantial extension of our previous work [19], adding the derivation of the SOB from the OG (Section 2.2 to 2.4), an outlier rejection component (Algorithm 1), and the method's first real-world verification using two experiments (Section 4.4).</p>
<p>Key publications on the Optimality Gap</p>
<p>Hobbs and Hepenstal [2] proved for linear programs that optimization is optimistically biased, given that there are errors in estimating the objective function coefficients.Furthermore, they demonstrated the "optimistic bias" of a nonlinear program, and mentioned the effect of errors on the parameters of linear constraints.The optimization problem introduced in Section 3 belongs to the class of SPs for which the assumption required in [2] are guaranteed to hold.The most common approaches to solve convex SPs are sample average approximation methods, including: (i) the Multiple Replications Procedure and its derivatives [3,20] which assess a solution's quality by comparing with sampled alternative solutions, and (ii) Retrospective Approximation [24,25] which iteratively improved the solution by lowering the error tolerance.Bastin et al. [26] extended the existing convergence guarantees from convex to non-convex SPs, showing almost sure convergence of the minimizers.</p>
<p>Prior work on the Reality Gap</p>
<p>Physics simulations have already been used successfully in robot learning.Traditionally, simulators are operating on a single nominal model, which makes the direct transfer of policies from simulation to reality highly vulnerable to model uncertainties and biases.Thus, model-based control in most cases relies on fine-tuned dynamics models.The mismatch between the simulated and the real world has been addressed by robotics researchers from different viewpoints.Prominent examples are: 1) adding i.i.d.noise to the observations and actions in order to mimic real-world sensor and actuator behavior [1], 2) repeating model generation and selection depending on the short-term state-action history [27], 3) learning a transferability function which maps solutions to a score that quantifies how well the simulation matches the reality [28], 4) adapting the simulator to better match the observed real-world data [17,29], 5) randomizing the physics simulation's parameters, and 6) applying adversarial perturbations to the system, where the last two approaches are particularly related and discussed in the Sections 5.4 and 5.5.The fourth point comprises methods based on system identification, which conceptually differ from the presented method since these seek to find the simulation closest to reality, e.g.minimal prediction error.A recent example in the field of robotics is the work by Hanna and Stone [29], where an action transformation is learned such that the transformed actions applied in simulation have the same effects as applying the original actions had on the real system.</p>
<p>Required Randomized Simulators</p>
<p>Simulators can be obtained by implementing a set of physical laws or by using general purpose physics engines.The associated physics parameters can be estimated by system identification, which involves executing control policies on the physical platform [30].Additionally, using the Gauss-Markov theorem one could also compute the parameters' covariance and hence construct a normal distribution for each domain parameter.Alternatively, the system dynamics can be captured using nonparametric methods like Gaussian processes [31].It is important to keep in mind, that even if the selected procedure yields a very accurate model parameter estimate, simulators are nevertheless just approximations of the real world and are thus always flawed.</p>
<p>As done in [10,11,14,15,16] we use the domain parameter distribution as a prior which ensures the physical plausibility of each parameter.Note that specifying this distribution in the current state-of-the-art requires the researcher to make design decisions.Chebotar et al. [17] presented a promising method which adapts the domain parameter distribution using real-world data in the loop.The main advantage is that this approach alleviates the need for hand-tuning the distributions of the domain parameters, which is currently a significant part of the hyper-parameter search.However, the initial distribution still demands for design decisions.On the downside, the adaptation requires data from the real robot which is considered significantly more expensive to obtain.Since we aim for performing a sim-to-real transfer without using any real-world data, the introduced method only samples from static probability distributions.</p>
<p>Background on Domain Randomization</p>
<p>There is a large consensus that further increasing the simulator's accuracy alone will not bridge the reality gap.Instead, the idea of domain randomization has recently gained momentum.The common characteristic of such approaches is the perturbation of the parameters which determine the physics simulator and the state estimation, including but not limited to the system dynamics.While the idea of randomizing the sensors and actuators dates back to at least 1995 [1], the systematic analysis of perturbed simulations in robot RL is a relatively new research direction.</p>
<p>Wang, Fleet, and Hertzmann [32] proposed sampling initial states, external disturbances, goals, as well as actuator noise from probability distributions and learned walking policies in simulation.Regarding robot RL, recent domain randomization methods focus on perturbing the parameters defining the system dynamics.Approaches cover: (i) trajectory optimization on finite model-ensembles [10] (ii) learning a feedforward NN policy for an under-actuated problem [33], (iii) using a risk-averse objective function [11], (iv) employing recurrent NN policies trained with experience replay [15], and (v) optimizing a policy from samples of a model randomly chosen from a set which is repeatedly fitted to real-world data [34].From the listed approaches [10,33,15] were able to cross the reality gap without acquiring samples from the real world.</p>
<p>Moreover, there is a significant amount of work applying domain randomization to computer vision.One example is the work by Tobin et al. [35] where an object detector for robot grasping is trained using multiple variants of the environment and applied to the real world.The approach presented by Pinto et al. [14] combines the concepts of randomized environments and actor-critic training, enabling the direct sim-to-real transfer of the abilities to pick, push, or move objects.Sadeghi and Levine [36] achieved the sim-to-real transfer by learning to fly a drone in visually randomized environments.The resulting deep NN policy was able to map from monocular images to normalized 3D drone velocities.In [37], a deep NN was trained to manipulate tissue using randomized vision data and the full state information.By combing generative adversarial networks and domain randomization, Bousmalis et al. [38] greatly reduced the number of necessary real-world samples for learning a robotic grasping task.</p>
<p>Domain randomization is also related to multi-task learning in the sense that one can view every instance of the randomized source domain as a separate task.In contrast to multi-task learning approaches as presented in [39,40], a policy learned with SPOTA does not condition on the task.Thus, during execution there is no need to infer the task i.e. domain parameters.</p>
<p>Randomization Trough Adversarial Perturbations</p>
<p>Another method for learning robust policies in simulation is to apply adversarial disturbances to the training process.Mandlekar et al. [12] proposed physically plausible perturbations by randomly deciding when to add a rescaled gradient of the expected return.Pinto et al. [13] introduced the idea of a second agent whose goal is to hinder the first agent from fulfilling its task.Both agents are trained simultaneously and make up a zero-sum game.In general, adversarial approaches may provide a particularly robust policy.However, without any further restrictions, it is always possible create scenarios in which the protagonist agent can never win, i.e., the policy will not learn the task.</p>
<p>CONCLUSION</p>
<p>We proposed a novel measure of the Simulation Optimization Bias (SOB) for quantifying the transferability of an arbitrary policy learned from a randomized source domain to an unknown target domain from the same domain parameter distribution.Based on this measure of the SOB, we developed a policy search meta-algorithm called Simulation-based Policy Optimization with Transferability Assessment (SPOTA).This gist of SPOTA is to iteratively increase the number of domains and thereby the sample size per iteration until an approximate probabilistic guarantee on the optimality gap holds.The required approximation of the optimality gap is obtained by comparing the current candidate policy against multiple reference policies evaluated in the associated reference domains.After training, we can make an approximation of the resulting policy's suboptimality when transferring to a different domain from the same (source) distribution.To verify our approach we conducted two sim-to-real experiments on second order nonlinear continuous control systems.The results showed that SPOTA policies were able to directly transfer from simulation to reality while the baseline without domain randomization failed.</p>
<p>In the future we will investigate different strategies for sampling the domain parameters to replace the i.i.d.sampling from hand-crafted distributions.One idea is to employ Bayesian optimization for selecting the next set of domain parameters.Thus, the domain randomization could be executed according to an objective, and potentially increase sample-efficiency.Furthermore, we plan to devise a formulation which frames domain randomization and policy search in one optimization problem.This would allow for an joint treatment of finding a policy and matching the simulator to the real world.Table 4: The domain parameter distributions and derived parameters for the Ball-Balancer (Figure 1 left).All parameters were randomized such that they stayed physically plausible.Normal distributions are parameterized with mean and standard deviation, uniform distributions with lower and upper bound.The lines separate the randomized domain parameters from the ones depending on these.</p>
<p>Figure 1 :
1
Figure 1: Evaluation platforms by Quanser [4]: (left) the 2 DoF Ball-Balancer, (right) the linear inverted pendulum, called Cart-Pole.Both systems are under-actuated nonlinear balancing problems with continuous state and action spaces.</p>
<p>Figure 4 :
4
Figure 4: True optimality gap G(θ c ), the approximation from n domains Ĝ(θ c ), and the simulation optimization bias b[ Ĵn (θ n )].Note that Ĝn (θ c ) ≥ G(θ c ) does not hold for every instance of the 100 random seeds, but is true in expectation.The variance in G(θ c ) is only caused by the variance in θ c .The shaded areas show ±1 standard deviation.</p>
<p>24 Bootstrapb bootstrapping 25 Computeb 27 Select
242527
n b times from G = { Ĝ1 nr,1 θ nc , . . ., Ĝn G nr,nr θ nc } to yield G B 1 , . . ., G B n the sample mean Ḡnr θ nc for the original set G 26 Compute the sample means ḠB nr,1 θ nc , . . ., ḠB nr,n b θ nc for the sets G B 1 , . . ., G B n the α-th quantile of the bootstrap samples' means and obtain the upper bound for the one-sided (1 − α)-level confidence interval ḠU nr θ nc ← 2 Ḡnr θ nc − Q α ḠB nr θ nc UCBOG 28 Set the new sample sizes n c ← NonDecrSeq(n c ) and n r ← NonDecrSeq(n r ) 29 while ḠU nr θ nc &gt; βall reference solutions are guaranteed to be global optima.</p>
<p>Figure 5 :
5
Figure 5: Upper Confidence Bound on the Optimality Gap (UCBOG) and number of candidate solution domain over the iteration count of SPOTA.Every iteration, the number and domains (dashed line) and hence the sample size is increased.The shaded area visualize ±1 standard deviation across 9 training runs on the simulated Ball-Balancer.</p>
<p>Figure 6 :
6
Figure 6: Evaluation of the learned control policies on the simulated Ball Balancer (top row) as well as the simulated Cart-Pole (bottom row), (a) varying the ball mass m b , (b) the viscous friction coefficient, (c) the action delay ∆t a , and (d) the motor pinion radius r mp .Every domain parameter configuration has been evaluated on 360 rollouts with different initial states, synchronized across all policies.The dashed lines mark the nominal parameter values.The solid lines represent the means, and shaded areas show ±1 standard deviation</p>
<p>Figure 7 :
7
Figure 7: Evaluation of the learned control policies on (a) the real Ball Balancer and (b) the real Cart-Pole.The results were obtained from 40 rollouts per policy on the Ball Balancer (5 repetitions for 8 different initial ball positions) as well as 10 rollouts (1 initial state) on the Cart-Pole.The dashed lines approximately mark the threshold where the tasks are solved, i.e., the ball is centered in the middle (a), or the pendulum is stabilized upright (b) at the end of the episode.</p>
<p>3</p>
<p>Simulation-based Policy Optimization with Transferability Assessment (SPOTA) input : probability distribution ν(ξ; φ), algorithm PolOpt, sequence NonDecrSeq, hyper-parameters n c , n r , n G , n J , n b , α, β output: policy π θ nc with a (1 − α)-level confidence on Ḡnr θ nc which is upper bounded by β 1 Initialize π θ nc randomly 2 do Sample n c i.i.d.physics simulators described by ξ 1 , . . ., ξ nc from ν(ξ; φ) Solve the approx.SP using ξ 1 , . . ., ξ nc and PolOpt to obtain θ nc candidate solution Compute the difference in return Ĝk nr,i θ nc ← Ĵn J θ k nr , ξ k i − Ĵn J θ nc , ξ k
5for k = 1, . . . , n G do6 7 8Sample n r i.i.d. physics simulators described by ξ k 1 , . . . , ξ k nr from ν(ξ; φ) Initialize θ k nr with θ nc and reset the exploration strategy Solve the approx. SP using ξ k 1 , . . . , ξ k nr and PolOpt to obtain θ k nrreference solution9for i = 1, . . . , n r do10with synchronized random seedssync initial states and observation noise11 12Estimate the candidate solution's return Ĵn J θ nc , ξ k i ← 1/n J Estimate the i-th reference solution's return Ĵn J θ k nr , ξ k i ← 1/n J n J j=1Ĵ θ nc , ξ k i n J j=1 Ĵ θ k nr , ξ k i13end14
[3,20]come negative (Line 14).This issue did not appear in previous work on assessing solution qualities of SPs[3,20], because they only covered convex problems, where Algorithm 1: 4 i 15 end 16 for k = 1, . . ., n G and i = 1, . . ., n r do outlier rejection 17 if Ĝk nr,i θ nc &lt; 0 then 18 for k = 1, . . ., n G , k = k do loop over other reference solutions 19 if Ĝk nr,i θ nc &gt; Ĝk nr,i θ nc then Ĝk nr,i θ nc ← Ĝk nr,i θ nc ; break replace solution</p>
<p>ACKNOWLEDGMENTSFabio Muratore gratefully acknowledges the financial support from Honda Research Institute Europe.Jan Peters received funding from the European Unions Horizon 2020 research and innovation programme under grant agreement No 640554.APPENDIX A MODELING DETAILS ON THE PLATFORMSThe Ball-Balancer is modeled as a nonlinear second-order dynamical system , with the motor shaft angles θ x and θ y , the ball positions relative to the place center x b and y b , the plate angle β and α around the x and y axis, and the commanded motorTo model the gears' backlash, we set all voltage values between V thold,− and V thold,+ to zero.These threshold values have been determined in separate experiments for both servo motors.The Ball-Balancer's domain parameters as well as the ones derived from them are listed in Table4.For the Ball-Balancer's we define the reward function asGiven a lower bound for the reward r min ∈ [0, 1], the reward function above yields values within [r min , 1] at each time step.We found that the scaling constant c &lt; 0 is beneficial for the learning procedure, since it prohibits the reward from going to zero too quickly.The constant's denominator can be easily inferred from the nominal state and action set's boundaries.The Cart-Pole is modeled as a nonlinear second-order dynamical system given by the solution ofwhere the commanded motor voltage V is encapsulated inThe system's state s is given by the cart's position x and the pole's angle α, which are defined to be zero at the rail's center and hanging down vertically, respectively.The Cart-Pole's domain parameters as well as the parameters derived from them are listed in Table5.Similar to the Ball-Balancer, the Cart-Pole's reward function is based on an exponentiated quadratic costThus, the reward is in range ]0, 1] for every time step.APPENDIX B PARAMETER VALUES FOR THE EXPERIMENTS
Noise and the reality gap: The use of simulation in evolutionary robotics. N Jakobi, P Husbands, I Harvey, Advances in Artificial Life. Granada, SpainJune 4-6, 1995</p>
<p>Is optimization optimistically biased?. B F Hobbs, A Hepenstal, Water Resources Research. 2521989</p>
<p>Monte carlo bounding techniques for determining solution quality in stochastic programs. W Mak, D P Morton, R K Wood, Oper. Res. Lett. 241-21999</p>
<p>Quanser platforms. </p>
<p>Human-level control through deep reinforcement learning. V Mnih, K Kavukcuoglu, D Silver, A A Rusu, J Veness, M G Bellemare, A Graves, M A Riedmiller, A Fidjeland, G Ostrovski, S Petersen, C Beattie, A Sadik, I Antonoglou, H King, D Kumaran, D Wierstra, S Legg, D Hassabis, Nature. 51875402015</p>
<p>Mastering the game of go without human knowledge. D Silver, J Schrittwieser, K Simonyan, I Antonoglou, A Huang, A Guez, T Hubert, L Baker, M Lai, A Bolton, Nature. 55076763542017</p>
<p>Continuous control with deep reinforcement learning. T P Lillicrap, J J Hunt, A Pritzel, N Heess, T Erez, Y Tassa, D Silver, D Wierstra, ICLR. San Juan, Puerto RicoMay 2-4, 2016</p>
<p>Proximal policy optimization algorithms. J Schulman, F Wolski, P Dhariwal, A Radford, O Klimov, 2017ArXiv e-prints</p>
<p>Sim-to-real robot learning from pixels with progressive nets. A A Rusu, M Vecerik, T Roth Örl, N Heess, R Pascanu, R Hadsell, CoRLNovember 13-15, 2017Mountain View, California, USA</p>
<p>Ensemblecio: Full-body dynamic motion planning that transfers to physical humanoids. I Mordatch, K Lowrey, E Todorov, IROS. September 28 -October 2, 2015</p>
<p>Towards generalization and simplicity in continuous control. A Rajeswaran, K Lowrey, E Todorov, S M Kakade, NIPS. Long Beach, CA, USA4-9 December, 2017</p>
<p>Adversarially robust policy learning: Active construction of physically-plausible perturbations. A Mandlekar, Y Zhu, A Garg, L Fei-Fei, S Savarese, IROS. September 24-28, 2017</p>
<p>Robust adversarial reinforcement learning. L Pinto, J Davidson, R Sukthankar, A Gupta, ICML. Sydney, NSW, AustraliaAugust 6-11. PMLR, 2017</p>
<p>Asymmetric actor critic for imagebased robot learning. L Pinto, M Andrychowicz, P Welinder, W Zaremba, P Abbeel, RSS. June 26-30, 2018</p>
<p>Sim-to-real transfer of robotic control with dynamics randomization. X B Peng, M Andrychowicz, W Zaremba, P Abbeel, ICRA. May 21-25, 2018</p>
<p>Preparing for the unknown: Learning a universal policy with online system identification. W Yu, J Tan, C K Liu, G Turk, RSS. July 12-16, 2017</p>
<p>Closing the simto-real loop: Adapting simulation randomization with real world experience. Y Chebotar, A Handa, V Makoviychuk, M Macklin, J Issac, N D Ratliff, D Fox, ICRA. May 20-24, 2019</p>
<p>Learning dexterous in-hand manipulation. M Openai, B Andrychowicz, M Baker, R Chociej, B Jozefowicz, J Mcgrew, A Pachocki, M Petron, G Plappert, A Powell, J Ray, S Schneider, J Sidor, P Tobin, L Welinder, W Weng, Zaremba, 20181808177ArXiv e-prints</p>
<p>Domain randomization for simulation-based policy optimization with transferability assessment. F Muratore, F Treede, M Gienger, J Peters, CoRL. Switzerland29-31 October, 2018</p>
<p>Assessing solution quality in stochastic programs. G Bayraksan, D P Morton, Math. Program. 1082-32006</p>
<p>Bootstrap methods: another look at the jackknife. B Efron, Annals of Statistics. 1979</p>
<p>Bootstrap confidence intervals. T J Diciccio, B Efron, Statistical Science. 1996</p>
<p>Adam: a method for stochastic optimization. D P Kingma, J Ba, ICLR. San Diego, CA, USAMay 7-9, 2015</p>
<p>Retrospectiveapproximation algorithms for the multidimensional stochastic root-finding problem. R Pasupathy, B W Schmeiser, ACM Trans. Model. Comput. Simul. 1922009</p>
<p>A guide to sample average approximation. S Kim, R Pasupathy, S G Henderson, Handbook of Simulation Optimization. Springer2015</p>
<p>Convergence theory for nonconvex stochastic programming with an application to mixed logit. F Bastin, C Cirillo, P L Toint, Math. Program. 1082-32006</p>
<p>Resilient machines through continuous self-modeling. J Bongard, V Zykov, H Lipson, Science. 31458022006</p>
<p>The transferability approach: Crossing the reality gap in evolutionary robotics. S Koos, J Mouret, S Doncieux, IEEE Trans. Evol. Comput. 1712013</p>
<p>Grounded action transformation for robot learning in simulation. J P Hanna, P Stone, AAAI. San Francisco, California, USAFebruary 4-9, 2017</p>
<p>Identification of Dynamic Systems: An Introduction with Applications. R Isermann, M Ünchhof, 2010Springer Science &amp; Business Media</p>
<p>Gaussian processes for machine learning, ser. Adaptive computation and machine learning. C E Rasmussen, C K I Williams, 2006MIT Press</p>
<p>Optimizing walking controllers for uncertain inputs and environments. J M Wang, D J Fleet, A Hertzmann, ACM Trans. Graph. 2942010</p>
<p>Unlocking the potential of simulators: Design with RL in mind. R Antonova, S Cruciani, 2017ArXiv e-prints</p>
<p>Model-ensemble trust-region policy optimization. T Kurutach, I Clavera, Y Duan, A Tamar, P Abbeel, ICLR. Vancouver, BC, CanadaApril 30 -May 3, 2018</p>
<p>Domain randomization for transferring deep neural networks from simulation to the real world. J Tobin, R Fong, A Ray, J Schneider, W Zaremba, P Abbeel, IROS. September 24-28, 2017</p>
<p>CAD2RL: real single-image flight without a single real image. F Sadeghi, S Levine, RSS. July 12-16, 2017</p>
<p>Sim-to-real reinforcement learning for deformable object manipulation. J Matas, S James, A J Davison, CoRL. Switzerland29-31 October, 2018</p>
<p>Using simulation and domain adaptation to improve efficiency of deep robotic grasping. K Bousmalis, A Irpan, P Wohlhart, Y Bai, M Kelcey, M Kalakrishnan, L Downs, J Ibarz, P Pastor, K Konolige, S Levine, V Vanhoucke, ICRA. May 21-25, 2018</p>
<p>Multi-task policy search for robotics. M P Deisenroth, P Englert, J Peters, D Fox, ICRA. Hong Kong, ChinaMay 31 -June 7, 2014</p>
<p>Hindsight experience replay. M Andrychowicz, D Crow, A Ray, J Schneider, R Fong, P Welinder, B Mcgrew, J Tobin, P Abbeel, W Zaremba, NIPS. Long Beach, CA, USA4-9 December, 2017</p>            </div>
        </div>

    </div>
</body>
</html>
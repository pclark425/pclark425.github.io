<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1554 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1554</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1554</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-30.html">extraction-schema-30</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-6b74c2676ac43528c133a30602a1cf759c921fc1</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/6b74c2676ac43528c133a30602a1cf759c921fc1" target="_blank">Extracting Action Sequences from Texts Based on Deep Reinforcement Learning</a></p>
                <p><strong>Paper Venue:</strong> International Joint Conference on Artificial Intelligence</p>
                <p><strong>Paper TL;DR:</strong> This paper proposes to extract action sequences from texts based on the deep reinforcement learning framework, and views ``selecting'' or ``eliminating'' words from texts as ``actions'', and texts associated with actions as ``states''.</p>
                <p><strong>Paper Abstract:</strong> Extracting action sequences from texts is

challenging, as it requires commonsense inferences based on world

knowledge. Although there has been work on extracting action scripts,

instructions, navigation actions, etc., they require either the

set of candidate actions be provided in advance, or action

descriptions are restricted to a specific form, e.g., description

templates. In this paper we aim to extract action sequences from

texts in \emph{free} natural language, i.e., without any restricted

templates, provided the set of actions is unknown. We

propose to extract action sequences from texts based on the deep

reinforcement learning framework. Specifically, we view ``selecting''

or ``eliminating'' words from texts as ``actions'', and texts associated with actions as ``states''. We build Q-networks to learn policies of extracting actions and extract plans from the labeled texts. We demonstrate the effectiveness of our approach on several datasets with comparison to state-of-the-art approaches.</p>
                <p><strong>Cost:</strong> 0.004</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1554",
    "paper_id": "paper-6b74c2676ac43528c133a30602a1cf759c921fc1",
    "extraction_schema_id": "extraction-schema-30",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.003658,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Extracting Action Sequences from Texts Based on Deep Reinforcement Learning</h1>
<p>Wenfeng Feng ${ }^{1}$, Hankz Hankui Zhuo ${ }^{1 *}$, Subbarao Kambhampati ${ }^{2}$<br>${ }^{1}$ School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China<br>${ }^{2}$ Department of Computer Science and Engineering, Arizona State University, Tempe, Arizona, US fengwf@mail2.sysu.edu.cn, zhuohank@mail.sysu.edu.cn, rao@asu.edu</p>
<h4>Abstract</h4>
<p>Extracting action sequences from texts is challenging, as it requires commonsense inferences based on world knowledge. Although there has been work on extracting action scripts, instructions, navigation actions, etc., they require either the set of candidate actions be provided in advance, or action descriptions are restricted to a specific form, e.g., description templates. In this paper we aim to extract action sequences from texts in free natural language, i.e., without any restricted templates, provided the set of actions is unknown. We propose to extract action sequences from texts based on the deep reinforcement learning framework. Specifically, we view "selecting" or "eliminating" words from texts as "actions", and texts associated with actions as "states". We build Q-networks to learn policies of extracting actions and extract plans from the labeled texts. We demonstrate the effectiveness of our approach on several datasets with comparison to state-of-the-art approaches.</p>
<h2>1 Introduction</h2>
<p>AI agents will increasingly find assistive roles in homes, labs, factories and public places. The widespread adoption of conversational agents such as Alexa, Siri and Google Home demonstrate the natural demand for such assistive agents. To go beyond supporting the simplistic "what is the weather?" queries however, these agents need domain-specific knowledge such as the recipes and standard operating procedures. While it is possible to hand-code such knowledge (as is done by most of the "skills" used by Alexa-like agents), ultimately that is too labor intensive an option. One idea is to have these agents automatically "read" instructional texts, typically written for human workers, and convert them into action sequences and plans for later use (such as learning domain models [Zhuo et al., 2014; Zhuo and Yang, 2014] or model-lite planning [Zhuo and Kambhampati, 2017]). Extracting action sequences from natural language texts meant for human consumption is however challenging, as it requires agents to understand complex contexts of actions.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>For example, in Figure 1, given a document of action descriptions (the left part of Figure 1) such as "Cook the rice the day before, or use leftover rice in the refrigerator. The important thing to remember is not to heat up the rice, but keep it cold.", which addresses the procedure of making egg fired rice, an action sequence of "cook(rice), keep(rice, cold)" or "use(leftover rice), keep(rice, cold)" is expected to be extracted. This task is challenging. For the first sentence, the agent needs to learn to figure out that "cook" and "use" are exclusive (denoted by "EX" in the middle of Figure 1), meaning that we could extract only one of them; for the second sentence, we need to learn to understand that among the three verbs "remember", "heat" and "keep", the last one is the best because the goal of this step is to "keep the rice cold" (denoted by "ES" indicating this action is essential). There is also another action "Recycle" denoted by "OP" indicating this action can be extracted optionally. We also need to consider action arguments which can be either "EX" or "ES" as well (as shown in the middle of Figure 1). The possible action sequences extracted are shown in the right part of Figure 1. This extraction problem is different from sequence labeling and dependency parsing, since we aim to extract "meaningful" or "correct" action sequences (which suggests some actions should be ignored because they are exclusive), such as "cook(rice), keep(rice, cold)", instead of "cook(rice),use(leftover rice), remember(thing), heat(rice), keep(rice, cold)" as would be extracted by LSTMCRF models[Ma and Hovy, 2016] or external NLP tools.</p>
<p>There has been work on extracting action sequences from action descriptions. For example, [Branavan et al., 2009] propose to map instructions to sequences of executable actions using reinforcement learning. [Mei et al., 2016; Daniele et al., 2017] interpret natural instructions as action sequences or generate navigational action description using an encoder-aligner-decoder structure. Despite the success of those approaches, they all require a limited set of action names given as input, which are mapped to action descriptions. Another approach, proposed by [Lindsay et al., 2017], builds action sequences from texts based on dependency parsers and then builds planning models, assuming texts are in restricted templates when describing actions.</p>
<p>In this paper, we aim to extract meaningful action sequences from texts in free natural language, i.e., without any restricted templates, even when the candidate set of actions</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Illustration of our action sequence extraction problem
is unknown. We propose an approach called EASDRL, which stands for Extracting Action Sequences from texts based on Deep Reinforcement Learning. In EASDRL, we view texts associated with actions as "states", and associating words in texts with labels as "actions", and then build deep Q-networks to extract action sequences from texts. We capture complex relations among actions by considering previously extracted actions as parts of states for deciding the choice of next operations. In other words, once we know action "cook(rice)" has been extracted and included as parts of states, we will choose to extract next action "keep(rice, cold)" instead of "use(leftover rice)" in the above-mentioned example.</p>
<p>In the remainder of paper, we first review previous work related to our approach. After that we give a formal definition of our plan extraction problem and present EASDRL in detail. We then evaluate EASDRL with comparison to state-of-theart approaches and conclude the paper with future work.</p>
<h2>2 Related Work</h2>
<p>There have been approaches related to our work besides the ones we mentioned in the introduction section. Mapping route instructions [Macmahon et al., 2006] to action sequences has aroused great interest in natural language processing community. Early approaches, such as [Chen and Mooney, 2011; Kim and Mooney, 2013], largely depend on specialized resources, i.e. semantic parsers, learned lexicons and re-rankers. Recently, LSTM encoder-decoder structure [Mei et al., 2016] has been applied to this problem and gets decent performance in processing single-sentence instructions, however, it could not handle multi-sentence texts well.</p>
<p>There is also a lot of work on learning STRIPS represen-
tation actions [Pomarlan et al., 2017] from texts. [Sil et al., 2010; Sil and Yates, 2011] learn sentence patterns and lexicons or use off-the-shelf toolkits, i.e., OpenNLP ${ }^{1}$ and Stanford CoreNLP ${ }^{2}$. [Lindsay et al., 2017] also build action models with the help of LOCM [Cresswell et al., 2009] after extracting action sequences by using NLP tools. These tools are trained for universal natural language processing tasks, they cannot solve the complex action sequence extraction problem well, and their performance will be greatly affected by POS-tagging and dependency parsing results. In this paper we aim to build a model that learns to directly extract action sequences without external tools.</p>
<h2>3 Problem Definition</h2>
<p>Our training data can be defined by $\Phi={\langle X, Y\rangle}$, where $X=\left\langle w_{1}, w_{2}, \ldots, w_{N}\right\rangle$ is a sequence of words and $Y=$ $\left\langle y_{1}, y_{2}, \ldots, y_{N}\right\rangle$ is a sequence of annotations. If $w_{i}$ is not an action name, $y_{i}$ is $\emptyset$. Otherwise, $y_{i}$ is a tuple $\left(\right.$ ActType, $\left.{E x A c t I d},{\langle A r g I d, E x A r g I d\rangle}\right)$ to describe type of the action name and its corresponding arguments. ActType indicates the type of action $a_{i}$ corresponding to $w_{i}$, which can be one of essential, optional and exclusive. The type essential suggests the corresponding action $a_{i}$ to be extracted, optional suggests $a_{i}$ that can be "optionally" extracted, exclusive suggests $a_{i}$ that is "exclusive" with other actions indicated by the set ${E x A c t I d}$ (in other words, either $a_{i}$ or exactly one action in ${E x A c t I d}$ can be extracted). ExActId is the index of the action exclusive with $a_{i}$. We denote the size of ${E x A c t I d}$ by $M$, i.e., $|{E x A c t I d}|=M$.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Note that " $M=0$ " indicates the type $A c t T y p e$ of action $a_{i}$ is either essential or optional, and " $M \neq 0$ " indicates $A c t T y p e$ is exclusive. ArgId is the index of the word composing arguments of $a_{i}$, and ExArgId is the index of words exclusive with ArgId. For example, as shown in Figure 2, given a text denoted by $X$, its corresponding annotation is shown in the figure denoted by $Y$. In $y_{1}$, " ${I I}$ " indicates the action exclusive with $w_{1}$ (i.e., "Hang") is "opt" with index 11. " ${\langle 3,5\rangle,\langle 9, \rangle}$ " indicates the corresponding arguments "engraving" and "lithograph" are exclusive, and the other argument "frame" with index 9 is essential since it is exclusive with an empty index, likewise for $y_{11}$. For $y_{2}, \ldots, y_{10}$ and $y_{12}, \ldots, y_{15}$, they are empty since their corresponding words are not action names. From $Y$, we can generate three possible actions as shown at the bottom of Figure 2.</p>
<p>As we can see from the training data, it is uneasy to build a supervised learning model to directly predict annotations for new texts $X$, since annotations $y_{i}$ is complex and the size $\left|y_{i}\right|$ varies with respect to different $w_{i}$ (different action names have different arguments with different lengths). We seek to build a unified framework to predict simple "labels" (corresponding to "actions" in reinforcement learning) for extracting action names and their arguments. We exploit the framework to learn two models to predict action names and arguments, respectively. Specifically, given a new text $X$, we would like to predict a sequence of operations $O=\left\langle o_{1}, o_{2}, \ldots, o_{N}\right\rangle$ (instead of annotations in $\Phi$ ) on $X$, where $o_{i}$ is an operation that selects or eliminates word $w_{i}$ in $X$. In other words, when predicting action names (or arguments), $o_{i}=$ Select indicates $w_{i}$ is extracted as an action name (or argument), while $o_{i}=$ Eliminate indicates $w_{i}$ is not extracted as an action name (or argument).</p>
<p>In summary, our action sequence extraction problem can be defined by: given a set of training data $\Phi$, we aim to learn two models (with the same framework) to predict action names and arguments for new texts $X$, respectively. The two models are</p>
<p>$$
\mathcal{F}<em 1="1">{\Phi}^{1}\left(O \mid X ; \theta</em>\right)
$$</p>
<p>and</p>
<p>$$
\mathcal{F}<em 2="2">{\Phi}^{2}\left(O \mid X, a ; \theta</em>\right)
$$</p>
<p>where $\theta_{1}$ and $\theta_{2}$ are parameters to be learnt for predicting action names and arguments, respectively. $a$ is an action name extracted based on $\mathcal{F}<em _Phi="\Phi">{\Phi}^{1}$. We train $\mathcal{F}</em>$. We will present the two models in detail in the following sections.}^{2}$ for extracting arguments based on ground-truth action names. When testing, we extract arguments based on the action names extracted by $\mathcal{F}_{\Phi}^{1</p>
<h2>4 Our EASDRL Approach</h2>
<p>In this section we present the details of EASDRL. As mentioned in the introduction section, our action sequence extraction problem can be viewed as a reinforcement learning problem. We thus first describe how to build states and operations given text $X$, and then present deep Q-networks to build the Q-functions. Finally we present the training procedure and give an overview of EASDRL. Note that we will use the term operation to represent the meaning of "action" in reinforcement learning since the term "action" has been used to represent an action name with arguments in this work.</p>
<h3>4.1 Generating State Representations</h3>
<p>In this subsection we address how to generate state representations from texts. As defined in the problem definition section, the space of operations is ${$ Select, Eliminate $}$. We view texts associated with operations as "states". Specifically, we represent a text $X$ by a sequence of vectors $\left\langle\mathbf{w}<em 2="2">{1}, \mathbf{w}</em>}, \ldots, \mathbf{w<em i="i">{N}\right\rangle$, where $\mathbf{w}</em>} \in \mathcal{R}^{K_{1}}$ is a $K_{1}$-dimension real-valued vector [Mikolov et al., 2013], representing the $i$ th word in $X$. Words of texts stay the same when we perform operations, so we embed operations in state representations to generate state transitions. We extend the set of operations to $\left{\right.$ NULL, Select, Eliminate $}$ where "NULL" indicates a word has not been processed. We represent the operation sequence $O$ corresponding to $X$ by a sequence of vectors $\left\langle\mathbf{o<em 2="2">{1}, \mathbf{o}</em>}, \ldots, \mathbf{o<em i="i">{N}\right\rangle$, where $\mathbf{o}</em>} \in \mathcal{R}^{K_{2}}$ is a $K_{2}$-dimension real-valued vector. In order to balance the dimension of $\mathbf{o<em i="i">{i}$ and $\mathbf{w}</em>}$, we generate each $\mathbf{o<em K__2="K_{2">{i}$ by a repeat-representation $[\cdot]</em>}}$, i.e., if $K_{2}=1, \mathbf{o<em 2="2">{i} \in{[0],[1],[2]}$, and if $K</em>}=3$, $\mathbf{o<em 1="1">{i} \in{[0,0,0],[1,1,1],[2,2,2]}$, where ${0,1,2}$ corresponds to ${$ NULL, Select, Eliminate $}$, respectively. We define a state $s$ as a tuple $\langle\mathbf{X}, \mathbf{O}\rangle$, where $\mathbf{X}$ is a matrix in $\mathcal{R}^{K</em>} \times N}, \mathbf{O}$ is a matrix in $\mathcal{R}^{K_{2} \times N}$. The $i$ th row of $s$ is denoted by $\left[\mathbf{w<em i="i">{i}, \mathbf{o}</em>}\right]$. The space of states is denoted by $\mathcal{S}$. A state $s$ is changed into a new state $s^{\prime}$ after performing an operation $\mathbf{o<em 1="1">{i}^{\prime}$ on $s$, such that $s^{\prime}=\left\langle\mathbf{X}, \mathbf{O}^{\prime}\right\rangle$, where $\mathbf{O}^{\prime}=\left\langle\mathbf{o}</em>}, \ldots, \mathbf{o<em i="i">{i-1}, \mathbf{o}</em>}^{\prime}, \mathbf{o<em N="N">{i+1}, \ldots, \mathbf{o}</em>}\right\rangle$. For example, consider a text "Cook the rice the day before..." and a state $s$ corresponding to it is shown in the left part of Figure 3. After performing an operation $\mathbf{o<em 1="1">{1}=$ Select on $s$, a new state $s^{\prime}$ (the right part) will be generated. In this way, we can learn $\theta</em>$ (Equation (1)) based on $s$ with deep Q-networks as introduced in the next subsection.}$ in $\mathcal{F}_{\Phi}^{1</p>
<p>After $\mathcal{F}<em _Phi="\Phi">{\Phi}^{1}$ is learnt, we can use it to predict action names, and then exploit the predicted action names to extract arguments by training $\mathcal{F}</em>}^{2}$ (Equation (2)). To do this, we would like to encode the predicted action names in states to generate a new state representation $\hat{s}$ for learning $\theta_{2}$ in $\mathcal{F<em a="a">{\Phi}^{2}$. We denote by $w</em>}$ the word corresponding to the action name. We build $\hat{s}$ by appending the distance between $w_{a}$ and $w_{j}$ based on their indices, such that $\hat{s}=\langle\mathbf{X}, \mathbf{D}, \mathbf{O}\rangle$, where $\mathbf{D}=$ $\left\langle\mathbf{d<em 2="2">{1}, \mathbf{d}</em>}, \ldots, \mathbf{d<em j="j">{N}\right\rangle$, where $\mathbf{d}</em>\right]}=\left[d_{j<em 3="3">{K</em>}}$ and $d_{j}=|a-j|$. Note that $\mathbf{d<em 3="3">{j}$ is a $K</em>$-dimension real-valued vector using repeatrepresentation $[\cdot]<em 3="3">{K</em>}}$. In this way we can learn $\mathcal{F<em 1="1">{\Phi}^{2}$ based on $\hat{s}$ with the same deep Q-networks. Note that in our experiments, we found that the results were the best when we set $K</em>$, suggesting the impact of word vectors, distance vectors and operation vectors was generally identical.}=K_{2}=K_{3</p>
<h3>4.2 Deep Q-networks for Operation Execution</h3>
<p>Given the formulation of states and operations, we aim to extract action sequences from texts. We construct sequences by repeatedly choosing operations given current states, and applying operations on current states to achieve new states.</p>
<p>In Q-Learning, this process can be described by a Qfunction and updating the Q-function iteratively according to Bellman equation. In our action sequence extraction problem, actions are composed of action names and action arguments. We need to first extract action names from texts and use the extracted action names to further extract action arguments. Specifically, we define two Q-functions $Q(s, o)$ and $Q(\hat{s}, o)$,</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Illustration of text X and its corresponding annotation Y</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Illustration of states and operations
where $\hat{s}$ contains the information of extracted action names, as defined in the last subsection. The update procedure based on Bellman equation and deep Q-networks can be defined by:</p>
<p>$$
\begin{aligned}
&amp; Q_{i+1}\left(s, o ; \theta_{1}\right)=E\left{r+\gamma \max <em i="i">{o^{\prime}} Q</em>\right) \mid s, o\right} \
&amp; Q_{i+1}\left(\hat{s}, o ; \theta_{2}\right)=E\left{r+\gamma \max }\left(s^{\prime}, o^{\prime} ; \theta_{1<em i="i">{o^{\prime}} Q</em>, o\right}
\end{aligned}
$$}\left(\hat{s}^{\prime}, o^{\prime} ; \theta_{2}\right) \mid \hat{s</p>
<p>where $Q_{i+1}\left(s, o ; \theta_{1}\right)$ and $Q_{i+1}\left(\hat{s}, o ; \theta_{2}\right)$ correspond to the deep Q-networks [Mnih et al., 2015] for extracting action names and arguments, respectively. As $i \rightarrow \infty, Q_{i} \rightarrow Q^{<em>}$. In this way, we can define $\mathcal{F}_{\Phi}^{1}=Q^{</em>}\left(s, o ; \theta_{1}\right)$ and $\mathcal{F}<em 2="2">{\Phi}^{2}=$ $Q^{*}\left(\hat{s}, o ; \theta</em>}\right)$ in Equations (1) and (2), and then use $\mathcal{F<em _Phi="\Phi">{\Phi}^{j}$ and $\mathcal{F}</em>$ to extract action names and arguments, respectively.}^{2</p>
<p>Since Convolutional Neural Networks (CNNs) are effective in natural language processing [Kim, 2014; Zhang and Wallace, 2015; Wang et al., 2017], we build CNN models to learn $Q\left(s, o, \theta_{1}\right)$ and $Q\left(\hat{s}, o, \theta_{2}\right)$. We adopt the CNN Architecture of [Zhang and Wallace, 2015]. To build the kernels of our CNN models, we test from uni-gram context to ten-gram context and observe that five-word context works well in our task. We thus design four types of kernels, which correspond to bigram, trigram, four-gram and five-gram, respectively.</p>
<h3>4.3 Computing Rewards</h3>
<p>In this subsection we compute the reward $r$ based on state $s$ and operation $o$. Specifically, $r$ is composed of two parts, i.e., basic reward and additional reward. For the basic reward at time step $\tau$, denoted by $r_{b, \tau}$, if a word is not an item (we use item to represent action name or action argument when it
is not confused), $r_{b, \tau}$ is +50 when the operation is correct and -50 otherwise. If a word is an essential item, $r_{b, \tau}=+100$ when the operation is correct and $r_{b, \tau}=-100$ when it is incorrect. If the word is an optional item, $r_{b, \tau}=+100$ when the operation is correct and $r_{b, \tau}=0$ when it is incorrect. If a word is an exclusive item, $r_{b, \tau}=+150$ when the operation is correct and $r_{b, \tau}=-150$ when it is incorrect. We denote that an operation is correct when it selects essential items, selects optional items, selects only one item of exclusive items or eliminates words that are not items.</p>
<p>Note that action names are key verbs of a text and action arguments are some nominal words, so the percentage of these words in a text is closely related to action sequence extraction process. We thus calculate the percentage, namely an item rate, denoted by $\delta=\frac{# \text { Item }}{# \text { Word }}$, where $#$ Item indicates the amount of action names or action arguments in all the annotated texts and $#$ Word indicates the total number of words of these texts. We define a real-time item rate as $\delta_{\tau}$ to denote the percentage of words that have been selected as action names or action arguments in a text after $\tau$ training steps, and $\delta_{0}=0$. On one hand, when $\delta_{\tau-1} \leq \delta$, a positive additional reward is added to $r_{b, \tau}$ if $r_{b, \tau} \geq 0$ (i.e., the operation is correct), otherwise a negative additional reward is added to $r_{b, \tau}$. On the other hand, when $\delta_{\tau}&gt;\delta$, which means that words selected as action names or action arguments are out of the expected number and it is more likely to be incorrect if subsequent words are selected, then a negative additional reward should be added to the basic reward. In this way, the reward $r_{\tau}$ at time step $\tau$ can be obtained by Equation (5),</p>
<p>$$
r_{\tau}= \begin{cases}r_{b, \tau}+\operatorname{sgn} r_{b, \tau} \cdot c \delta_{\tau-1} &amp; \delta_{\tau-1} \leq \delta \ r_{b, \tau}-c \delta_{\tau-1} &amp; \delta_{\tau-1}&gt;\delta\end{cases}
$$</p>
<p>where $c$ is a positive constant and $0 \leq \delta_{\tau-1}&lt;1$.</p>
<h3>4.4 Training Our Model</h3>
<p>To learn the parameters $\theta_{1}$ and $\theta_{2}$ of our two DQNs, we store transitions $\left\langle s, o, r, s^{\prime}\right\rangle$ and $\left\langle\hat{s}, o, r, \hat{s}^{\prime}\right\rangle$ in replay memories $\Omega$ and $\hat{\Omega}$, respectively, and exploit a mini-batch sampling strategy. As indicated in [Narasimhan et al., 2015], transitions that provide positive rewards can be used more often to learn optimal Q-values faster. We thus develop a positive-rate</p>
<p>based experience replay instead of randomly sampling transitions from $\Omega$ (or $\tilde{\Omega}$ ), where positive-rate indicates the percentage of transitions with positive rewards. To do this, we set a positive rate $\rho(0&lt;\rho&lt;1)$ and require the proportion of positive samples in each mini-batch be $\rho$.</p>
<p>We present the learning procedure of EASDRL in Algorithm 1, for building $\mathcal{F}<em 1="1">{\Phi}^{1}$. We can simply replace $s</em>}, \Omega$ and $\theta_{1}$ with $\hat{s<em 2="2">{1}, \hat{\Omega}$ and $\theta</em>}$ for building $\mathcal{F<em 1="1">{\Phi}^{2}$. In Step 4 of Algorithm 1, we generate the initial state $s</em>}$ ( $\hat{s<em _Phi="\Phi">{1}$ for learning $\mathcal{F}</em>$ as shown in Step 13.}^{2}$ ) for each training data $\Phi={\langle X, Y\rangle}$ by setting all operations $o_{i}$ in $s_{1}$ to be $N U L L$. We perform $N$ steps to execute one of the operations ${$ Select, Eliminate $}$ in Steps 6, 7 and 8. From Steps 10 and 11, we do a positive-rate based experience replay according to positive rate $\rho$. From Steps 12 and 13, we update parameters $\theta_{1}$ using gradient descent on the loss $\mathcal{L}\left(\theta_{1}\right)=\left(y_{j}-Q\left(s_{j}, o_{j} ; \theta_{1}\right)\right)^{2</p>
<p>With Algorithm 1, we are able to build $Q\left(s, o ; \theta_{1}\right)$ and execute operations ${$ Select, Eliminate $}$ to a new text by iteratively maximizing $Q\left(s, o ; \theta_{1}\right)$. Once we obtain operation sequences, we can generate action names and use them to build $Q\left(\hat{s}, o ; \theta_{2}\right)$ with $\hat{\Omega}$ and the same framework of Algorithm 1. We then exploit the built $Q\left(\hat{s}, o ; \theta_{2}\right)$ to extract action arguments. As a result, we can extract action sequences from texts using both of the built $Q\left(s, o ; \theta_{1}\right)$ and $Q\left(\hat{s}, o ; \theta_{2}\right)$.</p>
<div class="codehilite"><pre><span></span><code><span class="nt">Algorithm</span><span class="w"> </span><span class="nt">1</span><span class="w"> </span><span class="nt">Our</span><span class="w"> </span><span class="nt">EASDRL</span><span class="w"> </span><span class="nt">algorithm</span>
<span class="nt">Input</span><span class="o">:</span><span class="w"> </span><span class="nt">a</span><span class="w"> </span><span class="nt">training</span><span class="w"> </span><span class="nt">set</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">Phi</span><span class="err">\</span><span class="o">),</span><span class="w"> </span><span class="nt">positive</span><span class="w"> </span><span class="nt">rate</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">rho</span><span class="err">\</span><span class="o">),</span><span class="w"> </span><span class="nt">item</span><span class="w"> </span><span class="nt">rate</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">delta</span><span class="err">\</span><span class="o">)</span>
<span class="nt">Output</span><span class="o">:</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">parameters</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">theta_</span><span class="p">{</span><span class="err">1</span><span class="p">}</span><span class="err">\</span><span class="o">)</span>
<span class="w">    </span><span class="nt">Initialize</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">Omega</span><span class="o">=</span><span class="err">\</span><span class="nt">emptyset</span><span class="err">\</span><span class="o">),</span><span class="w"> </span><span class="nt">CNN</span><span class="w"> </span><span class="nt">with</span><span class="w"> </span><span class="nt">random</span><span class="w"> </span><span class="nt">values</span><span class="w"> </span><span class="nt">for</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">theta_</span><span class="p">{</span><span class="err">1</span><span class="p">}</span><span class="err">\</span><span class="o">)</span>
<span class="w">    </span><span class="nt">for</span><span class="w"> </span><span class="nt">epoch</span><span class="w"> </span><span class="err">\</span><span class="o">(=</span><span class="nt">1</span><span class="o">:</span><span class="w"> </span><span class="nt">H</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">do</span>
<span class="w">        </span><span class="nt">for</span><span class="w"> </span><span class="nt">each</span><span class="w"> </span><span class="nt">training</span><span class="w"> </span><span class="nt">data</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">langle</span><span class="w"> </span><span class="nt">X</span><span class="o">,</span><span class="w"> </span><span class="nt">Y</span><span class="err">\</span><span class="nt">rangle</span><span class="w"> </span><span class="err">\</span><span class="nt">in</span><span class="w"> </span><span class="err">\</span><span class="nt">Phi</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">do</span>
<span class="w">            </span><span class="nt">Generate</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">initial</span><span class="w"> </span><span class="nt">state</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">s_</span><span class="p">{</span><span class="err">1</span><span class="p">}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">based</span><span class="w"> </span><span class="nt">on</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">X</span><span class="err">\</span><span class="o">)</span>
<span class="w">            </span><span class="nt">for</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">tau</span><span class="o">=</span><span class="nt">1</span><span class="o">:</span><span class="w"> </span><span class="nt">N</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">do</span>
<span class="w">                </span><span class="nt">Perform</span><span class="w"> </span><span class="nt">an</span><span class="w"> </span><span class="nt">operation</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">o_</span><span class="p">{</span><span class="err">\tau</span><span class="p">}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">with</span><span class="w"> </span><span class="nt">probability</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">epsilon</span><span class="err">\</span><span class="o">)</span>
<span class="w">                </span><span class="nt">Otherwise</span><span class="w"> </span><span class="nt">select</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">o_</span><span class="p">{</span><span class="err">\tau</span><span class="p">}</span><span class="o">=</span><span class="err">\</span><span class="nt">max</span><span class="w"> </span><span class="nt">_</span><span class="p">{</span><span class="err">o</span><span class="p">}</span><span class="w"> </span><span class="nt">Q</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="nt">s_</span><span class="p">{</span><span class="err">\tau</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="nt">o</span><span class="w"> </span><span class="o">;</span><span class="w"> </span><span class="err">\</span><span class="nt">theta_</span><span class="p">{</span><span class="err">1</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="o">)</span>
<span class="w">                </span><span class="nt">Perform</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">o_</span><span class="p">{</span><span class="err">\tau</span><span class="p">}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">on</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">s_</span><span class="p">{</span><span class="err">\tau</span><span class="p">}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">to</span><span class="w"> </span><span class="nt">generate</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">s_</span><span class="p">{</span><span class="err">\tau+1</span><span class="p">}</span><span class="err">\</span><span class="o">)</span>
<span class="w">                </span><span class="nt">Calculate</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">r_</span><span class="p">{</span><span class="err">\tau</span><span class="p">}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">based</span><span class="w"> </span><span class="nt">on</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">s_</span><span class="p">{</span><span class="err">\tau+1</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="nt">o_</span><span class="p">{</span><span class="err">\tau</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="nt">Y</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">and</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">delta</span><span class="err">\</span><span class="o">)</span>
<span class="w">                </span><span class="nt">Store</span><span class="w"> </span><span class="nt">transition</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="nt">s_</span><span class="p">{</span><span class="err">\tau</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="nt">o_</span><span class="p">{</span><span class="err">\tau</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="nt">r_</span><span class="p">{</span><span class="err">\tau</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="nt">s_</span><span class="p">{</span><span class="err">\tau+1</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">in</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">Omega</span><span class="err">\</span><span class="o">)</span>
<span class="w">                </span><span class="nt">Sample</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="nt">s_</span><span class="p">{</span><span class="err">j</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="nt">o_</span><span class="p">{</span><span class="err">j</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="nt">r_</span><span class="p">{</span><span class="err">j</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="nt">s_</span><span class="p">{</span><span class="err">j+1</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">from</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">Omega</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">based</span><span class="w"> </span><span class="nt">on</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">rho</span><span class="err">\</span><span class="o">)</span>
<span class="w">                </span><span class="nt">Set</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">y_</span><span class="p">{</span><span class="err">j</span><span class="p">}</span><span class="o">=</span><span class="w"> </span><span class="err">\</span><span class="nt">begin</span><span class="p">{</span><span class="err">cases</span><span class="p">}</span><span class="nt">r_</span><span class="p">{</span><span class="err">j</span><span class="p">}</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="err">\</span><span class="nt">text</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="err">for</span><span class="w"> </span><span class="err">terminal</span><span class="w"> </span><span class="p">}</span><span class="w"> </span><span class="nt">s_</span><span class="p">{</span><span class="err">j+1</span><span class="p">}</span><span class="w"> </span><span class="err">\\</span><span class="w"> </span><span class="nt">r_</span><span class="p">{</span><span class="err">j</span><span class="p">}</span><span class="o">+</span><span class="err">\</span><span class="nt">gamma</span><span class="w"> </span><span class="err">\</span><span class="nt">max</span><span class="w"> </span><span class="nt">_</span><span class="p">{</span><span class="err">\sigma</span><span class="p">}</span><span class="w"> </span><span class="nt">Q</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="nt">s_</span><span class="p">{</span><span class="err">j+1</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="nt">o</span><span class="o">^</span><span class="p">{</span><span class="err">\prime</span><span class="p">}</span><span class="w"> </span><span class="o">;</span><span class="w"> </span><span class="err">\</span><span class="nt">theta_</span><span class="p">{</span><span class="err">1</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="err">\</span><span class="nt">text</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="err">otherwise</span><span class="w"> </span><span class="p">}</span><span class="err">\</span><span class="nt">end</span><span class="p">{</span><span class="err">cases</span><span class="p">}</span><span class="err">\</span><span class="o">)</span>
<span class="w">                </span><span class="nt">Update</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">theta_</span><span class="p">{</span><span class="err">1</span><span class="p">}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">based</span><span class="w"> </span><span class="nt">on</span><span class="w"> </span><span class="nt">loss</span><span class="w"> </span><span class="nt">function</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">mathcal</span><span class="p">{</span><span class="err">L</span><span class="p">}</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="err">\</span><span class="nt">theta_</span><span class="p">{</span><span class="err">1</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="o">)</span>
<span class="w">            </span><span class="nt">end</span><span class="w"> </span><span class="nt">for</span>
<span class="w">            </span><span class="nt">end</span><span class="w"> </span><span class="nt">for</span>
<span class="w">    </span><span class="nt">end</span><span class="w"> </span><span class="nt">for</span>
<span class="w">    </span><span class="nt">return</span><span class="w"> </span><span class="nt">The</span><span class="w"> </span><span class="nt">parameters</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">theta_</span><span class="p">{</span><span class="err">1</span><span class="p">}</span><span class="err">\</span><span class="o">)</span>
</code></pre></div>

<h2>5 Experiments</h2>
<h3>5.1 Datasets and Evaluation Metric</h3>
<p>We conducted experiments on three datasets, i.e., "Microsoft Windows Help and Support" (WHS) documents [Branavan et al., 2009], and two datasets collected from "WikiHow Home</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">WHS</th>
<th style="text-align: center;">CT</th>
<th style="text-align: center;">WHG</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Labeled texts</td>
<td style="text-align: center;">154</td>
<td style="text-align: center;">116</td>
<td style="text-align: center;">150</td>
</tr>
<tr>
<td style="text-align: left;">Input-output pairs</td>
<td style="text-align: center;">1.5 K</td>
<td style="text-align: center;">134 K</td>
<td style="text-align: center;">34 M</td>
</tr>
<tr>
<td style="text-align: left;">Action name rate (\%)</td>
<td style="text-align: center;">19.47</td>
<td style="text-align: center;">10.37</td>
<td style="text-align: center;">7.61</td>
</tr>
<tr>
<td style="text-align: left;">Action argument rate (\%)</td>
<td style="text-align: center;">15.45</td>
<td style="text-align: center;">7.44</td>
<td style="text-align: center;">6.30</td>
</tr>
<tr>
<td style="text-align: left;">Unlabeled texts</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">80</td>
</tr>
</tbody>
</table>
<p>Table 1: Datasets used in our experiments
and Garden ${ }^{\text {n3 }}$ (WHG) and "CookingTutorial"4 (CT). Details are presented in Table 1. Supervised learning models require that training data are one-to-one pairs (i.e. each word has a unique label), so we generate input-texts-to-output-labels based on annotation $Y$ (as defined in Section 3). In our task, a single text with $n$ optional items or $n$ exclusive pairs can generate more than $2^{n}$ potential label sequences (i.e. each item of them can be extracted or not be extracted). Especially, we observe that $n$ is larger than 30 in some texts of our datasets, which means more than 1 billion sequences will be generated. We thus restrict $n \leq 8$ (no more than $2^{8}$ label sequences) to generate reasonable number of sequences.</p>
<p>For evaluation, we first feed test texts to each model to output sequences of labels or operations. We then extract action sequences based on these labels or operations. After that, we compare these action sequences to their corresponding annotations and calculate #TotalTruth (total ground truth items), #TotalTagged (total extracted items), #TotalRight (total correctly extracted items). Finally we compute metrics: precision $=\frac{# \text { TotalRight }}{# \text { TotalTagged }}, \operatorname{recall}=$ $\frac{# \text { TotalRight }}{# \text { TotalTruth }}$, and $F 1=\frac{2 \times \text { precision } \times \text { recall }}{\text { precision }+ \text { recall }}$. We randomly split each dataset into 10 folds, calculated an average of performance over 10 runs via 10 -fold cross validation, and used the F1 metric to validate the performance in our experiments.</p>
<h3>5.2 Experimental Results</h3>
<p>We compare EASDRL to four baselines, as shown below:</p>
<ul>
<li>STFC: Stanford CoreNLP, an off-the-shelf tool, denoted by STFC, extracts action sequences by viewing root verbs as action names and objects as action arguments [Lindsay et al., 2017].</li>
<li>BLCC: Bi-directional LSTM-CNNs-CRF model [Ma and Hovy, 2016; Reimers and Gurevych, 2017] is a state-of-the-art sequence labeling approach. We finetuned parameters of the approach, including character embedding, embedding size, dropout rate, etc., and denoted the resulting approach by BLCC.</li>
<li>EAD: The Encoder-Aligner-Decoder approach maps instructions to action sequences proposed by [Mei et al., 2016], denoted by EAD.</li>
<li>CMLP: We consider a Combined Multi-layer Perceptron (CMLP), which consists of $N$ MLP classifiers. $N=$ 500 for action names extraction and $N=100$ for action arguments extraction. Each MLP classifier focuses on not only a single word but also the k-gram context.</li>
</ul>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Action Names</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Action Arguments</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Method</td>
<td style="text-align: center;">WHS</td>
<td style="text-align: center;">CT</td>
<td style="text-align: center;">WHG</td>
<td style="text-align: center;">WHS</td>
<td style="text-align: center;">CT</td>
<td style="text-align: center;">WHG</td>
</tr>
<tr>
<td style="text-align: center;">EAD-2</td>
<td style="text-align: center;">86.25</td>
<td style="text-align: center;">64.74</td>
<td style="text-align: center;">53.49</td>
<td style="text-align: center;">57.71</td>
<td style="text-align: center;">51.77</td>
<td style="text-align: center;">37.70</td>
</tr>
<tr>
<td style="text-align: center;">EAD-8</td>
<td style="text-align: center;">85.32</td>
<td style="text-align: center;">61.66</td>
<td style="text-align: center;">48.67</td>
<td style="text-align: center;">57.71</td>
<td style="text-align: center;">51.77</td>
<td style="text-align: center;">37.70</td>
</tr>
<tr>
<td style="text-align: center;">CMLP-2</td>
<td style="text-align: center;">83.15</td>
<td style="text-align: center;">83.00</td>
<td style="text-align: center;">67.36</td>
<td style="text-align: center;">47.29</td>
<td style="text-align: center;">34.14</td>
<td style="text-align: center;">32.54</td>
</tr>
<tr>
<td style="text-align: center;">CMLP-8</td>
<td style="text-align: center;">80.14</td>
<td style="text-align: center;">73.10</td>
<td style="text-align: center;">53.50</td>
<td style="text-align: center;">47.29</td>
<td style="text-align: center;">34.14</td>
<td style="text-align: center;">32.54</td>
</tr>
<tr>
<td style="text-align: center;">BLCC-2</td>
<td style="text-align: center;">90.16</td>
<td style="text-align: center;">80.50</td>
<td style="text-align: center;">69.46</td>
<td style="text-align: center;">93.30</td>
<td style="text-align: center;">$\mathbf{7 6 . 3 3}$</td>
<td style="text-align: center;">70.32</td>
</tr>
<tr>
<td style="text-align: center;">BLCC-8</td>
<td style="text-align: center;">89.95</td>
<td style="text-align: center;">72.87</td>
<td style="text-align: center;">59.63</td>
<td style="text-align: center;">93.30</td>
<td style="text-align: center;">$\mathbf{7 6 . 3 3}$</td>
<td style="text-align: center;">70.32</td>
</tr>
<tr>
<td style="text-align: center;">STFC</td>
<td style="text-align: center;">62.66</td>
<td style="text-align: center;">67.39</td>
<td style="text-align: center;">62.75</td>
<td style="text-align: center;">38.79</td>
<td style="text-align: center;">43.31</td>
<td style="text-align: center;">42.75</td>
</tr>
<tr>
<td style="text-align: center;">EASDRL</td>
<td style="text-align: center;">$\mathbf{9 3 . 4 6}$</td>
<td style="text-align: center;">$\mathbf{8 4 . 1 8}$</td>
<td style="text-align: center;">$\mathbf{7 5 . 4 0}$</td>
<td style="text-align: center;">$\mathbf{9 5 . 0 7}$</td>
<td style="text-align: center;">74.80</td>
<td style="text-align: center;">$\mathbf{7 5 . 0 2}$</td>
</tr>
</tbody>
</table>
<p>Table 2: F1 scores of all types of action names and arguments</p>
<p>When comparing with baselines, we adopt the settings used by [Zhang and Wallace, 2015] to build our CNN networks. We set the input dimension to be $(500 \times 100)$ for action names and $(100 \times 150)$ for action arguments, the number of featuremaps to be 32 . We used 0.25 dropout on the concatenated max pooling outputs and exploited a 256 dimensional fullyconnected layer before the final two dimensional outputs. We set the replay memory $\Omega=100000$, discount factor $\gamma=0.9$. We varied $\rho$ from 0.05 to 0.95 with the interval of 0.05 and found the best value is 0.80 (that is why we set $\rho=0.80$ in the experiment). We set $\delta=0.10$ for action names, $\delta=0.07$ for arguments according to Table 1, the constant $c=50$, learning rate of adam to be 0.001 , probability $\epsilon$ for $\epsilon$-greedy decreasing from 1 to 0.1 over 1000 training steps.</p>
<h2>Comparison with Baselines</h2>
<p>We set the restriction $n=2$ and $n=8$ for EAD, CMLP and BLCC which need one-to-one sequence pairs, and no restriction for STFC and EASDRL. In all of our datasets, the arguments of an action are either all essential arguments or one exclusive argument pair together with all other essential arguments, which means at most $2^{1}$ sequences can be generated. Therefore, the results of action arguments extraction are identical when $n=2$ and $n=8$. The experimental results are shown in Table 2. From Table 2, we can see that EASDRL performs the best on extracting both action names and action arguments in most datasets, except for CT dataset. We observe that the number of arguments in most texts of the CT dataset is very small, such that BLCC performs well on extracting arguments in the CT dataset. On the other hand, we can also observe that BLCC, EAD and CMLP get worse performance when relaxing the restriction on $n(n=2$ and $n=8)$. The reason is that when given a single text with many possible output sequences, these models learn common parts (essential items) of outputs, neglecting the different parts (optional or exclusive items). We can also see that both sequence labeling method and encoder-decoder structure do not work well, which exhibits that, in this task, our reinforcement learning framework can indeed outperform traditional methods.</p>
<p>In order to test and verify whether or not our EASDRL method can deal with complex action types well, we compare with baselines in extracting exclusive action names and exclusive action arguments. Results are shown in Table 3. In this part, our EASDRL model outperforms all baselines and leads more than $5 \%$ absolutely, which demonstrates the effectiveness of our EASDRL model in this task.</p>
<p>We would like to evaluate the impact of additional re-</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Action Names</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Action Arguments</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Method</td>
<td style="text-align: center;">WHS</td>
<td style="text-align: center;">CT</td>
<td style="text-align: center;">WHG</td>
<td style="text-align: center;">WHS</td>
<td style="text-align: center;">CT</td>
<td style="text-align: center;">WHG</td>
</tr>
<tr>
<td style="text-align: center;">EAD-2</td>
<td style="text-align: center;">26.60</td>
<td style="text-align: center;">21.76</td>
<td style="text-align: center;">22.75</td>
<td style="text-align: center;">40.78</td>
<td style="text-align: center;">47.91</td>
<td style="text-align: center;">39.81</td>
</tr>
<tr>
<td style="text-align: center;">EAD-8</td>
<td style="text-align: center;">22.12</td>
<td style="text-align: center;">17.01</td>
<td style="text-align: center;">23.12</td>
<td style="text-align: center;">40.78</td>
<td style="text-align: center;">47.91</td>
<td style="text-align: center;">39.81</td>
</tr>
<tr>
<td style="text-align: center;">CMLP-2</td>
<td style="text-align: center;">31.54</td>
<td style="text-align: center;">54.75</td>
<td style="text-align: center;">51.29</td>
<td style="text-align: center;">35.52</td>
<td style="text-align: center;">25.07</td>
<td style="text-align: center;">29.78</td>
</tr>
<tr>
<td style="text-align: center;">CMLP-8</td>
<td style="text-align: center;">26.90</td>
<td style="text-align: center;">51.80</td>
<td style="text-align: center;">41.03</td>
<td style="text-align: center;">35.52</td>
<td style="text-align: center;">25.07</td>
<td style="text-align: center;">29.78</td>
</tr>
<tr>
<td style="text-align: center;">BLCC-2</td>
<td style="text-align: center;">16.35</td>
<td style="text-align: center;">38.27</td>
<td style="text-align: center;">54.34</td>
<td style="text-align: center;">12.50</td>
<td style="text-align: center;">13.45</td>
<td style="text-align: center;">18.57</td>
</tr>
<tr>
<td style="text-align: center;">BLCC-8</td>
<td style="text-align: center;">19.55</td>
<td style="text-align: center;">35.01</td>
<td style="text-align: center;">41.27</td>
<td style="text-align: center;">12.50</td>
<td style="text-align: center;">13.45</td>
<td style="text-align: center;">18.57</td>
</tr>
<tr>
<td style="text-align: center;">STFC</td>
<td style="text-align: center;">46.40</td>
<td style="text-align: center;">50.28</td>
<td style="text-align: center;">44.32</td>
<td style="text-align: center;">50.00</td>
<td style="text-align: center;">46.40</td>
<td style="text-align: center;">50.32</td>
</tr>
<tr>
<td style="text-align: center;">EASDRL</td>
<td style="text-align: center;">56.19</td>
<td style="text-align: center;">66.37</td>
<td style="text-align: center;">68.29</td>
<td style="text-align: center;">66.67</td>
<td style="text-align: center;">54.24</td>
<td style="text-align: center;">55.67</td>
</tr>
</tbody>
</table>
<p>Table 3: F1 scores of exclusive action names and arguments
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Results of EASDRL ablation studies
ward and positive-rate based experience replay. We test our EASDRL model by removing positive-rate based experience replay (denoted by "-PR") or additional reward (denoted by "-AR"). Results are shown in Figure 4. We observe that removing either positive-rate based experience replay or additional reward degrades the performance of our model.</p>
<h2>Online Training Results</h2>
<p>To further test the robustness and self-learning ability of our approach, we design a human-agent interaction environment to collect the feedback from humans. The environment takes a text as input and present the results of EASDRL to humans. Humans adjust the output results, and the environment updates the deep Q-networks of EASDRL based on humans' adjustment. Before online training, we pre-train an initial model of EASDRL by combining all labeled texts of WHS, CT and WHG, with 30 labeled texts of WHG for testing. The accuracy of this initial model is low since it is domainindependent. We then use the unlabeled texts in WHG (i.e., 80 texts as indicated in the last row in Table 1) for online training. We "invited" humans to provide feedbacks for these 80 texts (with an average of 5 texts for each human). When a human finishes the job assigned to him, we update our model (as well as the baseline model). We compare EASDRL to the best offline-trained baseline BLCC-2. Figure 5 shows the results of online training, where "online collected texts" indicates the number of texts on which humans provide feedbacks. We can see that EASDRL outperforms BLCC-2 significantly, which demonstrates the effectiveness of our reinforcement learning framework.</p>
<h2>6 Conclusion</h2>
<p>In this paper, we proposed a novel approach EASDRL to automatically extract action sequences from texts based on deep reinforcement learning. To the best of our knowledge, EASDRL is the first approach that explores deep reinforcement learning to extract action sequences from texts. We em-</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Online test results of WHG dataset
pirically demonstrated that our EASDRL model outperforms state-of-the-art baselines on three datasets. We showed that EASDRL could better handle complex action types and arguments. We also exhibited the effectiveness of EASDRL in an online learning environment. In the future, it would be interesting to explore the feasibility of learning more structured knowledge from texts such as state sequences or action models for supporting planning.</p>
<h2>Acknowledgements</h2>
<p>Zhuo thanks the National Key Research and Development Program of China (2016YFB0201900), National Natural Science Foundation of China (U1611262), Guangdong Natural Science Funds for Distinguished Young Scholar (2017A030306028), Pearl River Science and Technology New Star of Guangzhou, and Guangdong Province Key Laboratory of Big Data Analysis and Processing for the support of this research. Kambhampati's research is supported in part by the AFOSR grant FA9550-18-1-0067, ONR grants N00014161-2892, N00014-13-1-0176, N00014- 13-1-0519, N00014-15-1-2027, and the NASA grant NNX17AD06G.</p>
<h2>References</h2>
<p>[Branavan et al., 2009] S. R. K. Branavan, Harr Chen, Luke S. Zettlemoyer, and Regina Barzilay. Reinforcement learning for mapping instructions to actions. In ACL, 2009.
[Chen and Mooney, 2011] David L. Chen and Raymond J. Mooney. Learning to interpret natural language navigation instructions from observations. In AAAI, 2011.
[Cresswell et al., 2009] Stephen Cresswell, Thomas Leo McCluskey, and Margaret Mary West. Acquisition of object-centred domain models from planning examples. In ICAPS, 2009.
[Daniele et al., 2017] Andrea F Daniele, Mohit Bansal, and Matthew R Walter. Navigational instruction generation as inverse reinforcement learning with neural machine translation. In HRI, 2017.
[Kim and Mooney, 2013] Joohyun Kim and Raymond J. Mooney. Unsupervised pcfg induction for grounded language learning with highly ambiguous supervision. In EMNLP, 2013.
[Kim, 2014] Yoon Kim. Convolutional neural networks for sentence classification. In EMNLP, 2014.
[Lindsay et al., 2017] Alan Lindsay, Jonathon Read, Joo F. Ferreira, Thomas Hayton, Julie Porteous, and Peter Gregory. Framer: Planning models from natural language action descriptions. In ICAPS, 2017.
[Ma and Hovy, 2016] Xuezhe Ma and Eduard H. Hovy. End-to-end sequence labeling via bi-directional lstm-cnns-crf. In $A C L, 2016$.
[Macmahon et al., 2006] Matt Macmahon, Brian Stankiewicz, and Benjamin Kuipers. Walk the talk: Connecting language, knowledge, and action in route instructions. In AAAI, 2006.
[Mei et al., 2016] Hongyuan Mei, Mohit Bansal, and Matthew R. Walter. Listen, attend, and walk: neural mapping of navigational instructions to action sequences. In AAAI, 2016.
[Mikolov et al., 2013] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations of words and phrases and their compositionality. In NIPS, 2013.
[Mnih et al., 2015] V Mnih, K Kavukcuoglu, D Silver, A. A. Rusu, J Veness, M. G. Bellemare, A Graves, M Riedmiller, A. K. Fidjeland, and G Ostrovski. Human-level control through deep reinforcement learning. Nature, 518(7540):529-33, 2015.
[Narasimhan et al., 2015] Karthik Narasimhan, Tejas D. Kulkarni, and Regina Barzilay. Language understanding for text-based games using deep reinforcement learning. In EMNLP, 2015.
[Pomarlan et al., 2017] Mihai Pomarlan, Sebastian Koralewski, and Michael Beetz. From natural language instructions to structured robot plans. In $K I, 2017$.
[Reimers and Gurevych, 2017] Nils Reimers and Iryna Gurevych. Reporting score distributions makes a difference: Performance study of lstm-networks for sequence tagging. In EMNLP, 2017.
[Sil and Yates, 2011] Avirup Sil and Alexander Yates. Extracting STRIPS representations of actions and events. In RANLP, 2011.
[Sil et al., 2010] Avirup Sil, Fei Huang, and Alexander Yates. Extracting action and event semantics from web text. In AAAI, 2010.
[Wang et al., 2017] Jin Wang, Zhongyuan Wang, Dawei Zhang, and Jun Yan. Combining knowledge with deep convolutional neural networks for short text classification. In IJCAI, 2017.
[Zhang and Wallace, 2015] Ye Zhang and Byron C. Wallace. A sensitivity analysis of (and practitioners' guide to) convolutional neural networks for sentence classification. CoRR, abs/1510.03820, 2015.
[Zhuo and Kambhampati, 2017] Hankz Hankui Zhuo and Subbarao Kambhampati. Model-lite planning: Case-based vs. model-based approaches. Artif. Intell., 246:1-21, 2017.
[Zhuo and Yang, 2014] Hankz Hankui Zhuo and Qiang Yang. Action-model acquisition for planning via transfer learning. Artif. Intell., 212:80-103, 2014.
[Zhuo et al., 2014] Hankz Hankui Zhuo, Hctor MuozAvila, and Qiang Yang. Learning hierarchical task network domains from partially observed plan traces. Artif. Intell., 212:134-157, 2014.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ https://www.wikihow.com/Category:Home-and-Garden
${ }^{4}$ http://cookingtutorials.com/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
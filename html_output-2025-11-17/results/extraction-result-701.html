<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-701 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-701</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-701</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-20.html">extraction-schema-20</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <p><strong>Paper ID:</strong> paper-272770520</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2409.13423v1.pdf" target="_blank">Causal Reinforcement Learning for Optimisation of Robot Dynamics in Unknown Environments</a></p>
                <p><strong>Paper Abstract:</strong> Autonomous operations of robots in unknown environments are challenging due to the lack of knowledge of the dynamics of the interactions, such as the objects' movability. This work introduces a novel Causal Reinforcement Learning approach to enhancing robotics operations and applies it to an urban search and rescue (SAR) scenario. Our proposed machine learning architecture enables robots to learn the causal relationships between the visual characteristics of the objects, such as texture and shape, and the objects' dynamics upon interaction, such as their movability, significantly improving their decision-making processes. We conducted causal discovery and RL experiments demonstrating the Causal RL's superior performance, showing a notable reduction in learning times by over 24.5% in complex situations, compared to non-causal models.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e701.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e701.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NOTEARS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DAGs with NO TEARS (NOTEARS) / Continuous optimization for structure learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A continuous optimization method for causal structure learning that casts the acyclicity constraint as a smooth function and uses gradient-based augmented Lagrangian optimization with sparsity regularization to recover weighted adjacency matrices (DAGs).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Dags with no tears: Continuous optimization for structure learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>NOTEARS</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Solves structure learning by representing the adjacency matrix W and enforcing acyclicity via a smooth constraint h(W)=tr(e^{W•W}) - d = 0, then optimizes a score (e.g., least squares for linear SEMs) subject to that constraint using augmented Lagrangian and gradient-based optimizers. Typically assumes a linear SEM, Gaussian noise, no hidden confounders, and applies L1 (or similar) regularization to promote sparse graphs. Outputs a weighted adjacency matrix that can be thresholded to produce a DAG.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Synthetic 2-3 variable datasets (offline) and Gymnasium 2D SAR interactive environment (data collected from agent interactions / Digital Mind)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Causal discovery experiments used synthetically generated observational datasets representing small 'universes' (2 or 3 variables, binary categories) to compare inferred graphs against ground truth; in the robotic SAR experiments, features (texture, shape, movability labels) collected by the agent's interactions (Digital Mind) were used as input to NOTEARS for structure learning. The Gymnasium SAR environment is interactive and permits the agent to actively interact with objects to collect data, but NOTEARS itself is applied as an offline structure-learning optimizer on collected observations.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Irrelevant/independent variables (handled only indirectly via sparsity regularization); assumes absence of hidden confounders and Gaussian noise, so confounding and other structural spurious signals are not explicitly addressed.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Spurious edges / incorrect structure are detected/evaluated via Structural Hamming Distance (SHD) and precision against known ground-truth graphs in synthetic experiments; NOTEARS does not include an internal explicit distractor-detection mechanism beyond regularization.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Reported that for 2-variable scenarios NOTEARS achieves ~0.9 precision and ~0.3 SHD with approximately 13.5 observations; for 3-variable scenarios stable recovery requires roughly 16 observations; also reported minimum samples for 75% precision (1 sample for 2 variables to reach initial 0.75, 4 samples for 3 variables).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td>One additional independent variable in 3-variable experiments (i.e., scenarios comparing 2 vs 3 variables where the extra variable can be independent/distracting).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>NOTEARS was used successfully to recover small causal graphs from synthetic and interaction-collected data; recovery quality (precision and SHD) degrades / requires more samples as the number of variables (and thus potential spurious permutations) increases — e.g., two-variable graphs converge with far fewer samples than three-variable graphs. The method assumes no hidden confounders and uses regularization to prefer sparse solutions, but the paper does not introduce explicit mechanisms to detect or refute spurious confounding or distractor variables beyond evaluation against ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Causal Reinforcement Learning for Optimisation of Robot Dynamics in Unknown Environments', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e701.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e701.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Causal curiosity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Causal curiosity: RL agents discovering self-supervised experiments for causal representation learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An intrinsic-reward based approach that encourages RL agents to design and execute experiments to discover causal factors in the environment, prioritizing exploratory interventions over task reward to learn causal dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Causal curiosity: Rl agents discovering self-supervised experiments for causal representation learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Causal curiosity (intrinsic reward for causal discovery)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Provides an intrinsic reward signal to an agent that incentivizes performing sequences of actions (experiments) that reveal causal structure in environment dynamics; the aim is self-supervised discovery of causal factors rather than maximizing external task reward. Reported to reduce the data required by downstream RL by a reported factor (~2.5x) in the cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>General interactive RL environments (as cited in related work)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Designed for interactive, experiment-capable environments where the agent can intervene and observe outcomes; explicitly encourages active experimentation to reveal causal mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td>Intrinsic-reward-driven exploration that actively seeks experiments expected to reveal causal factors (the agent selects actions/interventions guided by the intrinsic curiosity reward).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Cited claim that agents using causal curiosity can reduce the data required by RL agents by ~2.5x (as reported in the referenced paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Referenced as a promising approach to enable agents to discover causal processes through active exploratory interaction; the method emphasizes active experimentation which could help disambiguate spurious correlations in interactive settings, but this specific paper only cites the method and does not evaluate or implement its distractor-robust features.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Causal Reinforcement Learning for Optimisation of Robot Dynamics in Unknown Environments', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e701.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e701.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CausalDiscoveryToolbox (CDT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Causal Discovery Toolbox</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Python toolbox that implements a collection of causal discovery algorithms (including NOTEARS and others) and evaluation metrics for causal structure learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Causal discovery toolbox: Uncovering causal relationships in python.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>CausalDiscoveryToolbox (software)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>A software library bundling a range of causal discovery algorithms (constraint-based, score-based, continuous optimization approaches like NOTEARS, etc.) and utilities for preprocessing, evaluation, and visualization; used here as part of the implementation stack for running NOTEARS and computing metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Synthetic dataset experiments (offline) used in causal discovery section</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Applied to synthetic observational datasets produced for small-variable universes; not itself an environment but the implementation platform for running algorithms on those datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Used as part of the implementation stack to run causal discovery experiments and compute SHD/precision; the paper does not report CDT-specific distractor-robust algorithms beyond those available in the toolbox.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Causal Reinforcement Learning for Optimisation of Robot Dynamics in Unknown Environments', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e701.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e701.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CausalNex (Bayesian model)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CausalNex (Bayesian network/causal inference library)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Python library for building and reasoning with Bayesian networks and performing structure learning and probabilistic inference, used here to convert learned structure into causal probability estimates for object movability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>CausalNex (library + Bayesian modeling)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Provides Bayesian-network modeling and probabilistic inference tools; in this work the structural model from NOTEARS was fed into a Bayesian model (via CausalNex) to compute causal probabilities (e.g., probability that an object type is movable) used by the agent's Digital Mind.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Digital Mind / RL observation augmentation in Gymnasium SAR environment</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Used to convert structure learned by NOTEARS (from object-feature observations) into probabilistic estimates of movability that augment the RL agent's observation space; the overall environment is an interactive SAR grid where the agent physically interacts with objects.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>CausalNex was used to translate NOTEARS-inferred structure into causal probability estimates for object movability which were then provided to the A2C agent; the paper does not describe explicit distractor-robust procedures inside CausalNex for this use.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Causal Reinforcement Learning for Optimisation of Robot Dynamics in Unknown Environments', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e701.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e701.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SHD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Structural Hamming Distance</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph-distance metric that counts the number of edge insertions, deletions, and orientation errors required to transform an inferred graph into the ground-truth graph; used to quantify causal discovery errors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Structural Hamming Distance (SHD) metric</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Computes the total count of edge-wise discrepancies between inferred and true DAGs (missing edges, extra edges, reversed orientations); lower SHD indicates closer agreement with ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Synthetic causal discovery experiments (ground-truth graphs provided)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Used to evaluate inferred causal structures from synthetic datasets under multiple small-variable universes; serves as a detector of incorrect/spurious edges when ground truth is available.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Extra/spurious edges and misorientations in inferred graph (i.e., false positives/false orientations).</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Direct comparison to ground-truth graph; counts incorrect edges/orientations to signal spurious inferences.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Used to report convergence/stability of inferred graphs; examples: ~0.3 SHD reported for stable 2-variable recovery with ~13.5 observations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>SHD was the primary metric for detecting spurious and incorrect edges in synthetic experiments; the paper uses SHD alongside precision to quantify how sample size and number of variables affect the presence of spurious edges and overall structural accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Causal Reinforcement Learning for Optimisation of Robot Dynamics in Unknown Environments', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Dags with no tears: Continuous optimization for structure learning <em>(Rating: 2)</em></li>
                <li>Causal curiosity: Rl agents discovering self-supervised experiments for causal representation learning <em>(Rating: 2)</em></li>
                <li>Causal discovery toolbox: Uncovering causal relationships in python. <em>(Rating: 2)</em></li>
                <li>Review of causal discovery methods based on graphical models <em>(Rating: 2)</em></li>
                <li>A survey on causal discovery methods for iid and time series data <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-701",
    "paper_id": "paper-272770520",
    "extraction_schema_id": "extraction-schema-20",
    "extracted_data": [
        {
            "name_short": "NOTEARS",
            "name_full": "DAGs with NO TEARS (NOTEARS) / Continuous optimization for structure learning",
            "brief_description": "A continuous optimization method for causal structure learning that casts the acyclicity constraint as a smooth function and uses gradient-based augmented Lagrangian optimization with sparsity regularization to recover weighted adjacency matrices (DAGs).",
            "citation_title": "Dags with no tears: Continuous optimization for structure learning",
            "mention_or_use": "use",
            "method_name": "NOTEARS",
            "method_description": "Solves structure learning by representing the adjacency matrix W and enforcing acyclicity via a smooth constraint h(W)=tr(e^{W•W}) - d = 0, then optimizes a score (e.g., least squares for linear SEMs) subject to that constraint using augmented Lagrangian and gradient-based optimizers. Typically assumes a linear SEM, Gaussian noise, no hidden confounders, and applies L1 (or similar) regularization to promote sparse graphs. Outputs a weighted adjacency matrix that can be thresholded to produce a DAG.",
            "environment_name": "Synthetic 2-3 variable datasets (offline) and Gymnasium 2D SAR interactive environment (data collected from agent interactions / Digital Mind)",
            "environment_description": "Causal discovery experiments used synthetically generated observational datasets representing small 'universes' (2 or 3 variables, binary categories) to compare inferred graphs against ground truth; in the robotic SAR experiments, features (texture, shape, movability labels) collected by the agent's interactions (Digital Mind) were used as input to NOTEARS for structure learning. The Gymnasium SAR environment is interactive and permits the agent to actively interact with objects to collect data, but NOTEARS itself is applied as an offline structure-learning optimizer on collected observations.",
            "handles_distractors": false,
            "distractor_handling_technique": null,
            "spurious_signal_types": "Irrelevant/independent variables (handled only indirectly via sparsity regularization); assumes absence of hidden confounders and Gaussian noise, so confounding and other structural spurious signals are not explicitly addressed.",
            "detection_method": "Spurious edges / incorrect structure are detected/evaluated via Structural Hamming Distance (SHD) and precision against known ground-truth graphs in synthetic experiments; NOTEARS does not include an internal explicit distractor-detection mechanism beyond regularization.",
            "downweighting_method": null,
            "refutation_method": null,
            "uses_active_learning": null,
            "inquiry_strategy": null,
            "performance_with_robustness": "Reported that for 2-variable scenarios NOTEARS achieves ~0.9 precision and ~0.3 SHD with approximately 13.5 observations; for 3-variable scenarios stable recovery requires roughly 16 observations; also reported minimum samples for 75% precision (1 sample for 2 variables to reach initial 0.75, 4 samples for 3 variables).",
            "performance_without_robustness": null,
            "has_ablation_study": false,
            "number_of_distractors": "One additional independent variable in 3-variable experiments (i.e., scenarios comparing 2 vs 3 variables where the extra variable can be independent/distracting).",
            "key_findings": "NOTEARS was used successfully to recover small causal graphs from synthetic and interaction-collected data; recovery quality (precision and SHD) degrades / requires more samples as the number of variables (and thus potential spurious permutations) increases — e.g., two-variable graphs converge with far fewer samples than three-variable graphs. The method assumes no hidden confounders and uses regularization to prefer sparse solutions, but the paper does not introduce explicit mechanisms to detect or refute spurious confounding or distractor variables beyond evaluation against ground truth.",
            "uuid": "e701.0",
            "source_info": {
                "paper_title": "Causal Reinforcement Learning for Optimisation of Robot Dynamics in Unknown Environments",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Causal curiosity",
            "name_full": "Causal curiosity: RL agents discovering self-supervised experiments for causal representation learning",
            "brief_description": "An intrinsic-reward based approach that encourages RL agents to design and execute experiments to discover causal factors in the environment, prioritizing exploratory interventions over task reward to learn causal dynamics.",
            "citation_title": "Causal curiosity: Rl agents discovering self-supervised experiments for causal representation learning",
            "mention_or_use": "mention",
            "method_name": "Causal curiosity (intrinsic reward for causal discovery)",
            "method_description": "Provides an intrinsic reward signal to an agent that incentivizes performing sequences of actions (experiments) that reveal causal structure in environment dynamics; the aim is self-supervised discovery of causal factors rather than maximizing external task reward. Reported to reduce the data required by downstream RL by a reported factor (~2.5x) in the cited work.",
            "environment_name": "General interactive RL environments (as cited in related work)",
            "environment_description": "Designed for interactive, experiment-capable environments where the agent can intervene and observe outcomes; explicitly encourages active experimentation to reveal causal mechanisms.",
            "handles_distractors": null,
            "distractor_handling_technique": null,
            "spurious_signal_types": null,
            "detection_method": null,
            "downweighting_method": null,
            "refutation_method": null,
            "uses_active_learning": true,
            "inquiry_strategy": "Intrinsic-reward-driven exploration that actively seeks experiments expected to reveal causal factors (the agent selects actions/interventions guided by the intrinsic curiosity reward).",
            "performance_with_robustness": "Cited claim that agents using causal curiosity can reduce the data required by RL agents by ~2.5x (as reported in the referenced paper).",
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "Referenced as a promising approach to enable agents to discover causal processes through active exploratory interaction; the method emphasizes active experimentation which could help disambiguate spurious correlations in interactive settings, but this specific paper only cites the method and does not evaluate or implement its distractor-robust features.",
            "uuid": "e701.1",
            "source_info": {
                "paper_title": "Causal Reinforcement Learning for Optimisation of Robot Dynamics in Unknown Environments",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "CausalDiscoveryToolbox (CDT)",
            "name_full": "Causal Discovery Toolbox",
            "brief_description": "A Python toolbox that implements a collection of causal discovery algorithms (including NOTEARS and others) and evaluation metrics for causal structure learning.",
            "citation_title": "Causal discovery toolbox: Uncovering causal relationships in python.",
            "mention_or_use": "use",
            "method_name": "CausalDiscoveryToolbox (software)",
            "method_description": "A software library bundling a range of causal discovery algorithms (constraint-based, score-based, continuous optimization approaches like NOTEARS, etc.) and utilities for preprocessing, evaluation, and visualization; used here as part of the implementation stack for running NOTEARS and computing metrics.",
            "environment_name": "Synthetic dataset experiments (offline) used in causal discovery section",
            "environment_description": "Applied to synthetic observational datasets produced for small-variable universes; not itself an environment but the implementation platform for running algorithms on those datasets.",
            "handles_distractors": null,
            "distractor_handling_technique": null,
            "spurious_signal_types": null,
            "detection_method": null,
            "downweighting_method": null,
            "refutation_method": null,
            "uses_active_learning": null,
            "inquiry_strategy": null,
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "Used as part of the implementation stack to run causal discovery experiments and compute SHD/precision; the paper does not report CDT-specific distractor-robust algorithms beyond those available in the toolbox.",
            "uuid": "e701.2",
            "source_info": {
                "paper_title": "Causal Reinforcement Learning for Optimisation of Robot Dynamics in Unknown Environments",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "CausalNex (Bayesian model)",
            "name_full": "CausalNex (Bayesian network/causal inference library)",
            "brief_description": "A Python library for building and reasoning with Bayesian networks and performing structure learning and probabilistic inference, used here to convert learned structure into causal probability estimates for object movability.",
            "citation_title": "",
            "mention_or_use": "use",
            "method_name": "CausalNex (library + Bayesian modeling)",
            "method_description": "Provides Bayesian-network modeling and probabilistic inference tools; in this work the structural model from NOTEARS was fed into a Bayesian model (via CausalNex) to compute causal probabilities (e.g., probability that an object type is movable) used by the agent's Digital Mind.",
            "environment_name": "Digital Mind / RL observation augmentation in Gymnasium SAR environment",
            "environment_description": "Used to convert structure learned by NOTEARS (from object-feature observations) into probabilistic estimates of movability that augment the RL agent's observation space; the overall environment is an interactive SAR grid where the agent physically interacts with objects.",
            "handles_distractors": null,
            "distractor_handling_technique": null,
            "spurious_signal_types": null,
            "detection_method": null,
            "downweighting_method": null,
            "refutation_method": null,
            "uses_active_learning": null,
            "inquiry_strategy": null,
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "CausalNex was used to translate NOTEARS-inferred structure into causal probability estimates for object movability which were then provided to the A2C agent; the paper does not describe explicit distractor-robust procedures inside CausalNex for this use.",
            "uuid": "e701.3",
            "source_info": {
                "paper_title": "Causal Reinforcement Learning for Optimisation of Robot Dynamics in Unknown Environments",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "SHD",
            "name_full": "Structural Hamming Distance",
            "brief_description": "A graph-distance metric that counts the number of edge insertions, deletions, and orientation errors required to transform an inferred graph into the ground-truth graph; used to quantify causal discovery errors.",
            "citation_title": "",
            "mention_or_use": "use",
            "method_name": "Structural Hamming Distance (SHD) metric",
            "method_description": "Computes the total count of edge-wise discrepancies between inferred and true DAGs (missing edges, extra edges, reversed orientations); lower SHD indicates closer agreement with ground truth.",
            "environment_name": "Synthetic causal discovery experiments (ground-truth graphs provided)",
            "environment_description": "Used to evaluate inferred causal structures from synthetic datasets under multiple small-variable universes; serves as a detector of incorrect/spurious edges when ground truth is available.",
            "handles_distractors": null,
            "distractor_handling_technique": null,
            "spurious_signal_types": "Extra/spurious edges and misorientations in inferred graph (i.e., false positives/false orientations).",
            "detection_method": "Direct comparison to ground-truth graph; counts incorrect edges/orientations to signal spurious inferences.",
            "downweighting_method": null,
            "refutation_method": null,
            "uses_active_learning": null,
            "inquiry_strategy": null,
            "performance_with_robustness": "Used to report convergence/stability of inferred graphs; examples: ~0.3 SHD reported for stable 2-variable recovery with ~13.5 observations.",
            "performance_without_robustness": null,
            "has_ablation_study": false,
            "number_of_distractors": null,
            "key_findings": "SHD was the primary metric for detecting spurious and incorrect edges in synthetic experiments; the paper uses SHD alongside precision to quantify how sample size and number of variables affect the presence of spurious edges and overall structural accuracy.",
            "uuid": "e701.4",
            "source_info": {
                "paper_title": "Causal Reinforcement Learning for Optimisation of Robot Dynamics in Unknown Environments",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Dags with no tears: Continuous optimization for structure learning",
            "rating": 2,
            "sanitized_title": "dags_with_no_tears_continuous_optimization_for_structure_learning"
        },
        {
            "paper_title": "Causal curiosity: Rl agents discovering self-supervised experiments for causal representation learning",
            "rating": 2,
            "sanitized_title": "causal_curiosity_rl_agents_discovering_selfsupervised_experiments_for_causal_representation_learning"
        },
        {
            "paper_title": "Causal discovery toolbox: Uncovering causal relationships in python.",
            "rating": 2,
            "sanitized_title": "causal_discovery_toolbox_uncovering_causal_relationships_in_python"
        },
        {
            "paper_title": "Review of causal discovery methods based on graphical models",
            "rating": 2,
            "sanitized_title": "review_of_causal_discovery_methods_based_on_graphical_models"
        },
        {
            "paper_title": "A survey on causal discovery methods for iid and time series data",
            "rating": 1,
            "sanitized_title": "a_survey_on_causal_discovery_methods_for_iid_and_time_series_data"
        }
    ],
    "cost": 0.01345475,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Causal Reinforcement Learning for Optimisation of Robot Dynamics in Unknown Environments</p>
<p>Julian Gerald Dcruz 
School of Aerospace, Transport and Manufacturing
Cranfield University United Kingdom</p>
<p>JiaSam Mahoney 
School of Aerospace, Transport and Manufacturing
Cranfield University United Kingdom</p>
<p>Yun Chua 
School of Aerospace, Transport and Manufacturing
Cranfield University United Kingdom</p>
<p>Adoundeth Soukhabandith 
School of Aerospace, Transport and Manufacturing
Cranfield University United Kingdom</p>
<p>John Mugabe 
School of Aerospace, Transport and Manufacturing
Cranfield University United Kingdom</p>
<p>Weisi Guo 
School of Aerospace, Transport and Manufacturing
Cranfield University United Kingdom</p>
<p>Miguel Arana-Catania 
School of Aerospace, Transport and Manufacturing
Cranfield University United Kingdom</p>
<p>Causal Reinforcement Learning for Optimisation of Robot Dynamics in Unknown Environments
86A0F10892630B33A7C570F5C28B0F18causal learningreinforcement learningautonomous systemsrobotics
Autonomous operations of robots in unknown environments are challenging due to the lack of knowledge of the dynamics of the interactions, such as the objects' movability.This work introduces a novel Causal Reinforcement Learning approach to enhancing robotics operations and applies it to an urban search and rescue (SAR) scenario.Our proposed machine learning architecture enables robots to learn the causal relationships between the visual characteristics of the objects, such as texture and shape, and the objects' dynamics upon interaction, such as their movability, significantly improving their decision-making processes.We conducted causal discovery and RL experiments demonstrating the Causal RL's superior performance, showing a notable reduction in learning times by over 24.5% in complex situations, compared to non-causal models.</p>
<p>INTRODUCTION</p>
<p>Recently, the convergence of causality and reinforcement learning (RL) has become a vibrant area for progress, especially in crafting autonomous systems.Causality [1][2][3], the study of cause and effect, offers a more nuanced understanding of the environment than traditional correlation-based approaches.In RL, an agent learns to make decisions by interacting with its environment and receiving feedback in the form of rewards or penalties.Integrating causality into this framework allows agents to infer the causal relationships between their actions and the outcomes, leading to more robust decision-making processes.This causal understanding is crucial for tasks requiring long-term planning and for operating in dynamic or previously unseen environments.This project aims at enhancing efficiency in critical situations such as search and rescue (SAR) operations in smart cities using Causal Reinforcement Learning.To address this, we have developed a novel methodology that allows robots to learn about these causal relationships through direct interaction with their surroundings.By observing and manipulating objects, the robots can discern patterns and connections between what they see and how things move.This equips the robots with the foresight to anticipate outcomes and adapt swiftly, thereby accelerating the completion of their tasks and improving global multi-agent understanding of a collapsed world model during diverse disasters.</p>
<p>II. RELATED WORK</p>
<p>Yin et al. [4] aimed to improve the navigation of an autonomous agent without prior knowledge of the environment using off-policy RL.They used a Soft Actor-Critic with Curriculum Prioritization and Fuzzy This work was supported by EPSRC "TAS-S: Trustworthy Autonomous Systems: Security" (EP/V026763/1) Logic to assess and plan the navigation trajectory of the agent.To counteract the sim-to-real transfer problem, the authors proposed the use of Generative Adversarial Networks [5].Similarly, regarding the use of artificial intelligence to assist SAR missions.Zuluaga et al. [6] have adapted an RL module for UAVs to improve the efficiency of SAR missions.However, most existing UAVs performing these tasks rely on greedy or potential-based heuristics without the ability to learn.These approaches are often inaccurate in real-world applications; furthermore, they require knowledge of the search space beforehand.The authors have also acknowledged the difficulties of using this agent in high-dimensional continuous state/action spaces where the RL agent finds it difficult to find an optimal policy.In this project, we aim to further reinforce this by applying causal learning [7][8][9] to the agent on top of an RL module to further improve its efficiency.Sontakke et al. [10] proposed causal curiosity as "a novel intrinsic reward", capable of allowing an agent "to learn optimal sequences of actions and discover causal factors in the dynamics of the environment".They found that agents using causal learning can reduce the data required by the RL agents 2.5 times.The goal of an agent with causal curiosity is to allow agents to discover causal processes through exploratory interaction rather than focusing on maximizing the task reward.This project is focused on causal learning in enhancing robots' decisions [11] in SAR missions.Our causal learning work will apply the NOTEARS (Non-combinatorial Optimization via Trace Exponential and Augmented lagRangian for Structure learning) algorithm [12].It transforms the challenge of learning a causal graph structure into a smooth, solvable optimization problem using gradient-based techniques.The benefits of this approach include scalability to larger datasets and a more efficient convergence towards accurate causality models.It assumes that the noise factors are Gaussian and that there are no hidden confounders.NOTEARS favours simpler, sparse graphs, employing regularization to reinforce this sparsity.Conventional parametric models presuppose predetermined mathematical relationships between the variables, while this nonparametric model is more able to conform to the underlying data structure.Hu, T. [13] has applied simultaneous localization and mapping (SLAM), navigation algorithms, and a YOLOv7 neural network for object detection and depth estimation to create a robot capable of exploring unknown dynamic environments.SLAM can create a realistic map of the robot's surroundings and YOLOv7 is then used to detect humans in the environment; the robot then tries to predict the human's trajectory in the environment.An alternative way to localize the robot in its environment and map its surroundings is the long-term static mapping (LSM) and cloning localization (CL) method using a 3D LiDAR sensor [14].LSM oversees the creation of an initial static 2D grid map and 3D feature map, whilst CL tracks and matches the 3D feature map to the dynamic changes in the environment.The Robot Sensing Module harnesses sensor data from two primary sources: RGB visual sensors and position sensors, providing an understanding of the robot's environment, and its orientation and movement.The collected data undergo feature extraction, where characteristics such as texture, shape, and pixel attributes are discerned, along with histogram analysis and the detection of obstacles.These data allow it to distinguish between different materials and surfaces and helps the robot make informed decisions about how to interact with different objects.Post-extraction, feature pooling is utilised to reduce dimensionality, preparing the data for more efficient processing.This ability is crucial in disaster scenarios where quick assessment and identification of survivors and paths through debris are essential.It should also be noted that the integration of the Sensing Module with the Digital Mind allows for a dynamic feedback loop where visual data inform the robot's causal learning processes and enables the robot to adapt its strategies based on visual clues and learned experiences.</p>
<p>III. SYSTEM DESCRIPTION</p>
<p>A. System Overview</p>
<p>The RL Module enables the robot to move and interact with the objects in the environment.As the robot is deployed in an unknown environment, it must autonomously understand and navigate through it.RL involves learning to make decisions by taking actions that maximise a cumulative reward.Here we employ an Advantage Actor-Critic (A2C) algorithm [15].It makes use of two models: The actor takes actions in the environment based on its current policy and the critic assesses the action by calculating the value function of the resulting state generating a temporal difference error.</p>
<p>The Digital Mind allows the robot to perceive, understand, and interact with its surroundings meaningfully.It integrates sensor data to create a multidimensional representation of the surroundings, aiding navigation and interaction by identifying obstacles and spatial layouts.Central to this project is its ability for causal learning, where the robot engages with objects and observes outcomes to deduce physical laws.By pushing and moving objects, it learns causal relationships and predicts action consequences, forming Directed Acyclic Graphs (DAG) to model these relationships.The causal discovery [7][8][9] algorithm used is NOTEARS.The module estimates event probabilities based on experiences and observed causal relationships.Additionally, it interacts with the RL Module, updating digital world representations influenced by the robot's actions, allowing ongoing learning and refinement of causal models and decision-making processes.</p>
<p>A digital mind vector is initialised at the start of each episode, which stores the observations of the robot.The digital mind vector helps keep track of the object types the robot has interacted with, its previous actions, the movement status of the object if it has moved upon interaction, and the causal probability of the object.The digital mind vector is thus part of the robot's memory system.The movement status of the objects is tracked by checking if the object has moved upon interaction.Initially, with an unformed causal model, the robot is implemented to assume all objects have a 50% chance of being movable.This allows for updating each object type in the digital mind with their movement status.Each object texture type with its movement status is pulled from the digital mind and fed as input to the NOTEARS algorithm.The structural model created from the NOTEARS algorithm is then fed into the Bayesian model to predict the causal probability of the object being movable.The A2C RL algorithm with a Multi-Input Policy generates actions to be taken by the robot in the environment.The observation space information includes object positions, relative goal positions, collision data, and the history of previous actions.The robot's action space is restricted to forwards, backwards, left, and right.Inputs from computer vision, the digital mind, and causal probability contribute to constructing the robot's observation space.</p>
<p>IV. EXPERIMENTS</p>
<p>To simulate a SAR mission, we implemented an environment with varying numbers of objects with different types of textures and shapes.A robot with a camera had to find its way to the trapped individual/goal by moving the blocked objects in its pathway or avoiding immovable objects in its pathway.In the experiments, we test independently the two main modules of our architecture.The code of the experiments is publicly available 1 .</p>
<p>A. Causal Discovery</p>
<p>This experiment is designed to assess the effectiveness of the NOTEARS causal discovery algorithm in accurately inferring from data the causal relationships between the texture and shape of the objects and their movability.It evaluates the number of samples (here interactions with the objects) required for the algorithm to consistently infer the true causal graph.Causal relationships for all scenarios analysed Fig. 2 illustrates the diverse universes under investigation, encompassing scenarios with two and three variables where texture or shape are causally related or not to movability.For each of these scenarios, synthetic datasets are generated with varying numbers of samples to mimic different quantities of observational data.Data are randomly produced across two categories for each of ten distinct datasets per data point.</p>
<p>For each dataset size, the NOTEARS algorithm is applied to infer the causal graph.This is performed 10 times for 10 different datasets to introduce variability and robustness in the results.The Structural Hamming Distance (SHD) and precision metrics are calculated for each inferred graph compared to the true graph.The SHD quantifies the number of discrepancies between the inferred and true graphs, with 0 indicating a perfect match.Precision measures the proportion of correctly inferred edges out of all edges inferred by the algorithm, with 1 indicating all inferences are correct.The implementation is done using CausalNex 2 , NetworkX 3 , and CausalDiscoveryToolbox4 [16].</p>
<p>B. Causal Discovery Results</p>
<p>This section showcases the SHD and precision over the number of samples under the various universes.Sample size requirement for 75% precision.</p>
<p>To explore whether an increase in variables impacts the minimum number of samples needed, we selected a graph in which one variable (such as texture) is causally linked to movability.We then incrementally added new independent variables (increasing the node count) and recorded the minimum number of samples required to achieve 75% precision.Fig. 8 shows the minimum samples needed for 75% precision as the number of variables increases.</p>
<p>C. Causal Discovery Discussion</p>
<p>The previous results indicate that, for two variables, fewer samples are typically required to reach high precision.In contrast, the graph involving three variables shows increased variability due to a greater number of potential permutations.This requires a larger sample size to ensure stable convergence with high precision/low SHD.Fig. 8 illustrates that to reach an initial precision of 0.75, only one sample is needed for two variables, while three variables require four samples.Additionally, beyond initial precision, the analysis also delved into stability, examining how consistently the results converge.The data suggest that an approximate quantity of 13.5 observations, as seen in Figs. 3 and 4, is required to accurately infer the stable relational structure among two variables, achieving 0.3 SHD and 0.9 precision.When examining three variables, the number of necessary observations, as seen in Figs. 5, 6 and  7, incrementally rises to 16.This parameter will be a fundamental assumption in the subsequent RL experiments.</p>
<p>Graph structures comprising a limited number of variables exhibit fewer permutational outcomes, resulting in diminished variability within the potential graph configurations.In contrast, an increment in the variable count grows exponentially the potential edge permutations and thus the inferential complexity.The graphs reveal that the standard deviation is significantly higher in the graphs with three variables compared to those with two variables.Scenarios with a large number of variables demand a proportionately larger sample size to efficiently discern the true causal model, as delineated in Fig. 8.</p>
<p>In certain universes, it appears that a system with three variables might yield more precise results eventually than one with just two.This improvement could stem from the fact that the additional variable provides more data and contextual relationships, which NOTEARS can utilize to better identify the true graph structure.</p>
<p>D. Causal Reinforcement Learning</p>
<p>In the causal reinforcement learning experiments, we evaluate how causal relations formed by the agent in a highly complex ever-changing environment, as is in SAR, help provide on-policy RL algorithms with extra information in the observation space to solve the task more efficiently.In these experiments, we compare an agent that learns the causal relationships of the environment (using causal discovery procedures such as the one shown in the previous experiments) with a non-causal agent that does not have this knowledge.In a real-world SAR example, this causal knowledge may assist a robot for example in identifying which pieces of rubble are movable, which are hazardous, and which doors open and in what way.</p>
<p>Using the Gymnasium library we created a random 2D environment where the trapped individual (goal) is enclosed in a room with no straightforward openings to enter the room.Movable objects to simulate blocked openings or doors are placed as room boundaries, so the agent is required to move the objects located on the boundaries of the room to enter, as shown in Fig. 9.In these experiments we evaluate the scenarios described in Fig. 2, same as in the previous section, with the exception of the two trivial scenarios where none of the variables are connected to the movability.</p>
<p>In the implementation of the environment, factors such as the positions of the starting agent and the objects are random, to help the agents generalise their learning.</p>
<p>The reward function used is the following:</p>
<p>(1) A (20,20) grid size was used for the environment with a (7,7) inner room located in the middle of the environment with the goal inside of it.The agent actions are {forward, backwards, left, right} 1 cell movements.Each episode has 800 maximum number of steps, which represent an out-of-fuel scenario or out-of-range scenario in a SAR mission.The observation space is of a Box type implemented as an integer ranging from 0 to 8; the corresponding objects are the following: wall, free space, rough debris, rough column, smooth debris, smooth column, agent starting position, and goal.</p>
<p>The causal agent observation space includes information about each object's movability, since we assume here that the previous causal discovery phase was fully successful for this agent, while for the non-causal agent it is not included.</p>
<p>The experiments are evaluated using the following aggregated metrics across the parallel environments: Mean number of times the goal has been reached (MGR), Mean time taken to reach the goal (MTT), Mean number of movable object interactions (MMI), Mean number of non-movable object interactions (MII).</p>
<p>The RL algorithm used is A2C with a Multilayer Perceptron Policy.The hyperparameters used are discount factor gamma=0.995, number of steps to run each environment per update n_steps=100, entropy coefficient ent_coef=0.002,value function coefficient vf_coef=0.5,maximum gradient norm max_grad_norm=1, learning rate lr=0.0003, and Adam optimizer with epsilon value=1e-7.</p>
<p>The training is conducted over 8 million timesteps, with an evaluation interval at every 10,000 timesteps and a logging interval at every 5,000 timesteps, in an Nvidia Tesla T4 GPU with 16GB VRAM, 64GiB CPU RAM with 8 cores.</p>
<p>E. Causal Reinforcement Learning Results</p>
<p>In the first experiment there are 2 variables (texture, movability) and 2 types of objects: Rough objects which are immovable and smooth objects which are movable.The number of objects is varied in the simulations.The results are shown in Table I and Fig. 10.All plots in this section have applied a smoothing of 10% with a running average method to understand the general trend for comparison, and correspond to 18 objects.Fig. 10 shows both models learning, however, the causal models consistently reach the early stopping of mean_goal_reached=1 earlier than the non-causal models.As seen in Table I, in the more complex environments with 18 objects, the causal agent showed a better performance across all metrics conforming with our hypothesis that the additional causal knowledge on object movability helps the agent navigate the environment more efficiently.</p>
<p>In the second experiment, we used 3 variables (texture, shape, movability) and 4 types of objects, either rough or smooth textured, and shaped like debris or columns.In this case, the texture of the objects is causally connected to movability and the shape is disconnected.The results are shown in Table II and Fig. 11.Table II shows that for causal agents where the environment has objects with texture and shape, the more objects there are in the environment, the wider the difference in mean time taken and mean immovable interactions, and the causal agent has a significantly higher mean number of times goal has been reached compared to the non-causal agent.Fig. 11 shows how, with 18 objects, the causal agent shows a significant trend in better learning to reach the goal with optimized interactions with the movable and immovable objects in the environment.</p>
<p>In the third experiment, the same previous 3 variables (texture, shape, movability) and 4 types of objects are used.Here both the texture type and the shape are causally connected to movability and the inner room position is randomised each epoch creating a much more complex environment.Results are shown in Table III and Fig. 12. Fig. 12 shows the causal model reached the early stopping criteria at around 150M time-steps, whilst the non-causal model exceeded the 200M limit without reaching it.The causal model was able to train at a minimum of 24.5% faster than the non-causal model.</p>
<p>In a second evaluation of these models, the causal and non-causal models created at 151M time-steps are tested on 9 random environments.The causal model successfully reached the goal in these tests 70% of the time, with an average completion time of 3 seconds, whilst the non-causal model reached the goal in these tests 60% of the time, with an average completion time of 9 seconds.</p>
<p>F. Causal Reinforcement Learning Discussion</p>
<p>The results have demonstrated improved learning times across all tested environments, which included using fixed goal and random goal positions and differing numbers of objects and their classifying variables.It is observed across all the runs that as the environment becomes more complex with an increasing number of objects, the causal agent outperforms the non-causal agent with a significant gap across most of the metrics.Unexpectedly, the causal model did not show a large reduction in the number of interactions, but this will be addressed in future work via further exploration of additional reward strategies.We theorise that one of the major limitations in our model comes from the restricted field of view of the robot agent, which allows the agent to observe only the space directly in front of it.</p>
<p>The model can be improved further by better tuning the RL hyperparameters for our task and through the use of penalties applied when interacting with the immovable objects in the environment.We believe that applying this penalty may encourage the causal model to interact less with the immovable objects with no difference in movable interactions, whilst in the non-causal model, we may see a decrease in all interactions.</p>
<p>V. CONCLUSION</p>
<p>In this work we have proposed a new architecture for an autonomous robot combining sensing capabilities with causal understanding and autonomous decision-making via RL to enhance its efficiency.The system's main proposed goal is its use in SAR scenarios.</p>
<p>To validate our proposal, experiments were conducted on causal discovery and RL to explore the influence of causal knowledge on robot performance.We explored the system's ability to understand causal relationships, interpret visual and physical attributes of objects-like texture and shape-and predict the dynamic behaviour of unknown objects.</p>
<p>The causal discovery experiments using NOTEARS yielded key findings regarding the number of interactions necessary to learn the causal relationships of the environment, setting the stage for the subsequent RL simulations.Using the A2C algorithm we observed a remarkable improvement in learning times, with the Causal RL model accelerating task completion by over 24.5% in complex scenarios.While this research opens the door to future enhancements, the insights gleaned thus far underscore the significant promise of integrating causal learning with RL in SAR contexts.</p>
<p>Fig. 1 .
1
Fig. 1.Overarching system architecture proposed Fig.1presents a depiction of the overarching system architecture proposed.It includes the Robot Sensing Module, which captures and processes environmental data, the RL Module, which guides the robot's action selection, and the Digital Mind Module, where causal learning and digital representation storage occur.</p>
<p>Fig. 2 .
2
Fig. 2.</p>
<p>Fig. 3 .Fig. 4 .
34
Fig. 3. SHD/precision vs samples (2 causally related variables)</p>
<p>Fig. 5 .Fig. 6 .
56
Fig. 5. SHD/precision vs samples (3 variables with 2 causally related variables)</p>
<p>Fig. 8 .
8
Fig. 8.Sample size requirement for 75% precision.</p>
<p>Fig</p>
<p>Fig. 9. Example of simulation environment showing immovable/movable (blue/red) objects, and robot/goal (yellow/green).</p>
<p>Fig. 11 .
11
Fig. 11.MGR for 3 variables only partially connected scenario</p>
<p>TABLE I .
I
CAUSAL RL -2 VARIABLES CAUSALLY CONNECTED
Fig. 10. MGR for 2 variables causally connected scenarioAgentNo. of objectsMGRMTTMMIMIICausal60.980.524.448.70Non-Causal60.930.313.275.87Causal121.000.292.747.72Non-Causal120.950.293.438.31Causal181.000.202.976.55Non-Causal180.930.283.0810.24</p>
<p>TABLE II
II.CAUSAL RL -3 VARIABLES PARTIALLY CONNECTEDAgentNo. of objectsMGRMTTMMIMIICausal61.000.383.425.50Non-Causal60.940.293.485.53Causal121.000.213.537.77Non-Causal120.990.292.978.74Causal180.980.292.928.50Non-Causal180.760.582.6119.41</p>
<p>TABLE III .
III
CAUSAL RL -3 VARIABLES CAUSALLY CONNECTED AND
RANDOM GOAL POSITIONAgentNo. of objectsMGRMTTMMIMIICausal180.900.734.1649.67Non-Causal180.720.6810.6951.91
Fig. 12. MGR for 3 variables causally connected and random goal position scenario</p>
<p>https://github.com/Causal-Curiosity-in-Search-and-Rescue/Causal_RL_for_Robotics _in_Unknown_Environments
https://github.com/FenTechSolutions/CausalDiscoveryToolbox 3 https://github.com/networkx 2 https://github.com/mckinsey/causalnex</p>
<p>. J Pearl, Causality , 2009Cambridge university press</p>
<p>J Pearl, M Glymour, N P Jewell, Causal Inference in Statistics: A Primer. John Wiley &amp; Sons2016</p>
<p>Elements of causal inference: foundations and learning algorithms. J Peters, D Janzing, B Schölkopf, 2017The MIT Press</p>
<p>Autonomous navigation of mobile robots in unknown environments using off-policy reinforcement learning with curriculum learning. Y Yin, Z Chen, G Liu, J Yin, J Guo, Expert Systems with Applications. 2471232022024</p>
<p>Generative adversarial networks. I Goodfellow, J Pouget-Abadie, M Mirza, B Xu, D Warde-Farley, S Ozair, A Courville, Y Bengio, Communications of the ACM. 63112020</p>
<p>Deep reinforcement learning for autonomous search and rescue. J G C Zuluaga, J P Leidig, C Trefftz, G Wolffe, NAECON 2018-IEEE National Aerospace and Electronics Conference. 2018</p>
<p>Review of causal discovery methods based on graphical models. C Glymour, K Zhang, P Spirtes, Frontiers in genetics. 105242019</p>
<p>Methods and tools for causal discovery and causal inference. A R Nogueira, A Pugnana, S Ruggieri, D Pedreschi, J Gama, Wiley interdisciplinary reviews: data mining and knowledge discovery. 202212e1449</p>
<p>A survey on causal discovery methods for iid and time series data. U Hasan, E Hossain, M O Gani, arXiv:2303.150272023arXiv preprint</p>
<p>Causal curiosity: Rl agents discovering self-supervised experiments for causal representation learning. S A Sontakke, A Mehrjou, L Itti, B Schölkopf, International conference on machine learning. 2021</p>
<p>Causal machine learning: A survey and open problems. J Kaddour, A Lynch, Q Liu, M J Kusner, R Silva, arXiv:2206.154752022arXiv preprint</p>
<p>Dags with no tears: Continuous optimization for structure learning. X Zheng, B Aragam, P K Ravikumar, E P Xing, Advances in neural information processing systems. 201831</p>
<p>Vision Based Exploration of Robot in Fully Unknown Environment. T Hu, Mechanical Engineering and Materials Science Independent Study. 2192023</p>
<p>Long-term Static Mapping and Cloning Localization for autonomous robot navigation using 3D LiDAR in dynamic environments. Y.-C Lee, ' Lsmcl, Expert Systems with Applications. 2411226882024</p>
<p>Asynchronous methods for deep reinforcement learning. V Mnih, A P Badia, M Mirza, A Graves, T Harley, T P Lillicrap, D Silver, K Kavukcuoglu, Proceedings of the 33rd International Conference on International Conference on Machine Learning. the 33rd International Conference on International Conference on Machine Learning201648</p>
<p>Causal discovery toolbox: Uncovering causal relationships in python. D Kalainathan, O Goudet, R Dutta, Journal of Machine Learning Research. 21372020</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-948 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-948</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-948</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-22.html">extraction-schema-22</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <p><strong>Paper ID:</strong> paper-267411892</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.02716v1.pdf" target="_blank">Understanding the planning of LLM agents: A survey</a></p>
                <p><strong>Paper Abstract:</strong> As Large Language Models (LLMs) have shown significant intelligence, the progress to leverage LLMs as planning modules of autonomous agents has attracted more attention. This survey provides the first systematic view of LLM-based agents planning, covering recent works aiming to improve planning ability. We provide a taxonomy of existing works on LLM-Agent planning, which can be categorized into Task Decomposition, Plan Selection, External Module, Reflection and Memory. Comprehensive analyses are conducted for each direction, and further challenges for the field of research are discussed.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e948.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e948.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ZeroShot-CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-shot Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting strategy that elicits multi-step reasoning by using the instruction "Let's think step-by-step" without providing few-shot examples; used to induce chain-of-thought style internal reasoning in a single prompt call.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large language models are zero-shot reasoners</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>ZeroShot-CoT (text-davinci-003 via prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prompting-only technique applied to a hosted autoregressive LLM (text-davinci-003) to elicit step-by-step reasoning; no model fine-tuning or architectural changes.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>HotPotQA, FEVER</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td>reported severe degradation on QA benchmarks relative to few-shot; (paper: ZeroShot-CoT performed poorly on the two QA benchmarks in Table 2)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>chain-of-thought prompting (instructional only)</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>prompting only (zero-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>prompting strategy</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Use of the instruction "Let's think step-by-step" to elicit chain-of-thought reasoning without few-shot exemplars.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Paper reports severe performance degradation on two QA benchmarks compared to few-shot CoT (precise numeric drop not cleanly tabulated in text but described qualitatively as severe).</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>LLM needs few-shot examples to understand complex QA tasks; the single magic instruction is not sufficient for some QA tasks, indicating a reliance on exemplars rather than inherent zero-shot chain-of-thought capability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Understanding the planning of LLM agents: A survey', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e948.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e948.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FewShot-CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Few-shot Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique that provides a few exemplars demonstrating step-by-step reasoning (Chain-of-Thought) to guide the model's multi-step problem solving.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large language models are zero-shot reasoners</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>FewShot-CoT (text-davinci-003 via prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prompting-only technique using few human-provided chain-of-thought exemplars to steer the LLM's reasoning trajectory during decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>HotPotQA, FEVER</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td>higher than ZeroShot-CoT on QA benchmarks (table shows substantially better QA scores than ZeroShot-CoT; e.g. FewShot-CoT outperforms ZeroShot-CoT on HotPotQA and FEVER)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>chain-of-thought prompting with exemplars</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>prompting only (few-shot in-context learning)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>prompting strategy</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Adding few-shot CoT exemplars to the prompt to improve reasoning and QA performance.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Improved QA performance relative to zero-shot CoT in the paper's experiments (numerical QA entries in Table 2 show higher success rates for FewShot-CoT than ZeroShot-CoT).</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Complex QA tasks require exemplar-driven in-context learning; without exemplars, chain-of-thought prompting can fail.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Understanding the planning of LLM agents: A survey', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e948.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e948.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT-SC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought with Self-Consistency (CoT-SC)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Combines chain-of-thought prompting with self-consistency: sample multiple reasoning paths and aggregate (majority vote) to improve robustness of multi-step reasoning outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>CoT-SC (text-davinci-003 with Self-Consistency)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prompt-based approach that samples multiple chain-of-thought trajectories from an LLM and selects the answer by majority voting; increases token/computation cost.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>HotPotQA, FEVER</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td>reported in Table 2 (e.g. HotPotQA ~33%, FEVER ~62% as shown in the table rows for CoT-SC)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>ALFWorld, ScienceWorld</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>interactive gaming / embodied text environment</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>ALFWorld success rate ~57%; ScienceWorld average reward ~15.24 (as reported in Table 2)</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>chain-of-thought sampling, self-consistency aggregation</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>prompting only (sampling + aggregation)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>prompting strategy / multi-plan selection</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Generate multiple reasoning/planning trajectories via sampling and select consensus solution (self-consistency); increases compute/tokens to improve reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Improves both QA and interactive metrics over single-run CoT in this paper; Table 2 shows CoT-SC achieves reasonable interactive success (57% ALFWorld) and moderate QA scores (table values above zero).</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Stochastic single-run decoding is brittle; sampling and aggregation reduce variance but at higher cost â€” cost-benefit tradeoff affects interactive vs QA performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Understanding the planning of LLM agents: A survey', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e948.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e948.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReAct (Reasoning + Acting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting/agent framework that interleaves reasoning (Thought) and environment interaction (Action), enabling the LLM to both plan and act sequentially with intermediate observations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ReAct: Synergizing reasoning and acting in language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>ReAct (text-davinci-003 via prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prompt-driven agent architecture that alternates 'Thought' and 'Action' steps so the model can plan, query tools, and use observations to inform later steps; supports tool use and interactive environments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>HotPotQA, FEVER</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td>reported (Table 2 shows ReAct QA scores; e.g. HotPotQA ~34%, FEVER ~63%)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>ALFWorld, ScienceWorld</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>interactive gaming / sequential decision-making</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>ALFWorld success rate reported ~57%; ScienceWorld average reward ~15.05 (see Table 2)</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>interleaved decomposition, tool-use interface (action calls), chain-of-thought-like reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>prompting only / in-context behavior specification</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>architectural (agent loop) / prompting strategy</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Interleaving reasoning and acting so that the LLM uses immediate observation feedback to adjust subsequent actions (improves fault tolerance in interactive tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Comparable interactive performance to CoT-SC but with higher token costs; paper reports ReAct and CoT-SC have similar ALFWorld SR (~57%) while ReAct uses more tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Sequential decision-making requires grounding of actions into environment affordances and handling longer trajectories; pure QA reasoning skills aren't sufficient for interactive execution without action grounding and feedback loops.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Understanding the planning of LLM agents: A survey', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e948.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e948.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflexion (language agents with verbal reinforcement / self-reflection)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that augments agent behavior with self-evaluation: the agent generates reflections on failures and uses them to refine future planning, effectively adding an iterative self-correction loop.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reflexion: Language agents with verbal reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Reflexion (text-davinci-003 with reflection loop)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prompting-based iterative scheme where after initial plan generation and execution the LLM produces a reflection/evaluation which is then used to refine subsequent plans; increases tokens/rounds.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>HotPotQA, FEVER</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td>Improved QA performance relative to ReAct/CoT-SC in Table 2 (e.g. HotPotQA reported ~39%, FEVER ~68%)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>ALFWorld, ScienceWorld</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>interactive gaming / sequential decision-making</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>ALFWorld success rate improved from ~57% (CoT-SC/ReAct) to ~71%; ScienceWorld average reward improved from ~15.05 to ~19.39 (Table 2)</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>reflection mechanism, iterative refinement loop</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>prompting only (iterative verbal feedback within prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>prompting strategy / hybrid iterative method</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Add a reflection/evaluation phase after initial plan/execution, then refine plan using the generated critique; consumes roughly twice the tokens compared to some baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Significant interactive performance gains reported: ALFWorld SR improved from ~57% to ~71%; ScienceWorld AR from ~15.05 to ~19.39; smaller but positive QA gains (e.g. HotPotQA ~34% -> ~39%, FEVER ~63% -> ~68%).</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Interactive tasks benefit from explicit self-correction owing to long trajectories and environment feedback; LLMs' one-shot planning is error-prone for procedural tasks but can be improved by reflection loops.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Understanding the planning of LLM agents: A survey', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e948.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e948.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SayCan</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SayCan (value-grounded action selection)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that grounds LLM-generated actions into an action space using a value function or affordance scoring, enabling alignment between language plans and feasible actions in robotic/interactive settings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>SayCan (conceptual method referenced; not directly evaluated on QA in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Hybrid approach that uses an LLM to propose candidate actions and a value function (e.g., learned affordance model) to ground and rank these actions for execution in an environment.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>planning / tool-use grounding</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>action grounding via value function, hybrid LLM + affordance model</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>hybrid (LLM prompting + learned value function / embeddings)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>hybrid approach / architectural change (value-grounding)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Ground LLM outputs into discrete action affordances via a value function (e.g., textual embedding similarity) so that proposed actions map to environment-executable actions; used to make CoT outputs feasible as actions.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>LLMs generate plausible language plans but may propose infeasible environment actions; grounding via an affordance/value model reduces infeasibility in interactive tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Understanding the planning of LLM agents: A survey', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e948.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e948.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>External Planner Integration</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM + External Planner (symbolic or neural planners)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of hybrid interventions where LLMs formalize tasks into structured representations (e.g., PDDL, ASP) and external symbolic or neural planners perform the actual search/plan synthesis to ensure feasibility and efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>LLM + PDDL / LLM + ASP / LLM + neural planner (referenced methods such as LLM+P, LLM-DP, LLM+PDDL, LLM+ASP, CALM, SwiftSage)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLM used for natural language understanding and formalization; external planner (symbolic solvers like Fast-Downward, CLINGO or learned neural planners/decision transformers) conducts planning/search on formalized problem instance.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>planning / sequential decision-making</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>task formalization module, external symbolic/neural planner, hybrid pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>hybrid (LLM prompting/coding + external planner; some methods use imitation or RL to train small planners)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>architectural change / hybrid approach</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Use LLM to translate natural-language tasks into formal planning languages (PDDL, ASP) or generate candidate action priors, then run symbolic or neural planners to produce feasible and efficient plans; also includes dual-process designs (fast learned planner + slow LLM).</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Argued to improve feasibility and efficiency of plans and to address constraints that pure LLM generation misses; specific quantitative effects depend on individual referenced works (this survey reports these as promising approaches but does not run all of them end-to-end).</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>LLMs optimize next-token likelihood and may violate hard constraints or miss feasibility checks; symbolic planners enforce constraints and enable provable feasibility, bridging gaps between QA-style reasoning and executable interactive plans.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Understanding the planning of LLM agents: A survey', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e948.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e948.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Memory Interventions (RAG / Fine-tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-augmented Generation (RAG) and Fine-tuning / PEFT for embodied memory</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Two families of memory-based interventions: (1) RAG-style external memories retrieve relevant past experiences during planning; (2) fine-tuning/PEFT embeds agent experiences into model parameters to improve future planning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>RAG-based memory (MemoryBank, MemGPT, REMEMBER) and fine-tuning based (AgentTuning, CALM, TDT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>RAG: store experiences in external vector DB and retrieve by similarity at planning time; Fine-tuning: incorporate trajectories into model weights via supervised next-token or PEFT methods (LoRA, QLoRA).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>interactive/text-world tasks (e.g., ScienceWorld, ALFWorld in cited works)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>sequential decision-making / planning</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>external memory retrieval, long-term memory store, parameter-efficient fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>RAG (retrieval + prompting) or supervised fine-tuning / PEFT on trajectory data</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>architectural change (memory) / training method (fine-tune)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>RAG retrieves task-relevant past experiences to provide auxiliary signals for planning; fine-tuning embeds past trajectories into model weights so the LLM better recalls planning patterns;</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Survey reports memory-augmented agents show improved growth and fault tolerance in planning tasks; specific numbers depend on cited works (e.g., AgentTuning reports significant improvements on unseen planning tasks in their work).</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Interactive tasks require remembering episodic/long-horizon experience and environment-specific priors which static QA-style LLM behavior lacks; memory modules supply retrieval or learned priors to bridge this gap.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Understanding the planning of LLM agents: A survey', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e948.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e948.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflection & Refinement (self-refine, CRITIC, LEMA)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Iterative self-reflection and refinement methods</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Methods that use generated feedback, external tools, or stronger models to identify and correct errors in LLM-generated plans, and optionally fine-tune models on corrected data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Self-refine, CRITIC, LEMA (referenced approaches)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Self-refine: LLM generates feedback and refines outputs iteratively; CRITIC: uses external KBs/search for validation then corrects; LEMA: collect mistaken samples and correct them with stronger LLM (e.g., GPT-4), then fine-tune base model.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>planning / tool-use / sequential decision-making</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>reflection mechanism, external validation (tools/KB), dataset curation + fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>iterative prompting; some methods use stronger-LM-correction then supervised fine-tuning (LEMA)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>training method (data augmentation/fine-tuning) and prompting strategy (reflection)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Use LLMs to critique their own outputs or use external validators to find errors; corrected outputs can be used to fine-tune the agent for better future performance.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Survey notes significant performance improvements when corrected samples are used to fine-tune (LEMA reports notable gains across LLaMA scales in cited work); Reflexion also shows improved interactive success when using reflection loops.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>One-shot or single-pass planning is brittle; iterative error-detection and correction reduces hallucination and infeasible steps, improving procedural task performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Understanding the planning of LLM agents: A survey', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>ReAct: Synergizing reasoning and acting in language models <em>(Rating: 2)</em></li>
                <li>Reflexion: Language agents with verbal reinforcement learning <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>Tree of Thoughts: Deliberate problem solving with large language models <em>(Rating: 2)</em></li>
                <li>Large language models are zero-shot reasoners <em>(Rating: 2)</em></li>
                <li>LLM+P: Empowering large language models with optimal planning proficiency <em>(Rating: 2)</em></li>
                <li>LLM-DP: Dynamic planning with a LLM <em>(Rating: 1)</em></li>
                <li>LLM+PDDL <em>(Rating: 1)</em></li>
                <li>CALM: Keep calm and explore: Language models for action generation in text-based games <em>(Rating: 2)</em></li>
                <li>AgentTuning: Enabling generalized agent abilities for LLMs <em>(Rating: 2)</em></li>
                <li>MemoryBank: Enhancing large language models with long-term memory <em>(Rating: 2)</em></li>
                <li>Self-Refine: Iterative refinement with self-feedback <em>(Rating: 2)</em></li>
                <li>LEMA: Learning from mistakes makes LLM better reasoner <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-948",
    "paper_id": "paper-267411892",
    "extraction_schema_id": "extraction-schema-22",
    "extracted_data": [
        {
            "name_short": "ZeroShot-CoT",
            "name_full": "Zero-shot Chain-of-Thought prompting",
            "brief_description": "A prompting strategy that elicits multi-step reasoning by using the instruction \"Let's think step-by-step\" without providing few-shot examples; used to induce chain-of-thought style internal reasoning in a single prompt call.",
            "citation_title": "Large language models are zero-shot reasoners",
            "mention_or_use": "use",
            "model_or_agent_name": "ZeroShot-CoT (text-davinci-003 via prompting)",
            "model_description": "Prompting-only technique applied to a hosted autoregressive LLM (text-davinci-003) to elicit step-by-step reasoning; no model fine-tuning or architectural changes.",
            "model_size": null,
            "qa_task_name": "HotPotQA, FEVER",
            "qa_performance": "reported severe degradation on QA benchmarks relative to few-shot; (paper: ZeroShot-CoT performed poorly on the two QA benchmarks in Table 2)",
            "interactive_task_name": null,
            "interactive_task_type": null,
            "interactive_performance": null,
            "reports_both_qa_and_interactive": true,
            "performance_gap_observed": true,
            "architectural_features": "chain-of-thought prompting (instructional only)",
            "training_method": "prompting only (zero-shot)",
            "intervention_type": "prompting strategy",
            "intervention_description": "Use of the instruction \"Let's think step-by-step\" to elicit chain-of-thought reasoning without few-shot exemplars.",
            "intervention_effect": "Paper reports severe performance degradation on two QA benchmarks compared to few-shot CoT (precise numeric drop not cleanly tabulated in text but described qualitatively as severe).",
            "hypothesized_cause_of_gap": "LLM needs few-shot examples to understand complex QA tasks; the single magic instruction is not sufficient for some QA tasks, indicating a reliance on exemplars rather than inherent zero-shot chain-of-thought capability.",
            "uuid": "e948.0",
            "source_info": {
                "paper_title": "Understanding the planning of LLM agents: A survey",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "FewShot-CoT",
            "name_full": "Few-shot Chain-of-Thought prompting",
            "brief_description": "A prompting technique that provides a few exemplars demonstrating step-by-step reasoning (Chain-of-Thought) to guide the model's multi-step problem solving.",
            "citation_title": "Large language models are zero-shot reasoners",
            "mention_or_use": "use",
            "model_or_agent_name": "FewShot-CoT (text-davinci-003 via prompting)",
            "model_description": "Prompting-only technique using few human-provided chain-of-thought exemplars to steer the LLM's reasoning trajectory during decoding.",
            "model_size": null,
            "qa_task_name": "HotPotQA, FEVER",
            "qa_performance": "higher than ZeroShot-CoT on QA benchmarks (table shows substantially better QA scores than ZeroShot-CoT; e.g. FewShot-CoT outperforms ZeroShot-CoT on HotPotQA and FEVER)",
            "interactive_task_name": null,
            "interactive_task_type": null,
            "interactive_performance": null,
            "reports_both_qa_and_interactive": true,
            "performance_gap_observed": true,
            "architectural_features": "chain-of-thought prompting with exemplars",
            "training_method": "prompting only (few-shot in-context learning)",
            "intervention_type": "prompting strategy",
            "intervention_description": "Adding few-shot CoT exemplars to the prompt to improve reasoning and QA performance.",
            "intervention_effect": "Improved QA performance relative to zero-shot CoT in the paper's experiments (numerical QA entries in Table 2 show higher success rates for FewShot-CoT than ZeroShot-CoT).",
            "hypothesized_cause_of_gap": "Complex QA tasks require exemplar-driven in-context learning; without exemplars, chain-of-thought prompting can fail.",
            "uuid": "e948.1",
            "source_info": {
                "paper_title": "Understanding the planning of LLM agents: A survey",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "CoT-SC",
            "name_full": "Chain-of-Thought with Self-Consistency (CoT-SC)",
            "brief_description": "Combines chain-of-thought prompting with self-consistency: sample multiple reasoning paths and aggregate (majority vote) to improve robustness of multi-step reasoning outputs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_or_agent_name": "CoT-SC (text-davinci-003 with Self-Consistency)",
            "model_description": "Prompt-based approach that samples multiple chain-of-thought trajectories from an LLM and selects the answer by majority voting; increases token/computation cost.",
            "model_size": null,
            "qa_task_name": "HotPotQA, FEVER",
            "qa_performance": "reported in Table 2 (e.g. HotPotQA ~33%, FEVER ~62% as shown in the table rows for CoT-SC)",
            "interactive_task_name": "ALFWorld, ScienceWorld",
            "interactive_task_type": "interactive gaming / embodied text environment",
            "interactive_performance": "ALFWorld success rate ~57%; ScienceWorld average reward ~15.24 (as reported in Table 2)",
            "reports_both_qa_and_interactive": true,
            "performance_gap_observed": true,
            "architectural_features": "chain-of-thought sampling, self-consistency aggregation",
            "training_method": "prompting only (sampling + aggregation)",
            "intervention_type": "prompting strategy / multi-plan selection",
            "intervention_description": "Generate multiple reasoning/planning trajectories via sampling and select consensus solution (self-consistency); increases compute/tokens to improve reliability.",
            "intervention_effect": "Improves both QA and interactive metrics over single-run CoT in this paper; Table 2 shows CoT-SC achieves reasonable interactive success (57% ALFWorld) and moderate QA scores (table values above zero).",
            "hypothesized_cause_of_gap": "Stochastic single-run decoding is brittle; sampling and aggregation reduce variance but at higher cost â€” cost-benefit tradeoff affects interactive vs QA performance.",
            "uuid": "e948.2",
            "source_info": {
                "paper_title": "Understanding the planning of LLM agents: A survey",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "ReAct",
            "name_full": "ReAct (Reasoning + Acting)",
            "brief_description": "A prompting/agent framework that interleaves reasoning (Thought) and environment interaction (Action), enabling the LLM to both plan and act sequentially with intermediate observations.",
            "citation_title": "ReAct: Synergizing reasoning and acting in language models",
            "mention_or_use": "use",
            "model_or_agent_name": "ReAct (text-davinci-003 via prompting)",
            "model_description": "Prompt-driven agent architecture that alternates 'Thought' and 'Action' steps so the model can plan, query tools, and use observations to inform later steps; supports tool use and interactive environments.",
            "model_size": null,
            "qa_task_name": "HotPotQA, FEVER",
            "qa_performance": "reported (Table 2 shows ReAct QA scores; e.g. HotPotQA ~34%, FEVER ~63%)",
            "interactive_task_name": "ALFWorld, ScienceWorld",
            "interactive_task_type": "interactive gaming / sequential decision-making",
            "interactive_performance": "ALFWorld success rate reported ~57%; ScienceWorld average reward ~15.05 (see Table 2)",
            "reports_both_qa_and_interactive": true,
            "performance_gap_observed": true,
            "architectural_features": "interleaved decomposition, tool-use interface (action calls), chain-of-thought-like reasoning",
            "training_method": "prompting only / in-context behavior specification",
            "intervention_type": "architectural (agent loop) / prompting strategy",
            "intervention_description": "Interleaving reasoning and acting so that the LLM uses immediate observation feedback to adjust subsequent actions (improves fault tolerance in interactive tasks).",
            "intervention_effect": "Comparable interactive performance to CoT-SC but with higher token costs; paper reports ReAct and CoT-SC have similar ALFWorld SR (~57%) while ReAct uses more tokens.",
            "hypothesized_cause_of_gap": "Sequential decision-making requires grounding of actions into environment affordances and handling longer trajectories; pure QA reasoning skills aren't sufficient for interactive execution without action grounding and feedback loops.",
            "uuid": "e948.3",
            "source_info": {
                "paper_title": "Understanding the planning of LLM agents: A survey",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Reflexion",
            "name_full": "Reflexion (language agents with verbal reinforcement / self-reflection)",
            "brief_description": "A method that augments agent behavior with self-evaluation: the agent generates reflections on failures and uses them to refine future planning, effectively adding an iterative self-correction loop.",
            "citation_title": "Reflexion: Language agents with verbal reinforcement learning",
            "mention_or_use": "use",
            "model_or_agent_name": "Reflexion (text-davinci-003 with reflection loop)",
            "model_description": "Prompting-based iterative scheme where after initial plan generation and execution the LLM produces a reflection/evaluation which is then used to refine subsequent plans; increases tokens/rounds.",
            "model_size": null,
            "qa_task_name": "HotPotQA, FEVER",
            "qa_performance": "Improved QA performance relative to ReAct/CoT-SC in Table 2 (e.g. HotPotQA reported ~39%, FEVER ~68%)",
            "interactive_task_name": "ALFWorld, ScienceWorld",
            "interactive_task_type": "interactive gaming / sequential decision-making",
            "interactive_performance": "ALFWorld success rate improved from ~57% (CoT-SC/ReAct) to ~71%; ScienceWorld average reward improved from ~15.05 to ~19.39 (Table 2)",
            "reports_both_qa_and_interactive": true,
            "performance_gap_observed": true,
            "architectural_features": "reflection mechanism, iterative refinement loop",
            "training_method": "prompting only (iterative verbal feedback within prompts)",
            "intervention_type": "prompting strategy / hybrid iterative method",
            "intervention_description": "Add a reflection/evaluation phase after initial plan/execution, then refine plan using the generated critique; consumes roughly twice the tokens compared to some baselines.",
            "intervention_effect": "Significant interactive performance gains reported: ALFWorld SR improved from ~57% to ~71%; ScienceWorld AR from ~15.05 to ~19.39; smaller but positive QA gains (e.g. HotPotQA ~34% -&gt; ~39%, FEVER ~63% -&gt; ~68%).",
            "hypothesized_cause_of_gap": "Interactive tasks benefit from explicit self-correction owing to long trajectories and environment feedback; LLMs' one-shot planning is error-prone for procedural tasks but can be improved by reflection loops.",
            "uuid": "e948.4",
            "source_info": {
                "paper_title": "Understanding the planning of LLM agents: A survey",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "SayCan",
            "name_full": "SayCan (value-grounded action selection)",
            "brief_description": "A method that grounds LLM-generated actions into an action space using a value function or affordance scoring, enabling alignment between language plans and feasible actions in robotic/interactive settings.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "model_or_agent_name": "SayCan (conceptual method referenced; not directly evaluated on QA in this paper)",
            "model_description": "Hybrid approach that uses an LLM to propose candidate actions and a value function (e.g., learned affordance model) to ground and rank these actions for execution in an environment.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": null,
            "interactive_task_type": "planning / tool-use grounding",
            "interactive_performance": null,
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "action grounding via value function, hybrid LLM + affordance model",
            "training_method": "hybrid (LLM prompting + learned value function / embeddings)",
            "intervention_type": "hybrid approach / architectural change (value-grounding)",
            "intervention_description": "Ground LLM outputs into discrete action affordances via a value function (e.g., textual embedding similarity) so that proposed actions map to environment-executable actions; used to make CoT outputs feasible as actions.",
            "intervention_effect": null,
            "hypothesized_cause_of_gap": "LLMs generate plausible language plans but may propose infeasible environment actions; grounding via an affordance/value model reduces infeasibility in interactive tasks.",
            "uuid": "e948.5",
            "source_info": {
                "paper_title": "Understanding the planning of LLM agents: A survey",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "External Planner Integration",
            "name_full": "LLM + External Planner (symbolic or neural planners)",
            "brief_description": "A family of hybrid interventions where LLMs formalize tasks into structured representations (e.g., PDDL, ASP) and external symbolic or neural planners perform the actual search/plan synthesis to ensure feasibility and efficiency.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_or_agent_name": "LLM + PDDL / LLM + ASP / LLM + neural planner (referenced methods such as LLM+P, LLM-DP, LLM+PDDL, LLM+ASP, CALM, SwiftSage)",
            "model_description": "LLM used for natural language understanding and formalization; external planner (symbolic solvers like Fast-Downward, CLINGO or learned neural planners/decision transformers) conducts planning/search on formalized problem instance.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": null,
            "interactive_task_type": "planning / sequential decision-making",
            "interactive_performance": null,
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "task formalization module, external symbolic/neural planner, hybrid pipeline",
            "training_method": "hybrid (LLM prompting/coding + external planner; some methods use imitation or RL to train small planners)",
            "intervention_type": "architectural change / hybrid approach",
            "intervention_description": "Use LLM to translate natural-language tasks into formal planning languages (PDDL, ASP) or generate candidate action priors, then run symbolic or neural planners to produce feasible and efficient plans; also includes dual-process designs (fast learned planner + slow LLM).",
            "intervention_effect": "Argued to improve feasibility and efficiency of plans and to address constraints that pure LLM generation misses; specific quantitative effects depend on individual referenced works (this survey reports these as promising approaches but does not run all of them end-to-end).",
            "hypothesized_cause_of_gap": "LLMs optimize next-token likelihood and may violate hard constraints or miss feasibility checks; symbolic planners enforce constraints and enable provable feasibility, bridging gaps between QA-style reasoning and executable interactive plans.",
            "uuid": "e948.6",
            "source_info": {
                "paper_title": "Understanding the planning of LLM agents: A survey",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Memory Interventions (RAG / Fine-tuning)",
            "name_full": "Retrieval-augmented Generation (RAG) and Fine-tuning / PEFT for embodied memory",
            "brief_description": "Two families of memory-based interventions: (1) RAG-style external memories retrieve relevant past experiences during planning; (2) fine-tuning/PEFT embeds agent experiences into model parameters to improve future planning.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_or_agent_name": "RAG-based memory (MemoryBank, MemGPT, REMEMBER) and fine-tuning based (AgentTuning, CALM, TDT)",
            "model_description": "RAG: store experiences in external vector DB and retrieve by similarity at planning time; Fine-tuning: incorporate trajectories into model weights via supervised next-token or PEFT methods (LoRA, QLoRA).",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "interactive/text-world tasks (e.g., ScienceWorld, ALFWorld in cited works)",
            "interactive_task_type": "sequential decision-making / planning",
            "interactive_performance": null,
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "external memory retrieval, long-term memory store, parameter-efficient fine-tuning",
            "training_method": "RAG (retrieval + prompting) or supervised fine-tuning / PEFT on trajectory data",
            "intervention_type": "architectural change (memory) / training method (fine-tune)",
            "intervention_description": "RAG retrieves task-relevant past experiences to provide auxiliary signals for planning; fine-tuning embeds past trajectories into model weights so the LLM better recalls planning patterns;",
            "intervention_effect": "Survey reports memory-augmented agents show improved growth and fault tolerance in planning tasks; specific numbers depend on cited works (e.g., AgentTuning reports significant improvements on unseen planning tasks in their work).",
            "hypothesized_cause_of_gap": "Interactive tasks require remembering episodic/long-horizon experience and environment-specific priors which static QA-style LLM behavior lacks; memory modules supply retrieval or learned priors to bridge this gap.",
            "uuid": "e948.7",
            "source_info": {
                "paper_title": "Understanding the planning of LLM agents: A survey",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Reflection & Refinement (self-refine, CRITIC, LEMA)",
            "name_full": "Iterative self-reflection and refinement methods",
            "brief_description": "Methods that use generated feedback, external tools, or stronger models to identify and correct errors in LLM-generated plans, and optionally fine-tune models on corrected data.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_or_agent_name": "Self-refine, CRITIC, LEMA (referenced approaches)",
            "model_description": "Self-refine: LLM generates feedback and refines outputs iteratively; CRITIC: uses external KBs/search for validation then corrects; LEMA: collect mistaken samples and correct them with stronger LLM (e.g., GPT-4), then fine-tune base model.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": null,
            "interactive_task_type": "planning / tool-use / sequential decision-making",
            "interactive_performance": null,
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "reflection mechanism, external validation (tools/KB), dataset curation + fine-tuning",
            "training_method": "iterative prompting; some methods use stronger-LM-correction then supervised fine-tuning (LEMA)",
            "intervention_type": "training method (data augmentation/fine-tuning) and prompting strategy (reflection)",
            "intervention_description": "Use LLMs to critique their own outputs or use external validators to find errors; corrected outputs can be used to fine-tune the agent for better future performance.",
            "intervention_effect": "Survey notes significant performance improvements when corrected samples are used to fine-tune (LEMA reports notable gains across LLaMA scales in cited work); Reflexion also shows improved interactive success when using reflection loops.",
            "hypothesized_cause_of_gap": "One-shot or single-pass planning is brittle; iterative error-detection and correction reduces hallucination and infeasible steps, improving procedural task performance.",
            "uuid": "e948.8",
            "source_info": {
                "paper_title": "Understanding the planning of LLM agents: A survey",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "ReAct: Synergizing reasoning and acting in language models",
            "rating": 2
        },
        {
            "paper_title": "Reflexion: Language agents with verbal reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2
        },
        {
            "paper_title": "Tree of Thoughts: Deliberate problem solving with large language models",
            "rating": 2
        },
        {
            "paper_title": "Large language models are zero-shot reasoners",
            "rating": 2
        },
        {
            "paper_title": "LLM+P: Empowering large language models with optimal planning proficiency",
            "rating": 2
        },
        {
            "paper_title": "LLM-DP: Dynamic planning with a LLM",
            "rating": 1
        },
        {
            "paper_title": "LLM+PDDL",
            "rating": 1
        },
        {
            "paper_title": "CALM: Keep calm and explore: Language models for action generation in text-based games",
            "rating": 2
        },
        {
            "paper_title": "AgentTuning: Enabling generalized agent abilities for LLMs",
            "rating": 2
        },
        {
            "paper_title": "MemoryBank: Enhancing large language models with long-term memory",
            "rating": 2
        },
        {
            "paper_title": "Self-Refine: Iterative refinement with self-feedback",
            "rating": 2
        },
        {
            "paper_title": "LEMA: Learning from mistakes makes LLM better reasoner",
            "rating": 1
        }
    ],
    "cost": 0.0173535,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Understanding the planning of LLM agents: A survey
5 Feb 2024</p>
<p>Xu Huang xuhuangcs@mail.ustc.edu.cn 
University of Science and Technology of China
HefeiChina</p>
<p>Weiwen Liu liuweiwen8@huawei.com 
Huawei Noah's Ark Lab
ShenzhenChina</p>
<p>Xiaolong Chen chenxiaolong@mail.ustc.edu.cn 
University of Science and Technology of China
HefeiChina</p>
<p>Xingmei Wang xingmeiwang@mail.ustc.edu.cn 
University of Science and Technology of China
HefeiChina</p>
<p>Hao Wang haowang@ustc.edu.cn 
University of Science and Technology of China
HefeiChina</p>
<p>Defu Lian liandefu@ustc.edu.cn 
University of Science and Technology of China
HefeiChina</p>
<p>Yasheng Wang wangyasheng@huawei.com 
Huawei Noah's Ark Lab
ShenzhenChina</p>
<p>Ruiming Tang tangruiming@huawei.com 
Huawei Noah's Ark Lab
ShenzhenChina</p>
<p>Enhong Chen cheneh@ustc.edu.cn 
University of Science and Technology of China
HefeiChina</p>
<p>Understanding the planning of LLM agents: A survey
5 Feb 2024CBC6211751A2EBC449D75D8070A1BC51arXiv:2402.02716v1[cs.AI]
As Large Language Models (LLMs) have shown significant intelligence, the progress to leverage LLMs as planning modules of autonomous agents has attracted more attention.This survey provides the first systematic view of LLM-based agents planning, covering recent works aiming to improve planning ability.We provide a taxonomy of existing works on LLM-Agent planning, which can be categorized into Task Decomposition, Plan Selection, External Module, Reflection and Memory.Comprehensive analyses are conducted for each direction, and further challenges for the field of research are discussed.</p>
<p>Introduction</p>
<p>Autonomous agents have been recognized as intelligent entities capable of accomplishing specific tasks, via perceiving the environment, planning, and executing actions.Planning, as one of the most critical capabilities for agents, requires complicated understanding, reasoning, and decision-making progress [Ghallab et al., 2004].</p>
<p>Despite the abstract concept of planning, a general formulation of the planning tasks can be described as follows.Given time step t, with the environment denoted as E, the action space as A, the task goal as g, and the action at step t as a t âˆˆ A, the planning procedure can be expressed as the generation of a sequence of actions: p = (a0, a1, â€¢ â€¢ â€¢ , at) = plan(E, g; Î˜, P).</p>
<p>where Î˜ and P represent the parameters of the LLM and the prompts for the task, respectively.</p>
<p>Conventional works mainly rely on symbolic methods or reinforcement learning-based methods, such as Planning Domain Definition Language (PDDL) [Aeronautiques et al., 1998;Haslum et al., 2019] or policy learning [He et al., 2015;Yao et al., 2020a].However, those conventional methods have several limitations.Symbolic methods require conversion from flexible natural language-described problems into symbolic modeling, which may require human experts' efforts.Usually, this kind of method lacks error tolerance, resulting in failures even if there are only a few errors.Reinforcement learning (RL) methods are often combined with deep models, which serve as the policy network or reward model.While RL algorithms often require a large number of samples (interactions with the environment) to learn an effective policy, this can be impractical or costly in scenarios where collecting data is time-consuming or expensive.</p>
<p>In recent years, the emergence of Large Language Models (LLMs) has marked a paradigm shift.LLMs have achieved remarkable success across various domains, showcasing significant intelligence in reasoning, tool usage, planning, and instruction-following.The surprising intelligence of LLMs sheds light on employing LLMs as the cognitive core of agents, thereby offering the potential to improve planning ability.Numerous methodologies have been developed to harness the potential of LLMs for agent planning.While existing surveys have attempted to summarize techniques for LLMs [Zhao et al., 2023a], LLMs for decision-making [Yang et al., 2023a], reasoning [Sun et al., 2023], tool learning [Qin et al., 2023], and autonomous agents [Wang et al., 2023a], they often lack a detailed analysis of planning ability within the literature.In this survey, we analyze the latest research works and discuss the advantages and limitations, aiming to provide a systematic view of the planning ability of LLMbased agents.Existing methods are further categorized into five representative directions, with each direction undergoing comprehensive analysis.Furthermore, we have evaluated several representative methods on four benchmarks.To the best of our knowledge, this is the first work that comprehensively analyzes LLM-based agents from the planning abilities.</p>
<p>The subsequent sections of this paper are organized as follows.In Section 2, we categorize the works into five mainstream directions and analyze their ideas regarding planning ability.Sections 3 to 7 provide detailed discussions and analysis of each direction.Finally, Section 9 concludes the survey, offering insights into future directions in this field.</p>
<p>Taxonomy</p>
<p>As the research on the planning ability of LLM-based agents presents a flourishing scene, various methods have been proposed to exploit the upper limit of planning ability.To have a better bird's view of existing advanced works, we pick out some representative and influential works, analyzing their motivations and essential ideas.To provide a better understanding, we illustrate the analysis in Table 1.</p>
<p>According to the table, we present a novel and systematic taxonomy for LLM-based agent plannning that divides existing works into five important categories, covering task decomposition, multi-plan selection, external module-aided planning, reflection and refinement and memory-augmented planning, as illustrated in Figure 1.Here we briefly summarize those five directions as below.Task Decomposition.Tasks in real life are usually complicated and multi-step, bringing severe hardness for planning.This kind of method adopts the idea of divide and conquer, decomposing the complicated into several sub-tasks and then sequentially planning for each sub-task.The process could be formulated as follows: g0, g1, â€¢ â€¢ â€¢ , gn = decompose(E, g; Î˜, P);
p i = (a i 0 , a i 1 , â€¢ â€¢ â€¢ a i m ) = sub-plan(E, gi; Î˜, P).
Multi-plan Selection.This kind of method focuses on leading the LLM to "think" more, generating various alternative plans for a task.Then a task-related search algorithm is employed to select one plan to execute.The process could be formulated as follows:
P = p1, p2, â€¢ â€¢ â€¢ , pn = plan(E, g; Î˜, P); p * = select(E, g, P ; Î˜, F).
where F represents the search strategies, such as some tree search algorithms [Yao et al., 2023;Zhao et al., 2023b].</p>
<p>External Planner-Aided Planning.This methodology is crafted to employ an external planner to elevate the planning procedure, aiming to address the issues of efficiency and infeasibility of generated plans, while the LLM mainly plays the role in formalizing the tasks.The process could be formulated as follows:
h = formalize(E, g; Î˜, P); p = plan(E, g, h; Î¦).
where Î¦ denotes the external planner module, h represents the formalized information.</p>
<p>Reflection and Refinement.This methodology emphasizes improving planning ability through reflection and refinement.</p>
<p>It encourages LLM to reflect on failures and then refine the plan.The process could be formulated as follows: p0 = plan(E, g; Î˜, P); ri = reflect(E, g, pi; Î˜, P); pi+1 = refine(E, g, pi, ri; Î˜, P); Memory-augmented Planning.This kind of approach enhances planning with an extra memory module, in which valuable information is stored, such as commonsense knowledge, past experiences, domain-specific knowledge, et al.The information is retrieved when planning, serving as auxiliary signals.The process could be formulated as follows:
m = retrieve(E, g; M); p = plan(E, g, m; Î˜, P).
where M represents the memory module.</p>
<p>The five directions are interconnected rather than mutually exclusive, often involving the concurrent adoption of multiple techniques.In the subsequent sections, we delve deeper into the five research directions concerning LLM-agent planning, elucidating their motivations, proposing representation solutions, and addressing inherent limitations.</p>
<p>Task Decomposition</p>
<p>In real-world scenarios, environments are often characterized by complexity and variability, thereby addressing complex tasks through a one-step planning process is a formidable challenge.This simplification of complicated tasks is a remarkable human ability, evident in the decomposition of one task into several simpler sub-tasks [Schraagen et al., 2000], which is analogous to the well-known algorithmic strategy called "divide and conquer", as illustrated in Eq. ( 1).Task decomposition generally involves two crucial steps: firstly, decomposing the complex task, referred to as the "decompose" step, and secondly, planning for the sub-tasks, known as the "sub-plan step".Current methods for task decomposition in this domain generally fall into two categories: decomposition-first and interleaved decomposition, illustrated in Figure 2.</p>
<p>Goal
Sub Goal-1 â€¦ Decompose Sub Goal-2 Sub Goal-n â€¦ Sub-plan Sub Goal-1 â€¦ Sub Goal-2 Sub Goal-n â€¦ (a) Decomposition-First (b) Interleaved â‘  â‘¡ LLM Agent LLM Agent</p>
<p>Decomposition-First Methods</p>
<p>Decomposition-first methods decompose the task into subgoals first and then plan for each sub-goal successively, presented in Figure 2(a).The representative methods include HuggingGPT [Shen et al., 2023] utilizes various multimodal models from the Huggingface Hub to construct an intelligent agent for multimodal tasks.It is capable of handling tasks such as image generation, image classification, object recognition, video annotation, speech-to-text, et al.To facilitate collaboration between different models, the LLM acts as a controller, responsible for decomposing tasks inputted by humans, selecting models, and generating final responses.The most crucial stage is the initial task decomposition, where HuggingGPT explicitly instructs the LLM to break down the given task into sub-tasks, providing dependencies between tasks.Plan-and-Solve [Wang et al., 2023b] improves upon the Zero-shot Chain-of-Thought [Kojima et al., 2022] by transforming the original "Let's think stepby-step" into a two-step prompt instruction: "Let's first devise a plan" and "Let's carry out the plan".This zero-shot approach has achieved improvements in mathematical reasoning, common-sense reasoning, and symbolic reasoning.ProgPrompt [Singh et al., 2023] translates natural language descriptions of tasks into coding problems.It symbolizes the agent's action space and objects in the environment through code, with each action formalized as a function and each object represented as a variable.Consequently, task planning is naturally transformed into function generation.When executing tasks, the agent first generates a plan in the form of function callings and then executes them step by step.</p>
<p>Interleaved Decomposition Methods</p>
<p>Interleaved decomposition involves interleaved task decomposition and sub-task planning, where each decomposition only reveals one or two sub-tasks at the current state, illustrated in Figure 2(b).Representative methods in this category include the Chain-of-Thought (CoT) series [Wei et al., 2022;Kojima et al., 2022], ReAct [Yao et al., 2022], PAL [Gao et al., 2023] CoT guides the LLM in reasoning about complex problems through a few constructed trajectories, leveraging the LLM's reasoning abilities for task decomposition.Subsequently, Zero-shot CoT [Kojima et al., 2022] unlocks the LLM's zeroshot reasoning abilities with the magical instruction "Let's think step-by-step".In contrast to CoT, which embeds reasoning within the planning process, ReAct [Yao et al., 2022] decouples reasoning and planning.It alternates between reasoning (Thought step) and planning (Action step), demonstrating significant improvements in the planning capabilities.</p>
<p>Visual ChatGPT [Wu et al., 2023] utilizes ReAct's mechanism, employing LLM as the agent's brain equipped with a series of visual models, resulting in an agent with image processing capabilities.PAL [Gao et al., 2023] improves CoT by leveraging the LLM's coding abilities, guiding the LLM to generate code during reasoning.Finally, a code interpreter (such as Python) is used to comprehensively execute the codes to obtain the solution.This method proves helpful for agents in solving mathematical and symbolic reasoning problems.Program-of-Thought (PoT) [Chen et al., 2022] completely formalize the reasoning process as programming.The authors also leverage a CodeX [Chen et al., 2021b] model trained on code-related data, enhancing performance in mathematical and financial problems.</p>
<p>Discussions</p>
<p>For the decomposition-first method, the advantage lies in creating a stronger correlation between the sub-tasks and the original tasks, reducing the risk of task forgetting and hallucinations [Touvron et al., 2023].However, since the sub-tasks are predetermined at the beginning, additional mechanisms for adjustment are required otherwise one error in some step will result in failure, which will be discussed in Section 6.On the other hand, interleaved decomposition and sub-planning dynamically adjust decomposition based on environmental feedback, improving the fault tolerance.However, for complicated tasks, excessively long trajectories may lead to LLM experiencing hallucinations, deviating from the original goals during subsequent sub-tasks and sub-planning.</p>
<p>Although task decomposition significantly enhances the ability of LLM-Agent to solve complicated tasks, challenges persist.The first challenge is the additional overhead introduced by task decomposition.Decomposing a task into multiple sub-tasks requires more reasoning and generation, incurring additional time and computational costs.On the other hand, for highly complex tasks that are decomposed into dozens of sub-tasks, the planning is constrained by the context length of the LLM, leading to the forgetting of the planning trajectories.</p>
<p>Multi-Plan Selection</p>
<p>Due to the complexity of the tasks and the inherent uncertainty of LLM, the plans generated by the LLM-Agent for a given task can be diverse.Even though LLM possesses strong reasoning abilities, a single plan generated by LLM is likely to be suboptimal or even infeasible.A more natural approach is multi-plan selection, comprising two major steps: multiplan generation and optimal plan selection.</p>
<p>Multi-Plan Generation</p>
<p>Multi-plan generation involves generating a dozen paths of plans to comprise the candidate plan set.Mainstream methods consider employing uncertainty in the decoding process of generative models.</p>
<p>Self-consistency [Wang et al., 2022b] employs a simple intuition: the solutions for complex problems are rarely unique.In contrast to CoT, which generates a single path, Self-consistency obtains multiple distinct reasoning paths via sampling strategies embodied in the decoding process, such as temperature sampling, top-k sampling.Tree-of-Thought (ToT) [Yao et al., 2023] proposes two strategies to generate plans (i.e.thoughts): sample and propose.The sample strategy is consistent with Self-consistency, where LLM would sample multiple plans in decoding process.The propose strategy explicitly instructs the LLM to generate various plans via few-shot examples in prompts.Graph-of-Thought (GoT) [Besta et al., 2023] extends ToT by adding transformations of thoughts, which supports arbitrary thoughts aggregation.LLM-MCTS [Zhao et al., 2023b] and RAP [Hao et al., 2023] leverages LLM as the heuristic policy function for the Monte Carlo Tree Search (MCTS), where multiple potential actions are obtained by multiple calls.</p>
<p>Optimal Plan Selection</p>
<p>To select the optimal plan among the candidate plans, diverse strategies are adopted as heuristic search algorithms.</p>
<p>Self-consistency [Wang et al., 2022b] applies the naive majority vote strategy, regarding the plan with the most votes as the optimal choice.Benefiting from the tree architecture, Tree-of-Thought (ToT) [Yao et al., 2023] supports tree search algorithms, such as conventional BFS and DFS.When selecting a node for expansion, it uses LLM to evaluate multiple actions and chooses the optimal one.Similar with ToT, LLM-MCTS [Zhao et al., 2023b] and RAP [Hao et al., 2023] also employ a tree structure to assist in multi-plan search.Unlike ToT, they employ the Monte Carlo Tree Search (MCTS) algorithm for search.LLM A<em> [Xiao and Wang, 2023] utilizes the classic A</em> algorithm from artificial intelligence to assist LLM in search.The Chebyshev distance from the current position to the target position serves as the heuristic cost function for selecting the optimal path.</p>
<p>Discussions</p>
<p>The scalability of multi-plan selection is notably advantageous, providing a broader exploration of potential solutions in the expansive search space.However, this advantage comes with inherent trade-offs.The increased computational demands, especially for models with large token counts or computations, pose practical challenges.This cost consideration becomes crucial, particularly in scenarios where resource constraints are a significant factor, such as the online service.Moreover, the reliance on LLM for the evaluation of plans introduces new challenges.As LLM's performance in ranking tasks is still under scrutiny, there is a need for further validation and fine-tuning of its capabilities in this specific context.The stochastic nature of LLMs adds randomness to the selection, potentially affecting the consistency and reliability of the chosen plans.</p>
<p>External Planner-Aided Planning</p>
<p>Despite the powerful reasoning and task decomposition capabilities exhibited by Large Language Models (LLMs), challenges arise when confronted with environments featuring intricate constraints, such as mathematical problem-solving or generating admissible actions.To address challenges, several methods integrate LLMs with external planners.Such methods can be categorized into symbolic planners and neural planners based on the introduced planners.</p>
<p>Symbolic Planner</p>
<p>Symbolic planners have served as a fundamental component in the fields of automated planning for several decades.These approaches, based on well-established symbolic formalized models, such as PDDL models [Aeronautiques et al., 1998;Haslum et al., 2019], employ symbolic reasoning to identify optimal paths from initial states to desired goal states.</p>
<p>LLM+P [Liu et al., 2023a] enhances the planning proficiency of LLMs by incorporating a PDDL-based symbolic planner.Leveraging the semantic understanding and coding capabilities of LLM, the authors organize problems into textual language prompts inputted to LLM.This prompts LLM to organize the actions within the environment and specified tasks into the format of the PDDL language.Subsequently, after obtaining a formalized description, the authors employ the Fast-Downward1 solver for the planning process.Building upon LLM+P, LLM-DP [Dagan et al., 2023] is specifically designed for dynamic interactive environments.Upon receiving feedback from the environment, LLM processes the information, formalizes it into PDDL language, and then employs a BFS [Lipovetzky et al., 2014] solver to generate a plan.LLM+PDDL [Guan et al., 2023] also utilizes the PDDL language to formalize the task, incorporating an additional step for manual verification to check for potential issues in the PDDL model generated by LLM.During the planning process, the authors propose using the plan generated by LLM as an initial heuristic solution to accelerate the search process of local search planners, such as LPG [Gerevini and Serina, 2002].LLM+ASP [Yang et al., 2023b] transforms problems described in natural language by LLM into atomic facts, converting tasks into ASP problems.Subsequently, the ASP solver CLINGO is utilized to generate plans.Well-trained neural planners exhibit excellent planning capabilities within their respective domains and demonstrate superior planning efficiency due to their smaller parameter sizes.However, when faced with complex and less frequently encountered problems, where training data is scarce, these small models tend to perform poorly due to insufficient generalization ability.Therefore, several works explore combining an LLM with a light-weight neural planner, to further enhance the planning capabilities.CALM [Yao et al., 2020a] proposed an early approach that combines a language model with an RL-based neural planner.One language model processes textual environmental information, generating a set of candidate actions as priors based on the environmental information.A DRRN policy network is then employed to re-rank these candidate actions, ultimately selecting the optimal action.SwiftSage [Lin et al., 2023] leverages the dual-process theory from cognitive psychology, dividing the planning process into slow thinking and fast thinking.The slow-thinking process involves complex reasoning and rational deliberation while fast-thinking resembles an instinctive response developed through long-term training.The authors utilize a DT model, trained through imitation learning, as the fast-thinking model for rapid plan generation.When errors occur during plan execution, indicating a more complex problem, the agent switches to the slow-thinking process, where LLM engages in reasoning and planning based on the current state.This combination of fast and slow thinking has proven to be highly effective in terms of efficiency.</p>
<p>Neural Planner</p>
<p>Discussions</p>
<p>For those strategies that leverage an additional planner for assistance, LLM primarily plays a supportive role.Its main functions involve parsing textual feedback and providing additional reasoning information to assist in planning, particularly when addressing complex problems.Specifically, the enhancement of LLM's capabilities in code generation empowers the potential to deal with more general tasks for symbolic artificial intelligence.Actually, a significant drawback of traditional symbolic AI systems lies in the complexity and heavy reliance on human experts in constructing symbolic models, while LLM accelerates this process, facilitating faster and more optimal establishment of symbolic models.The advantages brought by symbolic systems include theoretical completeness, stability, and interpretability.The combination of statistical AI with LLM is poised to become a major trend in the future development of artificial intelligence.</p>
<p>Reflection and Refinement</p>
<p>Reflection and refinement are indispensable components in the planning process.They enhance the fault tolerance and error correction capabilities of LLM-Agent planning.Due to existing hallucination issues and insufficient reasoning abilities for complex problems, LLM-Agents may make errors and get stuck in "thought loops" during planning due to limited feedback.Reflecting on and summarizing failures helps agents correct errors and break out of such loops in subsequent attempts.</p>
<p>Self-refine [Madaan et al., 2023] utilizes an iterative process of generation, feedback, and refinement.After each generation, LLM generates feedback for the plan, facilitating adjustments based on the feedback.Reflexion [Shinn et al., 2023] extends ReAct by incorporating an evaluator to assess trajectories.LLM generates self-reflections upon error detection, aiding in error correction.CRITIC [Gou et al., 2023] uses external tools like Knowledge Bases and Search Engines to validate LLM-generated actions.It then leverages external knowledge for self-correction, significantly reducing factual errors.InteRecAgent [Huang et al., 2023b] employs a mechanism called ReChain for self-correction.An LLM is used to evaluate the response and tool-using plan generated by the interactive recommendation agent, summarize feedback on errors, and decide whether to restart planning.LEMA [An et al., 2023] gathers mistaken planning samples first and employs more powerful GPT-4 for correction.Those corrected samples are then used to fine-tune the LLM-Agent, resulting in significant performance improvements across various scales of the LLaMA model.</p>
<p>Particularly, the self-reflective strategy bears resemblance to the principles of reinforcement learning, where the agent plays the role of the decision-maker, such as the policy network.Environmental feedback triggers updates of the policy network.However, in contrast to deep reinforcement learning where updates are achieved by modifying model parameters, in the LLM agent, this update occurs through selfreflection by the LLM itself, culminating in textual verbal feedbacks.These textual feedbacks can serve as both longterm and short-term memory, influencing the agent's subsequent planning outputs through the prompts.Nevertheless, the convergence of this textual form of update currently lacks a guaranteed proof, indicating the inability to demonstrate that continual reflection can ultimately lead the LLM agent to a specified goal.</p>
<p>Memory-Augumented Planning</p>
<p>For agents, memory is a crucial pathway to enhance planning capabilities and the potential for growth.Regarding the memory mechanisms in LLM-Agents, there are currently two major approaches to enhance planning abilities through memory: RAG-based memory and embodied memory.</p>
<p>RAG-based Memory</p>
<p>Retrieval Augmented Generation (RAG) [Lewis et al., 2020;Mao et al., 2020;Cai et al., 2022] techniques are proposed to aid text generation with retrieved information.It is capable of enhancing the LLM with the latest knowledge, such as New Bing2 and Google Bard3 .For LLM agents, past experiences could be stored in the memory and retrieved when needed.The core idea of such methods is to retrieve task-relevant experiences from the memory during task planning.Among those methods, memories are typically stored in additional storage, and the forms are diverse, such as texts [Park et al., 2023;Liu et al., 2023b;Packer et al., 2023;Wang et al., 2023c;Zhong et al., 2023] 2023c] encode each memory using a text encoding model into a vector and establish an indexing structure, such as FAISS library [Johnson et al., 2019].During retrieval, the description of the current status is used as a query to retrieve memories from the memory pool.The difference between the three lies in the way memories are updated.MemGPT [Packer et al., 2023] leverages the concept of multiple levels of storage in computer architecture, abstracting the context of LLM into RAM and treating the additional storage structure as a disk.LLM can spontaneously decide whether to retrieve historical memories or save the current context to storage.REMEM-BER [Zhang et al., 2023a] stores historical memories in the form of a Q-value table, where each record is (environment, task, action, Q-value)-tuple.During retrieval, positive and negative memories are both retrieved for LLM to generate plan based on the similarity of the environment and task.</p>
<p>Embodied Memory</p>
<p>Embodied memory involves finetuning the LLM with the agent's historical experiential samples, embedding memories into the model parameters.Usually the experiential samples are collected from the agents's interactions with environment, which may consist of commonsense knowledge about the environment, task-related priors, and successful or failed experiences.While the cost of training a language model with more than billions of parameters is huge, parameter-efficient fine-tuning (PEFT) techniques are leveraged to reduce cost and speed up by training a small part of parameters only, such as LoRA, QLoRA, P-tuning, et al.</p>
<p>CALM [Yao et al., 2020b] utilizes ground-truth action trajectories collected from the text-world environment to finetune GPT-2 using next token prediction task, enabling it to memorize planning-related information and generalize well on planning tasks.Similarly, TDT [Wang et al., 2022a] uses collected Markov decision process data to fine-tune Text Decision Transformer (TDT).It achieves better success rates on more challenging ScienceWorld [Wang et al., 2022a] tasks.AgentTuning [Zeng et al., 2023] organizes plan trajectories from various tasks into a dialogue form to finetune the LLaMA model, showing significant improvements in performance on unseen planning tasks.</p>
<p>Discussions</p>
<p>The RAG-based and Fine-tuning-based memory approaches enhance LLM-Agent planning capabilities, each with distinct advantages and limitations.RAG-based methods offer realtime, low-cost external memory updates mainly in natural language text, but rely on the accuracy of retrieval algorithm.</p>
<p>Finetuning provides a larger memorization capacity through parameter modifications but has high memory update costs and struggles with retaining fine-grained details.Memory-enhanced LLM-Agents demonstrate enhanced growth and fault tolerance in planning, yet memory generation heavily depends on LLM's generation capabilities.Improving weaker LLM-Agents through self-generated memory remains a challenging area to explore.</p>
<p>Evaluation</p>
<p>Evaluating the planning capability of the agent is a critical issue in the research area.Here we investigate several mainstream benchmarking methods, categorizing them into the following types.Interactive Gaming Environments: Game environments may provide real-time multi-modal feedback based on the agent's actions, including textual and visual feedback.Currently, the most widely used gaming environment is Minecraft4 , where the agent needs to gather materials to create tools for obtaining more rewards.The quantity of tools created by the agent is often used as an evaluation metric.Another popular category is the text-based interactive environments, such as ALFWorld [Shridhar et al., 2020], ScienceWorld [Wang et al., 2022a], et al, where the agent locates in an environment described in natural language, with limited actions and locations.The success rate or the rewards obtained are commonly used as evaluation metrics.Compared with Minecraft, these text-based interactive environments are often simpler, with straightforward feedback and fewer feasible actions.Interactive Retrieval Environments: Interactive retrieval environments simulate the process of information retrieval and reasoning that humans undergo in real life.In these environments, agents are often allowed to interact with search engines and other web services, using actions such as searching keywords or executing click, forward, and backward operations to acquire more information, thereby obtaining an-swers to questions or completing information retrieval tasks.Commonly used retrieval environments include questionanswering tasks based on the Wikipedia engine [Yao et al., 2022] (such as HotPotQA [Yang et al., 2018] and Fever [Thorne et al., 2018]) and web browsing tasks to find specific information, including WebShop, Mind2Web [Deng et al., 2023], andWebArena [Zhou et al., 2023].The task success rate is usually used as the metric.Interactive Programming Environments: Interactive programming environments simulate the interaction between programmers and computers, testing the agent's planning ability in solving computer-related problems.In these environments, agents are required to interact with computers to solve problems by writing code or instructions.They would receive various feedback including compile and runtime error messages, as well as execution results.Popular interactive programming environments involve issues related to operating systems, databases, etc., such as Agent Bench [Liu et al., 2023c], MiniWoB++ [Kim and others, 2023].</p>
<p>Most of these existing interactive environments lack finegrained evaluation, where the performance is predominantly evaluated by the final success rate.Furthermore, unlike realworld scenarios where there are often multiple paths to complete a task, there is typically only one "golden" path in most simulated environments due to the high annotation cost.Experiments.We have conducted experiments on four benchmarks to validate the performance of representative works, shown in Table 2.We have implemented six promptbased methods due to limited budgets, covering task decomposition, multi-path selection, and reflection.As for the benchmarks, ALFWorld, ScienceWorld, HotPotQA, and FEVER are employed, involving interactive gaming and question-answering benchmarks.Since ALFWorld and Sci-enceWorld are involved in larger action space, the zero-shot method, i.e.ZeroShot-CoT, is not applicable due to unawareness of action space.SayCan improves CoT by grounding output actions into action space with a value function, which does not apply to QA tasks because there are only two actions: SEARCH[KEYWORD] and LOOKUP [KEYWORD].And we set the value function as a textual embedding model bgesmall-en-v1.5[Xiao and others, 2023].We obtain 3 actions and 5 answers each step for gaming tasks and QA tasks for CoT-SC, respectively.The round of retries in Reflexion is set to 1.We use the API of text-davinci-003 in OpenAI as LLM.(i) The performance increases with the expenses.As CoT-SC, ReAct and Reflexion are involved in multiple plans, additional thoughts, and reflections, respectively, their expenses are more than their backbone methods.Intuitively, more tokens represent more detailed thinking, resulting in performance improvements.(ii) Fewshot examples are suggested for complicated tasks.Despite that the magic instruction Let's think step by step can lead to more reasoning, ZeroShot-CoT exhibits severe performance degradation in two QA benchmarks, which demonstrates the necessity of the examples for LLM to further understand the task.(iii) Reflection plays a crucial role in improving the success rate, especially for complex tasks.Despite Reflexion con-suming about twice the tokens compared with ReAct, the improvements in complicated tasks are promising, such as ALF-World and ScienceWorld, which shows that LLM possesses the error-correcting capability.</p>
<p>Conclusions and Future Directions</p>
<p>Since LLM has shown the emergence of intelligence, there has been an increasing focus on using LLM to enhance the planning capabilities of agents.The major directions are summarized in Figure 1, with a detailed comparison and analysis of various methods presented in Sections 3 to 7. We also conducted experiments on four benchmarks, comparing the effectiveness of several representative methods and showing that performance increases with expenses.Despite the enhancements made by these works in planning capabilities, there are still some significant challenges.</p>
<p>Hallucinations.During the planning process, LLM often suffers from hallucinations, leading to irrational plans, unfaithfulness to task prompts, or failing to follow complex instructions.For instance, plans may include actions that interact with items not existed in the environment.Although these issues can be alleviated through careful prompt engineering, they reflect fundamental shortcomings in LLM [Zhang et al., 2023b;Huang et al., 2023a].</p>
<p>Feasibility of Generated Plans.LLM, being fundamentally based on statistical learning, optimizes the probability of the next word through massive data.Compared to symbolic artificial intelligence, this approach struggles to obey complex constraints, especially when dealing with less common constraints encountered during LLM training.Consequently, plans generated by LLM may lack feasibility without considering adequate preconditions.Connecting LLM with symbolic planning models without altering LLM itself is a promising future direction.</p>
<p>Efficiency of Generated Plans.Generating efficient plans is a crucial issue in planning.However, in existing LLM agents, planning is greedily based on generated plans from LLM output, without considering the efficiency of the generated plans.Therefore, future developments may require introducing additional efficiency evaluation modules to work in conjunction with LLM for more efficient plans.</p>
<p>Multi-Modal Environment Feedback.LLM is originally designed for processing textual inputs, but real-world environment feedback is often multi-modal, including images, audio, etc., which are challenging to describe in natural language.Therefore, LLM agents face limitations when handling such scenarios.Future considerations may involve integrating the development of multi-modal large models and revisiting related planning strategies.</p>
<p>Fine-grained Evaluation.As mentioned in Section 8, existing benchmarks mostly rely on the final completion status of tasks, lacking fine-grained step-wise evaluations.Additionally, environmental feedback is often rule-based, simplistic, and distant from real-world scenarios.A potential future direction is to leverage high-intelligence models like LLM to design more realistic evaluation environments.</p>
<p>[ An et al., 2023]</p>
<p>Figure 1 :
1
Figure 1: Taxonomy on LLM-Agent planning.</p>
<p>Figure 2 :
2
Figure 2: Types of task decomposition manners.</p>
<p>Neural planners are deep models trained on collected planning data with reinforcement learning or imitation learning techniques, showing effective planning abilities within the specific domain.For instance, DRRN [He et al., 2015] models the planning process as a Markov Decision Process through reinforcement learning, training a policy network to obtain a deep decision model.Decision Transformer (DT) [Chen et al., 2021a] empowers a transformer model to clone human decision-making behavior with planning data.</p>
<p>, tabular forms [Zhang et al., 2023a], knowledge graph [Pan et al., 2024], etc. Generative Agents [Park et al., 2023] store the daily experiences of human-like agents in text form and retrieve memories based on a composite score of recency and relevance to the current situation.Similarly, MemoryBank [Zhong et al., 2023], TiM [Liu et al., 2023b], and RecMind [Wang et al.,</p>
<p>Table 1 :
1
A taxonomy for existing LLM-Agent planning works.
MethodIdeaLLM's taskFormulationRepresentative worksTask DecompositionDivide and ConquerTask decomposition Subtask planning[gi] = decompose(E, g; Î˜, P); p i = sub-plan(E, gi; Î˜, P)CoT [2022], ReAct [2022], HuggingGPT [2023]Multi-planGenerate multiple plansPlans generationP = plan(E, g; Î˜, P);ToT [2023], GoT [2023],Selectionand select the optimalPlans evaluationp  *  = select(E, g, P ; Î˜, F)CoT-SC [2022b]External Planner-aidedFormalize tasks and utilize external plannerTask formalizationh = formalize(E, g; Î˜, P); p = plan(E, g, h; Î¦)LLM+P [2023a], LLM+PDDL [2023]Reflection &amp; RefinementReflect on experiences and refine plansPlan generation Reflection Refinementp0 = plan(E, g; Î˜, P); ri = reflect(E, g, pi; Î˜, P); pi+1 = refine(E, g, pi, ri; Î˜, P)Reflexion [2023], CRITIC [2023], Self-Refine [2023]Memory-aidedLeverage memoryPlan generationm = retrieve(E, g; M);REMEMBER [2023a],Planningto aid planningMemory extractionp = plan(E, g, m; Î˜, P)MemoryBank [2023]HuggingGPT [Shen et al., 2023], Plan-and-Solve [Wang etal., 2023b], ProgPrompt [Singh et al., 2023], et al.</p>
<p>Table 2 :
2
Evaluation of representative prompt-based methods on four interactive benchmarks.The SR, AR and EX are abbreviations of success rate, average rewards, and expenses respectively.The expenses are calculated based on the number of consumed tokens through OpenAI's API.Z-CoT and F-CoT represent Zeroshot-CoT and Fewshot-CoT, respectively.
AlfWorldScienceWorldHotPotQAFEVERMetricsSR(%) EX($)AR EX($) SR(%) EX($) SR(%) EX($)Z-CoTN/AN/AN/AN/A0.010.950.391.07F-CoT0.4398.60 16.58 272.220.325.730.612.25CoT-SC0.57 105.37 15.24 274.330.337.860.623.21SayCan0.60 113.61 12.36 125.71N/AN/AN/AN/AReAct0.57 152.18 15.05 356.030.3466.000.63 22.20Reflexion0.71 220.17 19.39 724.480.39 112.490.68 37.26</p>
<p>Shengnan An, Zexiong Ma, et al.Learning from mistakes makes llm better reasoner.arXiv preprint arXiv:2310.20689,2023.He et al., 2015] Ji He, Jianshu Chen, et al.Deep reinforcement learning with a natural language action space.arXiv preprint arXiv:1511.04636,2015.[Huang et al., 2023a] Lei Huang, Yu Weijiang, et al.A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions.arXiv preprint arXiv:2311.05232,2023.Park et al., 2023] Joon Sung Park, Joseph O'Brien, et al.Generative agents: Interactive simulacra of human behavior.In SUIST, pages 1-22, 2023.[Qin et al., 2023] Yujia Qin, Shengding Hu, et al.Tool learning with foundation models.Sun et al., 2023] Jiankai Sun, Chuanyang Zheng, et al.A survey of reasoning with foundation models.arXiv preprint arXiv:2312.11562,2023.[Thorne et al., 2018] James Thorne, Andreas Vlachos, et al.Fever: a large-scale dataset for fact extraction and verification.arXiv preprint arXiv:1803.05355,2018.[Touvron et al., 2023] Hugo Touvron, Louis Martin, et al.Llama 2: Open foundation and fine-tuned chat models.arXiv preprint arXiv:2307.09288,2023.[Wang et al., 2022a] Ruoyao Wang, Peter Jansen, et al.Scienceworld: Is your agent smarter than a 5th grader?arXiv preprint arXiv:2203.07540,2022.[Wang et al., 2022b] Xuezhi Wang, Jason Wei, et al.Selfconsistency improves chain of thought reasoning in language models.arXiv preprint arXiv:2203.11171,2022.[Wang et al., 2023a] Lei Wang, Chen Ma, et al.A survey on large language model based autonomous agents.arXiv preprint arXiv:2308.11432,2023.[Wang et al., 2023b] Lei Wang, Wanyu Xu, et al.Plan-andsolve prompting: Improving zero-shot chain-of-thought reasoning by large language models.arXiv preprint arXiv:2305.04091,2023.[Wang et al., 2023c] Yancheng Wang, Ziyan Jiang, et al.Recmind: Large language model powered agent for recommendation.arXiv preprint arXiv:2308.14296,2023.[Wei et al., 2022] Jason Wei, Xuezhi Wang, et al.Chainof-thought prompting elicits reasoning in large language models.NeurIPS, 35:24824-24837, 2022.[Wu et al., 2023] Chenfei Wu, Shengming Yin, et al.Visual chatgpt: Talking, drawing and editing with visual foundation models.arXiv preprint arXiv:2303.04671,2023.[Xiao and others, 2023] Shitao Xiao et al.C-pack: Packaged resources to advance general chinese embedding, 2023.[Xiao and Wang, 2023] Hengjia Xiao and Peng Wang.Llm a<em>: Human in the loop large language models enabled a</em> search for robotics.arXiv preprint arXiv:2312.01797,2023.[Yang et al., 2018] Zhilin Yang, Peng Qi, et al.Hotpotqa: A dataset for diverse, explainable multi-hop question answering.arXiv preprint arXiv:1809.09600,2018.[Yang et al., 2023a] Sherry Yang, Nachum Ofir, et al.Foundation models for decision making: Problems, methods, and opportunities.arXiv preprint arXiv:2303.04129,2023.[Yang et al., 2023b] Zhun Yang, Adam Ishay, and Joohyung Lee.Coupling large language models with logic programming for robust and general reasoning from text.arXiv preprint arXiv:2307.07696,2023.[Yao et al., 2020a] Shunyu Yao, Rohan Rao, et al.Keep calm and explore: Language models for action generation in text-based games.arXiv preprint arXiv:2010.02903,2020.[Yao et al., 2020b] Shunyu Yao, Rohan Rao, et al.Keep calm and explore: Language models for action generation in text-based games.arXiv preprint arXiv:2010.02903,2020.[Yao et al., 2022] Shunyu Yao, Jeffrey Zhao, et al.React: Synergizing reasoning and acting in language models.arXiv preprint arXiv:2210.03629,2022.[Yao et al., 2023] Shunyu Yao, Dian Yu, et al.Tree of thoughts: Deliberate problem solving with large language models.arXiv preprint arXiv:2305.10601,2023.[Zeng et al., 2023] Aohan Zeng, Mingdao Liu, et al.Agenttuning: Enabling generalized agent abilities for llms.arXiv preprint arXiv:2310.12823,2023.[Zhang et al., 2023a] Danyang Zhang, Lu Chen, et al.Large language model is semi-parametric reinforcement learning agent.arXiv preprint arXiv:2306.07929,2023.[Zhang et al., 2023b] Yue Zhang, Yafu Li, et al.Siren's song in the ai ocean: A survey on hallucination in large language models.arXiv preprint arXiv:2309.01219,2023.[Zhao et al., 2023a] Wayne Xin Zhao, Kun Zhou, et al.A survey of large language models.arXiv preprint arXiv:2303.18223,2023.[Zhao et al., 2023b] Zirui Zhao, Wee Sun Lee, and David Hsu.Large language models as commonsense knowledge for large-scale task planning.arXiv preprint arXiv:2305.14078,2023.[Zhong et al., 2023] Wanjun Zhong, Lianghong Guo, Qiqi Gao, and Yanlin Wang.Memorybank: Enhancing large language models with long-term memory.arXiv preprint arXiv:2305.10250,2023.[Zhou et al., 2023] Shuyan Zhou, Frank F Xu, et al.Webarena: A realistic web environment for building autonomous agents.arXiv preprint arXiv:2307.13854,2023.
arXiv preprintarXiv:2304.08354, 2023.[Schraagen et al., 2000] Jan Maarten Schraagen, Susan F[Besta et al., 2023] Maciej Besta, Nils Blach, et al. Graph of Chipman, et al. Cognitive task analysis. Psychology Press, 2000. thoughts: Solving elaborate problems with large language [Shen et al., 2023] Yongliang Shen, Kaitao Song, et al. Hug-models. arXiv preprint arXiv:2308.09687, 2023. ginggpt: Solving ai tasks with chatgpt and its friends in [Cai et al., 2022] Deng Cai, Yan Wang, Lemao Liu, and Shuming Shi. Recent advances in retrieval-augmented text huggingface. arXiv preprint arXiv:2303.17580, 2023. [Shinn et al., 2023] Noah Shinn, Federico Cassano, et al. generation. In SIGIR, pages 3417-3419, 2022. Reflexion: Language agents with verbal reinforcement [Chen et al., 2021a] Lili Chen, Kevin Lu, et al. Decision transformer: Reinforcement learning via sequence mod-eling. NeurIPS, 34:15084-15097, 2021. [Chen et al., 2021b] Mark Chen, Jerry Tworek, et al. Eval-uating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. learning. In NeurIPS, 2023. [Shridhar et al., 2020] Mohit Shridhar, Xingdi Yuan, et al. Alfworld: Aligning text and embodied environments for interactive learning. arXiv preprint arXiv:2010.03768, 2020. [Singh et al., 2023] Ishika Singh, Valts Blukis, et al. Prog-prompt: Generating situated robot task plans using large [Chen et al., 2022] Wenhu Chen, Xueguang Ma, et al. Pro-language models. In ICRA 2023, pages 11523-11530. gram of thoughts prompting: Disentangling computation IEEE, 2023. from reasoning for numerical reasoning tasks. arXiv preprint arXiv:2211.12588, 2022. [[Huang et al., 2023b] Xu Huang, Jianxun Lian, et al. Rec-ommender ai agent: Integrating large language mod-els for interactive recommendations. arXiv preprint arXiv:2308.16505, 2023. [Johnson et al., 2019] Jeff Johnson, Matthijs Douze, and HervÃ© JÃ©gou. Billion-scale similarity search with GPUs. IEEE Transactions on Big Data, 7(3):535-547, 2019. [Kim and others, 2023] Geunwoo Kim et al. Language models can solve computer tasks. arXiv preprint arXiv:2303.17491, 2023. [Kojima et al., 2022] Takeshi Kojima, Shixiang Shane Gu, et al. Large language models are zero-shot reasoners. NeurIPS, 35:22199-22213, 2022. [Lewis et al., 2020] Patrick Lewis, Ethan Perez, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. NeurIPS, 33:9459-9474, 2020.[Dagan et al., 2023] Gautier Dagan, Frank Keller, and AlexLascarides. Dynamic planning with a llm. arXiv preprint arXiv:2308.06391, 2023. [Deng et al., 2023] Xiang Deng, Yu Gu, et al. Mind2web: Towards a generalist agent for the web. arXiv preprint arXiv:2306.06070, 2023. [Gao et al., 2023] Luyu Gao, Aman Madaan, et al. Pal: Program-aided language models. In ICML, pages 10764-arXiv preprint [Lipovetzky et al., 2014] Nir Lipovetzky, Miquel Ramirez, arXiv:2305.17390, 2023. et al. Width and inference based planners: Siw, bfs (f), and probe. IPC, page 43, 2014. [Liu et al., 2023a] Bo Liu, Yuqian Jiang, et al. Llm+ p: Em-10799, 2023. [Gerevini and Serina, 2002] Alfonso Gerevini and Ivan Se-rina. Lpg: A planner based on local search for planningpowering large language models with optimal planning proficiency. arXiv preprint arXiv:2304.11477, 2023. [Liu et al., 2023b] Lei Liu, Xiaoyan Yang, et al. Think-in-graphs with action costs. In Aips, volume 2, pages 281-memory: Recalling and post-thinking enable llms with290, 2002.long-term memory. arXiv preprint arXiv:2311.08719,[Ghallab et al., 2004] Malik Ghallab, Dana Nau, et al. Auto-mated Planning: theory and practice. Elsevier, 2004.2023. [Liu et al., 2023c] Xiao Liu, Hao Yu, et al.Agent-[Gou et al., 2023] Zhibin Gou, Zhihong Shao, et al. Critic: Large language models can self-correct with tool-bench: Evaluating llms as agents. arXiv:2308.03688, 2023.arXiv preprintinteractive critiquing. arXiv preprint arXiv:2305.11738,[Madaan et al., 2023] Aman Madaan, Niket Tandon, , et al.2023.Self-refine: Iterative refinement with self-feedback. arXiv[Guan et al., 2023] Lin Guan, Karthik Valmeekam, et al.preprint arXiv:2303.17651, 2023.Leveraging pre-trained large language models to construct[Mao et al., 2020] Yuning Mao, Pengcheng He, Liu, et al.and utilize world models for model-based task planning.Generation-augmented retrieval for open-domain questionarXiv preprint arXiv:2305.14909, 2023.answering. arXiv preprint arXiv:2009.08553, 2020.[Hao et al., 2023] Shibo Hao, Yi Gu, et al. Reasoning with[Packer et al., 2023] Charles Packer, Vivian Fang, et al.language model is planning with world model. arXivMemgpt: Towards llms as operating systems. arXivpreprint arXiv:2305.14992, 2023.preprint arXiv:2310.08560, 2023.
[Haslum et al., 2019]Patrik Haslum, Nir Lipovetzky, et al.An introduction to the planning domain definition language, volume 13.Springer, 2019.[[Lin et al., 2023] Bill Yuchen Lin, Yicheng Fu, et al.Swiftsage: A generative agent with fast and slow thinking for complex interactive tasks.[Pan et al., 2024] Shirui Pan, Linhao Luo, et al.Unifying large language models and knowledge graphs: A roadmap.TKDE, 2024.[</p>
<p>https://github.com/aibasel/downward/tree/release-22.12.0
https://www.bing.com/
https://bard.google.com
https://www.minecraft.net/</p>
<p>Pddl-the planning domain definition language. Aeronautiques, Constructions Aeronautiques. 1998. 1998Technical Report</p>            </div>
        </div>

    </div>
</body>
</html>
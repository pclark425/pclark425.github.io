<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4852 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4852</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4852</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-104.html">extraction-schema-104</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents playing text games, with a focus on how memory is used, the type of memory mechanisms, comparative performance with and without memory, and any recommendations or challenges regarding memory usage.</div>
                <p><strong>Paper ID:</strong> paper-c18c544adc6c3f78843ac0d25473b9b94bc426b6</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/c18c544adc6c3f78843ac0d25473b9b94bc426b6" target="_blank">Evaluating Superhuman Models with Consistency Checks</a></p>
                <p><strong>Paper Venue:</strong> 2024 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML)</p>
                <p><strong>Paper TL;DR:</strong> This paper proposes a framework for evaluating superhuman models via consistency checks, and shows that regardless of a model’s (possibly superhuman) performance on these tasks, it is shown that regardless of a model’s performance on these tasks, one can discover logical inconsistencies in decision making.</p>
                <p><strong>Paper Abstract:</strong> If machine learning models were to achieve superhuman abilities at various reasoning or decision-making tasks, how would we go about evaluating such models, given that humans would necessarily be poor proxies for ground truth? In this paper, we propose a framework for evaluating superhuman models via consistency checks. Our premise is that while the correctness of superhuman decisions may be impossible to evaluate, we can still surface mistakes if the model’s decisions fail to satisfy certain logical, human-interpretable rules. As case studies, we instantiate our framework on three tasks where correctness of decisions is hard to evaluate due to either superhuman model abilities, or to otherwise missing ground truth: evaluating chess positions, forecasting future events, and making legal judgments. We show that regardless of a model’s (possibly superhuman) performance on these tasks, we can discover logical inconsistencies in decision making. For example: a chess engine assigning opposing valuations to semantically identical boards; GPT-4 forecasting that sports records will evolve non-monotonically over time; or an AI judge assigning bail to a defendant only after we add a felony to their criminal record.</p>
                <p><strong>Cost:</strong> 0.008</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Mastering the game of no-press Diplomacy via humanregularized reinforcement learning and planning <em>(Rating: 2)</em></li>
                <li>Auto-GPT: An autonomous GPT-4 experiment <em>(Rating: 2)</em></li>
                <li>LM vs LM: Detecting factual errors via cross examination <em>(Rating: 1)</em></li>
                <li>Are AlphaZero-like agents robust to adversarial perturbations? <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4852",
    "paper_id": "paper-c18c544adc6c3f78843ac0d25473b9b94bc426b6",
    "extraction_schema_id": "extraction-schema-104",
    "extracted_data": [],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Mastering the game of no-press Diplomacy via humanregularized reinforcement learning and planning",
            "rating": 2
        },
        {
            "paper_title": "Auto-GPT: An autonomous GPT-4 experiment",
            "rating": 2
        },
        {
            "paper_title": "LM vs LM: Detecting factual errors via cross examination",
            "rating": 1
        },
        {
            "paper_title": "Are AlphaZero-like agents robust to adversarial perturbations?",
            "rating": 1
        }
    ],
    "cost": 0.007597,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Evaluating Superhuman Models with Consistency Checks</h1>
<p>Lukas Fluri <em><br>ETH Zurich<br>flurilu@ethz.ch<br>Daniel Paleka </em><br>ETH Zurich<br>daniel.paleka@inf.ethz.ch</p>
<p>Florian Tramèr<br>ETH Zurich<br>florian.tramer@inf.ethz.ch</p>
<h4>Abstract</h4>
<p>If machine learning models were to achieve superhuman abilities at various reasoning or decision-making tasks, how would we go about evaluating such models, given that humans would necessarily be poor proxies for ground truth? In this paper, we propose a framework for evaluating superhuman models via consistency checks. Our premise is that while the correctness of superhuman decisions may be impossible to evaluate, we can still surface mistakes if the model's decisions fail to satisfy certain logical, human-interpretable rules. We instantiate our framework on three tasks where correctness of decisions is hard to evaluate due to either superhuman model abilities, or to otherwise missing ground truth: evaluating chess positions, forecasting future events, and making legal judgments. We show that regardless of a model's (possibly superhuman) performance on these tasks, we can discover logical inconsistencies in decision making. For example: a chess engine assigning opposing valuations to semantically identical boards; GPT-4 forecasting that sports records will evolve non-monotonically over time; or an AI judge assigning bail to a defendant only after we add a felony to their criminal record.</p>
<h2>1 Introduction</h2>
<p>Machine learning (ML) is making rapid progress on a variety of reasoning and decision-making tasks [13, 67]. It is thus conceivable that ML models could exhibit superhuman performance on these tasks in the future. The prospect of such models raises a fundamental question:</p>
<h2>How can we evaluate decisions made by superhuman models?</h2>
<p>The ability to evaluate models is essential for establishing their reliability and trustworthiness [11]. Yet, humans are necessarily poor proxies for the ground truth of any decision made by a superhuman model. It is thus unclear how we could discover and fix any remaining flaws or bugs in such models.</p>
<p>To illustrate the challenge, consider a model trained to play chess-a canonical setting where models surpass humans [67, 17]. While we can evaluate a chess model's superhuman performance "endto-end" by playing games (either in natural play or against a white-box adversary [53, 77, 74]), we lack the ability to find fine-grained mistakes in the model's core decision-making (i.e., individual moves)—where humans cannot determine ground truth.</p>
<p>We argue that as machine learning gets applied to more complex and high-stakes planning and decision-making (e.g., autonomous assistants [39]), it becomes critically important to develop methods to reason about and identify bugs in the model's (possibly superhuman) reasoning abilities.</p>
<p>Our main premise is that while we cannot evaluate the correctness of superhuman model decisions, we can often still measure the logical consistency of the model's decision-making process according</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Given a model that produces outputs or decisions that are hard to humanly verify (due to superhuman abilities or other difficulties in assessing ground truth), we propose to instead measure the model's <em>consistency</em> with respect to humanly verifiable rules. The right shows three sample scenarios where model outputs are hard to evaluate individually but clearly inconsistent as a whole.</p>
<p>to established human-interpretable rules. To illustrate, consider a <em>forecasting model</em> [82] that performs near or above a human level. Suppose this model assigns probability 50% to the event "Argentina will win the 2026 FIFA World Cup"; then, regardless of the correctness of that prediction, the model should logically assign a probability ≥ 50% to the event "Argentina survives the competitions' group stage". A lack of such logical consistency indicates that <em>at least one of the model's two forecasts is clearly wrong</em> (but we cannot know which one, a priori). We suggest that by proactively testing for such logical inconsistencies in decision-making, we can better ground the <em>trust</em> that users should put in a machine learning model, and proactively <em>detect and debug</em> model failures.</p>
<p>We propose a general framework to test model decisions against <em>consistency rules</em>. Informally, such a rule states that if inputs <em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, . . . , <em>x</em><sub>n</sub> satisfy some relation <em>P</em>(<em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, . . . , <em>x</em><sub>n</sub>), then this implies that the corresponding (unknown) ground truths <em>y</em><sub>1</sub>, <em>y</em><sub>2</sub>, . . . , <em>y</em><sub>n</sub> satisfy some relation <em>Q</em>(<em>y</em><sub>1</sub>, <em>y</em><sub>2</sub>, . . . , <em>y</em><sub>n</sub>). Given a model <em>f</em>, we then search for tuples of inputs <em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, . . . , <em>x</em><sub>n</sub> for which the model's decisions violate the consistency rule. From this, we can conclude that the model is necessarily wrong on <em>at least one of the tested inputs</em>.</p>
<p>We instantiate our framework to surface logical inconsistencies in three distinct settings. <sup>2</sup></p>
<p>We first consider chess AIs as a representative of models that are superhuman, <em>today</em>. We show that despite the superhuman play level, Leela Chess Zero [5] and Stockfish [70] can make simple evaluation blunders recognizable by a chess novice. For example, the model sometimes assigns highly different valuations to <em>semantically identical</em> chess positions (see Figure 1). These logical inconsistencies show that models with superhuman abilities can be prone to rare but severe blunders in individual decisions, that can be easily recognized.</p>
<p>While our main motivation is to evaluate models with superhuman abilities, there are few application settings (beyond games) for us to consider at the moment. We thus consider as case-studies additional settings where the correctness of model decisions is hard to assess (i.e., tasks that humans cannot solve perfectly) and where comparing humans and models can therefore be challenging (even if the models perform at a sub-human level).</p>
<p>The second task we consider is <em>forecasting future events</em> [82], a setting where ground truth is inherently unknowable (until the future). While current language models are likely worse at forecasting than humans, actually evaluating the accuracy of recent models (e.g., GPT-4) would require waiting until the resolution dates of each forecast. Nevertheless, we show that regardless of their true forecasting abilities, GPT-3.5-turbo and GPT-4 are <em>very inconsistent</em> forecasters. For example, the models' forecasts of various sporting records in successive years fail to improve monotonically. Such simple logical failures render any forecasts by these models inherently untrustworthy.</p>
<p><sup>2</sup> We make the data and code for our experiments available at https://github.com/ethz-spylab/superhuman-ai-consistency.</p>
<p>The third task we consider is to use AI models for legal judgments [19, 30, 51]. Both human-made and AI-made legal decisions can be hard to assess. One reason is unobserved outcomes, e.g., a crime recidivism prediction cannot be verified if the suspect is jailed. Humans may also simply disagree on the right decision, especially when considering metrics beyond "accuracy" such as fairness [76]. These issues have led to debated claims of superhuman ML legal abilities in the past [3, 51]. We show that regardless of a model's actual performance, we can exhibit obviously paradoxical judgments. Notably, if we ask GPT-3.5-turbo to make bail decisions, we find that a suspect would sometimes be more likely to be assigned bail if they committed more crimes.</p>
<p>In summary, in each of the settings we consider, we find that while the correctness of model decisions cannot be directly evaluated due to unknown ground truth, it is possible to build logical consistency checks that the model's decision-making process routinely fails. We view the existence of such flaws as a major barrier to placing trust in current models for critical decision-making scenarios.</p>
<h1>2 Related Work</h1>
<p>Testing or enforcing consistency between model outputs has a long history in machine learning. We discuss different lines of related work below and how our work connects to and extends these.
Training-time consistency checks. Many semi-supervised [21] and self-supervised [7] learning algorithms enforce an invariance or contra-variance in model outputs, e.g., invariant predictions under adversarial transformations [58] or contrastive learning of data augmentations [22]. These algorithms are typically used when ground-truth labels are expensive rather than fundamentally unknown.</p>
<p>Test-time consistency checks. Many works study invariance (or contra-variance) of ML models, and language models in particular, to natural [41, 54, 43, 37] or adversarial [72, 48, 75] transformations. Some more involved consistencies were studied in basic language modeling tasks [64, 32, 46, 47, 45]. Some works in testing complex AI systems develop methods that apply natural [73, 81] and adversarial [62] transformations that do not directly rely on, but nevertheless operate in domains with ground truth. We extend these works by employing broader notions of consistency (apart from invariances) in domains with no ground truth.</p>
<p>Most metrics for model fairness $[9,31]$ evaluate prediction invariance across individuals or populations, regardless of model correctness (although some metrics do take correctness into account [40]).
Metamorphic testing. Our consistency check approach can be seen as an instance of metamorphic testing [23], which tests whether a logical relation holds over multiple runs of a program. Metamorphic testing has been used to check invariance of ML models under semantic-preserving transforms, similarly to the test-time consistency checks above [79, 80, 28]. Closest to ours are $k$-safety [24] and [65], which test monotonicity properties of model outputs (in particular, [24] has a legal experiment similar to our Section 7, albeit with simpler models). Our work differs in its focus on settings where ground truth is not merely expensive to obtain, but explicitly beyond human knowledge.
Failure modes in superhuman models. ML models achieve undisputed superhuman performance for various games, e.g., chess [17, 67] or Go [66]. Yet, game-playing agents for Go can be defeated by simple adversarial strategies designed against them [53, 77, 74]. These strategies are either found "end-to-end" (via self-play against the victim) [77, 74], or by checking consistency over boards that appear semantically equivalent to an examiner (either a human observer or a stronger model) [53]. In contrast, we consider the problem of eliciting bugs in model decisions when a proxy for ground truth (better than the model being evaluated) is not available.
Scalable oversight. Our work relates to the problem of scalable oversight [2], the ability to supervise models when ground truth is hard or impossible to obtain (e.g., because model abilities match or exceed humans). Our work is complementary to prior methods, which make capable models and humans interact to extract confidently correct answers [11, 44]; we instead study how humans could probe such models for confidently incorrect answers, i.e., human-verifiable bugs.
Model truthfulness. There are many attempts at evaluating the truthfulness of language model outputs [33, 55]. We envision that consistency tests could serve as a method for detecting when models provide dishonest answers or lies [15, 6, 61, 14, 25], under the assumption that it is easier to provide consistent answers when telling the truth [44].</p>
<h1>3 Consistency Checks without Ground Truth</h1>
<p>In this section, we introduce a framework for checking the consistency of model decisions in the absence of known ground truth.
Let $f$ be an ML model that, on input $x \in \mathcal{X}$, produces an output $\hat{y} \in \mathcal{Y}$. We assume that correctness of the model is hard to measure because the ground truth $y$ is unknown (but it exists). Such AI models are common: examples we consider include systems with superhuman abilities (e.g., a neural network that evaluates a chess position) or any models whose predictions are otherwise hard to verify (e.g., $f$ predicts the likelihood of future events). The correctness of such models can sometimes be evaluated in hindsight (e.g., a chess AI's decisions can be assessed on aggregate at the end of a game), but this makes it hard to identify flaws in individual model decisions proactively.
We propose to instead evaluate the consistency of the model $f$ across related inputs $\left{x_{1}, x_{2}, \ldots\right}$. Even if we are unable to measure the correctness of any one of the corresponding model outputs $\left{\hat{y}<em 2="2">{1}, \hat{y}</em>, \ldots\right}$, we may still be able to assert that at least one of the model's outputs must be incorrect.
Formally, we assume the existence of humanly verifiable predicates $P: \mathcal{X}^{<em>} \mapsto{0,1}$ and $Q: \mathcal{Y}^{</em>} \mapsto$ ${0,1}$, so that if $P$ holds over some inputs then $Q$ logically holds over the corresponding ground truths. We then say that the model $f$ is consistent with respect to $(P, Q)$ if, for all inputs,</p>
<p>$$
P\left(x_{1}, x_{2}, \ldots\right) \Longrightarrow Q\left(f\left(x_{1}\right), f\left(x_{2}\right), \ldots\right)
$$</p>
<p>A simple form of consistency check is invariance, where $P$ and $Q$ are measures of closeness between inputs and corresponding outputs. Our formalism extends to more complex consistency constraints. For instance, we might check that inputs and outputs are monotonically related (e.g., forecasts of the 100 m world record should not increase over time). In Sections 4 to 7, we consider various instantiations of this general paradigm and show examples of models violating consistency checks for each.
Proving that a model is consistent is hard for most properties (e.g., verifying invariance to adversarial examples is NP hard [50]); but a single counter-example to Equation (1) suffices to establish inconsistency, which implies the model's decision-making cannot be trusted for absolute correctness.
A randomized model $f$ can be "self-inconsistent" [78], i.e. multiple calls to $f(x)$ produce differing outputs that violate the predicate $Q$. The self-consistency of randomized models can be improved by averaging over multiple model outputs [78]. A model that often produces logically inconsistent outputs due to randomness alone should obviously not be trusted for any high-stakes scenarios.
In this paper, we mainly consider "hard" consistency constraints, where Equation (1) always holds. This setting promotes soundness (every violation we find is a real "bug") over completeness (we may find fewer bugs). As in traditional software testing, we could relax this soundness requirement to find more potential consistency violations, that could then be further vetted by a human.</p>
<h2>4 Applications Overview</h2>
<p>We instantiate our framework to check for logical inconsistencies in three applications.</p>
<ul>
<li>In Section 5, we consider a canonical setting for superhuman ML: chess. Instead of evaluating a chess model "end-to-end" over entire games, we evaluate the consistency of the model's core decisions, namely the evaluation of individual board positions and moves.</li>
<li>In Section 6, we look at the forecasting abilities of large language models. We evaluate whether forecasts made by GPT-3.5-turbo and GPT-4 reflect a logically consistent internal world model.</li>
<li>In Section 7, we evaluate the consistency of language models for making legal predictions, namely detecting human rights violations and making bail decisions.</li>
</ul>
<p>In all cases, we find clear logical inconsistencies in model decisions, thus showing that these models' decisions cannot be trusted for correctness. While inconsistencies are rare for in-distribution data (especially for chess models), we show that adversarial search can find significantly more failures.</p>
<h1>5 Superhuman Chess AIs</h1>
<p>Game-playing AIs are a prime example of models that operate vastly beyond human levels [66, 67, 59]. We focus here on chess, a canonical example of a complex decision-making task where humans can easily evaluate end-to-end model performance (i.e., did the model win?), but not individual model decisions [52]. Nevertheless, the rules of chess encode a number of simple invariances that are readily apparent and verifiable by even amateur players-a perfect application for our framework.</p>
<h3>5.1 Logical Consistency Checks in Chess</h3>
<p>We test chess models on the following consistency rules (see Figure 2 and Appendix B. 1 for examples):</p>
<p>Forced moves: Chess positions sometimes allow a single legal move (e.g., if the king is in check and has only one square to move). The player's move thus has no impact on the game's outcome. Hence, the positions before and after the forced move should have the same evaluation.</p>
<p>Board transformations: The orientation of a chess board only matters in so far as pawns move in one direction, and the king can castle with a rook in its original position. Thus, for positions without pawns and castling, any change of orientation of the board (rotations by $90^{\circ}, 180^{\circ}$, or $270^{\circ}$, and board mirroring over the x -axis, y -axis, or either diagonal) has no effect on the game outcome.</p>
<p>Position mirroring: The previous two consistency checks apply to very specific positions. Position mirroring is a more general check applicable to arbitrary positions. It encodes the simple invariant that mirroring the players' position, such that White gets the piece-setup of Black and vice versa, with the rest of the game state fixed (e.g., castling rights), results in a semantically identical position.</p>
<p>Recommended move: We consider a finer-grained form of the forced-move check above. Namely, the model's evaluation of a position should remain similar if we play the strongest move predicted by the model. Indeed, chess engines typically aim to measure the expected game outcome under optimal play from both players, so any optimal move should not affect this measure. It is true that, as opposed to other checks, the reduced uncertainty as the game progresses guarantees some small degree of inconsistency (on the order of $1 / N$, where $N$ is the number of half-moves until the end of the game). We do not consider these small discrepancies as failures in any of our consistency checks.</p>
<h3>5.2 Experimental Setup</h3>
<p>We analyze Leela Chess Zero [5], an open-source chess engine that plays at a superhuman level. We use a deterministic setup which reduces inference speed but does not impact the model's strength. The parameters we use are listed in Appendix B.2. By default, board evaluations use 400 Monte-Carlo Tree Search (MCTS) node evaluations, which yields a good trade-off between evaluation speed and superhuman performance [57]. The evaluation result is a number in the range $[-1,1]$, which predicts the expected game outcome ( $1=$ Win, $0=$ Draw, $-1=$ Loss) under optimal play for the current player.</p>
<p>For forced moves, recommended moves, and position mirroring, we evaluate model consistency on 400k board positions from the Caissabase database [16]. We measure the difference in the model's evaluation after a forced/recommended move or board mirroring. For board transformations, we generate 200k synthetic pawnless positions (which are rare in Master-level games). We randomly sample positions with the same set of four non-pawn pieces for both players, without castling. We then apply 7 random board symmetries and measure the maximum difference in evaluations.</p>
<h3>5.3 Results</h3>
<p>A summary of our consistency checks can be found in Table 1. As expected from a superhuman chess AI, the model is consistent most of the time. Yet, in a small amount of cases, the model's evaluations differ widely on semantically identical positions. These consistency violations are evidence of incorrect decisions made by a model with superhuman abilities.</p>
<p>We show four striking failures in Figure 2 (more examples are in Appendix B.3). In Figures 2a and 2b rotating or mirroring the position (which should not change the probability of winning) changes the winning chances of the current player by up to $69 \%$. In Figures 2c and 2d, the model similarly</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Examples of consistency failures in Leela Chess Zero. The model assigns drastically different winning probabilities before and after a board rotation (a) or mirroring the position (b). Playing the only possible move changes Leela's winning probability drastically (c) and playing Leela's recommended best move Re8 is a blunder that reduces Black's estimated winning probability from $68 \%$ to $0 \%$. (d)</p>
<p>Table 1: Comparison of the number of failures found in Leela for different consistency constraints, measured by the absolute difference in evaluation between two semantically equivalent boards.</p>
<p>Difference in Evaluation</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Consistency check</th>
<th style="text-align: right;">Samples</th>
<th style="text-align: right;">$&gt;0.05$</th>
<th style="text-align: right;">$&gt;0.1$</th>
<th style="text-align: right;">$&gt;0.25$</th>
<th style="text-align: right;">$&gt;0.5$</th>
<th style="text-align: right;">$&gt;0.75$</th>
<th style="text-align: right;">$&gt;1.0$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Board transformations</td>
<td style="text-align: right;">200k</td>
<td style="text-align: right;">$20.2 \%$</td>
<td style="text-align: right;">$6.1 \%$</td>
<td style="text-align: right;">$0.6 \%$</td>
<td style="text-align: right;">$0.09 \%$</td>
<td style="text-align: right;">$0.02 \%$</td>
<td style="text-align: right;">$&lt;0.01 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Recommended moves</td>
<td style="text-align: right;">400k</td>
<td style="text-align: right;">$19.5 \%$</td>
<td style="text-align: right;">$2.6 \%$</td>
<td style="text-align: right;">$0.2 \%$</td>
<td style="text-align: right;">$0.03 \%$</td>
<td style="text-align: right;">$0.01 \%$</td>
<td style="text-align: right;">$&lt;0.01 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Forced moves</td>
<td style="text-align: right;">400k</td>
<td style="text-align: right;">$6.3 \%$</td>
<td style="text-align: right;">$0.4 \%$</td>
<td style="text-align: right;">$0.05 \%$</td>
<td style="text-align: right;">$0.01 \%$</td>
<td style="text-align: right;">$&lt;0.01 \%$</td>
<td style="text-align: right;">$&lt;0.01 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Position mirroring</td>
<td style="text-align: right;">400k</td>
<td style="text-align: right;">$0.4 \%$</td>
<td style="text-align: right;">$0.07 \%$</td>
<td style="text-align: right;">$0.01 \%$</td>
<td style="text-align: right;">$&lt;0.01 \%$</td>
<td style="text-align: right;">$0 \%$</td>
<td style="text-align: right;">$0 \%$</td>
</tr>
</tbody>
</table>
<p>drastically changes its win probability estimate after the forced- or recommended best move is played. In all four cases, the model's evaluation must thus be (very) wrong in at least one of the two boards (or both).</p>
<p>Such consistency failures can directly influence game outcomes. For example, the position in Figure 2d is from a Master-level chess game, where Leela's recommended move (Re8) is a blunder that offers White a mating opportunity.
Scaling search improves consistency, but slowly. In order to test how consistency scales with model strength, we vary the number of MCTS search nodes. The results can be seen in Figure 8 and Table 6. As expected, stronger models are more consistent. Yet, even when we increase the search nodes by $8 \times$, to 3,200 nodes, the number of failures only drops by $3-6.6 \times$. More precisely, with a larger number of search nodes, the logarithm of the number of inconsistencies scales almost linearly with the logarithm of the search node count, no matter what the inconsistency threshold is (see Figure 8).</p>
<p>Adversarial search finds more violations. So far, we used brute-force to search for consistency violations. This is rather inefficient, yet still succeeded in finding many bugs in strong models. We now consider adversarial searches for model failures. Specifically, for our experiment with board transformations, we replace the random sampling of synthetic positions with a genetic algorithm that optimizes positions to maximize model inconsistency (see Appendix B. 2 for details). The results are in Table 2. For the strongest model we consider (with 1,600 search nodes), our genetic algorithm finds up to $9 \times$ more failures than a random search. Because genetic algorithms are based on heuristics, with very little known about their convergence, we rerun the algorithm twice to see how stable it is and how much variation there is in the number of consistency failures found. While there is some small variation in the number of samples found, the algorithm performs stably. Our second run even found a consistency failure with a difference in evaluation larger than 1 , which is larger than anything the random search algorithm found.</p>
<p>Table 2: Comparison between using random search and adversarial search to find consistency failures for board transformations. The adversarial approach finds up to $9 \times$ more failures.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: right;"></th>
<th style="text-align: right;">Difference in Evaluation for Board Transformations</th>
<th style="text-align: right;"></th>
<th style="text-align: right;"></th>
<th style="text-align: right;"></th>
<th style="text-align: right;"></th>
<th style="text-align: right;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Method</td>
<td style="text-align: right;">Nodes</td>
<td style="text-align: right;">$&gt;0.05$</td>
<td style="text-align: right;">$&gt;0.1$</td>
<td style="text-align: right;">$&gt;0.25$</td>
<td style="text-align: right;">$&gt;0.5$</td>
<td style="text-align: right;">$&gt;0.75$</td>
<td style="text-align: right;">$&gt;1.0$</td>
</tr>
<tr>
<td style="text-align: left;">Random</td>
<td style="text-align: right;">1600</td>
<td style="text-align: right;">$15.0 \%$</td>
<td style="text-align: right;">$3.8 \%$</td>
<td style="text-align: right;">$0.4 \%$</td>
<td style="text-align: right;">$0.05 \%$</td>
<td style="text-align: right;">$0.01 \%$</td>
<td style="text-align: right;">$0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Adversarial</td>
<td style="text-align: right;">1600</td>
<td style="text-align: right;">$8.9 \%$</td>
<td style="text-align: right;">$3.7 \%$</td>
<td style="text-align: right;">$1.0 \%$</td>
<td style="text-align: right;">$0.2 \%$</td>
<td style="text-align: right;">$0.09 \%$</td>
<td style="text-align: right;">$0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Adversarial (run 2)</td>
<td style="text-align: right;">1600</td>
<td style="text-align: right;">$8.8 \%$</td>
<td style="text-align: right;">$3.8 \%$</td>
<td style="text-align: right;">$1.2 \%$</td>
<td style="text-align: right;">$0.5 \%$</td>
<td style="text-align: right;">$0.2 \%$</td>
<td style="text-align: right;">$&lt;0.01 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Adversarial (run 3)</td>
<td style="text-align: right;">1600</td>
<td style="text-align: right;">$8.5 \%$</td>
<td style="text-align: right;">$3.7 \%$</td>
<td style="text-align: right;">$0.9 \%$</td>
<td style="text-align: right;">$0.3 \%$</td>
<td style="text-align: right;">$0.08 \%$</td>
<td style="text-align: right;">$0 \%$</td>
</tr>
</tbody>
</table>
<h1>5.4 Consistency Tests for Other Chess AIs</h1>
<p>Finally, we test how well our method generalizes to other chess AI systems that use different methods to search and evaluate a position. We do this by evaluating Stockfish [70], another popular superhuman chess AI. Unlike Leela, Stockfish uses principal variation search [56] (PVS) to evaluate positions and find the best move to play. Furthermore, Stockfish can evaluate positions both using an efficiently updateable neural network (NNUE) or using a classical evaluation function that uses handcrafted features developed by human experts.
For both Stockfish versions, we run the same experiments as was done for Leela. We convert Stockfish's output to $[-1,1]$, the same range as Leela's output. Note, however, that despite the Stockfish results having the same domain as Leela's results, it is not possible to directly compare a difference in evaluation of Stockfish with one from Leela due to some technical differences (see Appendix B. 4 for more information).
The results of these experiments can be found in Appendix B.5. Stockfish is consistent on average, with most evaluated positions having a difference in evaluation $\leq 0.25$. However, as with Leela Chess Zero, we again find multiple consistency failures for all tested consistency constraints.</p>
<h3>5.5 Summary</h3>
<p>In this section, we demonstrated that: (1) even superhuman models can exhibit many humanly verifiable failures; (2) consistency tests are a general, reliable way to find such failures (even when they are very rare); (3) an adversarially guided search may be necessary to uncover the most pernicious bugs; and (4) superhuman models with different designs exhibit varying levels of consistency, which do not necessarily correlate with standard measures of performance.</p>
<h2>6 Forecasting Future Events with Large Language Models</h2>
<p>Predicting and modeling the future is an important task for which ground truth is inherently unknown: as the saying goes, "It is difficult to make predictions, especially about the future." Asking questions about the future is also a natural way to test a model's ability to reason about the world. While recent LLMs are fairly poor forecasters [82, 69], it has been conjectured that superhuman prediction abilities about the world would be key to building safe AI systems that do not pursue independent goals [10].</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Consistency violations when forecasting events with GPT-4. (a) three non-monotonic forecasts, and one monotonic one; (b) consistency on predicted probabilities of an event occurring or not occurring; (c) consistency on predicted probabilities for paraphrased events.</p>
<p>Table 3: Mean violation magnitude and fraction of "strong" violations (with value above ε = 0.2).</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Negation</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Paraphrasing</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Monotonicity</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Bayes' rule</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Model</td>
<td style="text-align: center;">$&gt;0.2$</td>
<td style="text-align: center;">Mean</td>
<td style="text-align: center;">$&gt;0.2$</td>
<td style="text-align: center;">Mean</td>
<td style="text-align: center;">$&gt;0.2$</td>
<td style="text-align: center;">Mean</td>
<td style="text-align: center;">$&gt;0.2$</td>
<td style="text-align: center;">Mean</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3.5-turbo</td>
<td style="text-align: center;">$52.6 \%$</td>
<td style="text-align: center;">0.34</td>
<td style="text-align: center;">$30.8 \%$</td>
<td style="text-align: center;">0.21</td>
<td style="text-align: center;">$42.0 \%$</td>
<td style="text-align: center;">0.23</td>
<td style="text-align: center;">$68.6 \%$</td>
<td style="text-align: center;">0.28</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4</td>
<td style="text-align: center;">$10.9 \%$</td>
<td style="text-align: center;">0.10</td>
<td style="text-align: center;">$12.5 \%$</td>
<td style="text-align: center;">0.13</td>
<td style="text-align: center;">$16.0 \%$</td>
<td style="text-align: center;">0.11</td>
<td style="text-align: center;">$58.8 \%$</td>
<td style="text-align: center;">0.25</td>
</tr>
</tbody>
</table>
<h3>6.1 Logical Consistency Checks in Forecasting</h3>
<p>AIs that we trust to make predictions about the world should have a logically consistent world model. For example, model forecasts should satisfy the rules of probability, and obey physical rules. We test forecasting models on the following consistency checks (see Appendix C.2 for examples):</p>
<p><strong>Negation:</strong> The probability that an event happens should complement the probability that the event does not happen. For example, the answers to <em>"Will over half of the US Senate be women in 2035?"</em> and <em>"Will less than or equal to half of the US Senate be women in 2035?"</em> must sum to one.</p>
<p><strong>Paraphrasing:</strong> The phrasing of an event should not affect the forecast. For example, <em>"Will the share of Cavendish bananas in global exports fall below 50% by 2035?"</em>, and <em>"Before 2035, will the Cavendish's contribution to worldwide banana exports drop under 50%?"</em> should have the same answer.</p>
<p><strong>Monotonicity:</strong> Quantities that are hard to predict may still evolve predictably over time. For example, the answer to <em>"How many people will have climbed Mount Everest by year X?"</em> cannot decrease with time, and <em>"What will the men's 100m world record be in year X?"</em> cannot increase with time.</p>
<p><strong>Bayes' rule:</strong> Given two events <em>A</em> and <em>B</em>, we can ask about not only unconditional probabilities <em>P(A)</em> and <em>P(B)</em> as in the previous checks but also conditional probabilities <em>P(A | B)</em> and <em>P(B | A)</em>. For the answers to be consistent, they should satisfy Bayes' rule: <em>P(A | B) P(B) = P(B | A) P(A)</em>.</p>
<h3>6.2 Experimental Setup</h3>
<p>We test OpenAI's GPT-3.5-turbo and GPT-4, with temperatures 0 and 0.5. To reduce variance in the final output, we run each experiment multiple times and take the median forecasted quantity. In all experiments, we craft one-shot reasoning demonstrations and use chain-of-thought prompting to produce the final answer. The exact query parameters and prompts are listed in Appendix C.1.</p>
<p>We create a benchmark of 380 forecasting questions, with a total of 1220 variants covering the four consistency checks below. For each check, we introduce a <em>violation metric</em>, normalized to [0, 1], to measure the extent to which the model is inconsistent.</p>
<p><strong>Negation:</strong> We sample 175 (question, negated question) pairs from the Autocast dataset [82], filtering out questions that resolve before 2025, due to concerns over data leakage in OpenAI's models. We</p>
<p>measure the strength of a violation as:</p>
<p>$$
\left|\operatorname{Pr}(A)-\left(1-\operatorname{Pr}\left(A^{c}\right)\right)\right| \in[0,1]
$$</p>
<p>Paraphrasing: We sample 104 questions from the Autocast dataset and generate three paraphrases for each question using GPT-4, with manual filtering of invalid paraphrases. We measure the strength of a violation as</p>
<p>$$
\max <em i="i">{i, j}\left|\operatorname{Pr}\left(A</em>\right)\right| \in[0,1]
$$}\right)-\operatorname{Pr}\left(A_{j</p>
<p>where $A_{i}$ is the $i$-th paraphrase.
Monotonicity: We create 50 questions asking for predictions in the years 2025, 2028, 2032, 2036, and 2040. We combine manual question creation and prompting GPT-4 to generate similar questions (with manual quality filtering). We cover three categories of questions having a monotonic relationship with time: (1) sports records; (2) number of people who accomplish a given feat, e.g. "How many people will have climbed Mount Everest by the year X?"; (3) total occurrences of some event, e.g. "How many new medicines will the FDA approve by the year X?" Given the Spearman rank correlation coefficient of the forecasts and the years, $\rho \in[-1,1]$, we measure the strength of a violation as</p>
<p>$$
(1-\rho) / 2 \in[0,1]
$$</p>
<p>Bayes' rule: We create 51 tuples of questions asking for probabilities of events resolving between 2024 and 2050. The first two questions in a tuple refer to two events $A$ and $B$, and the last two questions ask for $\operatorname{Pr}(A \mid B)$ and $\operatorname{Pr}(B \mid A)$. The events $A$ and $B$ are chosen to neither be independent nor causally related in an obvious way, to ensure asking about $A \mid B$ and $B \mid A$ is in-distribution. We combine manual question creation and prompting GPT-4 to generate similar questions. The violation metric is</p>
<p>$$
\left|\operatorname{Pr}(A \mid B) \operatorname{Pr}(B)-\operatorname{Pr}(B \mid A) \operatorname{Pr}(A)\right|^{1 / 2} \in[0,1]
$$</p>
<p>Full histograms of the violation metrics over different experiments are in Appendix C. 3 and Figure 13.</p>
<h1>6.3 Results</h1>
<p>We report the average of each violation metric and the number of "strong" violations that exceed a threshold $\varepsilon=0.2$. Our results are summarized in Figure 3 and Table 3, with raw results in Appendix C.3. Both GPT-3.5-turbo and GPT-4 (with temperature 0) are very inconsistent forecasters, with a large fraction of questions resulting in strong consistency violations. While we cannot verify the correctness of any of the models' forecasts, we can nevertheless assert that these forecasts are inherently unreliable. We see a clear improvement in consistency with GPT-4, except on our most complex Bayes' rule check. This indicates that more involved consistency checks could remain a reliable way of surfacing model failures, even if model abilities improve drastically in the future.</p>
<p>Are inconsistencies just due to randomness? Stochastic models can be inconsistent due to randomness alone. However, our tests show inconsistency far beyond the variance in model outputs (even with temperature zero, OpenAI's models exhibit some stochasticity [35, 20]). To verify this, we run a self-consistency version of our Paraphrasing experiment, where we query the exact same question four times. We find that stochasticity accounts for less than $20 \%$ of all the "strong" ( $\varepsilon=0.2$ ) violations we find. For details, and additional experiments with temperature 0.5, see Appendix C.3.2.</p>
<h3>6.4 Prompting for Consistency</h3>
<p>In this section, we try to prompt GPT-3.5-turbo and GPT-4 to be more consistent; this is a simple proxy for training models to be more consistent. The main question we ask is not whether there exist ways to improve consistency metrics, which we believe to be true and predictable: Table 3 hints that improvements in general capability lead to improvements in consistency. Rather, we ask whether improving some consistency metrics improves or degrades others. For example, it is not clear whether improving negation consistency would in general improve paraphrasing consistency, or even whether there is a tradeoff between the two. This is important because we can only test and train against a finite number of consistency metrics, and not the general notion of a "consistent world model". It would be excellent news if targeted improvement on some consistency checks generalized to others,</p>
<p>as this would give confidence that we could track consistency of superhuman models with some degree of confidence.</p>
<p>The following experiments deal with probabilistic forecasts (the Negation, Paraphrasing, and Bayes' rule checks); we do not test on the Monotonicity experiment because the model's output in those tasks is a scalar value.</p>
<p>Negation consistency prompting. We instruct the model to derive the opposite question at the beginning of the answer, and then answer the pair of questions simultaneously. The intuition for why this should help on the Negation check is as follows: the model is asked a pair of questions $a$ and $b$ (describing events $A$ and $\neg A$ ) in parallel. If it manages to derive $b$ from $a$ and vice versa at the start of its chain of thought, then it is going to reason through the same pair of questions both times, helping consistency. This can fail if the descriptions $a$ and $b$ are not natural negations of each other, or if answering $(a, b)$ is not equivalent to answering $(b, a)$; nevertheless we expect it to help on average.</p>
<p>We craft a system prompt instructing the model to follow the above, and a one-shot reasoning demonstration following a similar structure as the prompt in the original experiments. We keep other parameters the same as in the original experiments. In Table 4, we see the Negation violation metrics have improved on both models, with GPT-4 close to acing our (non-adversarial) tests with the 0.2 lenience threshold. The full results are in Table 12.</p>
<p>However, the violation on the Paraphrasing check got slightly worse, and on Bayes' rule has not changed significantly. We see this as a small bit of evidence that improving consistency on one check does not necessarily improve consistency in general.</p>
<p>Paraphrasing consistency prompting. We report a negative result here: we were not able to get the model to significantly improve on the full Paraphrasing check by prompting. The most promising method we tried was to instruct the model to derive a canonical paraphrase of the question, and answer it instead of the original question. The intuition is as follows: the model is asked for multiple descriptions $a_{1}, \ldots, a_{n}$ of the same event $A$ in parallel. If it derives the same canonical paraphrase $a^{\prime}$ for all of $a_{i}$, then it is going to answer the same question $a^{\prime}$ multiple times, helping consistency. The results are in Table 4 and Table 13. There is no clear improvement, due to the combination of the model not deriving the same paraphrase and (presumably) performance decay due to confusing instructions in the prompt.</p>
<p>This is not to discourage future work; it is likely we just did not find the right prompt. Paraphrasing has more degrees of freedom compared to negating the question, thus the Paraphrasing check might be harder to prompt or train for.</p>
<p>The prompts and the full results for both alternative prompting methods are in Appendix C.5.</p>
<h1>7 Legal Decision-making</h1>
<p>Reaching decisions on complex legal cases can be long and costly, and the "correctness" of decisions is often contested (e.g., as evidenced by appeal courts). ML has been explored both to automate the processing of legal information [19, 27] and even to reduce human biases in legal decisions [51].</p>
<p>Table 4: Comparing prompting methods (temperature 0 ). Mean violation magnitude and fraction of "strong" violations (with value above $\varepsilon=0.2$ ).</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Negation</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Paraphrasing</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Bayes' rule</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Model</td>
<td style="text-align: center;">$&gt;0.2$</td>
<td style="text-align: center;">Mean</td>
<td style="text-align: center;">$&gt;0.2$</td>
<td style="text-align: center;">Mean</td>
<td style="text-align: center;">$&gt;0.2$</td>
<td style="text-align: center;">Mean</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3.5-turbo, original results</td>
<td style="text-align: center;">$52.6 \%$</td>
<td style="text-align: center;">0.34</td>
<td style="text-align: center;">$30.8 \%$</td>
<td style="text-align: center;">0.21</td>
<td style="text-align: center;">$68.6 \%$</td>
<td style="text-align: center;">0.28</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3.5-turbo, negation prompting</td>
<td style="text-align: center;">$37.1 \%$</td>
<td style="text-align: center;">0.25</td>
<td style="text-align: center;">$41.3 \%$</td>
<td style="text-align: center;">0.28</td>
<td style="text-align: center;">$51.0 \%$</td>
<td style="text-align: center;">0.25</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3.5-turbo, paraphrase prompting</td>
<td style="text-align: center;">$44.0 \%$</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">$37.5 \%$</td>
<td style="text-align: center;">0.26</td>
<td style="text-align: center;">$45.1 \%$</td>
<td style="text-align: center;">0.22</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4, original results</td>
<td style="text-align: center;">$10.9 \%$</td>
<td style="text-align: center;">0.10</td>
<td style="text-align: center;">$12.5 \%$</td>
<td style="text-align: center;">0.13</td>
<td style="text-align: center;">$58.8 \%$</td>
<td style="text-align: center;">0.25</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4, negation prompting</td>
<td style="text-align: center;">$2.9 \%$</td>
<td style="text-align: center;">0.06</td>
<td style="text-align: center;">$17.3 \%$</td>
<td style="text-align: center;">0.17</td>
<td style="text-align: center;">$68.6 \%$</td>
<td style="text-align: center;">0.28</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4, paraphrase prompting</td>
<td style="text-align: center;">$12.6 \%$</td>
<td style="text-align: center;">0.13</td>
<td style="text-align: center;">$14.4 \%$</td>
<td style="text-align: center;">0.13</td>
<td style="text-align: center;">$62.7 \%$</td>
<td style="text-align: center;">0.27</td>
</tr>
</tbody>
</table>
<p>The difficulties in assessing the correctness or fairness of human legal decisions extend to AI tools that are used to assist or automate legal decisions. In this section, we show how to reveal clear logical inconsistencies in two different language models used for predicting legal verdicts: (1) a BERT model that evaluates violations of the European Convention of Human Rights; (2) GPT-3.5-turbo and GPT-4 models prompted to predict bail decisions given a defendant's criminal record.</p>
<h1>7.1 Logical Consistency Checks in Legal Decisions</h1>
<p>We consider two types of consistency checks:
Paraphrasing: We test whether changing the phrasing of a legal case changes the model's decision.
Partial ordering: While the "correctness" of legal decisions is hard to assess, there can still be clear ways of "ranking" different outcomes. We consider an extreme example here, where we test whether a bail-decision model could favorably switch its decision if the defendant commits more crimes.</p>
<h3>7.2 Experimental Setup</h3>
<p>Human rights violations: Our first task is to determine whether a legal case contains a violation of the European Court of Human Rights (ECHR). We use a prior dataset of ECHR cases [18] (these cases were first heard by various national courts, hinting at the difficulty of determining the correctness of such judgments). Each legal case in the dataset is a list of case facts, written in natural language. Our experimental setup follows Chalkidis et al. [19]. We use their pre-trained legal-BERT-sc model to encode each case fact, fine-tune a binary classifier on the ECHR training dataset, and sample a subset of 500 cases from the ECHR test set for evaluation. The full experimental pipeline is described in Appendix D.1.
We conduct two consistency experiments: the first is black-box, where we paraphrase a random case fact fed to the model, and measure the difference in model outputs. The second is a stronger white-box experiment, where we paraphrase the case fact that the model considers most important (as measured by the model's final attention layer). In both cases, we use GPT-3.5-turbo to automatically paraphrase case facts, and manually verify that the resulting fact remains semantically unchanged.</p>
<p>Bail decisions: Our second legal task is to make bail decisions given a suspect's criminal record. We use data collected by ProPublica to investigate biases in the COMPAS system [49]. The data contains a suspect's demographics, the arrest reason, and the number and type of crimes in their record. We ask GPT-3.5-turbo to decide if a suspect should be granted bail, using the same prompts as in prior work that asked humans [30] or LLMs [36] to predict recidivism risk. (see Appendix E. 1 for the exact prompts). The model replies with either YES, NO, or UNDECIDED for each case.
For 1560 suspects, we create 10 "counterfactual" suspects with criminal records that are either demonstrably worse or better than the original suspect, with other demographic data unchanged. We either switch the arrest crime between a misdemeanor and felony or change the number of prior crimes (see Appendix E.1). We query GPT-3.5-turbo with temperatures 0 and 0.5 and check for cases where the model switches its decision to approve bail when a suspect's record is made worse.
A similar experimental design was considered in Christakis et al. [24], with simpler neural network and decision tree classifiers. Our combined results show that very different model classes can exhibit similar logical inconsistencies.</p>
<h3>7.3 Results</h3>
<p>Human rights violations: Figure 5 shows the consistency of decisions on legal rights violations to paraphrasing. For random paraphrases (Figure 4a), the model is very consistent. The model flips its decision in some cases, but only for original predictions close to $50 \%$. Examples of violations are in Appendix D.2.
If we paraphrase the case fact that the model considers most important, consistency violations are much more severe (Figure 4b). In 50\% of cases where the model does not predict a human rights violation, paraphrasing flips the model's decision (flips in the opposite direction only occur in 7\% of cases, indicating a strong bias towards positive predictions). This shows again that white-box</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: Likelihood that our legal model predicts a human rights violation, before and after paraphrasing one case fact. Red-marked points are cases where the model's hard decision flips. (a) A case fact is chosen at random and paraphrased; (b) The case fact to which the model assigns the most importance is paraphrased.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6: Example of a paradoxical judgment of GPT-3.5-turbo on the COMPAS dataset.
adversarial testing may be critical for finding pernicious consistency bugs.</p>
<p>Bail decisions: We find that GPT-3.5-turbo is much more consistent here than on the forecasting tasks in Section 6, presumably due to the low dimensionality of our bail data. Nevertheless, with temperature 0 ., we still find consistency violations in 78 out of 1560 cases ( $5 \%$ ), where the model's original decision to deny bail is changed when presented with an objectively worse criminal record. An example of such a paradoxical judgment is illustrated in Figure 6, where the model would approve bail if the suspect had committed an additional crime.</p>
<p>The number of consistency violations for this task is much lower than in the other LLM tasks we considered. This is likely due to the input space being parametrized by a very small number of features, which makes it easier for the model to apply simple (and thus mostly consistent) decision rules. These decisions are not necessarily correct from a legal perspective, but we do not see as many clear inconsistencies. We provide more detailed results in Appendix E.2.</p>
<h1>8 Limitations and Future Outlook</h1>
<p>While we have succeeded in demonstrating clear logical consistency violations in a variety of settings and models, our current approach has some limitations that we hope future work can address.</p>
<p>Efficiency. First, some inconsistencies we find are rare, especially for superhuman models such as Leela. One reason is that we mainly search for bugs in a black-box manner with random sampling. As we have shown for both chess evaluations and legal decisions, a white-box adversarial search reveals many more violations. As models become stronger (and exhibit superhuman abilities on tasks beyond games), consistency bugs may be so rare that they can only be discovered by adversarially guided search. Even then, although finding polynomially verifiable inconsistencies is computable in the limit [38], it is unclear whether important inconsistencies can be detected efficiently.</p>
<p>Soundness. Second, while we focus on "hard" consistency constraints (i.e., which should always logically hold), our experiments sometimes use automated tools to generate (pseudo)-consistent tuples (e.g., via paraphrasing). While we manually checked these, it is possible that we missed some unsound checks (e.g. paraphrases that can be plausibly interpreted as describing different events). Again, as models become better and bugs rarer, relaxing soundness may be necessary in order to get checks with better completeness. Discovered bugs would then have to be further vetted by humans or trustworthy models. Concurrent work [26] has explored multi-turn cross-examination (as proposed in [8]) to elicit "soft" inconsistencies, although in settings where the ground truth is available. We leave it to future work to explore ways to automate and scale this process to superhuman models.</p>
<p>Feedback loops. Performative predictions [63, 4] are predictions which can influence the outcome they aim to predict. Our framework is not fit for performative prediction out-of-the-box, as it relies on asking instances of the model for predictions in parallel. For testing superhuman models that we use to make high stakes decisions, the performative prediction issue is critical. For example, we will not make the recommended decision if we detect an issue with the model's consistency because of</p>
<p>that recommendation, especially if the issue is about the model's honesty. Instead of honest reporting of beliefs, in this setting it makes more sense to consider fixed points: predictions which accurately reflect the beliefs after the predictions have been made [60]. There can be multiple distinct fixed points, which our consistency checks do not currently account for.
False negatives. Finally, as for any (incomplete) technique for discovering bugs, finding nothing does not mean an absence of bugs! While violations of our consistency checks are a clear sign that a model's correctness cannot be trusted for high-stakes settings, this does not imply that future, better models that pass simple consistency checks should be absolutely trusted.</p>
<h1>Acknowledgments and Disclosure of Funding</h1>
<p>Daniel Paleka is partially supported by New Science. We thank Jérémy Scheurer, Javier Rando, Edoardo Debenedetti, Maria Christakis, Craig Falls, and Owain Evans for useful feedback and ideas.</p>
<h2>References</h2>
<p>[1] Lev Abramov, Vladimir Bagirov, Mikhail Botvinnik, Srdan Cvetkovic, Miroslav Filip, Efim Geller, Aivars Gipslis, Eduard Gufeld, Vlastimil Hort, Garry Kasparov, Viktor Korchnoi, Zdenko Krnic, Bent Larsen, Aleksandar Matanović, Nikolay Minev, John Nunn, Bruno Parma, Lev Polugaevsky, Alexey Suetin, Evgeny Sveshnikov, Mark Taimanov, Dragan Ugrinovic, and Wolfgang Uhlmann. Encyclopaedia of chess openings, volume B (2nd ed.). Chess Informant, 1984. ISBN 0-7134-3716-2.
[2] Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mané. Concrete problems in AI safety. arXiv preprint arXiv:1606.06565, 2016.
[3] Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner. Machine bias. In Ethics of data and analytics, pages 254-264. Auerbach Publications, 2016.
[4] Stuart Armstrong and Xavier O’Rorke. Good and safe uses of ai oracles. arXiv preprint arXiv:1711.05541, 2017.
[5] Lc0 authors. What is Lc0?, 2018. URL https://lczero.org/dev/wiki/what-is-lc0/. [Online; Last accessed 05-April-2023].
[6] Anton Bakhtin, David J Wu, Adam Lerer, Jonathan Gray, Athul Paul Jacob, Gabriele Farina, Alexander H Miller, and Noam Brown. Mastering the game of no-press Diplomacy via humanregularized reinforcement learning and planning. arXiv preprint arXiv:2210.05492, 2022.
[7] Randall Balestriero, Mark Ibrahim, Vlad Sobal, Ari Morcos, Shashank Shekhar, Tom Goldstein, Florian Bordes, Adrien Bardes, Gregoire Mialon, Yuandong Tian, et al. A cookbook of self-supervised learning. arXiv preprint arXiv:2304.12210, 2023.
[8] Beth Barnes, Paul Christiano, L Ouyang, and G Irving. Writeup: Progress on AI safety via debate, 2020, 2020. URL https://www.alignmentforum.org/posts/ Br4xDbYu4Frwrb64a/writeup-progress-on-ai-safety-via-debate-1.
[9] Solon Barocas and Andrew D Selbst. Big data's disparate impact. California law review, pages $671-732,2016$.
[10] Yoshua Bengio. AI scientists: Safe and useful AI?, 2023. URL https://yoshuabengio.org/ 2023/05/07/ai-scientists-safe-and-useful-ai/. Online; accessed 10-May-2023.
[11] Samuel R. Bowman, Jeeyoon Hyun, Ethan Perez, Edwin Chen, Craig Pettit, Scott Heiner, Kamilė Lukošiūtė, Amanda Askell, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Christopher Olah, Daniela Amodei, Dario Amodei, Dawn Drain, Dustin Li, Eli Tran-Johnson, Jackson Kernion, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Liane Lovitt, Nelson Elhage, Nicholas Schiefer, Nicholas Joseph, Noemí Mercado, Nova DasSarma, Robin Larson, Sam McCandlish, Sandipan Kundu, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Timothy Telleen-Lawton, Tom Brown, Tom Henighan, Tristan Hume, Yuntao Bai, Zac Hatfield-Dodds, Ben Mann, and Jared</p>
<p>Kaplan. Measuring progress on scalable oversight for large language models. arXiv preprint arXiv:2211.03540, 2022.
[12] Gwern Branwen. The scaling hypothesis, 2021.
[13] Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, and Yi Zhang. Sparks of artificial general intelligence: Early experiments with GPT-4. arXiv preprint arXiv:2303.12712, 2023.
[14] Collin Burns, Haotian Ye, Dan Klein, and Jacob Steinhardt. Discovering latent knowledge in language models without supervision. arXiv preprint arXiv:2212.03827, 2022.
[15] Matthew Burtell and Thomas Woodside. Artificial influence: An analysis of AI-driven persuasion. arXiv preprint arXiv:2303.08721, 2023.
[16] Caissabase, 2023. URL http://caissabase.co.uk/. Accessed on 13-May-2023.
[17] Murray Campbell, A Joseph Hoane Jr, and Feng-hsiung Hsu. Deep blue. Artificial intelligence, 134(1-2):57-83, 2002.
[18] Ilias Chalkidis, Ion Androutsopoulos, and Nikolaos Aletras. Neural legal judgment prediction in English. arXiv preprint arXiv:1906.02059, 2019.
[19] Ilias Chalkidis, Manos Fergadiotis, Prodromos Malakasiotis, Nikolaos Aletras, and Ion Androutsopoulos. LEGAL-BERT: The Muppets straight out of law school. arXiv preprint arXiv:2010.02559, 2020.
[20] Sam Chann. Nondeterminism in Non-determinism in GPT-4 is caused by Sparse MoE, 2023. URL https://web.archive.org/web/20230908235421/https://152334h. github.io/blog/non-determinism-in-gpt-4/. Accessed on 27-Sept-2023.
[21] Olivier Chapelle, Bernhard Scholkopf, and Alexander Zien. Semi-supervised learning. IEEE Transactions on Neural Networks, 20(3):542-542, 2009.
[22] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In International conference on machine learning, pages 1597-1607. PMLR, 2020.
[23] Tsong Y Chen, Shing C Cheung, and Shiu Ming Yiu. Metamorphic testing: a new approach for generating next test cases. Technical report, The Hong Kong University of Science and Technology, 1998.
[24] Maria Christakis, Hasan Ferit Eniser, Jörg Hoffmann, Adish Singla, and Valentin Wüstholz. Specifying and testing $k$-safety properties for machine-learning models. arXiv preprint arXiv:2206.06054, 2022.
[25] Paul Christiano, Ajeya Cotra, and Mark Xu. Eliciting latent knowledge: How to tell if your eyes deceive you, 2022. URL https://www.alignmentforum.org/ posts/qHCDysDnvhteW7kRd/arc-s-first-technical-report-eliciting-latentknowledge. Accessed on 13-May-2023.
[26] Roi Cohen, May Hamri, Mor Geva, and Amir Globerson. LM vs LM: Detecting factual errors via cross examination. arXiv preprint arXiv:2305.13281, 2023.
[27] Junyun Cui, Xiaoyu Shen, Feiping Nie, Zheng Wang, Jinglong Wang, and Yulong Chen. A survey on legal judgment prediction: Datasets, metrics, models and challenges. arXiv preprint arXiv:2204.04859, 2022.
[28] Yao Deng, Guannan Lou, Xi Zheng, Tianyi Zhang, Miryung Kim, Huai Liu, Chen Wang, and Tsong Yueh Chen. BMT: Behavior driven development-based metamorphic testing for autonomous driving models. In 2021 IEEE/ACM 6th International Workshop on Metamorphic Testing (MET), pages 32-36. IEEE, 2021.
[29] Lc0 developers. Leela Chess Zero. https://github.com/LeelaChessZero/lc0, 2018.</p>
<p>[30] Julia Dressel and Hany Farid. The accuracy, fairness, and limits of predicting recidivism. Science advances, 4(1):eaao5580, 2018.
[31] Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairness through awareness. In Proceedings of the 3rd innovations in theoretical computer science conference, pages 214-226, 2012.
[32] Yanai Elazar, Nora Kassner, Shauli Ravfogel, Abhilasha Ravichander, Eduard Hovy, Hinrich Schütze, and Yoav Goldberg. Measuring and improving consistency in pretrained language models. Transactions of the Association for Computational Linguistics, 9:1012-1031, 2021.
[33] Owain Evans, Owen Cotton-Barratt, Lukas Finnveden, Adam Bales, Avital Balwit, Peter Wills, Luca Righetti, and William Saunders. Truthful AI: Developing and governing AI that does not lie. arXiv preprint arXiv:2110.06674, 2021.
[34] Niklas Fiekas. Syzygy endgame tablebases, 2023. URL https://syzygy-tables.info/. Accessed on 31-May-2023.
[35] Paul Fishwick. A question on determinism. OpenAI Comunity Forum, Aug 2021. URL https://web.archive.org/web/20230328011953/https://community.openai. com/t/a-question-on-determinism/8185/2.
[36] Deep Ganguli, Danny Hernandez, Liane Lovitt, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova Dassarma, Dawn Drain, Nelson Elhage, Sheer El Showk, Stanislav Fort, Zac Hatfield-Dodds, Tom Henighan, Scott Johnston, Andy Jones, Nicholas Joseph, Jackson Kernian, Shauna Kravec, Ben Mann, Neel Nanda, Kamal Ndousse, Catherine Olsson, Daniela Amodei, Tom Brown, Jared Kaplan, Sam McCandlish, Christopher Olah, Dario Amodei, and Jack Clark. Predictability and surprise in large generative models. In 2022 ACM Conference on Fairness, Accountability, and Transparency, pages 1747-1764, 2022.
[37] Matt Gardner, Yoav Artzi, Victoria Basmova, Jonathan Berant, Ben Bogin, Sihao Chen, Pradeep Dasigi, Dheeru Dua, Yanai Elazar, Ananth Gottumukkala, et al. Evaluating models' local decision boundaries via contrast sets. arXiv preprint arXiv:2004.02709, 2020.
[38] Scott Garrabrant, Tsvi Benson-Tilsen, Andrew Critch, Nate Soares, and Jessica Taylor. Logical induction. arXiv preprint arXiv:1609.03543, 2016.
[39] Significant Gravitas. Auto-GPT: An autonomous GPT-4 experiment, 2023. URL https: //github.com/Significant-Gravitas/Auto-GPT.
[40] Moritz Hardt, Eric Price, and Nati Srebro. Equality of opportunity in supervised learning. Advances in neural information processing systems, 29, 2016.
[41] Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. arXiv preprint arXiv:1903.12261, 2019.
[42] Dan Hendrycks and Mantas Mazeika. X-risk analysis for AI research. arXiv preprint arXiv:2206.05862, 2022.
[43] Arian Hosseini, Siva Reddy, Dzmitry Bahdanau, R Devon Hjelm, Alessandro Sordoni, and Aaron Courville. Understanding by understanding not: Modeling negation in language models. arXiv preprint arXiv:2105.03519, 2021.
[44] Geoffrey Irving, Paul Christiano, and Dario Amodei. AI safety via debate. arXiv preprint arXiv:1805.00899, 2018.
[45] Myeongjun Jang and Thomas Lukasiewicz. Consistency analysis of ChatGPT. arXiv preprint arXiv:2303.06273, 2023.
[46] Myeongjun Jang, Deuk Sin Kwon, and Thomas Lukasiewicz. Accurate, yet inconsistent? consistency analysis on language understanding models. arXiv preprint arXiv:2108.06665, 2021.</p>
<p>[47] Myeongjun Jang, Deuk Sin Kwon, and Thomas Lukasiewicz. BECEL: Benchmark for consistency evaluation of language models. In Proceedings of the 29th International Conference on Computational Linguistics, pages 3680-3696, Gyeongju, Republic of Korea, October 2022. International Committee on Computational Linguistics. URL https://aclanthology.org/ 2022.coling-1.324.
[48] Robin Jia and Percy Liang. Adversarial examples for evaluating reading comprehension systems. arXiv preprint arXiv:1707.07328, 2017.
[49] Surya Mattu Julia Angwin, Jeff Larson and Lauren Kirchner. Machine bias. https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing, May 2016. [Online; accessed 17-December-2022].
[50] Guy Katz, Clark Barrett, David L Dill, Kyle Julian, and Mykel J Kochenderfer. Reluplex: An efficient SMT solver for verifying deep neural networks. In Computer Aided Verification: 29th International Conference, CAV 2017, Heidelberg, Germany, July 24-28, 2017, Proceedings, Part I 30, pages 97-117. Springer, 2017.
[51] Jon Kleinberg, Himabindu Lakkaraju, Jure Leskovec, Jens Ludwig, and Sendhil Mullainathan. Human decisions and machine predictions. The quarterly journal of economics, 133(1):237-293, 2018.
[52] Will Knight. Alpha Zero's "alien" chess shows the power, and the peculiarity, of AI, 2017. URL https://www.technologyreview.com/2017/12/08/147199/.
[53] Li-Cheng Lan, Huan Zhang, Ti-Rong Wu, Meng-Yu Tsai, I Wu, Cho-Jui Hsieh, et al. Are AlphaZero-like agents robust to adversarial perturbations? arXiv preprint arXiv:2211.03769, 2022.
[54] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, Benjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian Cosgrove, Christopher D. Manning, Christopher Ré, Diana Acosta-Navas, Drew A. Hudson, Eric Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue Wang, Keshav Santhanam, Laurel Orr, Lucia Zheng, Mert Yuksekgonul, Mirac Suzgun, Nathan Kim, Neel Guha, Niladri Chatterji, Omar Khattab, Peter Henderson, Qian Huang, Ryan Chi, Sang Michael Xie, Shibani Santurkar, Surya Ganguli, Tatsunori Hashimoto, Thomas Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, and Yuta Koreeda. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110, 2022.
[55] Stephanie Lin, Jacob Hilton, and Owain Evans. TruthfulQA: Measuring how models mimic human falsehoods. arXiv preprint arXiv:2109.07958, 2021.
[56] T Anthony Marsland and Murray Campbell. Parallel search of strongly ordered game trees. ACM Computing Surveys (CSUR), 14(4):533-551, 1982.
[57] Marco Meloni. Stockfish and Lc0, test at different number of nodes, Nov 2022. URL https://www.melonimarco.it/en/2021/03/08/stockfish-and-lc0-test-at-different-number-of-nodes/. Accessed on 13-May-2023.
[58] Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii. Virtual adversarial training: A regularization method for supervised and semi-supervised learning. IEEE transactions on pattern analysis and machine intelligence, 41(8):1979-1993, 2018.
[59] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing Atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.
[60] Caspar Oesterheld, Johannes Treutlein, Emery Cooper, and Rubi Hudson. Incentivizing honest performative predictions with proper scoring rules. arXiv preprint arXiv:2305.17601, 2023.</p>
<p>[61] Alexander Pan, Chan Jun Shern, Andy Zou, Nathaniel Li, Steven Basart, Thomas Woodside, Jonathan Ng, Hanlin Zhang, Scott Emmons, and Dan Hendrycks. Do the rewards justify the means? Measuring trade-offs between rewards and ethical behavior in the MACHIAVELLI benchmark. arXiv preprint arXiv:2304.03279, 2023.
[62] Kexin Pei, Yinzhi Cao, Junfeng Yang, and Suman Jana. DeepXplore: Automated whitebox testing of deep learning systems. In proceedings of the 26th Symposium on Operating Systems Principles, pages 1-18, 2017.
[63] Juan C. Perdomo, Tijana Zrnic, Celestine Mendler-Dünner, and Moritz Hardt. Performative prediction, 2020.
[64] Marco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin, and Sameer Singh. Beyond accuracy: Behavioral testing of NLP models with CheckList. arXiv preprint arXiv:2005.04118, 2020.
[65] Arnab Sharma and Heike Wehrheim. Testing monotonicity of machine learning models, 2020.
[66] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, Yutian Chen, Timothy Lillicrap, Fan Hui, Laurent Sifre, George van den Driessche, Thore Graepel, and Demis Hassabis. Mastering the game of Go without human knowledge. Nature, 550(7676):354-359, 2017.
[67] David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. A general reinforcement learning algorithm that masters chess, Shogi, and Go through self-play. Science, 362(6419):1140-1144, 2018.
[68] Adam Slowik and Halina Kwasnicka. Evolutionary algorithms and their applications to engineering problems. Neural Computing and Applications, 32:12363-12379, 2020.
[69] Markus Sobkowski. Manifold Markets: User GPT-4 (Bot), 2023. URL https: //web.archive.org/web/20230511132857/https://manifold.markets/GPT4? tab=portfolio. Accessed on 11-May-2023.
[70] Stockfish 15.1. Stockfish 15.1, 2023. URL https://stockfishchess.org/. Accessed on 22-Jun-2023.
[71] Stockfish developers. Stockfish official repository. https://github.com/officialstockfish/Stockfish, 2023.
[72] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.
[73] Yuchi Tian, Kexin Pei, Suman Jana, and Baishakhi Ray. DeepTest: Automated testing of deep-neural-network-driven autonomous cars. In Proceedings of the 40th international conference on software engineering, pages 303-314, 2018.
[74] Finbarr Timbers, Nolan Bard, Edward Lockhart, Marc Lanctot, Martin Schmid, Neil Burch, Julian Schrittwieser, Thomas Hubert, and Michael Bowling. Approximate exploitability: Learning a best response in large games. arXiv preprint arXiv:2004.09677, 2020.
[75] Miles Turpin, Julian Michael, Ethan Perez, and Samuel R Bowman. Language models don't always say what they think: Unfaithful explanations in chain-of-thought prompting. arXiv preprint arXiv:2305.04388, 2023.
[76] Sahil Verma and Julia Rubin. Fairness definitions explained. In Proceedings of the international workshop on software fairness, pages 1-7, 2018.
[77] Tony Tong Wang, Adam Gleave, Nora Belrose, Tom Tseng, Joseph Miller, Michael D Dennis, Yawen Duan, Viktor Pogrebniak, Sergey Levine, and Stuart Russell. Adversarial policies beat professional-level Go AIs. arXiv preprint arXiv:2211.00241, 2022.</p>
<p>[78] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. Selfconsistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022.
[79] Xiaoyuan Xie, Joshua WK Ho, Christian Murphy, Gail Kaiser, Baowen Xu, and Tsong Yueh Chen. Testing and validating machine learning classifiers by metamorphic testing. Journal of Systems and Software, 84(4):544-558, 2011.
[80] Jie M Zhang, Mark Harman, Lei Ma, and Yang Liu. Machine learning testing: Survey, landscapes and horizons. IEEE Transactions on Software Engineering, 48(1):1-36, 2020.
[81] Mengshi Zhang, Yuqun Zhang, Lingming Zhang, Cong Liu, and Sarfraz Khurshid. DeepRoad: GAN-based metamorphic testing and input validation framework for autonomous driving systems. In Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering, pages 132-142, 2018.
[82] Andy Zou, Tristan Xiao, Ryan Jia, Joe Kwon, Mantas Mazeika, Richard Li, Dawn Song, Jacob Steinhardt, Owain Evans, and Dan Hendrycks. Forecasting future world events with neural networks. arXiv preprint arXiv:2206.15474, 2022.</p>
<h1>A Costs and Compute</h1>
<p>OpenAI API tokens. The forecasting experiments in Section 6 and the bail experiments in Section 7 were run on a total cost of less than $\$ 2000$ in OpenAI API tokens. The paraphrases for the ECHR experiments in Section 7 were generated using GPT-3.5-turbo, with the costs below $\$ 100$.</p>
<p>Compute cost. The experiments with Leela Chess Zero (see Section 5 and Appendix B), were done on a cluster with 8 NVIDIA RTX A6000 GPUs. The total single-GPU run-time of all experiments amounts to 73.5 GPU days.</p>
<h2>B Additional Details and Results for Chess Experiments</h2>
<h2>B. 1 Examples of Consistency Checks</h2>
<p>Figure 7 shows examples of our four consistency constraints. For the board transformations- and position mirroring consistencies, we check whether the evaluations of the original board and the transformed board are equal. For the forced move- and recommended move consistencies, we check whether the evaluations of the original board and the position after applying the best move are exactly the negative of each other. This is because Leela Chess Zero always scores a position from the perspective of the player to move.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 7: Examples of logical consistency constraints</p>
<h2>B. 2 Leela Chess Zero Experimental Setup</h2>
<p>Reproducibility. All parameters we use can be found in Table 5. In order to ensure reproducibility, we use a completely deterministic setup. This has an impact on inference speed as we disable several caching- and parallelization options but does not impact the model's strength. A small amount of stochasticity remains due to GPU inference. However, this impact is negligible and doesn't impact our results in any meaningful way. All chess positions we analyze in our experiments, together with the respective scores, can be found in the supplementary material.</p>
<p>Chess position selection. For forced moves, recommended moves, and position mirroring, we use 400k middle-game positions from master-level games, taken from Caissabase[16]. Middle-game positions are the most interesting positions to analyze, as the opening- and end-game have already been heavily studied and partially solved [1, 34]. However, there is no single widely agreed-upon</p>
<p>Table 5: All non-default settings used to configure Leela Chess Zero for our experiments. The remaining default settings can be found in the official GitHub repository [29] (using the branch and commit listed in the table).</p>
<table>
<thead>
<tr>
<th>Option</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Git-Branch</td>
<td>release/0.29</td>
</tr>
<tr>
<td>Commit id</td>
<td>ece6f22e</td>
</tr>
<tr>
<td>Backend</td>
<td>cuda-fp16</td>
</tr>
<tr>
<td>WeightsFile</td>
<td>Id: T807785</td>
</tr>
<tr>
<td>VerboseMoveStats</td>
<td>true</td>
</tr>
<tr>
<td>SmartPruningFactor</td>
<td>0</td>
</tr>
<tr>
<td>Threads</td>
<td>1</td>
</tr>
<tr>
<td>OutOfOrderEval</td>
<td>false</td>
</tr>
<tr>
<td>TaskWorkers</td>
<td>0</td>
</tr>
<tr>
<td>MinibatchSize</td>
<td>1</td>
</tr>
<tr>
<td>MaxPrefetch</td>
<td>0</td>
</tr>
<tr>
<td>NNCacheSize</td>
<td>200000</td>
</tr>
</tbody>
</table>
<p>definition of the chess middle game. In order to extract such positions automatically, we combine elements of multiple definitions and pick chess positions that a) occur after move 15; b) contain at least 10 pieces; c) contain more than 5 non-pawn and non-king pieces; and d) contain either at least one queen or more than 6 non-pawn and non-king pieces.</p>
<p>The board transformation inconsistency requires positions without any pawns and without castling rights. Since these are rather rare in master-level games, we randomly generate synthetic positions complying with these requirements. Each of these positions contains 8 pieces where both colors get the same set of four non-pawn pieces.</p>
<p>Chess position evaluation. Leela Chess Zero employs Monte Carlo Tree Search (MCTS) to evaluate a position, similar to the method used for the original AlphaZero [67]. Given any chess position $s$, a search will return for each possible move $a$ the following evaluations:</p>
<ul>
<li>An estimate $q$ of the expected game outcome $z$ when we play move $a$ in position $s$. We have $z \in{-1,0,1}$ (where $1=$ Win, $0=$ Draw, $-1=$ Loss for the current player) and $q \approx \mathbb{E}[z \mid s, a] \in[-1,1]$.</li>
<li>An estimate $d$ of the probability that playing $a$ in position $s$ ends in a draw.</li>
</ul>
<p>The evaluation of the position $s$ is then defined to be the evaluation of the best move $a$ which can be played in this position. In our experiments, we evaluate the difference in evaluation (i.e. the absolute difference between the two $q$ values).
Using expected game outcomes as board evaluations can be difficult to interpret. Therefore, for our plots of example chess positions, we use estimates of winning the current position (which is much more interpretable). Leela computes the winning probabilities directly from its output by making use of the following two simple properties:</p>
<p>$$
\begin{gathered}
\mathbb{E}[z \mid s, a]=p(z=1 \mid s, a)-p(z=-1 \mid s, a) \
p(z=1 \mid s, a)+p(z=0 \mid s, a)+p(z=-1 \mid s, a)=1
\end{gathered}
$$</p>
<p>Combining these two properties allows to compute the winning probability using just the q-value $q$ and the draw probability $d$ :</p>
<p>$$
p(z=1 \mid s, a)=\frac{1}{2}(\mathbb{E}[z \mid s, a]+1-p(z=0 \mid s, a)) \approx \frac{1}{2} \cdot(q+1-d)
$$</p>
<p>Adversarial search process. In Table 2 we use an adversarial search method to find consistency violations more efficiently. We implement this adversarial search by using an evolutionary algorithm [68]. Evolutionary algorithms are useful for our application because they only require black-box model access.</p>
<p>The goal of our optimization method is to find boards (also denoted by individuals) that violate the board transformation consistency constraint. More specifically, we limit ourselves in this experiment to finding boards that violate the $180^{\circ}$-rotation consistency constraint. Each individual is assigned a fitness value, defined as the difference in evaluation between a board and its $180^{\circ}$ rotated variant. We optimize a population of 1000 randomly initialized board positions over 20 generations (or until we hit an early-stopping criterion) after which we restart the search with a new, randomly initialized population of boards. We continue this process until we analyzed 50k positions in total, in order to be comparable to the brute-force search method used in Table 2 which analyzes the same number of boards.</p>
<p>In each generation, we first select the best-performing individuals, using tournament selection with $10 \%$ of the population. We then randomly create pairs of individuals and perform crossover by exchanging some pieces between the two boards. In the last step, we mutate each individual board by slightly changing the position in a random fashion.</p>
<p>During the mutation step, each board is mutated according to a randomly selected mutation rule from the following list:</p>
<ul>
<li>Flip the board along any of its given axes or diagonals.</li>
<li>Move one piece to a random empty square.</li>
<li>Move one piece to a randomly selected adjacent empty square.</li>
<li>Perform one legal move on the board (but don't capture any pieces).</li>
<li>Change the player to move.</li>
<li>Rotate the board by either $90^{\circ}, 180^{\circ}$ or $270^{\circ}$.</li>
<li>Substitute one piece by another piece for both players. This is possible due to the symmetric nature of our positions, which ensures that both players have the same set of pieces.</li>
</ul>
<p>For the crossover, we use an operator which swaps a pair of pieces of the same type and opposite color between the two boards. For example, if on Board 1 both players have a knight and on Board 2 both players have a bishop, our crossover function could exchange the two knights on Board 1 with the two bishops on Board 2.</p>
<h1>B. 3 Additional Leela Chess Zero Results</h1>
<p>Table 6: Comparison of the number of failures our method finds in increasingly stronger models, for recommended moves. The model strength is increased by using more MCTS search nodes.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Difference in Evaluation for Recommended Moves</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Search nodes</td>
<td style="text-align: center;">$&gt;0.05$</td>
<td style="text-align: center;">$&gt;0.1$</td>
<td style="text-align: center;">$&gt;0.25$</td>
<td style="text-align: center;">$&gt;0.5$</td>
<td style="text-align: center;">$&gt;0.75$</td>
<td style="text-align: center;">$&gt;1.0$</td>
</tr>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$53.9 \%$</td>
<td style="text-align: center;">$32.9 \%$</td>
<td style="text-align: center;">$11.2 \%$</td>
<td style="text-align: center;">$3.2 \%$</td>
<td style="text-align: center;">$1.2 \%$</td>
<td style="text-align: center;">$0.5 \%$</td>
</tr>
<tr>
<td style="text-align: center;">100</td>
<td style="text-align: center;">$31.5 \%$</td>
<td style="text-align: center;">$7.7 \%$</td>
<td style="text-align: center;">$0.5 \%$</td>
<td style="text-align: center;">$0.07 \%$</td>
<td style="text-align: center;">$0.03 \%$</td>
<td style="text-align: center;">$0.01 \%$</td>
</tr>
<tr>
<td style="text-align: center;">200</td>
<td style="text-align: center;">$26.8 \%$</td>
<td style="text-align: center;">$4.7 \%$</td>
<td style="text-align: center;">$0.3 \%$</td>
<td style="text-align: center;">$0.04 \%$</td>
<td style="text-align: center;">$0.02 \%$</td>
<td style="text-align: center;">$&lt;0.01 \%$</td>
</tr>
<tr>
<td style="text-align: center;">400</td>
<td style="text-align: center;">$19.5 \%$</td>
<td style="text-align: center;">$2.6 \%$</td>
<td style="text-align: center;">$0.2 \%$</td>
<td style="text-align: center;">$0.03 \%$</td>
<td style="text-align: center;">$0.01 \%$</td>
<td style="text-align: center;">$&lt;0.01 \%$</td>
</tr>
<tr>
<td style="text-align: center;">800</td>
<td style="text-align: center;">$12.8 \%$</td>
<td style="text-align: center;">$1.5 \%$</td>
<td style="text-align: center;">$0.1 \%$</td>
<td style="text-align: center;">$0.02 \%$</td>
<td style="text-align: center;">$&lt;0.01 \%$</td>
<td style="text-align: center;">$&lt;0.01 \%$</td>
</tr>
<tr>
<td style="text-align: center;">1600</td>
<td style="text-align: center;">$10.5 \%$</td>
<td style="text-align: center;">$1.0 \%$</td>
<td style="text-align: center;">$0.06 \%$</td>
<td style="text-align: center;">$&lt;0.01 \%$</td>
<td style="text-align: center;">$0 \%$</td>
<td style="text-align: center;">$0 \%$</td>
</tr>
<tr>
<td style="text-align: center;">3200</td>
<td style="text-align: center;">$6.5 \%$</td>
<td style="text-align: center;">$0.5 \%$</td>
<td style="text-align: center;">$0.03 \%$</td>
<td style="text-align: center;">$&lt;0.01 \%$</td>
<td style="text-align: center;">$0 \%$</td>
<td style="text-align: center;">$0 \%$</td>
</tr>
</tbody>
</table>
<p>Table 6 and Figure 8 depict a comparison of the number of Recommended move inconsistencies our method finds in increasingly superhuman Leela models, on human games. We find that consistency scales with model strength. Yet, even when we increase the search nodes by $8 \times$, to 3,200 nodes, the number of failures only drops by $3-6.6 \times$. Figure 9 contains histograms of our main results (see Table 1). We show a selection of failure examples from these experiments in Figure 10.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*Equal contribution.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
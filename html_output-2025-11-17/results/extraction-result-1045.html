<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1045 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1045</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1045</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-24.html">extraction-schema-24</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <p><strong>Paper ID:</strong> paper-274437388</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2412.00744v2.pdf" target="_blank">Open-World Drone Active Tracking with Goal-Centered Rewards</a></p>
                <p><strong>Paper Abstract:</strong> Drone Visual Active Tracking aims to autonomously follow a target object by controlling the motion system based on visual observations, providing a more practical solution for effective tracking in dynamic environments. However, accurate Drone Visual Active Tracking using reinforcement learning remains challenging due to the absence of a unified benchmark and the complexity of open-world environments with frequent interference. To address these issues, we pioneer a systematic solution. First, we propose DAT, the first open-world drone active air-to-ground tracking benchmark. It encompasses 24 city-scale scenes, featuring targets with human-like behaviors and high-fidelity dynamics simulation. DAT also provides a digital twin tool for unlimited scene generation. Additionally, we propose a novel reinforcement learning method called GC-VAT, which aims to improve the performance of drone tracking targets in complex scenarios. Specifically, we design a Goal-Centered Reward to provide precise feedback across viewpoints to the agent, enabling it to expand perception and movement range through unrestricted perspectives. Inspired by curriculum learning, we introduce a Curriculum-Based Training strategy that progressively enhances the tracking performance in complex environments. Besides, experiments on simulator and real-world images demonstrate the superior performance of GC-VAT, achieving a Tracking Success Rate of approximately 72% on the simulator. The benchmark and code are available at https://github.com/SHWplus/DAT_Benchmark.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1045.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1045.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GC-VAT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Goal-Centered Visual Active Tracking (GC-VAT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reinforcement-learning-based end-to-end drone visual active tracking agent that uses a novel goal-centered deviation reward and curriculum-based training to track ground targets in open-world, city-scale scenes with high-fidelity drone dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GC-VAT (Goal-Centered-VAT)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An end-to-end RL drone agent trained with Proximal Policy Optimization (PPO). Uses a CNN+GRU policy backbone, a novel goal-centered deviation reward (deviation ϕ metric and r_gc reward) to align rewards with image-projection geometry, and training augmentations including domain randomization (initial angle, horizontal/vertical distance, gimbal pitch).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated robotic agent (drone); also evaluated in zero-shot real-drone and real-image settings</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>DAT (Drone Active Tracking) benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>DAT is an open-world air-to-ground active tracking benchmark with 24 city-scale scenes (six scenario types × 4 weather conditions), high-fidelity Webots-based drone dynamics (mass, inertia, aerodynamics, gimbal response/jitter), human-like target behaviors (5 target categories, 24 target instances) produced via SUMO + controllers, and a digital-twin tool for unlimited scene generation. Scene complexity is characterized along seven axes (scene area, building density, road density, terrain density, tree density, tunnel density, color richness). Four weather domains (day/night/fog/snow) add domain variation; domain randomization is applied during training for robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Seven scene complexity aspects reported: scene area (km^2), building density, road density, terrain density, tree density, tunnel density, color richness; example numeric values: citystreet area 0.7 km^2, road density 38.7, tree density 97.5; village area 1.4 km^2, mountain density 20.1, tunnel density 6.3; downtown building density 304.9; desert mountain density 37.1, road density 31.0. Also #dynamic agents per map (≈40 vehicles).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>Varies by map: low-to-medium for desert/village/lake (simpler visual statistics), high for citystreet/downtown/farmland (high building/element density and occlusions); overall includes high-complexity scenes.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Number of scene instances = 24 (6 scene types × 4 weather settings); digital-twin supports unlimited procedural scene generation; domain randomization parameters include initial orientation range [−π,π], flight altitude sampled [13,22] m, camera pitch angle [0.6,1.38] rad, and explicit randomization factors AR (initial angle), HR (horizontal distance), VR (vertical distance), PR (gimbal pitch). Weather domains (day, night, fog, snow) measure cross-domain variation.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>High — 24 distinct scene-weather combinations plus unlimited scene generation and explicit domain randomization; cross-domain shifts (fog, night, snow) present.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Cumulative reward (CR) and Tracking Success Rate (TSR); also Correct Action Rate (CAR) for real-image/video action prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Simulator: within-scene average TSR ≈ 0.72 (72%), average CR improvements reported (e.g., CR increases relative to baseline); cross-scene average TSR ≈ 0.57 (57%); cross-domain average TSR ≈ 0.67 (67%). Real-world: zero-shot on DJI Mini 3 Pro reported TSR = 88.4% and CAR = 81.3%; zero-shot on held videos CAR ≈ 81.0% (24 videos).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Yes — the paper explicitly analyzes and experimentally demonstrates relationships and trade-offs: (1) higher visual/structural complexity (dense buildings, many road elements, occlusions) makes learning and convergence harder and can cause single-stage RL to fail; (2) high variation (many weather domains, randomized initial poses) increases generalization demands but, when combined with domain randomization, improves robustness; (3) simpler training environments speed up initial learning but risk convergence to suboptimal policies unless domain randomization is used; (4) curriculum learning (simple→complex) mitigates the negative effect of high complexity and speeds/ stabilizes convergence.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td>Tested (reported failure of single-stage RL): single-stage RL (no CBT) failed to converge on high-complexity maps (citystreet, downtown, farmland), resulting in very low TSR (reported baseline-like/failed TSR values on difficult maps, e.g., ≈0.06 for failed policies on some high-complexity tests).</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td>With GC-VAT + CBT + domain randomization the agent attains strong performance on complex/high-variation testbeds: within-scene average TSR ≈ 0.72 and cross-domain TSR ≈ 0.67, indicating successful handling of both high complexity and high variation when trained with CBT and randomization.</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td>Single-stage RL (no CBT) successfully converged on low-complexity maps (desert, village, lake); exact per-map TSR numbers for single-stage RL are not given as single scalar averages in the paper (convergence reported qualitatively), but succeeded without the CBT strategy.</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Two-stage Curriculum-Based Training (CBT): stage 1 simplified environment (straight-line trajectories, no obstacles) to learn centering behavior; stage 2 complex environments (varied trajectories, occlusions). PPO optimization with domain randomization (AR, HR, VR, PR) and entropy regularization.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Cross-scene generalization: GC-VAT improves average CR by +376% (37 → 176) and average TSR by +200% (0.19 → 0.57) relative to D-VAT. Cross-domain generalization: GC-VAT achieves average CR improvement +509% (35 → 213) and TSR boost +253% (0.19 → 0.67) vs D-VAT. Zero-shot sim-to-real: evaluated on recorded videos (VOT, DTB70, UAVDT) with CAR ≈ 81% and deployed on a real DJI Mini 3 Pro with zero-shot TSR = 88.4% and CAR = 81.3%. Robustness tests: under wind gusts and rain blur, TSR drop < 0.06–0.07 reported.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Training steps per map reported (total over 35 parallel agents): citystreet 19.2M, desert 13.4M, village 21.3M, downtown 19.8M, lake 9.9M, farmland 9.2M total environment interaction steps; training used 35 parallel environments (cumulative steps reported). Episode max length 1500 steps; Webots runs at 500Hz, algorithm updates every 4 steps (125Hz).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>1) A goal-centered deviation reward that accounts for image-to-ground affine projection (ϕ and r_gc) yields correct gradients and substantially better learning in unrestricted viewpoints compared to Euclidean distance rewards; 2) Curriculum-Based Training (simple→complex) is essential: single-stage RL fails to converge on visually complex scenes (citystreet, downtown, farmland) while CBT enables learning and generalization; 3) Domain randomization (especially initial-angle randomization) significantly improves exploration and generalization; 4) GC-VAT achieves markedly higher TSR/CR than distance-reward baselines (D-VAT/AOT), with within-scene TSR ≈72% and strong sim-to-real zero-shot transfer (real drone TSR 88.4%); 5) There is a trade-off: training in only simple environments speeds early learning but risks suboptimal policies unless combined with randomization and curriculum progression to complex conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Open-World Drone Active Tracking with Goal-Centered Rewards', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1045.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1045.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>D-VAT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>D-VAT (Distance-based Visual Active Tracking / D-VAT baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior reinforcement-learning visual active tracking method that uses Euclidean distance-based rewards and fixed forward camera assumptions; reproduced and used as a baseline in experiments showing failure under tilted/top-down views.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>D-vat: End-to-end visual active tracking for micro aerial vehicles.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>D-VAT baseline</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An RL-based VAT agent (originally using soft actor-critic and distance-based reward formulations) that assumes a fixed-forward camera viewpoint and uses Euclidean distance between target and image center in reward computation; reproduced here in discrete action settings for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated robotic agent (drone) used as baseline</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>DAT (Drone Active Tracking) benchmark (used as testbed)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same DAT benchmark as above (24 city-scale scenes, four weather domains, high-fidelity drone dynamics). D-VAT was evaluated across within-scene, cross-scene, and cross-domain tests within DAT.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Same DAT seven-aspect scene metrics (area, building density, road density, terrain/tree/tunnel densities, color richness) and weather domains; D-VAT performance particularly sensitive to viewpoint tilt/affine effects.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>Varies; D-VAT fails on high-complexity scenes (citystreet/downtown/farmland) under unrestricted viewpoints.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Evaluated across 24 scene-weather combinations and domain shifts (fog, night, snow); no curriculum or the same domain randomization regime as GC-VAT unless explicitly reproduced.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>High (tested across cross-scene and cross-domain splits), but D-VAT lacks robust reward geometry for varying viewpoints.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Cumulative reward (CR) and Tracking Success Rate (TSR)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported baseline average TSR ≈ 0.19 (19%) (this baseline value is used as the reference for percentage improvement statements in the paper). On many tilted top-down or complex scenes D-VAT produced very low rewards or failed behaviors (examples of per-map low TSR values ~0.06 reported for failed cases).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Yes — the paper shows D-VAT's distance-based reward misaligns with the image-ground projection under non-forward camera views, causing reward signals to be misleading in environments with viewpoint variation and higher scene complexity; D-VAT fails to generalize across varying viewpoints and complex scenes.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td>Poor — D-VAT and other distance-based reward methods fail on high-complexity maps under tilted/unrestricted viewpoints, producing low TSR (examples in experiments show TSR ≈ 0.06 in many failed cases).</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td>Poor — fails to learn effective policies in complex/high-variation settings due to reward misalignment; leads to agents driving the target out of view or failing to center it.</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td>Acceptable in some simpler maps — prior single-stage RL approaches (including distance-based reward methods) can converge on maps with uniform backgrounds or fewer occlusions (desert, village, lake) but perform poorly on more challenging scenes; quantitative per-map numbers not provided beyond qualitative convergence statements.</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Original D-VAT uses distance-based rewards and soft actor-critic in prior work; in this paper it was reproduced (converted to discrete actions) for direct comparison without GC-VAT reward or CBT.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>D-VAT shows poor cross-scene and cross-domain generalization on DAT due to reward misalignment with projection geometry: average TSR baseline ~0.19 and low CR; GC-VAT outperforms D-VAT by very large margins in CR and TSR across within-scene, cross-scene, and cross-domain evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Not explicitly reported for D-VAT in this paper beyond reproduction experiments; D-VAT reproductions failed to learn in many high-complexity conditions within the same training budgets reported for GC-VAT.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Distance-based (Euclidean) rewards commonly used in prior VAT methods (like D-VAT) do not correctly reflect target proximity under varying camera viewpoints (affine projection), causing misleading rewards and failed learning in top-down/tilted air-to-ground tasks; replacing them with deviation-based, projection-aware rewards and using curriculum/domain randomization substantially improves learning and generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Open-World Drone Active Tracking with Goal-Centered Rewards', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>D-vat: End-to-end visual active tracking for micro aerial vehicles. <em>(Rating: 2)</em></li>
                <li>End-to-end active object tracking and its real-world deployment via reinforcement learning <em>(Rating: 2)</em></li>
                <li>A survey on curriculum learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1045",
    "paper_id": "paper-274437388",
    "extraction_schema_id": "extraction-schema-24",
    "extracted_data": [
        {
            "name_short": "GC-VAT",
            "name_full": "Goal-Centered Visual Active Tracking (GC-VAT)",
            "brief_description": "A reinforcement-learning-based end-to-end drone visual active tracking agent that uses a novel goal-centered deviation reward and curriculum-based training to track ground targets in open-world, city-scale scenes with high-fidelity drone dynamics.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "GC-VAT (Goal-Centered-VAT)",
            "agent_description": "An end-to-end RL drone agent trained with Proximal Policy Optimization (PPO). Uses a CNN+GRU policy backbone, a novel goal-centered deviation reward (deviation ϕ metric and r_gc reward) to align rewards with image-projection geometry, and training augmentations including domain randomization (initial angle, horizontal/vertical distance, gimbal pitch).",
            "agent_type": "simulated robotic agent (drone); also evaluated in zero-shot real-drone and real-image settings",
            "environment_name": "DAT (Drone Active Tracking) benchmark",
            "environment_description": "DAT is an open-world air-to-ground active tracking benchmark with 24 city-scale scenes (six scenario types × 4 weather conditions), high-fidelity Webots-based drone dynamics (mass, inertia, aerodynamics, gimbal response/jitter), human-like target behaviors (5 target categories, 24 target instances) produced via SUMO + controllers, and a digital-twin tool for unlimited scene generation. Scene complexity is characterized along seven axes (scene area, building density, road density, terrain density, tree density, tunnel density, color richness). Four weather domains (day/night/fog/snow) add domain variation; domain randomization is applied during training for robustness.",
            "complexity_measure": "Seven scene complexity aspects reported: scene area (km^2), building density, road density, terrain density, tree density, tunnel density, color richness; example numeric values: citystreet area 0.7 km^2, road density 38.7, tree density 97.5; village area 1.4 km^2, mountain density 20.1, tunnel density 6.3; downtown building density 304.9; desert mountain density 37.1, road density 31.0. Also #dynamic agents per map (≈40 vehicles).",
            "complexity_level": "Varies by map: low-to-medium for desert/village/lake (simpler visual statistics), high for citystreet/downtown/farmland (high building/element density and occlusions); overall includes high-complexity scenes.",
            "variation_measure": "Number of scene instances = 24 (6 scene types × 4 weather settings); digital-twin supports unlimited procedural scene generation; domain randomization parameters include initial orientation range [−π,π], flight altitude sampled [13,22] m, camera pitch angle [0.6,1.38] rad, and explicit randomization factors AR (initial angle), HR (horizontal distance), VR (vertical distance), PR (gimbal pitch). Weather domains (day, night, fog, snow) measure cross-domain variation.",
            "variation_level": "High — 24 distinct scene-weather combinations plus unlimited scene generation and explicit domain randomization; cross-domain shifts (fog, night, snow) present.",
            "performance_metric": "Cumulative reward (CR) and Tracking Success Rate (TSR); also Correct Action Rate (CAR) for real-image/video action prediction.",
            "performance_value": "Simulator: within-scene average TSR ≈ 0.72 (72%), average CR improvements reported (e.g., CR increases relative to baseline); cross-scene average TSR ≈ 0.57 (57%); cross-domain average TSR ≈ 0.67 (67%). Real-world: zero-shot on DJI Mini 3 Pro reported TSR = 88.4% and CAR = 81.3%; zero-shot on held videos CAR ≈ 81.0% (24 videos).",
            "complexity_variation_relationship": "Yes — the paper explicitly analyzes and experimentally demonstrates relationships and trade-offs: (1) higher visual/structural complexity (dense buildings, many road elements, occlusions) makes learning and convergence harder and can cause single-stage RL to fail; (2) high variation (many weather domains, randomized initial poses) increases generalization demands but, when combined with domain randomization, improves robustness; (3) simpler training environments speed up initial learning but risk convergence to suboptimal policies unless domain randomization is used; (4) curriculum learning (simple→complex) mitigates the negative effect of high complexity and speeds/ stabilizes convergence.",
            "high_complexity_low_variation_performance": "Tested (reported failure of single-stage RL): single-stage RL (no CBT) failed to converge on high-complexity maps (citystreet, downtown, farmland), resulting in very low TSR (reported baseline-like/failed TSR values on difficult maps, e.g., ≈0.06 for failed policies on some high-complexity tests).",
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": "With GC-VAT + CBT + domain randomization the agent attains strong performance on complex/high-variation testbeds: within-scene average TSR ≈ 0.72 and cross-domain TSR ≈ 0.67, indicating successful handling of both high complexity and high variation when trained with CBT and randomization.",
            "low_complexity_low_variation_performance": "Single-stage RL (no CBT) successfully converged on low-complexity maps (desert, village, lake); exact per-map TSR numbers for single-stage RL are not given as single scalar averages in the paper (convergence reported qualitatively), but succeeded without the CBT strategy.",
            "training_strategy": "Two-stage Curriculum-Based Training (CBT): stage 1 simplified environment (straight-line trajectories, no obstacles) to learn centering behavior; stage 2 complex environments (varied trajectories, occlusions). PPO optimization with domain randomization (AR, HR, VR, PR) and entropy regularization.",
            "generalization_tested": true,
            "generalization_results": "Cross-scene generalization: GC-VAT improves average CR by +376% (37 → 176) and average TSR by +200% (0.19 → 0.57) relative to D-VAT. Cross-domain generalization: GC-VAT achieves average CR improvement +509% (35 → 213) and TSR boost +253% (0.19 → 0.67) vs D-VAT. Zero-shot sim-to-real: evaluated on recorded videos (VOT, DTB70, UAVDT) with CAR ≈ 81% and deployed on a real DJI Mini 3 Pro with zero-shot TSR = 88.4% and CAR = 81.3%. Robustness tests: under wind gusts and rain blur, TSR drop &lt; 0.06–0.07 reported.",
            "sample_efficiency": "Training steps per map reported (total over 35 parallel agents): citystreet 19.2M, desert 13.4M, village 21.3M, downtown 19.8M, lake 9.9M, farmland 9.2M total environment interaction steps; training used 35 parallel environments (cumulative steps reported). Episode max length 1500 steps; Webots runs at 500Hz, algorithm updates every 4 steps (125Hz).",
            "key_findings": "1) A goal-centered deviation reward that accounts for image-to-ground affine projection (ϕ and r_gc) yields correct gradients and substantially better learning in unrestricted viewpoints compared to Euclidean distance rewards; 2) Curriculum-Based Training (simple→complex) is essential: single-stage RL fails to converge on visually complex scenes (citystreet, downtown, farmland) while CBT enables learning and generalization; 3) Domain randomization (especially initial-angle randomization) significantly improves exploration and generalization; 4) GC-VAT achieves markedly higher TSR/CR than distance-reward baselines (D-VAT/AOT), with within-scene TSR ≈72% and strong sim-to-real zero-shot transfer (real drone TSR 88.4%); 5) There is a trade-off: training in only simple environments speeds early learning but risks suboptimal policies unless combined with randomization and curriculum progression to complex conditions.",
            "uuid": "e1045.0",
            "source_info": {
                "paper_title": "Open-World Drone Active Tracking with Goal-Centered Rewards",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "D-VAT",
            "name_full": "D-VAT (Distance-based Visual Active Tracking / D-VAT baseline)",
            "brief_description": "A prior reinforcement-learning visual active tracking method that uses Euclidean distance-based rewards and fixed forward camera assumptions; reproduced and used as a baseline in experiments showing failure under tilted/top-down views.",
            "citation_title": "D-vat: End-to-end visual active tracking for micro aerial vehicles.",
            "mention_or_use": "use",
            "agent_name": "D-VAT baseline",
            "agent_description": "An RL-based VAT agent (originally using soft actor-critic and distance-based reward formulations) that assumes a fixed-forward camera viewpoint and uses Euclidean distance between target and image center in reward computation; reproduced here in discrete action settings for comparison.",
            "agent_type": "simulated robotic agent (drone) used as baseline",
            "environment_name": "DAT (Drone Active Tracking) benchmark (used as testbed)",
            "environment_description": "Same DAT benchmark as above (24 city-scale scenes, four weather domains, high-fidelity drone dynamics). D-VAT was evaluated across within-scene, cross-scene, and cross-domain tests within DAT.",
            "complexity_measure": "Same DAT seven-aspect scene metrics (area, building density, road density, terrain/tree/tunnel densities, color richness) and weather domains; D-VAT performance particularly sensitive to viewpoint tilt/affine effects.",
            "complexity_level": "Varies; D-VAT fails on high-complexity scenes (citystreet/downtown/farmland) under unrestricted viewpoints.",
            "variation_measure": "Evaluated across 24 scene-weather combinations and domain shifts (fog, night, snow); no curriculum or the same domain randomization regime as GC-VAT unless explicitly reproduced.",
            "variation_level": "High (tested across cross-scene and cross-domain splits), but D-VAT lacks robust reward geometry for varying viewpoints.",
            "performance_metric": "Cumulative reward (CR) and Tracking Success Rate (TSR)",
            "performance_value": "Reported baseline average TSR ≈ 0.19 (19%) (this baseline value is used as the reference for percentage improvement statements in the paper). On many tilted top-down or complex scenes D-VAT produced very low rewards or failed behaviors (examples of per-map low TSR values ~0.06 reported for failed cases).",
            "complexity_variation_relationship": "Yes — the paper shows D-VAT's distance-based reward misaligns with the image-ground projection under non-forward camera views, causing reward signals to be misleading in environments with viewpoint variation and higher scene complexity; D-VAT fails to generalize across varying viewpoints and complex scenes.",
            "high_complexity_low_variation_performance": "Poor — D-VAT and other distance-based reward methods fail on high-complexity maps under tilted/unrestricted viewpoints, producing low TSR (examples in experiments show TSR ≈ 0.06 in many failed cases).",
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": "Poor — fails to learn effective policies in complex/high-variation settings due to reward misalignment; leads to agents driving the target out of view or failing to center it.",
            "low_complexity_low_variation_performance": "Acceptable in some simpler maps — prior single-stage RL approaches (including distance-based reward methods) can converge on maps with uniform backgrounds or fewer occlusions (desert, village, lake) but perform poorly on more challenging scenes; quantitative per-map numbers not provided beyond qualitative convergence statements.",
            "training_strategy": "Original D-VAT uses distance-based rewards and soft actor-critic in prior work; in this paper it was reproduced (converted to discrete actions) for direct comparison without GC-VAT reward or CBT.",
            "generalization_tested": true,
            "generalization_results": "D-VAT shows poor cross-scene and cross-domain generalization on DAT due to reward misalignment with projection geometry: average TSR baseline ~0.19 and low CR; GC-VAT outperforms D-VAT by very large margins in CR and TSR across within-scene, cross-scene, and cross-domain evaluations.",
            "sample_efficiency": "Not explicitly reported for D-VAT in this paper beyond reproduction experiments; D-VAT reproductions failed to learn in many high-complexity conditions within the same training budgets reported for GC-VAT.",
            "key_findings": "Distance-based (Euclidean) rewards commonly used in prior VAT methods (like D-VAT) do not correctly reflect target proximity under varying camera viewpoints (affine projection), causing misleading rewards and failed learning in top-down/tilted air-to-ground tasks; replacing them with deviation-based, projection-aware rewards and using curriculum/domain randomization substantially improves learning and generalization.",
            "uuid": "e1045.1",
            "source_info": {
                "paper_title": "Open-World Drone Active Tracking with Goal-Centered Rewards",
                "publication_date_yy_mm": "2024-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "D-vat: End-to-end visual active tracking for micro aerial vehicles.",
            "rating": 2,
            "sanitized_title": "dvat_endtoend_visual_active_tracking_for_micro_aerial_vehicles"
        },
        {
            "paper_title": "End-to-end active object tracking and its real-world deployment via reinforcement learning",
            "rating": 2,
            "sanitized_title": "endtoend_active_object_tracking_and_its_realworld_deployment_via_reinforcement_learning"
        },
        {
            "paper_title": "A survey on curriculum learning",
            "rating": 1,
            "sanitized_title": "a_survey_on_curriculum_learning"
        }
    ],
    "cost": 0.01906375,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Open-World Drone Active Tracking with Goal-Centered Rewards
22 Oct 2025</p>
<p>Haowei Sun 
South China University of Technology</p>
<p>Institute for Super Robotics (Huangpu)</p>
<p>Jinwu Hu fhujinwu@gmail.com 
South China University of Technology</p>
<p>Pazhou Laboratory</p>
<p>Zhirui Zhang 
South China University of Technology</p>
<p>Institute for Super Robotics (Huangpu)</p>
<p>Haoyuan Tian 
South China University of Technology</p>
<p>Institute for Super Robotics (Huangpu)</p>
<p>Xinze Xie 
South China University of Technology</p>
<p>Institute for Super Robotics (Huangpu)</p>
<p>Yufeng Wang 
South China University of Technology</p>
<p>Peng Cheng Laboratory</p>
<p>Xiaohua Xie 
Sun Yat-sen University</p>
<p>Yun Lin 
Harbin Engineering University</p>
<p>Zhuliang Yu zlyu@scut.edu.cn 
South China University of Technology</p>
<p>Institute for Super Robotics (Huangpu)</p>
<p>Mingkui Tan mingkuitan@scut.edu.cn 
South China University of Technology</p>
<p>Ministry of Education
Key Laboratory of Big Data and Intelligent Robot</p>
<p>Open-World Drone Active Tracking with Goal-Centered Rewards
22 Oct 20251DBD9258A59D8765E5BE3D7A6144FD1DarXiv:2412.00744v2[cs.RO]
Drone Visual Active Tracking aims to autonomously follow a target object by controlling the motion system based on visual observations, providing a more practical solution for effective tracking in dynamic environments.However, accurate Drone Visual Active Tracking using reinforcement learning remains challenging due to the absence of a unified benchmark and the complexity of open-world environments with frequent interference.To address these issues, we pioneer a systematic solution.First, we propose DAT, the first open-world drone active air-to-ground tracking benchmark.It encompasses 24 city-scale scenes, featuring targets with human-like behaviors and high-fidelity dynamics simulation.DAT also provides a digital twin tool for unlimited scene generation.Additionally, we propose a novel reinforcement learning method called GC-VAT, which aims to improve the performance of drone tracking targets in complex scenarios.Specifically, we design a Goal-Centered Reward to provide precise feedback across viewpoints to the agent, enabling it to expand perception and movement range through unrestricted perspectives.Inspired by curriculum learning, we introduce a Curriculum-Based Training strategy that progressively enhances the tracking performance in complex environments.Besides, experiments on simulator and real-world images demonstrate the superior performance of GC-VAT, achieving a Tracking Success Rate of approximately 72% on the simulator.The benchmark and code are available at https://github.com/SHWplus/DAT_Benchmark.</p>
<p>Introduction</p>
<p>Visual Active Tracking (VAT) aims to autonomously follow a target object by controlling the motion system of the tracker based on visual observations [80,75].It is widely used in real-world applications such as drone target tracking and security surveillance [22,73,77,54].Unlike passive visual tracking [3,74,33,5,9,84,67,58], which involves proposing a 2D bounding box for the target on a frameby-frame with a fixed camera pose, VAT actively adjusts the camera position to maintain the target within the field of view.Passive visual tracking often falls short in real-world scenarios due to the highly dynamic nature of most targets.Thus, VAT offers a more practical yet challenging solution for effective tracking in dynamic environments.Recently, VAT methods have evolved into two main categories: pipeline VAT methods [40,46,15] and reinforcement learning-based VAT methods [19,39,18,80].Pipeline VAT methods employ a sequential framework where the visual tracking [32,4,62,33] and control models are connected in series.The tracking model estimates the target position in the input image and the control model  generates control signals.While this modular design allows for clear task separation, it often requires significant manual effort to label the training data, and the control module requires additional tuning for different scenes.To address these issues, reinforcement learning-based VAT methods integrate visual tracking and control within a unified framework.These methods eliminate the need for separate tuning of the tracking and control modules by using a unified framework to map raw visual inputs directly to control actions.Therefore, the reinforcement learning-based VAT methods simplify system design and increase the efficiency of learning adaptive tracking behaviors in dynamic environments.</p>
<p>Unfortunately, achieving accurate drone VAT with reinforcement learning remains challenging, partly for the following reasons.1) Missing unified benchmark.Existing benchmark scenes are low in complexity, neglect tracker dynamics or rely on overly simplified models, making them inadequate to validate the agent performance (see Table 1).Previous methods [39,19,57] use rule-based target management, far from producing human-like target behaviors.Additionally, current 3D scenes are all manually constructed, leading to a heavy workload and limited scene number.2) Vast environments with complex interference.Open-world tracking involves large, dynamic environments with frequent interference.In previous methods [39,19], trackers can only capture images from a fixed horizontal viewpoint.However, the fixed forward viewpoint captures excessive sky, reducing target-related visual information, especially for air-to-ground tracking tasks.Besides, since VAT goal is to keep the target at the image center, such viewpoint restricts the tracker to the same height as the target, severely limiting the perception and movement range.Moreover, training directly in complex conditions leads to slow convergence or difficulty in building strong behaviors.</p>
<p>To address the above limitations, we first propose DAT, the first open-world active air-to-ground tracking benchmark that simulates real-world complexity (see Fig. 2(b)).Specifically, DAT provides 24 city-scale scenes, full-fidelity simulations of drone dynamics, and a lightweight tool that can be integrated into any 3D scene to enable human-like target behaviors.It also offers a digital twin tool that can generate unlimited 3D scenes from real-world environments, enabling unlimited scene expansion.Second, we propose a novel drone VAT with reinforcement learning method (called GC-VAT), aiming to improve adaptability in complex and diverse scenarios.Specifically, we design a Goal-Centered Reward to provide precise feedback across viewpoints, enabling the agent to expand perception range through unrestricted perspectives.Besides, we propose qualitative and theoretical methods to analyze the effectiveness of our reward.In addition, inspired by curriculum learning [65,41,83], we propose a Curriculum-Based Training strategy that progressively improves agent performance in complex environments.Our contributions are summarized as follows:</p>
<p>1) A comprehensive drone active tracking benchmark.We present DAT benchmark, featuring high-fidelity dynamics, 24 city-scale scenes, and tools for simulating human-like target behaviors and unlimited scenes generation, enabling rigorous algorithm validation.2) A novel drone active tracking method.We propose GC-VAT, which leverages a Goal-Centered Reward function and a Curriculum-Based Training strategy to enhance drone tracking performance in complex and dynamic environments.Besides, we propose qualitative and theoretical methods to analyze the effectiveness of our reward.3) Extensive experimental validation.Experiments on simulator and real-world images validate DAT usability and GC-VAT effectiveness, with GC-VAT achieving a Tracking Success Rate of approximately 72% on the simulator.Observation spaces.The target is initially positioned at the center of the field of view, and the observation space comprises data acquired from sensors (e.g., RGB images with 84 × 84 resolution).</p>
<p>Action spaces.</p>
<p>The action space can be either discrete or continuous.A discrete space defines a set of predefined drone maneuvers, whereas a continuous space allows direct control over the velocity.</p>
<p>Success criterion of DAT task.We define a success criterion when the model can keep the target object, which is initially located at the center of view, in the middle of the image for a long duration.</p>
<p>Challenges.Open-world drone active tracking is challenging due to limited data and high risks of trial-and-error in the real world, necessitating complex simulation environments.Additionally, the complexity and dynamics of open-world scenes further demand robust agent performance.</p>
<p>DAT Benchmark with Diverse Settings</p>
<p>We develop DAT, including 24 city-scale scenes built by an unlimited scene generation tool, highfidelity drone dynamics simulation, and a versatile pipeline for producing human-like target behaviors.</p>
<p>Diverse Scene Construction</p>
<p>Digital twin tool.Users can select any region from OpenStreetMap [1] to obtain countless scenes using our tool.Specifically, it generates a high-precision road network with traffic lights and rules, and it converts elevation and vegetation data into 3D assets placed in the scene.Moreover, all assets in the generated scene are editable, allowing for data augmentation.See Appendix B for details.</p>
<p>Scene construction.Based on the above tool, we construct 6 outdoor scenes under 4 weather conditions, modeling 7 real-world complexities.Specifically, the scene area, building density, and color richness depict the complexity of the visual background.Road density and terrain density affect the target behaviors.The tree density and tunnel density measure the level of visual occlusion.</p>
<p>As shown in Fig. 2(a), six scenarios exhibit unique and realistic complexity across the seven aspects:</p>
<p>• Citystreet scene covers an area of 0.7 square kilometers.It has a road density of 38.7 and a tree density of 97.5, mainly testing the agent's efficiency against tree occlusions.</p>
<p>• Village scene spans 1.4 square kilometers.This scene features a mountain density of 20.1 and a tunnel density of 6.3, requiring the agent to predict the target's movement when it is fully obscured.</p>
<p>• Downtown scene covers 0.8 square kilometers.It includes complex road elements and high building density of 304.9, challenging the agent's tracking accuracy and obstacle avoidance abilities.</p>
<p>• Lake scene encompasses 1.6 square kilometers.The density of road elements is 68.1, and the richness of background colors is 5, challenging the robustness across varying features and colors.</p>
<p>• Farmland scene covers an area of 0.7 square kilometers.The color richness is 5 and multiple color patches, challenging the agent's adaptability to multi-color environments.</p>
<p>• Desert scene covers 1.1 square kilometers.It includes a mountain density of 37.1 and a road density of 31.0.Some roads are covered by sand, testing the agent's adaptability to such conditions.</p>
<p>Four weather conditions are designed to test the agent's cross-domain adaptability.Foggy reduces visibility, night reduces brightness, and snow alters the color.The above 24 scenes (see Fig. 2(b)) can fully measure the agent tracking performance.See Appendix B for scene construction details.</p>
<p>Various Trackers and Targets Construction</p>
<p>Drone Active Tracking in the real world involves diverse targets depending on tasks.DAT provides diverse targets with human-like behaviors and enables high-fidelity tracker dynamics simulation.</p>
<p>Tracker.DAT benchmark supports two tracker types: drones and ground robots.The drone used is the DJI Matrice 100 [14], equipped with a 3-axis gimbal, allowing for precise camera adjustments.Unlike simpler kinematic models in [19] and methods that ignore the dynamics [39], DAT leverages webots [37] to simulate the drone's full dynamics, including mass, inertia, aerodynamics, and the response and jitter of the gimbal, closely matching real drones.See Appendix B for details.</p>
<p>Targets.DAT includes five categories of targets: automobile, motorbike, pedestrian, wheel robot, and legged robot, with a total of 24 tracking targets (see Fig. 2(d)).See Appendix B for details.</p>
<p>Target Management.We propose a novel pipeline to simulate realistic target behavior.Specifically, DAT first utilizes road networks generated by the tools described in Section 3.1, and directly integrates them with the SUMO traffic simulator [36].Then, random trajectories are assigned to each vehicle, with SUMO managing its motion.To bridge the gap between simulation and visualization, we implement a controller that translates motion data into human-like driving behaviors for 3D vehicles (see Fig. 2(c)).Our controller also adheres to traffic rules and can simulate phenomena such as traffic light waits and traffic jams.Even better, the controller can be applied to any 3D scene.</p>
<p>VAT with Reinforcement Learning</p>
<p>In this paper, we primarily focus on visual active tracking (VAT), a core task within DAT benchmark.We propose a drone visual active tracking with reinforcement learning method called Goal-Centered-VAT (GC-VAT), aiming to improve the performance of tracking targets in complex scenes.As shown in Fig. 1, we model drone active tracking as a Markov Decision Process (MDP) and train a Drone Agent capable of adapting to unrestricted viewpoint conditions to track a target in the open scene.</p>
<p>MDP for Drone Active Tracking</p>
<p>We seek to learn end-to-end drone tracking policies in dynamic environments by modeling the task as an MDP: ⟨S, A, R, γ, T S, A, R, γ, T S, A, R, γ, T ⟩.In this representation, S S S denotes the state space, A A A represents the action space, and γ γ γ is the discount factor.At each time step t, the agent takes the state s t ∈ S S S as input and performs an action a t ∈ A A A. Next, the simulator transitions to the next state s t+1 = T (s t , a t ) and calculates the reward r t = R(s t , a t ) for the current step.The details of the MDP are as follows:</p>
<p>State S S S is the visual information of the scene.At each time step t, the camera captures one image of size 84 × 84 as the current state.</p>
<p>Action A A A is a set of discrete actions, including forward, backward, leftward, rightward, turn left, turn right, and stop movements.At each time step, the Drone Agent selects an action a t ∈ A A A based on the state s t and actively controls the camera movement.</p>
<p>Transition T (s t , a t ) is a function T T T : S S S × A A A → S S S that maps s t to s t+1 .In this paper, we use the webots dynamics engine to provide a realistic transition function.</p>
<p>Reward R(s t , a t ) is the reward function.The goal-centered rewards are given in Section 4.2.Network structure of Drone Agent.Similar to previous works [39,80], we select a backbone architecture consisting of a CNN followed by a GRU network [11] (see Appendix C.2).</p>
<p>Key Challenges in Drone Active</p>
<p>Tracking.In open-world environments, drones face unpredictable target behaviors and frequent occlusions.Designing a single reward that encourages diverse and robust tracking actions is extremely difficult.To address this, we propose a goal-centered reward in Section 4.2.Moreover, given the vast observation space, discovering successful policies is non-trivial.</p>
<p>To facilitate efficient learning, we introduce a curriculum-based training strategy in Section 4.4.</p>
<p>Goal-Centered Reward Design</p>
<p>For drone tracking a ground-based target, our reward is designed to characterize the target's position in the image and guide the drone to keep it centered.Therefore, we first need to select an appropriate distance metric to quantify the proximity between the target and the center in the image plane.</p>
<p>Since the drone typically captures images from a top-down perspective, the image plane is not parallel to the ground.Due to the affine transformation, the projection of the image plane becomes a trapezoid (see Fig. 3(b)), and the physical distance between the drone and the target cannot directly correspond to their pixel distance.Existing methods [39,19] compute the Euclidean distance between the drone and the target, which may not accurately reflect their spatial relationship in the image plane.</p>
<p>To address this issue, we employ a deviation metric ϕ(•, •) to measure the distance between the target and the image center projection, as illustrated in Fig. 3(b).Specifically, given a target point P g and the image center projection C g , the deviation metric is computed by
ϕ(Pg, Cg) = | Pg − Cg | | Eg(Pg, Cg) − Cg | ,(1)
where |P g − C g | denotes the distance from P g to C g and |E g (P g , C g )−C g | represents distance from point E g (P g , C g ) to the center.E g (P g , C g ) is the intersection of the line connecting P g and C g with the projected image boundary, as shown in Fig. 3(b).</p>
<p>The deviation ϕ(•, •) is designed to ensure that targets inside the image are closer to the center than those outside, with contours shown in Fig. 4(b).</p>
<p>Principles for Reward Design.The objective of the VAT task is to keep the target at the image center.Thus, the targets closer to the image center projection should get higher reward values.For deviation metric ϕ(•, •), the design principle of the reward function R ϕ (•) is defined as:
∀P1, P2 ∈ W, if ϕ1 &lt; ϕ2, then R ϕ (ϕ1) &gt; R ϕ (ϕ2),(2)
where W denotes the valid region with non-zero reward, and ϕ 1 , ϕ 2 represents the deviation from the target point to the image center projection.</p>
<p>Goal-Centered Reward Function.Our reward r gc (•) decreases as the target moves away from the projected image center C g , and is zero if outside, as shown as follows:
r gc (P g )= tanh (α(1−ϕ(P g , C g )) 3 ), P g ∈ I clip 0, otherwise .(3)
The attenuation degree of r gc (•) can be adjusted using the hyperparameter α, set to 4. The tanh(•) provides a strong indication of the task goal due to its relatively quick decay at the image center.I clip is the truncated image range set to prevent the drone from keeping the target at the edge of the image.</p>
<p>The truncation of the image can be controlled using the hyperparameter λ clip as: Append to buffer:
λ clip = H I clip /B ← B • r k , B r ← B r • τ k 10:
if k mod n = 0 then 11:</p>
<p>Update policy using PPO:
θ k+1 ← PPO_Update(θ k , B r ) 12:
Clear rollout buffer: B r ← ∅ Clear buffer: B ← ∅, B r ← ∅ More details about the Goal-Centered Reward.The reward function (Eq. 3) relies on the projections of the four corners and image center to compute deviation ϕ(•, •).As shown in Fig. 3(a), in the camera frame {c}, the image center and four corner points have the coordinates
C(−f, 0, 0), LU (−f,− 1 2 W, 1 2 H), LD(−f,− 1 2 W,− 1 2 H), RU (−f, 1 2 W, 1 2 H), RD(−f, 1 2 W,− 1 2 H),
where W and H are the image width and height and f denotes the camera focal length, which can be computed using the pinhole imaging principle [8] as: f = W 2 tan ( 1 2 F oV ) .F oV is the camera field of view.Next, the equations of the lines connecting the image center and the four corner points to the optical center O c (0, 0, 0) can be obtained in frame {c}(light blue dashed lines in Fig. 3(a)):
                 lLUO c : x −f = 2y −W = 2z H lLDO c : x −f = 2y −W = 2z −H lRUO c : x −f = 2y W = 2z H lRDO c : x −f = 2y W = 2z −H lCO c : y = 0, z = 0 ,(4)
where l LU Oc is the line connecting LU to O c , similarly for l LDOc , l RU Oc , l RDOc and l COc .Thus, the projections of the points can be obtained by intersecting the lines with the ground plane.</p>
<p>Therefore, we next derive the expressions for the ground plane and the target.For clarity, we adopt a unified representation in frame {c}.In DAT scenes, the road surfaces are smooth.Thus, in the world frame {w} (see Fig. 3(a)), the ground plane G w is defined as: z = h, where h denotes the ground height.For simplicity, we here express G w in {c} as G c : A g x+B g y+C g z+D g = 0, with A g , B g , C g , D g derived in Appendix C.2. Furthermore, the target coordinates P v = (x v , y v , z v , 1) T in {w} can be transformed to {c} using homogeneous transformation matrix [7] T cw : P g = T −1 cw P v .Subsequently, the ground projections can be obtained by intersecting lines in Eq. 4 and G c :
                   LUg : (−f, − 1 2 W, 1 2 H)t lu , t lu = Dg(Agf + 1 2 BgW − 1 2 CgH) −1 LDg : (−f, − 1 2 W, − 1 2 H)t ld , t ld = Dg(Agf + 1 2 BgW + 1 2 CgH) −1 RUg : (−f, 1 2 W, 1 2 H)tru, tru = Dg(Agf − 1 2 BgW − 1 2 CgH) −1 RDg : (−f, 1 2 W, − 1 2 H)t rd , t rd = Dg(Agf − 1 2 BgW + 1 2 CgH) −1 Cg : (− Dg Ag , 0, 0) ,(5)
where LU g , LD g , RU g , RD g and C g are the projections of LU , LD, RU , RD and C. Using target coordinates P g and Eq. 5, the reward is computed as Eq. 3. See Appendix C.2 for details.</p>
<p>Theoretical Gurantees on Reward Design</p>
<p>Existing methods [39,19] assume a fixed forward camera view and use distance-based rewards.However, when the view changes, these rewards may fail due to the affine transformation effect in image projection.We hereby provide a theoretical analysis to show that commonly used distancebased rewards will fail when the camera deviates from a fixed horizontal forward view.</p>
<p>To this end, we define R d (•) as a distance-based reward using Euclidean distance between the target and the image center projection.A distance-based reward R d (•) satisfying Eq. 2 may still assign higher rewards to targets farther from the center under the metric ϕ(•, •), rendering it ineffective.In contrast, any deviation-based reward R ϕ (•) satisfying Eq. 2 can effectively reflect the target position.</p>
<p>Proposition 1 The commonly used Euclidean distance d(•, •) between the target and the image center proposition does not align with the deviation ϕ(•, •) of the target from the image center projection, when the camera is not at a fixed horizontal forward viewpoint.That is:
∃P1, P2 ∈ Ip, s.t. ϕ1 &lt; ϕ2, d(P1,Cg) &gt; d(P2,Cg),(6)
where ϕ i = ϕ(P i , C g ), P i are points in the projection region I p , C g is the image center projection.See Appendix C.1 for theoretical proof.</p>
<p>Remark 1.A distance-based reward R d (•) satisfying Eq. 2 results in targets closer to the center receiving lower rewards, when the camera is not at a fixed horizontal forward viewpoint.That is:
∃P1, P2 ∈ Ip, s.t. ϕ1 &lt; ϕ2, R d (d1) &lt; R d (d2),(7)
where d i = d(P i , C g ), and ϕ i = ϕ(P i , C g ) .This illustrates the failure of the distance-based reward under these viewpoints.See Appendix C.1 for theoretical proof.</p>
<p>Qualitative Analysis.According to the Theoretical Analysis above, rewards should decrease monotonically along the deviation contours in Fig. 4(b) as the target moves toward the projection boundary.Thus, the reward contours must align with the deviation contours.The contours of r gc (•) in Fig. 4(b) perfectly align, indicating accurate position feedback.In contrast, D-VAT [19] (see Fig. 4(a)) shows misaligned contours, explaining its failure as noted in Remark 1.</p>
<p>Training with Curriculum Learning</p>
<p>DAT scenes contain numerous dynamic targets and obstacles, hindering convergence and performance.</p>
<p>Progressively training the agent from simpler to more complex environments enhances performance and accelerates learning for the final task [63].Therefore, we propose a Curriculum-Based Training (CBT) strategy to optimize reinforcement learning training in complex environments.</p>
<p>To address the challenges, we employ the Proximal Policy Optimization (PPO) [55] algorithm, known for its efficiency in control tasks.To further enhance agent adaptability and robustness, we apply domain randomization during agent training.Specifically, we randomize the drone's initial position and orientation relative to the target to promote diverse behaviors.Additionally, we randomize the gimbal pitch angle to improve the agent's spatial perception.See Appendix C.2 for further details.</p>
<p>Given the scene complexity, we adopt a CBT strategy, which divides the model training into two stages.The first stage consists of a simplified environment with straight line target trajectories and no obstacles.The agent learns to center the target through the reward r t in Eq. 3. In the second stage, the agent encounters more varied target movements and complex visual information, such as tree occlusions and crosswalks.The goal of the agent is to develop stronger generalization abilities based on task understanding in the first stage.See Algorithm 1 for the pseudocode of the CBT strategy.</p>
<p>Experiments</p>
<p>Experimental Settings</p>
<p>Experimental Setup.We conduct cross-scene and cross-domain tests.The former tests an agent trained under daytime conditions in unseen scenes with the same weather.The latter evaluates the agent in the same scene under varying weather conditions.See Appendix E.1 for details.</p>
<p>Metrics.We use cumulative reward (CR = E l t=1 r gc ) and tracking success rate (T SR =
1 E ml E l
t=1 r dt × 100%) to evaluate the agent performance.CR primarily reflects how well the agent centers the target over episode length E l , while T SR measures the ability to keep the target in view, with r dt = 1 meaning the target is within the view (See Appendix C), and E ml denoting the  Step(M )</p>
<p>f armland-da Step(M ) Step(M ) 0 1.7 3.3 5.0 6.6 8.3 9.9</p>
<p>Step(M ) Step(M )    Baselines.We reproduce two SOTA methods: AOT [39] and D-VAT [19].Both baselines and other methods [81,18] use distance-based rewards.As concluded in Section 4.3, they may fail in tilted top-down views.Thus, these baselines sufficiently highlight GC-VAT superiority.See Appendix D.</p>
<p>Comparison Experiments</p>
<p>We compare our GC-VAT with the SOTA methods for within-scene performance and cross-scene cross-domain generalization performance on DAT benchmark.As shown in Fig. 5, our method achieves consistently higher and steadily increasing rewards throughout training, demonstrating its effectiveness.Both AOT and D-VAT methods fail to learn effective policies due to the misleading feedback from their distance-based rewards.In particular, AOT learns to quickly drive the target out of view, resulting in a rapidly declining reward curve.The results validate the theoretical analysis in Section 4.3.It is worth noting that although AOT and D-VAT exhibit low variance in their experimental results, consistently low rewards typically indicate a failure to learn effective tracking policies.</p>
<p>Within-scene performance.We train the model on all scenes and evaluate it on the original scene.Our GC-VAT performs significantly better than other methods as shown in Table 2.For the CR, the average performance improvement on six maps relative to the D-VAT method is 591%(35 → 242).Regarding the T SR, the average enhancement is 279%(0.19→ 0.72).</p>
<p>Cross-scene performance.Our method demonstrates strong cross-scene generalization, as shown in Table 2. Specifically, GC-VAT achieves a 376%(37 → 176) improvement in average CR and a 200%(0.19→ 0.57) improvement in average T SR compared to D-VAT.</p>
<p>Cross-domain performance.As shown in Table 3, our method outperforms existing methods significantly in cross-domain generalization.Specifically, GC-VAT demonstrates an average CR enhancement of 509%(35 → 213) relative to D-VAT and T SR boost of 253%(0.19→ 0.67).</p>
<p>Ablation Experiments</p>
<p>We conduct ablation experiments on Goal-Centered Reward to validate the results of the analysis presented in Section 4.3.Moreover, we verify whether the Curriculum-Based Training strategy and domain rondomization from Section 4.4 lead to a significant performance improvement.We present results on the farmland map in Table 4, with additional results provided in Appendix E.3.</p>
<p>Effectiveness of reward design.We contrast the performance of GC-VAT method when using the reward defined in Eq. 3 and that in [19].As shown in Table 4, significant performance enhancements (about 800% improvement in T SR across-scene and cross-domain) are evident with the utilization of Eq. 3.These results strongly corroborate the analysis in Section 4.3 and underscore the effectiveness of the proposed reward.See Appendix E.3 for more experimental results.</p>
<p>Effectiveness of CBT strategy and domain randomization.As shown in Table 4, without the CBT strategy, the model fails to learn effective tracking policies, resulting in consistently low rewards across different tests.In addition, our domain randomization approach yields significant improvements.Specifically, AR, HR, V R, and P R denote the randomization of the drone's initial angle, horizontal and vertical distance relative to the target, and gimbal pitch angle, respectively.Among these, AR contributes the most to performance gains, indicating that encouraging diverse actions through angle randomization facilitates the agent's exploration of optimal policies.</p>
<p>Robustness under wind gusts and precipitation.To further investigate the impact of real-world disturbances on the GC-VAT method, we conduct rigorous tests under wind gusts and sensor degradation caused by precipitation.Specifically, we simulate wind effects by applying randomized perturbations along the forward, lateral, and yaw directions during testing.The results are summarized in Table 5, where the model is trained on citystreet-day and evaluated on citystreet-foggy with added wind perturbations.The Tracking Success Rate (TSR) drops by less than 0.06, demonstrating that GC-VAT maintains strong robustness under significant wind disturbances.See Appendix E.3 for more details.</p>
<p>To simulate the blurring caused by raindrops, we follow established practices in test-time adaptation literature [34].Specifically, we train the policy on citystreet-day map and evaluate under synthetically generated rain in within-scene, cross-scene, and cross-domain settings.To ensure realism, we exclude snowy conditions from the cross-domain evaluation, as snow and rain rarely co-occur in real-world environments.The results in Table 6 show only marginal performance degradation (less than 0.07 in TSR) under rain simulation, confirming that GC-VAT is robust to blurring caused by raindrops.</p>
<p>Robustness to distractors and novel targets.As shown in Table 5, our model maintains high tracking performance even when a similar-looking vehicle is introduced near the target, demonstrating its ability to effectively distinguish the true target from confusers.In addition, we evaluate GC-VAT on an unseen target class (bus).As shown in Table 6, our model maintains strong tracking performance, with a TSR drop of less than 0.03 when encountering this novel object.</p>
<p>Predicted Actions</p>
<p>Experiments in Real-world Scenarios</p>
<p>Effectiveness on real-world images.Due to the difficulty real-robot evaluation, we follow [39] and validate GC-VAT on real images.We perform zero-shot transfer tests using 8 videos each from VOT [30], DTB70 [35] and UAVDT [20] datasets.Although camera control is unavailable in recorded videos, we can feed frames into the model and verify the reasonableness of its predicted actions.</p>
<p>The output actions for the VOT video car16 are shown in Fig. 6.Each point represents the target position in the image, with colors indicating different actions.As Fig. 6 illustrates, when the target is located on the left (right) side, the tracker tends to move left (right), attempting to bring the target to the center.Quantitatively, we use the Correct Action Rate, i.e., the accuracy of predicted actions, to evaluate the performance.As shown in Table 7, GC-VAT achieves an average Correct Action Rate (CAR) of 81.0% across 24 videos, demonstrating its effectiveness.More importantly, it is significantly superior to random policy (p &lt; 0.001) as verified by a t-test.See Appendix E.4 for more results.</p>
<p>Effectiveness on real drones.Furthermore, as a critical step beyond image-based evaluation, we conduct real-world experiments on a DJI Mini 3 Pro [13] drone.As shown in Fig. 7, we deploy GC-VAT on a laptop equipped with an RTX 3050 GPU and an Intel i5 CPU, use the DJI Mobile SDK [12] to obtain images, and control the drone with the predicted actions.The entire pipeline operates at over 30 FPS.As Fig. 8 illustrates, the model can output actions that maintain the target at the image center.Quantitatively, GC-VAT achieves an average zero-shot TSR of 88.4% and a CAR of 81.3%.This successful zero-shot Sim-to-Real transfer validates the practical applicability of our approach.</p>
<p>Conclusion and Potential Impacts</p>
<p>In this paper, we propose the first open-world drone active air-to-ground tracking benchmark, called DAT.DAT benchmark encompasses 24 city-scale scenes, featuring targets with human-like behaviors and high-fidelity dynamics simulation.DAT also provides a digital twin tool for unlimited scene generation.DAT benchmark has the potential to impact several key areas, including: 1) Forgetting in Reinforcement Learning, 2) Robustness in Reinforcement Learning, 3) Multi-Agent Reinforcement Learning, and 4) Sim-to-Real Deployment.Additionally, we propose a reinforcement learningbased drone tracking method called GC-VAT, aiming to improve the performance of drone tracking targets in complex scenarios.Specifically, we design a Goal-Centered Reward to provide precise feedback across viewpoints to the agent, enabling it to expand perception range through unrestricted perspectives.Then we propose qualitative and theoretical methods to analyze the reward effectiveness.Moreover, inspired by curriculum learning, we implement a Curriculum-Based Training strategy that progressively improves agent performance in increasingly complex scenarios.Experiments on the simulator and real-world images validate the analysis and demonstrate that our method is significantly superior to the SOTA methods.</p>
<p>Supplementary Materials for "Open-World Drone Active Tracking with</p>
<p>Goal-Centered Rewards" Most of the proposed visual tracking benchmarks belong to passive visual tracking.LaSOT [23] and OTB2015 [71] benchmarks contain a large number of ground-based videos.These benchmarks include target videos, and the tracking algorithms utilize both the video frames and the target labels for tracking.However, ground cameras tend to be affected by occlusion and suffer from the shortcoming of limited perceptual range, so the need for drone viewpoint tracking is gradually increasing in practical applications.UAV123 [43] and VisDrone2019 [21] benchmarks are proposed for drone viewpoint, expanding the spatial dimension of perception.Meanwhile, the single-object tracking benchmarks have difficulties for many targets.MOT20 [17] and TAO [16] benchmarks are proposed for multi-object tracking to solve the above problems.In addition, the above benchmarks include videos from the RGB camera.The RGB camera's recognition capabilities are limited in complex scenes, such as ocean environments, and challenging weather conditions, including nighttime and foggy.IPATCH [47] provides extra infrared images and other sensors like GPS to supplement the information of the sea scene.Huang et al. propose Anti-UAV410 [28], which provides infrared camera images for drone tracking.</p>
<p>Visual object tracking methods can be categorized into three main types: Tracking by Detection, Detection and Tracking (D&amp;T), and pure tracking.Tracking by Detection methods [5,69,6] treat tracking as a sequence of independent detection tasks.These methods use object detection algorithms [50,52] to identify the target object in each frame, connecting the detections through data association methods [31,70] for continuous tracking.While effective in multi-target tracking, these methods may suffer from high computational demands and issues with target occlusion.D&amp;T approaches [66,79,48] integrate detection and tracking, creating end-to-end models that ensure seamless information flow and reduce redundant calculations through shared feature extraction networks.Pure tracking methods can be categorized into two main types: Correlation Filters (CF) [72,44,27] and Siamese Networks (SN) [32,4,62].CF-based models train correlation filters on regions of interest, while SN-based models compare target templates with search areas to enable precise single-target tracking.</p>
<p>A.2 Visual Active Tracking</p>
<p>Passive visual tracking often falls short in real-world scenarios due to the highly dynamic nature of most targets.Visual Active Tracking (VAT) aims to autonomously follow a target object by controlling the motion system of the tracker based on visual observations [40,80,75].Thus, VAT offers a more practical yet challenging solution for effective tracking in dynamic environments.Maalouf et al. [40] propose a two-stage tracking method (named FAn), which is based on a tracking model and a PID control model.This method accomplishes the fusion of perception and decision-making by transferring control information from the visual tracking model to the control model.However, the visual network necessitates extensive human labeling effort and the control model requires parameter adjustments for each scene, significantly constraining the model's generalizability.Recently, many approaches [39,18,80,19] model the VAT task as a Markov Decision Process and employ endto-end training with reinforcement learning, resulting in a significant enhancement of the agent's generalizability.</p>
<p>The complexity and diversity of VAT benchmarks are crucial for training agents with high generalizability.One common approach [19,18,80] to enhancing environmental diversity involves modifying texture features and lighting conditions within a single scene.However, these methods often result in low scene fidelity and unrealistic object placement.While UE4 [24] is used to create photorealistic environments in some benchmarks [80,39], these benchmarks still face limitations in diversity and map size.Furthermore, the scenarios provided by these methods are often task-specific, offering limited configurability and lacking a unified benchmark for VAT tasks.</p>
<p>Existing approaches to VAT frequently neglect the randomness of target trajectories and the scalability of platforms.Target trajectories are typically predefined by rule-based patterns [19,18,39], which significantly restrict the exploration space.Zhong et al. [80] introduce learnable agents as targets, increasing trajectory randomness but adding additional cost.Most benchmarks provide only a single category of target [19,18,80,39], limiting scalability and necessitating repetitive work for environment development.Zhou et al. [82] utilize CoppeliaSim [2] to provide five categories of noncooperative space objects.However, the use of a solid black background makes it unsuitable for general VAT scenarios.In contrast, our environment supports diverse, real-world target types and offers unified, lightweight management of target behaviors, ensuring both rationality and randomness in their actions.</p>
<p>A.3 Reinforcement learning in Visual Tracking</p>
<p>Reinforcement learning (RL) is widely used in large language models [26] and robot control [53] to improve exploration performance.It is also commonly applied in visual object tracking [76,51,78].Song et al. [59] propose a decision-making mechanism based on hierarchical reinforcement learning (HRL), which achieves state-of-the-art performance while maintaining a balance between accuracy and computational efficiency.However, the actions generated by reinforcement learning in the aforementioned work cannot directly influence the camera's viewpoint, thereby failing to fully leverage the decision-making capabilities.Real-world applications increasingly require robust tracking in highly dynamic scenes, motivating researchers to explore reinforcement learning agents for effectively synchronizing visual perception and decision-making in VAT tasks.Dionigi et al. [19] demonstrate the feasibility of reinforcement learning for drone VAT missions.However, the assumption of a fixed-forward perspective limits its applicability in real-world tasks.</p>
<p>A.  until they are trained on the entire dataset.CL is widely used in large language models [64] and robot control.As for robot control, reinforcement learning training is difficult due to the complexity of the training scenarios and the large action spaces.Therefore, curriculum learning is often required to reduce the difficulty of agent training.For instance, many works improve the walking ability of legged robots by adjusting terrain parameters through curriculum learning [53,41].Other studies improve the pushing and grasping performance of robotic arms by progressively increasing task difficulty [38,61,45].</p>
<p>In this paper, Curriculum Learning is introduced in the VAT task, and the training environment is transitioned from simple features to complex scenarios to achieve successful tracking of agent in complex outdoor environments.</p>
<p>B More Details of DAT Benchmark</p>
<p>More details of the digital twin tool.Our digital twin tool is based on the osm_importer tool in the webots simulation software.Users first need to download the map description file (.osm file) for a specific area from the OpenStreetMap website.Then, the tool preprocesses the map according to the configuration, modifying information such as the number of lanes and lane directions, and converts the processed file into a road network file (.net.xml file) that can be read by SUMO.Following this, the tool adds traffic lights and intersection traffic rules to the road network based on the configuration, ensuring that the traffic flow operates correctly when the map is converted into a 3D scene.Finally, the tool reads the road, vegetation, and building information and converts them into PROTO assets for webots, which can then be correctly recognized and used by the DAT benchmark.</p>
<p>Scenario Construction.Among the DAT scenes, three scenarios: citystreet, downtown, and lake are directly derived from real-world locations with the digital twins tool.Specifically, the citystreet scenario is based on a small town in Los Angeles, the downtown scenario is derived from Manhattan, and the lake scenario is modeled after Wolf Lake Memorial Park in Indiana.In contrast, the village, desert, and farmland maps possess complex and unique features that are not adequately captured by OpenStreetMap (OSM) data.For example, the village map features mountainous terrain with tunnels, while the farmland map is characterized by diverse multicolored patterns.To overcome these limitations, we use Creo software [29] to model detailed scene elements, which are then integrated into the webots for constructing realistic maps.</p>
<p>Targets.All tracking target illustrations are presented in Fig. 9. Specifically, Fig. 9(a) presents automobile and motorbike tracking targets, including passenger vehicles (the first seven cars), buses, trucks, trailers, and motorcycles (such as scooters and motorbikes).These two categories of tracking targets leverage Simulation of Urban Mobility (SUMO) [36] for road behavior modeling and interaction management with other targets.In contrast, Fig. 9(b)-(d) display pedestrian, wheeled robot, and legged robot tracking targets, respectively.These three types of targets utilize SUMO paths for position initialization and rely on specific controllers for action and behavior management.</p>
<p>Sensors.In the real world VAT tasks, a single camera cannot ensure the agent's stability and robustness.Thus, integration with other sensors is often required.The DAT benchmark provides common sensors that can obtain the drone's state parameters relative to the world coordinate system.The drone's position and velocity are determined using GPS, while its acceleration is measured by an accelerometer, providing essential self-referential data for visual navigation tasks.Angular velocity is recorded via a gyroscope, and Euler angles obtained from the IMU are converted into quaternions to facilitate state estimation and ensure orientation stability.Additionally, the RPLIDAR A2, provided by DAT, generates point cloud data, which supports tasks such as obstacle avoidance and navigation by delivering detailed environmental information.The specific sensors, parameters and potential tasks are in Table 8.</p>
<p>Additional Parameters.The training process of VAT agents often requires additional parameters for effective reward design.To facilitate this, DAT benchmark provides 4 categories comprising a total of 13 parameters, supporting diverse reward design strategies, as detailed in Table 9.</p>
<p>First are the camera parameters, which mainly include image width cameraWidth, image height cameraHeight, field of view cameraFov, and focal length cameraF.Utilizing these, the camera plane can be projected onto the ground to aid in reward construction.</p>
<p>Next is the homogeneous transformation matrix (HTM).In the reward design, coordinate transformations are often required to express physical quantities within a unified coordinate system, enabling consistent calculations.For example, prior studies [19,39,18] transform the position, velocity, and acceleration of targets into the tracker's coordinate system to construct rewards.To support such operations, DAT benchmark provides T cw , the HTM mapping the drone camera coordinate system to the world coordinate system, and T tw , the HTM mapping the tracking target's coordinate system to the world coordinate system.</p>
<p>Additionally, for the state of the tracker itself, cameraMidPos represents the position of the drone camera's optical center in the world coordinate system.The parameter crash indicates whether the drone collides with any buildings in the scene, which can be used in reward design for obstacle avoidance tasks.</p>
<p>Lastly, for ease of model training in simulations, reward design often depends on some privileged information, i.e., variables that are almost impossible to obtain in real-world settings.Thus, DAT benchmark also provides such adaptations.For example, carMidGlobalPos gives the target's position in the world coordinate system, and carDronePosOri represents the target's orientation and position relative to the drone coordinate system, frequently used in VAT reward design [19,39,18].Furthermore, information on the target's direction and type is provided.</p>
<p>Task Configuration.We encapsulate the scenes, tasks, and domain randomization into Python classes, and provide 3 different environment classes for different algorithm requirements.The base environment class directly interacts with webots and is designed to support asynchronous reinforcement learning algorithms, such as the asynchronous advantage actor-critic (A3C) algorithm [42].The Gymnasium environment class wraps the base environment class into a Gymnasium [60] interface, enabling direct compatibility with popular reinforcement learning libraries, such as Stable-Baselines3 [49] and Tianshou [68] for efficient algorithm development and evaluation.The parallel environment class encapsulates the base environment class to enable parallel execution, providing direct support for synchronous algorithms, such as proximal policy optimization (PPO) [55] and soft actor-critic (SAC) [25].Additionally, the scenario selection, tracker and target configuration, SUMO parameters, task additional parameters, and randomization methods can all be efficiently customized through a JSON configuration file.Given:</p>
<p>1.In the image plane, the deviation ϕ(•, •) of point A and B from the image center O is the same, i.e. ϕ(A, O) = ϕ(B, O).</p>
<ol>
<li>In the projection plane, the Euclidean distance from A ′ and C ′ to the ideal position O ′ are equal, i.e.
d(A ′ , O ′ ) = d(O ′ , C ′ ).
It is evident that:</li>
</ol>
<p>1.For any point D ′ on line segment B ′ C ′ , the following relationship holds:
d(O ′ , D ′ ) &gt; d(A ′ , O ′ ).</p>
<p>The corresponding point D in the image lies on line segment BC, and thus ϕ(D, O) &lt; ϕ(A, O).</p>
<p>Thus, it is clear that the actual distance between the target and the ideal position is inconsistent with the deviation of the target from the image center in the image.</p>
<p>Theoretical proof of Remark 1.According to Proposition 1, the following relationship between the Euclidean distance and the deviation holds:
∃P1, P2 ∈ Ip, s.t. ϕ1 &lt; ϕ2, d1 &gt; d2,(8)
where ϕ i = ϕ(P i , C g ) and d i = d(P i , C g ).Therefore, for a distance-based reward function R d (•) that satisfies the Reward Design Principle, it follows that: Domain Randomization.While simpler settings facilitate the agent's learning of task objectives, they also heighten the risk of the agent rapidly converging to a suboptimal action distribution, undermining the exploration process.Consequently, implementing domain randomization is essential.This is achieved through the randomization of the drone's initial position and orientation relative to the target, necessitating a broader range of actions to maximize rewards.Moreover, to enhance the agent's spatial perception ability, randomization is also introduced in its gimbal pitch angle.
∃P 1 , P 2 ∈ I p , s.t. ϕ 1 &lt; ϕ 2 , R d (d 1 ) &lt; R d (d 2 ).(9)
In our two-stage curriculum learning process, we employ identical domain randomization.The flight altitude is selected from the interval [13,22]m, and the camera pitch angle is chosen from [0.6, 1.38]rad.These parameters are consistent throughout each episode.Meanwhile, the drone's initial orientation relative to the target fluctuates within the range [−π, π]rad, and the target's initial position is set between [−4.5, −</p>
<p>Details on coordinate transformations.Given two planes P 0 : n0 x T +D 0 = 0 and P 1 : n1 x T +D 1 = 0, along with the HTM T 01 from P 0 to P 1 .The T 01 is defined as:
T01 = R01 t01 0 1 .(10)
Hence, the expression of plane P 1 can be obtained using the analytical expression of plane P 0 and T 01 as follows:
n1 T = R01 n0 T , D1 = D0 − n1t01.(11)
Considering the ground plane G w : z = h in the world coordinate system {w}, with representation in the camera coordinate system {c} denoted as G c : A g x+B g y +C g z +D g = 0, the vectors of these two planes are P Gw = (0, 0, 1, −h) and
P Gc = (A g , B g , C g , D g ).
Furthermore, from Table 9, we can obtain the HTM T cw from {c} to {w} defined as follows:
Tcw = Rcw tcw 0 1 ,(12)
where R cw is the rotation matrix from {c} to {w}, which can be expressed in row vector form as: R cw = [r 1 , r 2 , r 3 ] T .Therefore, the homogeneous transformation matrix (HTM) T wc , which represents the transformation from the world coordinate system {w} to the camera coordinate system {c}, can be expressed as follows:
Twc = R T cw −R T cw tcw 0 1 .(13)
Using Eq. 11 and the matrix T wc , the plane G c can be formulated as P Gc = (r T 3 , −h + r T 3 R T cw t cw ).Privileged knowledge available for Drone Agent.During training in the simulator, the drone agent has access to additional information (e.g., the precise location of the target).However, during testing and real-world deployment, such privileged knowledge is not available.Sparse Reward.In addition to the dense reward function described in the main text, we also provide a sparse reward function design.The sparse reward only provides a fixed reward when the target is within the image and no reward when it is outside.The definition of r d is as follows.
r d = 1, t ∈ I 0, otherwise ,(14)
where I represents the image range.This reward can be used to construct the metric, Tracking Success Rate (TSR).Training algorithm.For the training method of GC-VAT, we choose to use PPO algorithm.PPO algorithm regulates the speed of gradient updates by constraining the magnitude of policy changes r t , expressed as follows:
rt(θ) = π θ (at|st) π θ old (at|st) ,(15)
where π θ and π θ old are the new and old policies.Additionally, to enhance the agent's exploration, we introduce an entropy loss term H, formulated as:
H(π θ (s)) = − a π θ (a|s) log π θ (a|s).(16)
The optimization objective for the actor is as follows:
LA = Ê[min (rt Ât, clip(rt, 1−ϵ, 1+ϵ) Ât)+βH],(17)
where Ât is the advantage function, ϵ is the clip parameter, and β is the entropy coefficient.The expression of Ât is:
Ât = E l −t l=0 (γλ) l δ t+l ,(18)
where T, λ, δ t+l are the data collection step, generalized advantage estimator (GAE) [56] discount factor and temporal difference error respectively.The optimization objective expression of the critic network V is defined as:
LC = Êt[(rt + γV (st+1) − V (st)) 2 ].(19)
The hyperparameters of the PPO algorithm used in this article are set as follows: discount factor γ = 0.9, GAE discount factor λ = 0.95, entropy coefficient β = 0.01, PPO clipping parameter ϵ = 0.2.</p>
<p>Curriculum Learning for Agent Training.We introduce a Curriculum-Based Training (CBT) strategy designed to progressively enhance the performance of the tracker.In the first-stage curriculum, the agent is trained to track vehicles moving along straight trajectories without occlusions or extra interference.In the second-stage curriculum, the agent is exposed to visually complex environments and tasked with tracking targets exhibiting diverse and dynamic behaviors.The scenario of each stage is shown in Fig. 12, where the upper row is the first-stage environment, and the lower row corresponds to the second-stage environment.</p>
<p>D Baselines</p>
<p>Active Object Tracking (AOT) [39].In this paper, the agent learns to follow a fixed target-tracking trajectory using A3C.In addition, the agent uses the following reward:
r = A − ( x 2 + (y − d) 2 c + λ | ω |),(20)
where d represents the optimal distance between the tracker and the target, c is the maximum allowable distance, and A denotes the maximum reward.In the original paper, c = 200 and A = 1.0.During our replication, we set A = 1.0, but due to the drone's camera being tilted downward, a value of c = 200 would far exceed the camera's field of view, which is unrealistic.Therefore, we modify the parameter c to be the maximum offset distance that keeps the target within the image, i.e., c = 9.</p>
<p>D-VAT [19].In this approach, the agent uses an asymmetric Actor-Critic network structure and the soft actor-critic learning method [25] to accomplish the task of drone tracking another drone.In the  actual comparative experiments, we convert it from a continuous action space to a discrete action space, referring to [10].Additionally, the method uses the following reward function.
r(k) = re(k)−kvrv(k)−kuru(k) ∥y(k)∥ &gt; dm −kc otherwise,(21)
In the above equation Eq.21, r v (k) and r u (k) are regularization terms for the drone's speed and output control, as shown in Eq. 22.For the discrete action space, the regularization term has a fixed value for a given action.This term only regularizes the linear velocity of the drone, which causes the drone to tend to perform rotational movements.Therefore, in the reproduction process, we set k v = 0 and k u = 0. Additionally, due to the unexpectedly large acceleration values obtained for the target relative to the tracker under the discrete action setting, we set the input acceleration of the critic network to a(k) = 0.
rv(k) = ∥v(k)∥ 1 + ∥v(k)∥ , ru(k) = ∥u(k)∥ 1 + ∥u(k)∥ . (22)
It is important to note that in the AOT and D-VAT experiments, the target is initially positioned at the center of the tracker's image, and the initial forward directions of both the tracker and the target are aligned.Additionally, since the success criterion of DAT requires the agent to keep the target at the center of its view, the optimal distance between the tracker and the target is defined as the distance in the forward direction when the target is at the center of the camera's field of view.The tracker's flight altitude is set to 22 meters, and the gimbal pitch angle is 1.37 radians, which remains consistent with the parameters used during testing.10.</p>
<p>Ablation Experiment Settings.In this section, we introduce the training conditions of the singlestage RL and GC-VAT, as well as the criteria for stage transitions.In single-stage RL, the agent is placed in one of six scenarios (citystreet, desert, village, downtown, lake, and farmland) for training.For GC-VAT, the agent is first trained in an environment where a randomly colored target moves straight along a line without obstacles.After convergence, the model is then trained in the corresponding complex scenarios.The transition steps T for GC-VAT are in Table 11.</p>
<p>E.2 Comparison Experiments</p>
<p>We provide a comprehensive analysis of the comparative experimental results.Specifically, we provide detailed evaluations for within-scene (same scenes, same weather), crossscene and cross-domain testing.Table 12 reports the CR metric of three models under cross-scene and cross-domain conditions, while Table 13 presents the T SR metric.</p>
<p>As shown in Table 12 and Table 13, the proposed GC-VAT significantly outperforms SOTA methods.Due to the reward design based on physical distance, both the AOT [39] and D-VAT In contrast, the proposed GC-VAT achieves superior convergence across all scenes.Specifically, in cross-scene experiments, the testing performance of the agent on the downtown map is relatively low, indicating that dense buildings and complex road elements pose significant challenges to the agent.</p>
<p>Conversely, the testing performance on the village map is comparatively high, suggesting that the uniform color and simpler road conditions in the village map present fewer challenges.</p>
<p>For cross-domain testing experiments, the agent performs well under night and foggy conditions but struggles under snow conditions.This indicates that the proposed GC-VAT exhibits strong robustness to changes in lighting and visibility but is less adaptive to variations in scene tone.</p>
<p>E.3 Ablation Experiments</p>
<p>We present a comprehensive analysis of the ablation studies.First, we provide the reward curves for the citystreet, desert, village, downtown, lake, and farmland maps (see Fig. 13).Next, we provide detailed experimental results on the effectiveness of the Curriculum-Based Training strategy, as shown in Table 14 and Table 15.</p>
<p>Finally, the effectiveness of the reward in the GC-VAT can be found in Table 16 and Table 17.</p>
<p>Effectiveness of reward design.To experimentally validate the effectiveness of the reward design proposed in this paper and to corroborate the theoretical proof in Appendix C.1, we conduct ablation experiments on the reward function.The comparative method utilizes the reward function from [19].The detailed experimental results for the CR and T SR metrics are provided in Table 16 and Table 17.</p>
<p>For within-scene testing, the GC-VAT achieves an average improvement of 1100%(0.06 → 0.72) in the T SR metric compared to the reward design in [19].In cross-scene and cross-domain testing, the GC-VAT achieves average enhancements of 850%(0.06→ 0.57) and 1017%(0.06→ 0.67) in the T SR metric, respectively.These results demonstrate the high effectiveness of the proposed reward.The experimental results demonstrate that single-stage reinforcement learning methods without the CBT strategy successfully learn task objectives and achieve convergence on the desert, village, and lake maps.These three maps exhibit similar environmental characteristics: the desert and village maps feature uniform background colors and relatively simple road elements.Although the desert map has road segments partially covered by sand, these challenges are easy for the agent to overcome.Similarly, while the village map includes tunnels that may block vision, the proportion of tunnels is low.Additionally, although the lake map exhibits diverse background colors, the diversity primarily arises from vegetation-covered areas, which occupy a small proportion of the map, resulting in low challenges for the agent.In contrast, single-stage reinforcement learning methods without the CBT strategy fail to converge on the citystreet, downtown, and farmland maps.This suggests that as the visual complexity of scenes and the density of elements increase, directly applying single-stage reinforcement learning is highly challenging and unlikely to converge.These results demonstrate the effectiveness of the CBT strategy.</p>
<p>Robustness under wind gusts and precipitation.In the wind gust simulation experiments, we apply wind velocities in the range of [2.5, 7.5]m/s for forward and lateral directions, and angular rate disturbances of [0.05, 0.15]rad/s around the yaw axis to mimic turbulence and gusts.</p>
<p>E.4 Experiments in Real-world Scenarios</p>
<p>We selected eight video sequences each from the VOT [30], DTB70 [35], and UAVDT [20] datasets to evaluate the transferability of GC-VAT.Specifically, from the VOT benchmark, we chose the videos car1, car3, car6, car8, carchase, car16, following, and car9.From the DTB70 benchmark, we selected Car2, Car4, Car5, RcCar4, Car8, RaceCar, RaceCar1, and RcCar3.From the UAVDT benchmark, we chose S1603, S0201, S0101, S0306, S1201, S0303, S1301, and S1701.We provide qualitative visualizations for representative video sequences.Specifically, Fig. 14 shows the output actions for a video in VOT [30] named car6.Fig. 15 shows the output actions for a video in VOT named car8.Fig. 16 shows the output actions for a video in DTB70 [35] named Car4.Fig. 17 shows the output actions for a video in VOT named RaceCar1.</p>
<p>F Limitation</p>
<p>Although we validate the effectiveness of DAT and GC-VAT using real-world images and simple real-world scenarios, deploying the algorithm in truly open environments remains highly challenging.This is primarily due to the presence of numerous similar interfering objects and the high complexity of real-world conditions, which still exhibit a significant gap compared to simulated environments.We will further enhance the algorithm's adaptability and conduct testing in real open-world environments.</p>
<p>generate deepfakes for disinformation.On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.• The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.• If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).</p>
<p>Safeguards</p>
<p>Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?Answer: [NA] Justification: Our paper poses no such risks.Guidelines:</p>
<p>• The answer NA means that the paper poses no such risks.</p>
<p>• Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.• Datasets that have been scraped from the Internet could pose safety risks.The authors should describe how they avoided releasing unsafe images.• We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.12. Licenses for existing assets Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?Answer: [Yes] Justification: See anonymous homepage for details.Guidelines:</p>
<p>• The answer NA means that the paper does not use existing assets.</p>
<p>• The authors should cite the original paper that produced the code package or dataset.</p>
<p>• The authors should state which version of the asset is used and, if possible, include a URL.• The name of the license (e.g., CC-BY 4.0) should be included for each asset.</p>
<p>• For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.• If assets are released, the license, copyright information, and terms of use in the package should be provided.For popular datasets, paperswithcode.com/datasetshas curated licenses for some datasets.Their licensing guide can help determine the license of a dataset.• For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.• If this information is not available online, the authors are encouraged to reach out to the asset's creators.13.New assets Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?</p>
<p>Figure 1 :
1
Figure 1: A pipeline for drone VAT.</p>
<p>( a )Figure 2 :
a2
Figure 2: Statistics and simulator component examples of DAT.(a) Statistics on 7 complexity aspects in DAT scenes.(b) Example scenes of DAT.(c) Diverse behaviors of targets.(d) Examples of the tracking targets.More details can be found at https://github.com/SHWplus/DAT_Benchmark.</p>
<p>Figure 3 :Figure 4 :
34
Figure 3: Diagram of reward acquisition.</p>
<p>Figure 5 :
5
Figure 5: Reward values during training.</p>
<p>9±1 0.06±0.008±1 0.05±0.009±0 0.06±0.00w/o CBT 46±2 0.23±0.0153±16 0.26±0.0746±2 0.23±0.01w/o AR 106±88 0.44±0.23 92±72 0.37±0.1980±63 0.36±0.19w/o HR 174±1180.49±0.30148±1290.48±0.32184±1240.57±0.30w/o VR 211±1380.63±0.35161±1150.54±0.32203±1170.60±0.32w/o PR 139±1190.61±0.33 124±85 0.48±0.25145±1220.52±0.28Ours 243±117 0.68±0.32162±106 0.54±0.26222±110 0.65±0.27maximum episode length.Agents are initialized at four relative angles to the target ([0, π 2 , π, 3π 2 ] rad), with 10 episodes per angle (40 total).The mean and variance of these results are calculated for each map, and the final cross-scene and cross-domain performance are averaged across different scenes.</p>
<p>Figure 6 :
6
Figure 6: Results on real-world images.</p>
<p>Figure 7 :
7
Figure 7: Schematic of the real-world deployment pipeline.</p>
<p>Figure 8 :
8
Figure 8: Results on real drones.</p>
<p>Contents A Related Work A. 1 E. 1
11
Passive Object Tracking . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .A.2 Visual Active Tracking . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .A.3 Reinforcement learning in Visual Tracking . . . . . . . . . . . . . . . . . . . . . . . . . . .A.4 Curriculum Learning in Robot Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . .B More Details of DAT Benchmark C More Details of Proposed GC-VAT C.1 Theoretical Proof of Reward Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .C.2 More Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .Experiment Settings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .E.2 Comparison Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .E.3 Ablation Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .E.4 Experiments in Real-world Scenarios . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</p>
<p>Figure 9 :
9
Figure 9: Examples of DAT benchmark targets.(a) Illustration of tracking targets for 10 types of automobile and 2 types of motorbike.(b) Illustration of tracking targets for the pedestrian type.(c) Illustration of tracking targets for 5 types of wheeled robot.(d) Illustration of tracking targets for 6 types of legged robot.</p>
<p>Figure 10 :
10
Figure 10: Diagram for the theoretical proof of Proposition 1.</p>
<p>Figure 11 :Figure 12 :
1112
Figure 11: Network structure of Drone Agent.</p>
<p>Figure 13 :
13
Figure 13: Schematic diagram of reward curves on DAT scenes.</p>
<p>Figure 14 :
14
Figure 14: Qualitative results on images from the car6 video sequence.Arrows link data points to the visualization of associated scenarios.</p>
<p>Figure 15 :
15
Figure 15: Qualitative results on the car8 video.</p>
<p>Figure 16 :
16
Figure 16: Qualitative results on the Car4 video.</p>
<p>[19] fail to accurately reflect the agent's tracking performance from a top-down perspective (see Appendix C.1 for theoretical proof), leading to misleading training signals for the tracker.Consequently, neither AOT nor D-VAT can effectively learn meaningful features, resulting in irregular performance distributions.</p>
<p>Figure 17 :
17
Figure 17: Qualitative results on the RaceCar1 video.</p>
<p>Table 1 :
1
Comparison of DAT benchmark with simulators where existing methods are located.AD-VAT+ [80]D-VAT [19] AOT [39]DAT
Scenes84224Targets11124TrackerGroundDroneGroundBothDynamics✗Simplified✗Full PhysicsTarget Behavior Policy-based Rule-based Rule-based Human-likeScene BuildingManualManualManual Digital Twin</p>
<p>H, where H and H I clip are the heights of the original and the truncated image.We set λ clip = 0.7.Initial policy parameters θ 0 , phase threshold η, total steps N , rollout steps n 2: Initialize: Training phase phase ← 1, reward buffer B ← ∅, rollout buffer B r ← ∅ 3: for each step k = 0, 1, ..., N − 1 do
Algorithm 1 Curriculum-Based Training (CBT)1: Input: 4: if phase = 1 then5:Configure simple environment: linear target trajectories + no obstacles6:else7:Configure complex environment: varied target movements + obstacles/occlusions8:Collect transition τ k = (s t , s t+1 , a t , r t ) with rewards calculated via (3)9:</p>
<p>Table 2 :
2
Results of within and across scenes on DAT benchmark.±3 0.25 ±0.02 9 ±1 0.06 ±0.00 46 ±5 0.23 ±0.03 54 ±5 0.29 ±0.01 47 ±3 0.24 ±0.02 60 ±25 0.23 ±0.01 D-VAT 48 ±8 0.26 ±0.02 47 ±13 0.26 ±0.04 44 ±8 0.22 ±0.05 9 ±1 0.06 ±0.01 46 ±8 0.26 ±0.06 13 ±1 0.07 ±0.00 Ours 279 ±110 0.80 ±0.30 307 ±124 0.84 ±0.29 239 ±134 0.73 ±0.32 203 ±119 0.65 ±0.30 181 ±116 0.61 ±0.31 243 ±117 0.68 ±0.32 Cross Scene AOT 48 ±5 0.24 ±0.02 9 ±0 0.06 ±0.00 52 ±11 0.25 ±0.03 52 ±6 0.28 ±0.0348 ±5 0.24 ±0.02 49 ±7 0.24 ±0.03 D-VAT 49 ±9 0.26 ±0.04 48 ±8 0.27 ±0.03 50 ±14 0.25 ±0.06 8 ±1 0.05 ±0.00 51 ±14 0.25 ±0.06 14 ±1 0.07 ±0.01 Ours 144 ±111 0.52 ±0.29 229 ±115 0.67 ±0.27 156 ±119 0.55 ±0.31 201 ±121 0.64 ±0.30 163 ±115 0.51 ±0.29 162 ±106 0.54 ±0.26
Methodcitystreet CR T SRCRdesert T SRvillage CR T SRdowntown CR T SRCRlake T SRfarmland CR T SRWithin SceneAOT 49 citvstreet-da</p>
<p>Table 3 :
3
Results of cross domain on DAT.22±0.0244±7 0.22±0.0244±7 0.22±0.02D-VAT 35±7 0.19±0.0337±7 0.19±0.0334±6 0.20±0.03Ours 217±125 0.64±0.32243±114 0.76±0.26178±105 0.60±0.26
MethodCRnight T SRCRfoggy T SRCRsnow T SRAOT42±4 0.</p>
<p>Table 4 :
4
Results of ablation experiments on DAT.</p>
<p>Table 5 :
5
Performance under wind disturbances and target distractors.
CRTSRw/ Forward302 ±94 0.91 ±0.18w/ Lateral304 ±82 0.91 ±0.19w/ Yaw301 ±120 0.88 ±0.23w/ Distractor 293 ±120 0.91 ±0.15Ours316 ±840.94 ±0.14</p>
<p>Table 6 :
6
Performance under rainy conditions and unseen targets.We evaluate the model trained on citystreet-day.±110 0.74 ±0.29 139 ±109 0.45 ±0.30 274 ±103 0.77 ±0.29 Unseen Target 222 ±92 0.79 ±0.25 131 ±89 0.50 ±0.33 207 ±94 0.79 ±0.27 Ours 279 ±110 0.80 ±0.30 144 ±111 0.52 ±0.29 258 ±110 0.82 ±0.23
MethodWithin-Scene CR TSRCross-Scene CR TSRCross-Domain CR TSRw/ rain 266 Predicted ActionForwardBackwardLeftRight</p>
<p>Table 7 :
7
Effectiveness of GC-VAT on Sim2Real test.We select eight video sequences from each dataset for evaluation.
Average Correct Action RateRandom 0.4130.4260.421Ours0.7950.8330.802
[20]oVOT[30]DTB70[35]UAVDT[20]</p>
<p>Table 8 :
8
State parameters of DAT benchmark.
CategorySensorParameterTypeDescriptionPotential TasksVisionCamera LiDARImage LidarCloud vector2000 MatImages Point cloud (m)Default sensor Obstacle avoidanceGPSPosition Linearvector3 vector3Position (m) Linear velocity (m/s)Visual navigation Visual navigationMotionAccelerometerAccvector3Acceleration (m/s 2 )Visual navigationGyroscopeAngularvector3Angular velocity (rad/s) Posture stabilizationIMUAngle Orientationvector3 vector4Euler angles (rad) Quaternion representation Posture stabilization Posture stabilization</p>
<p>Table 9 :
9
[16]rd parameters of DAT benchmark.The homogeneous transformation matrices (HTM) T cw and T tw are 4×4 square matrices.Therefore, their data type double[16]corresponds to a double array of length 16.
ParameterTypeDescriptioncameraWidthdoubleimage width(px)cameraHeightdoubleimage height(px)cameraFovdoublecamera field of view(rad)cameraFdoubleestimated camera focal length(px)Tcwdouble[16]HTM of the camera relative to the world frameTtwdouble[16]HTM of the vehicle relative to the world framecameraMidGlobalPosvector3dground projection of camera center mapped in the world framecarMidGlobalPosvector3dcoordinates of the vehicle center in the world framecameraMidPosvector3dcoordinates of the camera center in the world framecarDronePosOrivector4d1D orientation + 3D position of vehicle in the drone framecrashdoublewhether tracker collides with a buildingcarDirdoublecar direction(0-stop,1-go straight,2-turn left,3-turn right)carTypenamestringtracking target type</p>
<p>Table 10 :
10
Total training steps on different scenes.During the training process, we employ a parallel training approach involving 35 agents.Consequently, the reported total training steps represent the cumulative steps taken by all agents combined.
ScenecitystreetdesertvillagedowntownlakefarmlandSteps (M)19.213.421.319.89.99.2</p>
<p>Table 11 :
11
Transition steps across different scenes.The structure of the GC-VAT is shown in Fig.11.In this figure, C8×8-16S4 represents 16 convolutional filters of size 8×8 and stride 4. GRU256 denotes a GRU network with 256 hidden units, and FC200 represents a fully connected layer with 200 neurons.
ScenecitystreetdesertvillagedowntownlakefarmlandT (M)10.06.28.010.35.64.1C.2 More DetailsNetwork Structure.</p>
<p>Table 13 :
13
The detailed results of comparison experiments on T SR metric.±0.02 0.24 ±0.03 0.22 ±0.03 0.25 ±0.02 0.23 ±0.03 0.24 ±0.01 0.25 ±0.02 0.25 ±0.02 0.24 ±0.02 D-VAT 0.26 ±0.02 0.25 ±0.04 0.25 ±0.02 0.32 ±0.08 0.27 ±0.04 0.19 ±0.01 0.26 ±0.02 0.28 ±0.02 0.29 ±0.02 GC-VAT 0.80 ±0.30 0.54 ±0.32 0.50 ±0.32 0.45 ±0.30 0.44 ±0.24 0.66 ±0.27 0.72 ±0.29 0.93 ±0.14 0.79 ±0.24 ±0.00 0.06 ±0.00 0.06 ±0.00 0.06 ±0.01 0.06 ±0.00 0.06 ±0.00 0.06 ±0.01 0.06 ±0.01 0.06 ±0.01 D-VAT 0.27 ±0.02 0.26 ±0.04 0.25 ±0.02 0.32 ±0.07 0.23 ±0.03 0.26 ±0.01 0.26 ±0.04 0.26 ±0.04 0.26 ±0.04 GC-VAT 0.73 ±0.31 0.84 ±0.29 0.87 ±0.19 0.38 ±0.32 0.56 ±0.28 0.82 ±0.25 0.57 ±0.31 0.86 ±0.28 0.86 ±0.22 ±0.03 0.25 ±0.04 0.23 ±0.03 0.24 ±0.02 0.25 ±0.02 0.26 ±0.06 0.23 ±0.03 0.23 ±0.03 0.23 ±0.03 D-VAT 0.23 ±0.04 0.23 ±0.04 0.22 ±0.05 0.31 ±0.14 0.24 ±0.06 0.22 ±0.01 0.22 ±0.04 0.22 ±0.05 0.23 ±0.05 GC-VAT 0.72 ±0.28 0.51 ±0.34 0.73 ±0.32 0.46 ±0.29 0.59 ±0.33 0.48 ±0.31 0.71 ±0.32 0.71 ±0.32 0.40 ±0.29 ±0.04 0.26 ±0.05 0.27 ±0.02 0.29 ±0.01 0.29 ±0.03 0.29 ±0.02 0.29 ±0.01 0.29 ±0.01 0.29 ±0.01 D-VAT 0.05 ±0.00 0.05 ±0.00 0.06 ±0.00 0.06 ±0.01 0.05 ±0.00 0.06 ±0.00 0.06 ±0.01 0.06 ±0.00 0.06 ±0.00 GC-VAT 0.77 ±0.31 0.65 ±0.30 0.67 ±0.29 0.65 ±0.30 0.49 ±0.29 0.63 ±0.33 0.58 ±0.31 0.65 ±0.29 0.64 ±0.28 ±0.02 0.25 ±0.03 0.23 ±0.03 0.24 ±0.02 0.24 ±0.02 0.24 ±0.01 0.24 ±0.01 0.24 ±0.02 0.24 ±0.01 D-VAT 0.25 ±0.04 0.23 ±0.04 0.23 ±0.05 0.30 ±0.15 0.26 ±0.06 0.22 ±0.01 0.26 ±0.06 0.26 ±0.06 0.25 ±0.06 GC-VAT 0.43 ±0.25 0.47 ±0.30 0.64 ±0.31 0.43 ±0.28 0.61 ±0.31 0.59 ±0.30 0.59 ±0.39 0.62 ±0.32 0.41 ±0.24 ±0.02 0.24 ±0.04 0.22 ±0.03 0.25 ±0.02 0.24 ±0.02 0.23 ±0.01 0.23 ±0.01 0.23 ±0.01 0.23 ±0.01 D-VAT 0.07 ±0.01 0.07 ±0.01 0.07 ±0.00 0.08 ±0.01 0.07 ±0.00 0.07 ±0.00 0.08 ±0.00 0.07 ±0.00 0.08 ±0.00 GC-VAT 0.48 ±0.24 0.59 ±0.34 0.72 ±0.26 0.33 ±0.20 0.58 ±0.28 0.68 ±0.32 0.67 ±0.32 0.78 ±0.22 0.51 ±0.28
Within / Cross SceneCross DomainTrain: citystreet citystreetdesertvillage downtownlakefarmlandnightfoggysnowAOT 0.25 Train: desert citystreetdesertvillage downtownlakefarmlandnightfoggysnowAOT 0.06 Train: village citystreetdesertvillage downtownlakefarmlandnightfoggysnowAOT 0.25 Train: downtown citystreetdesertvillage downtownlakefarmlandnightfoggysnowAOT 0.30 Train: lake citystreetdesertvillage downtownlakefarmlandnightfoggysnowAOT 0.25 Train: farmland citystreetdesertvillage downtownlakefarmlandnightfoggysnowAOT0.24</p>
<p>Table 14 :
14
Effectiveness of CBT strategy on the DAT benchmark, results from CR metric.±112 153 ±119 135 ±109 112 ±92 191 ±122 257 ±126 316 ±84 202 ±119 ±99 284 ±92 175 ±102 236 ±123 266 ±110 241 ±127 279 ±120 306 ±95 GC-VAT 278 ±111 307 ±124 305 ±94 119 ±110 170 ±139 275 ±121 182 ±131 307 ±124 307 ±97 ±139 239 ±134 93 ±102 153 ±115 140 ±118 257 ±122 257 ±120 114 ±115 ±136 202 ±129 203 ±119 189 ±93 223 ±114 167 ±135 165 ±126 178 ±125 ±117 183 ±110 185 ±102 102 ±57 GC-VAT 112 ±86 144 ±110 203 ±133 143 ±134 181 ±116 214 ±111 190 ±129 168 ±110 99 ±67
Within / Cross SceneCross DomainTrain: citystreet citystreetdesertvillage downtownlakefarmlandnightfoggysnoww/o CBT GC-VAT 279 ±110 129 Train: desert 54 ±7 37 ±21 citystreet desert30 ±6 village downtown 30 ±1448 ±13 lake48 ±4 farmland54 ±9 night54 ±9 foggy54 ±9 snoww/o CBT 253 ±132 302 Train: village citystreet desertvillage downtownlakefarmlandnightfoggysnoww/o CBT GC-VAT 234 ±122 160 Train: downtown citystreet 230 ±120 197 ±124 255 ±118 desert village downtown 59 ±69126 ±105 182 ±120 267 ±93 208 ±141 73 ±68 lake farmland night foggy snoww/o CBT GC-VAT 209 ±131 184 Train: lake 54 ±9 49 ±13 citystreet desert47 ±8 village downtown 57 ±1551 ±9 lake48 ±4 farmland29 ±3 night57 ±15 foggy58 ±15 snoww/o CBT 187 ±123 198 Train: farmland citystreet 124 ±90 88 ±52 191 ±108 93 ±75 desert village downtown lake farmlandnightfoggysnoww/o CBT GC-VAT52 ±9 162 ±89 170 ±125 237 ±128 47 ±9 45 ±969 ±42 81 ±7150 ±9 159 ±119 243 ±117 253 ±109 245 ±117 168 ±105 46 ±2 46 ±2 46 ±3 46 ±2</p>
<p>Table 15 :
15
Effectiveness of CBT strategy on the DAT benchmark, results from T SR metric.±0.05 0.14 ±0.10 0.20 ±0.10 0.31 ±0.15 0.28 ±0.06 0.21 ±0.01 0.30 ±0.05 0.30 ±0.05 0.30 ±0.05 GC-VAT 0.80 ±0.30 0.54 ±0.32 0.50 ±0.32 0.45 ±0.30 0.44 ±0.24 0.66 ±0.27 0.72 ±0.29 0.93 ±0.14 0.79 ±0.24 ±0.28 0.75 ±0.32 0.66 ±0.34 0.52 ±0.28 0.69 ±0.24 0.74 ±0.26 0.59 ±0.36 0.74 ±0.34 0.75 ±0.34 GC-VAT 0.73 ±0.31 0.84 ±0.29 0.87 ±0.19 0.38 ±0.32 0.56 ±0.28 0.82 ±0.25 0.57 ±0.31 0.86 ±0.28 0.86 ±0.22 ±0.28 0.62 ±0.28 0.82 ±0.16 0.23 ±0.17 0.46 ±0.25 0.58 ±0.33 0.71 ±0.28 0.69 ±0.33 0.40 ±0.24 GC-VAT 0.72 ±0.28 0.51 ±0.34 0.73 ±0.32 0.46 ±0.29 0.59 ±0.33 0.48 ±0.31 0.71 ±0.32 0.71 ±0.32 0.40 ±0.29 ±0.04 0.27 ±0.03 0.27 ±0.03 0.33 ±0.06 0.28 ±0.03 0.27 ±0.01 0.33 ±0.06 0.33 ±0.06 0.33 ±0.06 GC-VAT 0.77 ±0.31 0.65 ±0.30 0.67 ±0.29 0.65 ±0.30 0.49 ±0.29 0.63 ±0.33 0.58 ±0.31 0.65 ±0.29 0.64 ±0.28 ±0.30 0.47 ±0.29 0.45 ±0.22 0.44 ±0.23 0.57 ±0.28 0.59 ±0.26 0.78 ±0.22 0.62 ±0.24 0.33 ±0.15 GC-VAT 0.43 ±0.25 0.47 ±0.30 0.64 ±0.31 0.43 ±0.28 0.61 ±0.31 0.59 ±0.30 0.59 ±0.39 0.62 ±0.32 0.41 ±0.24 ±0.04 0.24 ±0.04 0.23 ±0.05 0.31 ±0.14 0.26 ±0.06 0.23 ±0.01 0.23 ±0.01 0.23 ±0.01 0.23 ±0.01 GC-VAT 0.48 ±0.24 0.59 ±0.34 0.72 ±0.26 0.33 ±0.20 0.58 ±0.28 0.68 ±0.32 0.67 ±0.32 0.78 ±0.22 0.51 ±0.28
Within / Cross SceneCross DomainTrain: citystreet citystreetdesertvillage downtownlakefarmlandnightfoggysnoww/o CBT 0.30 Train: desert citystreetdesertvillage downtownlakefarmlandnightfoggysnoww/o CBT 0.83 Train: village citystreetdesertvillage downtownlakefarmlandnightfoggysnoww/o CBT 0.73 Train: downtown citystreetdesertvillage downtownlakefarmlandnightfoggysnoww/o CBT 0.29 Train: lake citystreetdesertvillage downtownlakefarmlandnightfoggysnoww/o CBT 0.51 Train: farmland citystreetdesertvillage downtownlakefarmlandnightfoggysnoww/o CBT0.26</p>
<p>Table 16 :
16
Effectiveness of reward design on the DAT benchmark, results from CR metric.
Within / Cross SceneCross DomainTrain: citystreet citystreet desert village downtown lake farmland nightfoggysnowRD-VAT9±18±18±08±19±09±09±19±19±1GC-VAT279±110 129±112 153±119 135±109 112±92 191±122 257±126 316±84 202±119Train: desertcitystreet desert village downtown lake farmland nightfoggysnowRD-VAT9±19±08±19±08±010±08±110±18±0GC-VAT278±111 307±124 305±94 119±110 170±139 275±121 182±131 307±124 307±97Train: villagecitystreet desert village downtown lake farmland nightfoggysnowRD-VAT9±18±19±19±18±19±08±18±18±1GC-VAT234±122 160±139 239±134 93±102 153±115 140±118 257±122 257±120 114±115Train: downtown citystreet desert village downtown lake farmland nightfoggysnowRD-VAT8±18±08±19±18±18±19±19±19±0GC-VAT209±131 184±136 202±129 203±119 189±93 223±114 167±135 165±126 178±125Train: lakecitystreet desert village downtown lake farmland nightfoggysnowRD-VAT11±311±19±19±29±08±09±010±18±1GC-VAT112±86 144±110 203±133 143±134 181±116 214±111 190±129 168±110 99±67Train: farmland citystreet desert village downtown lake farmland nightfoggysnowRD-VAT9±18±18±19±18±19±19±09±09±0GC-VAT162±89 170±125 237±128 81±71 159±119 243±117 253±109 245±117 168±105</p>
<p>Table 17 :
17
Effectiveness of reward design on the DAT benchmark, results from T SR metric.±0.00 0.05 ±0.00 0.06 ±0.00 0.06 ±0.00 0.06 ±0.00 0.06 ±0.00 0.06 ±0.00 0.06 ±0.01 0.06 ±0.00 GC-VAT 0.80 ±0.30 0.54 ±0.32 0.50 ±0.32 0.45 ±0.30 0.44 ±0.24 0.66 ±0.27 0.72 ±0.29 0.93 ±0.14 0.79 ±0.24 ±0.31 0.84 ±0.29 0.87 ±0.19 0.38 ±0.32 0.56 ±0.28 0.82 ±0.25 0.57 ±0.31 0.86 ±0.28 0.86 ±0.22 ±0.28 0.51 ±0.34 0.73 ±0.32 0.46 ±0.29 0.59 ±0.33 0.48 ±0.31 0.71 ±0.32 0.71 ±0.32 0.40 ±0.29 ±0.01 0.06 ±0.00 0.06 ±0.00 0.06 ±0.00 0.05 ±0.00 0.06 ±0.00 0.06 ±0.00 0.06 ±0.00 0.06 ±0.00 GC-VAT 0.77 ±0.31 0.65 ±0.30 0.67 ±0.29 0.65 ±0.30 0.49 ±0.29 0.63 ±0.33 0.58 ±0.31 0.65 ±0.29 0.64 ±0.28 ±0.01 0.09 ±0.01 0.07 ±0.00 0.06 ±0.01 0.06 ±0.00 0.06 ±0.00 0.06 ±0.00 0.08 ±0.00 0.06 ±0.00 GC-VAT 0.43 ±0.25 0.47 ±0.30 0.64 ±0.31 0.43 ±0.28 0.61 ±0.31 0.59 ±0.30 0.59 ±0.39 0.62 ±0.32 0.41 ±0.24 ±0.00 0.05 ±0.00 0.05 ±0.00 0.06 ±0.01 0.05 ±0.00 0.06 ±0.00 0.06 ±0.00 0.06 ±0.00 0.06 ±0.00 GC-VAT 0.48 ±0.24 0.59 ±0.34 0.72 ±0.26 0.33 ±0.20 0.58 ±0.28 0.68 ±0.32 0.67 ±0.32 0.78 ±0.22 0.51 ±0.28
Within / Cross SceneCross DomainTrain: citystreet citystreetdesertvillage downtownlakefarmlandnightfoggysnowRD-VAT 0.06 Train: desert citystreetdesertvillage downtownlakefarmlandnightfoggysnowRD-VAT0.06 ±0.00 0.06 ±0.00 0.06 ±0.00 0.06 ±0.01 0.06 ±0.00 0.10 ±0.00 0.06 ±0.00 0.09 ±0.01 0.06 ±0.00GC-VAT 0.73 Train: village citystreetdesertvillage downtownlakefarmlandnightfoggysnowRD-VAT0.06 ±0.01 0.06 ±0.00 0.06 ±0.00 0.06 ±0.01 0.05 ±0.00 0.06 ±0.00 0.05 ±0.00 0.06 ±0.00 0.06 ±0.00GC-VAT 0.72 Train: downtown citystreetdesertvillage downtownlakefarmlandnightfoggysnowRD-VAT 0.06 Train: lake citystreetdesertvillage downtownlakefarmlandnightfoggysnowRD-VAT 0.10 Train: farmland citystreetdesertvillage downtownlakefarmlandnightfoggysnowRD-VAT0.06</p>
<p>The training involves a range of 9.2M to 21.3M steps across 35 parallel environments.The webots runs at 500Hz, with the algorithm updating every four steps (125Hz).Episodes last up to 1500 steps and were terminated early if the drone lost the target for over 100 consecutive steps, collided, or crashed.The drone translation speed is set to 40m/s, and rotational speed to 2rad/s.The map features 40 vehicles, each with a maximum speed of 20m/s and acceleration of ±25m/s 2 .During testing, the altitude is set to 22m, the pitch angle to 1.37rad, and the target initializes at the camera's center.The drone's translation speed is set to a higher value to prevent it from becoming too similar to the target's speed (with a maximum of 20 m/s).This prevents simple forward movement from yielding excessively high reward evaluations.If the drone's speed is set lower (e.g., 20 m/s), it may adopt a suboptimal strategy, relying solely on one action.Due to the varying challenges posed by different scene maps, the convergence speed of the agent differs across experiments.The training steps are shown in Table
E More ExperimentsE.1 Experiment SettingsMore Implementation Details.</p>
<p>Table 18 :
18
Results (metric is Correct Action Rate) of 8 videos in VOT benchmark.
Videocar1car3car6car8Random 0.418 0.4340.418 0.430Ours0.696 0.8450.7540.833Videocarchase car16 following car9Random 0.429 0.4210.314 0.439Ours0.870 0.8340.7730.756</p>
<p>Table 19 :
19
Results of 8 videos in DTB70 benchmark.
VideoCar2 Car4Car5RcCar4Random 0.419 0.4210.4290.436Ours0.757 0.8940.8930.876VideoCar8 RaceCar RaceCar1 RcCar3Random 0.411 0.4620.4300.400Ours0.803 0.7130.8800.851</p>
<p>Effectiveness of Curriculum-Based Training strategy.To validate the effectiveness of the proposed Curriculum-Based Training (CBT) strategy, we conduct ablation experiments by removing the CBT module.The results for the CR and T SR metrics are presented in Table 14 and Table 15, respectively.</p>
<p>Table 20 :
20
[20]lts of 8 videos in UAVDT[20]benchmark.
VideoS1603 S0201 S0101 S0306Random 0.435 0.438 0.422 0.437Ours0.8960.8060.8650.773VideoS1201 S0303 S1301 S1701Random 0.445 0.385 0.397 0.407Ours0.8670.7350.7600.713
AcknowledgementsThis work was partially supported by the Joint Funds of the National Natural Science Foundation of China (Grant No.U24A20327).Justification: See Section 1 for details.Guidelines:• The answer NA means that the abstract and introduction do not include the claims made in the paper.• The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations.A No or NA answer to this question will not be perceived well by the reviewers.• The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.• It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.LimitationsQuestion: Does the paper discuss the limitations of the work performed by the authors?Answer: [Yes]Justification: See Appendix F for limitations.Guidelines:• The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.• The authors are encouraged to create a separate "Limitations" section in their paper.• The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally).The authors should reflect on how these assumptions might be violated in practice and what the implications would be.• The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs.In general, empirical results often depend on implicit assumptions, which should be articulated.• The authors should reflect on the factors that influence the performance of the approach.For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting.Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.• The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.• If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.• While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper.The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community.Reviewers will be specifically instructed to not penalize honesty concerning limitations.Theory assumptions and proofsQuestion: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer:[Yes]Justification: See Section 4.3 and Section C.1 for complete theoretical proof.Guidelines:• The answer NA means that the paper does not include theoretical results.• All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.• All assumptions should be clearly stated or referenced in the statement of any theorems.• The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.• Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.• Theorems and Lemmas that the proof relies upon should be properly referenced.Experimental result reproducibilityQuestion: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?Answer: [Yes] Justification: See Section 5 and Appendix C.2 for all information needed.Guidelines:• The answer NA means that the paper does not include experiments.• If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.• If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.• Depending on the contribution, reproducibility can be accomplished in various ways.For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model.In general.releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.• While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution., with an open-source dataset or instructions for how to construct the dataset).(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility.In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.Guidelines:• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.• If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.• The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).10.Broader impacts Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?Answer: [Yes] Justification: See Section 6 for potential impacts of our paper.Guidelines:• The answer NA means that there is no societal impact of the work performed.• If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.• Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.• The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments.However, if there is a direct path to any negative applications, the authors should point it out.For example, it is legitimate to point out that an improvement in the quality of generative models could be used toAnswer: [Yes]Justification: We provide a well-organized documentation in anonymous homepage.Guidelines:• The answer NA means that the paper does not release new assets.• Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates.This includes details about training, license, limitations, etc. • The paper should discuss whether and how consent was obtained from people whose asset is used.Guidelines:• The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.• Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.• According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.Institutional review board (IRB) approvals or equivalent for research with human subjectsQuestion: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?Answer: [NA] Justification: Our paper does not involve crowdsourcing nor research with human subjects.Guidelines:• The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.• Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research.If you obtained IRB approval, you should clearly state this in the paper.• We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.• For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.Declaration of LLM usageQuestion: Does the paper describe the usage of LLMs if it is an important, original, or non-standard component of the core methods in this research?Note that if the LLM is used only for writing, editing, or formatting purposes and does not impact the core methodology, scientific rigorousness, or originality of the research, declaration is not required.Answer: [NA]Justification: The core method development in our paper does not involve LLMs as any important, original, or non-standard components.Guidelines:• The answer NA means that the core method development in this research does not involve LLMs as any important, original, or non-standard components.• Please refer to our LLM policy (https://neurips.cc/Conferences/2025/LLM)for what should or should not be described.
2025.10.09Openstreetmap. </p>
<p>Coppelia Robotics AG. Coppeliasim (formerly v-rep. </p>
<p>Visual tracking with online multiple instance learning. Boris Babenko, Ming-Hsuan Yang, Serge Belongie, 2009 IEEE Conference on computer vision and Pattern Recognition. IEEE2009</p>
<p>Fully-convolutional siamese networks for object tracking. Luca Bertinetto, Jack Valmadre, Joao F Henriques, Andrea Vedaldi, Philip Hs, Torr , Computer Vision-ECCV 2016 Workshops. Amsterdam, The NetherlandsSpringerOctober 8-10 and 15-16, 2016. 2016Proceedings, Part II 14</p>
<p>Simple online and realtime tracking. Alex Bewley, Zongyuan Ge, Lionel Ott, Fabio Ramos, Ben Upcroft, 2016 IEEE international conference on image processing (ICIP). IEEE2016</p>
<p>Iemask r-cnn: Information-enhanced mask r-cnn. Xiuli Bi, Jinwu Hu, Bin Xiao, Weisheng Li, Xinbo Gao, IEEE Transactions on Big Data. 922022</p>
<p>Homogeneous transformation matrix. Sébastien Briot, Wisama Khalil, Sébastien Briot, Wisama Khalil, Dynamics of Parallel Robots: From Rigid Bodies to Flexible Elements. 2015</p>
<p>Introduction to optics. Germain Chartier, 2005Springer1</p>
<p>Seqtrack: Sequence to sequence learning for visual object tracking. Xin Chen, Houwen Peng, Dong Wang, Huchuan Lu, Han Hu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)June 2023</p>
<p>Soft actor-critic for discrete action settings. Petros Christodoulou, arXiv:1910.072072019arXiv preprint</p>
<p>Empirical evaluation of gated recurrent neural networks on sequence modeling. Junyoung Chung, Caglar Gulcehre, Kyunghyun Cho, Yoshua Bengio, NIPS 2014 Workshop on Deep Learning. December 2014. 2014</p>
<p>Dji mobile sdk. Da-Jiang Innovations, 2025</p>
<p>Da-Jiang Innovations, Support for dji mini 3 pro. 2025. 2025.10.09</p>
<p>Da-Jiang Innovations, Support for matrice 100. 2025. 2025.10.09</p>
<p>Stable and consistent object tracking: An active vision approach. Dibyendu Kumar Das, Mouli Laha, Somajyoti Majumder, Dipnarayan Ray, Advanced Computational and Communication Paradigms: Proceedings of International Conference on ICACCP 2017. Springer20182</p>
<p>Tao: A large-scale benchmark for tracking any object. Achal Dave, Tarasha Khurana, Pavel Tokmakov, Cordelia Schmid, Deva Ramanan, Computer Vision -ECCV 2020. Andrea Vedaldi, Horst Bischof, Thomas Brox, Jan-Michael Frahm, ChamSpringer International Publishing2020</p>
<p>Mot20: A benchmark for multi object tracking in crowded scenes. Patrick Dendorfer, Hamid Rezatofighi, Anton Milan, Javen Qinfeng Shi, Daniel Cremers, Ian D Reid, Stefan Roth, Konrad Schindler, Laura Leal-Taix'e, ArXiv, abs/2003.090032020</p>
<p>Enhancing continuous control of mobile robots for end-to-end visual active tracking. Alessandro Devo, Alberto Dionigi, Gabriele Costante, Robotics and Autonomous Systems. 1421037992021</p>
<p>D-vat: End-to-end visual active tracking for micro aerial vehicles. Alberto Dionigi, Simone Felicioni, Mirko Leomanni, Gabriele Costante, IEEE Robotics and Automation Letters. 2024</p>
<p>The unmanned aerial vehicle benchmark: Object detection and tracking. Dawei Du, Yuankai Qi, Hongyang Yu, Yifan Yang, Kaiwen Duan, Guorong Li, Weigang Zhang, Qingming Huang, Qi Tian, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)September 2018</p>
<p>Visdrone-sot2019: The vision meets drone single object tracking challenge results. Dawei Du, Pengfei Zhu, Longyin Wen, Xiao Bian, Haibin Ling, Qinghua Hu, Jiayu Zheng, Tao Peng, Xinyao Wang, Yue Zhang, Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops. the IEEE/CVF International Conference on Computer Vision Workshops2019</p>
<p>A review of quadrotor: An underactuated mechanical system. J Bara, Homayoun Emran, Najjaran, Annual Reviews in Control. 462018</p>
<p>Lasot: A high-quality benchmark for large-scale single object tracking. Liting Heng Fan, Fan Lin, Peng Yang, Ge Chu, Sijia Deng, Hexin Yu, Yong Bai, Chunyuan Xu, Haibin Liao, Ling, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2019</p>
<p>Epic Games. Unreal engine 4. </p>
<p>Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, Sergey Levine, International conference on machine learning. PMLR2018</p>
<p>Efficient dynamic ensembling for multiple LLM experts. Jinwu Hu, Yufeng Wang, Shuhai Zhang, Kai Zhou, Guohao Chen, Yu Hu, Bin Xiao, Mingkui Tan, Proceedings of the Thirty-Fourth International Joint Conference on Artificial Intelligence, IJCAI 2025. the Thirty-Fourth International Joint Conference on Artificial Intelligence, IJCAI 2025Montreal, CanadaAugust 16-22, 2025. 2025</p>
<p>Dcfnet: Discriminant correlation filters network for visual tracking. Wei-Ming Hu, Qiang Wang, Jin Gao, Bing Li, Stephen Maybank, Journal of Computer Science and Technology. 3932024</p>
<p>Anti-uav410: A thermal infrared benchmark and customized scheme for tracking drones in the wild. Bo Huang, Jianan Li, Junjie Chen, Gang Wang, Jian Zhao, Tingfa Xu, IEEE Transactions on Pattern Analysis and Machine Intelligence. 2023</p>
<p>A novel performance evaluation methodology for single-target trackers. Matej Kristan, Jiri Matas, Aleš Leonardis, Tomas Vojir, Roman Pflugfelder, Gustavo Fernandez, Georg Nebehay, Fatih Porikli, Luka Čehovin, IEEE Transactions on Pattern Analysis and Machine Intelligence. 3811Nov 2016</p>
<p>The hungarian method for the assignment problem. Harold W Kuhn, Naval research logistics quarterly. 21-21955</p>
<p>Siamrpn++: Evolution of siamese visual tracking with very deep networks. Bo Li, Wei Wu, Qiang Wang, Fangyi Zhang, Junliang Xing, Junjie Yan, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2019</p>
<p>High performance visual tracking with siamese region proposal network. Bo Li, Junjie Yan, Wei Wu, Zheng Zhu, Xiaolin Hu, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2018</p>
<p>Heavy rain image restoration: Integrating physics model and conditional adversarial learning. Ruoteng Li, Loong-Fah Cheong, Robby T Tan, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2019</p>
<p>Visual object tracking for unmanned aerial vehicles: A benchmark and new motion models. Siyi Li, Dit-Yan Yeung, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial IntelligenceFeb. 201731</p>
<p>Microscopic traffic simulation using sumo. Pablo Alvarez Lopez, Michael Behrisch, Laura Bieker-Walz, Jakob Erdmann, Yun-Pang Flötteröd, Robert Hilbrich, Leonhard Lücken, Johannes Rummel, Peter Wagner, Evamarie Wießner, 2018 21st international conference on intelligent transportation systems (ITSC). IEEE2018</p>
<p>Cyberbotics Ltd, Webots, Open source mobile robot simulation software. </p>
<p>Accelerating reinforcement learning for reaching using continuous curriculum learning. Sha Luo, Hamidreza Kasaei, Lambert Schomaker, 2020 International Joint Conference on Neural Networks (IJCNN). IEEE2020</p>
<p>End-to-end active object tracking and its real-world deployment via reinforcement learning. Wenhan Luo, Peng Sun, Fangwei Zhong, Wei Liu, Tong Zhang, Yizhou Wang, IEEE transactions on pattern analysis and machine intelligence. 201942</p>
<p>Follow anything: Open-set detection, tracking, and following in real-time. Alaa Maalouf, Ninad Jadhav, Krishna Murthy Jatavallabhula, Makram Chahine, M Daniel, Robert J Vogt, Antonio Wood, Daniela Torralba, Rus, IEEE Robotics and Automation Letters. 942024</p>
<p>Learning robust perceptive locomotion for quadrupedal robots in the wild. Takahiro Miki, Joonho Lee, Jemin Hwangbo, Vladlen Lorenz Wellhausen, Marco Koltun, Hutter, Science robotics. 76228222022</p>
<p>Asynchronous methods for deep reinforcement learning. Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, Koray Kavukcuoglu, Proceedings of The 33rd International Conference on Machine Learning. The 33rd International Conference on Machine LearningPMLR201648</p>
<p>A benchmark and simulator for uav tracking. Matthias Mueller, Neil Smith, Bernard Ghanem, Computer Vision-ECCV 2016: 14th European Conference. Amsterdam, The NetherlandsSpringerOctober 11-14, 2016. 2016Proceedings, Part I 14</p>
<p>Context-aware correlation filter tracking. Matthias Mueller, Neil Smith, Bernard Ghanem, 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2017</p>
<p>Curriculum-reinforcement learning on simulation platform of tendon-driven high-degree of freedom underactuated manipulator. Keung Or, Kehua Wu, Kazashi Nakano, Masahiro Ikeda, Mitsuhito Ando, Yasuo Kuniyoshi, Ryuma Niiyama, Frontiers in Robotics and AI. 1010665182023</p>
<p>Fast-tracker 2.0: Improving autonomy of aerial tracking with active vision and human location regression. Neng Pan, Ruibin Zhang, Tiankai Yang, Can Cui, Chao Xu, Fei Gao, IET Cyber-Systems and Robotics. 342021</p>
<p>Pets 2016: Dataset and challenge. Luis Patino, Tom Cane, Alain Vallee, James Ferryman, 2016 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW). 2016</p>
<p>Chained-tracker: Chaining paired attentive regression results for end-to-end joint multiple-object detection and tracking. Jinlong Peng, Changan Wang, Fangbin Wan, Yang Wu, Yabiao Wang, Ying Tai, Chengjie Wang, Jilin Li, Feiyue Huang, Yanwei Fu, Computer Vision-ECCV 2020: 16th European Conference. Glasgow, UKSpringerAugust 23-28, 2020. 2020Proceedings, Part IV 16</p>
<p>Stable-baselines3: Reliable reinforcement learning implementations. Antonin Raffin, Ashley Hill, Adam Gleave, Anssi Kanervisto, Maximilian Ernestus, Noah Dormann, Journal of Machine Learning Research. 222682021</p>
<p>You only look once: Unified, real-time object detection. Redmon, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2016</p>
<p>Deep reinforcement learning with iterative shift for visual tracking. Liangliang Ren, Xin Yuan, Jiwen Lu, Ming Yang, Jie Zhou, Proceedings of the European conference on computer vision (ECCV). the European conference on computer vision (ECCV)2018</p>
<p>Faster r-cnn: Towards real-time object detection with region proposal networks. Kaiming Shaoqing Ren, Ross He, Jian Girshick, Sun, IEEE transactions on pattern analysis and machine intelligence. 201639</p>
<p>Learning to walk in minutes using massively parallel deep reinforcement learning. Nikita Rudin, David Hoeller, Philipp Reist, Marco Hutter, Conference on Robot Learning. PMLR2022</p>
<p>An autonomous drone for search and rescue in forests using airborne optical sectioning. David C Schedl, Indrajit Kurmi, Oliver Bimber, Science Robotics. 62021</p>
<p>John Schulman, arXiv:1707.06347Proximal policy optimization algorithms. 2017arXiv preprint</p>
<p>High-dimensional continuous control using generalized advantage estimation. John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, Pieter Abbeel, arXiv:1506.024382015arXiv preprint</p>
<p>Airsim: High-fidelity visual and physical simulation for autonomous vehicles. Shital Shah, Debadeepta Dey, Chris Lovett, Ashish Kapoor, Field and Service Robotics. 2017</p>
<p>Visual tracking: An experimental survey. W M Arnold, Dung M Smeulders, Rita Chu, Simone Cucchiara, Afshin Calderara, Mubarak Dehghan, Shah, IEEE transactions on pattern analysis and machine intelligence. 201336</p>
<p>Online decision based visual tracking via reinforcement learning. Ke Song, Wei Zhang, Ran Song, Yibin Li, Neural Information Processing Systems. 2020</p>
<p>Gymnasium: A standard interface for reinforcement learning environments. Mark Towers, Ariel Kwiatkowski, Jordan Terry, John U Balis, Gianluca De Cola, Tristan Deleu, Manuel Goulão, Andreas Kallinteris, Markus Krimmel, K G Arjun, arXiv:2407.170322024arXiv preprint</p>
<p>Learning task-independent joint control for robotic manipulators with reinforcement learning and curriculum learning. Lars Vaehrens, Daniel Díez Álvarez, Ulrich Berger, Simon Bøgh, 2022 21st IEEE International Conference on Machine Learning and Applications (ICMLA). IEEE2022</p>
<p>Fast online object tracking and segmentation: A unifying approach. Qiang Wang, Li Zhang, Luca Bertinetto, Weiming Hu, Philip Hs, Torr , Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition. the IEEE/CVF conference on Computer Vision and Pattern Recognition2019</p>
<p>A survey on curriculum learning. Xin Wang, Yudong Chen, Wenwu Zhu, IEEE transactions on pattern analysis and machine intelligence. 202144</p>
<p>Enhancing user-oriented proactivity in open-domain dialogues with critic guidance. Yufeng Wang, Jinwu Hu, Ziteng Huang, Kunyang Lin, Zitian Zhang, Peihao Chen, Yu Hu, Qianyue Wang, Zhuliang Yu, Bin Sun, Xiaofen Xing, Qingfang Zheng, Mingkui Tan, Proceedings of the Thirty-Fourth International Joint Conference on Artificial Intelligence, IJCAI 2025. the Thirty-Fourth International Joint Conference on Artificial Intelligence, IJCAI 2025Montreal, CanadaAugust 16-22, 2025. 2025</p>
<p>Efficienttrain++: Generalized curriculum learning for efficient visual backbone training. Yulin Wang, Yang Yue, Rui Lu, Yizeng Han, Shiji Song, Gao Huang, IEEE Transactions on Pattern Analysis and Machine Intelligence. 2024</p>
<p>Towards real-time multi-object tracking. Zhongdao Wang, Liang Zheng, Yixuan Liu, Yali Li, Shengjin Wang, European conference on computer vision. Springer2020</p>
<p>Detection, tracking, and counting meets drones in crowds: A benchmark. Longyin Wen, Dawei Du, Pengfei Zhu, Qinghua Hu, Qilong Wang, Liefeng Bo, Siwei Lyu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2021</p>
<p>Tianshou: A highly modularized deep reinforcement learning library. Jiayi Weng, Huayu Chen, Dong Yan, Kaichao You, Alexis Duburcq, Minghao Zhang, Yi Su, Hang Su, Jun Zhu, Journal of Machine Learning Research. 232672022</p>
<p>Simple online and realtime tracking with a deep association metric. Nicolai Wojke, Alex Bewley, Dietrich Paulus, 2017 IEEE international conference on image processing (ICIP). IEEE2017</p>
<p>Simple online and realtime tracking with a deep association metric. Nicolai Wojke, Alex Bewley, Dietrich Paulus, 2017 IEEE international conference on image processing (ICIP). IEEE2017</p>
<p>Object tracking benchmark. Yi Wu, Jongwoo Lim, Ming-Hsuan Yang, IEEE Transactions on Pattern Analysis and Machine Intelligence. 3792015</p>
<p>Cirnet: An improved rgbt tracking via cross-modality interaction and re-identification. Weidai Xia, Dongming Zhou, Jinde Cao, Yanyu Liu, Ruichao Hou, Neurocomputing. 4932022</p>
<p>Multi-uav cooperative system for search and rescue based on yolov5. Linjie Xing, Xiaoyan Fan, Yaxin Dong, Zenghui Xiong, Lin Xing, Yang Yang, Haicheng Bai, Chengjiang Zhou, International Journal of Disaster Risk Reduction. 761029722022</p>
<p>Joint feature learning and relation modeling for tracking: A one-stream framework. Botao Ye, Hong Chang, Bingpeng Ma, Shiguang Shan, Xilin Chen, European Conference on Computer Vision. Springer2022</p>
<p>Active learning for deep visual tracking. Di Yuan, Xiaojun Chang, Qiao Liu, Yi Yang, Dehua Wang, Minglei Shu, Zhenyu He, Guangming Shi, IEEE Transactions on Neural Networks and Learning Systems. 2023</p>
<p>Action-decision networks for visual tracking with deep reinforcement learning. Sangdoo Yun, Jongwon Choi, Youngjoon Yoo, Kimin Yun, Jin Young Choi, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2017</p>
<p>A novel uav path planning approach: Heuristic crossing search and rescue optimization algorithm. Chaoqun Zhang, Wenjuan Zhou, Weidong Qin, Weidong Tang, Expert Systems with Applications. 2151192432023</p>
<p>Da Zhang, Hamid Maei, Xin Wang, Yuan-Fang Wang, arXiv:1701.08936Deep reinforcement learning for visual object tracking in videos. 2017arXiv preprint</p>
<p>Fairmot: On the fairness of detection and re-identification in multiple object tracking. Yifu Zhang, Chunyu Wang, Xinggang Wang, Wenjun Zeng, Wenyu Liu, International journal of computer vision. 1292021</p>
<p>Ad-vat+: An asymmetric dueling mechanism for learning and understanding visual active tracking. Fangwei Zhong, Peng Sun, Wenhan Luo, Tingyun Yan, Yizhou Wang, IEEE transactions on pattern analysis and machine intelligence. 201943</p>
<p>Empowering embodied visual tracking with visual foundation models and offline rl. Fangwei Zhong, Kui Wu, Hai Ci, Churan Wang, Hao Chen, European Conference on Computer Vision. Springer2024</p>
<p>Space noncooperative object active tracking with deep reinforcement learning. Dong Zhou, Guanghui Sun, Wenxiao Lei, Ligang Wu, IEEE Transactions on Aerospace and Electronic Systems. 5862022</p>
<p>Boosting pseudolabeling with curriculum self-reflection for attributed graph clustering. Pengfei Zhu, Jialu Li, Yu Wang, Bin Xiao, Jinglin Zhang, Wanyu Lin, Qinghua Hu, IEEE Transactions on Neural Networks and Learning Systems. 2024</p>
<p>Detection and tracking meet drones challenge. Pengfei Zhu, Longyin Wen, Dawei Du, Xiao Bian, Qinghua Heng Fan, Haibin Hu, Ling, IEEE Transactions on Pattern Analysis and Machine Intelligence. 44112022</p>            </div>
        </div>

    </div>
</body>
</html>
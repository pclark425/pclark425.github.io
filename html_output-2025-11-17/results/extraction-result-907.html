<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-907 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-907</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-907</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-22.html">extraction-schema-22</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <p><strong>Paper ID:</strong> paper-0479547a69ffa48fe106137eba8358a9d8a2cc48</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/0479547a69ffa48fe106137eba8358a9d8a2cc48" target="_blank">Tool Learning with Large Language Models: A Survey</a></p>
                <p><strong>Paper Venue:</strong> Frontiers Comput. Sci.</p>
                <p><strong>Paper TL;DR:</strong> This survey focuses on reviewing existing literature from the two primary aspects (1) why tool learning is beneficial and (2) how tool learning is implemented, enabling a comprehensive understanding of tool learning with LLMs.</p>
                <p><strong>Paper Abstract:</strong> Recently, tool learning with large language models (LLMs) has emerged as a promising paradigm for augmenting the capabilities of LLMs to tackle highly complex problems. Despite growing attention and rapid advancements in this field, the existing literature remains fragmented and lacks systematic organization, posing barriers to entry for newcomers. This gap motivates us to conduct a comprehensive survey of existing works on tool learning with LLMs. In this survey, we focus on reviewing existing literature from the two primary aspects (1) why tool learning is beneficial and (2) how tool learning is implemented, enabling a comprehensive understanding of tool learning with LLMs. We first explore the"why"by reviewing both the benefits of tool integration and the inherent benefits of the tool learning paradigm from six specific aspects. In terms of"how", we systematically review the literature according to a taxonomy of four key stages in the tool learning workflow: task planning, tool selection, tool calling, and response generation. Additionally, we provide a detailed summary of existing benchmarks and evaluation methods, categorizing them according to their relevance to different stages. Finally, we discuss current challenges and outline potential future directions, aiming to inspire both researchers and industrial developers to further explore this emerging and promising area. We also maintain a GitHub repository to continually keep track of the relevant papers and resources in this rising area at https://github.com/quchangle1/LLM-Tool-Survey.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e907.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e907.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM QA vs Interactive Gap (survey-level)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Performance gap between LLM question-answering and interactive/procedural tasks (survey-level observation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The survey repeatedly notes that large pretrained LMs excel at standard QA/NLP tasks but often underperform on interactive or procedural tasks (tool use, multi-step planning, sequential decision-making) due to limited parametric knowledge, lack of execution interfaces, sensitivity to prompts, and inability to incorporate dynamic tool feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Generic large language models (e.g., GPT-family, LLaMA-family)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large pre-trained transformer LMs with strong zero-/few-shot QA and generation capabilities but limited native mechanisms for executing external actions or iterative tool-driven plans.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>tool use / multi-step reasoning / planning / sequential decision-making</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>LLMs rely on fixed parametric knowledge and pattern matching from pretraining, lack execution interfaces and iterative feedback loops, are sensitive to prompt perturbations, and have limited context/utility for handling external tool outputs, causing poorer performance on interactive/procedural tasks versus static QA.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Tool Learning with Large Language Models: A Survey', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e907.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e907.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (plugins)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 with plugin/tool integration</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Survey cites GPT-4 as an example of a powerful LLM that mitigates knowledge and capability limits by calling external plugins/tools and integrating returned results into final responses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>GPT-4 (with plugin API calls)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source large transformer model (GPT-4) augmented with a plugin interface that allows the model to call external APIs and incorporate their outputs into responses.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>Plugin/API-based question answering and tool-augmented tasks</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>tool use / API calling</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>tool-use interface (plugin architecture), ability to integrate external responses</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>architectural change (plugin/tool integration)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Expose a plugin API so the LLM can request external data or actions at inference time and integrate the tool outputs into final answers.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Qualitative improvement in accuracy and timeliness of responses by augmenting parametric knowledge with tool outputs (no numeric metrics provided in survey).</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Parametric knowledge is outdated/limited; plugin integration provides dynamic, up-to-date, and execution-capable information that QA-only LLMs lack.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Tool Learning with Large Language Models: A Survey', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e907.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e907.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Toolformer (GPT-J finetune)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Toolformer (fine-tuning a base LM with API-call tokens)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Toolformer fine-tunes a base LM (reported in prior work on GPT-J) by inserting API-call tokens into training so the model learns when and how to call external tools, improving tool-awareness and usage.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Toolformer-finetuned GPT-J</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A base LM (GPT-J) fine-tuned to predict API-call tokens and the surrounding context so the model learns to invoke tools during generation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>6B</td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>Tool invocation and tool-augmented QA/tasks</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>tool use / tool calling</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>no structural architecture change to transformer backbone; training augments token vocabulary with API-call tokens</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>supervised fine-tuning on augmented token sequences (tool-call tokens)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>training method (fine-tuning with tool-call supervision)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Fine-tune the LM on corpora where API-call tokens are inserted where tool calls are useful, teaching the model to predict and format tool calls as part of generation.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Reported qualitative improvement in tool-awareness and tool-usage capabilities (survey reports improvement but does not provide numeric before/after metrics).</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Pretrained LMs do not receive explicit supervision for tool invocation; adding token-level supervision teaches the model execution affordances it lacked from pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Tool Learning with Large Language Models: A Survey', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e907.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e907.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ToolLLaMA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ToolLLaMA (fine-tuning LLaMA for tool usage)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>ToolLLaMA fine-tunes the LLaMA 7B model using instruction-solution pairs (from DFSDT) to improve planning and API calling skills for tool-augmented tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>ToolLLaMA (LLaMA-7B fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLaMA 7B base model fine-tuned on datasets of instruction-to-solution pairs specialized for tool selection, parameter extraction, and API calling.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>Tool selection, API calling, and multi-step tool-augmented problem solving</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>tool use / planning / multi-step reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>supervised fine-tuning (instruction-solution pairs)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>training method (fine-tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Fine-tune an open-source LLM with datasets generated by DFSDT to teach correct planning and API invocation behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Improved planning and API-calling capabilities relative to unfine-tuned baselines (no numeric metrics reported in survey).</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Lack of targeted instructive examples for tool use in pretraining; fine-tuning injects this behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Tool Learning with Large Language Models: A Survey', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e907.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e907.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TRICE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TRICE (two-stage behavior cloning + RLEF)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>TRICE uses a two-stage pipeline: behavior cloning (instruct-tuning) to imitate tool-use demonstrations, followed by reinforcement learning from environment feedback (RLEF) using tool execution results to further improve performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Model trained with TRICE pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An LLM first trained via supervised imitation of tool-usage traces, then further optimized using reinforcement learning leveraging tool-execution feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>Tool-augmented decision making and API calling</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>tool use / sequential decision-making / planning</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>behavior cloning (supervised), then reinforcement learning from environment feedback (RLEF)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>training method (hybrid: supervised + RL with tool feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Stage 1: instruct-tune the model to mimic tool usage; Stage 2: apply RL using actual tool execution feedback to refine policies and improve robustness to tool errors.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Survey reports this pipeline as improving tool-usage and robustness (no numeric metrics provided).</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Supervised imitation alone does not account for stochastic tool outputs; RL with real execution feedback helps models adapt and correct decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Tool Learning with Large Language Models: A Survey', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e907.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e907.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DFSDT / Depth First Search for decision-making</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DFSDT (Depth-First Search based decision strategy for tool selection)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>DFSDT introduces a depth-first search strategy to mitigate error propagation in sequential tool selection decisions, improving decision-making accuracy in multi-step tool use.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>LLM + DFSDT planning strategy</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An LLM guided by a depth-first search over possible decision branches to explore alternative tool-usage sequences and reduce cascading errors.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>Tool selection and multi-step tool-usage planning</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>planning / multi-step reasoning / tool use</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>search-guided planner (DFS) wrapped around LLM decision-making</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>algorithmic/planning intervention (search over decision branches)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Use DFS to enumerate and evaluate multiple decision branches for tool selection, reducing reliance on a single greedy plan and limiting error propagation.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Reported to improve decision accuracy and robustness in tool selection tasks; no numeric values provided in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Greedy one-shot planning amplifies single-step errors; search-based planning mitigates propagation by exploring alternatives.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Tool Learning with Large Language Models: A Survey', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e907.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e907.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReAct / Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReAct and Chain-of-Thought (prompting-based reasoning + action)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompting strategies that combine explicit chain-of-thought reasoning with actions (tool calls) â€” ReAct integrates reasoning traces with environment actions, while Chain-of-Thought encourages step-by-step reasoning to improve planning and multi-step performance without model finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>LLMs using ReAct / Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prompting techniques applied at inference time to induce stepwise reasoning and interleave reasoning with actions/tool invocations, leveraging in-context learning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>Multi-step reasoning and tool-augmented tasks</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>multi-step reasoning / tool use / planning</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>no architectural change; relies on prompt engineering to elicit internal reasoning traces</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>prompting only / in-context learning</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>prompting strategy (tuning-free)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Use few-shot or zero-shot prompts (e.g., 'let's think step by step') and ReAct-style templates to interleave natural language reasoning and tool actions to improve decision-making during interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Improves planning and tool-selection behavior in many cases (survey notes empirical benefit but does not list standardized metrics); however, still often underperforms specialized finetuned models.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Prompt-induced reasoning helps but cannot fully compensate for lack of explicit execution and feedback-aware training.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Tool Learning with Large Language Models: A Survey', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e907.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e907.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ToolBench findings (open-source LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ToolBench / ToolBench1/2 analyses on open-source LLM tool learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The survey summarizes that ToolBench-style evaluations find open-source LLMs often struggle at tool learning tasks, and that interventions like fine-tuning, demonstration retrieval, and system prompts substantially improve performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Open-source LLMs evaluated in ToolBench analyses</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Various open-source base LMs evaluated for tool selection, parameter extraction, and API calling; baseline (unfine-tuned) models exhibit weaknesses on interactive tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>Benchmarks spanning tool selection, tool calling, response generation (e.g., ToolBench1/2, API-Bank, API-BLEND)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>tool use / tool calling / tool selection / multi-step reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>training method + prompting + retrieval/demonstration augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>ToolBench analysis recommends supervised fine-tuning on tool-use datasets, retrieval of demonstrations, and system prompt engineering to mitigate poor tool-use performance of open LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Survey reports these interventions significantly enhance tool-learning effectiveness for open-source models (no standardized numeric metrics provided in survey).</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Open LMs lack specialized training data and in-context examples for tool usage; adding targeted fine-tuning and retrieval/demos improves capability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Tool Learning with Large Language Models: A Survey', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Toolformer <em>(Rating: 2)</em></li>
                <li>ToolLLaMA <em>(Rating: 2)</em></li>
                <li>ToolBench1 <em>(Rating: 2)</em></li>
                <li>ToolBench2 <em>(Rating: 2)</em></li>
                <li>TRICE <em>(Rating: 2)</em></li>
                <li>DFSDT <em>(Rating: 2)</em></li>
                <li>ReAct: Synergizing Reasoning and Acting <em>(Rating: 2)</em></li>
                <li>Chain of Thought <em>(Rating: 2)</em></li>
                <li>ToolQA <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-907",
    "paper_id": "paper-0479547a69ffa48fe106137eba8358a9d8a2cc48",
    "extraction_schema_id": "extraction-schema-22",
    "extracted_data": [
        {
            "name_short": "LLM QA vs Interactive Gap (survey-level)",
            "name_full": "Performance gap between LLM question-answering and interactive/procedural tasks (survey-level observation)",
            "brief_description": "The survey repeatedly notes that large pretrained LMs excel at standard QA/NLP tasks but often underperform on interactive or procedural tasks (tool use, multi-step planning, sequential decision-making) due to limited parametric knowledge, lack of execution interfaces, sensitivity to prompts, and inability to incorporate dynamic tool feedback.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_or_agent_name": "Generic large language models (e.g., GPT-family, LLaMA-family)",
            "model_description": "Large pre-trained transformer LMs with strong zero-/few-shot QA and generation capabilities but limited native mechanisms for executing external actions or iterative tool-driven plans.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": null,
            "interactive_task_type": "tool use / multi-step reasoning / planning / sequential decision-making",
            "interactive_performance": null,
            "reports_both_qa_and_interactive": null,
            "performance_gap_observed": true,
            "architectural_features": null,
            "training_method": null,
            "intervention_type": null,
            "intervention_description": null,
            "intervention_effect": null,
            "hypothesized_cause_of_gap": "LLMs rely on fixed parametric knowledge and pattern matching from pretraining, lack execution interfaces and iterative feedback loops, are sensitive to prompt perturbations, and have limited context/utility for handling external tool outputs, causing poorer performance on interactive/procedural tasks versus static QA.",
            "uuid": "e907.0",
            "source_info": {
                "paper_title": "Tool Learning with Large Language Models: A Survey",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "GPT-4 (plugins)",
            "name_full": "GPT-4 with plugin/tool integration",
            "brief_description": "Survey cites GPT-4 as an example of a powerful LLM that mitigates knowledge and capability limits by calling external plugins/tools and integrating returned results into final responses.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "GPT-4 (with plugin API calls)",
            "model_description": "Closed-source large transformer model (GPT-4) augmented with a plugin interface that allows the model to call external APIs and incorporate their outputs into responses.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "Plugin/API-based question answering and tool-augmented tasks",
            "interactive_task_type": "tool use / API calling",
            "interactive_performance": null,
            "reports_both_qa_and_interactive": null,
            "performance_gap_observed": true,
            "architectural_features": "tool-use interface (plugin architecture), ability to integrate external responses",
            "training_method": null,
            "intervention_type": "architectural change (plugin/tool integration)",
            "intervention_description": "Expose a plugin API so the LLM can request external data or actions at inference time and integrate the tool outputs into final answers.",
            "intervention_effect": "Qualitative improvement in accuracy and timeliness of responses by augmenting parametric knowledge with tool outputs (no numeric metrics provided in survey).",
            "hypothesized_cause_of_gap": "Parametric knowledge is outdated/limited; plugin integration provides dynamic, up-to-date, and execution-capable information that QA-only LLMs lack.",
            "uuid": "e907.1",
            "source_info": {
                "paper_title": "Tool Learning with Large Language Models: A Survey",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Toolformer (GPT-J finetune)",
            "name_full": "Toolformer (fine-tuning a base LM with API-call tokens)",
            "brief_description": "Toolformer fine-tunes a base LM (reported in prior work on GPT-J) by inserting API-call tokens into training so the model learns when and how to call external tools, improving tool-awareness and usage.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_or_agent_name": "Toolformer-finetuned GPT-J",
            "model_description": "A base LM (GPT-J) fine-tuned to predict API-call tokens and the surrounding context so the model learns to invoke tools during generation.",
            "model_size": "6B",
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "Tool invocation and tool-augmented QA/tasks",
            "interactive_task_type": "tool use / tool calling",
            "interactive_performance": null,
            "reports_both_qa_and_interactive": null,
            "performance_gap_observed": true,
            "architectural_features": "no structural architecture change to transformer backbone; training augments token vocabulary with API-call tokens",
            "training_method": "supervised fine-tuning on augmented token sequences (tool-call tokens)",
            "intervention_type": "training method (fine-tuning with tool-call supervision)",
            "intervention_description": "Fine-tune the LM on corpora where API-call tokens are inserted where tool calls are useful, teaching the model to predict and format tool calls as part of generation.",
            "intervention_effect": "Reported qualitative improvement in tool-awareness and tool-usage capabilities (survey reports improvement but does not provide numeric before/after metrics).",
            "hypothesized_cause_of_gap": "Pretrained LMs do not receive explicit supervision for tool invocation; adding token-level supervision teaches the model execution affordances it lacked from pretraining.",
            "uuid": "e907.2",
            "source_info": {
                "paper_title": "Tool Learning with Large Language Models: A Survey",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "ToolLLaMA",
            "name_full": "ToolLLaMA (fine-tuning LLaMA for tool usage)",
            "brief_description": "ToolLLaMA fine-tunes the LLaMA 7B model using instruction-solution pairs (from DFSDT) to improve planning and API calling skills for tool-augmented tasks.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_or_agent_name": "ToolLLaMA (LLaMA-7B fine-tuned)",
            "model_description": "LLaMA 7B base model fine-tuned on datasets of instruction-to-solution pairs specialized for tool selection, parameter extraction, and API calling.",
            "model_size": "7B",
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "Tool selection, API calling, and multi-step tool-augmented problem solving",
            "interactive_task_type": "tool use / planning / multi-step reasoning",
            "interactive_performance": null,
            "reports_both_qa_and_interactive": null,
            "performance_gap_observed": true,
            "architectural_features": null,
            "training_method": "supervised fine-tuning (instruction-solution pairs)",
            "intervention_type": "training method (fine-tuning)",
            "intervention_description": "Fine-tune an open-source LLM with datasets generated by DFSDT to teach correct planning and API invocation behaviors.",
            "intervention_effect": "Improved planning and API-calling capabilities relative to unfine-tuned baselines (no numeric metrics reported in survey).",
            "hypothesized_cause_of_gap": "Lack of targeted instructive examples for tool use in pretraining; fine-tuning injects this behavior.",
            "uuid": "e907.3",
            "source_info": {
                "paper_title": "Tool Learning with Large Language Models: A Survey",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "TRICE",
            "name_full": "TRICE (two-stage behavior cloning + RLEF)",
            "brief_description": "TRICE uses a two-stage pipeline: behavior cloning (instruct-tuning) to imitate tool-use demonstrations, followed by reinforcement learning from environment feedback (RLEF) using tool execution results to further improve performance.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_or_agent_name": "Model trained with TRICE pipeline",
            "model_description": "An LLM first trained via supervised imitation of tool-usage traces, then further optimized using reinforcement learning leveraging tool-execution feedback.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "Tool-augmented decision making and API calling",
            "interactive_task_type": "tool use / sequential decision-making / planning",
            "interactive_performance": null,
            "reports_both_qa_and_interactive": null,
            "performance_gap_observed": true,
            "architectural_features": null,
            "training_method": "behavior cloning (supervised), then reinforcement learning from environment feedback (RLEF)",
            "intervention_type": "training method (hybrid: supervised + RL with tool feedback)",
            "intervention_description": "Stage 1: instruct-tune the model to mimic tool usage; Stage 2: apply RL using actual tool execution feedback to refine policies and improve robustness to tool errors.",
            "intervention_effect": "Survey reports this pipeline as improving tool-usage and robustness (no numeric metrics provided).",
            "hypothesized_cause_of_gap": "Supervised imitation alone does not account for stochastic tool outputs; RL with real execution feedback helps models adapt and correct decisions.",
            "uuid": "e907.4",
            "source_info": {
                "paper_title": "Tool Learning with Large Language Models: A Survey",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "DFSDT / Depth First Search for decision-making",
            "name_full": "DFSDT (Depth-First Search based decision strategy for tool selection)",
            "brief_description": "DFSDT introduces a depth-first search strategy to mitigate error propagation in sequential tool selection decisions, improving decision-making accuracy in multi-step tool use.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_or_agent_name": "LLM + DFSDT planning strategy",
            "model_description": "An LLM guided by a depth-first search over possible decision branches to explore alternative tool-usage sequences and reduce cascading errors.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "Tool selection and multi-step tool-usage planning",
            "interactive_task_type": "planning / multi-step reasoning / tool use",
            "interactive_performance": null,
            "reports_both_qa_and_interactive": null,
            "performance_gap_observed": true,
            "architectural_features": "search-guided planner (DFS) wrapped around LLM decision-making",
            "training_method": null,
            "intervention_type": "algorithmic/planning intervention (search over decision branches)",
            "intervention_description": "Use DFS to enumerate and evaluate multiple decision branches for tool selection, reducing reliance on a single greedy plan and limiting error propagation.",
            "intervention_effect": "Reported to improve decision accuracy and robustness in tool selection tasks; no numeric values provided in survey.",
            "hypothesized_cause_of_gap": "Greedy one-shot planning amplifies single-step errors; search-based planning mitigates propagation by exploring alternatives.",
            "uuid": "e907.5",
            "source_info": {
                "paper_title": "Tool Learning with Large Language Models: A Survey",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "ReAct / Chain-of-Thought prompting",
            "name_full": "ReAct and Chain-of-Thought (prompting-based reasoning + action)",
            "brief_description": "Prompting strategies that combine explicit chain-of-thought reasoning with actions (tool calls) â€” ReAct integrates reasoning traces with environment actions, while Chain-of-Thought encourages step-by-step reasoning to improve planning and multi-step performance without model finetuning.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_or_agent_name": "LLMs using ReAct / Chain-of-Thought prompting",
            "model_description": "Prompting techniques applied at inference time to induce stepwise reasoning and interleave reasoning with actions/tool invocations, leveraging in-context learning.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "Multi-step reasoning and tool-augmented tasks",
            "interactive_task_type": "multi-step reasoning / tool use / planning",
            "interactive_performance": null,
            "reports_both_qa_and_interactive": null,
            "performance_gap_observed": true,
            "architectural_features": "no architectural change; relies on prompt engineering to elicit internal reasoning traces",
            "training_method": "prompting only / in-context learning",
            "intervention_type": "prompting strategy (tuning-free)",
            "intervention_description": "Use few-shot or zero-shot prompts (e.g., 'let's think step by step') and ReAct-style templates to interleave natural language reasoning and tool actions to improve decision-making during interactions.",
            "intervention_effect": "Improves planning and tool-selection behavior in many cases (survey notes empirical benefit but does not list standardized metrics); however, still often underperforms specialized finetuned models.",
            "hypothesized_cause_of_gap": "Prompt-induced reasoning helps but cannot fully compensate for lack of explicit execution and feedback-aware training.",
            "uuid": "e907.6",
            "source_info": {
                "paper_title": "Tool Learning with Large Language Models: A Survey",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "ToolBench findings (open-source LLMs)",
            "name_full": "ToolBench / ToolBench1/2 analyses on open-source LLM tool learning",
            "brief_description": "The survey summarizes that ToolBench-style evaluations find open-source LLMs often struggle at tool learning tasks, and that interventions like fine-tuning, demonstration retrieval, and system prompts substantially improve performance.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_or_agent_name": "Open-source LLMs evaluated in ToolBench analyses",
            "model_description": "Various open-source base LMs evaluated for tool selection, parameter extraction, and API calling; baseline (unfine-tuned) models exhibit weaknesses on interactive tasks.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "Benchmarks spanning tool selection, tool calling, response generation (e.g., ToolBench1/2, API-Bank, API-BLEND)",
            "interactive_task_type": "tool use / tool calling / tool selection / multi-step reasoning",
            "interactive_performance": null,
            "reports_both_qa_and_interactive": null,
            "performance_gap_observed": true,
            "architectural_features": null,
            "training_method": null,
            "intervention_type": "training method + prompting + retrieval/demonstration augmentation",
            "intervention_description": "ToolBench analysis recommends supervised fine-tuning on tool-use datasets, retrieval of demonstrations, and system prompt engineering to mitigate poor tool-use performance of open LMs.",
            "intervention_effect": "Survey reports these interventions significantly enhance tool-learning effectiveness for open-source models (no standardized numeric metrics provided in survey).",
            "hypothesized_cause_of_gap": "Open LMs lack specialized training data and in-context examples for tool usage; adding targeted fine-tuning and retrieval/demos improves capability.",
            "uuid": "e907.7",
            "source_info": {
                "paper_title": "Tool Learning with Large Language Models: A Survey",
                "publication_date_yy_mm": "2024-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Toolformer",
            "rating": 2
        },
        {
            "paper_title": "ToolLLaMA",
            "rating": 2
        },
        {
            "paper_title": "ToolBench1",
            "rating": 2
        },
        {
            "paper_title": "ToolBench2",
            "rating": 2
        },
        {
            "paper_title": "TRICE",
            "rating": 2
        },
        {
            "paper_title": "DFSDT",
            "rating": 2
        },
        {
            "paper_title": "ReAct: Synergizing Reasoning and Acting",
            "rating": 2
        },
        {
            "paper_title": "Chain of Thought",
            "rating": 2
        },
        {
            "paper_title": "ToolQA",
            "rating": 1
        }
    ],
    "cost": 0.015196999999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Tool Learning with Large Language Models: A Survey</h1>
<p>Changle QU ${ }^{1}$, Sunhao $\mathrm{DAI}^{1}$, Xiaochi WEI ${ }^{2}$, Hengyi CAI ${ }^{3}$, Shuaiqiang WANG ${ }^{2}$, Dawei YIN $^{2}$, Jun XU( $\boxtimes)^{1}$, Ji-Rong WEN ${ }^{1}$<br>1 Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, 100872, China<br>2 Baidu Inc., Beijing 100193, China<br>3 Institute of Computing Technology, Chinese Academy of Sciences, Beijing 100864, China</p>
<p>(c) Higher Education Press 2024</p>
<h4>Abstract</h4>
<p>Recently, tool learning with large language models (LLMs) has emerged as a promising paradigm for augmenting the capabilities of LLMs to tackle highly complex problems. Despite growing attention and rapid advancements in this field, the existing literature remains fragmented and lacks systematic organization, posing barriers to entry for newcomers. This gap motivates us to conduct a comprehensive survey of existing works on tool learning with LLMs. In this survey, we focus on reviewing existing literature from the two primary aspects (1) why tool learning is beneficial and (2) how tool learning is implemented, enabling a comprehensive understanding of tool learning with LLMs. We first explore the "why" by reviewing both the benefits of tool integration and the inherent benefits of the tool learning paradigm from six specific aspects. In terms of "how", we systematically review the literature according to a taxonomy of four key stages in the tool learning workflow: task planning, tool selection, tool calling, and response generation. Additionally, we provide a detailed summary of existing</p>
<p>benchmarks and evaluation methods, categorizing them according to their relevance to different stages. Finally, we discuss current challenges and outline potential future directions, aiming to inspire both researchers and industrial developers to further explore this emerging and promising area.</p>
<p>Keywords Tool Learning, Large Language Models, Agent</p>
<h2>1 Introduction</h2>
<p>"Sharp tools make good work."
â€”The Analects: Wei Ling Gong
Throughout history, humanity has continually sought innovation, utilizing increasingly sophisticated tools to boost efficiency and enhance capabilities $[1,2]$. These tools, extending both our intellect and physicality, have been crucial in driving social and cultural evolution [3]. From primitive</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p><strong>Figure 1</strong> An illustration of the development trajectory of tool learning. We present the statistics of papers with the publication year and venue, with each venue uniquely represented by a distinct color. For each time period, we have selected a range of representative landmark studies that have significantly contributed to the field. (Note that we use the institution of the first author as the representing institution in the figure.)</p>
<p>Stone tools to advanced machinery, this progression has expanded our potential beyond natural limits, enabling more complex and efficient task management [4].</p>
<p>Today, we are experiencing a new technological renaissance, driven by breakthroughs in artificial intelligence, especially through the development of large language models (LLMs). Pioneering models such as ChatGPT [5] have demonstrated remarkable capabilities, marking significant progress in a range of natural language processing (NLP) tasks, including summarization [6, 7], machine translation [8, 9], question answering [10, 11], <em>etc</em>. However, despite their impressive capabilities, LLMs often struggle with complex computations and delivering accurate, timely information due to their reliance on fixed and parametric knowledge [12, 13]. This inherent limitation frequently results in responses that are plausible yet factually incorrect or outdated (often referred to as hallucination) [14, 15], posing significant risks and misleading users.</p>
<p>With the continuous enhancement of LLMs capabilities, it is expected that LLMs will become proficient in using tools to solve complex problems as human [16], a concept known as tool learning with LLMs. Tool learning emerges as a promising solution to mitigate these limitations of LLMs by enabling dynamic interaction with external tools [17â€“20]. This approach not only enhances problem-solving capabilities of LLMs but also broadens their functional scope [21â€“23]. For instance, LLMs can perform complex calculations using a calculator tool, access real-time weather updates through weather APIs, and execute programming code via interpreters [24, 25]. This integration significantly improves their response accuracy to user queries, facilitating more effective and reliable user interactions. As this field continues to evolve, tool-augmented LLMs are expected to play a pivotal role in the future of NLP [26, 27], offering more versatile and adaptable solutions [28, 29].</p>
<p>As shown in Figure 1, the past year has witnessed</p>
<p>a rapid surge in research efforts on tool learning concurrent with the rise of LLMs. Notably, in practical applications, GPT-4 [5] addresses its knowledge limitations and augments its capabilities by calling on plugins, ultimately integrating the returned results of plugins with its internal knowledge to generate better responses for users. Within the research community, much effort has been made in exploring how to evaluate the tool learning capabilities of LLMs [30-32] and how to enhance it to strengthen the capabilities of LLMs [33-35]. Given the increasing attention and rapid development of tool learning with LLMs, it is essential to systematically review the most recent advancements and challenges, so as to benefit researchers and industrial developers in understanding the current progress and inspire more future work in this area.</p>
<p>In this survey, we conduct a systematic exploration of existing studies in two primary dimensions: (1) why tool learning is beneficial and (2) how tool learning is implemented. Specifically, the "why tool learning" dimension examines both the advantages of tool integration and the inherent benefits of the tool learning paradigm, while the "how tool learning" dimension details the four stages of the entire tool learning workflow: task planning, tool selection, tool calling, and response generation. These dimensions are foundational to understanding tool learning with LLMs. Moreover, we provide a systematic summary of existing benchmarks and evaluation methods, classifying them based on their focus across different stages. Finally, we discuss the current challenges and propose future directions, offering critical insights to facilitate the development of this promising and burgeoning research area. We also maintain a GitHub repository to continually keep track of the relevant papers and resources in this rising area at https:
//github.com/quchangle1/LLM-Tool-Survey.
It is worth noting that while other surveys provide comprehensive overviews of techniques and methods used by LLMs [36], applications in planning [37], reasoning [38, 39], agents [40-42], and retrieval-augmented generation [43,44], they often mention tools or tool learning but do not extensively explore this aspect. Compared with them, our survey provides a focused and detailed analysis of tool learning with LLMs, especially elucidating the dual aspects of why tool learning is essential for LLMs and how tool learning can be systematically implemented. Through these two principle aspects, we offer an up-to-date and comprehensive review of tool learning with LLMs. Meanwhile, we also acknowledge the foundational contributions of earlier perspective papers like those by Mialon et al. (2023) [45] and Qin et al. (2023) [16], which initially highlighted the promising opportunities that tools present to enhance LLMs capabilities. Since the field has seen rapid growth with many new studies emerging, our survey provides a broader introduction to these latest developments. Additionally, a more recent survey [46] discusses various tooling scenarios and approaches employed in language models, serving as an excellent supplement to our comprehensive review.</p>
<p>The remaining part of this paper (as illustrated in Figure 2) is organized as follows: We begin by introducing the foundational concepts and terminology related to tool learning (Â§2). Following this, we explore the significance of tool learning for LLMs from six specific aspects (Â§3). We then systematically review the recent advancements in tool learning, focusing on four distinct stages of the tool learning workflow (Â§4). Subsequently, we provide a summary of the resources available for tool learning, including benchmarks and evaluation</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2 The overall structure of this paper.
methods (Â§5). Next, we discuss the current challenges in the field and outline open directions for future research (Â§6). Lastly, we conclude the survey by summarizing the key findings (Â§7).</p>
<h2>2 Background</h2>
<p>In this section, we provide an overview of the key concepts and terminology related to tool learning,</p>
<p>offering a clearer understanding of the fundamental aspects of this field.</p>
<p>What is a Tool? The definition of a tool is notably broad within the context of augmented LLMs. Mialon et al. (2023) [45] articulate a tool as "the external employment of an unattached or manipulable attached environmental object to alter more efficiently the form, position, or condition of another object." On the other hand, Wang et al. (2024) [46] define a tool as "An LM-used tool is a function interface to a computer program that runs externally to the $L M$, where the $L M$ generates the function calls and input arguments in order to use the tool." Similarly, it is our contention that any method enhancing LLMs through external means qualifies as a tool. Notably, retrieval-augmented generation (RAG) represents a specific instance of tool learning, wherein the search engine is employed as a tool for LLMs. Meanwhile, the definition of "tool" often remains vague and inconsistent across different papers. For example, some studies distinctly define tools and APIs, positing that a tool comprises an aggregation of multiple APIs [18, 33, 53]. Conversely, other studies treat each API as an independent tool [19, 30, 122]. In this survey, adhering to the definitions of tools established earlier in the text, we consider each API as an individual tool.</p>
<p>What is Tool Learning? Tool learning refers to the process that "aims to unleash the power of LLMs to effectively interact with various tools to accomplish complex tasks" [18]. This paradigm significantly improves the ability of LLMs to solve complex problems. For example, when ChatGPT receives a user query, it evaluates the necessity of calling a specific tool. If a tool is required, ChatGPT will transparently outline the problem-solving process using the tool, explaining the rationale behind its responses, thereby ensuring the user receives a well-informed answer. Moreover, in instances where the initial solution fails, ChatGPT will reassess its tool selection and employ an alternative to generate a new response.</p>
<h2>3 Why Tool Learning?</h2>
<p>In this section, we will delineate the multifaceted importance of tool learning for LLMs from two principal perspectives: the benefits of tool integration and the benefits of the tool learning paradigm itself. On the one hand, tool integration into LLMs enhances capabilities across several domains, namely knowledge acquisition, expertise enhancement, automation and efficiency, and interaction enhancement. On the other hand, the adoption of the tool learning paradigm bolsters the robustness of responses and transparency of generation processes, thereby enhancing interpretability and user trust, as well as improving system robustness and adaptability. Subsequent subsections will elaborate on these six aspects in detail, outlining why tool learning is important for LLMs.</p>
<h3>3.1 Knowledge Acquisition</h3>
<p>Although LLMs have showcased their immense capabilities across various fields [161], their abilities are still bounded by the extent of knowledge learned during pre-training [12]. This embedded knowledge is finite and lacks the ability to acquire updated information. Additionally, the effectiveness of LLMs is further compromised by prompts from users, which may not always be meticulously crafted. Consequently, LLMs are prone to generating contents that seem superficially plausible but may contain factual inaccuracies, which is known as hallucination. A promising approach to mitigate</p>
<p>these limitations involves augmenting LLMs with the capability to access external tools, which allows LLMs to acquire and integrate external knowledge dynamically. For example, the employment of search engine tool can enable LLMs to access contemporary information [17, 28, 47-51], while the integration of database tool allows LLMs to access structured databases to retrieve specific information or execute complex queries, thus expanding their knowledge base [52-57]. Additionally, connections to weather tools allow for real-time updates on weather conditions, forecasts, and historical data $[19,31,33]$, and interfacing with mapping tools enables LLMs to get and provide geographical data, aiding in navigation and location-based queries [16]. Through these enhancements, LLMs can surpass traditional limitations, offering more accurate and contextually relevant outputs.</p>
<h3>3.2 Expertise Enhancement</h3>
<p>Given the fact that LLMs are trained on datasets comprising general knowledge, they often exhibit deficiencies in specialized domains. While LLMs demonstrate robust problem-solving capabilities for basic mathematical problems, excelling in operations such as addition, subtraction, and exhibiting reasonable proficiency in multiplication tasks, their abilities significantly decline when confronted with division, exponentiation, logarithms, trigonometric functions, and other more complex composite functions [162, 163]. This limitation extends to tasks involving code generation [164, 165] and chemistry and physics problems [73, 74], etc., further underscoring the gap in their expertise in more specialized areas. Consequently, it is feasible to employ specific tools to augment the domain-specific expertise of LLMs [60, 61, 76, 166]. For example, LLMs can use online calculators or mathematical
tools to perform complex calculations, solve equations, or analyze statistical data [27, 58-66]. Additionally, the integration of external programming resources such as Python compilers and interpreters allows LLMs to receive code execution feedback, which is essential for refining code to align with user requirements and to optimize the code generation [24, 25, 67-72]. Moreover, LLMs can also leverage tools in fields such as chemistry [73-75], biology [76], economics [77-79], medicine [80,81], and recommendation systems [35] to enhance their domain-specific expertise. This approach not only mitigates the expertise gap in LLMs but also enhances their utility in specialized applications by providing domain-specific knowledge.</p>
<h3>3.3 Automation and Efficiency</h3>
<p>LLMs are fundamentally language processors that lack the capability to execute external actions independently, such as reserving conference rooms or booking flight tickets [46]. The integration of LLMs with external tools facilitates the execution of such tasks by simply populating tool interfaces with the necessary parameters. For example, LLMs can employ task automation tools to automate repetitive tasks such as scheduling [17], setting reminders [55], and filtering emails [18], thereby enhancing their practicality for user assistance. Moreover, by interfacing with project management and workflow tools, LLMs can aid users in managing tasks, monitoring progress, and optimizing work processes [18]. In addition, the integration with online shopping assistants not only simplifies the shopping process [21] but also enhances processing efficiency and user experience. Furthermore, employing data table processing tools enables LLMs to perform data analysis and visualization directly [16], thereby simplifying the data manipulation process of users.</p>
<h3>3.4 Interaction Enhancement</h3>
<p>Due to the diverse and multifaceted nature of user queries in the real-world, which may encompass multiple languages and modalities, LLMs often face challenges in consistently understanding different types of input. This variability can lead to ambiguities in discerning the actual user intent [88]. The deployment of specialized tools can significantly enhance the perceptual capabilities of LLMs. For example, LLMs can utilize multi-modal tools, such as speech recognition and image analysis, to better understand and respond to a broader spectrum of user inputs [29, 82-88]. Moreover, by interfacing with machine translator tools, LLMs have the capability to convert languages in which they are less proficient into languages they comprehend more effectively [16,17]. Additionally, the integration of advanced natural language processing tools can augment the linguistic understanding of LLMs, thereby optimizing dialogue management and intent recognition $[18,89,90]$. Such advancements may include platforms that utilize contextual understanding models to elevate the performance of chatbot systems. Ultimately, improving perceptual input and sensory perception is crucial for the progression of LLMs capabilities in managing intricate user interactions.</p>
<h3>3.5 Enhanced Interpretability and User Trust</h3>
<p>A significant concern with current LLMs is their opaque, "black-box" nature, which does not reveal the decision-making process to users [167, 168], thereby severely lacking in interpretability. This opacity often leads to skepticism about the reliability of the response provided by LLMs and makes it challenging to ascertain their correctness [169]. Moreover, interpretability is particularly crucial in high-stakes domains such as aviation, healthcare and finance $[16,77]$, where accuracy is imperative. Therefore, understanding and explaining LLMs is crucial for elucidating their behaviors [168]. Some studies have enhanced the accuracy and interpretability of LLMs by enabling them to generate text with citations [170, 171]. In contrast, through the utilization of tool learning, LLMs can exhibit each step of their decision-making process, thereby making their operations more transparent [16]. Even in cases of erroneous outputs, such transparency allows users to quickly identify and understand the source of errors, which facilitates a better understanding and trust in the decisions of LLMs, thus enhancing effective human-machine collaboration.</p>
<h3>3.6 Improved Robustness and Adaptability</h3>
<p>Existing research indicates that LLMs are highly sensitive to user inputs within prompts [172-174]. Merely minor modifications to these inputs can elicit substantial changes in the responses, highlighting a lack of robustness in LLMs. In the real world, different users have varying interests and ways of asking questions, leading to a diverse array of prompts. The integration of specialized tools has been proposed as a strategy to reduce reliance on the statistical patterns in the training data [16-18,54, 89]. Though the input format from the user is different, the input and output of the tool are the same. This enhancement increases the resistance of LLMs to input perturbations and their adaptability to new environments. Thus, such integration not only stabilizes the models in uncertain conditions but also reduces the risks associated with input errors.</p>
<h2>4 How Tool Learning?</h2>
<p>In this section, we will first introduce the overall paradigm of tool learning, which includes four distinct stages and two typical paradigms. Following</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3 The overall workflow for tool learning with large language models. The left part illustrates the four stages of tool learning: task planning, tool selection, tool calling, and response generation. The right part shows two paradigms of tool learning: Tool Learning with One-step Task Solving and Tool Learning with Iterative Task Solving.
this framework, we provide a detailed review of each stage within the tool learning workflow, along with the latest advancements associated with each stage. It's important to note that many works involve multiple stages of tool learning, but we only discuss its core stages here. For each stage, we also present a practical, real-world example utilizing GPT-4 for tool learning to address a specific problem, which are designed to help newcomers better understand what each stage involves and how it is implemented.</p>
<h3>4.1 Overall Paradigm of Tool Learning</h3>
<p>In this section, we will introduce the entire process of tool learning, including four stages and two paradigms involved in the utilization of toolaugmented LLMs.</p>
<p>Four Stages of Tool Learning. As illustrated in the left part of Figure 3, the typical process of tool learning comprises four stages: task planning, tool selection, tool calling, and response generation, which is adopted in numerous works related to tools [89, 93, 94]. This process outlines the user</p>
<p>interaction pipeline with tool-augmented LLMs: given a user question, the preliminary stage involves the LLMs analyzing the requests of users to understand their intent and decompose it into potential solvable sub-questions. Subsequently, the appropriate tools are selected to tackle these sub-questions. This tool selection process is categorized into two types based on whether a retriever is used: retrieverbased tool selection and LLM-based tool selection. Recently, there has been an increasing focus on initially using a retriever to filter out the top-k suitable tools [18, 34, 122]. This necessity stems from the fact real-world systems usually have a vast number of tools, rendering it impractical to incorporate the descriptions of all tools as input for LLMs due to the constraints related to length and latency [124]. Subsequently, the user query along with the selected tools are furnished to the LLMs, enabling it to select the optimal tool and configure the necessary parameters for tool calling. This necessitates that the LLMs possess a keen awareness of using tools and be able to correctly select the tools needed. Moreover, it is imperative for the LLMs to extract the correct tool parameters from the user query, a process that demands not only the accuracy of the parameter content but also adherence to the specific format requirements. Following the invocation of the tool, the LLMs utilizes the results returned by the tool to craft a superior response for the user.</p>
<p>Two Paradigms of Tool Learning. As illustrated in the right part of Figure 3, the paradigms for employing tool learning can be categorized into two types: tool learning with one-step task solving and tool learning with iterative task solving. These are also referred to as planning without feedback and planning with feedback in Wang et al. (2024) [40], and decomposition-first and interleaved decomposi-
tion in Huang et al. (2024) [37]. In earlier studies on tool learning $[17,69,89]$, the primary paradigm is tool learning with one-step task solving: upon receiving a user question, LLMs would analyze the requests of user to understand the user intent and immediately plan all the sub-tasks needed to solve the problem. The LLMs would then directly generate a response based on the results returned by the selected tools without considering the possibility of errors during the process or altering the plan based on tool feedback. Subsequent researches introduce a new paradigm known as tool learning with iterative task solving $[18,30,34,93]$. This approach does not commit to a complete task plan upfront. Instead, it allows for iterative interactions with the tools, adjusting the sub-tasks progressively based on tool feedback. This enables LLMs to address the problem step-by-step, refining its plan continuously in response to the results returned by tools, thereby improving problem-solving capabilities. This shift from a one-step to an iterative approach marks a significant advancement in the field of tool learning.</p>
<h3>4.2 Task Planning</h3>
<p>Generally, in real-world scenarios, user queries often embody complex intent. Thus, it is essential to first engage in task planning to conduct a comprehensive analysis of the user intent. This stage involves the decomposition of a user question into multiple sub-questions as required to achieve the specified objective. Additionally, given the dependency relationships inherent among some tasks, the planner is also tasked with delineating the dependencies and execution sequence of the decomposed tasks, thereby facilitating the establishment of interconnections between the sub-questions. For example, a real example for task planning with GPT-4 is shown as follows:</p>
<h2>An Example for Task Planning with GPT-4</h2>
<p>Instruction Prompt: You are currently in the task planning stage. You are given a user query requiring multi-step actions and reasoning. You will break down the user's query into sub-questions, and you only need to output these sub-questions after the breakdown. Ensure that the original problem is comprehensively covered through the minimal number of sub-questions.
User Question: I would like to know the value of 5 ounces of gold plus 1 million AMZN stocks in CNY.
Output: 1. What is the current price of gold per ounce in USD? 2. What is the current stock price of Amazon (AMZN) per share in USD? 3. What is the current exchange rate between USD and CNY (Chinese Yuan)?</p>
<p>Tool learning with iterative task solving paradigm is not convenient for staged demonstrations, so this example uses the tool learning with one-step task solving paradigm. From this example, we can see that GPT-4 breaks down the user's question into three sub-questions. Since internal knowledge of GPT-4 does not contain the latest information, it is necessary to obtain the current prices of gold and Amazon (AMZN) stock, as well as the latest exchange rate between USD and CNY. By breaking down the query in this manner, GPT-4 can address complex questions step by step, demonstrating its logical analysis capabilities and ability to handle multi-step tasks. Next, we will introduce the latest developments in two categories: tuning-free methods and tuning-based methods.</p>
<h3>4.2.1 Tuning-free Methods</h3>
<p>Existing studies $[50,175,176]$ demonstrate that the innate abilities of LLMs enable effective planning
through methods such as few-shot or even zero-shot prompting. For example, some studies [177-180] leverage prompts to decompose complex tasks into simpler sub-tasks, facilitating a structured plan of action. ART [50] constructs a task library, from which it retrieves examples as few-shot prompts when encountering real-world tasks. RestGPT [93] introduces a Coarse-to-Fine Online Planning approach, an iterative task planning methodology that enables LLMs to progressively refine the process of task decomposition. HuggingGPT [89] leverages a sophisticated prompt design framework, which integrates specification-based instructions with demonstration based parsing methods. ToolChain* [95] employs a planning mechanism by constructing the entire action space as a decision tree, where each node within the tree represents a potential API function call. TPTU [94] introduces a structured framework specifically designed for LLM-based AI agents, incorporating two distinct types of agents: the One-step agent and the sequential agent. Attention Buckets [97] operates in parallel with unique RoPE angles, forming distinct waveforms that compensate for each other, reducing the risk of LLMs missing critical information. ControlLLM [96] introduces a paradigm known as Thoughts-on-Graph (ToG), which leverages Depth-First Search (DFS) on a pre-constructed tool graph to identify solutions. PLUTO [98] uses an autoregressive planning approach to iteratively improve performance by generating hypotheses, performing cluster analysis, and refining sub-queries until the initial query is satisfied. ATC [99] enables LLMs to independently learn and master new tools by using a chain of tools and a black-box probing method to identify and record tool usage. Tool-Planner [181] organizes tools into toolkits based on API functions with similar functionality, enabling LLMs to plan across dif-</p>
<p>ferent toolkits. SGC [100] enhances task planning by integrating GNNs with LLMs, enabling more efficient and accurate sub-task selection within task graphs. Sum2Act [101] guides LLMs to solve complex tasks by summarizing progress at each step and adjusting actions based on task state and errors. BTP [102] creates an optimal plan for tool usage under budget constraints, allowing LLMs to efficiently manage costs while solving user queries. DRAFT [103] proposes a novel framework aimed at dynamically adjusting and optimizing tool documentation based on the interaction feedback between LLMs and external tools, thereby improving LLMs' comprehension and tool-using capabilities.</p>
<h3>4.2.2 Tuning-based Methods</h3>
<p>Though LLMs demonstrate impressive performance in zero-shot or few-shot settings, they remain less effective compared to models that have been finetuned [182]. Toolformer [17] employs API calls that actually assist the model in predicting future tokens to fine-tune GPT-J, which enhances the awareness and capability of LLMs to utilize tools effectively. TaskMatrix.AI [104] leverages Reinforcement Learning from Human Feedback (RLHF) to utilize the knowledge and insights gained through human feedback, thereby enhancing the foundation model. Toolink [105] innovates by decomposing the target task into a toolkit for problem-solving, then employing a model to utilize these tools to answer queries via a chain-of-solving (CoS) approach. TPTU-v2 [106] develops an LLM finetuner to fine-tune a base LLM using a meticulously curated dataset, so that the finetuned LLM can be more capable of task planning and API calls, especially for domain-specific tasks. $\alpha$-UMi [107] presents a novel two-phase training paradigm where a foundational large language model is first extensively
fine-tuned and then replicated as a planner for further fine-tuning on planning tasks. COA [108] trains LLMs to first decode reasoning chains with abstract placeholders, and then call domain tools to reify each reasoning chain by filling in specific knowledge. DEER [109] stimulates decision-making awareness in LLMs across various scenarios by automatically generating tool usage examples with multiple decision branches, and enhances the generalization ability of LLMs towards unseen tools through proposing novel tool sampling strategies. SOAP [111] first lets the LLM generate a feasible API calling plan, i.e. solution, based on complex user inputs, and then allows the LLM to generate executable API calling code based on the generated solution. TP-LLaMA [112] introduces a method to create preference data from thought trees by using previously ignored failed explorations, creating a dataset for DPO to update the LLM's policy. APIGen [113] creates an automated pipeline for generating diverse, high-quality, and verifiable function-calling datasets, significantly enhancing the performance of models fine-tuned with this data.</p>
<p>Remark. In summary, task planning, as the initial stage of tool learning, is crucial for solving the entire problem. Although there are many methods currently available to enhance the task planning capabilities of LLMs, generating a perfect plan directly when facing complex issues remains challenging. Furthermore, tool learning is a process involving interaction between LLMs and tools. How to better utilize feedback from tools to improve planning is still a question worthy of investigation.</p>
<h3>4.3 Tool Selection</h3>
<p>After the task planning phase, LLMs have already decomposed the user question into multiple sub-</p>
<p>questions. In order to better address these subquestions, it is necessary to select appropriate tools. The tool selection process involves choosing through a retriever or directly allowing LLMs to pick from a provided list of tools. When there are too many tools, a tool retriever is typically used to identify the top- $K$ relevant tools to offer to the LLMs, a process known as retriever-based tool selection. If the quantity of tools is limited or upon receiving the tools retrieved during the tool retrieval phase, the LLMs need to select the appropriate tools based on the tool descriptions and the sub-question, which is known as LLM-based tool selection. For example, an example for tool selection with GPT-4 is shown as follows:</p>
<h2>An Example for Tool Selection with GPT-4</h2>
<p>Instruction Prompt: You are currently in the tool selection stage. You are given candidate tools that can be potentially used to solve the sub-question. Among candidate tools, select a list of relevant tools that would help solve the sub-question.
Sub-question 1: What is the current price of gold per ounce in USD?
Candidate Tools: 1.Metals Prices Rates API: The latest API endpoint will return real-time exchange rate data updated every 60 seconds. 2.Medium: Get official news from Medium. 3.Cryptocurrency Markets: Recently published cryptocurrencies videos.
Output: 1.Metals Prices Rates API: The latest API endpoint will return real-time exchange rate data updated every 60 seconds.
Sub-question 2: $\cdots$
$\vdots$
Output: $\cdots$</p>
<p>From this example, we can see that for the sub-
question about obtaining the price of gold, GPT-4 can correctly select the necessary tools. Specifically, when faced with multiple candidate tools, GPT-4 can analyze the features of each tool and choose the one most suitable for answering the question. In this example, GPT-4 selects the Metals Prices Rates API because it provides real-time updated information on gold prices. This demonstrates accuracy and effectiveness of GPT-4 in tool selection. Next, we will introduce the latest developments in two categories: retriever-based tool selection and LLM-based tool selection.</p>
<h3>4.3.1 Retriever-based Tool Selection</h3>
<p>Real-world systems often incorporate a wide array of tools, making it impractical to input descriptions of all tools into LLMs due to length limitations and latency constraints. Therefore, to fully exploit the potential of tool-augmented LLMs, it is crucial to develop an efficient tool retrieval system. This system aims to bridge the gap between the broad capabilities of LLMs and the practical limitations of input size by efficiently selecting the top- $K$ most suitable tools for a given query from a vast tool set. State-of-the-art retrieval methods can be categorized into two types: term-based and semantic-based.</p>
<p>Term-based Methods. Term-based methods (i.e., sparse retrieval) represent both documents and queries as high-dimensional sparse vectors based on terms, as exemplified by TF-IDF [114] and BM25 [115]. These methods employ exact term matching to achieve efficient alignment between queries and documents. For example, Gorilla [53] employs BM25 and GPTIndex to construct a retriever for implementing tool retrieval.</p>
<p>Semantic-based Methods. Conversely, semanticbased methods (i.e., dense retrieval) utilize neu-</p>
<p>ral networks to learn the semantic relationship between queries and tool descriptions [116-120], and then calculate the semantic similarity using methods such as cosine similarity. Recently, there has been a burgeoning interest in the development and refinement of more efficient tool retrievers. Some studies $[18,34,106]$ train a Sentence-Bert model as the tool retriever, enabling the high-efficiency retrieval of relevant tools. CRAFT [121] instructs LLMs to generate a fictitious tool description based on the given query and then employs this fabricated tool to conduct a search. Anantha et al. (2023) [122] propose ProTIP based on the concept of task decomposition. Xu et al. (2024a) [183] propose a method that enhances tool retrieval by leveraging feedback from LLMs to progressively refine instructions and align retrieval with tool usage. COLT [124] proposes a novel tool retrieval approach using GNNs, identifying that a critical dimension often overlooked in conventional tool retrieval methodologies is the necessity to ensure the completeness of the tools retrieved. In addition to the recall phase, Zheng et al. (2024) [123] also take into account the re-ranking stage of tool retrieval. They consider the differences between seen and unseen tools, as well as the hierarchical structure of the tool library. Building on these considerations, they propose an adaptive and hierarchy-aware Re-ranking method, ToolRerank. Meanwhile, we can also directly employ off-the-shelf embeddings $[184,185]$ to get the representations of user queries and tool descriptions. In conclusion, constructing an efficient tool retriever is of paramount importance.</p>
<p>Remark. Although traditional information retrieval methods are suitable for tool retrieval scenarios, they still have issues such as focusing solely on semantic similarity and ignoring the hierarchical
structure of the tools, etc. Future work should consider the unique needs and characteristics specific to tool retrieval scenarios in order to build a more effective tool retriever.</p>
<h3>4.3.2 LLM-based Tool Selection</h3>
<p>In instances where the quantity of tool libraries is limited or upon receiving the tools retrieved from the tool retrieval phase, it is feasible to incorporate the descriptions and parameter lists of these tools into the input context along with the user query provided to LLMs. Subsequently, LLMs are tasked with selecting the appropriate tools from the available tool list based on the user query. Given that the resolution of queries is occasionally sensitive to the order in which tools are invoked, there is a necessity for serial tool calling, where the output of one tool may serve as the input parameter for another. Consequently, this demands a high degree of reasoning capability from the LLMs. It must adeptly select the correct tools based on the information currently at its disposal and the information that needs to be acquired. Existing methods can be similarly categorized into tuning-free and tuning-based approaches.</p>
<p>Tuning-free Methods. Tuning-free methods capitalize on the in context learning ability of LLMs through strategic prompting [89, 93]. For instance, Wei et al. (2022) [91] introduce the concept of chain of thought (COT), effectively incorporating the directive "let's think step by step" into the prompt structure. Further advancing this discourse, Yao et al. (2022) [92] propose ReACT, a framework that integrates reasoning with action, thus enabling LLMs to not only justify actions but also to refine their reasoning processes based on feedback from the environment (e.g., the output of tools). This development marks a significant step forward in</p>
<p>enhancing the adaptability and decision-making capabilities of LLMs by fostering a more dynamic interaction between reasoning and action. Building upon these insights, Qin et al. (2024) [18] propose DFSDT method, which addresses the issue of error propagation by incorporating a depthfirst search strategy to improve decision-making accuracy. ChatCoT [125] integrates tool use into multi-turn reasoning, allowing LLMs to seamlessly combine conversation-based reasoning with tool manipulation. ToolNet [126] organizes tools into a directed graph, enabling LLMs to navigate from an initial tool node, iteratively selecting tools until the task is completed. AnyTool [129] designs a more efficient tool retriever by leveraging the hierarchical structure of tools. GeckOpt [130] narrows down tool selection by adding intent-driven gating.</p>
<p>Tuning-based Methods. Tuning-based methods directly fine-tune the parameters of LLMs on the tool learning dataset to master tool usage. Toolbench [33] analyzes the challenges faced by opensource LLMs during the tool learning process, suggesting that fine-tuning, along with utilizing demonstration retrieval and system prompts, can significantly enhance the effectiveness of LLMs in tool learning. TRICE [128] proposes a two-stage framework, which initially employs behavior cloning for instruct-tuning of the LLMs to imitate the behavior of tool usage, followed by further reinforcing the model through RLEF by utilizing the tool execution feedback. ToolLLaMA [18] employs the instruction solution pairs derived from DFSDT method to fine-tune the LLaMA 7B model. Confucius [34] acknowledges the diversity in tool complexity and proposes a novel tool learning framework. ToolVerifier [127] introduces a self-verification method which distinguishes between close candidates by self-asking
contrastive questions during tool selection.
Remark. By comparing the aforementioned methods, we can find that the tuning-based method improves the capability of LLMs in tool selection by modifying model parameters. This approach can integrate extensive knowledge about tools, but it is only applicable to open-source LLMs and incurs substantial computational resource consumption. Conversely, the tuning-free method enhances the capability of LLMs in tool selection using precise prompting strategies or by modifying existing mechanisms, and it is compatible with all LLMs. However, since the possibilities for designing prompts are limitless, finding the ideal way to create the perfect prompt is still a major challenge.</p>
<p>In real-world applications, the sheer volume of available tools, coupled with constraints such as limited context length and high latency, makes it impractical to present all options to LLMs simultaneously. To address this challenge, traditional retrieval methods are typically employed first to filter and narrow down the pool of potential tools. This initial step is crucial for managing the complexity and ensuring that the subsequent LLM-based selection is both efficient and effective. After this retrieval process, the LLM can then refine the selection, making the final choice that aligns with its reasoning and preferences. This two-step approach highlights the complementary roles of both traditional retrieval methods and LLM-based techniques, demonstrating their collective importance in optimizing the tool selection process for real-world scenarios.</p>
<h3>4.4 Tool Calling</h3>
<p>In the tool calling stage, LLMs need to extract the required parameters from the user query in accordance with the specifications outlined in the tool</p>
<p>description and request data from tool servers. This process mandates that the LLMs not only correctly extract the parameters' content and format but also adhere strictly to the prescribed output format to prevent the generation of superfluous sentences. For example, an example for tool calling with GPT-4 is shown as follows:</p>
<h2>An Example for Tool Calling with GPT-4</h2>
<p>Instruction Prompt: You are currently in the tool calling stage. You are given selected tools that can be potentially used to solve the subquestion. Your goal is to extract the required parameters needed to call the tool from the subquestion based on the tool descriptions. Output in the following format: {parameter name: parameter, $\cdots$, parameter name: parameter}
Sub-question 1: What is the current price of gold per ounce in USD?
Selected Tools: Tool Name: {Metals Prices Rates API}. Tool description: {The latest API endpoint will return real-time exchange rate data updated every 60 seconds.} Required params: { [name: symbols, type: STRING, description: Enter a list of comma-separated currency codes or metal codes to limit output codes., name: base, type: STRING, description: Enter the three-letter currency code or metal code of your preferred base currency.] }
Output: {symbols: "XAU", base: "USD"}
Sub-question 2: $\cdots$
Output: $\cdots$</p>
<p>From this example, we can see that GPT-4 can extract the necessary parameters for calling a tool based on the provided user question and the selected tool's documentation. Specifically, GPT-4 can parse the critical information in the tool description and
accurately identify which parameters need to be provided. Next, we will introduce the latest developments in the same way as the previous two stages, dividing them into tuning-free methods and tuning-based methods.</p>
<h3>4.4.1 Tuning-free Methods</h3>
<p>Tuning-free methods predominantly leverage the few-shot approach to provide demonstrations for parameter extraction or rule-based methods, thereby enhancing the capability of LLMs to identify parameters [93,96,126,186]. Reverse Chain [131] utilizes reverse thinking by first selecting a final tool for a task and then having the LLMs populate the necessary parameters; if any are missing, an additional tool is chosen based on the description to complete them and accomplish the task. EasyTool [132] enhances the comprehension of LLMs regarding tool functions and parameter requirements by prompting ChatGPT to rewrite tool descriptions, making them more concise and incorporating guidelines for tool functionality directly within the descriptions. Xu et al. (2024b) [187] propose a method that compresses tool documentation into summary sequences while preserving key information, enabling efficient tool usage in LLMs with minimal performance loss. ConAgents [133] introduces a multi-agent collaborative framework, featuring a specialized execution agent tasked with parameter extraction and tool calling.</p>
<h3>4.4.2 Tuning-based Methods</h3>
<p>Some studies enhance the tool calling capabilities of LLMs using tuning-based methods [53, 127, 128]. For example, GPT4Tools [134] enhances the opensource LLMs by integrating tool usage capabilities through fine-tuning with LoRA optimization</p>
<p>techniques, using a dataset of tool usage instructions generated by ChatGPT. Toolkengpt [54] uses special tokens called "toolkens" to seamlessly call tools, switching to a special mode upon predicting a toolken to generate required input parameters and integrate the output back into the generation process. Themis [135] enhances the interpretability and scoring reliability of RMs by integrating tool usage and reasoning processes in an auto-regressive manner, dynamically determining which tools to call, how to pass parameters, and effectively incorporating the results into the reasoning process. STE [136] coordinates three key mechanisms in biological systems for the successful use of tools: trial and error, imagination, and memory, aiding LLMs in the accurate use of its trained tools.</p>
<p>Moreover, given the frequent occurrence of calling errors during the utilization of tools, such as incorrect formatting of input parameters, input parameters exceeding acceptable ranges of the tool, and tool server error, it is imperative to integrate error handling mechanisms. These mechanisms are designed to refine the action based on the error messages returned upon calling failure. This enables a more resilient and adaptive system, ensuring continuity and efficiency in tool learning even in the face of operational disruptions.</p>
<p>Remark. To sum up, although tuning-based methods can yield better results, they heavily rely on the tools seen in the training set, and perform poorly with new tools not included in the dataset. Additionally, they face challenges such as catastrophic forgetting and a lack of robust generalization. In contrast, tuning-free methods offer significant flexibility and do not require the construction of a specific dataset. So both tuning-based methods and tuning-free methods are crucial for enhancing the
tool calling capabilities of LLMs.</p>
<h3>4.5 Response Generation</h3>
<p>Given the diversity and complexity of tool outputs, which can include various formats like text, numbers, code, videos, and images, it is often impractical to present these results directly to users. Upon receiving outputs from tools, LLMs must synthesize information relevant to user queries and integrate their own knowledge to construct comprehensive responses. For example, an example for response generation with GPT-4 is shown as follows:</p>
<h2>An Example for Response Generation with GPT-4</h2>
<p>Instruction Prompt: You are currently in the response generation stage. You are given the output of selected tools that can be potentially used to solve the sub-task. Your goal is to generate a response to the user question based on the results from tools and your internal knowledge.
User Question: I would like to know the value of 5 ounces of gold plus 1 million AMZN stocks in CNY.
Return from Tool Calling: The stock price of AMZN is 130 USD, XAU USD is 1919.7605807522993, and the exchange rate between USD and YNC is 7.24.
Output: $\cdots$ Total value in CNY: Total value $=$ Value of gold in CNY + Value of stocks in CNY $=69,495.87+941,200,000=941,269,495.87$ CNY. Therefore, the combined value of 5 ounces of gold and 1 million Amazon stocks in Chinese Yuan is approximately 941, 269, 495.87 $\cdots$.</p>
<p>From this example, we can see that GPT-4 can provide a better response by combining the results from tools and its internal knowledge. Specifically, GPT-4 uses real-time data provided by tools, such</p>
<p>as the prices of gold and Amazon stock and the exchange rate, to calculate the final answer to the user's question. This demonstrates the ability of GPT-4 to integrate multiple information sources and perform complex calculations. We categorize the latest advancements in this stage into two types: direct insertion methods and information integration methods.</p>
<h3>4.5.1 Direct Insertion Methods</h3>
<p>The methods adopted in the early work involved directly inserting the output of tools into the generated response $[17,26,46,54]$. For instance, if the user query is "How is the weather today?", LLMs produce a response like "It's Weather()" (as illustrated in Figure 3), which is subsequently replaced with the result returned by the tool (e.g., from "It's Weather()." to "It's rainy."). However, given the outputs of tools are unpredictable, this method could potentially affect the user experience.</p>
<h3>4.5.2 Information Integration Methods</h3>
<p>Most methodologies opt to incorporate the output of tools into the context as input to LLMs, thereby enabling the LLMs to craft a superior reply based on the information provided by the tool [89, 189, 190]. However, due to the limited context length of LLMs, some tool outputs cannot be directly fed into them. Consequently, various methods have emerged to address this issue. For example, RestGPT [93] simplifies the lengthy results using the pre-created schema, which is a documentation that elaborates on the examples, format, and possible errors. ToolLLaMA [18] resorts to truncation, cutting the output to fit within the length constraints, which potentially loses the required information to solve the user query. Conversely, ReCOMP [137] develops a compressor to condense lengthy information into
a more succinct format, which keeps only the most useful information. ConAgents [133] proposes a schema-free method, enabling the observing agent to dynamically generate a function adaptive to extracting the target output following the instruction. And some studies suggest that refining the response generated by LLMs using the tool feedback is more effective than generating the response after invoking the tool $[51,191,192]$.</p>
<p>Remark. In conclusion, direct insertion methods embed tool outputs directly into the generated response. These approaches are straightforward but are only suitable for simple tool outputs. Conversely, information integration methods allow LLMs to process tool results to generate responses. These methods are more powerful and can provide better responses, enhancing user experience. However, future work should consider how to address issues related to overly lengthy tool outputs and the inclusion of multiple other modalities. Meanwhile, some studies highlight that LLMs can generate biased or harmful content and potentially leak sensitive information [193-195]. By incorporating external tools, whether through direct insertion methods or information integration methods, the generated responses are influenced by the tool results, which can help mitigate some of the biases and harmful content originating from the LLM itself. Despite these benefits, the introduction of external tools necessitates rigorous validation of tool outputs to prevent adversarial attacks. Without adequate validation, attackers could manipulate tool results, causing LLMs to generate harmful or malicious responses. Future work should focus on enhancing the ability of LLMs to detect harmful information within tool outputs and develop effective filtering mechanisms to prevent the generation of harmful content.</p>
<p>Table 1 A detailed list of different benchmarks and their specific configurations. Symbols (1), (2), (3), and (4) represent the four stages in tool learning-task planning, tool selection, tool calling, and response generation, respectively.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Benchmark</th>
<th style="text-align: center;">Focus</th>
<th style="text-align: center;"># Tools</th>
<th style="text-align: center;"># Instances</th>
<th style="text-align: center;">Tool Source</th>
<th style="text-align: center;">Multi-tool?</th>
<th style="text-align: center;">Executable?</th>
<th style="text-align: center;">Time</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">General Benchmarks</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">API-Bank [30]</td>
<td style="text-align: center;">(1), (2), (3), (4)</td>
<td style="text-align: center;">73</td>
<td style="text-align: center;">314</td>
<td style="text-align: center;">Manual Creation</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">2023-04</td>
</tr>
<tr>
<td style="text-align: center;">APIBench [53]</td>
<td style="text-align: center;">(2), (3)</td>
<td style="text-align: center;">1,645</td>
<td style="text-align: center;">16,450</td>
<td style="text-align: center;">Public Models</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">2023-05</td>
</tr>
<tr>
<td style="text-align: center;">ToolBench1 [33]</td>
<td style="text-align: center;">(2), (3)</td>
<td style="text-align: center;">232</td>
<td style="text-align: center;">2,746</td>
<td style="text-align: center;">Public APIs</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">2023-05</td>
</tr>
<tr>
<td style="text-align: center;">ToolAlpaca [19]</td>
<td style="text-align: center;">(2), (3), (4)</td>
<td style="text-align: center;">426</td>
<td style="text-align: center;">3,938</td>
<td style="text-align: center;">Public APIs</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">2023-06</td>
</tr>
<tr>
<td style="text-align: center;">RestBench [93]</td>
<td style="text-align: center;">(1), (2), (3)</td>
<td style="text-align: center;">94</td>
<td style="text-align: center;">157</td>
<td style="text-align: center;">RESTful APIs</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">2023-06</td>
</tr>
<tr>
<td style="text-align: center;">ToolBench2 [18]</td>
<td style="text-align: center;">(1), (2), (3), (4)</td>
<td style="text-align: center;">16,464</td>
<td style="text-align: center;">126,486</td>
<td style="text-align: center;">Rapid API</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">2023-07</td>
</tr>
<tr>
<td style="text-align: center;">MetaTool [31]</td>
<td style="text-align: center;">(1), (2)</td>
<td style="text-align: center;">199</td>
<td style="text-align: center;">21,127</td>
<td style="text-align: center;">OpenAI Plugins</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">2023-10</td>
</tr>
<tr>
<td style="text-align: center;">TaskBench [188]</td>
<td style="text-align: center;">(1), (2), (3)</td>
<td style="text-align: center;">103</td>
<td style="text-align: center;">28,271</td>
<td style="text-align: center;">Public APIs</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">2023-11</td>
</tr>
<tr>
<td style="text-align: center;">T-Eval [32]</td>
<td style="text-align: center;">(1), (2), (3)</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">533</td>
<td style="text-align: center;">Manual Creation</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">2023-12</td>
</tr>
<tr>
<td style="text-align: center;">ToolEyes [138]</td>
<td style="text-align: center;">(1), (2), (3), (4)</td>
<td style="text-align: center;">568</td>
<td style="text-align: center;">382</td>
<td style="text-align: center;">Manual Creation</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">2024-01</td>
</tr>
<tr>
<td style="text-align: center;">UltraTool [139]</td>
<td style="text-align: center;">(1), (2), (3)</td>
<td style="text-align: center;">2,032</td>
<td style="text-align: center;">5,824</td>
<td style="text-align: center;">Manual Creation</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">2024-01</td>
</tr>
<tr>
<td style="text-align: center;">API-BLEND [141]</td>
<td style="text-align: center;">(2), (3)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">189,040</td>
<td style="text-align: center;">Exsiting Datasets</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">2024-02</td>
</tr>
<tr>
<td style="text-align: center;">Seal-Tools [140]</td>
<td style="text-align: center;">(2), (3)</td>
<td style="text-align: center;">4,076</td>
<td style="text-align: center;">14,076</td>
<td style="text-align: center;">Manual Creation</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">2024-05</td>
</tr>
<tr>
<td style="text-align: center;">ShortcutsBench [142]</td>
<td style="text-align: center;">(2), (3)</td>
<td style="text-align: center;">1,414</td>
<td style="text-align: center;">7,627</td>
<td style="text-align: center;">Public APIs</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">2024-07</td>
</tr>
<tr>
<td style="text-align: center;">GTA [143]</td>
<td style="text-align: center;">(2), (3) (4)</td>
<td style="text-align: center;">14</td>
<td style="text-align: center;">229</td>
<td style="text-align: center;">Public APIs</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">2024-07</td>
</tr>
<tr>
<td style="text-align: center;">WTU-Eval [144]</td>
<td style="text-align: center;">(1)</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">916</td>
<td style="text-align: center;">BMTools</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">2024-07</td>
</tr>
<tr>
<td style="text-align: center;">AppWorld [145]</td>
<td style="text-align: center;">(1), (2), (3)</td>
<td style="text-align: center;">457</td>
<td style="text-align: center;">750</td>
<td style="text-align: center;">FastAPI</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">2024-07</td>
</tr>
<tr>
<td style="text-align: center;">Other Benchmarks</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">ToolQA [55]</td>
<td style="text-align: center;">QA</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">1,530</td>
<td style="text-align: center;">Manual Creation</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">2023-06</td>
</tr>
<tr>
<td style="text-align: center;">ToolEmu [146]</td>
<td style="text-align: center;">Safety</td>
<td style="text-align: center;">311</td>
<td style="text-align: center;">144</td>
<td style="text-align: center;">Manual Creation</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">2023-09</td>
</tr>
<tr>
<td style="text-align: center;">ToolTalk [147]</td>
<td style="text-align: center;">Conversation</td>
<td style="text-align: center;">28</td>
<td style="text-align: center;">78</td>
<td style="text-align: center;">Manual Creation</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">2023-11</td>
</tr>
<tr>
<td style="text-align: center;">VIoT [148]</td>
<td style="text-align: center;">VIoT</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">1,841</td>
<td style="text-align: center;">Public Models</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">2023-12</td>
</tr>
<tr>
<td style="text-align: center;">RoTBench [149]</td>
<td style="text-align: center;">Robustness</td>
<td style="text-align: center;">568</td>
<td style="text-align: center;">105</td>
<td style="text-align: center;">ToolEyes</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">2024-01</td>
</tr>
<tr>
<td style="text-align: center;">MLLM-Tool [88]</td>
<td style="text-align: center;">Multi-modal</td>
<td style="text-align: center;">932</td>
<td style="text-align: center;">11,642</td>
<td style="text-align: center;">Public Models</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">2024-01</td>
</tr>
<tr>
<td style="text-align: center;">ToolSword [150]</td>
<td style="text-align: center;">Safety</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">440</td>
<td style="text-align: center;">Manual Creation</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">2024-02</td>
</tr>
<tr>
<td style="text-align: center;">SciToolBench [151]</td>
<td style="text-align: center;">Sci-Reasoning</td>
<td style="text-align: center;">2,446</td>
<td style="text-align: center;">856</td>
<td style="text-align: center;">Manual Creation</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">2024-02</td>
</tr>
<tr>
<td style="text-align: center;">InjecAgent [153]</td>
<td style="text-align: center;">Safety</td>
<td style="text-align: center;">17</td>
<td style="text-align: center;">1,054</td>
<td style="text-align: center;">Public APIs</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">2024-02</td>
</tr>
<tr>
<td style="text-align: center;">StableToolBench [152]</td>
<td style="text-align: center;">Stable</td>
<td style="text-align: center;">16,464</td>
<td style="text-align: center;">126,486</td>
<td style="text-align: center;">ToolBench2</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">2024-03</td>
</tr>
<tr>
<td style="text-align: center;">m\&amp;m's [87]</td>
<td style="text-align: center;">Multi-modal</td>
<td style="text-align: center;">33</td>
<td style="text-align: center;">4,427</td>
<td style="text-align: center;">Public Models</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">2024-03</td>
</tr>
<tr>
<td style="text-align: center;">GeoLLM-QA [166]</td>
<td style="text-align: center;">Remote Sensing</td>
<td style="text-align: center;">117</td>
<td style="text-align: center;">1,000</td>
<td style="text-align: center;">Public Models</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">2024-04</td>
</tr>
<tr>
<td style="text-align: center;">ToolLens [124]</td>
<td style="text-align: center;">Tool Retrieval</td>
<td style="text-align: center;">464</td>
<td style="text-align: center;">18,770</td>
<td style="text-align: center;">ToolBench2</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">2024-05</td>
</tr>
<tr>
<td style="text-align: center;">SoAyBench [111]</td>
<td style="text-align: center;">Academic Seeking</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">792</td>
<td style="text-align: center;">AMiner</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">2024-05</td>
</tr>
<tr>
<td style="text-align: center;">ToolSandbox [155]</td>
<td style="text-align: center;">Conversation</td>
<td style="text-align: center;">34</td>
<td style="text-align: center;">1,032</td>
<td style="text-align: center;">Rapid API</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">2024-08</td>
</tr>
<tr>
<td style="text-align: center;">CToolEval [154]</td>
<td style="text-align: center;">Chinese</td>
<td style="text-align: center;">398</td>
<td style="text-align: center;">6,816</td>
<td style="text-align: center;">Public Apps</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">2024-08</td>
</tr>
</tbody>
</table>
<h2>5 Benchmarks, Toolkits, and Evaluation</h2>
<p>In this section, we systematically summarize and categorize the benchmarks, toolkits, and evaluation methods that are tailored specifically to the various
stages of tool learning. This provides a structured overview of the evaluation protocols and toolkits used to validate the effectiveness of tool learning methods, aiming to make it more convenient for readers to engage with and implement tool learning.</p>
<h3>5.1 Benchmarks</h3>
<p>With the advancement of research in tool learning, a considerable number of benchmarks have been developed and made available. In our survey, we compile a selection of 33 popular benchmarks ${ }^{1)}$, as shown in Table 1. Each benchmark evaluates distinct facets of tool learning, offering significant contributions to their respective fields. We categorize these benchmarks into two principal classes: general benchmarks and other benchmarks.</p>
<p>General Benchmarks. Given the current uncertainty regarding the capacity of LLMs to effectively utilize tools, a large number of benchmarks have been established to evaluate the tool learning proficiency of LLMs. As tool learning comprises four distinct stages, existing benchmarks focus on evaluating the capabilities of LLMs at different stages. For instance, MetaTool [31] and WTU-Eval [144] benchmarks are designed to assess whether LLMs can recognize the necessity of using tools and appropriately select the most suitable tool to fulfill user requirements. This assessment particularly focuses on the stages of task planning and tool selection. On the other hand, APIBench [53], ToolBench1 [33], API-BLEND [141], and Seal-Tools [140] concentrate on the abilities of LLMs to accurately choose the right tool and configure the correct parameters for its invocation, which correspond to the tool selection and tool calling stages, respectively. Additionally, RestBench [93], TaskBench [188], TEval [32], and UltraTool [139] extend their focus to include task planning, tool selection, and tool calling, covering three of the four stages. Subsequent studies such as API-Bank [30], ToolBench2 [18],</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>and ToolEyes [138] have provided a more comprehensive evaluation of the tool usage capabilities of LLMs, spanning all four stages of tool learning. Notably, ToolBench2 has constructed the existing largest tool learning dataset, comprising 16,464 tools and 126,486 instances. However, the tools included in these benchmarks often suffer from quality issues, such as being inaccessible or nonfunctional. Additionally, the queries in these benchmarks are typically generated by LLMs, which may not accurately reflect the true needs of users. In contrast, GTA [143], ShortcutsBench [142], and AppWorld [145] address these limitations by collecting real-world tools and generating queries driven by actual user needs.</p>
<p>Other Benchmarks. In addition to general benchmarks, there are also benchmarks specifically designed for particular tasks. For example, ToolQA [55] focuses on enhancing the question-answering capabilities of LLMs through the use of external tools, which has developed a dataset comprising questions that LLMs can only answer with the assistance of these external tools. ToolTalk [147] and ToolSandbox [155] concentrate on the ability of LLMs to utilize tools within multi-turn dialogues. VIoT [148] focuses on the capability of using Viot tools with LLMs. RoTBench [149], ToolSword [150] and ToolEmu [146] are benchmarks that emphasize the robustness and safety issues in tool learning. These benchmarks highlight the need to improve the robustness and safety of LLMs in tool learning applications. MLLM-Tool [88] and m\&amp;m's [87] extend tool learning into the multi-modal domain, assessing tool usage capabilities of LLMs in multi-modal contexts. Meanwhile, StableToolBench [152] advocates for the creation of a large-scale and stable benchmark for tool learning. SciToolBench [151]</p>
<p>introduces a novel task named tool-augmented scientific reasoning, expanding the frontier of tool learning with LLMs applications. GeoLLM-QA [166] is designed to capture complex remote sensing workflows where LLMs handle complex data structures, nuanced reasoning, and interactions with dynamic user interfaces. Moreover, ToolLens [124], acknowledging that user queries in the real world are often concise yet have ambiguous and complex intent, has created a benchmark focused on the tool retrieval stage. Building on this, SoayBench [111] introduces a specialized focus on academic information seeking, providing a comprehensive benchmark dataset along with the SOAYEval evaluation method. Lastly, CToolEval [154] extends the concept of tool learning to Chinese societal applications. The CToolEval benchmark is crafted to evaluate the performance of LLMs within the unique context of Chinese societal challenges, emphasizing the applicability and relevance of LLMs in this domain.</p>
<h3>5.2 Toolkits</h3>
<p>Recently, several open-sourced libraries and toolkits for achieving tool learning have been proposed.</p>
<p>LangChain is a framework for developing applications powered by large language models. It enables the creation of complex workflows by allowing LLMs to interact with APIs, databases, and other systems. LangChain is ideal for building intelligent assistants and automated systems. It can be accessed at the link: github.com/langchain-ai/ langchain.</p>
<p>Auto-GPT is an open-source application designed to enable large language models to autonomously perform complex tasks with minimal human input. It allows models to break down goals into subtasks and execute them sequentially, including accessing the internet and interacting with APIs. Auto-GPT
is ideal for projects requiring autonomous operation, such as automated content creation and research. It can be accessed at the link: github.com/ Significant-Gravitas/Auto-GPT.</p>
<p>BabyAGI is an open-source framework for creating autonomous AI agents that manage and execute tasks with minimal oversight. Its modular design allows developers to extend capabilities by integrating additional tools or APIs, making it ideal for flexible and scalable automation. It can be accessed at the link: github.com/yoheinakajima/babyagi.</p>
<p>BMTools [16] is an open-source repository designed to enhance LLMs by integrating them with various tools and providing a community-driven platform for developing and sharing these tools. The repository allows for easy plugin creation through simple Python functions and supports the integration of external ChatGPT plugins, making it a powerful resource for extending model capabilities. It can be accessed at the link: github.com/openbmb/ BMTools.</p>
<p>WebCPM [196] is a framework designed for developing Chinese long-form question-answering applications using interactive web search. It allows large language models to simulate human-like web browsing behaviors to retrieve and synthesize information from various sources. WebCPM excels in creating sophisticated workflows where LLMs interact with search engines, extract relevant data, and generate comprehensive answers. It can be accessed at the link: github.com/WebCPM/WebCPM.</p>
<h3>5.3 Evaluation</h3>
<p>In this section, we will introduce the evaluation methods corresponding to the four stages of tool learning.</p>
<p>Task Planning. The task planning capabilities of</p>
<p>LLMs can be evaluated in several ways. Firstly, it is crucial to assess whether LLMs correctly identify if a given query requires a external tool, measuring the accuracy of tool usage awareness [31, 139]. Next, the effectiveness of the proposed task planning in addressing the query should be evaluated, using metrics like the pass rate provided by ChatGPT [18] or human evaluations [93]. Furthermore, the precision of the plan generated by LLMs can be quantitatively analyzed by comparing it to the gold solution, ensuring its alignment and accuracy $[18,32,93]$.</p>
<p>Tool Selection. Existing works employ several metrics to evaluate the effectiveness of tool selection from different perspectives, including Recall, NDCG, and COMP.</p>
<p>Recall@ $K$ [156] is measured by calculating the proportion of selected top- $K$ tools that are present in the set of ground-truth tools:</p>
<p>$$
\operatorname{Recall} @ K=\frac{1}{|Q|} \sum_{q=1}^{|Q|} \frac{\left|T_{q}^{K} \cap T_{q}^{<em>}\right|}{\left|T_{q}^{</em>}\right|}
$$</p>
<p>where $Q$ is the set of queries, $T_{q}^{*}$ is the set of relevant tools for the query $q$, and $T_{q}^{K}$ is the top- $K$ tools for the query $q$ selected by the model.</p>
<p>NDCG@K [157] metric not only considers the proportion of positive tools but also takes into account their positions within the list:</p>
<p>$$
\begin{gathered}
\mathrm{DCG}<em i="1">{q} @ K=\sum</em>{\log }^{K} \frac{2^{g_{i}}-1<em q="1">{2}(i+1)} \
\mathrm{NDCG} @ K=\frac{1}{|Q|} \sum</em>}^{|Q|} \frac{\mathrm{DCG<em q="q">{q} @ K}{\mathrm{IDCG}</em>
\end{gathered}
$$} @ K</p>
<p>where $g_{i}$ is the graded relevance sore for the $i$ th selected tool, and $\mathrm{IDCG}_{q} @ K$ denotes ideal discounted cumulative gain at the rank position $k$.</p>
<p>COMP@K [124] is designed to measure whether
the top- $K$ selected tools form a complete set with respect to the ground-truth set:</p>
<p>$$
\operatorname{COMP} @ K=\frac{1}{|Q|} \sum_{q=1}^{|Q|} \mathbb{I}\left(\Phi_{q} \subseteq \Psi_{q}^{K}\right)
$$</p>
<p>where $\Phi_{q}$ denotes the set of ground-truth tools for query $q, \Psi_{q}^{K}$ represents the top- $K$ tools retrieved for query $q$, and $\mathbb{I}(\cdot)$ is an indicator function that returns 1 if the retrieval results include all groundtruth tools within the top- $K$ results for query $q$, and 0 otherwise.</p>
<p>Tool Calling. In the stage of tool calling, LLMs are required to generate requests for tool calling in a specified format. The effectiveness of LLMs in executing tool calling functions can be assessed by evaluating whether the parameters input by LLMs are consistent with the stipulations delineated in the tool documentation [32, 138, 139]. This assessment entails verifying whether the parameters provided match those required by the specific tool, including confirming if all required parameters are included and whether the output parameters meet the required range and format.</p>
<p>Response Generation. The ultimate goal of tool learning is to enhance the capability of LLMs to effectively address downstream tasks. Consequently, the effectiveness of tool utilization is often evaluated based on the performance in solving these downstream tasks [19, 138]. This necessitates that the LLMs consolidate information gathered throughout the entire process, providing a direct response to the user query. The quality of the final response can be assessed using metrics such as BLEU score [158], ROUGE-L [159], exact match [160], F1 [141], and other relevant indicators.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1)}$ Given the growing interest in tool learning, this survey may not encompass all benchmarks. We welcome suggestions to expand this list.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1974 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1974</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1974</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-41.html">extraction-schema-41</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer experiments for robotic manipulation that report details about actuator dynamics modeling, parameter fidelity, task characteristics, and transfer performance.</div>
                <p><strong>Paper ID:</strong> paper-279391337</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2506.11775v2.pdf" target="_blank">ExoStart: Efficient learning for dexterous manipulation with sensorized exoskeleton demonstrations</a></p>
                <p><strong>Paper Abstract:</strong> ,</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1974.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1974.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer experiments for robotic manipulation that report details about actuator dynamics modeling, parameter fidelity, task characteristics, and transfer performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ExoStart</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ExoStart: Efficient learning for dexterous manipulation with sensorized exoskeleton demonstrations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A real-to-sim-to-real pipeline that collects human demonstrations with a sensorized exoskeleton, applies sampling-based trajectory optimization (dynamics filtering) to produce dynamically-feasible simulated trajectories, bootstraps an auto-curriculum RL (DemoStart) teacher in MuJoCo, and distills to vision-based student policies (ACT) that transfer zero-shot to a Kuka+Shadow DEX-EE real robot.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Seven contact-rich dexterous tasks: Key Lock, Nut Unscrew, Peg Insertion, Box Stand, Cube Flip, Case Open, Bulb Install</td>
                        </tr>
                        <tr>
                            <td><strong>task_timescale</strong></td>
                            <td>10s per episode for most tasks; 15s for Nut Unscrew (episode limits reported)</td>
                        </tr>
                        <tr>
                            <td><strong>task_contact_ratio</strong></td>
                            <td>contact-rich</td>
                        </tr>
                        <tr>
                            <td><strong>task_precision_requirement</strong></td>
                            <td>high precision (tight insertions and alignment for tasks such as Peg Insertion and Bulb Install); no numerical mm tolerances reported</td>
                        </tr>
                        <tr>
                            <td><strong>actuator_parameters_modeled</strong></td>
                            <td>Simulated dynamics in MuJoCo with randomized physical parameters including object and link mass and inertia, friction (object and links), joint armature, joint damping, and joint friction-loss; simulation/control integration at 200 Hz (simulation state/control update); the arm control loop modeled as Cartesian velocity-to-joint mapping (differential IK) and the hand modeled as a joint-position controlled 12-DoF Shadow DEX-EE in simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>actuator_parameters_simplified</strong></td>
                            <td>No explicit low-level motor dynamics (motor bandwidth, torque limits, electrical dynamics), no explicit backlash or detailed actuator compliance models; joint controllers modeled as idealized joint-position controllers; contact modeled with MuJoCo soft-contact (known to allow penetration/artifacts).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level_description</strong></td>
                            <td>High-frequency MuJoCo physics simulation (200 Hz) with domain randomization over physical properties and visual randomization, but simplified actuator/motor-level models (ideal joint controllers) and known contact-model simplifications.</td>
                        </tr>
                        <tr>
                            <td><strong>parameter_specific_fidelity</strong></td>
                            <td>Reported numerical randomization/augmentation: object pose offsets sampled in XY in [-1, +1] cm and angle offsets in [-0.1, +0.1] rad; external perturbations applied after grasp uniformly sampled in [0,1] N (or zero with 0.5 probability); photometric image augmentations and Gaussian image noise (std not always for dynamics). No quantitative error bounds reported for friction/mass/inertia/armature/damping randomization (i.e., ranges for those dynamics parameters are not specified). Control/update rates: sim at 200 Hz, real env run at 20 Hz; hand controller at 1 kHz on real robot.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Task success rate. Teacher (feature-based) policies in simulation: typically >=95% success (evaluated over >250,000 episodes) except Nut Unscrew at 89%; Distilled vision-based student policies evaluated on the real robot: typically >50% success over 50 episodes for most tasks, with Bulb Install at 2% success (student).</td>
                        </tr>
                        <tr>
                            <td><strong>sim_vs_real_performance</strong></td>
                            <td>Substantial degradation in many cases: teacher sim success (≈95%+) vs student real success typically >50% (i.e., a drop on the order of tens of percentage points); Bulb Install shows near-total failure despite high sim performance (large sim->real gap).</td>
                        </tr>
                        <tr>
                            <td><strong>sensitivity_analysis_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>sensitivity_analysis_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_reported</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td>Dynamics filtering planner run in a loop for ~3 hours per real-world demonstration to generate diverse feasible trajectories; for distillation, 250,000 simulation episodes were rolled out per task to collect observation-action pairs; ablation showed training with raw (unfiltered) data required ~6x more policy updates for Peg Insertion (18M vs 3M updates). Planner sampling settings reported (e.g., 40 sampling trajectories, sampling spline points=3).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison</strong></td>
                            <td>Yes — ablations compare (a) using dynamics-filtered trajectories vs using raw unfiltered demonstrations: dynamics filtering substantially improved learning efficiency and convergence (reduced training updates dramatically); (b) they note that a refined simulation model could improve sim-to-real transfer but sometimes decreased simulated teacher performance (Nut Unscrew anecdote); (c) control-rate mismatch (MJPC at 200 Hz vs deployment environment at 20 Hz) caused behavior cloning from MJPC to fail in simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>Physical randomization: friction, mass, inertia of objects and robot links, joint armature, joint damping, and joint friction-loss were randomized (no precise numeric ranges given). Object initial pose offsets: position ±1 cm (XY) and angle ±0.1 rad. External force perturbations applied after grasp for 10 timesteps drawn from [0,1] N (or zero with 0.5 probability). Visual randomization: camera locations, lighting, object color/texture; photometric image distortions: brightness in [-0.5, 0.5], contrast in [0.5, 2.0], hue in [-0.5, 0.5], saturation in [0.5, 1.5]; additional Gaussian noise added to images.</td>
                        </tr>
                        <tr>
                            <td><strong>robot_type</strong></td>
                            <td>7-DoF Kuka LBR iiwa 14 arm + Shadow DEX-EE multi-fingered hand (hand: 12 DoF; three 4-DoF fingers), fixed-base manipulator setup with multiple fixed cameras and two wrist cameras</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_analysis</strong></td>
                            <td>Identified primary failure causes: (1) contact-model mismatch in MuJoCo (soft-contact allowed object penetration) leading to agents exploiting simulation artifacts (explains Bulb Install failures); (2) control-rate and low-level actuator mismatch (e.g., MJPC tied to 200 Hz sim integration vs real environment at 20 Hz) created a dynamics gap; (3) insufficient diversity in demonstrations (single-demo seed reduced real success); policies can also deviate from demonstrated strategies (emergent behaviors) that reduce real-world robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_for_theory</strong></td>
                            <td>High-level dynamics filtering plus domain randomization over gross physical parameters (mass, inertia, friction, armature, damping) and using demonstration-bootstrapped auto-curriculum RL enables substantial sim-to-real transfer for complex dexterous tasks, but two classes of actuator/dynamics fidelity are critical: accurate contact modeling (contact fidelity) and matching low-level actuator/control-rate dynamics (controller bandwidth and update-rate). Failures (e.g., Bulb Install) demonstrate that poor contact fidelity or mismatched control-rate/actuator representations can produce simulation exploits that catastrophically degrade real-world transfer.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1974.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1974.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer experiments for robotic manipulation that report details about actuator dynamics modeling, parameter fidelity, task characteristics, and transfer performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BulbInstall (task)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bulb Install (bayonet bulb insertion)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A stringent insertion task requiring grasp, in-hand re-orientation, and accurate alignment of bulb insertion pins into a bayonet socket; in this paper it shows near-zero zero-shot real-world transfer despite high simulation training performance due to contact-model mismatch.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Bulb Install: grasp the bulb, re-orient in-hand, and install into bayonet socket</td>
                        </tr>
                        <tr>
                            <td><strong>task_timescale</strong></td>
                            <td>10s episode limit (standard unless otherwise specified)</td>
                        </tr>
                        <tr>
                            <td><strong>task_contact_ratio</strong></td>
                            <td>contact-rich (tight insertion with contact against socket walls and precise pin alignment)</td>
                        </tr>
                        <tr>
                            <td><strong>task_precision_requirement</strong></td>
                            <td>high precision (slot alignment of insertion pins required); no numerical tolerance reported</td>
                        </tr>
                        <tr>
                            <td><strong>actuator_parameters_modeled</strong></td>
                            <td>Simulation included MuJoCo contact model (soft-contact) and randomized physical properties as for other tasks (mass/inertia/friction/armature/damping), and used simulated joint-position control at 200 Hz.</td>
                        </tr>
                        <tr>
                            <td><strong>actuator_parameters_simplified</strong></td>
                            <td>Contact model in MuJoCo allowed penetration (soft contact) — i.e., contact physics are simplified/approximate; no explicit low-level motor/electrical/actuator bandwidth or backlash modeling specified.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level_description</strong></td>
                            <td>MuJoCo-level physics with randomized gross dynamics parameters but low-fidelity contact interactions (soft-contact penetration) and simplified actuator models.</td>
                        </tr>
                        <tr>
                            <td><strong>parameter_specific_fidelity</strong></td>
                            <td>Contact model fidelity explicitly called out as poor (penetration observed); no quantitative fidelity or error bounds reported for actuator parameters (bandwidth/friction accuracy not quantified).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Task success rate on real robot: 2% (student policy, evaluated over 50 episodes). Simulation teacher success for tasks generally reported >=95% (not explicitly broken out per task in text), indicating a large sim->real discrepancy for this task.</td>
                        </tr>
                        <tr>
                            <td><strong>sim_vs_real_performance</strong></td>
                            <td>Very large gap: high sim performance (teacher policies generally high) vs student real success 2% for Bulb Install.</td>
                        </tr>
                        <tr>
                            <td><strong>sensitivity_analysis_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>sensitivity_analysis_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_reported</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison</strong></td>
                            <td>Yes — qualitative comparison: refined simulation model variants improved sim-to-real transfer in some cases (authors note a refined sim model reduced simulated teacher performance but improved transfer for Nut Unscrew), while for Bulb Install the contact-model inaccuracy explains the failure.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>Same domain randomization as main pipeline (physical parameters, object pose offsets ±1 cm, angle ±0.1 rad, external perturbations up to 1 N, visual randomization and photometric distortions).</td>
                        </tr>
                        <tr>
                            <td><strong>robot_type</strong></td>
                            <td>Kuka LBR iiwa 14 arm with Shadow DEX-EE hand</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_analysis</strong></td>
                            <td>Primary identified cause: MuJoCo soft-contact model permitted object penetration through thin connector walls during simulated insertion; agent learned to exploit this, producing behaviors not transferable to the real bayonet insertion where penetration is impossible.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_for_theory</strong></td>
                            <td>For tight insertion contact-heavy tasks, contact-model fidelity (avoiding penetrations, accurate contact stiffness/penetration behavior) is a dominant factor for successful sim-to-real transfer; without accurate contact physics, policies can learn non-physical exploits that destroy real-world performance even when other dynamics parameters are randomized.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1974.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1974.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer experiments for robotic manipulation that report details about actuator dynamics modeling, parameter fidelity, task characteristics, and transfer performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DynamicsFilter_MJPC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sampling-based trajectory optimization dynamics filtering (MJPC / predictive sampling)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sampling-based trajectory optimizer (MJPC predictive sampling implementation) is used as a dynamics filter to turn noisy exoskeleton demonstration recordings into multiple dynamically-feasible simulated trajectories, by sampling control sequences for short horizons and selecting those that minimize tracking costs; these filtered trajectories bootstrap DemoStart RL.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Preprocessing stage applied to all demonstration-driven tasks (produces S_demo dynamically feasible state set used during RL)</td>
                        </tr>
                        <tr>
                            <td><strong>task_timescale</strong></td>
                            <td>Planner and simulation run at 200 Hz; per-demo planner runs for ~3 hours to collect diverse filtered trajectories; agent planning horizon/configs reported (agent horizon 0.25 s, agent timestep 0.25 s in Table 4 — planner sampling parameters provided).</td>
                        </tr>
                        <tr>
                            <td><strong>task_contact_ratio</strong></td>
                            <td>contact-rich (tracking includes fingertip-to-object-keypoint distances and enforces contact constraints implicitly via simulator dynamics).</td>
                        </tr>
                        <tr>
                            <td><strong>task_precision_requirement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>actuator_parameters_modeled</strong></td>
                            <td>Planner applies control sequences to the MuJoCo simulated robot model with state/control integration at 200 Hz; joint dynamics and contact interactions are simulated as in MuJoCo; control inputs are sampled sequences (no explicit separate low-level actuator electrical model).</td>
                        </tr>
                        <tr>
                            <td><strong>actuator_parameters_simplified</strong></td>
                            <td>Low-level actuator dynamics (bandwidth, electrical actuator dynamics, actuator delays/backlash/compliance) are not explicitly modeled; zero-order action hold and other ad-hoc mitigations were mentioned downstream because of control-rate mismatches.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level_description</strong></td>
                            <td>High-frequency trajectory optimization on MuJoCo dynamics (200 Hz), producing dynamically consistent trajectories under the MuJoCo contact and joint models — but does not capture detailed motor-level/servo dynamics or reduce contact-model simplifications.</td>
                        </tr>
                        <tr>
                            <td><strong>parameter_specific_fidelity</strong></td>
                            <td>Planner configuration reported (e.g., 40 sampling trajectories, sampling spline points=3, sampling exploration=0.08); simulation/control at 200 Hz; number of filtered trajectories per task reported (25–150). No explicit fidelity percentages for actuator parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Number of successfully filtered trajectories used for RL per task (25–150); qualitative effect on learning: policies seeded with filtered trajectories converged faster and more robustly; ablation: training with raw demonstrations (no filtering) either failed (Box Stand) or took ~6x more updates (Peg Insertion: 18M vs 3M).</td>
                        </tr>
                        <tr>
                            <td><strong>sim_vs_real_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sensitivity_analysis_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>sensitivity_analysis_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_reported</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td>Planner ran for ~3 hours per demonstration in experiments to generate diverse trajectories; planner sampling and horizon parameters reported; simulation and planner run at 200 Hz. Using filtered data reduced RL training requirements substantially compared to raw data.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison</strong></td>
                            <td>Dynamics-filtered data vs raw recorded data: filtered data produced much faster RL convergence and improved robustness; however, attempting to behavioral-clone directly from MJPC (filtered) episodes failed due to control-rate mismatch between MJPC (200 Hz) and the real robot environment (20 Hz), showing a fidelity mismatch between planner/simulation control rates and deployment.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>Filtering step includes augmentation of object poses and shapes (object pose offsets, shape/size replacements) to create diverse S_demo; explicit actuator parameter randomization is applied later during RL training rather than during dynamics filtering.</td>
                        </tr>
                        <tr>
                            <td><strong>robot_type</strong></td>
                            <td>Used with the same Kuka LBR iiwa 14 + Shadow DEX-EE robot model in MuJoCo</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_analysis</strong></td>
                            <td>Key failure mode when using MJPC outputs directly: control-rate mismatch (MJPC tied to 200 Hz integration; real robot environment run at 20 Hz) introduced a dynamics gap that produced zero success when behavior-cloning directly from MJPC episodes. This highlights the need to match control/update rates and low-level actuator abstractions between sim and real.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_for_theory</strong></td>
                            <td>Dynamics filtering produces higher-quality, dynamically-feasible trajectories that markedly speed RL training and improve robustness, but the benefit depends on matching the simulator's control/update dynamics to the deployment controller (control-rate and actuator abstraction mismatches can negate advantages and cause transfer failure).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Demostart: Demonstration-led auto-curriculum applied to sim-to-real with multi-fingered robots. <em>(Rating: 2)</em></li>
                <li>Predictive sampling: Real-time behaviour synthesis with mujoco. <em>(Rating: 2)</em></li>
                <li>Learning complex dexterous manipulation with deep reinforcement learning and demonstrations <em>(Rating: 1)</em></li>
                <li>Learning dexterity in-hand manipulation <em>(Rating: 1)</em></li>
                <li>ContactMPC: Towards online adaptive control for contact-rich dexterous manipulation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1974",
    "paper_id": "paper-279391337",
    "extraction_schema_id": "extraction-schema-41",
    "extracted_data": [
        {
            "name_short": "ExoStart",
            "name_full": "ExoStart: Efficient learning for dexterous manipulation with sensorized exoskeleton demonstrations",
            "brief_description": "A real-to-sim-to-real pipeline that collects human demonstrations with a sensorized exoskeleton, applies sampling-based trajectory optimization (dynamics filtering) to produce dynamically-feasible simulated trajectories, bootstraps an auto-curriculum RL (DemoStart) teacher in MuJoCo, and distills to vision-based student policies (ACT) that transfer zero-shot to a Kuka+Shadow DEX-EE real robot.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_name": "Seven contact-rich dexterous tasks: Key Lock, Nut Unscrew, Peg Insertion, Box Stand, Cube Flip, Case Open, Bulb Install",
            "task_timescale": "10s per episode for most tasks; 15s for Nut Unscrew (episode limits reported)",
            "task_contact_ratio": "contact-rich",
            "task_precision_requirement": "high precision (tight insertions and alignment for tasks such as Peg Insertion and Bulb Install); no numerical mm tolerances reported",
            "actuator_parameters_modeled": "Simulated dynamics in MuJoCo with randomized physical parameters including object and link mass and inertia, friction (object and links), joint armature, joint damping, and joint friction-loss; simulation/control integration at 200 Hz (simulation state/control update); the arm control loop modeled as Cartesian velocity-to-joint mapping (differential IK) and the hand modeled as a joint-position controlled 12-DoF Shadow DEX-EE in simulation.",
            "actuator_parameters_simplified": "No explicit low-level motor dynamics (motor bandwidth, torque limits, electrical dynamics), no explicit backlash or detailed actuator compliance models; joint controllers modeled as idealized joint-position controllers; contact modeled with MuJoCo soft-contact (known to allow penetration/artifacts).",
            "fidelity_level_description": "High-frequency MuJoCo physics simulation (200 Hz) with domain randomization over physical properties and visual randomization, but simplified actuator/motor-level models (ideal joint controllers) and known contact-model simplifications.",
            "parameter_specific_fidelity": "Reported numerical randomization/augmentation: object pose offsets sampled in XY in [-1, +1] cm and angle offsets in [-0.1, +0.1] rad; external perturbations applied after grasp uniformly sampled in [0,1] N (or zero with 0.5 probability); photometric image augmentations and Gaussian image noise (std not always for dynamics). No quantitative error bounds reported for friction/mass/inertia/armature/damping randomization (i.e., ranges for those dynamics parameters are not specified). Control/update rates: sim at 200 Hz, real env run at 20 Hz; hand controller at 1 kHz on real robot.",
            "transfer_success_metric": "Task success rate. Teacher (feature-based) policies in simulation: typically &gt;=95% success (evaluated over &gt;250,000 episodes) except Nut Unscrew at 89%; Distilled vision-based student policies evaluated on the real robot: typically &gt;50% success over 50 episodes for most tasks, with Bulb Install at 2% success (student).",
            "sim_vs_real_performance": "Substantial degradation in many cases: teacher sim success (≈95%+) vs student real success typically &gt;50% (i.e., a drop on the order of tens of percentage points); Bulb Install shows near-total failure despite high sim performance (large sim-&gt;real gap).",
            "sensitivity_analysis_performed": false,
            "sensitivity_analysis_results": null,
            "computational_cost_reported": true,
            "computational_cost_details": "Dynamics filtering planner run in a loop for ~3 hours per real-world demonstration to generate diverse feasible trajectories; for distillation, 250,000 simulation episodes were rolled out per task to collect observation-action pairs; ablation showed training with raw (unfiltered) data required ~6x more policy updates for Peg Insertion (18M vs 3M updates). Planner sampling settings reported (e.g., 40 sampling trajectories, sampling spline points=3).",
            "fidelity_comparison": "Yes — ablations compare (a) using dynamics-filtered trajectories vs using raw unfiltered demonstrations: dynamics filtering substantially improved learning efficiency and convergence (reduced training updates dramatically); (b) they note that a refined simulation model could improve sim-to-real transfer but sometimes decreased simulated teacher performance (Nut Unscrew anecdote); (c) control-rate mismatch (MJPC at 200 Hz vs deployment environment at 20 Hz) caused behavior cloning from MJPC to fail in simulation.",
            "domain_randomization_used": true,
            "domain_randomization_details": "Physical randomization: friction, mass, inertia of objects and robot links, joint armature, joint damping, and joint friction-loss were randomized (no precise numeric ranges given). Object initial pose offsets: position ±1 cm (XY) and angle ±0.1 rad. External force perturbations applied after grasp for 10 timesteps drawn from [0,1] N (or zero with 0.5 probability). Visual randomization: camera locations, lighting, object color/texture; photometric image distortions: brightness in [-0.5, 0.5], contrast in [0.5, 2.0], hue in [-0.5, 0.5], saturation in [0.5, 1.5]; additional Gaussian noise added to images.",
            "robot_type": "7-DoF Kuka LBR iiwa 14 arm + Shadow DEX-EE multi-fingered hand (hand: 12 DoF; three 4-DoF fingers), fixed-base manipulator setup with multiple fixed cameras and two wrist cameras",
            "transfer_failure_analysis": "Identified primary failure causes: (1) contact-model mismatch in MuJoCo (soft-contact allowed object penetration) leading to agents exploiting simulation artifacts (explains Bulb Install failures); (2) control-rate and low-level actuator mismatch (e.g., MJPC tied to 200 Hz sim integration vs real environment at 20 Hz) created a dynamics gap; (3) insufficient diversity in demonstrations (single-demo seed reduced real success); policies can also deviate from demonstrated strategies (emergent behaviors) that reduce real-world robustness.",
            "key_finding_for_theory": "High-level dynamics filtering plus domain randomization over gross physical parameters (mass, inertia, friction, armature, damping) and using demonstration-bootstrapped auto-curriculum RL enables substantial sim-to-real transfer for complex dexterous tasks, but two classes of actuator/dynamics fidelity are critical: accurate contact modeling (contact fidelity) and matching low-level actuator/control-rate dynamics (controller bandwidth and update-rate). Failures (e.g., Bulb Install) demonstrate that poor contact fidelity or mismatched control-rate/actuator representations can produce simulation exploits that catastrophically degrade real-world transfer.",
            "uuid": "e1974.0"
        },
        {
            "name_short": "BulbInstall (task)",
            "name_full": "Bulb Install (bayonet bulb insertion)",
            "brief_description": "A stringent insertion task requiring grasp, in-hand re-orientation, and accurate alignment of bulb insertion pins into a bayonet socket; in this paper it shows near-zero zero-shot real-world transfer despite high simulation training performance due to contact-model mismatch.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_name": "Bulb Install: grasp the bulb, re-orient in-hand, and install into bayonet socket",
            "task_timescale": "10s episode limit (standard unless otherwise specified)",
            "task_contact_ratio": "contact-rich (tight insertion with contact against socket walls and precise pin alignment)",
            "task_precision_requirement": "high precision (slot alignment of insertion pins required); no numerical tolerance reported",
            "actuator_parameters_modeled": "Simulation included MuJoCo contact model (soft-contact) and randomized physical properties as for other tasks (mass/inertia/friction/armature/damping), and used simulated joint-position control at 200 Hz.",
            "actuator_parameters_simplified": "Contact model in MuJoCo allowed penetration (soft contact) — i.e., contact physics are simplified/approximate; no explicit low-level motor/electrical/actuator bandwidth or backlash modeling specified.",
            "fidelity_level_description": "MuJoCo-level physics with randomized gross dynamics parameters but low-fidelity contact interactions (soft-contact penetration) and simplified actuator models.",
            "parameter_specific_fidelity": "Contact model fidelity explicitly called out as poor (penetration observed); no quantitative fidelity or error bounds reported for actuator parameters (bandwidth/friction accuracy not quantified).",
            "transfer_success_metric": "Task success rate on real robot: 2% (student policy, evaluated over 50 episodes). Simulation teacher success for tasks generally reported &gt;=95% (not explicitly broken out per task in text), indicating a large sim-&gt;real discrepancy for this task.",
            "sim_vs_real_performance": "Very large gap: high sim performance (teacher policies generally high) vs student real success 2% for Bulb Install.",
            "sensitivity_analysis_performed": false,
            "sensitivity_analysis_results": null,
            "computational_cost_reported": false,
            "computational_cost_details": null,
            "fidelity_comparison": "Yes — qualitative comparison: refined simulation model variants improved sim-to-real transfer in some cases (authors note a refined sim model reduced simulated teacher performance but improved transfer for Nut Unscrew), while for Bulb Install the contact-model inaccuracy explains the failure.",
            "domain_randomization_used": true,
            "domain_randomization_details": "Same domain randomization as main pipeline (physical parameters, object pose offsets ±1 cm, angle ±0.1 rad, external perturbations up to 1 N, visual randomization and photometric distortions).",
            "robot_type": "Kuka LBR iiwa 14 arm with Shadow DEX-EE hand",
            "transfer_failure_analysis": "Primary identified cause: MuJoCo soft-contact model permitted object penetration through thin connector walls during simulated insertion; agent learned to exploit this, producing behaviors not transferable to the real bayonet insertion where penetration is impossible.",
            "key_finding_for_theory": "For tight insertion contact-heavy tasks, contact-model fidelity (avoiding penetrations, accurate contact stiffness/penetration behavior) is a dominant factor for successful sim-to-real transfer; without accurate contact physics, policies can learn non-physical exploits that destroy real-world performance even when other dynamics parameters are randomized.",
            "uuid": "e1974.1"
        },
        {
            "name_short": "DynamicsFilter_MJPC",
            "name_full": "Sampling-based trajectory optimization dynamics filtering (MJPC / predictive sampling)",
            "brief_description": "A sampling-based trajectory optimizer (MJPC predictive sampling implementation) is used as a dynamics filter to turn noisy exoskeleton demonstration recordings into multiple dynamically-feasible simulated trajectories, by sampling control sequences for short horizons and selecting those that minimize tracking costs; these filtered trajectories bootstrap DemoStart RL.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_name": "Preprocessing stage applied to all demonstration-driven tasks (produces S_demo dynamically feasible state set used during RL)",
            "task_timescale": "Planner and simulation run at 200 Hz; per-demo planner runs for ~3 hours to collect diverse filtered trajectories; agent planning horizon/configs reported (agent horizon 0.25 s, agent timestep 0.25 s in Table 4 — planner sampling parameters provided).",
            "task_contact_ratio": "contact-rich (tracking includes fingertip-to-object-keypoint distances and enforces contact constraints implicitly via simulator dynamics).",
            "task_precision_requirement": null,
            "actuator_parameters_modeled": "Planner applies control sequences to the MuJoCo simulated robot model with state/control integration at 200 Hz; joint dynamics and contact interactions are simulated as in MuJoCo; control inputs are sampled sequences (no explicit separate low-level actuator electrical model).",
            "actuator_parameters_simplified": "Low-level actuator dynamics (bandwidth, electrical actuator dynamics, actuator delays/backlash/compliance) are not explicitly modeled; zero-order action hold and other ad-hoc mitigations were mentioned downstream because of control-rate mismatches.",
            "fidelity_level_description": "High-frequency trajectory optimization on MuJoCo dynamics (200 Hz), producing dynamically consistent trajectories under the MuJoCo contact and joint models — but does not capture detailed motor-level/servo dynamics or reduce contact-model simplifications.",
            "parameter_specific_fidelity": "Planner configuration reported (e.g., 40 sampling trajectories, sampling spline points=3, sampling exploration=0.08); simulation/control at 200 Hz; number of filtered trajectories per task reported (25–150). No explicit fidelity percentages for actuator parameters.",
            "transfer_success_metric": "Number of successfully filtered trajectories used for RL per task (25–150); qualitative effect on learning: policies seeded with filtered trajectories converged faster and more robustly; ablation: training with raw demonstrations (no filtering) either failed (Box Stand) or took ~6x more updates (Peg Insertion: 18M vs 3M).",
            "sim_vs_real_performance": null,
            "sensitivity_analysis_performed": false,
            "sensitivity_analysis_results": null,
            "computational_cost_reported": true,
            "computational_cost_details": "Planner ran for ~3 hours per demonstration in experiments to generate diverse trajectories; planner sampling and horizon parameters reported; simulation and planner run at 200 Hz. Using filtered data reduced RL training requirements substantially compared to raw data.",
            "fidelity_comparison": "Dynamics-filtered data vs raw recorded data: filtered data produced much faster RL convergence and improved robustness; however, attempting to behavioral-clone directly from MJPC (filtered) episodes failed due to control-rate mismatch between MJPC (200 Hz) and the real robot environment (20 Hz), showing a fidelity mismatch between planner/simulation control rates and deployment.",
            "domain_randomization_used": true,
            "domain_randomization_details": "Filtering step includes augmentation of object poses and shapes (object pose offsets, shape/size replacements) to create diverse S_demo; explicit actuator parameter randomization is applied later during RL training rather than during dynamics filtering.",
            "robot_type": "Used with the same Kuka LBR iiwa 14 + Shadow DEX-EE robot model in MuJoCo",
            "transfer_failure_analysis": "Key failure mode when using MJPC outputs directly: control-rate mismatch (MJPC tied to 200 Hz integration; real robot environment run at 20 Hz) introduced a dynamics gap that produced zero success when behavior-cloning directly from MJPC episodes. This highlights the need to match control/update rates and low-level actuator abstractions between sim and real.",
            "key_finding_for_theory": "Dynamics filtering produces higher-quality, dynamically-feasible trajectories that markedly speed RL training and improve robustness, but the benefit depends on matching the simulator's control/update dynamics to the deployment controller (control-rate and actuator abstraction mismatches can negate advantages and cause transfer failure).",
            "uuid": "e1974.2"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Demostart: Demonstration-led auto-curriculum applied to sim-to-real with multi-fingered robots.",
            "rating": 2
        },
        {
            "paper_title": "Predictive sampling: Real-time behaviour synthesis with mujoco.",
            "rating": 2
        },
        {
            "paper_title": "Learning complex dexterous manipulation with deep reinforcement learning and demonstrations",
            "rating": 1
        },
        {
            "paper_title": "Learning dexterity in-hand manipulation",
            "rating": 1
        },
        {
            "paper_title": "ContactMPC: Towards online adaptive control for contact-rich dexterous manipulation",
            "rating": 1
        }
    ],
    "cost": 0.01860525,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>ExoStart: Efficient learning for dexterous manipulation with sensorized exoskeleton demonstrations
2 Sep 2025</p>
<p>Zilin Si 
Carnegie Mellon University</p>
<p>Google DeepMind</p>
<p>Jose Enrique Chen 
Google DeepMind</p>
<p>M Emre Karagozler 
Google DeepMind</p>
<p>Antonia Bronars 
Massachusetts Institute of Technology</p>
<p>Jonathan Hutchinson 
Google DeepMind</p>
<p>Thomas Lampe 
Google DeepMind</p>
<p>Nimrod Gileadi 
Google DeepMind</p>
<p>Taylor Howell 
Google DeepMind</p>
<p>Stefano Saliceti 
Google DeepMind</p>
<p>Lukasz Barczyk 
Google DeepMind</p>
<p>Ilan Olivarez Correa 
Google DeepMind</p>
<p>Tom Erez 
Google DeepMind</p>
<p>Mohit Shridhar 
Google DeepMind</p>
<p>Murilo Fernandes Martins 
Google DeepMind</p>
<p>Konstantinos Bousmalis 
Google DeepMind</p>
<p>Nicolas Heess 
Google DeepMind</p>
<p>Francesco Nori 
Google DeepMind</p>
<p>Maria Bauza Villalonga 
Google DeepMind</p>
<p>ExoStart: Efficient learning for dexterous manipulation with sensorized exoskeleton demonstrations
2 Sep 2025AF07510B989CB489B1C7DE43D4CDF66FarXiv:2506.11775v2[cs.RO]Multi-finger robotic handsin-hand manipulationlearning from demonstrationstrajectory optimizationcurriculum learningsim2real transfer
Recent advancements in teleoperation systems have enabled highquality data collection for robotic manipulators, showing impressive results in learning manipulation at scale.This progress suggests that extending these capabilities to robotic hands could unlock an even broader range of manipulation skills, especially if we could achieve the same level of dexterity that human hands exhibit.However, teleoperating robotic hands is far from a solved problem, as it presents a significant challenge due to the high degrees of freedom of robotic hands and the complex dynamics occurring during contact-rich settings.In this work, we present ExoStart, a general and scalable learning framework that leverages human dexterity to improve robotic hand control.In particular, we obtain high-quality data by collecting direct demonstrations without a robot in the loop using a sensorized low-cost wearable exoskeleton, capturing the rich behaviors that humans can demonstrate with their own hands.We also propose a simulation-based dynamics filter that generates dynamically feasible trajectories from the collected demonstrations and use the generated trajectories to bootstrap an auto-curriculum reinforcement learning method that relies only on simple sparse rewards.The ExoStart pipeline is generalizable and yields robust policies that transfer zero-shot to the real robot.Our results demonstrate that ExoStart can generate dexterous real-world hand skills, achieving a success rate above 50% on a wide range of complex tasks such as opening an AirPods case or inserting and turning a key in a lock.More details and videos can be found in https://sites.google.com/view/exostart.</p>
<p>Introduction</p>
<p>While robotic hands aim to replicate human dexterity, achieving versatile and dexterous manipulation still depends critically on developing reliable and generalizable control methods.Recent advances on learning-based approaches for control have demonstrated promising results by successfully learning from high-quality data collected on systems with simple manipulators [2,3].However, extending these strategies to robotic hands remains challenging as it requires access to robust and scalable data collection systems, particularly when collecting high-quality data under contact-rich dynamics.</p>
<p>Despite recent progress in developing data collection systems, current methods remain limited and far from fully unlocking the dexterous potential of robotic hands.A common strategy involves We collect demonstrations with a sensorized exoskeleton by directly interacting with the object in the real world; (b) Dynamics filtering: We apply trajectory optimization to recover dynamically feasible simulated trajectories from the raw demonstrations; (c) Auto-curriculum RL and distillation: We use an auto-curriculum RL method [1] to train a teacher policy from the filtered trajectories, and then distill it into a vision-based student policy that transfers zero-shot to the real world.</p>
<p>teleoperating the robotic hands, often using wearable gloves [4,5,6,7], VR devices [8,9,10], or vision-based human hand tracking [11,12,13,14].However, these approaches often struggle with kinematic and dynamic mismatches between human and robotic hands, necessitating complex and specialized motion re-targeting, which often slows down and limits the expressiveness of the demonstrations.To address this, some approaches use a kinematic replica of the robotic hand for teleoperation [15,16,17], which eliminates the kinematic gap and avoids motion re-targeting.Yet, these systems still rely on physical robots for data collection, leading to scalability challenges in terms of maintenance, complexity, and cost.Moreover, the inherent limitations of teleoperation, such as restricted haptic feedback, visibility, and latency, further hinder the demonstration of fine, dexterous, and reactive manipulation.This work introduces a low-cost and scalable approach for collecting real-world demonstrations that capture the nuances of human dexterity without relying on a physical robot.Our solution, Ex-oStart, is based on developing a sensorized wearable exoskeleton that is kinematically equivalent to a robotic hand.We design this exoskeleton as a glove, which enables operators to directly manipulate objects and feel the physical interactions as they demonstrate a task, as shown in Fig. 1 (a).Embedded sensors on each joint record the finger motions of the exoskeleton, ensuring that the movements can be directly mapped to the robotic hand and thus avoiding the motion re-targeting problem.To mitigate noise and inconsistencies inherent in the real-world sensor data, we develop a dynamics filter [18] to recover trajectories that respect the simulated environment's dynamics.We then use the resultant trajectories to bootstrap an auto-curriculum RL method [1] based on sparse rewards, to efficiently learn policies that can transfer zero-shot to the real robot.</p>
<p>Our proposed modular pipeline, ExoStart (Fig. 1), provides a reliable and robust recipe for collecting high-quality demonstrations and learning policies for real-world autonomous dexterous manipulation with robotic hands.Specifically, our main contributions include: (1) a scalable data collection setup based-on a low-cost sensorized exoskeleton for capturing expressive real-world human demonstrations without requiring a robot system in the loop; (2) a real-to-sim-to-real learning pipeline using dynamics filtering and auto-curriculum RL to efficiently learn policies that transfer to the real robot from a handful demonstrations and minimal reward function design; and (3) a comprehensive experimental validation and ablations demonstrating the effectiveness, scalability, and robustness of our method across diverse challenging dexterous tasks (Fig. 3).</p>
<p>Related Work</p>
<p>Imitation learning and teleoperation</p>
<p>Imitation learning enables autonomous robots by learning policies from expert demonstrations.Recent advances have focused on improving teleoperation systems to enhance the quality of expert data collection [19,20,15,16,7].While these systems have significantly advanced robotic manipulation, they remain constrained by the inherent limitations of teleoperation, such as restricted operator feedback, system complexity, and latency.To mitigate some of these limitations, kinesthetic teaching offers an alternative by allowing operators to physically guide the robot in real-world demonstrations.Earlier studies in kinesthetic teaching primarily focused on controlling robotic arms and parallel jaw grippers [21,22].More recently, [23] extended this approach to a multi-fingered robotic hand, without controlling the robot arm, by collecting force sensor data from the finger joints during demonstrations and applying diffusion policies [24] that incorporate this force information.However, kinesthetics control of high-DoF robot systems-comprising both arms and hands-remains challenging, as operators often struggle to manipulate many degrees of freedom simultaneously.</p>
<p>A more compelling strategy for collecting demonstrations involves directly capturing human interactions with the environment, thus avoiding the limitations introduced by using robots as intermediaries during data collection.Prior research has investigated learning policies from demonstrations collected in the wild using sensorized parallel jaw grippers [25,26,27].Other methods, such as those presented in [28,29,30], have utilized human manipulation videos and applied motion retargeting to robotic hands.More relevant to our work, [6] introduced a portable MoCap system for bi-manual dexterous robots, enabling direct real-world imitation learning.However, these techniques are often limited by the kinematic and dynamic mismatch between the operator hands and the robotic hands, requiring specialized learning algorithms to map the human hand movements to the robotic hands.Instead, we propose equipping demonstrators with a sensorized exoskeleton to collect human demonstrations, leveraging human sensory perception and motor skills while reducing the domain gap and improving data collection efficiency.</p>
<p>Learning in simulation and sim-to-real transfer</p>
<p>Reinforcement learning (RL) in simulation has been widely adopted for learning generalizable manipulation skills for dexterous robotic hands, with domain randomization enabling the transfer of these skills to the real-world [31,32,33,34,35].However, training RL policies for highly dexterous tasks that can be transferred to the real world is non-trivial, often requiring excessive data and computational resources, careful reward function design, or an intricate learning scheme.</p>
<p>Previous research has shown that using demonstrations can facilitate RL training and improve learning efficiency [36,37,38,39].In this work, we also follow the demonstration-guided RL approach, where we use an auto-curriculum learning method, DemoStart [1], that leverages a small number of demonstrations and learns from sparse binary rewards.This approach, however, typically relies on dynamically feasible trajectories collected in simulation which can be challenging to obtain for high-DoF robot systems, such as robotic hands.Previous work has explored leveraging existing human hand motion datasets [37,40] to guide RL training, but this still requires addressing the motion re-targeting problem resulting from the morphology gap between human and robotic hands.Another simulation-based approach is to directly synthesize hand motions to generate optimal trajectories by formulating it as a constrained optimization problem [41,42].Similarly, we use trajectory optimization to convert collected real-world human demonstrations into dynamically feasible trajectories in simulation, making them suitable for training with auto-curriculum RL.</p>
<p>Methods</p>
<p>Our proposed framework, ExoStart, follows the real-to-sim-to-real pipeline illustrated in Fig. 1.It begins by collecting direct human demonstrations via an exoskeleton, which are then processed through a simulation-based dynamics filter to recover dynamically feasible trajectories.Subsequently, an auto-curriculum RL method driven by simple binary rewards is bootstrapped using these trajectories and the learned policies are transferred from simulation to the real robot in a zero-shot manner.</p>
<p>Direct human demonstration collection with an exoskeleton</p>
<p>We propose the use of a sensorized wearable exoskeleton for data collection (Fig. 2 (b)), which allows operators to directly interact with the real environment.This device, in the form of a glove with rigid segments connected by joints, is designed to replicate the kinematic structure and joint limits of a robotic hand.In this work, we build an exoskeleton for the Shadow DEX-EE hand [43].The kinematic correspondence between the robotic hand and the exoskeleton allows for a direct one-to-one mapping from the human hand movements to the robotic hand, due to the shared kinematic constraints.The exoskeleton's mechanical structure (Fig. 2 (a)) is primarily assembled from low-cost 3D-printed components that accurately match the hand's palmar surface geometries.Soft elastomer pads on the fingertips and middle phalanxes provide tactile feedback to the operator during data collection.A joint position sensor embedded in each finger joint tracks the finger movements.In addition, we use a camera-based pose estimator to track and estimate the Cartesian poses of the exoskeleton and objects, noting that any other 6D pose trackers [44] could be used as an alternative.Further details are provided in Appendix 8.1.1.</p>
<p>Dynamics filtering</p>
<p>Our framework requires dynamically feasible state trajectories in simulation to bootstrap policy learning.However, the collected raw real-world data is affected by hardware noise (including electronics and manufacturing errors) and visual tracking inaccuracies, resulting in trajectories that may not respect the simulated environment's dynamics.To clean them up, we use a sampling-based trajectory optimization method [45] as a dynamics filter to generate simulated trajectories that are dynamically feasible and closely track the recorded data.</p>
<p>For each time step, we first estimate the desired joint positions and object poses by interpolating the raw data collected in Sec.3.1.We then iteratively sample a set of control input sequences for a finite horizon, and select the sequence that minimizes a cost that measures the discrepancy between the real world measurements and the simulated state trajectories.Finally, we execute the first control input in this sequence, and repeat the same procedure for the rest of the demonstrated trajectory.</p>
<p>The cost consists of a weighted sum of tracking terms on the 1) arm end-effector poses, 2) the object poses, 3) the arm joint positions, and 4) the distances between the fingertips and the vertices of the bounding box of the object being manipulated.We also add a penalty cost on the finger velocities as a regularization term.This task-independent solution encourages the filtered trajectories to match the demonstrated motions while respecting the simulated environment's dynamics.Further details are presented in Appendix 8.1.2.</p>
<p>Auto-curriculum reinforcement learning and distillation</p>
<p>We use DemoStart [1], an auto-curriculum RL method to train feature-based teacher policies in simulation and distill them to vision-based policies that transfer to the real robot.This method learns complex manipulation behaviors from only a sparse binary reward function and a handful of feasible state trajectories in simulation.The binary reward function is simple to define, relying solely on a small number of success criteria (as shown in Appendix 8.3): a reward of 1 is given if all criteria are met, and 0 otherwise.The state trajectories, obtained from Section 3.2, help guide the learning process by biasing the policy towards the human demonstrations.By leveraging RL, the resulting policies not only outperform the provided human demonstrations but also achieve greater robustness to a wider range of initial conditions.</p>
<p>Following [1], we train the teacher policies in a distributed actor-learner setup [46] using privileged observations.We define an RL environment in simulation with a native initial state distribution for objects and robot poses (S native ) resembling those in the real robot environment, alongside the set of dynamically feasible states (S demo ) generated in Section 3.2.During training, the auto-curriculum RL method samples an initial state from either S native or S demo , performs a handful of policy rollouts (4 in our experiments) starting from the sampled state, and uses Zero-Variance Filtering2 to identify whether the state yields a strong learning signal.If so, then actors collect experience by executing the policy through episodes starting from this initial sampled state, and send data to the learner via a replay buffer [47].Physical domain randomization is incorporated during training to improve policy robustness [31].</p>
<p>The final stage involves distilling the trained teacher policies into vision-based student policies for zero-shot real-world deployment.This distillation process begins by rolling out episodes from the feature-based teacher policies in simulation to gather observation-action pairs.To mitigate sim-toreal gaps, domain randomization is applied during these rollouts.Subsequently, a student policy is trained via behavioral cloning using the collected simulated data and Action Chunking with Transformers (ACT) [48], which we found to be more performant than the Perceiver-Actor-Critic [49] implementation used in [1] The resulting vision-based policy is then directly evaluated on the real robot.Further details are available in Appendix 8.1.3.</p>
<p>Experimental Setup</p>
<p>Real-world and simulated setup</p>
<p>Our real robot setup features a Kuka LBR iiwa 14 arm equipped with a Shadow DEX-EE hand [43] as its end-effector.The arm is positioned beside a basket, with three fixed cameras pointing towards its center.The robotic hand has two cameras mounted on its wrist.We control the arm's endeffector Cartesian velocities (6 DoF) and the hand's joint positions (12 DoF).We replicate this setup in simulation using MuJoCo [50].Both the simulation and real-world robot environments run at 20 Hz.Further details are in Appendix 8.2.</p>
<p>To evaluate our proposed framework, we select seven representative and challenging dexterous manipulation tasks (Fig. 3): Key Lock, Nut Unscrew, Peg Insertion, Box Stand, Cube Flip, Case Open, and Bulb Install.These tasks demand a range of complex skills-such as grasping objects in random orientations, in-hand reorientation, articulated object manipulation, precise shape matching, and tight insertions-which are difficult to accomplish using traditional teleoperation methods.We believe their diversity and difficulty make them a compelling benchmark for assessing generalization, dexterity, and robustness in real-world robotic hand control.</p>
<p>Each experimental object has two versions: a target object and an AR-tagged replica (Fig. 2 (c)).The target objects consist of off-the-shelf items as well as custom 3D-printed objects.The ARtagged objects are 3D-printed replicas of the target objects, with AR markers attached at pre-defined object surface locations to ease pose estimation during demonstrations.Note that the AR-tagged objects are only needed for data collection, as the target objects are always used during the realworld evaluation.We use the target object's model and textures during simulation and distillation to ensure that the policies can be transferred to the real world.</p>
<p>ExoStart pipeline setup</p>
<p>Demonstration collection During real-world data collection, we utilize the AR-tagged objects and the exoskeleton described in Section 3.1.Each of the exoskeleton's finger joints has an embedded magnet and a Hall effect sensor, providing 12-dimensional joint angle data (three 4-DoF fingers).To estimate the arm's 6D end-effector pose, we attach a cube with 4×4 AR tags on each face to the exoskeleton's frame and use a Jacobian-based Inverse Kinematics solver [51] to determine the arm's 7-DoF joint positions.To minimize visual occlusions during demonstrations, we employ a total of five basket cameras.The glove's sensor data is acquired and streamed at 200 Hz via a custom micro-controller unit board.The arm joint positions, end-effector poses, and object poses are recorded at 10 Hz and synchronized with the glove's finger joint data through ROS.</p>
<p>Table 1 lists the number of demonstrations collected directly in the real world with the exoskeleton for each of the tasks, which ranges between 9 and 15.Note that the number of demonstrations collected is not indicative of the task difficulty, in fact in Section 5.1 we show that ExoStart can even work with just one demonstration.At the beginning of each episode, the operator wears the exoskeleton and starts with an open hand above the basket surface.All demonstrations are collected within a 15 seconds time limit, which shows the efficiency of our data collection.</p>
<p>Dynamics filtering As described in Section 3.2, we use a sampling-based planner to track the real-world trajectories while respecting the simulated environments' dynamics.In our experiments, the cost terms are shared among tasks and the parameters for every task were initialized with the same values, with minimal tuning from one task to the next.We run the planner on each real-world demonstration in a loop for three hours to collect diverse successful trajectories for policy learning using the same reward function as in the RL environment.The number of successfully filtered trajectories for each task is shown in Table 1 and ranges between 25 and 150.Further details are shown in Appendix 8.1.2.</p>
<p>Policy learning and distillation</p>
<p>We feed the dynamically feasible simulated states into the De-moStart pipeline to train a feature-based teacher policy.For each task, we define a simple binary reward which is set to 1 if the task is achieved (Fig. 3 illustrates examples of successful states), and zero otherwise.The task description and success criteria are included in Appendix 8.3.</p>
<p>Then we distill the teacher policies into vision-based student polices for real-world robot deployment.The observations include the images from the three basket cameras and two wrist cameras, the arm joint positions, the arm end-effector poses, and the DEX hand finger joint positions.For each # demos # filtered Teacher (Sim) Student (Real)  1: Experimental results.# of demos: the number of demonstrations collected with the exoskeleton in the real world.# of filtered: the number of demonstrations after applying dynamics filtering.Teacher: the success rate of the feature-based teacher policy during the episode roll-out in simulation (evaluated with &gt;250,000 episodes); Student: the success rate of the vision-based student policy in the real environment (evaluated with 50 episodes); Time limit: the time limit for each episode during evaluation, in seconds.</p>
<p>task, we roll out 250,000 episodes to generate trajectories of observation-action pairs in simulation, and use these to train an ACT policy which is deployed on the real-world robot zero-shot.</p>
<p>Results and Discussion</p>
<p>Table 1 presents the performance of our final vision-based student policies in solving the real-world tasks, as well as the performance of the feature-based teacher policies in simulation.Each task has a wide range of initial conditions, both in the simulated and real-world environments.More details of the task initial conditions are shown in Appendix 8.3.Consistent with DemoStart, our teacher policies achieved success rates of 95% or higher for most tasks, except for 89% for the Nut Unscrew task, evaluated over more than 250, 000 episodes in simulation.It is important to note that the teacher policy results reported in Table 1 correspond to the specific policy used for distillation.We found that although we could obtain higher-performing teacher policies in simulation, this did not always translate to higher real-world performance.The lower success rate for the Nut Unscrew teacher policy can be attributed to the use of a refined simulation model, which improved sim-to-real transfer but slightly reduced performance in simulation.</p>
<p>The distilled student policies demonstrated success rates exceeding 50% across all real-world evaluations over 50 episodes, except for the Bulb Install task.This represents a significant advancement over DemoStart, which only successfully transferred pick-and-place tasks that did not involve inhand object re-orientation.This improvement highlights the advantage of leveraging expressive demonstrations to bootstrap reinforcement learning, enabling control policies to acquire dexterous behaviors from human demonstrations while the exploratory nature of RL enhances robustness for real-world deployment.The results also demonstrate the applicability of our method to solve highly challenging real-world tasks: opening an AirPods case in-hand, and inserting a light bulb into a bayonet connector with a tight fitting.</p>
<p>At the time of writing, the Bulb Install task exhibits a non-zero but very low success rate (2%).Despite this, we view this is as a positive result that showcases how expressive demonstrations can result in dexterous behaviors even in the presence of a large sim-to-real gap.In this task, we observed that MuJoCo's soft-contact model led to object penetration, allowing the bulb to pass through the thin connector walls during the insertion phase in simulation.Consequently, the agent learned to exploit this discrepancy, explaining most real-world failures which happened during the insertion phase.Nonetheless, achieving zero-shot success in such a challenging real-world task highlights the value of high-quality demonstration data in guiding policy learning.We also note that improving the contact model of the simulator, while extremely relevant, is beyond the scope of this work.</p>
<p>Ablation studies</p>
<p>Direct human demonstration vs teleoperation We assess the benefits of direct object interaction by using the same exoskeleton described in Sec. 3 eration in simulation was chosen as it is required to generate dynamically feasible trajectories for our pipeline, effectively serving as a substitute for both real-world data collection and the dynamics filtering process.We limited our data collection to 10 minutes per episode and 1 hour per task, stopping at 1 hour or 10 successful episodes.Table 2 shows the success rates and average successful episode times for the Peg Insertion, Nut Unscrew, and Cube Flip tasks.Unsurprisingly, direct demonstrations yielded higher success rates and lower data collection times.Teleoperation for the Peg Insertion task was six times slower due to the difficulty in flipping the peg in-hand, requiring a pick-and-place strategy.The Nut Unscrew task failed during teleoperation due to finger coordination challenges, with episodes timing out.The Cube Flip task was unsuccessful due to the inability of the user to perform in-hand rotation of the cube.We found that although teleoperation is a viable alternative for pick-and-place tasks, direct demonstrations are significantly more effective for dexterous manipulations involving in-hand reorientation and finger coordination, which we attribute to the force feedback that the operator gets during the interaction.</p>
<p>Directly using real-world raw data for RL policy learning Dynamics filtering provides two key benefits: 1) smoothing noisy raw sensor data into dynamically feasible states, and 2) augmenting the recorded data by generating more than one feasible trajectory for each demonstration.To assess the importance of using a dynamics filter, we directly fed the raw sensor data into the DemoStart pipeline to train policies for the Box Stand and Peg Insertion tasks.For the Box Stand task, learning failed to converge.For the Peg Insertion task, training took six times longer (18 million vs. 3 million policy updates).This demonstrates that it is possible to learn RL policies with the raw unfiltered data, but dynamics filtering greatly enhances the learning efficiency and robustness.Although in the future the dynamics filtering step could be merged into the auto-curriculum RL pipeline, we found that optimizing the tracking costs through RL was much harder than through trajectory optimization.</p>
<p>Data efficiency Our framework typically relies on 9 to 15 real-world demonstrations for sufficient data coverage.To evaluate the impact of using a smaller number of real-world demonstrations, we conducted an experiment with only one single real demonstration for solving the Key Lock task.We used the dynamically filtered trajectories from this single demonstration to initialize the policy training and applied our standard pipeline.While simulation performance of the teacher policy remained unchanged at 99% success, real-world performance dropped to 17/50, compared to 28/50 with multiple demonstrations.Qualitatively, we observed that the learned policy was less robust especially during the grasping phase, causing the object to be dropped while trying to insert the key in the lock.This suggests that the lack of data diversity might negatively affect the robustness of policy's behavior and impact the sim-to-real transfer.</p>
<p>Conclusions</p>
<p>This work introduces ExoStart, an efficient and scalable learning framework for dexterous manipulation with robotic hands.Our framework achieves over 50% success on a wide range of tasks demonstrating challenging behaviors, including grasping objects in random configurations, in-hand re-orientation, articulated object manipulation, precise shape matching, and insertion.Through extensive ablation studies, we demonstrate that: 1) our exoskeleton demonstrations are more efficient and expressive compared to teleoperation, 2) dynamics filtering can improve the quality and quantity of the demonstrations, further improving learning efficiency; and 3) seeding RL with high-quality demonstrations improves generalizability, robustness, and enables complex dexterous behaviors on real robots.</p>
<p>Limitations</p>
<p>Sim-to-real gap</p>
<p>This work focuses on developing a framework for collecting high-quality hand demonstrations and training policies that transfer to real robots in a zero-shot manner.A key limitation of this approach is the challenge of bridging the sim-to-real gap, which arises from inaccuracies in modeling the physical world in simulation.This issue is particularly critical in reinforcement learning, where policies may exploit simulation-specific artifacts, ultimately hindering real-world performance.As discussed in Section 5, this gap contributes to lower success rates in the Bulb Install task, where the insertion phase is not accurately captured in simulation.In recent years, significant advances in simulation fidelity and domain randomization techniques have helped reduce the sim-to-real gap, enabling more complex tasks to be realistically modeled.Looking forward, we expected that continued improvements in simulation will further expand the range of tasks that can be effectively trained and transferred to real-world settings.</p>
<p>Motion re-targeting from human hands to robotic hands</p>
<p>Utilizing an exoskeleton for data acquisition provided a direct one-to-one mapping and allowed the operator to leverage the robot's hand structure beyond just the fingertip positions.However, this approach may be difficult to generalize across different hand morphologies, requiring the design of an exoskeleton for each robotic hand.While 3D printing significantly accelerates our ability to create these exoskeletons, it may be impractical to do for a large number of hands.In the future, we could explore using generic data collection gloves [6] or direct human hand motion capture [52] to collect hand-object interaction data, and leverage dynamics filtering to re-target the motions to various robotic hands [53].</p>
<p>Object pose tracking</p>
<p>The current methodology relies on a set of cameras and 3D-printed, AR-tagged replicas for object pose estimation.While this simplified the pose estimation problem for our setup, it introduced challenges for tracking small objects (e.g., an AirPods case) and restricted tracking to rigid bodies.Future work could investigate integrating state-of-the-art 6-DOF pose estimators [54,55,56,57] to mitigate hardware dependencies, enhance tracking accuracy, and enable the system to track deformable and non-rigid objects.</p>
<p>Deviation from demonstrated motions during policy learning</p>
<p>In some cases, bootstrapping our RL method with the human demonstrations resulted in undesirable emergent behaviors.These included deviations from the demonstrated actions and physically unrealistic movements, such as exploiting task shortcuts and large joint velocities.To mitigate this, supplementary termination conditions were introduced to guide policy learning.Despite these terminations, certain deviations persisted, such as the policy learned to use two fingers to unscrew the nut although the operator used three fingers during the demonstrations.This presents the tradeoff in constraining RL with demonstrations: we could force the policy's behaviors to match the demonstrations but this could lead to sub-optimal learning and increase the complexity of reward and termination criteria; alternatively, we could avoid defining additional restrictions, which would typically lead to undesired behaviors.Future work could focus on developing methods to constrain motion generation without compromising generalization capabilities during RL training.</p>
<p>Mixing simulated and real-world data during distillation</p>
<p>We applied extensive domain randomization and performed system identification to minimize the sim-to-real gap.While this approach enabled us to achieve over 50% success rates on real-world tasks, there is considerable room for improvement.We explored incorporating real-world success episodes of our distilled policy evaluation back into a second ACT policy training round.We collected 100 successful real-world episodes on the Peg Insertion task, and tested two training strategies: 1) mixing our original simulated data and the newly collected real-world success data at a 9:1 ratio, and 2) using only real-world data.Real-world evaluations yielded 22/50 successes with the 9:1 mixed data and 32/50 with only the real-world data.Compared to 27/50 successes using only simulated data, real-to-real behavior cloning (BC) showed a slight improvement.We believe this can be used as a starting point to generate more data for online RL in the real world.</p>
<p>Behavioral cloning on dynamically filtered data</p>
<p>We assessed whether we could do behavioral directly from the dynamically filtered data.Note that this is not possible to do directly from the real-world demonstration data as our method does not record any actions.To study this, we trained an ACT policy by directly using the dynamically filtered episodes of the Cube Flip task.The training dataset consisted of 586 successful episodes, augmented with visual randomization to ensure consistency with our standard pipeline.It is important to note that the control rate difference between the simulation used for MJPC (200 Hz) and the real robot environment (20 Hz) was significant, which we attempted to address using zeroorder action hold.This difference was due to a constraint on the open-sourced MJPC implementation [45], which ties the control rate to the integration timestep of the simulation, that resulted in unstable simulations at lower rates.We found that the control rate mismatch introduced a substantial dynamics gap and resulted in zero success during evaluation, even within the simulation environment and the same initial conditions.Although other methods to address the control rate difference could be explored, such as averaging the actions or more advanced methods, we did not further explore this approach.Our exoskeleton is designed with a glove form-factor that replicates the kinematic constraints of the Shadow DEX-EE (DEX) hand [43].Like the DEX hand, each finger possesses 4 DoF, and the fingers are arranged with one finger opposing the other two fingers.To ensure user comfort across varying hand sizes, the entire exoskeleton can be scaled.We employed a scaling factor of 0.8 in our experiments, which provided the most comfortable fit for the operator.Furthermore, to enhance thumb flexibility, we customized the glove's thumb with a virtual linkage structure, as illustrated in Fig. 2 (a).The exoskeleton's finger components are 3D-printed from rigid materials and assembled into a cohesive structure.Soft silicone elastomer pads cover the fingertips and middle phalanges, mirroring the DEX hand's finger surfaces and allowing users to experience haptic feedback through these padded contact points.</p>
<p>Finger movements are tracked by joint position sensors integrated into each joint of the exoskeleton.This is achieved by embedding a ring magnet within each joint, which moves relative to a mounted Hall-effect sensor.The resulting changes in the magnetic field are measured by the sensor to determine joint angles.These sensor readings are sequentially routed from the fingertips to the finger bases and then transmitted through a micro-controller unit board.To capture the hand's 6D pose, an AR tag cube is attached to the glove's wrist.Similarly, AR tags are affixed to experimental objects for pose tracking.Five cameras positioned around the workspace are used to calibrate and track these AR tags.All collected data-hand wrist pose, finger joint positions, and object poses-is synchronized at 10 Hz using ROS during data acquisition.3: Cost terms and parameters used for dynamics filtering.In our experiments, we use 8 keypoints per object -located at each of the vertices of the object's bounding box -and associate a cost between each keypoint and each of the fingertips.The L22 norm is part of the open-sourced MJPC implementation [45], and is a generalization of the "smooth-abs" function defined in [58].</p>
<p>. The collected real-world data often contains noise, leading to penetrations and jumps in the recorded motions.Using this data as demonstrations can lead to slower training times or negatively affect the success rate of our resultant policies.To address this, we employ sampling-based trajectory optimization with MJPC to recover dynamically feasible trajectories.We define cost terms to track the demonstrated motions, while ensuring that the trajectories respect the simulated environment's dynamics.The list of cost terms is shown Table 3.
Planner
For all tasks, the planner utilizes the cross entropy method with the parameters shown in Table 4.</p>
<p>In our experiments, the simulation state and control inputs are updated at a rate of 200 Hz.Since the glove's initial position during data collection usually differs from the RL training setup, we synthesize the first 20 frames by interpolating between the simulated environment's initial pose and the glove's initial pose.We also use the task's binary reward function as a success detector to filter out unsuccessful trajectories.The number of successful filtered episodes used for each task is shown Table 1.</p>
<p>We opt for a sampling-based planner due to its ability to tackle dexterous manipulation tasks with complex contact dynamics, which are challenging to optimize with gradient-based planners.Furthermore, the inherent randomness of the sampling approach yields a diverse set of trajectories even from a single demonstration.This diversity is beneficial for the auto-curriculum RL algorithm, as it provides a broader state distribution to reset the episodes in the RL environment.</p>
<p>Dynamics filtering serves two main purposes: recovering feasible trajectories and augmenting the demonstration data.For free floating objects like those in the Cube Flip and Box Stand tasks, we also augment the object poses by randomly sampling a constant pose offset that we apply to both the object and the robotic hand, as shown in Fig. 4 (a).Similarly, we are also able to change the shape and size of the objects without the need to collect new demonstrations by replacing the simulated object, as shown in Fig. 4 (b).</p>
<p>Policy learning and distillation with DemoStart</p>
<p>We formulate the policy learning problem as a Markov decision process (MDP), which we solve with RL.For each time step t, the agent predicts an action a t ∈ A based on its current state s t ∈ S, receives a reward r t+1 = R(s t , a t ) ∈ R, and transits to the next state s t+1 given the transition probability distribution p(•|s t , a t ).</p>
<p>Following the DemoStart [1] pipeline, we use an auto curriculum learning method to train a featurebased teacher policy that we later distill into a vision-based policy.We first define two different state distributions, S native and S demo .S native contains the set of initial states for the RL simulated</p>
<p>Experimental setup</p>
<p>The real-world and simulated setups are shown in Figure 5.Our setup consists of a 7-DoF Kuka LBR iiwa 14 arm, a Shadow DEX-EE (DEX) hand [43], a basket with slanted walls, and five Basler ACA1920-40GC GigE cameras with standard lenses -three attached to the basket and two attached to the wrist of the DEX hand.The Kuka arm is controlled through a differential inverse kinematics controller [51] that maps 6D Cartesian velocities to the joint velocities of the arm running at 200 Hz.The hand is controlled through a joint position controller running at 1 kHz.Both the simulation and real-world robot environments run at 20 Hz.</p>
<p>To collect real-world human demonstrations, we 3D-print object replicas with AR-tag holders and glue 13.1×13.1 mm x mm AR tags to them, as shown in Figure 2 (c).We collect the demonstrations with the same robot setup on which we run our real robot experiments.To reduce visual occlusion, we add two extra Basler ACA1920-40GC GigE cameras which we only use during the real-world data collection.To ensure consistent positions of the fixed objects, we use a jig to place the objects in pre-defined locations on the basket surface.During data collection, a human operator wears the exoskeleton, and demonstrates the tasks directly on the basket surface with the AR-tagged object replicas.</p>
<p>Tasks</p>
<p>The seven tasks used for experimental validation are shown in Fig. 3.For each task, we present a high level description, as well as their success criteria, termination criteria, and initial conditions below:</p>
<ol>
<li>Key Lock: pick up the key, insert it into the lock, and turn it 90 degrees.</li>
</ol>
<p>• Success criteria: a) the tip of the key is touching the back end of the lock; b) the key is turned 90 degrees; and c) the key is not being grasped.• Termination criteria: a) the palm of the hand is not facing down, i.e. the angle between the normal of the transversal plane of the hand and the gravity vector is more than 30 degrees.• Initial conditions in simulation: a) lock is fixed to a pre-defined location; b) key is initialized either in front of the lock or randomly around the basket surface, with a 0.95 and 0.05 probability, respectively.• Initial conditions in real: a) lock is fixed to a pre-defined location; b) key is initialized in front of the lock.</p>
<ol>
<li>Nut Unscrew: unscrew the nut by twisting 720 degrees, and place it onto the holder.</li>
</ol>
<p>• Success criteria: a) the nut is standing on its side; b) the nut is on the holder; and 3) the nut is not being grasped.• Termination criteria: a) the palm of the hand is not facing down, with a tolerance of 30 degrees.• Initial conditions in simulation: a) bolt is fixed to a pre-defined location; b) nut is initialized either fully threaded, threaded at a random height, or randomly dropped in the basket with probability of 0.6, 0.3, 0.1, respectively.• Initial conditions in real: a) bolt is fixed to a pre-defined location; b) nut is initialized fully threaded.</p>
<ol>
<li>Peg Insertion: pick up the peg, re-orient the peg 180 degrees in-hand, and insert the peg into the star-shape socket.</li>
</ol>
<p>• Success criteria: a) the peg is facing down; b) the tip center of the peg is touching the bottom center of the star socket; and c) the peg is not being grasped.• Termination criteria: a) the palm of the hand is not facing down, with a tolerance of 30 degrees; or 2) the prop orientation changes anywhere outside of a 3 x 3 cm area around the square socket.• Initial conditions in simulation: a) socket base is fixed to a pre-defined location; b) peg is either initialized in the square socket or randomly dropped in the basket, with a 0.8 and 0.2 probability, respectively.• Initial conditions in real: a) socket base is fixed to a pre-defined location; b) peg is initialized in the square socket.</p>
<ol>
<li>Box Stand: pick up the box, and make it stand vertically on the table with the blue segment on top.</li>
</ol>
<p>• Success criteria: a) the box is standing with the blue segment on the top; and 2) the box is not being grasped.• Termination criteria: a) the palm of the hand is not facing down, with a tolerance of 45 degrees.• Initial conditions in simulation and real: box is randomly initialized on the basket surface.</p>
<ol>
<li>Cube Flip: grasp the cube and re-orient it 180 degrees in-hand.</li>
</ol>
<p>• Success criteria: a) at least two fingers are in contact with the cube; b) cube is at least 5cm above the basket surface; and c) the cube has been rotated by 180 degrees from its initial orientation after being lifted.• Termination criteria: a) the palm of the hand is not facing down, with a tolerance of 30 degrees.• Initial conditions in simulation and real: cube is randomly initialized on the basket surface.</p>
<ol>
<li>Case Open: pick up the AirPods case, and open it in-hand.</li>
</ol>
<p>• Success criteria: a) case is at least 5cm above the basket surface; and b) the case cap is open, i.e., the hinge joint position is at least 90 degrees.• Termination criteria: the case is open (at least 10 degrees) while the case is in collision with the basket surface.• Initial conditions in simulation and real: case is closed and randomly initialized on the basket surface.</p>
<ol>
<li>Bulb Install: grasp the bulb, re-orient it in-hand, and install it into the bayonet socket.</li>
</ol>
<p>• Success criteria: 1) light bulb is upright; and 2) The two insertion pins of the bulb are in the two slots of the bayonet socket.• Termination criteria: N/A.</p>
<p>• Initial conditions in simulation and real: a) bayonet socket is fixed to a pre-defined location; b) bulb is randomly initialized on the basket surface.</p>
<p>We use the same initial conditions for the objects during training and evaluation in simulation.To improve the robustness of the trained policy, we uniformly sample a position offset from [-1, 1] cm in the XY plane and an angle offset from [−0.1,0.1]radians along gravity vector, and apply these offsets to the initial pose of any fixed object.</p>
<p>Figure 1 :
1
Figure 1: Overview of the ExoStart framework.(a) Human demonstration with an exoskeleton: We collect demonstrations with a sensorized exoskeleton by directly interacting with the object in the real world; (b) Dynamics filtering: We apply trajectory optimization to recover dynamically feasible simulated trajectories from the raw demonstrations; (c) Auto-curriculum RL and distillation:We use an auto-curriculum RL method[1] to train a teacher policy from the filtered trajectories, and then distill it into a vision-based student policy that transfers zero-shot to the real world.</p>
<p>Figure 2 :
2
Figure 2: (a) Finger design of the exoskeleton; (b) Sensorized data collection environment; (c) 3Dprinted and real objects used for policy evaluation (w/o AR tags) and data collection (w/ AR tags).</p>
<p>Figure 3 :
3
Figure 3: The seven tasks used for experimental validation.See Appendix 8.3 for task definitions.</p>
<p>Figure 4 :
4
Figure 4: We can augment the (a) object position and location, and (b) object shape and size from single demonstration with dynamics filtering.</p>
<p>Figure 5 :
5
Figure 5: Simulated and real-world robot environments.</p>
<p>Table 2 :
2
.1 to teleoperate the robot in simulation.Teleop-Number of successfully collected episodes with direct human demonstrations vs teleoperation, and the average duration of the successful episodes.
Peg InsertionNut UnscrewCube FlipTeleop Direct Teleop Direct Teleop Direct# of successful demos9/1010/104/1010/100/1010/10Time (seconds)48839315N/A10</p>
<p>Table 4 :
4
Planner configurations and parameters for dynamics filtering.
configuration ParameterAgent horizon0.25Agent timestep0.25Sampling trajectories40Sampling spline points 3Sampling exploration0.08
This consists of only selecting states where some of the rollouts failed and some succeed, in other words, they reach a mix of 0 and 1 final rewards, as introduced in[1].
AcknowledgmentsThe authors would like to thank Iman Khan, Mohammed Umayir Ahmed, Neil Sreendra, Nicolo Pantano, Ahmed Bajaber, Alim Jalloh, Nathan Batchelor, Federico Casarini, Jingwei Zhang, Alexander Herzog, Joss Moore, Leonard Hasenclever, Serkan Cabi, Stefan Welker, Takuma Yoneda, and Yuval Tassa for their help with experiments and their advice.Feature-based policy observations Dimensionenvironment that resemble the initial conditions of the real robot environment, usually supplemented through domain randomization techniques to ensure a good coverage of initial conditions.S demo is the set of dynamically feasible states obtained from the dynamics filter, which contains the states of every successful trajectory.As described in Section 3.3, DemoStart samples an initial state from either S native or S demo during training to reset the RL environment episodes, and identifies whether the state yields a strong learning signal.If it does, this initial state is used to collect experience by executing the policy through episodes starting from this state.All the episodes terminate after 10 seconds (200 environment steps) except for the Nut Unscrew task which terminates after 15 seconds (300 steps).The observations used during this training step is shown in Table5.To reduce the sim-to-real domain gap, we randomize the physical properties during the teacher policy training, and both the physical and visual properties during distillation.The randomized physical parameters include the friction, mass, and inertia of objects and robot links, as well as the armature, damping, and frictionloss of the robot joints.The randomized visual parameters include the camera locations, lighting conditions, as well as the color and texture of every object.For every task, we also apply external force perturbations on the object after it has been grasped.These perturbations are applied for 10 timesteps and are either zero (with a 0.5 probability) or uniformly sampled from [0,1] N, initiated only when at least two fingers are in contact with the object.To train policies for real robot deployment, we distill the teacher feature-based policies into visionbased student policies using ACT[48].Note that this differs from the original DemoStart pipeline which used a Perceiver-Actor-Critic[49]for distillation, which we found was less performant than ACT for our tasks.During the policy training, we add additionally Gaussian noise (std = 0.3) and photometric distortions including brightness (−0.5, 0.5), contrast (0.5, 2.0), hue (−0.5, 0.5), and saturation (0.5, 1.5) to images.The observations used by the distilled vision-based student policies are shown in Table.6.During policy evaluation, we disable temporal aggregation and set the query frequency between one and four depending on the task.
Demostart: Demonstration-led auto-curriculum applied to sim-to-real with multi-fingered robots. M Bauza, J E Chen, V Dalibard, N Gileadi, R Hafner, M F Martins, J Moore, R Pevceviciute, A Laurens, D Rao, arXiv:2409.066132024arXiv preprint</p>
<p>G R Team, S Abeyruwan, J Ainslie, J.-B Alayrac, M G Arenas, T Armstrong, A Balakrishna, R Baruch, M Bauza, M Blokzijl, arXiv:2503.20020Gemini robotics: Bringing ai into the physical world. 2025arXiv preprint</p>
<p>0 : A vision-language-action flow model for general robot control. K Black, N Brown, D Driess, A Esmail, M Equi, C Finn, N Fusai, L Groom, K Hausman, B Ichter, arXiv:2410.241642024arXiv preprint</p>
<p>High-fidelity grasping in virtual reality using a glove-based system. H Liu, Z Zhang, X Xie, Y Zhu, Y Liu, Y Wang, S.-C Zhu, 2019 international conference on robotics and automation (icra). IEEE2019</p>
<p>Nimbro avatar: Interactive immersive telepresence with force-feedback telemanipulation. M Schwarz, C Lenz, A Rochow, M Schreiber, S Behnke, 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2021</p>
<p>C Wang, H Shi, W Wang, R Zhang, L Fei-Fei, C K Liu, arXiv:2403.07788Dexcap: Scalable and portable mocap data collection system for dexterous manipulation. 2024arXiv preprint</p>
<p>Z.-H Yin, C Wang, L Pineda, F Hogan, K Bodduluri, A Sharma, P Lancaster, I Prasad, M Kalakrishnan, J Malik, arXiv:2502.04307Foundation controller for unprecedented dexterity. 2025arXiv preprint</p>
<p>Holo-dex: Teaching dexterity with immersive mixed reality. S P Arunachalam, I Güzey, S Chintala, L Pinto, 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE2023</p>
<p>Open-television: Teleoperation with immersive active visual feedback. X Cheng, J Li, S Yang, G Yang, X Wang, arXiv:2407.015122024arXiv preprint</p>
<p>Bunnyvisionpro: Real-time bimanual dexterous teleoperation for imitation learning. R Ding, Y Qin, J Zhu, C Jia, S Yang, R Yang, X Qi, X Wang, arXiv:2407.031622024arXiv preprint</p>
<p>Dexpilot: Vision-based teleoperation of dexterous robotic hand-arm system. A Handa, K Van Wyk, W Yang, J Liang, Y.-W Chao, Q Wan, S Birchfield, N Ratliff, D Fox, 2020 IEEE International Conference on Robotics and Automation (ICRA). IEEE2020</p>
<p>Robotic telekinesis: Learning a robotic hand imitator by watching humans on youtube. A Sivakumar, K Shaw, D Pathak, arXiv:2202.104482022arXiv preprint</p>
<p>Anyteleop: A general vision-based dexterous robot arm-hand teleoperation system. Y Qin, W Yang, B Huang, K Van Wyk, H Su, X Wang, Y.-W Chao, D Fox, arXiv:2307.045772023arXiv preprint</p>
<p>Leap hand: Low-cost, efficient, and anthropomorphic hand for robot learning. K Shaw, A Agarwal, D Pathak, arXiv:2309.064402023arXiv preprint</p>
<p>S Yang, M Liu, Y Qin, R Ding, J Li, X Cheng, R Yang, S Yi, X Wang, arXiv:2408.11805Ace: A cross-platform visual-exoskeletons system for low-cost dexterous teleoperation. 2024arXiv preprint</p>
<p>Z Si, K L Zhang, Z Temel, O Kroemer, Tilde, arXiv:2405.18804Teleoperation for dexterous in-hand manipulation learning with a deltahand. 2024arXiv preprint</p>
<p>R Zhong, C Cheng, J Xu, Y Wei, C Guo, D Zhang, W Dai, H Lu, arXiv:2503.10554Nuexo: A wearable exoskeleton covering all upper limb rom for outdoor data collection and teleoperation of humanoid robots. 2025arXiv preprint</p>
<p>Dynamics filter-concept and implementation of online motion generator for human figures. K Yamane, Y Nakamura, IEEE transactions on robotics and automation. 1932003</p>
<p>Gello: A general, low-cost, and intuitive teleoperation framework for robot manipulators. P Wu, Y Shentu, Z Yi, X Lin, P Abbeel, 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2024</p>
<p>Z Fu, T Z Zhao, C Finn, arXiv:2401.02117Mobile aloha: Learning bimanual mobile manipulation with low-cost whole-body teleoperation. 2024arXiv preprint</p>
<p>Learning compliant manipulation through kinesthetic and tactile human-robot interaction. K Kronander, A Billard, IEEE transactions on haptics. 732013</p>
<p>Imitation learning of positional and force skills demonstrated via kinesthetic teaching and haptic input. P Kormushev, S Calinon, D G Caldwell, Advanced Robotics. 2552011</p>
<p>Dexforce: Extracting forceinformed actions from kinesthetic demonstrations for dexterous manipulation. C Chen, Z Yu, H Choi, M Cutkosky, J Bohg, arXiv:2501.103562025arXiv preprint</p>
<p>Diffusion policy: Visuomotor policy learning via action diffusion. C Chi, RSSS Feng, RSSY Du, RSSZ Xu, RSSE Cousineau, RSSB Burchfiel, RSSS Song, RSSProceedings of Robotics: Science and Systems. Robotics: Science and Systems2023</p>
<p>Grasping in the wild: Learning 6dof closedloop grasping from low-cost demonstrations. S Song, A Zeng, J Lee, T Funkhouser, IEEE Robotics and Automation Letters. 532020</p>
<p>N M M Shafiullah, A Rai, H Etukuru, Y Liu, I Misra, S Chintala, L Pinto, arXiv:2311.16098On bringing robots home. 2023arXiv preprint</p>
<p>C Chi, Z Xu, C Pan, E Cousineau, B Burchfiel, S Feng, R Tedrake, S Song, arXiv:2402.10329Universal manipulation interface: In-the-wild robot teaching without in-the-wild robots. 2024arXiv preprint</p>
<p>Dexterous imitation made easy: A learning-based framework for efficient dexterous manipulation. S P Arunachalam, S Silwal, B Evans, L Pinto, 2023 ieee international conference on robotics and automation (icra). IEEE2023</p>
<p>Learning dexterity from internet videos. K Shaw, S Bahl, D Pathak, Videodex, Conference on Robot Learning. PMLR2023</p>
<p>Third-person visual imitation learning via decoupled hierarchical controller. P Sharma, D Pathak, A Gupta, Advances in Neural Information Processing Systems. 201932</p>
<p>Learning dexterous in-hand manipulation. O M Andrychowicz, B Baker, M Chociej, R Jozefowicz, B Mcgrew, J Pachocki, A Petron, M Plappert, G Powell, A Ray, The International Journal of Robotics Research. 3912020</p>
<p>A system for general in-hand object re-orientation. T Chen, J Xu, P , Conference on Robot Learning. PMLR2022</p>
<p>Z.-H Yin, B Huang, Y Qin, Q Chen, X Wang, arXiv:2303.10880Rotating without seeing: Towards in-hand dexterity through touch. 2023arXiv preprint</p>
<p>General in-hand object rotation with vision and touch. H Qi, B Yi, S Suresh, M Lambeta, Y Ma, R Calandra, J Malik, Conference on Robot Learning. PMLR2023</p>
<p>Lessons from learning to spin" pens. J Wang, Y Yuan, H Che, H Qi, Y Ma, J Malik, X Wang, arXiv:2407.189022024arXiv preprint</p>
<p>Advances in neural information processing systems. S , 19969Learning from demonstration</p>
<p>Learning complex dexterous manipulation with deep reinforcement learning and demonstrations. A Rajeswaran, V Kumar, A Gupta, G Vezzani, J Schulman, E Todorov, S Levine, arXiv:1709.100872017arXiv preprint</p>
<p>M Vecerik, T Hester, J Scholz, F Wang, O Pietquin, B Piot, N Heess, T Rothörl, T Lampe, M Riedmiller, arXiv:1707.08817Leveraging demonstrations for deep reinforcement learning on robotics problems with sparse rewards. 2017arXiv preprint</p>
<p>Reinforcement and imitation learning for diverse visuomotor skills. Y Zhu, Z Wang, J Merel, A Rusu, T Erez, S Cabi, S Tunyasuvunakool, J Kramár, R Hadsell, N De Freitas, arXiv:1802.095642018arXiv preprint</p>
<p>Object-centric dexterous manipulation from human motion data. Y Chen, C Wang, Y Yang, C K Liu, arXiv:2411.040052024arXiv preprint</p>
<p>Synthesis of detailed hand manipulations using contact sampling. Y Ye, C K Liu, ACM Transactions on Graphics (ToG). 3142012</p>
<p>Synthesis of interactive hand manipulation. C K Liu, Proceedings of the 2008 ACM SIGGRAPH/Eurographics Symposium on Computer Animation. the 2008 ACM SIGGRAPH/Eurographics Symposium on Computer Animation2008</p>
<p>. Shadow Robot Company. Dex-ee. 2024</p>
<p>A survey of 6dof object pose estimation methods for different application scenarios. J Guan, Y Hao, Q Wu, S Li, Y Fang, Sensors. 24410762024</p>
<p>T Howell, N Gileadi, S Tunyasuvunakool, K Zakka, T Erez, Y Tassa, arXiv:2212.00541Predictive sampling: Real-time behaviour synthesis with mujoco. 2022arXiv preprint</p>
<p>Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures. L Espeholt, H Soyer, R Munos, K Simonyan, V Mnih, T Ward, Y Doron, V Firoiu, T Harley, I Dunning, International conference on machine learning. PMLR2018</p>
<p>Reinforcement learning for robots using neural networks. L.-J Lin, 1992Carnegie Mellon University</p>
<p>Learning fine-grained bimanual manipulation with low-cost hardware. T Z Zhao, V Kumar, S Levine, C Finn, arXiv:2304.137052023arXiv preprint</p>
<p>Offline actor-critic reinforcement learning scales to large models. J T Springenberg, A Abdolmaleki, J Zhang, O Groth, M Bloesch, T Lampe, P Brakel, S Bechtle, S Kapturowski, R Hafner, N Heess, M Riedmiller, 2024</p>
<p>Mujoco: A physics engine for model-based control. E Todorov, T Erez, Y Tassa, 2012 IEEE/RSJ international conference on intelligent robots and systems. IEEE2012</p>
<p>D Barker, M Blokzijl, J E Chen, C Fantacci, R Jeong, D Khosid, A Laurens, R Pevceviciute, A Raju, J.-B Regli, J Scholz, O Sushkov, dm robotics: Libraries, tools, and tasks created and used for robotics research at deepmind. </p>
<p>Maniptrans: Efficient dexterous bimanual manipulation transfer via residual learning. K Li, P Li, T Liu, Y Li, S Huang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2025</p>
<p>ContactMPC: Towards online adaptive control for contactrich dexterous manipulation. A Lakshmipathy, N S Pollard, 2nd Workshop on Dexterous Manipulation: Design, Perception and Control (RSS). 2024</p>
<p>Poserbpf: A rao-blackwellized particle filter for 6d object pose tracking. X Deng, A Mousavian, Y Xiang, F Xia, T Bretl, D Fox, Robotics: Science and Systems (RSS). 2019</p>
<p>Bundletrack: 6d pose tracking for novel objects without instance or category-level 3d models. B Wen, K E Bekris, IEEE/RSJ International Conference on Intelligent Robots and Systems. 2021</p>
<p>se(3)-tracknet: Data-driven 6d pose tracking by calibrating image residuals in synthetic domains. B Wen, C Mitash, B Ren, K E Bekris, 10.1109/IROS45743.2020.9341314IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). 2020. Oct 2020</p>
<p>Foundationpose: Unified 6d pose estimation and tracking of novel objects. B Wen, W Yang, J Kautz, S Birchfield, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Synthesis and stabilization of complex behaviors through online trajectory optimization. Y Tassa, T Erez, E Todorov, 10.1109/IROS.2012.63860252012 IEEE/RSJ International Conference on Intelligent Robots and Systems. 2012</p>            </div>
        </div>

    </div>
</body>
</html>
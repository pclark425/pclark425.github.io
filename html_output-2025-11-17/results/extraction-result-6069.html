<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6069 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6069</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6069</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-120.html">extraction-schema-120</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <p><strong>Paper ID:</strong> paper-a141bbb96b4c1b2f28305883fcc332ad33eb536e</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/a141bbb96b4c1b2f28305883fcc332ad33eb536e" target="_blank">Improving requirements completeness: automated assistance through large language models</a></p>
                <p><strong>Paper Venue:</strong> Requirements Engineering</p>
                <p><strong>Paper TL;DR:</strong> BERT’s predictions effectively highlight terminology that is missing from requirements, BERT outperforms simpler baselines in identifying relevant yet missing terminology, and a machine learning-based filter reduces noise in the predictions, enhancing BERT’s effectiveness for completeness checking of requirements.</p>
                <p><strong>Paper Abstract:</strong> Natural language (NL) is arguably the most prevalent medium for expressing systems and software requirements. Detecting incompleteness in NL requirements is a major challenge. One approach to identify incompleteness is to compare requirements with external sources. Given the rise of large language models (LLMs), an interesting question arises: Are LLMs useful external sources of knowledge for detecting potential incompleteness in NL requirements? This article explores this question by utilizing BERT. Specifically, we employ BERT’s masked language model to generate contextualized predictions for filling masked slots in requirements. To simulate incompleteness, we withhold content from the requirements and assess BERT’s ability to predict terminology that is present in the withheld content but absent in the disclosed content. BERT can produce multiple predictions per mask. Our first contribution is determining the optimal number of predictions per mask, striking a balance between effectively identifying omissions in requirements and mitigating noise present in the predictions. Our second contribution involves designing a machine learning-based filter to post-process BERT’s predictions and further reduce noise. We conduct an empirical evaluation using 40 requirements specifications from the PURE dataset. Our findings indicate that: (1) BERT’s predictions effectively highlight terminology that is missing from requirements, (2) BERT outperforms simpler baselines in identifying relevant yet missing terminology, and (3) our filter reduces noise in the predictions, enhancing BERT’s effectiveness for completeness checking of requirements.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6069.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6069.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BERT-CompletenessRecommender</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BERT-based requirements completeness recommender</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system instantiated in this paper that uses BERT's masked language model to generate contextualized candidate terms for masked nouns/verbs in requirements, post-processes them with rule-based filters and a supervised ML classifier (trained on corpus-derived features) to surface terminology that hints at missing content.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>BERT-based requirements completeness recommender (this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Given a natural-language requirements specification, the system masks nouns/verbs one at a time and queries BERT's masked language model to obtain multiple candidate fills; it removes trivial/common predictions, builds a domain-specific corpus (Wikipedia via WikiDoMiner) to compute frequency and TF-IDF based features, and applies a pre-trained supervised classifier (RF or SVM depending on filtering strictness) to filter non-relevant predictions; the final output is a deduplicated list of recommended terms likely missing from the specification.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>BERT (BERT-base-cased, masked language model / MLM)</td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td>Input: natural-language requirements specifications from PURE (subset). Experiments used a 40-document subset of PURE (~24k sentences across the subset; documents partitioned into P1 and P2). The system operates on each requirement sentence by masking one noun or verb at a time and requesting multiple predictions per mask (empirically recommended: 15 predictions per mask).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Not a classical literature distillation pipeline; approach uses masked language modelling (MLM) as a contextual suggestion mechanism to surface candidate domain terms (i.e., alternative wording / missing terminology). Post-processing constitutes knowledge-synthesis components: removal of trivial/common words, domain-corpus frequency and TF-IDF feature extraction (Wikipedia corpus via WikiDoMiner), and supervised ML filtering (features F1–F13) to reduce noise.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>A ranked/deduplicated list of candidate terms (lemmatized) recommended as likely-relevant terminology missing from the input requirements spec.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Simulated-withholding evaluation: for each document, randomly split sentences into disclosed and withheld portions (50-50 and 90-10 experiments); run BERT on disclosed portion only, produce candidate terms, and measure how many candidate terms match terms in the withheld portion. Matching uses cosine similarity >= 85% over GloVe embeddings (non-exact matching). Metrics: Accuracy (precision-like: fraction of predicted unique terms that match withheld novel terms) and Coverage (recall-like: fraction of withheld novel terms matched by predictions). Additional evaluation: comparisons against three baselines (common words list, TF-IDF from domain corpus, WordNet synonyms), cross-validation for ML classifier selection (10-fold), experiments with under-sampling and cost-sensitive learning for classifier tuning, and human assessment of non-exact matches.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Key quantitative results: best BERT configuration used 15 predictions per mask; unfiltered predictions produced average Coverage ≈ 38% and Accuracy ≈ 12% on development data (similar ≈40% Coverage on held-out test set). ML-based filtering (Random Forest for strict/moderate, SVM for lenient/cost-sensitive) substantially increased Accuracy at the cost of reduced Coverage: e.g., under a 50-50 split, strict filtering raised proportion relevant to ≈48% (from ≈12% unfiltered) while lowering Coverage (lenient filter preserved more Coverage). The filter selection trade-offs were reported: RF better for aggressive filtering, SVM better for lenient filtering with higher recall. BERT outperformed three non-LLM baselines in both Accuracy and Coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td>PURE dataset (subset of 40 publicly-available requirements documents from PURE). Domain corpora extracted from Wikipedia via WikiDoMiner were used to compute corpus-frequency and TF-IDF features. Pre-trained GloVe (50-dim) used for semantic similarity/matching.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Significant noise (many false positives) in raw MLM predictions requiring post-filtering; trade-off between Coverage and precision—more predictions increase Coverage but add noise; evaluation based on simulated withholding rather than ground-truth human-labeled missing requirements; limited to BERT (no evaluation of larger generative LLMs like GPT variants); domain corpus extraction depth (they used depth=0) affects term coverage; matching relies on non-exact thresholds (85% cosine) which introduces some uncertainty (human assessment indicated most non-exact matches are useful but subjective). Computational/resource limits prevented exploring larger BERT variants or other LLM families.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons_to_other_methods</strong></td>
                            <td>Compared experimentally to three simple baselines: (1) lists of most common English words (250–2000), (2) TF-IDF top terms from a domain-specific Wikipedia corpus (threshold 0.01), and (3) WordNet synonyms of tokens in the disclosed portion. BERT-based MLM predictions (with filtering) outperformed all three baselines in both Accuracy and Coverage. The paper contrasts their approach with prior RE completeness methods that rely on additional artifacts (domain models, interviews), noting the advantage of not requiring external artifacts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving requirements completeness: automated assistance through large language models', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6069.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6069.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>U1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM autonomous knowledge gathering/summarization/integration (conceptual use case)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A conceptual use case described in the paper where an LLM autonomously gathers, summarizes, and integrates information from external sources (including its pretraining data) to support completeness checking or to provide external knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>U1 (conceptual LLM knowledge-gathering use case)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Conceptual description only: LLMs are viewed as potential autonomous external knowledge sources that could collect and synthesize information from external sources or their pretraining corpora to inform completeness checks.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Not instantiated in this paper (conceptual).</td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td>Not instantiated; concept implies arbitrary external textual sources (web/Wikipedia/pretraining corpora) as input.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Conceptual: autonomous gathering and summarization/integration (would correspond to summarization/aggregation/synthesis methods), but no implementation or operational details provided in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Conceptually: synthesized information or summaries usable as external knowledge for completeness checking.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Not evaluated in this work (mentioned as a motivating use case).</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>No empirical results presented for this conceptual use case in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td>Not applicable / not instantiated here.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Paper notes external sources may themselves be incomplete and that external completeness is relative; no empirical instantiation or evaluation of U1 within this work.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons_to_other_methods</strong></td>
                            <td>U1 is contrasted with U2 (the variant-suggestion use case which the authors instantiate using BERT). No quantitative comparisons provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving requirements completeness: automated assistance through large language models', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6069.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6069.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>U2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM suggestion of alternative versions / variant generation (instantiated use case)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A second conceptual use case (U2) described and instantiated in the paper: asking an LLM to suggest alternative phrasings or variants of existing requirements to reveal potentially missing concepts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>U2 (LLM-based variant suggestion instantiated with BERT)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Instantiate U2 by using BERT's MLM to mask words in disclosed requirement sentences and generate alternative fills; these alternatives can expose terminology that appears in withheld (simulated-missing) portions, thereby hinting at incompleteness.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>BERT (BERT-base-cased, MLM)</td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td>Input: masked requirement sentences from PURE subset; each mask is a single noun or verb per sentence; multiple masks per document/sentence across 40 documents (~24k sentences).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Variant generation via masked language modelling: produce multiple candidate substitutes per mask (recommended 15) to surface alternative domain terms; combine predictions across sentences to form a bag of candidate terms.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Bag/list of candidate alternative terms for masked positions; after filtering, a final list of recommended terms not present in disclosed text.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Same simulated-withholding evaluation as the main system: measure whether BERT's suggested variants match terms in the withheld portion using GloVe-based cosine similarity (>=85%) and compute Accuracy/Coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Instantiation showed BERT MLM predictions can hint at a sizeable fraction of missing terminology (≈38–40% Coverage unfiltered) but with low raw Accuracy (~12% unfiltered); post-filtering substantially raises precision while reducing Coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td>PURE subset; domain corpora via WikiDoMiner; GloVe embeddings for matching.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>High volume of non-relevant alternatives requiring filtering; resource limits prevented exploring larger/generative LLMs; evaluation used synthetic withholding not human-labeled omissions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons_to_other_methods</strong></td>
                            <td>This U2 instantiation (BERT MLM + filtering) was compared experimentally to three non-LLM baselines and found superior in both Accuracy and Coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving requirements completeness: automated assistance through large language models', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6069.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6069.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ShenBreaux-MLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Shen & Breaux's MLM-based domain knowledge extraction (related work mention)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A related work cited in the paper that uses MLM (BERT) to predict substitute tokens for masks in user-authored scenarios to extract domain models from scenario corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Shen & Breaux MLM-based domain model extraction (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>According to the paper, Shen and Breaux gather scenario corpora across domains, use typed dependencies and seed question templates with masked tokens, and apply MLM to predict substitutes to extract domain entities and build basic domain models.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>BERT MLM (as reported in the paper's related work summary); the current paper does not provide further implementation details.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td>User-authored scenario corpora from four directory-service domains (apartments, hiking trails, restaurants, health clinics) as described in the cited work (this paper only summarizes that prior work).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Masked language modelling to predict substitute tokens for masked words within seed templates; extracted substitutes are used to build domain knowledge models (typed dependencies-based).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Basic domain models extracted from scenarios (entities/relations), not explicitly a literature-scale theory distillation system.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Not detailed in this paper (only a brief summary in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>No quantitative results provided in this paper; cited as similar in spirit to using MLM to generate alternative entities.</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td>User-authored scenario corpora across four domains (as reported by the cited work); exact details not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>This paper notes that Shen & Breaux's work is not concerned with checking completeness and that it does not address the large number of non-relevant alternatives problem—which the present paper aims to mitigate via ML-based filtering.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons_to_other_methods</strong></td>
                            <td>Mentioned as related but distinct: Shen & Breaux use MLM to extract entities from scenario corpora, whereas the present work focuses on completeness checking and adds ML-based filtering to reduce false positives.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving requirements completeness: automated assistance through large language models', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6069.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6069.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NoRBERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NoRBERT (Non-functional and functional Requirements classification using BERT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A BERT-based unsupervised representation learning method for requirements classification mentioned in the related work; cited as using BERT but not employing MLM for masked-token prediction in the same manner as this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>NoRBERT (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Summarized in the paper as an unsupervised representation-learning method built on BERT for classifying functional and non-functional requirements; not applied here.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>BERT (as referenced in related work summary); no implementation or version details provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td>Not specified in this paper (mentioned in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Representation learning for classification (not masked-token based distillation); details not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Representations for classification of requirements (not a literature-synthesis/theory-distillation output).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Not described here (only referenced as related work).</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>No results given in this paper; cited to indicate other BERT-based uses in RE.</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Not discussed in this paper; listed as related RE work that uses BERT for purposes other than MLM-driven completion suggestions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons_to_other_methods</strong></td>
                            <td>Mentioned as part of literature using BERT; no direct comparison reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving requirements completeness: automated assistance through large language models', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6069.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6069.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sainani-BERT-Contracts</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sainani et al. BERT-based extraction/classification from software engineering contracts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A related work mentioned that uses BERT to automate extraction and classification of requirements from contracts; referenced as employing BERT but not using MLM in the completion-suggestion way used in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Sainani et al. BERT contract extraction (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Briefly summarized as a work that applies BERT for extracting and classifying requirement statements from software engineering contracts (not using MLM for masked‑token completion in the same manner).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>BERT (as noted in related work summary); no further details provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td>Software engineering contracts (details not provided in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Extraction and classification (not described in detail here).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Extracted and classified requirement statements (not a literature-synthesis product).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Not provided in this paper; only referenced.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Not discussed in this paper; included to illustrate other BERT usages in requirements engineering.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons_to_other_methods</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving requirements completeness: automated assistance through large language models', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6069.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6069.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT/GPT-family (future mention)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT / GPT family of generative models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper notes ChatGPT and GPT-family generative LLMs as likely, though unexplored here, superior alternatives to BERT for the task and as promising candidates for interactive or conversational completeness support.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ChatGPT / GPT-family (mentioned as potential future alternative)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Mentioned as modern generative LLMs that could be instantiated to perform the use cases U1/U2 (autonomous gathering/summarization and variant suggestion) and likely offer improved performance and interactive capabilities compared to BERT; no experiments performed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Not used in this paper (cited generically as GPT family / ChatGPT).</td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td>Not applicable to this paper; suggested future avenue.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Not implemented here; conceptually could support summarization, conversational clarification, multi-document synthesis, or iterative elicitation of missing content.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Not implemented here; conceptually would produce summaries, suggestions, or conversational guidance.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Not evaluated in this work; authors explicitly state they have not explored ChatGPT/GPT experimentally.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>No results in this paper; authors conjecture GPT-like models would likely perform better than BERT for the task.</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td>Not applicable in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Authors note they did not explore these models due to timing/resource constraints; potential future research direction.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons_to_other_methods</strong></td>
                            <td>Authors hypothesize GPT-like models may be superior to BERT but provide no empirical comparison in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving requirements completeness: automated assistance through large language models', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6069",
    "paper_id": "paper-a141bbb96b4c1b2f28305883fcc332ad33eb536e",
    "extraction_schema_id": "extraction-schema-120",
    "extracted_data": [
        {
            "name_short": "BERT-CompletenessRecommender",
            "name_full": "BERT-based requirements completeness recommender",
            "brief_description": "A system instantiated in this paper that uses BERT's masked language model to generate contextualized candidate terms for masked nouns/verbs in requirements, post-processes them with rule-based filters and a supervised ML classifier (trained on corpus-derived features) to surface terminology that hints at missing content.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "BERT-based requirements completeness recommender (this paper)",
            "system_description": "Given a natural-language requirements specification, the system masks nouns/verbs one at a time and queries BERT's masked language model to obtain multiple candidate fills; it removes trivial/common predictions, builds a domain-specific corpus (Wikipedia via WikiDoMiner) to compute frequency and TF-IDF based features, and applies a pre-trained supervised classifier (RF or SVM depending on filtering strictness) to filter non-relevant predictions; the final output is a deduplicated list of recommended terms likely missing from the specification.",
            "llm_model_used": "BERT (BERT-base-cased, masked language model / MLM)",
            "input_type_and_size": "Input: natural-language requirements specifications from PURE (subset). Experiments used a 40-document subset of PURE (~24k sentences across the subset; documents partitioned into P1 and P2). The system operates on each requirement sentence by masking one noun or verb at a time and requesting multiple predictions per mask (empirically recommended: 15 predictions per mask).",
            "distillation_approach": "Not a classical literature distillation pipeline; approach uses masked language modelling (MLM) as a contextual suggestion mechanism to surface candidate domain terms (i.e., alternative wording / missing terminology). Post-processing constitutes knowledge-synthesis components: removal of trivial/common words, domain-corpus frequency and TF-IDF feature extraction (Wikipedia corpus via WikiDoMiner), and supervised ML filtering (features F1–F13) to reduce noise.",
            "output_type": "A ranked/deduplicated list of candidate terms (lemmatized) recommended as likely-relevant terminology missing from the input requirements spec.",
            "evaluation_methods": "Simulated-withholding evaluation: for each document, randomly split sentences into disclosed and withheld portions (50-50 and 90-10 experiments); run BERT on disclosed portion only, produce candidate terms, and measure how many candidate terms match terms in the withheld portion. Matching uses cosine similarity &gt;= 85% over GloVe embeddings (non-exact matching). Metrics: Accuracy (precision-like: fraction of predicted unique terms that match withheld novel terms) and Coverage (recall-like: fraction of withheld novel terms matched by predictions). Additional evaluation: comparisons against three baselines (common words list, TF-IDF from domain corpus, WordNet synonyms), cross-validation for ML classifier selection (10-fold), experiments with under-sampling and cost-sensitive learning for classifier tuning, and human assessment of non-exact matches.",
            "results": "Key quantitative results: best BERT configuration used 15 predictions per mask; unfiltered predictions produced average Coverage ≈ 38% and Accuracy ≈ 12% on development data (similar ≈40% Coverage on held-out test set). ML-based filtering (Random Forest for strict/moderate, SVM for lenient/cost-sensitive) substantially increased Accuracy at the cost of reduced Coverage: e.g., under a 50-50 split, strict filtering raised proportion relevant to ≈48% (from ≈12% unfiltered) while lowering Coverage (lenient filter preserved more Coverage). The filter selection trade-offs were reported: RF better for aggressive filtering, SVM better for lenient filtering with higher recall. BERT outperformed three non-LLM baselines in both Accuracy and Coverage.",
            "datasets_or_benchmarks": "PURE dataset (subset of 40 publicly-available requirements documents from PURE). Domain corpora extracted from Wikipedia via WikiDoMiner were used to compute corpus-frequency and TF-IDF features. Pre-trained GloVe (50-dim) used for semantic similarity/matching.",
            "challenges_or_limitations": "Significant noise (many false positives) in raw MLM predictions requiring post-filtering; trade-off between Coverage and precision—more predictions increase Coverage but add noise; evaluation based on simulated withholding rather than ground-truth human-labeled missing requirements; limited to BERT (no evaluation of larger generative LLMs like GPT variants); domain corpus extraction depth (they used depth=0) affects term coverage; matching relies on non-exact thresholds (85% cosine) which introduces some uncertainty (human assessment indicated most non-exact matches are useful but subjective). Computational/resource limits prevented exploring larger BERT variants or other LLM families.",
            "comparisons_to_other_methods": "Compared experimentally to three simple baselines: (1) lists of most common English words (250–2000), (2) TF-IDF top terms from a domain-specific Wikipedia corpus (threshold 0.01), and (3) WordNet synonyms of tokens in the disclosed portion. BERT-based MLM predictions (with filtering) outperformed all three baselines in both Accuracy and Coverage. The paper contrasts their approach with prior RE completeness methods that rely on additional artifacts (domain models, interviews), noting the advantage of not requiring external artifacts.",
            "uuid": "e6069.0",
            "source_info": {
                "paper_title": "Improving requirements completeness: automated assistance through large language models",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "U1",
            "name_full": "LLM autonomous knowledge gathering/summarization/integration (conceptual use case)",
            "brief_description": "A conceptual use case described in the paper where an LLM autonomously gathers, summarizes, and integrates information from external sources (including its pretraining data) to support completeness checking or to provide external knowledge.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "U1 (conceptual LLM knowledge-gathering use case)",
            "system_description": "Conceptual description only: LLMs are viewed as potential autonomous external knowledge sources that could collect and synthesize information from external sources or their pretraining corpora to inform completeness checks.",
            "llm_model_used": "Not instantiated in this paper (conceptual).",
            "input_type_and_size": "Not instantiated; concept implies arbitrary external textual sources (web/Wikipedia/pretraining corpora) as input.",
            "distillation_approach": "Conceptual: autonomous gathering and summarization/integration (would correspond to summarization/aggregation/synthesis methods), but no implementation or operational details provided in the paper.",
            "output_type": "Conceptually: synthesized information or summaries usable as external knowledge for completeness checking.",
            "evaluation_methods": "Not evaluated in this work (mentioned as a motivating use case).",
            "results": "No empirical results presented for this conceptual use case in this paper.",
            "datasets_or_benchmarks": "Not applicable / not instantiated here.",
            "challenges_or_limitations": "Paper notes external sources may themselves be incomplete and that external completeness is relative; no empirical instantiation or evaluation of U1 within this work.",
            "comparisons_to_other_methods": "U1 is contrasted with U2 (the variant-suggestion use case which the authors instantiate using BERT). No quantitative comparisons provided.",
            "uuid": "e6069.1",
            "source_info": {
                "paper_title": "Improving requirements completeness: automated assistance through large language models",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "U2",
            "name_full": "LLM suggestion of alternative versions / variant generation (instantiated use case)",
            "brief_description": "A second conceptual use case (U2) described and instantiated in the paper: asking an LLM to suggest alternative phrasings or variants of existing requirements to reveal potentially missing concepts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "U2 (LLM-based variant suggestion instantiated with BERT)",
            "system_description": "Instantiate U2 by using BERT's MLM to mask words in disclosed requirement sentences and generate alternative fills; these alternatives can expose terminology that appears in withheld (simulated-missing) portions, thereby hinting at incompleteness.",
            "llm_model_used": "BERT (BERT-base-cased, MLM)",
            "input_type_and_size": "Input: masked requirement sentences from PURE subset; each mask is a single noun or verb per sentence; multiple masks per document/sentence across 40 documents (~24k sentences).",
            "distillation_approach": "Variant generation via masked language modelling: produce multiple candidate substitutes per mask (recommended 15) to surface alternative domain terms; combine predictions across sentences to form a bag of candidate terms.",
            "output_type": "Bag/list of candidate alternative terms for masked positions; after filtering, a final list of recommended terms not present in disclosed text.",
            "evaluation_methods": "Same simulated-withholding evaluation as the main system: measure whether BERT's suggested variants match terms in the withheld portion using GloVe-based cosine similarity (&gt;=85%) and compute Accuracy/Coverage.",
            "results": "Instantiation showed BERT MLM predictions can hint at a sizeable fraction of missing terminology (≈38–40% Coverage unfiltered) but with low raw Accuracy (~12% unfiltered); post-filtering substantially raises precision while reducing Coverage.",
            "datasets_or_benchmarks": "PURE subset; domain corpora via WikiDoMiner; GloVe embeddings for matching.",
            "challenges_or_limitations": "High volume of non-relevant alternatives requiring filtering; resource limits prevented exploring larger/generative LLMs; evaluation used synthetic withholding not human-labeled omissions.",
            "comparisons_to_other_methods": "This U2 instantiation (BERT MLM + filtering) was compared experimentally to three non-LLM baselines and found superior in both Accuracy and Coverage.",
            "uuid": "e6069.2",
            "source_info": {
                "paper_title": "Improving requirements completeness: automated assistance through large language models",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "ShenBreaux-MLM",
            "name_full": "Shen & Breaux's MLM-based domain knowledge extraction (related work mention)",
            "brief_description": "A related work cited in the paper that uses MLM (BERT) to predict substitute tokens for masks in user-authored scenarios to extract domain models from scenario corpora.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Shen & Breaux MLM-based domain model extraction (mentioned)",
            "system_description": "According to the paper, Shen and Breaux gather scenario corpora across domains, use typed dependencies and seed question templates with masked tokens, and apply MLM to predict substitutes to extract domain entities and build basic domain models.",
            "llm_model_used": "BERT MLM (as reported in the paper's related work summary); the current paper does not provide further implementation details.",
            "input_type_and_size": "User-authored scenario corpora from four directory-service domains (apartments, hiking trails, restaurants, health clinics) as described in the cited work (this paper only summarizes that prior work).",
            "distillation_approach": "Masked language modelling to predict substitute tokens for masked words within seed templates; extracted substitutes are used to build domain knowledge models (typed dependencies-based).",
            "output_type": "Basic domain models extracted from scenarios (entities/relations), not explicitly a literature-scale theory distillation system.",
            "evaluation_methods": "Not detailed in this paper (only a brief summary in related work).",
            "results": "No quantitative results provided in this paper; cited as similar in spirit to using MLM to generate alternative entities.",
            "datasets_or_benchmarks": "User-authored scenario corpora across four domains (as reported by the cited work); exact details not provided here.",
            "challenges_or_limitations": "This paper notes that Shen & Breaux's work is not concerned with checking completeness and that it does not address the large number of non-relevant alternatives problem—which the present paper aims to mitigate via ML-based filtering.",
            "comparisons_to_other_methods": "Mentioned as related but distinct: Shen & Breaux use MLM to extract entities from scenario corpora, whereas the present work focuses on completeness checking and adds ML-based filtering to reduce false positives.",
            "uuid": "e6069.3",
            "source_info": {
                "paper_title": "Improving requirements completeness: automated assistance through large language models",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "NoRBERT",
            "name_full": "NoRBERT (Non-functional and functional Requirements classification using BERT)",
            "brief_description": "A BERT-based unsupervised representation learning method for requirements classification mentioned in the related work; cited as using BERT but not employing MLM for masked-token prediction in the same manner as this paper.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "NoRBERT (mentioned)",
            "system_description": "Summarized in the paper as an unsupervised representation-learning method built on BERT for classifying functional and non-functional requirements; not applied here.",
            "llm_model_used": "BERT (as referenced in related work summary); no implementation or version details provided in this paper.",
            "input_type_and_size": "Not specified in this paper (mentioned in related work).",
            "distillation_approach": "Representation learning for classification (not masked-token based distillation); details not provided in this paper.",
            "output_type": "Representations for classification of requirements (not a literature-synthesis/theory-distillation output).",
            "evaluation_methods": "Not described here (only referenced as related work).",
            "results": "No results given in this paper; cited to indicate other BERT-based uses in RE.",
            "datasets_or_benchmarks": "Not specified in this paper.",
            "challenges_or_limitations": "Not discussed in this paper; listed as related RE work that uses BERT for purposes other than MLM-driven completion suggestions.",
            "comparisons_to_other_methods": "Mentioned as part of literature using BERT; no direct comparison reported here.",
            "uuid": "e6069.4",
            "source_info": {
                "paper_title": "Improving requirements completeness: automated assistance through large language models",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Sainani-BERT-Contracts",
            "name_full": "Sainani et al. BERT-based extraction/classification from software engineering contracts",
            "brief_description": "A related work mentioned that uses BERT to automate extraction and classification of requirements from contracts; referenced as employing BERT but not using MLM in the completion-suggestion way used in this paper.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Sainani et al. BERT contract extraction (mentioned)",
            "system_description": "Briefly summarized as a work that applies BERT for extracting and classifying requirement statements from software engineering contracts (not using MLM for masked‑token completion in the same manner).",
            "llm_model_used": "BERT (as noted in related work summary); no further details provided in this paper.",
            "input_type_and_size": "Software engineering contracts (details not provided in this paper).",
            "distillation_approach": "Extraction and classification (not described in detail here).",
            "output_type": "Extracted and classified requirement statements (not a literature-synthesis product).",
            "evaluation_methods": "Not provided in this paper; only referenced.",
            "results": "Not provided here.",
            "datasets_or_benchmarks": "Not provided here.",
            "challenges_or_limitations": "Not discussed in this paper; included to illustrate other BERT usages in requirements engineering.",
            "comparisons_to_other_methods": "Not reported here.",
            "uuid": "e6069.5",
            "source_info": {
                "paper_title": "Improving requirements completeness: automated assistance through large language models",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "ChatGPT/GPT-family (future mention)",
            "name_full": "ChatGPT / GPT family of generative models",
            "brief_description": "The paper notes ChatGPT and GPT-family generative LLMs as likely, though unexplored here, superior alternatives to BERT for the task and as promising candidates for interactive or conversational completeness support.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "ChatGPT / GPT-family (mentioned as potential future alternative)",
            "system_description": "Mentioned as modern generative LLMs that could be instantiated to perform the use cases U1/U2 (autonomous gathering/summarization and variant suggestion) and likely offer improved performance and interactive capabilities compared to BERT; no experiments performed in this paper.",
            "llm_model_used": "Not used in this paper (cited generically as GPT family / ChatGPT).",
            "input_type_and_size": "Not applicable to this paper; suggested future avenue.",
            "distillation_approach": "Not implemented here; conceptually could support summarization, conversational clarification, multi-document synthesis, or iterative elicitation of missing content.",
            "output_type": "Not implemented here; conceptually would produce summaries, suggestions, or conversational guidance.",
            "evaluation_methods": "Not evaluated in this work; authors explicitly state they have not explored ChatGPT/GPT experimentally.",
            "results": "No results in this paper; authors conjecture GPT-like models would likely perform better than BERT for the task.",
            "datasets_or_benchmarks": "Not applicable in this work.",
            "challenges_or_limitations": "Authors note they did not explore these models due to timing/resource constraints; potential future research direction.",
            "comparisons_to_other_methods": "Authors hypothesize GPT-like models may be superior to BERT but provide no empirical comparison in this paper.",
            "uuid": "e6069.6",
            "source_info": {
                "paper_title": "Improving requirements completeness: automated assistance through large language models",
                "publication_date_yy_mm": "2023-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
            "rating": 2
        }
    ],
    "cost": 0.01986025,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Improving Requirements Completeness: Automated Assistance through Large Language Models</h1>
<p>Dipeeka Luitel ${ }^{1} \cdot$ Shabnam Hassani ${ }^{1} \cdot$ Mehrdad Sabetzadeh ${ }^{1}$</p>
<p>the date of receipt and acceptance should be inserted later</p>
<h4>Abstract</h4>
<p>Natural language (NL) is arguably the most prevalent medium for expressing systems and software requirements. Detecting incompleteness in NL requirements is a major challenge. One approach to identify incompleteness is to compare requirements with external sources. Given the rise of large language models (LLMs), an interesting question arises: Are LLMs useful external sources of knowledge for detecting potential incompleteness in NL requirements? This article explores this question by utilizing BERT. Specifically, we employ BERT's masked language model (MLM) to generate contextualized predictions for filling masked slots in requirements. To simulate incompleteness, we withhold content from the requirements and assess BERT's ability to predict terminology that is present in the withheld content but absent in the disclosed content. BERT can produce multiple predictions per mask. Our first contribution is determining the optimal number of predictions per mask, striking a balance between effectively identifying omissions in requirements and mitigating noise present in the predictions. Our second contribution involves designing a machine learning-based filter to post-process BERT's predictions and further reduce noise. We conduct an empirical evaluation using 40 requirements specifications from the PURE dataset. Our findings indicate that: (1) BERT's predictions effectively highlight terminology that is missing from re-</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>quirements, (2) BERT outperforms simpler baselines in identifying relevant yet missing terminology, and (3) our filter reduces noise in the predictions, enhancing BERT's effectiveness for completeness checking of requirements.</p>
<p>Keywords. Requirements Completeness, Natural Language Processing (NLP), Machine Learning (ML), Large Language Models (LLMs), BERT.</p>
<h2>1 Introduction</h2>
<p>Natural language (NL) is widely used in industry to express systems and software requirements. Despite its prevalence, NL requirements are prone to incompleteness. Improving the completeness of NL requirements is an important yet challenging problem in requirements engineering (RE) [54,53]. The RE literature identifies two different notions of completeness [53]: (1) Internal completeness pertains to requirements being closed in terms of the functions and qualities that can be deduced solely from the requirements. (2) External completeness focuses on ensuring that requirements encompass all the information suggested by external sources of knowledge. These sources can include individuals (such as stakeholders) or artifacts like higher-level requirements and existing system descriptions [4]. External completeness is a relative measure since the external sources may themselves be incomplete, or not all relevant external sources may be known [53]. While external completeness cannot be defined in absolute terms, relevant external sources, when available, can be useful for detecting missing requirements-related information.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1: Illustrative requirements specification split into a disclosed and a withheld part. The withheld part simulates requirements omissions. Masking words in the disclosed part and having BERT make predictions for the masks reveals some terms that appear only in the withheld part.</p>
<h3>1.1 Motivation</h3>
<p>Natural Language Processing (NLP) is a powerful tool for computer-assisted verification of external completeness when requirements and external sources of knowledge are presented in text format. For example, Ferrari et al. [24] utilize NLP to assess the completeness of requirements by analyzing interview transcripts. In a similar vein, Dalpiaz et al. [15] integrate NLP with visualization techniques to identify disparities in stakeholders' perspectives. These disparities are in turn examined as potential indicators of incompleteness.</p>
<p>Large language models (LLMs) provide a fresh opportunity to employ NLP for improving the external completeness of requirements. Through self-supervised learning, LLMs have been pre-trained on vast collections of textual data, e.g., millions of Wikipedia articles. This opens up the possibility of using LLMs as external knowledge sources for completeness checking. More precisely, we see two primary use cases, denoted as U1 and U2 below, for LLMs in relation to external completeness:</p>
<p>U1 LLMs can be used for autonomously gathering, summarizing, and integrating new information from external sources, including their own pre-training data.
U2 LLMs can be tasked with suggesting alternative versions of existing material. While the results often do not convey the exact same meaning as the original material, they largely preserve context. An exami-
nation of the resulting variant material can reveal pertinent information that has been overlooked.</p>
<p>In this article, we propose an instantiation of U2 (above) employing BERT (Bidirectional Encoder Representations from Transformers) [16] as our chosen LLM.</p>
<p>BERT has been trained to predict masked tokens by finding words or phrases that most closely match the surrounding context. To illustrate how BERT can help realize U2, consider the example in Fig. 1. In this example, we have masked one word, denoted as [MASK], in each of requirements R1, R2 and R3. We have then had BERT make five predictions for filling each masked slot. For instance, in R1, the masked word is availability. The predictions made by BERT are: performance, efficiency, stability, accuracy, and reliability. As seen from the figure, one of these predictions, namely stability, is a word that appears in R6. Similarly, the predictions that BERT makes for the masked words in R2 and R3 (audit and connectivity) reveal new terminology that is present in R4 and R5 (network, traffic, comply and security). In this example, if requirements R4-R6 were to be missing, BERT's predictions over R1-R3 would provide useful cues about some of the missing concepts.</p>
<h3>1.2 Contributions</h3>
<p>The core utility of an LLM lies in its ability to predict and generate text that is both coherent and contextu-</p>
<p>ally relevant. This characteristic makes LLMs potentially useful tools for making recommendations on how to make requirements more complete. To systematically assess the usefulness of an LLM for this purpose, we need a strategy to evaluate the predictive accuracy of the LLM in identifying relevant content that is absent from requirements. To this end, we simulate missing requirements information by randomly withholding a portion of a given requirements specification. We disclose the remainder of the specification to the LLM of choice, in our case BERT, to obtain predictions. Requirements in the disclosed portion are revealed one at a time to BERT for obtaining predictions of masked tokens. In our example of Fig. 1, the disclosed part would be requirements R1-R3, and the withheld part would be requirements R4-R6.</p>
<p>When BERT is employed as a recommender in the manner described above, an important consideration is striking a trade-off between valuable and superfluous recommendations. Our first contribution is configuring BERT's number of predictions per mask to find a reasonable balance between the identification of simulated omissions vis-à-vis the generation of unhelpful predictions or noise. We observe that achieving good coverage of requirements omissions through BERT predictions results in a significant amount of noise. Some of the noise can be easily filtered. For instance, in the example of Fig. 1, one can dismiss the predictions of service and system (made over R3); these words already appear in the disclosed portion, thereby providing no cues about missing terminology. Furthermore, one can dismiss words that carry little meaning, e.g., "any", "other" and "each", should such words appear among the predictions. After applying these obvious filters, the predictions still remain considerably noisy. Our second contribution is a machine learning-based filter that post-processes predictions made by BERT, aiming to reduce the occurrence of noise in the predictions.</p>
<p>We based our solution development and evaluation on a set of 40 requirements specifications from the PURE dataset [26]. These specifications collectively comprise over 23,000 sentences. To support replication and enable future research, we have made our implementation and evaluation artifacts publicly available [36].</p>
<p>Our evaluation results suggest that BERT's masked language model has the potential to assist in improving the completeness of requirements. Nevertheless, our current work does not attempt to build a user-facing tool for using BERT in requirements completeness checking, nor does it conduct user studies to measure practical benefits. While our findings are encouraging, they do not constitute conclusive evidence of usefulness but
rather represent a necessary first step towards further investigations in the future.</p>
<p>This article extends a previous conference paper [37] published at the 29th Working Conference on Requirements Engineering: Foundation for Software Quality (REFSQ'23). Compared to the conference version, the present article offers enhancements to the background and related work, provides substantial new evaluation that includes a comparison with baselines, and features improvements to the implementation of our approach.</p>
<h3>1.3 Organization</h3>
<p>Section 2 provides background information. Section 3 examines the related work, focusing on completeness checking of NL requirements and NLP techniques in RE. Section 4 presents our approach. Section 5 reports on our experimental design, analysis and results. Section 6 employs human feedback to validate a design choice in our evaluation regarding the matching of predictions with simulated omissions. Section 7 discusses the limitations of our approach and the validity of our findings. Section 8 summarizes the article and suggests directions for future research.</p>
<h2>2 Background</h2>
<p>Below, we review the background for our work, covering the NLP pipeline, large language models, word embeddings, machine learning and corpus extraction.</p>
<h3>2.1 NLP Pipeline</h3>
<p>NLP is a branch of artificial intelligence (AI) that is concerned with automated analysis and representation of natural language, both text and speech [51]. NLP is usually performed using a pipeline of modules [31]. The exact pipeline varies depending on the specific NLP task, and the tools and models used to perform it. The NLP pipeline we need in this article is presented in Fig. 2. We use the annotations produced by this pipeline for several purposes, including the identification and lemmatization of terms in requirements documents as well as processing predictions made by BERT in their surrounding context.</p>
<p>The modules in our NLP pipeline are as follows:
Tokenizer. The tokenizer breaks the text into individual words or tokens. Tokenization can be performed using various techniques such as whitespace-based, rulebased, or statistical methods [32]. For example, using</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 2: NLP Pipeline.
whitespace, the sentence "The model shall be implemented in Python." would be tokenized as: 'the', 'model', 'shall', 'be', 'implemented', 'in', 'Python' and '.'.
Sentence Splitter. The sentence splitter breaks the text into individual sentences. Sentence splitting typically uses punctuation marks, capitalization, and other cues to identify sentence boundaries [32]. It is important to note that the structure of what constitutes a sentence is predefined and may not necessarily correspond to grammatically correct sentences.
Part-of-speech (POS) Tagger. The POS tagger assigns to each word in each sentence a POS label, such as noun, verb, adjective or adverb. Continuing with our previous example, "The model shall be implemented in Python.", the POS tags assigned would be: 'The': DT (determiner), 'model': NN (singular noun), 'shall': MD (modal auxiliary verb), 'be': VB (base form verb), 'implemented': VBN (past participle verb), 'in': IN (preposition), 'Python': NNP (singular proper noun), and '.': PUNCT (punctuation).
Lemmatizer. The lemmatizer reduces words to their base form, known as the lemma. This process helps normalize words with different inflections, allowing the pipeline to treat them as the same entity. Lemmatization improves text analysis by reducing vocabulary size and ensuring words with the same root meaning are treated equally. For example, the lemma for both 'running' and 'ran' is 'run'.</p>
<h3>2.2 Large Language Models (LLMs)</h3>
<p>A large language model (LLM) is a type of artificialintelligence model designed specifically to understand and generate human language. LLMs are typically built using deep-learning techniques, particularly variants of neural networks such as transformers [16]. In this article, our LLM of choice is the Bidirectional Encoder Representations from Transformers (BERT) [16]. BERT is pre-trained using two self-supervised tasks: Masked Language Modelling (MLM) and Next Sentence Prediction (NSP). We use the MLM to identify closely related alternative words that may be relevant but are currently missing from an input RS.</p>
<p>MLM, or the Cloze task, is a procedure of randomly masking a percentage of tokens from a natural-language input and then attempting to predict the masked token [16]. When feeding an input text to BERT, there are three special tokens to take into consideration. '[CLS]' is a classification token appended to the start of every input to demarcate the beginning of the text. '[SEP]' is a separator token to mark the end of one sentence from the beginning of another. And, '[MASK]' is a masking token for the MLM task; '[MASK]' replaces an actual word in a sentence, prompting the prediction of contextualized tokens likely to match the masked word. Continuing with the example from Section 2.1, the sentence "The model shall be implemented in Python." would be modified into "[CLS] The model shall be implemented in Python. [SEP]" before tokenization. If we were to mask the word 'Python', the sentence would be updated to "[CLS] The model shall be implemented in [MASK]. [SEP]". Examples of predictions made by BERT are provided over requirements R1-R3 in Fig. 1.</p>
<p>BERT uses a bidirectional encoder for generating context-aware word representations. The encoder consists of a stack of Transformer blocks that use selfattention mechanisms [48] to encode the input sequence in both directions. BERT Base and BERT Large are two types of the BERT model. BERT Large, while generally more accurate, requires more computational resources. BERT Base has 12 encoder layers with a hidden size of 768,12 self-attention heads, and $\approx 110$ million trainable parameters. Further variations of BERT include cased and uncased models. For BERT uncased, the text has been lower-cased before tokenization, whereas in BERT cased, the tokenized text is the same as the input text. Previous RE research suggests that the cased model is preferred over uncased for analyzing requirements $[30,19]$. To both mitigate computation costs and follow best practices in RE, we employ the BERT-base-cased model for our experimentation.</p>
<h3>2.3 Word Embeddings</h3>
<p>In our work, predictions generated by MLM may have similar meanings in the context of a match against the ground truth, even if the terms themselves are not lexically identical. We thus need a semantic notion of similarity for matching predictions from the MLM against what is desired based on the ground truth. For instance, we aim to establish matches between terms such as (i) 'key' and 'unlock', and (ii) 'encryption' and 'security', despite them not being lexically equivalent. For this, we use cosine similarity over word embeddings. Cosine similarity quantifies the similarity of two words by calculating the distance between their vector representations [38]. To obtain a vector representation, we must first transform each word into its own word embedding. Word embeddings are mathematical representations of words as dense numerical vectors capturing syntactic and semantic regularities [39].</p>
<p>To construct word embeddings, we use GloVe (Global Vectors for Word Representation), an unsupervised machine learning model that creates word embeddings using a co-occurrence matrix [42]. Different versions of GloVe have different dimensionality for embeddings. We use GloVe with a dimensionality of 50 . Our decision to employ GloVe's pre-trained model for obtaining non-contextualized word embeddings [42] is motivated by striking a trade-off between accuracy and efficiency. BERT generates contextualized word embeddings; however, these embeddings are expensive to compute because they take context into consideration. BERT embeddings thus do not scale well when a large number of pairwise term comparisons is required, which happens to be the case in our evaluation.</p>
<h3>2.4 Machine Learning (ML)</h3>
<p>Machine learning (ML) can be divided into three main types: unsupervised learning, supervised learning, and reinforcement learning. Unsupervised learning uses unlabelled data, which lacks predefined categories, to find patterns or relationships in the data without prior knowledge of the output labels. In supervised learning, a labelled dataset is used to learn the relationships between the input features and the output labels. Finally, reinforcement learning focuses on agents interacting with an environment and learning optimal actions through trial and error by receiving feedback in the form of rewards or penalties based on the consequences of their actions. In this article, we use supervised learning for distinguishing relevant from non-relevant predictions made by BERT. Our empirical evaluation (Section 5) examines several widely used supervised ML al-
gorithms, namely Neural Network (NN), Decision Tree (DT), Logistic Regression (LR), Random Forest (RF), and Support Vector Machine (SVM), in order to determine which one(s) are most accurate for our purpose.</p>
<p>Classification techniques rely on the extraction of relevant features from the input data. The features can be nominal, numeric, or ordinal. Numeric features are continuous or discrete numerical values. Nominal features are discrete values that describe some categorical aspect of the data. Ordinal features have an inherent order through ranking to indicate a higher or lower value in relation to one another. Feature selection is the process of selecting a subset of relevant features from the original set of features to improve the accuracy and efficiency of a machine learning model $[34,12]$. The goal is to eliminate irrelevant or redundant features that do not contribute significantly to the model's performance, while retaining the most important features. We employ feature selection to rank the importance of features and ensure that only the most valuable ones are computed and retained. Our features for learning and our process for creating labelled data are discussed in Sections 4 and 5, respectively.</p>
<p>Classification models have a tendency to predict the more prevalent class(es) [49]. Furthermore, classification algorithms typically give equal treatment to different misclassification types when minimizing misclassification. In many problems, however, the costs associated with different misclassification are not symmetric. In our context, non-relevant terms outnumber relevant ones. This increases the likelihood of terms being classified as non-relevant, thus increasing the risk of false negatives (i.e., useful terms being filtered out). We under-sample the majority class (i.e., non-relevant) to counter imbalance in the training set and thus reduce the risk of filtering useful information [9]. We assign a higher penalty to relevant terms being filtered than non-relevant terms being classified as relevant. In other words, we prioritize Recall over Precision which is often necessary in RE tasks. A second strategy we employ in order to prioritize Recall is cost sensitive learning (CSL). CSL can improve accuracy by assigning different misclassification costs to different classes or types of errors, with the aim of minimizing the prevalence of certain types of misclassified data [23]. In our approach, filtering a false negative (i.e., "relevant" prediction) is more detrimental than filtering a false positive (i.e., "non-relevant" prediction). We thus use CSL to adjust the model's parameters and bias it towards reducing the prevalence of false negatives, even if it leads to a higher number of false positives, to ensure that useful predictions do not get filtered.</p>
<h3>2.5 Domain-corpus Extraction</h3>
<p>Domain-specific corpora are useful resources for improving the accuracy of automation in RE [20]. The domain can be any specific subject or field, such as healthcare, telecommunications, or law. Domain-specific corpus extraction is the process of creating a corpus from existing texts or documents that are relevant to a particular domain. In this article, we use domain-specific corpora to enhance the accuracy of classifiers in filtering out non-relevant terms from BERT predictions.</p>
<p>The first step in domain-corpus extraction is to define the domain's scope and determine relevant sources to the domain. If a domain-specific corpus already exists, there may be no need to generate a new one. However, if a suitable corpus is not available, then one can be extracted manually or automatically. Corpus creation can be based on domain documents from sources such as books, magazines, and online resources.</p>
<p>Due to its extensive coverage and diverse range of articles, Wikipedia is among the most common sources used for corpus extraction [20,14,25,21]. Manual corpus extraction involves searching for and selecting relevant texts or documents by hand. This process can be timeconsuming and requires domain expertise to ensure that the selected texts are representative of the domain. Automated corpus extraction navigates a collection of textual documents according to pre-specified criteria to build a corpus. The criteria for extraction may be based on keywords, topic modelling, or named-entity recognition [20]. To avoid burdening users with manual tasks and minimize the cost of using our approach, we opt for automated corpus extraction. To do so, we use an existing tool, named WikiDoMiner [21]. This tool has been specifically designed to generate domain-specific corpora by crawling Wikipedia.</p>
<p>WikiDoMiner allows control over corpus expansion through a depth parameter. A depth of zero creates a corpus of articles directly matching the key phrases in the input document, while increasing the depth results in larger corpora that encompass sub-categories of Wikipedia articles. To better understand how WikiDoMiner queries Wikipedia, consider the scenario borrowed from Ezzini et al. [21] and illustrated in Fig. 3. In Wikipedia, articles are categorized and can belong to multiple hierarchical categories. For instance, a search for the keyword "rail transport" in the Railway domain may yield an article titled "Rail Transport." By analyzing the category structure of this article, we find that it falls under a category with the same name, referred to as Category A in Fig. 3. Further exploration of a sub-category such as "Rail Infrastructure" reveals additional pages and sub-categories. As we ex-
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 3: Traversing Wikipedia Categories [21].
plain in Section 4, we use domain-specific corpora extracted by WikiDoMiner to calculate frequency-based statistics for filtering.</p>
<h2>3 Related Work</h2>
<p>In this section, we discuss and compare with pertinent areas of related work in the RE literature, focusing on (1) completeness checking of NL requirements, and (2) NLP for requirements engineering.</p>
<h3>3.1 Completeness Checking of NL Requirements</h3>
<p>There are various methods for determining the completeness of NL requirements. España et al. [18] measure completeness by comparing use cases against information systems through communication analysis. The authors evaluate the level of completeness achieved by reviewed models against a reference model. Gigante et al. [27] use ontological engineering to determine completeness of requirements. They model requirements in the form of (subject, predicate, object) triplets. These triplets are then compared against an external source to verify completeness.</p>
<p>Eckhardt et al. [17] propose a framework consisting of a unified model for performance requirements and a content model to capture relevant content for addressing incompleteness. This process utilizes sentence patterns derived from the content model to evaluate completeness of requirements. Alrajeh et al. [2] create synthetic obstacles to verify completeness of requirements.</p>
<p>The approach uses model checking and iteratively generates domain-specific obstacles, with the process ending after achieving a domain-complete set of obstacles. The above works cross validate requirements against an external model to measure completeness. Our approach seeks to address the same challenge by using a generative language model, BERT, and its vast pre-training data as a knowledge source for making contextualized predictions. Arora et al. [4] conduct a case study to detect external incompleteness of requirements using domain models. The authors simulate requirements omissions and demonstrate that UML class diagrams can display a near-linear sensitivity to detecting missing and under-specified requirements. Dalpiaz et al. [15] develop a technique based on NLP and visualization to explore commonalities and differences between multiple viewpoints and thereby help stakeholders pinpoint occurrences of ambiguity and incompleteness. Differences may occur when terms appear in a single viewpoint, i.e., the situation where a viewpoint refers to concepts that do not appear in other viewpoints. In the above works, the sources of knowledge used for completeness checking are existing development artifacts. Our approach does not require any user-provided artifacts. We leverage BERT's capacity to predict relevant terminology, independently of supplementary artifacts, to ensure domain independence.</p>
<p>Bhatia et al. [11] address incompleteness in privacy policies by representing data actions as semantic frames. A semantic frame is constructed by identifying relevant questions for the data action, as semantic roles. Semantic roles represent the relationship of different clauses in statements to the main action. They identify the expected semantic roles for a given frame, and consequently determine incompleteness by identifying missing role values. Cejas et al. [3] use NLP and ML for completeness checking of privacy policies. Their approach identifies instances of pre-defined concepts such as "controller" and "legal basis" in a given policy. They create a conceptual model as a hierarchical representation of metadata types referring to different GDPR concepts based on hypothesis coding. It then verifies through rules whether all applicable concepts are covered. The above works deal with privacy policies only and have a predefined conceptual model for textual content. Our BERT-based approach is not restricted to a particular application domain and does not have a fixed conceptualization of the textual content under analysis. Instead, we utilize BERT's pre-training and attention mechanism to make contextualized recommendations for improving completeness.</p>
<h3>3.2 NLP for Requirements Engineering</h3>
<p>Natural Language Processing for Requirements Engineering (NLP4RE) is a field that employs techniques from NLP to address challenges faced in the RE domain. Applications of NLP4RE include terminology extraction [28], requirements similiarity and retrieval [1], user story analysis [35], and legal requirements analysis [46].</p>
<p>The state of the art in NLP4RE has been extensively covered in a recent literature review by Zhao et al. [52]. Two of the papers identified in this literature review utilize BERT-based language modelling, although neither work focuses on evaluating completeness of requirements. The first paper by Hey et al. [30] presents a new method for unsupervised representation learning, called NoRBERT (Non-functional and functional Requirements classification using BERT). NoRBERT can be used for a wide range of NLP tasks, particularly when labeled data is limited or unavailable. The second paper by Sainani et al. [43] uses BERT for automating the extraction and classification of requirements from software engineering contracts. Although both papers employ BERT, neither work applies BERT's MLM task in their approach.</p>
<p>In more recent literature, Shen and Breaux [45] propose an NLP-based approach for extracting domain knowledge from word embeddings and user-authored scenarios. Their approach involves gathering a corpus of scenarios authored by users in four distinct directoryservice domains - apartments, hiking trails, restaurants, and health clinics. Then, the authors extract basic domain models from these scenarios by utilizing typed dependencies. The authors use seed question templates that include a domain-specific noun, seed verb and mask, and utilize MLM to predict substitute tokens for the mask. While this approach is not concerned with checking the completeness of requirements, it uses BERT's MLM for generating alternative entities by masking words in requirements statements. Our approach uses BERT's MLM in a similar manner. In contrast to the above work, we take steps to address the challenge arising from such use of BERT over requirements, namely the large number of non-relevant alternatives (false positives) generated. We propose a ML-based filter that uses a combination of NLP and statistics extracted from a domain-specific corpus to reduce the incidence of false positives.</p>
<h2>4 Approach</h2>
<p>Figure 4 provides an overview of our approach. The input to the approach is a (textual) requirement speci-</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig. 4: Approach Overview.</p>
<p>fication (RS). The approach has six steps. The first step is to parse the RS. The second step is to generate predictions for masked words using BERT. The third step is to remove the predictions that provide little or no additional information. The fourth step is to construct a domain-specific corpus for the given RS. Using this corpus and the results from Step 1, the fifth step is to build a feature matrix for ML-based filtering of non-relevant terms from predictions by BERT. The sixth and last step is to feed the computed feature matrix to a (pretrained) classifier in an attempt to remove noise (non-relevant words) from the predictions. The output of the approach is a list of recommended terms that are likely relevant to the RS but are currently absent from it.</p>
<h3>4.1 Parsing RS using NLP</h3>
<p>To recommend relevant terms that may be missing from a given RS, we first apply the RS the NLP pipeline presented in Section 2. Annotations resulting from this step include tokens, sentences, POS tags, and lemmas. Subsequent steps utilize the POS tags for masking and predicting terms.</p>
<h3>4.2 Obtaining Predictions from BERT</h3>
<p>The core meaning-bearing elements of requirements are <em>nouns</em> and <em>verbs</em> [33, 4]. We iterate through each sentence in the annotated RS derived from Step 1. Based on the POS tags from Step 1, we mask one noun or one verb at a time, creating a sentence with a single concealed word. Consider the sentence: "The system shall generate reports on inventory levels, product movement, and sales history." The POS tags for the tokens in this sentence are: 'The' (DT); 'system' (NN); 'shall' (MD); 'generate' (VB); 'reports' (NNS); 'on' (IN); 'inventory' (NN); 'levels' (NNS); ',' (COMMA); 'product' (NN); 'movement' (NN); ',' (COMMA); 'and' (CC); 'sales' (NNS); 'history' (NN); '.' (PERIOD). For this sentence, examples of sentences with one concealed word are: "The system shall [MASK] reports on inventory levels, product movement, and sales history", and "The system shall generate [MASK] on inventory levels, product movement, and sales history". Each modified sentence is inputted to BERT, which generates a configurable number of predictions for the masked word. For example, in our illustration shown in Fig. 1, we use BERT to generate five predictions per masked word. Based on our empirical evaluation in Section 5, we recommend using 15 predictions per masked word. BERT generates a probability score for each prediction, indicating its level of confidence. We retain the probability scores for use in Step 5 of our approach.</p>
<h3>4.3 Removing Obviously Unuseful Predictions</h3>
<p>We attempt to improve the accuracy of our approach by removing predictions that are clearly not useful. The first category of predictions we discard are those that are already present in the RS. Consider the following sentence: "The system shall provide a programmable interface to support system integration." If the word 'integration' is masked and BERT happens to predict exactly the same term back, then we do not include the prediction in the prediction list, because it does not offer any new insights. We identify two further categories of predictions that are unlikely to contribute meaningfully to the final output: (1) predictions that fall within the top 250 most commonly used words in the English language, since these words are too generic, and (2) predictions that fall within vague words and stop words for requirements, as per the lists compiled by Berry et al. [10] and Arora et al. [5] [6]. The end result of this step is a more focused list of predictions that is cleared of obviously unhelpful terms.</p>
<h3>4.4 Generating Domain-specific Corpus for RS</h3>
<p>In Step 4 of our approach, we use WikiDoMiner (introduced in Section 2.5) to automatically extract a domainspecific corpus for an input RS [21].</p>
<p>For example, if an aerospace engineering RS is fed into WikiDoMiner, the resulting corpus may contain articles on aircrafts, aviation, and fluid dynamics. Recall that as the depth parameter of WikiDoMiner increases, the corpus grows larger, encompassing more sub-categories of Wikipedia articles at each level. In our work, we limit our search to direct article matches only (i.e., depth $=0$ ). This decision was motivated by the following considerations: First, by limiting the depth to zero, we significantly reduce the volume of text being processed, enabling faster corpus generation. Second, focusing on direct article matches helps scope the expansion of terminology to the content that is immediately relevant to the domain of the input RS. During our exploratory investigation, we discovered that increasing the depth value results in diluting domainspecificity. Our decision to use a depth value of zero strikes a balance between corpus size and domain coverage, yielding better-suited corpora for our purposes. Step 5 uses the domain-specific corpus to compute features, which are subsequently employed for filtering nonrelevant predictions.</p>
<h3>4.5 Building Feature Matrix for Filtering</h3>
<p>For each prediction from Step 3, we compute a feature vector as input for a ML-based classifier that decides whether the prediction is "relevant" or "non-relevant" to the input RS. Our features are listed and explained in Table 1.</p>
<p>The main principle behind our feature design has been to keep the features generic and in a normalized form. Being generic is important because we do not want the features to rely on any particular domain or terminology. Having the features in a normalized form is important for allowing labelled data from multiple documents to be combined for training, and for the resulting ML models to be applicable to unseen documents. The output of this step is a feature matrix where each row represents a prediction (from Step 3) and each column represents a feature as defined in Table 1.</p>
<p>Most of the features listed in Table 1 can be easily understood based on the accompanying definitions. Below, we provide additional explanation for F10-F13 which are more involved. F10 and F11 are based on quantile bucketing. To categorize the frequency of predictions into discrete intervals or "buckets", we can divide the terms into equal-sized groups based on per-
centiles. Suppose that we have a bag of 1000 predictions generated by BERT, and that we wish to create 10 quantile buckets based on the frequency of these predictions. To do so, we first count the number of occurrences of each predicted word and sort them in descending order of frequencies. We then divide the words into 10 equally-sized groups based on their rank to create the quantile buckets. We assign the most frequently predicted words to bucket 0 , and the least frequently predicted words to bucket 9 . F12 and F13 are based on Term Frequency-Inverse Document Frequency (TFIDF) [44] - a common technique for measuring the importance of terms (words) in a particular domain.</p>
<h3>4.6 Filtering Noise from Predictions</h3>
<p>The predictions from Step 3 are noisy (i.e., have many false positives). To reduce the noise, we subject the predictions to a pre-trained ML-based filter. The most accurate ML algorithm for this purpose is selected empirically (see RQ3 in Section 5). The selected algorithm is trained on the development and training portion of our dataset ( $P_{1}$ in Table 2, as we discuss in Section 5). Due to our features in Table 1 being generic and normalized, the resulting ML model can be used as-is over unseen documents without re-training (see RQ4 in Section 5 for evaluation of effectiveness). The output of this step is the list of BERT predictions that are classified as "relevant" by our filter; duplicates are excluded from the final results.</p>
<h2>5 Evaluation</h2>
<p>In this section, we empirically evaluate our approach. During the process, we also build the pre-trained ML model required by Step 6 of the approach (Fig 4).</p>
<h3>5.1 Research Questions (RQs)</h3>
<p>Our evaluation answers the following RQs using part of the PURE dataset [26]. In lieu of expert input about incompleteness for the documents in this dataset, we apply the withholding strategy discussed in Section 1.2 to simulate incompleteness.
RQ1. How accurately can BERT predict relevant but missing terminology for an input RS? The number of predictions generated by BERT per mask is a configurable parameter. RQ1 identifies the optimal value offering the best balance for producing useful recommendations. This investigation will focus on the optimal number of predictions per mask between</p>
<p>Table 1: Features for Learning Relevance and Non-relevance of Predictions Made by BERT.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">ID</th>
<th style="text-align: left;">Type (T), Definition (D) and Intuition (I)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">F1</td>
<td style="text-align: left;">(T) Nominal (D) POS tag of the masked word (noun or verb). (I) This feature is helpful if nouns and <br> verbs happen to influence relevance in different ways. <br> (T) Nominal (D) POS tag of the prediction; this is obtained by replacing the masked word with the <br> predicted word and running the NLP pipeline on the resulting sentence. (I) The intuition is similar to <br> F1, except that predictions are not necessarily nouns or verbs and can, e.g., be adjectives or adverbs.</td>
</tr>
<tr>
<td style="text-align: left;">F3</td>
<td style="text-align: left;">(T) Nominal (Boolean) (D) True if F1 and F2 match; otherwise, False. (I) A mismatch between F1 and <br> F2 could be an indication that the prediction is non-relevant.</td>
</tr>
<tr>
<td style="text-align: left;">F4</td>
<td style="text-align: left;">(T) Numeric (D) Length (in characters) of the masked word. (I) Words that are too short may give <br> little information. As such, predictions resulting from masking short words could be non-relevant.</td>
</tr>
<tr>
<td style="text-align: left;">F5</td>
<td style="text-align: left;">(T) Numeric (D) Length (in characters) of the prediction. (I) Predictions that are too short could be <br> non-relevant.</td>
</tr>
<tr>
<td style="text-align: left;">F6</td>
<td style="text-align: left;">(T) Numeric (D) min (F4, F5)/max (F4, F5). (I) A small ratio (i.e., a large difference in length between <br> the prediction and the masked word) could indicate non-relevance.</td>
</tr>
<tr>
<td style="text-align: left;">F7</td>
<td style="text-align: left;">(T) Numeric (D) The confidence score that BERT provides alongside the prediction. (I) A prediction <br> with a high confidence score could have an increased likelihood of being relevant.</td>
</tr>
<tr>
<td style="text-align: left;">F8</td>
<td style="text-align: left;">(T) Numeric (D) Levenshtein distance between the prediction and the masked word. (I) A small Lev- <br> enshtein distance between the prediction and the masked word could indicate relevance.</td>
</tr>
<tr>
<td style="text-align: left;">F9</td>
<td style="text-align: left;">(T) Numeric (D) Semantic similarity computed as cosine similarity over word embeddings. (I) A pre- <br> diction that is close in meaning to the masked word could have a higher likelihood of being relevant.</td>
</tr>
<tr>
<td style="text-align: left;">F10*</td>
<td style="text-align: left;">(T) Ordinal (D) A value between zero and nine, indicating how frequently the prediction (in lemmatized <br> form) appears across all BERT-generated predictions over a given RS. (I) A smaller value could indicate <br> a higher likelihood of relevance.</td>
</tr>
<tr>
<td style="text-align: left;">F11* ${ }^{\dagger}$</td>
<td style="text-align: left;">(T) Ordinal (D) A value between zero and nine, indicating how frequently the prediction (in lemmatized <br> form) appears in the domain-specific corpus. (I) A smaller value could indicate a higher likelihood of <br> relevance.</td>
</tr>
<tr>
<td style="text-align: left;">F12 ${ }^{\dagger}$</td>
<td style="text-align: left;">(T) Numeric (D) Average TF-IDF rank of the prediction across all articles in the domain-specific corpus. <br> (I) A higher rank could indicate a higher likelihood of relevance.</td>
</tr>
<tr>
<td style="text-align: left;">F13 ${ }^{\dagger}$</td>
<td style="text-align: left;">(T) Numeric (D) Maximum TF-IDF rank of the prediction across all articles in the domain-specific <br> corpus. (I) Same intuition as that for F12.</td>
</tr>
</tbody>
</table>
<p>*Zero is most frequent (top ten percentile) and nine is least frequent (bottom ten percentile). ${ }^{\dagger}$ Feature uses domain-specific corpus. ${ }^{\ddagger}$ TF-IDF values are normalized by Euclidean norm.
the range of 5 and 20 . Choosing an optimal number of predictions is essential for ensuring that our approach maintains the potential to provide benefits. If the number of predictions is too low, the approach may miss out on relevant recommendations. Conversely, if the number of predictions is too high, the approach may generate too much noise, thus making it difficult for users to identify the most relevant recommendations.
RQ2. How does our approach compare to baselines? RQ2 examines whether the contextualized recommendations made by BERT are advantageous over recommendations that one can obtain through simpler means. To this end, we conduct a comparative analysis of the quality of predictions generated by our approach against three baseline methods.
RQ3. Which ML classification algorithm most accurately filters unuseful predictions made by BERT? Useful recommendations from BERT come
alongside a considerable amount of noise. RQ3 investigates different ML algorithms for filtering this noise. The RQ further explores the impact of data balancing and cost-sensitive learning to mitigate over-filtering.
RQ4. How accurate are the recommendations generated by our approach over unseen documents? In RQ4, we combine the best BERT configuration from RQ1 with the top-performing filter models from RQ3, and measure the accuracy of this combination over unseen data.</p>
<h3>5.2 Implementation and Availability</h3>
<p>Figure 5 presents an overview of the implementation of our approach. The implementation is mostly in Python. Our NLP pipeline is implemented using SpaCy version 3.2.2. To extract word embeddings, we use GloVe [42].</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Fig. 5: Tool Overview.</p>
<p>For obtaining masked language model predictions from BERT, we use the Transformers library version 4.16.2 by Hugging Face (https://huggingface.co/), operated in PyTorch version 1.10.2+cu113. Our ML-based filters are implemented using WEKA 3-8-5 [50] - a lightweight tool for data mining and knowledge discovery. We use standard implementations of Levenshtein distance and cosine similarity (over word embeddings) to implement features F8 and F9 of Table 1, respectively. The TF-IDF-based features in Table 1, namely F12 and F13, are calculated using the TfidfVectorizer from scikit-learn version 1.0.2. The corpus required for the TF-IDF-based features is automatically constructed using the WikiDoMiner tool [21]. In addition, WordNet [22] is used for discovering synonyms in one of our baselines (Baseline 3 as we discuss under EXPII in Section 5.4). All our implementation and evaluation artifacts are publicly available [36].</p>
<h3>5.3 Dataset</h3>
<p>Our evaluation is based on the PURE (Public Requirements) dataset [26]. PURE is a collection of 79 publicly available requirements documents, containing approximately 34,000 sentences. In our evaluation, we use 40 out of the 79 documents in PURE. Many of the documents in PURE require manual cleanup (e.g., removal of table of contents, headers, section markers, etc.).</p>
<p>Table 2: Our Dataset (Subset of PURE [26]). $P_{1}$ is for development and training and $P_{2}$ for testing.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Training $\left(\mathbf{P}_{1}\right)$</th>
<th style="text-align: center;">Testing $\left(\mathbf{P}_{2}\right)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Domain</td>
<td style="text-align: center;">Security</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">sprat, <br> ccms, dii</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Finance</td>
<td style="text-align: center;">gamma, <br> jse</td>
<td style="text-align: center;">e-procurement</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Administration</td>
<td style="text-align: center;">tachonet, <br> nasa x38, <br> nenios, <br> libra</td>
<td style="text-align: center;">inventory</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Astronomy</td>
<td style="text-align: center;">evla back, <br> gemini</td>
<td style="text-align: center;">esa, <br> telescope</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Energy</td>
<td style="text-align: center;">pnnl</td>
<td style="text-align: center;">themas, <br> elsfork</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Communications</td>
<td style="text-align: center;">philips, <br> ctc network</td>
<td style="text-align: center;">agentmom, <br> tcs</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Hardware <br> Design</td>
<td style="text-align: center;">beyond</td>
<td style="text-align: center;">evla corr</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Medicine</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">micro care</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Databases</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">npac</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Games</td>
<td style="text-align: center;">space fractions, <br> multi-mahjong</td>
<td style="text-align: center;">qheadache</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Art</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">colorcast</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Weather</td>
<td style="text-align: center;">clarus low, <br> grid bgc</td>
<td style="text-align: center;">clarus high</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Legal</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">ijis</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Transport</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">rlcs</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">UX/Visualization</td>
<td style="text-align: center;">watcom gui, <br> sce api,</td>
<td style="text-align: center;">grid 3D</td>
</tr>
<tr>
<td style="text-align: center;">Statistics</td>
<td style="text-align: center;">Number of <br> sentences</td>
<td style="text-align: center;">11712</td>
<td style="text-align: center;">12694</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Number of tokens</td>
<td style="text-align: center;">185552</td>
<td style="text-align: center;">193926</td>
</tr>
</tbody>
</table>
<p>We found 40 to be a good compromise between the effort needed to spend on cleanup and having a dataset that is large enough for statistical significance testing, mitigating the effects of random variation, and training ML-based filters. The selected documents, listed in Table 2, cover 15 domains. We partition the documents into two (disjoint) subsets $P_{1}$ and $P_{2} . P_{1}$ is used for answering RQ1, RQ2, and RQ3; and, $P_{2}$ is used for answering RQ4. Our procedure for assigning documents to $P_{1}$ or $P_{2}$ was as follows: We first randomly selected one document per domain and put it into $P_{2}$; this is to maximize domain representation in RQ4. From the rest, we randomly selected 20 documents for inclusion in $P_{1}$, while attempting to have $P_{1}$ represent half of the data in terms of token count. Any remaining document after this process was assigned to $P_{2}$, thus giving us 20 documents in $P_{2}$ as well. Table 2 provides domain information and summary statistics for documents in $P_{1}$ and $P_{2}$ after cleanup.</p>
<h3>5.4 Analysis Procedure</h3>
<p>EXPI. This experiment answers RQ1. For every document $p \in P_{1}$, we randomly partition the set of sentences in $p$ into two subsets of (almost) equal sizes. In line with our arguments in Section 1, we disclose one of these subsets to BERT and withhold the other. We apply Steps 1, 2 and 3 of our approach (Fig. 4) to the disclosed portion, as if this portion were the entire input document. We run Step 2 of our approach with four different numbers of predictions per mask: $5,10,15$, and 20 . For every document $p \in P_{1}$, we compute two metrics, Accuracy and Coverage, defined in Section 5.5. As the number of predictions per mask increases from 5 to 20 , the predictions made by BERT reveal more terms that are relevant to the withheld portion. Nevertheless, as we will see in Section 5.6, the benefits diminish beyond 15 predictions per mask.</p>
<p>To ensure that the trends we see as we increase the number of predictions per mask are not due to random variation, we pick different shuffles of each document $p$ across different numbers of predictions per mask. For example, the disclosed and withheld portions for a given document $p$ when experimenting with 5 predictions per mask are different random subsets than when experimenting with 10 predictions per mask.
EXPII. This experiment answers RQ2. To ensure that our approach is worthwhile, we need to compare it to baselines. Since our approach uses an LLM as its external source of knowledge, we cannot compare it to existing external completeness checking approaches which require additional artifacts such as domain models or interview transcripts; such artifacts are not available for our requirements dataset. To demonstrate that our approach is worthwhile, we define three common-sense baselines that are not based on LLMs and, at the same time, do not require any purpose-built artifacts. The first baseline, Baseline 1, is the list of 250 -to-2000 most common words in the English language. This baseline enables us to assess whether BERT leads to contextualized predictions that have greater specificity than generic recommendations. The second baseline, Baseline 2, uses TF-IDF obtained over the domain-specific corpus generated in Step 4 of our approach. Specifically, the baseline recommends all terms with a TF-IDF score exceeding a predetermined threshold, which we have set at 0.01 . This baseline captures the most relevant terminology in a domain. By comparing against Baseline 2, we examine whether BERT's context-specific predictions have an advantage over the domain-specific but non-contextualized terms derived from a corpus. The third baseline, Baseline 3, collects WordNet synonyms of the words found in the disclosed portion of
a given RS. Comparing with Baseline 3 allows us to assess whether the predictions generated by BERT extend beyond synonyms that can be obtained through simpler methods than an LLM. Appendix A outlines our three baselines in pseudo-code form. To facilitate a direct comparison with EXPI results, we apply the three baselines to $P_{1}$ and evaluate their performance using the Accuracy and Coverage metrics (defined in Section 5.5).
EXPIII. This experiment answers RQ3 and further constructs the training set for the ML classifier in Step 6 of our approach (Fig. 4). We recall the disclosed and withheld portions as defined in EXPI. For every document $p \in P_{1}$, we label predictions as "relevant" or "non-relevant" using the following procedure: Any prediction matching some term in the withheld portion is labelled "relevant". The criterion for deciding whether two terms match is a cosine similarity of $\geq 85 \%$ over GloVe word embeddings (introduced in Section 2). All other predictions are labelled "non-relevant". The threshold of $85 \%$ allows only terms with the same lemma or with very high semantic similarity to be matched. In Section 6, we empirically justify the chosen threshold, ensuring its conservative nature to minimize superfluous matches.</p>
<p>For each prediction, a set of features is calculated as detailed in Step 5 of our approach. It is paramount to note that Step 4, which is a prerequisite to Step 5, exclusively uses the content of the disclosed portion without any knowledge of the withheld portion. The above process produces labelled data for each $p \in P_{1}$. We aggregate all the labelled data into a single training set. This is possible because our features (listed in Table 1) are generic and normalized. This process ensures that the ML-based filter is trained on a diverse range of data, allowing it to effectively filter out non-relevant predictions and yield more accurate results.</p>
<p>Equipped with a training set, we compare five ML algorithms: Feed Forward Neural Network (NN), Decision Tree (DT), Logistic Regression (LR), Random Forest (RF) and Support Vector Machine (SVM). All algorithms are tuned with optimal hyperparameters that maximize classification accuracy over the training set. For tuning, we apply multisearch hyperparameter optimization using random search [8]. The basis for tuning and comparing algorithms is ten-fold cross validation. We experiment with under-sampling the "non-relevant" class with and without cost-sensitive learning (CSL); the motivation is reducing false negatives (i.e., relevant terms incorrectly classified as "non-relevant"). For CSL, we assign double the cost (penalty) to false negatives compared to false positives (i.e., noise). We further assess the importance of our features using information</p>
<p>gain (IG) [49]. In our context, IG measures how efficient a given feature is in discriminating "non-relevant" from "relevant" predictions. A higher IG value implies a higher discriminative power.
$E X P I V$. This experiment answers RQ4 by applying our end-to-end approach to unseen requirements documents, i.e., $P_{2}$. To conduct EXPIV, we need a pretrained classifier for Step 6 of our approach (Fig. 4). This classifier needs to be independent of $P_{2}$. We build this classifier using the training set derived from $P_{1}$, as discussed in EXPIII. EXPIV follows the same strategy as in EXPI, which is to randomly withhold a portion of each document $p$ (now in $P_{2}$ rather than in $P_{1}$ ) and attempting to predict the novel terms of the withheld portion. In contrast to EXPI, in EXPIV, predictions made by BERT are post-processed by a filter to reduce noise. Furthermore, whereas EXPI split each document into two roughly equal portions, i.e., a 50-50 split, EXPIV considers two different split ratios: a 50-50 split similar to EXPI, as well as a 90-10 split, where $90 \%$ of the document is disclosed and only $10 \%$ is withheld. The 50-50 split enables us to examine the usefulness of our approach where there is major incompleteness (i.e., half of the content is missing). The 90-10 split represents the situation where there is minor incompleteness.</p>
<p>We repeat EXPIV five times for each $p \in P_{2}$. This mitigates random variation resulting from the random selection of the disclosed and withheld portions, thus yielding more realistic ranges for performance. In EXPIV, we study three levels of filtering for the two split ratios considered. Noting that there are 20 documents in $P_{2}$, the results reported for EXPIV are based on 20 (documents) * 5 (repetitions) * 3 (filtering levels) * 2 (split ratios) $=600$ runs of our approach.</p>
<h3>5.5 Metrics</h3>
<p>We define separate metrics for measuring (1) the quality of term predictions and (2) the performance of filtering. The first set of metrics is used in RQ1, RQ2 and RQ4; and, the second set is used in RQ3. To define our metrics, we need to introduce some notation. Let Lem : bag $\rightarrow$ bag be a function that takes a bag of words and returns another bag of words by lemmatizing every element in the input bag. Let U : bag $\rightarrow$ set be a function that removes duplicates from a bag and returns a set. Let $C$ denote the set of common words and stopwords as explained under Step 3 in Section 4. Given a document $p$ treated as a bag of words, the terminological content of $p$ 's disclosed portion, denoted $h_{1}$, is given by set $X=\mathrm{U}\left(\operatorname{Lem}\left(h_{1}\right)\right)$. In a similar vein, the terminological content of $p$ 's withheld portion, de-
noted $h_{2}$, is given by set $Y=\mathrm{U}\left(\operatorname{Lem}\left(h_{2}\right)\right)$. What we would like to achieve through BERT is to predict as much of the novel terminology in the withheld portion as possible. This novel terminology can be defined as set $N=(Y-X)-C$. Let bag $V$ be the output of Step 3 (Fig. 4) when the approach is applied exclusively to the disclosed portion of a given document (i.e., $h_{1}$ ). Note that $V$ is already free of any terminology that appears in the disclosed portion, as well as of all common words and stopwords.
Quality of term predictions. Let set $D$ denote the (duplicate-free) lemmatized predictions that have the potential to hint at novel terminology in the withheld portion of a given document. Formally, let $D=\mathrm{U}(\operatorname{Lem}(V))$. We define two metrics, Accuracy and Coverage to measure the quality of $D$. Accuracy is the ratio of terms in $D$ matching some term in $N$, to the total number of terms in $D$. That is, Accuracy $=|{t \in$ $D$ s.t. $t$ matches some $\left.t^{\prime} \in N\right}| /|D|$. A term $t$ matches another term $t^{\prime}$ if the word embeddings have a cosine similarity of $\geq 85 \%$ (already discussed under EXPIII in Section 5.4). The second metric, Coverage, is defined as the ratio of terms in $N$ matching some term in $D$, to the total number of terms in $N$. That is, Coverage $=|{t \in N$ s.t. $t$ matches some $t^{\prime} \in D}| /|N|$. The intuition for Accuracy and Coverage is the same as that for the standard Precision and Recall metrics, respectively. Nevertheless, since our matching is inexact and based on a similarity threshold, it is possible for more than one term in $D$ to match an individual term in $N$. Coverage, as we define it, excludes multiple matches, providing a measure of how much of the novel terminology in the withheld portion is hinted at by BERT.
Quality of filtering. As explained earlier, our filter is a binary classifier to distinguish relevance and nonrelevance for the outputs from BERT. To measure filtering performance, we use the standard metrics of Classification Accuracy, Precision and Recall. True positive (TP), false positive (FP), true negative (TN) and false negative (FN) are defined as follows: A TP is a classification of "relevant" for a term that has a match in set $N$ (defined earlier). A FP is a classification of "relevant" for a term that does not have a match in $N$. A TN is a classification of "non-relevant" for a term that does not have a match in $N$. A FN is a classification of "non-relevant" for a term that does have a match in $N$. Classification Accuracy is calculated as $(T P+T N) /(T P+T N+F P+F N)$. Precision is calculated as $T P /(T P+F P)$ and Recall as $T P /(T P+F N)$. We note that the Classification Accuracy metric defined for filtering is distinct from the Accuracy metric defined for term predictions by BERT, and it is consistently referred to as such to prevent ambiguity.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" />
(a) Accuracy.
<img alt="img-6.jpeg" src="img-6.jpeg" />
(b) Coverage.</p>
<p>Fig. 6: (a) Accuracy and (b) Coverage for Different Numbers of Predictions per Mask</p>
<h3>5.6 Results</h3>
<p>RQ1. Figures 6a and 6b provide boxplots for Accuracy and Coverage with the number of predictions by BERT ranging from 5 to 20 in increments of 5 . Each boxplot is based on 20 datapoints; each datapoint represents one document in $P_{1}$.</p>
<p>We perform statistical significance tests on the obtained metrics using Wilcoxon's rank sum test [13] and Vargha-Delaney's $\hat{A}<em 12="12">{12}$ [47]. Wilcoxon's rank-sum test is a non-parametric statistical test used to assess whether there is a significant difference between the distributions of two independent samples by comparing the ranks of the observations. Vargha-Delaney's $\hat{A}</em> 12$ value ranges from 0 to 1 , where 0.5 indicates no difference between the two groups. Values less than 0.5 suggest the second group tends to}$ is a non-parametric effect size measure that quantifies the magnitude of the difference between two groups. This metric measures the likelihood that an observation from one group is greater than an observation from another group. In our case, each group represents performance readings from a given approach for producing term recommendations. The $\hat{A</p>
<p>Table 3: Statistical Tests for the Results of Fig. 6.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Accuracy</th>
<th style="text-align: center;">Coverage</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">5 vs. 10</td>
<td style="text-align: center;">p-value</td>
<td style="text-align: center;">$&lt;&lt;0.01$</td>
<td style="text-align: center;">$&lt;&lt;0.01$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\hat{\mathrm{A}}_{12}$</td>
<td style="text-align: center;">$0.76(\mathrm{~L})$</td>
<td style="text-align: center;">$0.21(\mathrm{~L})$</td>
</tr>
<tr>
<td style="text-align: center;">5 vs. 15</td>
<td style="text-align: center;">p-value</td>
<td style="text-align: center;">$&lt;&lt;0.001$</td>
<td style="text-align: center;">$&lt;&lt;0.001$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\hat{\mathrm{A}}_{12}$</td>
<td style="text-align: center;">$0.89(\mathrm{~L})$</td>
<td style="text-align: center;">$0.15(\mathrm{~L})$</td>
</tr>
<tr>
<td style="text-align: center;">5 vs. 20</td>
<td style="text-align: center;">p-value</td>
<td style="text-align: center;">$&lt;&lt;0.01$</td>
<td style="text-align: center;">$&lt;&lt;0.01$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\hat{\mathrm{A}}_{12}$</td>
<td style="text-align: center;">$0.96(\mathrm{~L})$</td>
<td style="text-align: center;">$0.13(\mathrm{~L})$</td>
</tr>
<tr>
<td style="text-align: center;">10 vs. 15</td>
<td style="text-align: center;">p-value</td>
<td style="text-align: center;">$&lt;0.01$</td>
<td style="text-align: center;">0.22</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\hat{\mathrm{A}}_{12}$</td>
<td style="text-align: center;">$0.73(\mathrm{M})$</td>
<td style="text-align: center;">$0.39(\mathrm{~S})$</td>
</tr>
<tr>
<td style="text-align: center;">10 vs. 20</td>
<td style="text-align: center;">p-value</td>
<td style="text-align: center;">$&lt;&lt;0.01$</td>
<td style="text-align: center;">0.21</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\hat{\mathrm{A}}_{12}$</td>
<td style="text-align: center;">$0.83(\mathrm{~L})$</td>
<td style="text-align: center;">$0.38(\mathrm{~S})$</td>
</tr>
<tr>
<td style="text-align: center;">15 vs. 20</td>
<td style="text-align: center;">p-value</td>
<td style="text-align: center;">0.24</td>
<td style="text-align: center;">0.88</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\hat{\mathrm{A}}_{12}$</td>
<td style="text-align: center;">$0.61(\mathrm{~S})$</td>
<td style="text-align: center;">$0.49(\mathrm{~N})$</td>
</tr>
</tbody>
</table>
<p>Effect size: Large (L), Medium (M), Small (S), Negligible (N)
have higher values, while values greater than 0.5 suggest the first group tends to have higher values. For instance, if $\hat{A} 12$ equals 0.8 when comparing the performance of two approaches, $A$ and $B$, it implies that there is an $80 \%$ probability that a randomly chosen performance result from $A$ will have a higher value than a randomly chosen performance result from $B$, suggesting a notable advantage of $A$ over $B$. The $\hat{A}_{12}$ is typically categorized as negligible, small, medium, or large based on the computed numeric value. Negligible signifies a very small difference, small implies a modest difference, medium indicates a moderate difference, and large denotes a substantial difference. We apply widely used thresholds, as suggested by Hess and Kromrey [29], to derive these categories.</p>
<p>Table 3 shows the results of the statistical tests. Each row in the table compares Accuracy and Coverage across two levels of predictions per mask. For example, the 5 vs. 10 row compares the metrics for when BERT generates 5 predictions per mask versus when it generates 10 .</p>
<p>For Accuracy, Fig. 6a shows a downward trend as the number of predictions per mask increases. Based on Table 3, the decline in Accuracy is statistically significant with each increase in the number of predictions, the exception being the increase from 15 to 20 , where the decline is not statistically significant. For Coverage, Fig. 6b shows an upward but saturating trend. Five predictions per mask is too few: all other levels are significantly better. Twenty is too many, notably because of the lack of a significant difference for Coverage in the 10 vs. 20 row of Table 3. The choice is thus between 10 and 15 . We select 15 as this yields an average increase of $3.2 \%$ in Coverage compared to 10 predictions per mask. This increase is not statistically significant. Nevertheless, the price to pay is an average decrease of $(14.12-11.97=) 2.15 \%$ in Accuracy. Given</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Fig. 7: (a) Accuracy and (b) Coverage of Baselines Compared to BERT</p>
<p>Table 4: Statistical Tests for the Results of Fig. 7.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Accuracy</th>
<th style="text-align: center;">Coverage</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Unfiltered vs.</td>
<td style="text-align: center;">p-value</td>
<td style="text-align: center;">$&lt;&lt;0.01$</td>
<td style="text-align: center;">$&lt;&lt;0.01$</td>
</tr>
<tr>
<td style="text-align: center;">Baseline 1</td>
<td style="text-align: center;">Â12</td>
<td style="text-align: center;">$0.05(\mathrm{~L})$</td>
<td style="text-align: center;">$0.10(\mathrm{~L})$</td>
</tr>
<tr>
<td style="text-align: center;">Unfiltered vs.</td>
<td style="text-align: center;">p-value</td>
<td style="text-align: center;">0.05</td>
<td style="text-align: center;">$&lt;&lt;0.01$</td>
</tr>
<tr>
<td style="text-align: center;">Baseline 2</td>
<td style="text-align: center;">Â12</td>
<td style="text-align: center;">$0.32(\mathrm{M})$</td>
<td style="text-align: center;">$0.00(\mathrm{~L})$</td>
</tr>
<tr>
<td style="text-align: center;">Unfiltered vs.</td>
<td style="text-align: center;">p-value</td>
<td style="text-align: center;">$&lt;&lt;0.01$</td>
<td style="text-align: center;">$&lt;&lt;0.01$</td>
</tr>
<tr>
<td style="text-align: center;">Baseline 3</td>
<td style="text-align: center;">Â12</td>
<td style="text-align: center;">$0.01(\mathrm{~L})$</td>
<td style="text-align: center;">$0.00(\mathrm{~L})$</td>
</tr>
</tbody>
</table>
<p>Effect size: Large (L), Medium (M), Small (S), Negligible ( $N$ )
the importance of Coverage, we deem 15 to be a better compromise than 10 .</p>
<p>The answer to RQ1 is: When requirements omissions are simulated by withholding, having BERT make 15 predictions per mask is the best trade-off for detecting missing terminology. BERT predicts terms that, on average, hint at $\approx 4$ out of 10 omissions (Coverage $\approx 38 \%$ ). On average, $\approx 1$ in 8 predictions is relevant (Accuracy $\approx 12 \%$ ).
$\boldsymbol{R} \boldsymbol{Q} \boldsymbol{2}$. Figures 7 a and 7 b respectively compare the Accuracy and Coverage of the three baselines described in Section 5.4 (EXPII) against the best BERT configuration identified in RQ1 (i.e., with 15 predictions made per mask). Statistical significance testing results are provided in Table 4. BERT outperforms all three baselines in terms of both Accuracy and Coverage. Two remarks need to be made regarding the baseline results: First, the Coverage of Baselines 1 and 2 can be increased by respectively increasing the number of most common words and adjusting the TF-IDF cutoff threshold (see EXPII in Section 5.4). However, doing so entails a trade-off, as it leads to a decline in Accuracy for these baselines. Second, while the inclusion of Baselines 1 and 2 is important for benchmarking BERT's performance, a fundamental distinction exists between these two baselines and both our BERT-based solution and Baseline 3. BERT and Baseline 3 can trace their recommendations to the elements in the input requirements document. In contrast, Baseline 1 produces the same results regardless of the input requirements document. And, as for Baseline 2, there is no direct and easily interpretable connection between the recommendations and the requirements from which they originated. Although this difference is not reflected in the Accuracy and Coverage results, it is still significant for developing a practical solution. In real-world scenarios, engineers are unlikely to be interested in reviewing a list of recommendations without an explanation regarding the basis for these recommendations.</p>
<p>The answer to RQ2 is: None of the three baselines discussed in Section 5.4 present a better alternative to BERT due to a significant Coverage deficit coupled with lower Accuracy.
$\boldsymbol{R} \boldsymbol{Q} \boldsymbol{3}$. Table 5 shows the results for ML-algorithm selection using the full $\left(P_{1}\right)$ training set ( 61,996 datapoints), the under-sampled training set ( 36,842 datapoints), and the under-sampled training set alongside CSL. Classification Accuracy, Precision and Recall are calculated using ten-fold cross validation. In the table, we highlight the best result for each metric in bold. When one uses the full training set (option 1) or the under-sampled training set without CSL (option 2), Random Forest (RF) turns out to be the best alternative. When the under-sampled training set is combined with CSL (option 3), RF still has the best Accuracy and Precision. However, Support Vector Machine (SVM) presents a moderate advantage in terms of Recall. Since option 3 is meant at further improving the filter's Recall, we pick SVM as the best alternative for this particular option. Figure 8 lists the features of Table 1 in descending order</p>
<p>Table 5: ML Algorithm Selection (RQ3). All algorithms have tuned hyperparameters.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Neural Network</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Decision Tree</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Logistic Regression</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Random Forest</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Support Vector Machine</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">A(\%)</td>
<td style="text-align: center;">$\mathbf{P}(\%)$</td>
<td style="text-align: center;">R(\%)</td>
<td style="text-align: center;">A(\%)</td>
<td style="text-align: center;">$\mathbf{P}(\%)$</td>
<td style="text-align: center;">R (\%)</td>
<td style="text-align: center;">A (\%)</td>
<td style="text-align: center;">$\mathbf{P}(\%)$</td>
<td style="text-align: center;">R (\%)</td>
<td style="text-align: center;">A (\%)</td>
<td style="text-align: center;">$\mathbf{P}(\%)$</td>
<td style="text-align: center;">R (\%)</td>
<td style="text-align: center;">A (\%)</td>
<td style="text-align: center;">$\mathbf{P}(\%)$</td>
<td style="text-align: center;">R (\%)</td>
</tr>
<tr>
<td style="text-align: center;">Full Training Set $\left(P_{1}\right)^{+}$</td>
<td style="text-align: center;">81.1</td>
<td style="text-align: center;">69.5</td>
<td style="text-align: center;">65.1</td>
<td style="text-align: center;">82.3</td>
<td style="text-align: center;">74.6</td>
<td style="text-align: center;">61.5</td>
<td style="text-align: center;">81.3</td>
<td style="text-align: center;">73.5</td>
<td style="text-align: center;">58.1</td>
<td style="text-align: center;">84.1</td>
<td style="text-align: center;">76.4</td>
<td style="text-align: center;">67.0</td>
<td style="text-align: center;">81.4</td>
<td style="text-align: center;">74.1</td>
<td style="text-align: center;">57.4</td>
</tr>
<tr>
<td style="text-align: center;">Under-sampled ${ }^{\dagger}$</td>
<td style="text-align: center;">78.9</td>
<td style="text-align: center;">77.6</td>
<td style="text-align: center;">81.2</td>
<td style="text-align: center;">78.9</td>
<td style="text-align: center;">78.3</td>
<td style="text-align: center;">79.9</td>
<td style="text-align: center;">79.0</td>
<td style="text-align: center;">79.1</td>
<td style="text-align: center;">78.7</td>
<td style="text-align: center;">80.3</td>
<td style="text-align: center;">78.8</td>
<td style="text-align: center;">83.0</td>
<td style="text-align: center;">79.2</td>
<td style="text-align: center;">77.9</td>
<td style="text-align: center;">81.7</td>
</tr>
<tr>
<td style="text-align: center;">Under-sampled + CSL ${ }^{\ddagger}$</td>
<td style="text-align: center;">77.6</td>
<td style="text-align: center;">72</td>
<td style="text-align: center;">90.4</td>
<td style="text-align: center;">77.3</td>
<td style="text-align: center;">71.7</td>
<td style="text-align: center;">90.3</td>
<td style="text-align: center;">77.1</td>
<td style="text-align: center;">71.7</td>
<td style="text-align: center;">91.7</td>
<td style="text-align: center;">79.1</td>
<td style="text-align: center;">74.5</td>
<td style="text-align: center;">88.6</td>
<td style="text-align: center;">76.8</td>
<td style="text-align: center;">70</td>
<td style="text-align: center;">92.4</td>
</tr>
</tbody>
</table>
<p>$A(\%)=$ Classification Accuracy, $P(\%)=$ Precision of the "relevant" class, $R(\%)=$ Recall of the "relevant" class; all values are percentages.
${ }^{+}$Strict filtering (option 1), ${ }^{\dagger}$ Moderate filtering (option 2), ${ }^{\ddagger}$ Lenient filtering (option 3)
of information gain (IG), averaged across options 1, 2 and 3 . We observe that our corpus-based features (F11F13) are among the most influential features, thus justifying the use of a domain-specific corpus extractor in our approach.
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Fig. 8: Feature Importance (Avg).</p>
<p>Compared to option 1, options 2 and 3 get progressively more lax by filtering less. We answer RQ4 using RF for options 1 and 2 and SVM for option 3. For better intuition, we refer to option 1 as strict, option 2 as moderate and option 3 as lenient.</p>
<p>The answer to RQ3 is: $R F$ and $S V M$ yield the most accurate filter for unuseful predictions. $R F$ is a better alternative for more aggressive filtering, whereas SVM is a better alternative for more lax filtering (thus better preserving Recall).
$\boldsymbol{R} \mathbf{Q 4}$. In RQ1-RQ3, we applied a 50-50 split strategy to tune our approach over the training portion of our data. In RQ4, we examine how effective our approach is over our test set, i.e., $P_{2}$ in Table 2. For RQ4, as noted in EXPIV (Section 5.4), we consider both a 5050 split strategy as well as a $90-10$ strategy, respectively capturing major and minor incompleteness.</p>
<p>Table 6 shows boxplot results for different levels of filtering (unfiltered, lenient, moderate and strict) organized by $50-50$ and $90-10$ split strategies. We recall from EXPIV that five different random shuffles are performed for each $p \in P_{2}$. Each plot in Table 6 is therefore based on $5 * 20=100$ datapoints.</p>
<p>Without filters and over our test set, we observe an average Coverage of $40.04 \%$ for the $50-50$ split strategy and average Coverage of $39.25 \%$ for the $90-10$ strategy. The different split strategies represent remarkably different situations. Comparatively, the $90-10$ strategy has $90 / 50=1.8$ times more textual data to generate predictions on but the incompleteness is merely $10 / 50$, i.e., one fifth, of the situation in the $50-50$ split. That is, our approach has been capable of predicting approximately $40 \%$ of the missing terminology, irrespectively of the level of incompleteness. There is nonetheless a notable difference in Accuracy between the 50-50 and $90-10$ splits, with Accuracy for the former split strategy standing at $12.11 \%$ and for the latter - at $2.25 \%$. The lower Accuracy with a $90-10$ split is explained by a combination of a higher number of predictions and a limited number of targets, stemming from the limited extent of incompleteness.</p>
<p>Ultimately, which filtering option the user selects depends on how the user wishes to balance the overhead of reviewing non-relevant recommendations against potentially finding a larger number of relevant terms missing from requirements.</p>
<p>For the $50-50$ split strategy, the lenient filter increases Accuracy by an average of $\approx 13 \%$ while decreasing Coverage by $\approx 5 \%$. The moderate filter increases Accuracy by an average of $\approx 21 \%$ while decreasing Coverage by $\approx 12 \%$. And, the strict filter increases Accuracy by an average of $\approx 36 \%$ while decreasing Coverage by $\approx 20 \%$. As shown in Table 7, in the case of the $50-50$ split, the strict and moderate filters increase Accuracy and decrease Coverage in a statistically significantly way and with large effect sizes. The lenient filter, on the other hand, increases Accuracy significantly and</p>
<p>Table 6: Accuracy and Coverage over Test Set (RQ4).
<img alt="img-9.jpeg" src="img-9.jpeg" />
with a large effect size, while not negatively impacting Coverage in a statistically significant manner.</p>
<p>As for the 90-10 split, the lenient, moderate, and strict filters increase Accuracy by an average of $\approx 1.76 \%$, $\approx 3.34 \%$, and $\approx 6.07 \%$, respectively. While increments in accuracy may appear small, it is important to note that false positives are significantly more prevalent than true
positives when the anticipated amount of incompleteness is small. As such, the accuracy ratio for different filtering levels is a better indication of how useful filtering is. For example, consider the Accuracy ratio between the lenient filter and the unfiltered data, which is calculated as $4.41 / 2.25=1.96$. This ratio signifies that, despite a relatively modest improvement in Accuracy</p>
<p>Table 7: Statistical Tests for the Results of Table 6.
(a) 50-50 Split.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Accuracy</th>
<th style="text-align: center;">Coverage</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Unfiltered vs.</td>
<td style="text-align: center;">p-value</td>
<td style="text-align: center;">$&lt;&lt;0.01$</td>
<td style="text-align: center;">0.17</td>
</tr>
<tr>
<td style="text-align: center;">Lenient Filter</td>
<td style="text-align: center;">Â 12</td>
<td style="text-align: center;">$0.03(\mathrm{~L})$</td>
<td style="text-align: center;">$0.63(\mathrm{~S})$</td>
</tr>
<tr>
<td style="text-align: center;">Unfiltered vs.</td>
<td style="text-align: center;">p-value</td>
<td style="text-align: center;">$&lt;&lt;0.01$</td>
<td style="text-align: center;">$&lt;&lt;0.01$</td>
</tr>
<tr>
<td style="text-align: center;">Moderate Strict</td>
<td style="text-align: center;">Â 12</td>
<td style="text-align: center;">$0.00(\mathrm{~L})$</td>
<td style="text-align: center;">$0.80(\mathrm{~L})$</td>
</tr>
<tr>
<td style="text-align: center;">Unfiltered vs. <br> Strict Filter</td>
<td style="text-align: center;">p-value</td>
<td style="text-align: center;">$&lt;&lt;0.01$</td>
<td style="text-align: center;">$&lt;&lt;0.01$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Â 12</td>
<td style="text-align: center;">$0.00(\mathrm{~L})$</td>
<td style="text-align: center;">$0.98(\mathrm{~L})$</td>
</tr>
</tbody>
</table>
<p>Effect size: Large (L), Medium (M), Small (S), Negligible (N)
(i.e., $4.41-2.25=1.76 \%$ ), the lenient filter effectively reduces the volume of terms requiring manual inspection by about half, thus making filtering worthwhile.</p>
<p>In the 90-10 split strategy, the increase in Accuracy and the decrease in Coverage are statistically significant when filters are applied. As shown in Table 7, all effect sizes are large with one exception: the decline in Coverage brought about by the lenient filter vs. the unfiltered has a medium effect size. This suggests that the lenient filter has a less severe impact on Accuracy compared to both moderate and strict filtering.</p>
<p>If one takes the preservation of Coverage as the main deciding factor, the lenient filter would be the best trade-off. When the anticipated amount of incompleteness is large (represented by the 50-50 split strategy), the lenient filter considerably improves Accuracy without a major impact on Coverage. And, when the anticipated amount of incompleteness is small (represented by the 90-10 split strategy), the lenient filter removes a substantial number of false negatives (around half) while decreasing Coverage with a moderate effect size.</p>
<p>We answer $\boldsymbol{R} \boldsymbol{Q} \boldsymbol{4}$ in two distinct scenarios: (S1) when a significant degree of incompleteness is expected, represented by withholding $50 \%$ of the content in a requirements document, and (S2) when a minor level of incompleteness is anticipated, represented by withholding $90 \%$ of the content.
(b) 90-10 Split.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Accuracy</th>
<th style="text-align: center;">Coverage</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Unfiltered vs.</td>
<td style="text-align: center;">p-value</td>
<td style="text-align: center;">$&lt;&lt;0.01$</td>
<td style="text-align: center;">$&lt;&lt;0.01$</td>
</tr>
<tr>
<td style="text-align: center;">Lenient Filter</td>
<td style="text-align: center;">Â 12</td>
<td style="text-align: center;">$0.21(\mathrm{~L})$</td>
<td style="text-align: center;">$0.67(\mathrm{M})$</td>
</tr>
<tr>
<td style="text-align: center;">Unfiltered vs.</td>
<td style="text-align: center;">p-value</td>
<td style="text-align: center;">$&lt;&lt;0.01$</td>
<td style="text-align: center;">$&lt;&lt;0.01$</td>
</tr>
<tr>
<td style="text-align: center;">Moderate Strict</td>
<td style="text-align: center;">Â 12</td>
<td style="text-align: center;">$0.12(\mathrm{~L})$</td>
<td style="text-align: center;">$0.81(\mathrm{~L})$</td>
</tr>
<tr>
<td style="text-align: center;">Unfiltered vs. <br> Strict Filter</td>
<td style="text-align: center;">p-value <br> Â 12</td>
<td style="text-align: center;">$&lt;&lt;0.01$</td>
<td style="text-align: center;">$&lt;&lt;0.01$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Â 12</td>
<td style="text-align: center;">$0.08(\mathrm{~L})$</td>
<td style="text-align: center;">$0.93(\mathrm{~L})$</td>
</tr>
</tbody>
</table>
<p>Effect size: Large (L), Medium (M), Small (S), Negligible (N)</p>
<p>For S1, when applying a strict filter, $\approx 48 \%$ of recommendations from our approach are relevant, compared to $\approx 25 \%$ with a lenient filter. Under lenient filtering, our recommendations provide cues for $\approx 35 \%$ of the (simulated) missing terminology. This number decreases to $20 \%$ with a strict filter. As for S2 - a more challenging scenario than S1 due to a larger number of recommendations and substantially fewer target terms for incompleteness - unfiltered recommendations still offer clues for $\approx 39 \%$ of the missing terminology. Filtering remains crucial for S2 as it eliminates a significant number of non-relevant terms; however, the prevalence of non-relevant terms remains high despite filtering. Further, due to the much smaller number of target terms compared to S1, the negative impact of filtering on relevant recommendations is more pronounced. This results in uncovering $\approx 33 \%$, $\approx 27 \%$, and $\approx 18 \%$ of missing terminology for lenient, moderate, and strict filtering, respectively.</p>
<h2>6 Discussion</h2>
<p>As explained in Section 5.4 (EXPIII) and our metric discussion in Section 5.5, we employ an $85 \%$ cosine similarity threshold on word embeddings to assess whether predictions by BERT are good matches for the novel terms in the withheld half of a requirements document. This method presents an advantage over exact lexical matches by considering the semantic similarity between predictions and novel terms, thereby resulting in more thorough and nuanced matches. At the same time, it is important to exercise caution when determining the threshold, as setting it too low can result in erroneously considering dissimilar terms as valid matches, thus potentially negatively impacting the reliability of our evaluation. We selected the $85 \%$ threshold based on an exploratory analysis in our earlier conference paper [37], where we surmised that the chosen threshold would be conservative enough to rule out the majority of dissimilar matches. In this article, we seek to offer empirical</p>
<p><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Fig. 9: Proportions of Exact vs. Non-exact Matches over Test Set.
evidence for our chosen threshold and quantify its influence on the interpretation of our findings.</p>
<p>We structure our discussion in two stages. First, we determine what proportion of the matches admitted by this threshold are exact lexical matches. Exact matches do not require human validation to assess their quality. In the second stage, we collect human feedback on the quality of a selected subset of non-exact matches. The goal here is to systematically estimate the percentage of these matches that provide useful insights to humans.</p>
<p>Figure 9 shows the proportions of exact versus nonexact matches. Each data point represents a single run of our approach over each document in the test set $\left(P_{2}\right)$, without any filters applied. The results indicate that the majority of matches are exact, with less than a quarter of them being non-exact. To assess the quality of nonexact matches, we engaged two non-authors to obtain unbiased human feedback. Both individuals are PhD students in Computer Science with excellent proficiency in English, over four years of industry experience, and prior exposure to requirements and requirements engineering. We selected 40 non-exact-match examples at random from our experimentation with the test set. These examples can be found in our supplementary material [36]. The two individuals were interviewed separately, allowing us to assess interrater agreement. The question that an individual had to answer for each example was as follows: For this given sentence, is term $X$ (predicted by BERT) a useful match for term $Y$ (which appears in the sentence)?</p>
<p>The notion of "useful" is subjective. To determine which predictions were considered useful, we relied on individual discretion while encouraging discussion to support the decisions made. Before each interview, we reviewed the criteria for usefulness. In particular, we clarified that a useful match could come in two ways: (1) a synonym that conveyed the same meaning as the original word, or (2) a term that would prompt reflec-
tion and deeper thought, such as a more specific term like "car" being provided for a more general original term like "vehicle" or vice versa.</p>
<p>The first individual found $75 \%(30 / 40)$ of the nonexact predictions to be useful, while the second individual found $87.5 \%(35 / 40)$ of the predictions to be useful. Out of the 40 examples shown to the individuals, there were only four cases (i.e., $10 \%$ ) where both individuals considered the non-exact prediction to be unuseful. Cohen's Kappa ( $\kappa$ ) was used to measure the agreement level among the individuals. The result was $\kappa=0.44$, indicating moderate agreement. This implies that determining usefulness involves a non-negligible degree of subjectivity, which aligns with previous research in RE highlighting the subjective nature of concepts like relevance and usefulness [7]. At the same time, we observe that depending on how one aggregates the human feedback, between $75 \%$ and $90 \%$ of the non-exact matches are useful. Noting that, on average, approximately $76 \%$ of the matches are exact (Fig. 9), the unuseful cases are a proportion of the remaining average $24 \%$. We thus estimate the error arising from non-exact matching to range from $2.4 \%(24 \% \times 10 \%)$ to $6 \%(24 \% \times 25 \%)$. Despite this margin of error, we consider the trade-off of employing non-exact matching worthwhile, given that, based on our results, useful matches outnumber unuseful ones by a ratio of at least three to one.</p>
<h2>7 Limitations and Validity Considerations</h2>
<p>In this section, we discuss limitations and threats to validity.</p>
<h3>7.1 Limitations</h3>
<p>The experimentation we conducted in this article has a number of limitations that warrant further investigation. The first limitation results from a lack of access to domain experts who could effectively identify genuine cases of incompleteness in the requirements. As a result, we had to resort to simulating incompleteness by withholding content from existing requirements. While this allows for some measure of evaluation for our proposed approach's effectiveness, it may not accurately represent the actual incomplete requirements encountered in real-world scenarios. To develop a more precise understanding of our approach's utility, it is essential to conduct future user studies involving domain experts who can provide insights into the nature and extent of incompleteness in requirements.</p>
<p>Another potential limitation is the size and diversity of our dataset. Although our dataset covers 15 distinct</p>
<p>domains, it may not be representative of all possible scenarios for different industries and contexts. To provide a more comprehensive evaluation of the approach, future experimentation should consider utilizing the entirety of PURE [26] and potentially other datasets.</p>
<p>We used the original BERT model in our experiments due to limited computational resources and were unable to systematically explore the impact of using BERT variants. Although our preliminary experiments did not suggest significant improvements from using BERT variants, further rigorous empirical investigation is needed to confirm this.</p>
<p>Lastly, we note that our research coincided with the release of ChatGPT [40], which has had a remarkable impact on AI-enabled software engineering research in the past months. While our main proposition, namely using large language models as an external source of knowledge for enhancing requirements completeness, can also be instantiated with ChatGPT and the GPT family of language models [41], we have not yet explored this avenue of research. It is plausible, and in fact likely, that ChatGPT would be a superior alternative to BERT as an external knowledge source for requirements completion and incompleteness mitigation.</p>
<h3>7.2 Internal Validity</h3>
<p>We intentionally seed incompleteness into requirements to evaluate the effectiveness of our approach in detecting the omissions. We nevertheless recognize that random variation could potentially influence our findings. We employed several strategies to mitigate the impact of random variation. First, we utilize a substantial dataset consisting of 40 requirements documents, as depicted in Table 2. This large dataset helps minimize the effects of random variation, thereby increasing the robustness of our findings. Furthermore, we enhance the reliability of our experiment by repeating each test for a given document five times. This is achieved by shuffling the withheld and disclosed portions of the document, as outlined in Section 5.4.</p>
<p>Our cleaning of the PURE dataset is limited to the simple changes outlined in Section 5.3. We have chosen not to remove non-requirements from these documents. This decision was driven by the lack of clear differentiation between requirements and non-requirements in many PURE documents. Given that the demarcation of requirements can be subjective without domain expertise, we exercised caution and refrained from attempting it. Nevertheless, we examined the documents in PURE where an explicit distinction between requirements and non-requirements existed and where the requirements content dominated non-requirements con-
tent in terms of token count. We did not observe subpar results for documents with higher proportions of requirements. In fact, the results for these documents are slightly better than the averages reported. Consequently, we do not believe that the absence of a clear separation between requirements and non-requirements significantly impacted our results. At the same time, when there is an opportunity for an objective separation between requirements and non-requirements, it may be more sensible to scope completeness checking to the requirements statements only.</p>
<h3>7.3 Construct Validity</h3>
<p>We took the following steps to ensure that our metrics are an accurate reflection of the phenomena under investigation. First, we excluded any terminology already present in the disclosed portion of the requirements documents. This process eliminates the possibility of including non-novel terms. Second, we removed duplicates, common words, and stopwords from our analysis. These measures help ensure that we provide an objective assessment of novel terms predicted by BERT.</p>
<p>We further note that we use non-exact matching for assessing the quality of predictions. To address potential construct-validity risks associated with non-exact matching, we analyzed the prevalence of non-exact matches and also gathered third-party human feedback to obtain unbiased opinions about such matches. As we argued in Section 6, our estimated error margin for non-exact matching is small and therefore unlikely to significantly impact construct validity.</p>
<h3>7.4 External Validity</h3>
<p>Our evaluation is based on 40 requirements documents from PURE [26]. These documents span 15 different domains and originate from a variety of sources. The size and diversity of our dataset help provide confidence in the generalizability of our findings. Nevertheless, to further enhance external validity, it would be beneficial to conduct experiments with a broader range of requirements documents.</p>
<h2>8 Conclusion</h2>
<p>In this article, we explored the usefulness of large language models (LLMs) for detecting incompleteness in natural-language requirements. Specifically, our focus was on evaluating the effectiveness of BERT's maskedword predictions in identifying relevant but absent terminology within requirements.</p>
<p>In lieu of access to human experts and documented instances of incompleteness, we simulated incompleteness by withholding content from existing documents. Our investigation consisted of three steps. In the first step, we found the optimal number of predictions per masked word, achieving a balance between noise and relevant predictions generated by BERT. In the second step, we evaluated different machine learning classifiers to remove noise from the predictions. Finally, we compared the filtered predictions with terms in withheld section of requirements documents using new, unseen data. Our findings suggest that BERT's masked-word predictions can hint at a sizeable number of instances of incompleteness. Furthermore, the filtering process enhances the accuracy of the predictions by reducing the prevalence of non-relevant terms.</p>
<p>Throughout the research, several areas for improvement came to light. One such area is expanding the dataset used. We limited our analysis to half of the PURE dataset, considering the trade-off between data cleaning effort and computation costs while maintaining a reasonable dataset size. Using the entire PURE dataset, instead of just half, would yield a larger and more diverse sample, and consequently more robust and reliable findings. Another important area for improvement is our evaluation. Currently, our evaluation involves simulating omissions by withholding data, which provides useful insights but falls short of capturing the practical implications of our approach. An actual evaluation of BERT, used for detecting incompleteness in requirements, will need to develop a practical use case for presenting BERT's predictions to users within the context in which the predictions were derived. Such an evaluation necessarily has to involve domain experts, requirements engineers, and other stakeholders who can vet BERT's predictions, determining whether they represent genuine instances of incompleteness or are superfluous. Our current results merely gauge the potential of BERT for assisting with requirements completeness checking. To determine whether this potential can be transformed into practical benefits, user studies are a must - something that our current work does not perform and is left for future research.</p>
<p>Finally, noting the fast-evolving landscape of LLMs towards generative models like GPT, it is important to re-evaluate our approach using such models. We anticipate major improvements, particularly with the potential for interactive and conversational exchanges that can incrementally improve requirements completeness.
Acknowledgements. This work was funded by the Natural Sciences and Engineering Research Council of Canada (NSERC) under the Discovery and Discovery Accelerator programs.</p>
<h2>References</h2>
<ol>
<li>Muhammad Abbas, Alessio Ferrari, Anas Shatnawi, Eduard Paul Enoiu, and Mehrdad Saadatmand. Is requirements similarity a good proxy for software similarity? an empirical investigation in industry. In 27th International Working Conference on Requirements Engineering: Foundation for Software Quality (REFSQ'21), 2021.</li>
<li>Dalal Alrajeh, Jeff Kramer, Axel van Lamsweerde, Alessandra Russo, and Sebastian Uchitel. Generating obstacle conditions for requirements completeness. In 34th International Conference on Software Engineering (ICSE'12), 2012.</li>
<li>Orlando Amaral Cejas, Sallam Abualhaija, Damiano Torre, Mehrdad Sabetzadeh, and Lionel Briand. AIenabled automation for completeness checking of privacy policies. IEEE Transactions on Software Engineering, 48(11), 2022.</li>
<li>Chetan Arora, Mehrdad Sabetzadeh, and Lionel Briand. An empirical study on the potential usefulness of domain models for completeness checking of requirements. Empirical Software Engineering, 24(4), 2019.</li>
<li>Chetan Arora, Mehrdad Sabetzadeh, Lionel Briand, and Frank Zimmer. Automated checking of conformance to requirements templates using natural language processing. IEEE Transactions on Software Engineering, 41(10), 2015.</li>
<li>Chetan Arora, Mehrdad Sabetzadeh, Lionel Briand, and Frank Zimmer. Automated extraction and clustering of requirements glossary terms. IEEE Transactions on Software Engineering, 43(10), 2017.</li>
<li>Chetan Arora, Mehrdad Sabetzadeh, Shiva Nejati, and Lionel Briand. An active learning approach for improving the accuracy of automated domain model extraction. ACM Transactions on Software Engineering and Methodology, 28, 2019.</li>
<li>James Bergstra and Yoshua Bengio. Random search for hyper-parameter optimization. Journal of Machine Learning Research, 13(2), 2012.</li>
<li>Daniel Berry. Empirical evaluation of tools for hairy requirements engineering tasks. Empirical Software Engineering, 26, 112021.</li>
<li>Daniel M. Berry, Erik Kamsties, and Michael Krieger. From contract drafting to software specification: Linguistic sources of ambiguity, a handbook, 2003. https://cs. useter1co.ca/ obserry/handbook/ambiguityHandbook.pdf.</li>
<li>Jaspreet Bhatia and Travis Breaux. Semantic incompleteness in privacy policy goals. In 26th IEEE International Requirements Engineering Conference (RE'18), 2018.</li>
<li>Jie Cai, Jiawei Luo, Shulin Wang, and Sheng Yang. Feature selection in machine learning: A new perspective. Neurocomputing, 300, 2018.</li>
<li>J. Anthony Capon. Elementary Statistics for the Social Sciences: Study Guide. Wadsworth, 1991.</li>
<li>Gaoying Cui, Qin Lu, Wenjie Li, and Yi-Rong Chen. Corpus exploitation from Wikipedia for ontology construction. In 6th International Conference on Language Resources and Evaluation (LREC'08), 2008.</li>
<li>Fabiano Dalpiaz, Ivor van der Schalk, and Garm Lucassen. Pinpointing ambiguity and incompleteness in requirements engineering via information visualization and NLP. In 24th International Working Conference on Requirements Engineering: Foundation for Software Quality (REFSQ'18), 2018.</li>
<li>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirec-</li>
</ol>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>Dipeeka Luitel, Shabnam Hassani, Mehrdad Sabetzadeh
E-mail: Dipeeka.Luitel, S.Hassani, M.Sabetzadeh @uottawa.ca
${ }^{1}$ School of Electrical Engineering and Computer Science, University of Ottawa, Canada&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
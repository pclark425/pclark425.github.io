<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1034 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1034</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1034</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-24.html">extraction-schema-24</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <p><strong>Paper ID:</strong> paper-49569263</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/1807.01521v3.pdf" target="_blank">Curiosity Driven Exploration of Learned Disentangled Goal Spaces</a></p>
                <p><strong>Paper Abstract:</strong> : Intrinsically motivated goal exploration processes enable agents to explore efﬁciently complex environments with high-dimensional continuous actions. They have been applied successfully to real world robots to discover repertoires of policies producing a wide diversity of effects. Often these algorithms relied on engineered goal spaces but it was recently shown that one can use deep representation learning algorithms to learn an adequate goal space in simple environments. In this paper we show that using a disentangled goal space (i.e. a representation where each latent variable is sensitive to a single degree of freedom) leads to better exploration performances than an entangled one. We further show that when the representation is disentangled, one can leverage it by sampling goals that maximize learning progress in a modular manner. Finally, we show that the measure of learning progress, used to drive curiosity-driven exploration, can be used simultaneously to discover abstract independently controllable features of the environment.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1034.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1034.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MGE-βVAE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Modular Goal Exploration with learned disentangled goal space (β-VAE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A curiosity-driven modular Intrinsically Motivated Goal Exploration Process (IMGEP) that uses a β-VAE learned disentangled latent representation as a modular goal space and samples modules by estimated learning/competence progress.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>7-joint simulated robotic arm (IMGEP agent)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Simulated 7-joint robotic arm that learns via Intrinsically Motivated Goal Exploration Processes (IMGEP) using a modular goal sampling policy driven by competence/learning progress; representation learning stage uses β-VAE (disentangled) learned from passive observations, followed by goal-directed active exploration using nearest-neighbor inverse-model meta-policy.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Arm-2-Balls</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>A 2D/visual simulated scene rendered as 64×64 images containing two balls of different sizes: one graspable and controllable by the arm; one uncontrollable distractor that follows a random walk. The arm is rotating with unit length; reachable object positions are bounded in [-1,1]^4 for the two objects' x/y positions.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Number of objects = 2 (one controllable, one distractor); observation dimensionality = 64×64 pixels; outcome discretization = 900-grid cells for evaluation (30×30); environment dynamics unknown and stochastic for distractor (random walk).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>medium (multiple objects including uncontrollable distractor; high-dimensional pixel observations)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Distractor motion stochasticity (random walk with tunable noise); training observation distribution sampled uniformly over object positions in [-1,1]^4; experiments varied distractor noise level.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium-high (the distractor executes a random walk producing temporal variation; experiment also studied low-noise distractor case)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exploration coverage: ratio of discretized reachable cells visited by the controllable ball in a 900-cell grid (30×30), and number of unique outcomes explored; also speed of coverage over episodes.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Achieved exploration coverage comparable to the modular algorithm with engineered features (MGE-EFR); evaluated over 20 trials × 10,000 episodes each. Theoretical maximum reachable coverage ratio ≈ π/4 ≈ 0.785. Exact numeric curves shown in paper but per-condition final percentages not tabulated numerically in text.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Explicit: disentangled modular representation allows ignoring distractor-induced variation via projection operators, so in environments with extra (high-variation) distractors modular IMGEP with disentangled goals yields much better exploration; conversely when distractor variation is low, modularization confers no advantage. Trade-off: representation disentanglement × modular sampling interacts with environment variation to determine exploration efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td>Qualitative: when distractor noise is low (low variation) the distractor no longer acts as distractor and modular algorithm shows no advantage over flat algorithms; MGE-βVAE still learns good inverse models. (No precise numeric value reported.)</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td>Qualitative: strong performance — MGE-βVAE substantially outperforms flat/entangled methods in presence of an actively varying distractor, matching engineered-feature modular performance. (No single numeric final coverage value reported in text.)</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Two-stage: (1) passive representation learning (β-VAE) from uniformly sampled images of scene positions; (2) active modular curiosity-driven exploration (IMGEP) with competence-progress-driven module sampling and nearest-neighbor inverse-model meta-policy. Bootstrap with initial random parameterization experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Reported experiments ran 20 trials of 10,000 episodes each; MGE-βVAE explores more unique states in fewer experiments than entangled counterparts and RPE (exact episode counts to reach thresholds are shown in curves but not tabulated).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using a disentangled (β-VAE) representation as a modular goal space and sampling modules by learning progress yields exploration efficiency comparable to using engineered semantic features, especially in environments with uncontrollable, high-variation distractors; the modular architecture can ignore distractor noise via projection and focuses on controllable latent factors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Curiosity Driven Exploration of Learned Disentangled Goal Spaces', 'publication_date_yy_mm': '2018-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1034.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1034.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RGE-βVAE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Random Goal Exploration with learned disentangled goal space (β-VAE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A flat (non-modular) IMGEP that uses a β-VAE learned disentangled latent space as a single goal space and samples goals uniformly within it.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>7-joint simulated robotic arm (IMGEP agent)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Same simulated arm using IMGEP but with a flat goal space sampled uniformly (RGE) over the entire β-VAE latent embedding rather than modular module selection by competence progress.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Arm-2-Balls</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same two-ball pixel environment; one controllable ball and one distractor with random walk; high-dimensional pixel inputs (64×64).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Two objects (controllable + distractor); observation dimensionality 64×64; goal-space dimensionality = learned latent dimensionality (10 latent variables used in models).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>medium</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Distractor random walk noise; outcome variation as in MGE-βVAE experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium-high</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exploration coverage ratio over 900 discretized cells; speed of coverage over episodes.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Better than Random Parameter Exploration but worse than modular MGE-βVAE; requires larger exploration noise to produce wide variety of observations (no single numeric final coverage provided in text).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>With a disentangled latent space, flat goal sampling still benefits but does not exploit modular learning progress to focus on controllable factors; thus in the presence of distractor variation flat-sampling is less efficient than modular-sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td>Qualitative: inferior to MGE-βVAE when distractor variation is significant; still performs better than random parameter sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Two-stage: passive β-VAE representation learning then flat IMGEP goal sampling over learned latent space; bootstrap with random parameterization.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>20 trials × 10,000 episodes; requires higher exploration noise for broad coverage compared to modular variant.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A disentangled representation helps exploration even when used in a flat goal-sampling scheme, but modular sampling by learning progress yields superior performance by focusing on controllable latent factors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Curiosity Driven Exploration of Learned Disentangled Goal Spaces', 'publication_date_yy_mm': '2018-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1034.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1034.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MGE-VAE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Modular Goal Exploration with learned entangled goal space (VAE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A modular IMGEP that uses a VAE-learned entangled latent representation partitioned into modules; module sampling by competence progress is applied but modules do not correspond to single controllable factors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>7-joint simulated robotic arm (IMGEP agent)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Same simulated arm using modular IMGEP where the goal space is a VAE-learned entangled latent space; projection/grouping into modules done but latents are entangled, so modules mix controllable and uncontrollable factors.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Arm-2-Balls</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Two-ball pixel environment with one controllable ball and one distractor random-walking ball; pixel inputs 64×64.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>2 objects; latent dimensionality 10; modules created by grouping latent variables (e.g., 2D planes sorted by KL), but entanglement mixes object factors across modules.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>medium</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Distractor random walk noise; latent entanglement yields spurious correlations across modules creating apparent variation in each module.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium-high</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exploration coverage ratio over 900 discretized cells; module interest (learning progress) trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Worse than MGE-βVAE and often worse than flat RGE when representation is entangled; modular architecture can be detrimental when modules do not align with controllable factors (no exact numeric coverage provided).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>When representation is entangled, modularization harms performance because entanglement introduces spurious correlations and noisy interest estimates across modules; high environment variation (distractor) contaminates modules and misleads module sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td>Poorer exploration than modular/disentangled counterpart; modules show random-walk interest curves and do not focus on controllable features.</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Two-stage: passive VAE representation learning (β=1, capacity=0) then modular IMGEP with competence-progress module sampling; bootstrap with random parameterization.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>20 trials × 10,000 episodes; observed to need larger exploration noise to achieve diversity; overall less sample-efficient than disentangled modular approach.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using a modular IMGEP with an entangled learned representation degrades performance: entanglement mixes controllable and uncontrollable factors across modules, leading to noisy interest signals and suboptimal module selection, especially in presence of distractor variation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Curiosity Driven Exploration of Learned Disentangled Goal Spaces', 'publication_date_yy_mm': '2018-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1034.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1034.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RGE-VAE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Random Goal Exploration with learned entangled goal space (VAE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A flat IMGEP that uses a VAE-learned entangled latent space as a single goal space and samples goals uniformly.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>7-joint simulated robotic arm (IMGEP agent)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Same simulated arm using IMGEP with a single entangled latent goal space; goals sampled uniformly over the learned latent variables.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Arm-2-Balls</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Two-ball pixel scene with one controllable ball and one random-walking distractor; observations are 64×64 images.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Two objects; latent dimensionality 10; entangled latent factors meaning single latents encode mixed generative factors.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>medium</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Distractor random walk; sampling of training images uniformly over object positions for representation learning.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium-high</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exploration coverage ratio (900-cell grid), speed of coverage vs episodes.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Performs better than Random Parameter Exploration but worse than methods using disentangled representations; tends to produce observations clustered in smaller region unless exploration noise is large.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Entangled representation fails to disentangle controllable factors from distractor variation, so flat IMGEP with entangled latent space is vulnerable to variation and less efficient.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td>Inferior exploration coverage compared to disentangled counterparts; requires larger exploration noise to compensate.</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Two-stage: passive VAE representation learning then flat IMGEP goal sampling; bootstrap with random parameterization.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>20 trials × 10,000 episodes; less sample-efficient than disentangled approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Flat IMGEP with an entangled learned goal space provides limited exploration efficiency; entanglement blurs controllability signals and makes exploration rely on larger random motor noise.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Curiosity Driven Exploration of Learned Disentangled Goal Spaces', 'publication_date_yy_mm': '2018-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1034.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1034.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RPE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Random Parameter Exploration</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline exploration strategy that uniformly samples policy parameters (θ) with no goal-directed mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>7-joint simulated robotic arm (baseline agent)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Simulated arm whose exploration is pure random sampling of motor parameterizations (θ) providing a lower-bound baseline for exploration efficiency comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Arm-2-Balls</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same two-ball pixel environment with a controllable ball and an uncontrollable distractor performing a random walk; observations are 64×64 pixels.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Two objects; high-dimensional action parameter space (controller parameterizations), stochastic distractor.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>medium</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Distractor random walk; exploration policy does not adapt to variation.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium-high</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exploration coverage ratio over 900 discretized cells and number of times the ball was effectively handled.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Much poorer performance than all IMGEP variants; fails to produce wide variety of observations (curves in paper show low coverage), exact numeric coverage not tabulated in text.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Random parameter search is strongly inefficient in environments with high-dimensional continuous actions and distractors; does not exploit structure or reduce effect of variation.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td>Poor exploration coverage and lower number of successful interactions (e.g., ball handling) relative to IMGEP methods.</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Single-phase random sampling across parameterizations; used as baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Very low; across 10,000 episodes produces far fewer unique reached outcomes than IMGEP variants.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Random parameter exploration fails in high-dimensional continuous action settings and in presence of distractors; IMGEPs (especially modular + disentangled) strongly outperform RPE.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Curiosity Driven Exploration of Learned Disentangled Goal Spaces', 'publication_date_yy_mm': '2018-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1034.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1034.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MGE-EFR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Modular Goal Exploration with Engineered Features Representation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Modular IMGEP that uses handcrafted semantic goal spaces (engineered features) corresponding to true object positions as an upper-bound comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>7-joint simulated robotic arm (IMGEP agent with engineered goals)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Simulated arm using IMGEP where the goal modules are handcrafted engineered features containing true object degrees of freedom (positions of the two balls); module sampling by competence progress and inverse-model meta-policy used.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Arm-2-Balls</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same two-ball pixel environment, but goal space provided directly as semantic positions of the two balls in [-1,1]^4 (two modules: controllable ball and distractor).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Number of objects = 2; goal-space dimensionality = 4 (x/y for each ball); observation complexity (pixels) is abstracted away by engineered features.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>low-to-medium from the goal-space perspective (engineered semantic features simplify structure)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Distractor random walk still present but represented semantically in the distractor module; module sampling can avoid distractor if low learning progress.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium-high (in environment), but modular engineerd goal space allows ignoring distractor variation via module selection.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exploration coverage ratio over 900 discretized cells for controllable ball, number of handled episodes; speed of coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Serves as an upper bound: modular IMGEP with engineered features achieves high coverage and serves as performance target that MGE-βVAE matches qualitatively; theoretical max coverage ratio ≈ π/4 ≈ 0.785.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>With engineered modular goals aligned to true degrees of freedom, modular IMGEP easily focuses on controllable modules and ignores distractor variation, yielding best exploration performance; demonstrates the value of aligning goal-space structure to environment structure.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td>High performance; modular engineered feature approach efficiently handles distractor variation by focusing on controllable module.</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>IMGEP with modular engineered goal spaces; module sampling driven by competence progress; inverse-model meta-policy; bootstrap with random parameterization.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>High sample-efficiency relative to other baselines; achieves wide coverage with fewer experiments compared to RPE and entangled learned-space methods.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Engineered modular goal spaces aligned with environment structure provide an upper bound on exploration performance; learned disentangled (β-VAE) modular spaces can match this performance, indicating representational quality is key to effective exploration in varied environments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Curiosity Driven Exploration of Learned Disentangled Goal Spaces', 'publication_date_yy_mm': '2018-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Active learning of inverse models with intrinsically motivated goal exploration in robots <em>(Rating: 2)</em></li>
                <li>Unsupervised Learning of Goal Spaces for Intrinsically Motivated Goal Exploration <em>(Rating: 2)</em></li>
                <li>Modular active curiosity-driven discovery of tool use <em>(Rating: 2)</em></li>
                <li>Early Visual Concept Learning with Unsupervised Deep Learning <em>(Rating: 1)</em></li>
                <li>beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework <em>(Rating: 2)</em></li>
                <li>Disentangling the independently controllable factors of variation by interacting with the world <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1034",
    "paper_id": "paper-49569263",
    "extraction_schema_id": "extraction-schema-24",
    "extracted_data": [
        {
            "name_short": "MGE-βVAE",
            "name_full": "Modular Goal Exploration with learned disentangled goal space (β-VAE)",
            "brief_description": "A curiosity-driven modular Intrinsically Motivated Goal Exploration Process (IMGEP) that uses a β-VAE learned disentangled latent representation as a modular goal space and samples modules by estimated learning/competence progress.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "7-joint simulated robotic arm (IMGEP agent)",
            "agent_description": "Simulated 7-joint robotic arm that learns via Intrinsically Motivated Goal Exploration Processes (IMGEP) using a modular goal sampling policy driven by competence/learning progress; representation learning stage uses β-VAE (disentangled) learned from passive observations, followed by goal-directed active exploration using nearest-neighbor inverse-model meta-policy.",
            "agent_type": "simulated agent",
            "environment_name": "Arm-2-Balls",
            "environment_description": "A 2D/visual simulated scene rendered as 64×64 images containing two balls of different sizes: one graspable and controllable by the arm; one uncontrollable distractor that follows a random walk. The arm is rotating with unit length; reachable object positions are bounded in [-1,1]^4 for the two objects' x/y positions.",
            "complexity_measure": "Number of objects = 2 (one controllable, one distractor); observation dimensionality = 64×64 pixels; outcome discretization = 900-grid cells for evaluation (30×30); environment dynamics unknown and stochastic for distractor (random walk).",
            "complexity_level": "medium (multiple objects including uncontrollable distractor; high-dimensional pixel observations)",
            "variation_measure": "Distractor motion stochasticity (random walk with tunable noise); training observation distribution sampled uniformly over object positions in [-1,1]^4; experiments varied distractor noise level.",
            "variation_level": "medium-high (the distractor executes a random walk producing temporal variation; experiment also studied low-noise distractor case)",
            "performance_metric": "Exploration coverage: ratio of discretized reachable cells visited by the controllable ball in a 900-cell grid (30×30), and number of unique outcomes explored; also speed of coverage over episodes.",
            "performance_value": "Achieved exploration coverage comparable to the modular algorithm with engineered features (MGE-EFR); evaluated over 20 trials × 10,000 episodes each. Theoretical maximum reachable coverage ratio ≈ π/4 ≈ 0.785. Exact numeric curves shown in paper but per-condition final percentages not tabulated numerically in text.",
            "complexity_variation_relationship": "Explicit: disentangled modular representation allows ignoring distractor-induced variation via projection operators, so in environments with extra (high-variation) distractors modular IMGEP with disentangled goals yields much better exploration; conversely when distractor variation is low, modularization confers no advantage. Trade-off: representation disentanglement × modular sampling interacts with environment variation to determine exploration efficiency.",
            "high_complexity_low_variation_performance": "Qualitative: when distractor noise is low (low variation) the distractor no longer acts as distractor and modular algorithm shows no advantage over flat algorithms; MGE-βVAE still learns good inverse models. (No precise numeric value reported.)",
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": "Qualitative: strong performance — MGE-βVAE substantially outperforms flat/entangled methods in presence of an actively varying distractor, matching engineered-feature modular performance. (No single numeric final coverage value reported in text.)",
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Two-stage: (1) passive representation learning (β-VAE) from uniformly sampled images of scene positions; (2) active modular curiosity-driven exploration (IMGEP) with competence-progress-driven module sampling and nearest-neighbor inverse-model meta-policy. Bootstrap with initial random parameterization experiments.",
            "generalization_tested": false,
            "generalization_results": null,
            "sample_efficiency": "Reported experiments ran 20 trials of 10,000 episodes each; MGE-βVAE explores more unique states in fewer experiments than entangled counterparts and RPE (exact episode counts to reach thresholds are shown in curves but not tabulated).",
            "key_findings": "Using a disentangled (β-VAE) representation as a modular goal space and sampling modules by learning progress yields exploration efficiency comparable to using engineered semantic features, especially in environments with uncontrollable, high-variation distractors; the modular architecture can ignore distractor noise via projection and focuses on controllable latent factors.",
            "uuid": "e1034.0",
            "source_info": {
                "paper_title": "Curiosity Driven Exploration of Learned Disentangled Goal Spaces",
                "publication_date_yy_mm": "2018-07"
            }
        },
        {
            "name_short": "RGE-βVAE",
            "name_full": "Random Goal Exploration with learned disentangled goal space (β-VAE)",
            "brief_description": "A flat (non-modular) IMGEP that uses a β-VAE learned disentangled latent space as a single goal space and samples goals uniformly within it.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "7-joint simulated robotic arm (IMGEP agent)",
            "agent_description": "Same simulated arm using IMGEP but with a flat goal space sampled uniformly (RGE) over the entire β-VAE latent embedding rather than modular module selection by competence progress.",
            "agent_type": "simulated agent",
            "environment_name": "Arm-2-Balls",
            "environment_description": "Same two-ball pixel environment; one controllable ball and one distractor with random walk; high-dimensional pixel inputs (64×64).",
            "complexity_measure": "Two objects (controllable + distractor); observation dimensionality 64×64; goal-space dimensionality = learned latent dimensionality (10 latent variables used in models).",
            "complexity_level": "medium",
            "variation_measure": "Distractor random walk noise; outcome variation as in MGE-βVAE experiments.",
            "variation_level": "medium-high",
            "performance_metric": "Exploration coverage ratio over 900 discretized cells; speed of coverage over episodes.",
            "performance_value": "Better than Random Parameter Exploration but worse than modular MGE-βVAE; requires larger exploration noise to produce wide variety of observations (no single numeric final coverage provided in text).",
            "complexity_variation_relationship": "With a disentangled latent space, flat goal sampling still benefits but does not exploit modular learning progress to focus on controllable factors; thus in the presence of distractor variation flat-sampling is less efficient than modular-sampling.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": "Qualitative: inferior to MGE-βVAE when distractor variation is significant; still performs better than random parameter sampling.",
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Two-stage: passive β-VAE representation learning then flat IMGEP goal sampling over learned latent space; bootstrap with random parameterization.",
            "generalization_tested": false,
            "generalization_results": null,
            "sample_efficiency": "20 trials × 10,000 episodes; requires higher exploration noise for broad coverage compared to modular variant.",
            "key_findings": "A disentangled representation helps exploration even when used in a flat goal-sampling scheme, but modular sampling by learning progress yields superior performance by focusing on controllable latent factors.",
            "uuid": "e1034.1",
            "source_info": {
                "paper_title": "Curiosity Driven Exploration of Learned Disentangled Goal Spaces",
                "publication_date_yy_mm": "2018-07"
            }
        },
        {
            "name_short": "MGE-VAE",
            "name_full": "Modular Goal Exploration with learned entangled goal space (VAE)",
            "brief_description": "A modular IMGEP that uses a VAE-learned entangled latent representation partitioned into modules; module sampling by competence progress is applied but modules do not correspond to single controllable factors.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "7-joint simulated robotic arm (IMGEP agent)",
            "agent_description": "Same simulated arm using modular IMGEP where the goal space is a VAE-learned entangled latent space; projection/grouping into modules done but latents are entangled, so modules mix controllable and uncontrollable factors.",
            "agent_type": "simulated agent",
            "environment_name": "Arm-2-Balls",
            "environment_description": "Two-ball pixel environment with one controllable ball and one distractor random-walking ball; pixel inputs 64×64.",
            "complexity_measure": "2 objects; latent dimensionality 10; modules created by grouping latent variables (e.g., 2D planes sorted by KL), but entanglement mixes object factors across modules.",
            "complexity_level": "medium",
            "variation_measure": "Distractor random walk noise; latent entanglement yields spurious correlations across modules creating apparent variation in each module.",
            "variation_level": "medium-high",
            "performance_metric": "Exploration coverage ratio over 900 discretized cells; module interest (learning progress) trajectories.",
            "performance_value": "Worse than MGE-βVAE and often worse than flat RGE when representation is entangled; modular architecture can be detrimental when modules do not align with controllable factors (no exact numeric coverage provided).",
            "complexity_variation_relationship": "When representation is entangled, modularization harms performance because entanglement introduces spurious correlations and noisy interest estimates across modules; high environment variation (distractor) contaminates modules and misleads module sampling.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": "Poorer exploration than modular/disentangled counterpart; modules show random-walk interest curves and do not focus on controllable features.",
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Two-stage: passive VAE representation learning (β=1, capacity=0) then modular IMGEP with competence-progress module sampling; bootstrap with random parameterization.",
            "generalization_tested": false,
            "generalization_results": null,
            "sample_efficiency": "20 trials × 10,000 episodes; observed to need larger exploration noise to achieve diversity; overall less sample-efficient than disentangled modular approach.",
            "key_findings": "Using a modular IMGEP with an entangled learned representation degrades performance: entanglement mixes controllable and uncontrollable factors across modules, leading to noisy interest signals and suboptimal module selection, especially in presence of distractor variation.",
            "uuid": "e1034.2",
            "source_info": {
                "paper_title": "Curiosity Driven Exploration of Learned Disentangled Goal Spaces",
                "publication_date_yy_mm": "2018-07"
            }
        },
        {
            "name_short": "RGE-VAE",
            "name_full": "Random Goal Exploration with learned entangled goal space (VAE)",
            "brief_description": "A flat IMGEP that uses a VAE-learned entangled latent space as a single goal space and samples goals uniformly.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "7-joint simulated robotic arm (IMGEP agent)",
            "agent_description": "Same simulated arm using IMGEP with a single entangled latent goal space; goals sampled uniformly over the learned latent variables.",
            "agent_type": "simulated agent",
            "environment_name": "Arm-2-Balls",
            "environment_description": "Two-ball pixel scene with one controllable ball and one random-walking distractor; observations are 64×64 images.",
            "complexity_measure": "Two objects; latent dimensionality 10; entangled latent factors meaning single latents encode mixed generative factors.",
            "complexity_level": "medium",
            "variation_measure": "Distractor random walk; sampling of training images uniformly over object positions for representation learning.",
            "variation_level": "medium-high",
            "performance_metric": "Exploration coverage ratio (900-cell grid), speed of coverage vs episodes.",
            "performance_value": "Performs better than Random Parameter Exploration but worse than methods using disentangled representations; tends to produce observations clustered in smaller region unless exploration noise is large.",
            "complexity_variation_relationship": "Entangled representation fails to disentangle controllable factors from distractor variation, so flat IMGEP with entangled latent space is vulnerable to variation and less efficient.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": "Inferior exploration coverage compared to disentangled counterparts; requires larger exploration noise to compensate.",
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Two-stage: passive VAE representation learning then flat IMGEP goal sampling; bootstrap with random parameterization.",
            "generalization_tested": false,
            "generalization_results": null,
            "sample_efficiency": "20 trials × 10,000 episodes; less sample-efficient than disentangled approaches.",
            "key_findings": "Flat IMGEP with an entangled learned goal space provides limited exploration efficiency; entanglement blurs controllability signals and makes exploration rely on larger random motor noise.",
            "uuid": "e1034.3",
            "source_info": {
                "paper_title": "Curiosity Driven Exploration of Learned Disentangled Goal Spaces",
                "publication_date_yy_mm": "2018-07"
            }
        },
        {
            "name_short": "RPE",
            "name_full": "Random Parameter Exploration",
            "brief_description": "Baseline exploration strategy that uniformly samples policy parameters (θ) with no goal-directed mechanism.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "7-joint simulated robotic arm (baseline agent)",
            "agent_description": "Simulated arm whose exploration is pure random sampling of motor parameterizations (θ) providing a lower-bound baseline for exploration efficiency comparisons.",
            "agent_type": "simulated agent",
            "environment_name": "Arm-2-Balls",
            "environment_description": "Same two-ball pixel environment with a controllable ball and an uncontrollable distractor performing a random walk; observations are 64×64 pixels.",
            "complexity_measure": "Two objects; high-dimensional action parameter space (controller parameterizations), stochastic distractor.",
            "complexity_level": "medium",
            "variation_measure": "Distractor random walk; exploration policy does not adapt to variation.",
            "variation_level": "medium-high",
            "performance_metric": "Exploration coverage ratio over 900 discretized cells and number of times the ball was effectively handled.",
            "performance_value": "Much poorer performance than all IMGEP variants; fails to produce wide variety of observations (curves in paper show low coverage), exact numeric coverage not tabulated in text.",
            "complexity_variation_relationship": "Random parameter search is strongly inefficient in environments with high-dimensional continuous actions and distractors; does not exploit structure or reduce effect of variation.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": "Poor exploration coverage and lower number of successful interactions (e.g., ball handling) relative to IMGEP methods.",
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Single-phase random sampling across parameterizations; used as baseline.",
            "generalization_tested": false,
            "generalization_results": null,
            "sample_efficiency": "Very low; across 10,000 episodes produces far fewer unique reached outcomes than IMGEP variants.",
            "key_findings": "Random parameter exploration fails in high-dimensional continuous action settings and in presence of distractors; IMGEPs (especially modular + disentangled) strongly outperform RPE.",
            "uuid": "e1034.4",
            "source_info": {
                "paper_title": "Curiosity Driven Exploration of Learned Disentangled Goal Spaces",
                "publication_date_yy_mm": "2018-07"
            }
        },
        {
            "name_short": "MGE-EFR",
            "name_full": "Modular Goal Exploration with Engineered Features Representation",
            "brief_description": "Modular IMGEP that uses handcrafted semantic goal spaces (engineered features) corresponding to true object positions as an upper-bound comparison.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "7-joint simulated robotic arm (IMGEP agent with engineered goals)",
            "agent_description": "Simulated arm using IMGEP where the goal modules are handcrafted engineered features containing true object degrees of freedom (positions of the two balls); module sampling by competence progress and inverse-model meta-policy used.",
            "agent_type": "simulated agent",
            "environment_name": "Arm-2-Balls",
            "environment_description": "Same two-ball pixel environment, but goal space provided directly as semantic positions of the two balls in [-1,1]^4 (two modules: controllable ball and distractor).",
            "complexity_measure": "Number of objects = 2; goal-space dimensionality = 4 (x/y for each ball); observation complexity (pixels) is abstracted away by engineered features.",
            "complexity_level": "low-to-medium from the goal-space perspective (engineered semantic features simplify structure)",
            "variation_measure": "Distractor random walk still present but represented semantically in the distractor module; module sampling can avoid distractor if low learning progress.",
            "variation_level": "medium-high (in environment), but modular engineerd goal space allows ignoring distractor variation via module selection.",
            "performance_metric": "Exploration coverage ratio over 900 discretized cells for controllable ball, number of handled episodes; speed of coverage.",
            "performance_value": "Serves as an upper bound: modular IMGEP with engineered features achieves high coverage and serves as performance target that MGE-βVAE matches qualitatively; theoretical max coverage ratio ≈ π/4 ≈ 0.785.",
            "complexity_variation_relationship": "With engineered modular goals aligned to true degrees of freedom, modular IMGEP easily focuses on controllable modules and ignores distractor variation, yielding best exploration performance; demonstrates the value of aligning goal-space structure to environment structure.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": "High performance; modular engineered feature approach efficiently handles distractor variation by focusing on controllable module.",
            "low_complexity_low_variation_performance": null,
            "training_strategy": "IMGEP with modular engineered goal spaces; module sampling driven by competence progress; inverse-model meta-policy; bootstrap with random parameterization.",
            "generalization_tested": false,
            "generalization_results": null,
            "sample_efficiency": "High sample-efficiency relative to other baselines; achieves wide coverage with fewer experiments compared to RPE and entangled learned-space methods.",
            "key_findings": "Engineered modular goal spaces aligned with environment structure provide an upper bound on exploration performance; learned disentangled (β-VAE) modular spaces can match this performance, indicating representational quality is key to effective exploration in varied environments.",
            "uuid": "e1034.5",
            "source_info": {
                "paper_title": "Curiosity Driven Exploration of Learned Disentangled Goal Spaces",
                "publication_date_yy_mm": "2018-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Active learning of inverse models with intrinsically motivated goal exploration in robots",
            "rating": 2,
            "sanitized_title": "active_learning_of_inverse_models_with_intrinsically_motivated_goal_exploration_in_robots"
        },
        {
            "paper_title": "Unsupervised Learning of Goal Spaces for Intrinsically Motivated Goal Exploration",
            "rating": 2,
            "sanitized_title": "unsupervised_learning_of_goal_spaces_for_intrinsically_motivated_goal_exploration"
        },
        {
            "paper_title": "Modular active curiosity-driven discovery of tool use",
            "rating": 2,
            "sanitized_title": "modular_active_curiositydriven_discovery_of_tool_use"
        },
        {
            "paper_title": "Early Visual Concept Learning with Unsupervised Deep Learning",
            "rating": 1,
            "sanitized_title": "early_visual_concept_learning_with_unsupervised_deep_learning"
        },
        {
            "paper_title": "beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework",
            "rating": 2,
            "sanitized_title": "betavae_learning_basic_visual_concepts_with_a_constrained_variational_framework"
        },
        {
            "paper_title": "Disentangling the independently controllable factors of variation by interacting with the world",
            "rating": 2,
            "sanitized_title": "disentangling_the_independently_controllable_factors_of_variation_by_interacting_with_the_world"
        }
    ],
    "cost": 0.014756499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Curiosity Driven Exploration of Learned Disentangled Goal Spaces</p>
<p>Adrien Laversanne-Finot adrien.laversanne-finot@inria.fr 
Flowers Team Inria and Ensta-ParisTech
France</p>
<p>Alexandre Péré alexandre.pere@inria.fr 
Flowers Team Inria and Ensta-ParisTech
France</p>
<p>Pierre-Yves Oudeyer pierre-yves.oudeyer@inria.fr 
Flowers Team Inria and Ensta-ParisTech
France</p>
<p>Curiosity Driven Exploration of Learned Disentangled Goal Spaces
Goal explorationMulti-goal learningIntrinsic motivationIndepen- dently controllable features
Intrinsically motivated goal exploration processes enable agents to autonomously sample goals to explore efficiently complex environments with high-dimensional continuous actions. They have been applied successfully to real world robots to discover repertoires of policies producing a wide diversity of effects. Often these algorithms relied on engineered goal spaces but it was recently shown that one can use deep representation learning algorithms to learn an adequate goal space in simple environments. However, in the case of more complex environments containing multiple objects or distractors, an efficient exploration requires that the structure of the goal space reflects the one of the environment. In this paper we show that using a disentangled goal space leads to better exploration performances than an entangled one. We further show that when the representation is disentangled, one can leverage it by sampling goals that maximize learning progress in a modular manner. Finally, we show that the measure of learning progress, used to drive curiosity-driven exploration, can be used simultaneously to discover abstract independently controllable features of the environment. The code used in the experiments is available at https://github.com/flowersteam/ Curiosity_Driven_Goal_Exploration.</p>
<p>Introduction</p>
<p>A key challenge of lifelong learning is how embodied agents can discover the structure of their environment and learn what outcomes they can produce and control. Within a developmental perspective [1,2], this entails two closely linked challenges. The first challenge is that of exploration: how can learners self-organize their own exploration curriculum to discover efficiently a maximally diverse set of outcomes they can produce. The second challenge is that of learning disentangled representations of the world out of low-level observations (e.g. pixel level), and in particular, discovering abstract high-level features that can be controlled independently.</p>
<p>Exploring to discover how to produce diverse sets of outcomes. Discovering autonomously a diversity of outcomes that can be produced on the environment through rolling out motor programs has been shown to be highly useful for embodied learners. This is key for acquiring world models and repertoires of parameterized skills [3,4,5], to efficiently bootstrap exploration for deep reinforcement learning problems with rare or deceptive rewards [6,7], or to quickly repair strategies in case of damages [8]. However, this problem is particularly difficult in high-dimensional continuous action and state spaces encountered in robotics given the strong constraints on the number of samples that can be experimented. In many cases, naive random exploration of motor commands is highly inefficient due to high-dimensional action spaces, redundancies in the sensorimotor system, or to the presence of "distractors" that cannot be controlled [3].</p>
<p>Several approaches to organize exploration can be considered. First, imitation learning can be used to take advantage of observations of another agent acting on the environment [9]. While observing the environment changing as a consequence of other agent's actions can often be leveraged, there are many cases where it is either impossible for other agents to demonstrate how to act, or for the learner to observe the motor program used by the other agent. For these reasons, various forms of autonomous curiosity-driven learning approaches have been proposed [10], often inspired by forms of spontaneous exploration displayed by human children [11]. Some of these approaches have used the framework of (deep) reinforcement learning, considering intrinsic rewards valuing states or actions in terms of novelty, information gain, or prediction errors, e.g. [12,13,14,5]. However, many of these approaches are not directly applicable to high-dimensional redundant continuous action spaces [12,15], or face complexity challenges to be applicable to real world robots [16,17].</p>
<p>Another approach to curiosity-driven exploration is known as Intrinsically Motivated Goal Exploration Processes (IMGEPs) [3,18], an architecture closely related to Goal Babbling [19]. The general idea of IMGEPs is to equip the agent with a goal space, where each point is a vector of (target) features of behavioural outcomes. During exploration, the agent samples goals in this goal space according to a certain strategy. A powerful strategy for selecting goals is to maximize empirical competence progress using multi-armed bandits [3]. This enables to automate the formation of a learning curriculum where goals are progressively explored from simple to more complex, avoiding goals that are either too simple or too complex. For each sampled goal the agent dedicates a budget of experiments to improve its performance regarding this particular goal. IMGEPs are often implemented using a population approach, where the agent stores an archive of all the policy parameters and the corresponding outcomes. This makes the approach powerful since the agent is able to leverage each encountered past experience when facing a new goal. This approach has been shown to enable high-dimensional robots to learn very efficiently locomotion skills [3], manipulation of soft objects [19,20] or tool use [18]. Related approaches were recently experimented in the context of Deep Reinforcement Learning, such as in Hindsight Experience Replay [21] and Reverse Curriculum Learning [22] (however using monolithic goal parameterized policies), and within the Power Play framework [23].</p>
<p>Learning disentangled representations of goal spaces. Even if IMGEP approaches have been shown to be very powerful, one limit has been to rely on engineered representations of goal spaces. For example, experiments in [3,7,18,22,21] have leveraged the availability of goal spaces that directly encoded the position, speed or trajectories of objects/bodies. A major challenge is how to learn goal spaces in cases where only low-level perceptual measures are available to the learner (e.g. pixels). A first step in this direction was presented in [24], using deep networks and algorithms such as Variational AutoEncoders (VAEs) to learn goal spaces as a latent representation of the environment. In simple simulated scenes where a robot arm learned to interact with a single controllable object, this approach was shown to be as efficient as using handcrafted goal features. But [24] did not study what was the impact of the quality of the learned representation. Moreover, when the environment contains several objects including a distractor object, an efficient exploration of the environment is possible only if the structure of the goal space reflects the one of the environment. For example, when objects are represented as abstract distinct entities, modular curiosity-driven goal exploration processes can be leveraged for efficient exploration, by focusing on objects that provide maximal learning progress, and avoiding distractor objects that are either trivial or not controllable [18]. An open question is thus whether it is possible to learn goal spaces with adequate disentanglement properties and develop exploration algorithms that can leverage those learned disentangled properties from low-level perceptual measures.</p>
<p>Discovering high-level controllable features of the environment. Although methods to learn disentangled representation of the world exist [25,26,27], they do not allow to distinguish features that are controllable by the learner from features describing external phenomena that are outside the control of the agent. However, identifying such independantly controllable features [28] is of paramount importance for agents to develop compact world models that generalize well, as well as to grow efficiently their repertoire of skills. One idea to address this challenge, initially explored in [29], is that learners may identify and characterize controllable sets of features as sensorimotor space manifolds where it is possible to learn how to control perceptual values with actions, i.e. where learning progress is possible. Unsupervised learning approaches could then build coarse categories distinguishing the body, controllable objects, other animate agents, and uncontrollable objects as entities with different learning progress profiles [29]. However, this work only considered identifying learnable and controllable manifolds among sets of engineered features.</p>
<p>In this paper, we explore the idea that a useful learned representation for efficient exploration would be a factorized representation where each latent variable would be sensitive to changes made in a single true dregree of freedom of the environment, while being invariant to changes in other degrees of freedom [30]. Further on, we investigate how independently controllable features of the environment can be identified among these disentangled variables through interactions with the environement. We study this question using β-VAEs [25,31] which is a natural extension of VAEs and have been shown to provide good disentanglement properties. We extend the experimental framework of [24], simulating a robot arm learning how it can produce outcomes in a scene with two objects, including a distractor. In order to assess the role of the representation we use a two-stage process, which first learns to see and then learns to act. The first stage consists of a representation learning phase where the agent builds a representation of the world by passively observing it (events in the environment are assumed to be produced by another agent in this phase, see [24]. In the second phase the agent uses this representation to interact with the world, by sampling goals that provide high learning progress, and where goals are target values of one or several latent variables to be reached through action. This procedure was adopted for two reasons. For one, it is similar to the developmental progression in infant development, where the infant first spends most of his time observing the world due to limitations in motor exploration. Secondly, it helps in understanding the impact of disentanglement given the multiple components of the architecture.</p>
<p>The first contribution we make in this paper is to study the impact of using a learned disentangled goal space representations on the efficiency of exploration and discovery of a diversity of outcomes in IMGEPs. To the best of our knowledge, it is the first time that the role of disentanglement is studied in the context of exploration. Precisely, we show that:</p>
<p>• using a disentangled state representation is beneficial to exploration: using IMGEPS, the agents explores more states in fewer experiments than when the representation is entangled. • disentangled representations learned by β-VAEs can be further leveraged by modular curiosity-driven IMGEPs to explore as efficiently as using handcrafted low-dimensional scene features, in experiments that include both controllable and distractor objects. On the contrary, we show that representations learned by VAEs are not sufficiently structured to enable a similarly efficient exploration.</p>
<p>The second contribution of this article is to show that identifying abstract independently controllable features from low-level perception can emerge from a representation learning pipeline where learning disentangled features from passive observations (β-VAEs) is followed by curiosity-driven active exploration driven by the maximization of learning progress. This second phase allows in particular to distinguish features related to controllable objects (disentangled features with high learning progress) from features related to distractors (disentangled features with low learning progress).</p>
<p>Modular goal exploration with learned goal spaces</p>
<p>This section introduces Intrinsically Motivated Goal Exploration Processes with modular goal spaces as they are typically used in environments with handcrafted goal spaces. It then describes the architecture used in this article where the handcrafted goal space is replaced by a representation of the space that is learned before exploration and then used as a goal space for IMGEPs. The overall architecture is summarized in Figure 1.</p>
<p>Intrinsically motivated goal exploration processes with modular goal spaces</p>
<p>To fully understand the IMGEP approach, one must imagine the agent as performing a sequence of contextualized and parameterized experiments. The problem of exploration is defined using the following elements:</p>
<p>• A context space C. The context c represents the initial state of the environment. It corresponds to parameters of the experiment that are not chosen by the agent. • A parameterization space Θ. The parameterization θ corresponds to the parameters of the experiment that the agent can control at will (e.g. motor commands for a robot). • An observation space O. Here we consider an observation o to be a vector representing all the signals captured by the agent sensors during an experiment (e.g. raw images). • An environment dynamic D : C, Θ → O which maps parameters performed in a certain context, to observations (or outcomes). This dynamic is considered unknown.</p>
<p>For instance, as presented in Figure 2, a parameterization could be the weights of a closed-loop neural network controller for a robot manipulating a ball. A context could be the initial position of the ball and an observation could be the position of the ball at the end of a fixed duration experiment. The exploration problem can then be simply put as:</p>
<p>Given a budget of n experiments to perform, how to gather tuples
{(c i , θ i , o i )} i=1...n which maximize the diversity of the set of observations {o i } i=1...n .
One approach that was shown to produce good exploration performances is Intrinsically Motivated Goal Exploration Processes (IMGEPs). This algorithmic architecture uses the following elements:</p>
<p>• A goal space T . The elements τ ∈ T represent the goals that the agent can set for himself. We also use the term task to refer to an element of T .</p>
<p>• A goal sampling policy γ : T → [0, 1]. This distribution allows the agent to choose a goal in the goal space. Depending on the exploration strategy being active or fixed, this distribution can evolve during exploration. • A Meta-Policy mechanism (or Internal Predictive Model) Π : T , C → Θ, which given a goal and a context, outputs a parameterization that is most likely to produce an observation fulfilling the goal, under the current knowledge. • A cost function C : T , O → R, internally used by the Meta-Policy. This cost function outputs the fitness of an observation for a given task τ .</p>
<p>When the environment is simple, such as for experiments presented in [24] where a robotic arm explore its possible interactions with a single object, the structure of the goal space is not critical. However, in more complex scenes with multiple objects (e.g. including tools or objects that cannot be controlled), it was shown in [32] that it is important to have a goal space which reflects the structure of the environment. In particular, having a modular goal space, i.e. of the form T = N i=1 T i , where the T i are different modules representing the properties of various objects, leads to much better exploration performances. In that case a goal can correspond to achieving an observation where a given object is in a given position.</p>
<p>The algorithmic architecture works as follows: at each step, the exploration process samples a module, then samples a a goal in this module, observes the context, executes a meta-policy mechanism to guess the best policy parameters for this goal, which it then uses to perform the experiment. The observation is then compared to the goal, and used to update the meta-policy (leveraging the information for other goals) as well as the module sampling policy. The general idea of IMGEPs is depicted in Figure 2. Depending on the algorithmic instantiation of this architecture, different Meta-Policy mechanisms can be used [3,32]. In any case, the Meta-Policy must be initialized using a buffer of experiments {c i , θ i , o i } containing at least two different o i . As such, a bootstrap of several Random Parameterization Exploration iterations is always performed at the beginning. This leads to Algorithmic Architecture 1. The reader can refer to Appendix 6.1 for a detailed explanation of the Meta-Policy implementation. 
γ(τ ) = γ(τ |i)p(i),(1)
where p(i) is the probability to sample the T i module, and γ(τ |i) is the probability to sample the goal τ given that the module i was selected. The strength of the modular architecture is that modules can be selected using a curiosity-driven active module sampling scheme. In this scheme, γ(τ |i) is fixed, and p(i) is updated at time t according to:
p(i) := 0.9 × Υ i (t) N k=1 Υ k (t) + 0.1 × 1 N ,(2)
where Υ i (t) is an interest measure based on the estimation of the average improvement of the precision of the meta-policy for fulfilling goals in T i , which is a form of learning progress called competence progress (see [3] and Appendix 6.1 for further details on the interest measure). The second term of Equation (2) forces the agent to explore a random module 10% of the time. The general idea is that monitoring the learning progress allows the agent to concentrate on objects which can be learned to control while ignoring objects that cannot.</p>
<p>Modular Unsupervised Goal-space Learning for IMGEP</p>
<p>In [24], an algorithm for Unsupervised Goal-space Learning (UGL) was proposed. The principle is to let the agent observe another agent producing a diversity of observations {o i }. This set of observations is used to learn a low-dimensional representation which is then employed as a goal-space.</p>
<p>In these experiments, there is always a single goal space corresponding to the learned representation of the environment. However, if one wishes to use the algorithm presented in the previous section, it is necessary to have different goal spaces: one for each module.</p>
<p>In order to use a Modular Goal Exploration strategy with a learned goal space, we propose Algorithm 2, which performs Modular Unsupervised Goal-space Learning (MUGL) and is represented in Figure 3. The idea is to learn a representation of the observations in the same way as UGL. The modules are then defined as subsets of latent variables. For example, a module could be made of the first and second latent variables. Accordingly, goals sampled by this module are defined as achieving a certain value for the first and second latent variables of the representation of an observation. The underlying rationale is that, if we manage to learn a disentangled representation of the observations, each latent variable would correspond to a single property of a single object. Thus, by forming modules containing only latent variables corresponding to the same object, the exploration algorithm may be able to explore the different objects separately.</p>
<p>After learning the representation, a specific criterion is used to decide how the latent variables should be grouped to form modules. In the particular case of VAEs and βVAEs, the grouping is made by projecting latent variables which have similar Kullback-Leibler on their respective subspace (see Appendix 6.1). Since representations learned with VAEs and βVAEs come with a prior over the latent variables, instead of estimating the modular goal-policies γ(τ |k), we used the Gaussian prior Generate an ensemble of projection operators assumed during training. Finally, the cost function C k (τ, o) is defined, using the distance between the goal and the k-th projection of the latent representation of the observation.
{P k } 7 Estimate γ(τ |k) from {P k R(o i )} i=0,...,nr using algorithm E 8 Set the cost functions to be C k (τ, o) = P k R(o) − τ 9 return The goal modules {R, P k , γ(τ |k), C k }.
The overall approach combining IMGEPs with learned modular goal spaces is summarized in Figure 1. Note that the algorithm proposed in [24] is a particular instance of this architecture with only one module containing all the latent variables. In this case there is no module sampling strategy, and only a goal sampling strategy. This specific case is here referred to as Random Goal Exploration (RGE).</p>
<p>Experiments</p>
<p>We carried out experiments in a simulated environment to address the following questions:</p>
<p>• To what extent is the structure of the learned representation important for the performance of IMGEP-UGL in terms of efficiently discovering a diversity of observations? • Is it possible to leverage the structure of the representation with Modular Curiosity-Driven Goal Exploration algorithms? • Can the learning progress measure of goal exploration be used to identify controllable abstract features of the environment?</p>
<p>Environment We experimented on the Arm-2-Balls environment, where a rotating 7-joints robotic arm evolves in an environment containing two balls of different sizes, as represented in Figure 4. One ball can be grasped and moved around in the scene by the robotic arm. The other ball acts as a distractor: it cannot be grasped nor moved by the robotic arm but follows a random walk.</p>
<p>The agent perceives the scene as a 64 × 64 pixels image. For the representation learning phase, we generated a set of images where the positions of the two balls were uniformly distributed over [−1, 1] 4 . These images were then used to learn a representation using a VAE or a βVAE. In order to assess the importance of the disentangled representation, we used the same disentangled/entangled representation for all the instantiations of the exploration algorithms. This allowed us to study the effect of disentangled representations by eliminating the variance due to the inherent difficulty of learning such representations.</p>
<p>Baselines Results obtained using IMGEPs with learned goal spaces are compared to two baselines: </p>
<p>Results</p>
<p>To assess the performances of the MGE algorithm on learned goal spaces, we experimented with two different representations coming from two learning algorithms: β-VAE (disentangled) and VAE (entangled). In each case, we ran 20 trials of 10,000 episodes each, for both the RGE and MGE exploration algorithms. One episode is defined as one experimentation/roll-out of a parameter θ.</p>
<p>Exploration performances The exploration performance of all the algorithms was measured according to the number of cells reached by the ball in a discretized grid of 900 cells (30 cells for each dimension of the ball that can be moved; the distractor is not accounted for in the exploration evaluation). Not all cells can be reached given that the arm is rotating and is of unit length: the maximum ratio between the number of reached cells and all the cells is approximately π/4 ≈ 0.8.</p>
<p>In Figure 5, we can see the evolution of the ratio of the number of cells visited with respect to all the cells through exploration. First, one can see that all the algorithms have much better performances than the naive RPE, both in term of speed of exploration and final performance. Secondly, for both RGE and MGE with learned goal spaces, using a disentangled representation is beneficial. One can also see that when the representation used as a goal space is disentangled, the modular architecture (MGE-βVAE) performs much better than the flat architecture (RGE-βVAE), with performances that match the modular architecture with engineered features (MGE-EFR). However, when the representation is entangled, using a modular architecture is actually detrimental to the performances since each module encodes then only partially for the ball position. Figure 5 also shows that the MGE architectures with a disentangled representation performs particularly well even if the exploration noise is low whereas the RGE architectures or MGE architectures with an entangled representation relies on a large exploration noise to produce a large variety of observations. We cross-refer to Appendix 6.7 for examples of exploration curves together with exploration scatters.</p>
<p>Benefits of disentanglement and modules</p>
<p>The evolution of the interest of the different modules through exploration is represented in Figure 6a. First, in the disentangled case, one can see that the interest is high only for the modules corresponding to the latent variables encoding for the ball position. 1 This is natural since these latent variables are the only ones that can be learned to control with motor commands. In the entangled case, the interest of each module follows a random trajectory, with no module standing out with a particular interest. This effect can be understood as follows: the entanglement introduces spurious correlations between the observations and the tasks in every module, which bring the interest measures to follow random fluctuations based on the collected observations. These correlations, in turn, lead the agent to sample more frequently policies that in fact did not have any impact on the observation, making the overall performance worse (see Appendix 6.1 and 6.6 for details).</p>
<p>When the representation used as a goal space is disentangled, the modular approach is particularly well suited in the presence of distractors. Indeed, thanks to the projection operator, the noise introduced in the latent variables by the random walk of the distractor is completely ignored by the module which contains the latent variables of the ball. This allows to learn a better inverse model for modules which ignore the distractor, which in turn yields a better exploration (see Appendix 6.1 and 6.6 for details).</p>
<p>Independently Controllable Features</p>
<p>As explained above and illustrated in Figure 6a, when the representation is disentangled, the MGE algorithm is able to monitor the learnability of certain modules (possibly individual latent features, see 6.5), and leverage it to focus exploration on goals with high learning progress. This is illustrated on the interest curves by the clear difference in interest between modules where learning progress happens and those where it does not. It happens that modules that produce high learning progress correspond precisely to modules that can be controlled. As such, as a side benefit of using modular goal exploration algorithms, the agent discovers in an unsupervised manner which are the features of the environment that can be controlled (and in turn explores them more). This knowledge could then be used by another algorithm whose performance depends on its ability to know which are the independantly controllable features of the environment.</p>
<p>Conclusion</p>
<p>In this paper we studied the role of the structure of learned goal space representations in IMGEPs. More specifically, we have shown that when the representation possesses good disentanglement properties, they can be leveraged by a curiosity-driven modular goal exploration architecture and lead to highly efficient exploration. In particular, this enables exploration performances as good as when using engineered features. In addition, the monitoring of learning progress enables the agent to discover which latent features can be controlled by its actions, and focus its exploration by setting goals in their corresponding subspace.</p>
<p>The perspectives of this work are twofold. First it would be interesting to show how the initial representation learning step could be performed online. Secondly, beyond using learning progress to discover controllable features during exploration, it would be interesting to re-use this knowledge to acquire more abstract representations and skills.</p>
<p>Finally, as mentioned in the introduction, another advantage of using a disentangled representation is that, as was shown in [31], it evinces superior performances in a transfer learning scenario. Both approaches are not incompatible and one could envision a scheme where one would learn a disentangled representation in a simulated environment and use this representation to perform exploration in a real world environment. 6 Appendices</p>
<p>Intrinsically Motivated Goal Exploration Processes</p>
<p>In this part, we give further explanations on Intrinsically Motivated Goal Exploration Processes.</p>
<p>Meta-Policy Mechanism</p>
<p>This mechanism allows, given a context c and a goal τ , to find the parameters θ that are most likely to produce an observation o fulfilling the task τ . The notion of an observation o fulfilling a task τ is quantified using a cost function C : T × O → R. The cost function can be seen as representing the fitness of the observation o regarding the task τ .</p>
<p>The meta-policy can be constructed in two different ways which are depicted in Figure 7:</p>
<p>• Direct-Model Meta-Policy: In this case, an approximate phenomenon dynamic modelD is learned using a regressor (e.g. LWR). The model is then updated regularly by performing a training step with the newly acquired data. At execution time, for a given goal τ , a loss function is defined over the parameterization space through L(θ) = C(τ,D(θ, c)). A black-box optimization algorithm, such as L-BFGS, is then used to optimize this function and find the optimal set of parameters θ (see [3,32,33] for examples of such meta-policy implementations in the IMGEP framework). In our case, we took the approach of using an Inverse-Model based Meta-Policy. We draw the attention of the reader on the following implementation details:</p>
<p>• Depending on the case, multiple observations, and consequently multiple parameters can optimally solve a task, while a combination of them cannot. This is known as the redundancy problem in robotics and special approaches must be used to handle it when learning inverse models, in particular within the IMGEP framework [3]. This has also been tackled under the terminology of multi-modality in [34]. To solve this problem, we used a κ-nn regressor with κ = 1. • Turning observations {o i } into goals {τ i } may prove difficult in some cases. Indeed, it may happen that a given observation does not solve optimally any task in the goal space, or that it solves optimally multiple tasks. In our case, we assumed that the learned encoder is a one-to-one map from observation space to goal space and thus, that every observation solves optimally a unique task in each module. Hence, tasks were associated to observations using the encoder R: τ i := R(o i ). • Since the different modules are associated to projection operators, each produced observation o optimally solve one task for each module. Indeed, if we consider projections on the canonical axis of the latent space, o will solve one task for each module, corresponding to each component of R(o). This mechanism allows to leverage information of every single observation, for all goal-space modules. For this reason, one κ-nearest-neighbor model was used for each module of the goal space. At each exploration iteration all the modules are updated using their associated projection operators on the embedding of the outcome.</p>
<p>Our particular implementation of the Meta-Policy is outlined in Algorithm 3. The Meta-Policy is instantiated with one database per goal module. Each database store the representations of the observations projected on its associated subspace together with the associated contexts and parameterizations. Given that the meta policy is implemented with a nearest neighbor regressor, training the meta policy simply amounts to updating all the databases. Note that, as stated above, even though at each step the goal is sampled in only one module, the observation obtained after an exploration iteration is used to update all databases. Active module sampling based on Interest measure Recalling from the paper, at each iteration, the probability of sampling a specific module T i is given by:
γ(i) := 0.9 × Υ i (t) N k=1 Υ k (t) + 0.1 × 1 N ,
where Υ i (t) represents the interest of the T i module after t iterations.</p>
<p>Let H (i) t = {(τ k , θ k , P i R(o k ))} τ k ∈Ti be the history of experiments obtained when the goal was sampled in module T i . The progress in module i at exploration step t is defined as:
δ (i) t = C i (τ t , P i R(o )) − C i (τ t , P i R(o t )),(3)
where o t and τ t are respectively the observation and goal for the current exploration step and o is the observation associated to the experiment in H (i) t for which the goal τ is the closest to τ t . The interest of a module is designed to track the progress. Specifically, the interest of each module is updated according to:
Υ i (t) = n − 1 n Υ i (t − 1) + 1 n δ t ,(4)
where n = 1000 is a decay rate that ensures that if no progress is made the interest of the module will go to zero over time. One can refer to [32] for details on this approach.</p>
<p>Projection criterion for VAE and βVAE An important aspect of the MUGL algorithm is the choice of the projection operators {P k }. In this work, the representation learning algorithms are VAE and βVAE. In this case, two projection schemes can be considered:</p>
<p>• Projection on all canonical axis: n d projection operators, each projecting the latent point on a single latent axis. • Projection on 2D planes sorted by D KL : n d 2 projection operators, each projecting on a 2D plane aligned with latent axis. The grouping of dimensions as 2D planes is performed by sorting the dimensions by increasing D KL , i.e. the divergence is computed for each dimension, by projecting the latent representation on the dimension and measuring its divergence with the unit gaussian prior. Latent dimensions are then grouped two by two according to their D KL value.</p>
<p>In this work we mainly considered the second grouping scheme. The first grouping scheme could be considered to discover which features can be controlled. Of course in practice one often does not know in advance how many latent variables should be grouped together and it can be necessary to consider more advanced grouping schemes. In practice it is often the case that latent variables which correspond to the same objects have similar KL divergence value (see Figure 8 for an example of a training curve and appendix 6.2 for an explanation of this phenomenon). As such it could be envisioned to group latent variables which have similar KL divergence together.</p>
<p>Deep Representation Learning Algorithms</p>
<p>In this section we summarize the theoretical arguments behind Variational AutoEncoder (VAE) and βVAE.</p>
<p>Variational Auto-Encoders (VAEs) Let x ∈ X be a set of observations. If we assume that the observed data are realizations of a random variable, we can hypothesize that they are conditioned by a random vector of independent factors z, i.e. that p(x, z) = p(z)p θ (x, z), where p(z) is a prior distribution over z and p θ (x, z) is a conditional distribution. In this setting, given a i.i.d dataset X = {x 1 , . . . , x N }, learning the model amount to searching the parameters θ that maximizes the dataset likelihood:
log L(D) = N i=1 log p θ (x i )(5)
However, in most cases, the marginal probability:
p θ (x) = p(x, z)dz(6)
and the posterior probability:
p θ (z|x) = p(x, z) p(z) = p(x, z) p(x, z)dz(7)
are both computationally intractable, making the maximum likelihood estimation unfeasible. To overcome this problem, we can introduce an arbitrary distribution q φ (z|x) and remark that the following holds:
log p θ (x) = L(x; θ, φ) + D KL [q φ (z|x) p θ (z|x)] ,(8)
where D KL denotes the Kullback-Leibler (KL) divergence and
L(x; θ, φ) = E z∼q φ (z|x) [log p θ (x|z)] − D KL [q φ (z|x) p(z)].(9)
Since the KL divergence is non-negative, it follows from (8) that:
L(x; θ, φ) ≤ log p θ (x) − D KL <a href="10">q φ (z|x) p θ (z|x)</a>
for any distribution q, hence the name of Evidence Lower Bound (ELBO). Consequently, maximizing the ELBO has the effect to maximize the log likelihood, while minimizing the KL-Divergence between the approximate q φ (z|x) distribution, and the true unknown posterior p θ (z|x). The approach taken by VAEs is to learn the parameters of both conditional distributions p θ (x|z) and q φ (z|x) as non-linear functions. This is done by maximizing the ELBO of the dataset:
L(θ, φ) = N i=1 L(x i ; θ, φ)(11)
by jointly optimizing over the parameters θ and φ. When the prior p(z) is an isotropic unit Gaussian distribution and the variational approximation q φ (z|x) follow a Multivariate Gaussian distribution with diagonal covariance, the KL divergence term can be computed in a closed form.</p>
<p>β Variational Auto-Encoders (βVAEs) In essence, a VAE can be understood as an AutoEncoder with stochastic units (q φ (z|x) plays the role of an encoder while p θ (x|z) plays the role of the decoder), together with a regularization term given by the KL divergence between the approximation of the posterior and the prior. This term ensures that the latent space is structured. The existence of a prior over the latent variables gives the ability to use a VAE as a generative model, and latent variables sampled according to the prior p(z) can be transformed by the decoder into samples.</p>
<p>Ideally, in order to be more easily interpretable, we would like to have a disentangled representation, i.e. a representation where a single latent is sensitive to changes in only one generative factor while being invariant to changes in other factors. When the prior distribution p(z) is an isotropic unit Gaussian distribution (p(z) = N (0, I)) the role of the regularization term can be understood as a pressure that encourages the VAE to learn independent latent factors z. As such, it was suggested in [25,31] that modifying the training objective to:
L(x; θ, φ) = E z∼q φ (z|x) [log p θ (x|z)] − βD KL [q φ (z|x) p θ (z)],(12)
where β is an additional parameter, will allow one to control the degree of applied pressure to learn independent generating factors by tuning the parameter β. In particular values of β higher than 1 should lead to representations with better disentanglement properties.</p>
<p>One of the drawbacks of βVAE is that for large values of β the reconstruction cost is often dominated by the KL divergence term. This leads to poor reconstructed samples where the model ignores some of the factors of variation altogether. In order to tackle this issue, it was further suggested in [35] to modify the training objective to be:
L(x; θ, φ) = E z∼q φ (z|x) [log p θ (x|z)] − β|D KL [q φ (z|x) p θ (z)] − C|,(13)
where C is a new parameter that defines the capacity of the VAE. The value of C determines the capacity of the network to encode information in the latent variables. For low values of the capacity the network will mostly reconstruct properties which have a high reconstruction cost whereas high capacity ensures that the network can have a low reconstruction error. By optimizing the training objective (13) with a gradually increased capacity the network will start to encode features with high reconstruction cost and then progressively encode more factors of variations whilst retaining disentangling in previously learned factors. At the end of the training one should thus obtain a representation with good disentanglement properties where each factor of variation is encoded into a unique latent variable.</p>
<p>In our experiments we used the training objective of Eq. (13) as detailed in Sec. 6.4.</p>
<p>Disentanglement properties</p>
<p>We compared the disentanglement properties of two representations. One with the procedure outlined in Sec. 6.2 with β = 150 and a capacity linearly increased to 12 over the course of the training. The other representation was a vanilla VAE with β = 1. In order to assess the disentanglement properties of the two representations we performed a latent traversal study. The results of which are displayed in Figure 9.</p>
<p>It was experimentally observed that the positions of the two balls were indeed disentangled in most cases when the representation was obtained using a βVAE even though the data used for the training was generated using independent samples for the position of the two balls. As explained in the previous section, this effect can be understood as follows: since the two balls do not have the same reconstruction cost, the VAE tends to reconstruct the object with the highest reconstruction cost first (in this case the largest ball), and when the capacity reaches the adequate value, it starts reconstructing the other ball [35]. It follows that the latent variables encoding for the position of the two balls are often disentangled.</p>
<p>Details of Neural Architectures and training</p>
<p>Model Architecture The encoder for the VAEs consisted of 4 convolutional layers, each with 32 channels, 4x4 kernels, and a stride of 2. This was followed by 2 fully connected layers, each of 256 units. The latent distribution consisted of one fully connected layer of 20 units parametrizing the mean and log standard deviation of 10 Gaussian random variables. The decoder architecture was the transpose of the encoder, with the output parametrizing Bernoulli distributions over the pixels. ReLu were used as activation functions. This architecture is based on the one proposed in [25].</p>
<p>Training details For the training of the disentangled representation we followed the procedure outlined in Sec. 6.2. The value of β was 150 and the capacity was linearly increased from 0 to 12 over the course of 400,000 training iterations. The optimizer used was Adam [36] with a learning rate of 5e −5 and batch size of 64. The overall training of the representation took 1M training iterations. For the training of the entangled representation the same procedure was followed except that β was set to 1 and that the capacity was set to 0.</p>
<p>Interest curves for Projection on all canonical axis</p>
<p>In the main text of the paper we discussed the case of 5 modules. In general one can imagine having one modules per latent variable. In this case the agent would learn to discover and control each of the latent variables separately.</p>
<p>In Figure 10 is represented the interest curves when there are 10 modules, one for each latent variable. When the representation is disentangled (βVAE), the interest is high only for modules which encode for some degrees of freedom of the ball. On the other hand, when the representation is entangled, the interest follows some kind of random walk for all modules. This is due to the fact that all the modules encode for both the ball and the distractor position which introduces some noise in the prediction of each module. </p>
<p>Effect of noise in the distractor</p>
<p>We also experimented with different noise level in the displacement of the distractor. As expected, when the noise level is low, the distractor does not move very far from its initial position and no longer acts as a distractor. In this case there is no advantage of using a modular algorithm as illustrated by Figure 11. However, it is still beneficial to have a disentangled representation since it helps in learning good inverse models.</p>
<p>Exploration Curves</p>
<p>Examples of exploration curves obtained with all the exploration algorithms discussed in this paper ( Figure 12 for algorithms with engineered features representation and Figure 13 for algorithms with learned goal spaces). It is clear that the random parameterization exploration algorithm fails to produce a wide variety of observations. Although the random goal exploration algorithms perform much better than the random parameterization algorithm, they tend to produce observations that are cluttered in a small region of the space. On the other hand the observations obtained with modular goal exploration algorithms are scattered over all the accessible space, with the exception of the case where the goal space is entangled (VAE). </p>
<p>Figure 1 :
1The IMGEP-MUGL approach.</p>
<p>Figure 2 :
2Intrinsically Motivated Goal Exploration Process examplified.</p>
<p>Algorithmic Architecture 1 :
1Curiosity Driven Modular Goal Exploration Strategy Input: Goal modules (engineered or learned with MUGL): {R, P i , γ(·|i), C i }, Meta-Policy Π, goal for module i, τ ∼ γ(·|i) 13 Compute θ using Meta-Policy Π on tuple (c, τ, i) 14 Perform experiment and retrieve observation o 15 Append (c, θ, o) to H 16 Update Meta-Policy Π with (c, θ, o) 17 Update module sampling probability p according to Eq. (2) 18 return The history H In a modular architecture the goal sampling policy reads:</p>
<p>Figure 3 :
3The three main steps of the MUGL algorithm</p>
<p>Algorithm 2 :
2Modular Unsupervised Goal-space Learning (MUGL) Input: Representation learning algorithm R (e.g. VAE, βVAE), sample to database D o = {o i } i=0,...,nr 5 Learn an embedding function R : O → R n d using algorithm R on data D o 6</p>
<p>Figure 4 :
4A roll-out of experiment in the Arm-2-Balls environment. The blue ball can be grasped and moved, while the orange one is a distractor that can not be handled, and follows a random walk.</p>
<p>Figure 5 :
5Exploration ratio during exploration for different exploration noises.• The first baseline is the naive approach of Random Parameter Exploration (RPE), where exploration is performed by uniformly sampling parameterizations θ. In the case of hard exploration problems, this strategy is regarded as a low performing one, since no previous information is leveraged to choose the next parameterization. This strategy gives a lower bound on the expected performances of exploration algorithms.• The second baseline is Modular Goal Exploration with Engineered Features Representation (MGE-EFR): it corresponds to a modular IMGEP in which the goal space is handcrafted and corresponds to the true degrees of freedom of the environment. In the Arm-2-Balls environment it corresponds to the positions of the two balls, given as a point in [−1, 1] 4 . Since essentially all the information is available to the agent under a highly semantic form, it is expected to give an upper bound on the performances of the exploration algorithms. We performed experiments with both one module (RGE-EFR) and two modules (one for the ball and one for the distractor) (MGE-EFR).</p>
<p>Figure 6 :
6Interest evolution for each module through epochs. In the case of a disentangled representation the algorithm shows interest only for the module which correspond to latent variables encoding for the position of the ball (which is unknown by the agent, which does not distinguish between the ball and the distractor).</p>
<p>Figure 7 :
7The two different approaches to construct a meta-policy mechanism.</p>
<p>•
Inverse-Model Meta-Policy: Here, an inverse modelĨ : T × C → Θ is learned from the history H which contains all the previous experiments in the form of tuples (c i , θ i , o i ). To do so, every experiments observations o i must be turned into a task τ i . The inverse model can then be learned using usual regression techniques from the set {(τ i , c i , θ i )}.</p>
<p>Algorithm 3 :
3Meta-Policy (simple implementation using a nearest-neighbor model) 1 Require: Goal modules: {R, P k , γ(τ |k), C k } k∈{1,..,n mod } 2 Function Initialize_Meta-Policy(H): 3 for k ∈ {1, .., n mod } do 4 database k ← VoidDatabase 5 for (c, θ, o) ∈ H do 6 Add (c, θ, P k R(o)) to database k 7 Function Update_Meta-Policy(c, θ, o): 8 for k ∈ {1, .., n mod } do 9 Add (c, θ, P k R(o)) to database k 10 Function Infer_parameterization(c, τ, k): 11 θ ← NearestNeighbor(database k , c, τ ) 12 return θ</p>
<p>Figure 8 :
8Kullback-Leibler divergence of each latent variable over training.</p>
<p>( a )Figure 9 :Figure 10 :Figure 11 :
a91011Disentangled latent representation learned by βVAE (b) Entangled latent representation learned with VAE (a) Latent traversal study for a disentangled representation (βVAE). Each row represents a latent variable and rows are ordered by KL divergence (lowest at the bottom). Each row represents the reconstruction obtained from the traversal of each latent variable over three standard variation around the unit Gaussian prior mean while keeping the other latent variables to the value obtained by running inference on an image of the dataset. From the picture it is clear that the first two latent variables encode the x and y position of the Ball and that the third and fourth latent variables encode the x and y position of the Distractor. At the end of the training the remaining latent variables have converged to the unit Gaussian prior. (b) Similar analysis for an entangled representation (VAE). No latent variable encode for a single factor of variation. Interest curves for Projection on all canonical axis Exploration ratio through epochs for all the exploration algorithms in the Arm-2-Balls environment with a distractor that does not move.</p>
<p>Figure 12 :
12Goal Exploration with Engineered Features Representation (RGE-EFR) (c) Modular Goal Exploration with Engineered Features Representation (MGE-EFR) Examples of achieved observations together with the ratio of covered cells in the Arm-2-Balls environment for RPE, MGE-EFR and RGE-EFR exploration algorithms. The number of times the ball was effectively handled is also represented.
The semantic mapping between latent variables and external objects is made by the experimenter.
AcknowledgmentsWe would like to thank Olivier Sigaud for helpful comments on an earlier version of this article.
Intrinsically Motivated Learning in Natural and Artificial Systems. G Baldassarre, M Mirolli, 10.1007/978-3-642-32375-1Springer9783642323Berlin Heidelberg; Berlin, HeidelbergG. Baldassarre and M. Mirolli. Intrinsically Motivated Learning in Natural and Artificial Systems, volume 9783642323. Springer Berlin Heidelberg, Berlin, Heidelberg, 2013. ISBN 978-3-642-32374-4. doi:10.1007/978-3-642-32375-1.</p>
<p>From Babies to Robots: The Contribution of Developmental Robotics to Developmental Psychology. A Cangelosi, M Schlesinger, 10.1111/cdep.12282Child Development Perspectives. A. Cangelosi and M. Schlesinger. From Babies to Robots: The Contribution of Developmental Robotics to Developmental Psychology. Child Development Perspectives, feb 2018. ISSN 17508592. doi:10.1111/cdep.12282.</p>
<p>Active learning of inverse models with intrinsically motivated goal exploration in robots. A Baranes, P Y Oudeyer, 10.1016/j.robot.2012.05.008Robotics and Autonomous Systems. 611A. Baranes and P. Y. Oudeyer. Active learning of inverse models with intrinsically motivated goal exploration in robots. Robotics and Autonomous Systems, 61(1):49-73, 2013. ISSN 09218890. doi:10.1016/j.robot.2012.05.008.</p>
<p>Active learning of parameterized skills. B. Da Silva, G Konidaris, A Barto, International Conference on Machine Learning. B. Da Silva, G. Konidaris, and A. Barto. Active learning of parameterized skills. In International Conference on Machine Learning, pages 1737-1745, 2014.</p>
<p>Intrinsically motivated model learning for developing curious robots. T Hester, P Stone, Artificial Intelligence. 247T. Hester and P. Stone. Intrinsically motivated model learning for developing curious robots. Artificial Intelligence, 247:170-186, 2017.</p>
<p>Improving exploration in evolution strategies for deep reinforcement learning via a population of novelty-seeking agents. E Conti, V Madhavan, F P Such, J Lehman, K O Stanley, J Clune, arXiv:1712.06560arXiv preprintE. Conti, V. Madhavan, F. P. Such, J. Lehman, K. O. Stanley, and J. Clune. Improving exploration in evolution strategies for deep reinforcement learning via a population of novelty-seeking agents. arXiv preprint arXiv:1712.06560, 2017.</p>
<p>GEP-PG: Decoupling exploration and exploitation in deep reinforcement learning. C Colas, O Sigaud, P.-Y Oudeyer, International Conference on Machine Learning (ICML). C. Colas, O. Sigaud, and P.-Y. Oudeyer. GEP-PG: Decoupling exploration and exploitation in deep reinforcement learning. In International Conference on Machine Learning (ICML), 2018.</p>
<p>Robots that can adapt like animals. A Cully, J Clune, D Tarapore, J.-B Mouret, Nature. 5217553503A. Cully, J. Clune, D. Tarapore, and J.-B. Mouret. Robots that can adapt like animals. Nature, 521(7553):503, 2015.</p>
<p>A survey of robot learning from demonstration. B D Argall, S Chernova, M Veloso, B Browning, Robotics and autonomous systems. 575B. D. Argall, S. Chernova, M. Veloso, and B. Browning. A survey of robot learning from demonstration. Robotics and autonomous systems, 57(5):469-483, 2009.</p>
<p>Computational theories of curiosity-driven learning. P.-Y Oudeyer, The New Science of Curiosity. NOVA. G. GordonP.-Y. Oudeyer. Computational theories of curiosity-driven learning. In G. Gordon, editor, The New Science of Curiosity. NOVA, 2018.</p>
<p>Curiosity and exploration. D E Berlyne, Science. 1533731D. E. Berlyne. Curiosity and exploration. Science, 153(3731):25-33, 1966.</p>
<p>Unifying count-based exploration and intrinsic motivation. M Bellemare, S Srinivasan, G Ostrovski, T Schaul, D Saxton, R Munos, Advances in Neural Information Processing Systems. M. Bellemare, S. Srinivasan, G. Ostrovski, T. Schaul, D. Saxton, and R. Munos. Unifying count-based exploration and intrinsic motivation. In Advances in Neural Information Processing Systems, pages 1471-1479, 2016.</p>
<p>A laplacian framework for option discovery in reinforcement learning. M C Machado, M G Bellemare, M Bowling, International Conference on Machine Learning. M. C. Machado, M. G. Bellemare, and M. Bowling. A laplacian framework for option discovery in reinforcement learning. In International Conference on Machine Learning, 2017.</p>
<p>Intrinsic motivation and reinforcement learning. A G Barto, Intrinsically motivated learning in natural and artificial systems. SpringerA. G. Barto. Intrinsic motivation and reinforcement learning. In Intrinsically motivated learning in natural and artificial systems, pages 17-47. Springer, 2013.</p>
<p>Curiosity-driven exploration by selfsupervised prediction. D Pathak, P Agrawal, A A Efros, T Darrell, arXiv:1705.05363arXiv preprintD. Pathak, P. Agrawal, A. A. Efros, and T. Darrell. Curiosity-driven exploration by self- supervised prediction. arXiv preprint arXiv:1705.05363, 2017.</p>
<p>R Houthooft, X Chen, Y Duan, J Schulman, F De Turck, P Abbeel, arXiv:1605.09674Curiosity-driven exploration in deep reinforcement learning via bayesian neural networks. arXiv preprintR. Houthooft, X. Chen, Y. Duan, J. Schulman, F. De Turck, and P. Abbeel. Curiosity-driven exploration in deep reinforcement learning via bayesian neural networks. arXiv preprint arXiv:1605.09674, 2016.</p>
<h1>exploration: A study of count-based exploration for deep reinforcement learning. H Tang, R Houthooft, D Foote, A Stooke, X Chen, Y Duan, J Schulman, F De Turck, P Abbeel, arXiv:1611.04717arXiv preprintH. Tang, R. Houthooft, D. Foote, A. Stooke, X. Chen, Y. Duan, J. Schulman, F. De Turck, and P. Abbeel. # exploration: A study of count-based exploration for deep reinforcement learning. arXiv preprint arXiv:1611.04717, 2016.</h1>
<p>Intrinsically motivated goal exploration processes with automatic curriculum learning. S Forestier, Y Mollard, P.-Y Oudeyer, arXiv:1708.02190arXiv preprintS. Forestier, Y. Mollard, and P.-Y. Oudeyer. Intrinsically motivated goal exploration processes with automatic curriculum learning. arXiv preprint arXiv:1708.02190, 2017.</p>
<p>Goal babbling permits direct learning of inverse kinematics. M Rolf, J J Steil, M Gienger, 10.1109/TAMD.2010.2062511IEEE Transactions on Autonomous Mental Development. 23M. Rolf, J. J. Steil, and M. Gienger. Goal babbling permits direct learning of inverse kinematics. IEEE Transactions on Autonomous Mental Development, 2(3):216-229, 2010. ISSN 19430604. doi:10.1109/TAMD.2010.2062511.</p>
<p>Socially guided intrinsic motivation for robot learning of motor skills. S M Nguyen, P.-Y Oudeyer, Autonomous Robots. 363S. M. Nguyen and P.-Y. Oudeyer. Socially guided intrinsic motivation for robot learning of motor skills. Autonomous Robots, 36(3):273-294, 2014.</p>
<p>Hindsight Experience Replay. M Andrychowicz, F Wolski, A Ray, J Schneider, R Fong, P Welinder, B Mcgrew, J Tobin, P Abbeel, W Zaremba, Nips. M. Andrychowicz, F. Wolski, A. Ray, J. Schneider, R. Fong, P. Welinder, B. McGrew, J. Tobin, P. Abbeel, and W. Zaremba. Hindsight Experience Replay. In Nips, jul 2017. URL http: //arxiv.org/abs/1707.01495.</p>
<p>Reverse curriculum generation for reinforcement learning. C Florensa, D Held, M Wulfmeier, P Abbeel, arXiv:1707.05300arXiv preprintC. Florensa, D. Held, M. Wulfmeier, and P. Abbeel. Reverse curriculum generation for reinforcement learning. arXiv preprint arXiv:1707.05300, 2017.</p>
<p>Powerplay: Training an increasingly general problem solver by continually searching for the simplest still unsolvable problem. J Schmidhuber, Frontiers in psychology. 4313J. Schmidhuber. Powerplay: Training an increasingly general problem solver by continually searching for the simplest still unsolvable problem. Frontiers in psychology, 4:313, 2013.</p>
<p>Unsupervised Learning of Goal Spaces for Intrinsically Motivated Goal Exploration. A Péré, S Forestier, O Sigaud, P.-Y Oudeyer, ICLR. A. Péré, S. Forestier, O. Sigaud, and P.-Y. Oudeyer. Unsupervised Learning of Goal Spaces for Intrinsically Motivated Goal Exploration. In ICLR, pages 1-26, 2018. URL http://arxiv. org/abs/1803.00781.</p>
<p>I Higgins, L Matthey, X Glorot, A Pal, B Uria, C Blundell, S Mohamed, A Lerchner, arXiv:1606.05579Early Visual Concept Learning with Unsupervised Deep Learning. arXiv preprintI. Higgins, L. Matthey, X. Glorot, A. Pal, B. Uria, C. Blundell, S. Mohamed, and A. Ler- chner. Early Visual Concept Learning with Unsupervised Deep Learning. arXiv preprint arXiv:1606.05579, jun 2016. URL http://arxiv.org/abs/1606.05579.</p>
<p>X Chen, Y Duan, R Houthooft, J Schulman, I Sutskever, P Abbeel, arXiv:1606.03657InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets. arXiv preprintX. Chen, Y. Duan, R. Houthooft, J. Schulman, I. Sutskever, and P. Abbeel. InfoGAN: Inter- pretable Representation Learning by Information Maximizing Generative Adversarial Nets. arXiv preprint arXiv:1606.03657, 2016.</p>
<p>Variation network: Learning high-level attributes for controlled input manipulation. Open review. Anonymous, Anonymous. Variation network: Learning high-level attributes for controlled input manipulation. Open review, 2017. URL https://openreview.net/forum?id=ryfaViR9YX.</p>
<p>Disentangling the independently controllable factors of variation by interacting with the world. V Thomas, E Bengio, W Fedus, J Pondard, P Beaudoin, H Larochelle, J Pineau, D Precup, Y Bengio, arXiv:1708.01289arXiv preprintV. Thomas, E. Bengio, W. Fedus, J. Pondard, P. Beaudoin, H. Larochelle, J. Pineau, D. Precup, and Y. Bengio. Disentangling the independently controllable factors of variation by interacting with the world. arXiv preprint arXiv:1708.01289, 2017. URL http://acsweb.ucsd.edu/ {~}wfedus/pdf/ICF{<em>}NIPS{</em>}2017{_}workshop.pdf.</p>
<p>Intrinsic motivation systems for autonomous mental development. P Y Oudeyer, F Kaplan, V V Hafner, 10.1109/TEVC.2006.890271IEEE Transactions on Evolutionary Computation. 112P. Y. Oudeyer, F. Kaplan, and V. V. Hafner. Intrinsic motivation systems for autonomous mental development. IEEE Transactions on Evolutionary Computation, 11(2):265-286, apr 2007. ISSN 1089778X. doi:10.1109/TEVC.2006.890271.</p>
<p>Representation learning: A review and new perspectives. Y Bengio, A Courville, P Vincent, 10.1109/TPAMI.2013.50IEEE Transactions on Pattern Analysis and Machine Intelligence. 358Y. Bengio, A. Courville, and P. Vincent. Representation learning: A review and new perspectives. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(8):1798-1828, 2013. ISSN 01628828. doi:10.1109/TPAMI.2013.50.</p>
<p>beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework. I Higgins, L Matthey, A Pal, C Burgess, X Glorot, M Botvinick, S Mohamed, A Lerchner, I. Higgins, L. Matthey, A. Pal, C. Burgess, X. Glorot, M. Botvinick, S. Mohamed, and A. Lerch- ner. beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework. In ICLR, 2017. URL https://openreview.net/forum?id=Sy2fzU9gl.</p>
<p>Modular active curiosity-driven discovery of tool use. S Forestier, P Y Oudeyer, 10.1109/IROS.2016.7759584IEEE International Conference on Intelligent Robots and Systems. S. Forestier and P. Y. Oudeyer. Modular active curiosity-driven discovery of tool use. IEEE International Conference on Intelligent Robots and Systems, 2016-Novem:3965-3972, 2016. ISSN 21530866. doi:10.1109/IROS.2016.7759584.</p>
<p>Behavioral Diversity Generation in Autonomous Exploration through Reuse of Past Experience. F C Y Benureau, P.-Y Oudeyer, 10.3389/frobt.2016.00008Frontiers in Robotics and AI. 3F. C. Y. Benureau and P.-Y. Oudeyer. Behavioral Diversity Generation in Autonomous Explo- ration through Reuse of Past Experience. Frontiers in Robotics and AI, 3(March), 2016. ISSN 2296-9144. doi:10.3389/frobt.2016.00008.</p>
<p>Zero-Shot Visual Imitation. D Pathak, P Mahmoudieh, G Luo, P Agrawal, D Chen, Y Shentu, E Shelhamer, J Malik, A A Efros, T Darrell, ICLR. D. Pathak, P. Mahmoudieh, G. Luo, P. Agrawal, D. Chen, Y. Shentu, E. Shelhamer, J. Malik, A. A. Efros, and T. Darrell. Zero-Shot Visual Imitation. In ICLR, pages 1-12, 2018. URL http://arxiv.org/abs/1804.08606.</p>
<p>Understanding disentangling in β-VAE. C P Burgess, I Higgins, A Pal, L Matthey, N Watters, G Desjardins, A Lerchner, Nips. C. P. Burgess, I. Higgins, A. Pal, L. Matthey, N. Watters, G. Desjardins, and A. Lerchner. Understanding disentangling in β-VAE. In Nips, 2017.</p>
<p>Adam: a Method for Stochastic Optimization. D P Kingma, J L Ba, International Conference on Learning Representations. D. P. Kingma and J. L. Ba. Adam: a Method for Stochastic Optimization. International Conference on Learning Representations, 2015.</p>
<p>Random Goal Exploration with an entangled representation (VAE) as a goal space. RGE-VAERandom Goal Exploration with an entangled representation (VAE) as a goal space (RGE-VAE)</p>
<p>Modular Goal Exploration with an entangled representation (VAE) as a goal space. MGE-VAEModular Goal Exploration with an entangled representation (VAE) as a goal space (MGE-VAE)</p>
<p>Random Goal Exploration with a disentangled representation (βVAE) as a goal space. RGE-βVAERandom Goal Exploration with a disentangled representation (βVAE) as a goal space (RGE-βVAE)</p>
<p>Modular Goal Exploration with a disentangled representation (βVAE) as a goal space. MGE-βVAEModular Goal Exploration with a disentangled representation (βVAE) as a goal space (MGE-βVAE)</p>
<p>Examples of achieved observations together with the ratio of covered cells in the Arm-2-Balls environment for MGE and RGE exploration algorithms using learned goal spaces (VAE and βVAE). The number of times the ball was effectively handled is also represented. Figure. 13Figure 13: Examples of achieved observations together with the ratio of covered cells in the Arm-2- Balls environment for MGE and RGE exploration algorithms using learned goal spaces (VAE and βVAE). The number of times the ball was effectively handled is also represented.</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-368 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-368</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-368</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-15.html">extraction-schema-15</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <p><strong>Paper ID:</strong> paper-270702560</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2406.16294v1.pdf" target="_blank">LangSuit · E: Planning, Controlling and Interacting with Large Language Models in Embodied Text Environments</a></p>
                <p><strong>Paper Abstract:</strong> Recent advances in Large Language Models (LLMs) have shown inspiring achievements in constructing autonomous agents that rely on language descriptions as inputs. However, it remains unclear how well LLMs can function as few-shot or zero-shot embodied agents in dynamic interactive environments. To address this gap, we introduce LangSuit · E, a versatile and simulation-free testbed featuring 6 representative embodied tasks in textual embodied worlds. Compared with previous LLM-based testbeds, LangSuit · E (i) offers adaptability to diverse environments without multiple simulation engines, (ii) evaluates agents’ capacity to develop “internalized world knowledge” with embodied observations, and (iii) allows easy customization of communication and action strategies. To address the embodiment challenge, we devise a novel chain-of-thought (CoT) schema, EmMem , which summarizes embodied states w.r.t. history information. Comprehensive benchmark results illustrate challenges and insights of embodied planning. LangSuit · E represents a significant step toward building embodied gener-alists in the context of language models.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e368.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e368.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EmMem</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Embodied Memory (EmMem)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A chain-of-thought prompting schema introduced in this paper that forces the LLM to explicitly summarize and predict its embodied state (position/orientation/inventory/action-history/subgoals) in natural language before planning the next action, yielding an emergent in-context embodied memory used for planning in partially-observable textual worlds.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Chat-based transformer API model (gpt-3.5-turbo) used as the planner/controller. Runs with chain-of-thought style prompts (EmMem combined with ReAct or Reflexion) and varying temperature settings; no architectural modifications reported.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>LangSuit•E (multiple tasks: Instruction Grounding, Household, Rearrangement, Interactive QA, MA-Household, MA-WAH)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>A simulation-free textual embodied environment where agents receive only partial textual observations (what is in front/left/right within configurable view distance) and must execute sequences of low-level or high-level actions (move, turn, pick_up, put, open, etc.) to complete multi-step household, navigation and manipulation tasks under partial observability.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>navigation + object manipulation + multi-step household planning (instruction following, multi-agent cooperation)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial+procedural+object-relational</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pre-training on large text corpora (model weights) + in-context examples and the sequence of textual embodied observations provided online (prompted summaries)</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>chain-of-thought style prompting / in-context summarization (EmMem) combined with ReAct or Reflexion prompting; few-shot examples included in some prompts</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>explicit natural-language embodied-state summaries stored in the prompt/history (agent position/orientation, inventory, observed objects, action history, subgoals) — an auto-regressive textual memory that the model conditions on for subsequent planning (analogy drawn to grid-cell integration)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success Rate (SR), Goal-Conditioned SR (Goal-SR), Misplaced, Fixed Strict, Accuracy (for IQA), Path-weighted SR (path efficiency)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>EmMem combined with existing CoT strategies produced consistent improvements in many settings: e.g., small improvement in Instruction Grounding high-level SR (ReAct -> ReAct+EmMem: ~84% -> ~85%); Household path-weighted SR increased (Reflexion -> Reflexion+EmMem: path-weighted SR from 28.23% -> 34.45% reported in paper); multi-agent MA-WAH (Reflexion+EmMem) reached SR ~64% and Goal-SR ~90.37% (Table 4). Household high-level Reflexion-EmMem SR ≈ 40% (Table 5). Improvements and exact magnitudes depend on task and action granularity (low-level vs high-level).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Improved retention and use of action history and self-state (position/orientation/inventory) across turns; better low-level procedural control when EmMem explicitly summarized past observations; improved multi-step decomposition and subgoal tracking; helped with planning when perceptual input is limited to local textual observations.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Does not close the gap to supervised fine-tuned (SFT) experts; CoT alone can increase hallucinations (noted drop in IQA performance in low-level ReAct setting); EmMem sometimes increases number of steps (longer plans) and can still fail on fine-grained manipulation or when detailed spatial precision is required; small open-source models (e.g., LLaMA-7B) failed regardless of prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to baseline prompting strategies: Act-only (~baseline), ReAct, Reflexion; Reflexion typically outperforms ReAct and Act, and EmMem variants (ReAct+EmMem, Reflexion+EmMem) further improve or stabilize performance. Example high-level numbers: Act IG SR ~77%, ReAct ~84%, ReAct+EmMem ~85%, Reflexion ~86%, Reflexion+EmMem ~86% (Table 3, high-level).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Ablating EmMem (i.e., ReAct vs ReAct+EmMem or Reflexion vs Reflexion+EmMem) generally reduces success rates or goal-conditioned success; paper shows EmMem is more beneficial in low-level action settings (larger relative gains) than in high-level actions. Example: Household path-weighted SR: Reflexion 28.23% -> Reflexion+EmMem 34.45% (path-weighted metric; Table 7).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Explicitly asking the LLM to produce and maintain a natural-language embodied memory (EmMem) helps the model encode and utilize spatial, procedural and object-relational knowledge when operating without direct sensory input: it increases self-awareness about position/orientation/inventory and action history, improves multi-step planning and low-level control, mitigates long-context 'lost-in-middle' issues by emphasizing middle context, and yields consistent gains when combined with multi-trial/reflexion strategies; however, CoT can increase hallucinations for certain perceptual QA tasks and open-source small models remain ineffective without further training.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LangSuit · E: Planning, Controlling and Interacting with Large Language Models in Embodied Text Environments', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e368.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e368.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LangSuit•E</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LangSuit•E testbed</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simulation-free textual embodied-world benchmark introduced in this paper that exposes agents to partial textual observations and supports six representative embodied tasks (Instruction Grounding, Rearrangement, Interactive QA, Household, Multi-agent Teach, Multi-agent Watch-and-Help) to evaluate LLMs as embodied agents without visual perceptual noise.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs (evaluated primarily with gpt-3.5-turbo; GPT-4 mentioned as used in prior work but experiments here focus on gpt-3.5-turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LangSuit•E is not a model but a textual environment: it supplies configurable, partial natural-language observations, feedback on actions, and supports custom action spaces (low-level and high-level), multi-agent communication and human-in-the-loop interactions; it also includes an expert module to generate gold trajectories using A* for path planning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>LangSuit•E benchmark suite (6 embodied tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Unified textual embodied environment where agents receive only local textual observations (what is visible in front/left/right within view distance) and must execute sequences of actions under physical constraints to accomplish tasks like moving objects, answering environment-dependent questions, and multi-agent cooperation; supports customization of view/manipulate distances and action sets.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>navigation, object manipulation, instruction following, multi-agent cooperation, interactive QA</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial+procedural+object-relational</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>textual descriptions generated by the environment + (optional) gold expert trajectories produced by an internal expert module; models rely on pre-trained knowledge plus in-context prompts</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>prompting of LLMs with task instructions, examples, and step-wise textual observations; optionally asking for human help via ask actions</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>environmental state and percepts represented as natural-language observations (object lists, relative location info like 'on your left/ in front'), optional expert trajectories (sequences of actions), and agent-maintained textual memory (EmMem summaries) in prompt history</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success Rate, Goal-SR, Misplaced, Fixed Strict, Accuracy (IQA), Path-weighted SR, average steps</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Benchmark results show wide variability across tasks and prompting strategies; high-level actions are considerably easier (higher SRs) than low-level action settings; example ranges reported: Instruction Grounding SRs ~77–86% (high-level, depending on strategy), Household SRs ~10–40% depending on strategy and metric, multi-agent MA-WAH SRs up to ~64% with Reflexion+EmMem (Table 3–4).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Text-only partial observations suffice for LLMs to perform many high-level planning decisions when supplied with consistent history and prompts; agents can use pre-trained commonsense and the textual observation stream to find objects and follow instructions at a coarse level.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Performance drops substantially in fine-grained low-level action settings and in IQA/counting/spatial-relationship questions when CoT increases hallucinations; small open-source LLMs failed without fine-tuning; long action sequences and missing global context remain challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared approaches include Act-only, ReAct, Reflexion, and their EmMem-augmented variants; Reflexion+EmMem often achieves the best or competitive performance across tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>The paper shows comparisons between strategies with and without EmMem (and with/without multi-trial Reflexion); removing EmMem reduces performance especially in low-level action experiments and path-weighted metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A textual, simulation-free embodied testbed can reveal how LLMs form internal world models and plan without direct sensory input; EmMem-like prompting to maintain an explicit textual embodied memory is particularly beneficial for low-level control and multi-agent coordination under partial observability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LangSuit · E: Planning, Controlling and Interacting with Large Language Models in Embodied Text Environments', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e368.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e368.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>gpt-3.5-turbo (as evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>gpt-3.5-turbo (OpenAI chat model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A chat-optimized transformer LLM accessed via API and used as the primary planner/controller in experiments; evaluated under various prompting strategies (Act, ReAct, ReAct+EmMem, Reflexion, Reflexion+EmMem) and temperature settings on LangSuit•E tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>API-accessed chat transformer; used off-the-shelf (no fine-tuning) with chain-of-thought style prompts and in-context examples, temperature varied (0.5, 1.0, 1.5) in some experiments; no direct sensory modules attached (operates on textual descriptions only).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>LangSuit•E tasks (Instruction Grounding, Household, Rearrangement, IQA, MA-Household, MA-WAH)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>See LangSuit•E description: partially-observable text-based indoor scenarios requiring navigation, object manipulation and multi-step planning from textual inputs only.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>navigation, object manipulation, multi-step instruction following, multi-agent cooperation</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial+procedural+object-relational</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>implicit knowledge from pre-training on text corpora; task knowledge elicited via in-context examples and step-wise textual observations from LangSuit•E</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>zero-shot / few-shot prompting, chain-of-thought prompting variants (ReAct, Reflexion) and EmMem in-context memory summarization</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>implicit knowledge in model weights plus explicit natural-language state/memory placed into prompt; actions are produced as textual tokens representing discrete actions</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>SR, Goal-SR, Misplaced, Fixed Strict, Accuracy (IQA), Path-weighted SR, Avg Steps</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Varied by task and prompt: high-level actions yielded much higher SRs (Instruction Grounding SRs ~77–86% across strategies); low-level actions had far lower SRs (single-digit to low-twenties percent range in many tasks); Household SR with Reflexion+EmMem around ~40% (Table 5); multi-agent tasks achieved higher Goal-SR with EmMem augmentations (see Table 4). Exact per-strategy numbers reported in Tables 3–7 of the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Good at high-level planning, leveraging commonsense and step decomposition when given textual context and memory; multi-agent coordination via chat actions performs well because the environment occasionally supplies self-state feedback to agents.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Struggles in fully low-level, fine-grained manipulation and long-step navigation without global context; chain-of-thought can worsen hallucination for some QA-style tasks; small open-source models performed poorly.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to prompting baselines Act, ReAct, Reflexion; Reflexion (multi-trial, self-reflection) often improved performance over ReAct and Act; EmMem further improves or stabilizes performance in many cases.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Paper shows results across prompting ablations (with/without EmMem, with/without Reflexion) demonstrating EmMem's positive effect and trade-offs (more steps but higher SR); small LLMs without SFT fail even with prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A strong chat LLM like gpt-3.5-turbo can leverage pre-trained text knowledge and carefully structured prompts (EmMem + CoT strategies) to encode and use spatial, procedural and object-relational knowledge in purely textual embodied tasks, but performance depends strongly on action granularity, prompt design, and model capacity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LangSuit · E: Planning, Controlling and Interacting with Large Language Models in Embodied Text Environments', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e368.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e368.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReAct (Reason+Act)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A chain-of-thought method that interleaves natural-language reasoning steps (Thought) with environment actions (Act) to improve decision-making; used here as one of the prompting baselines and combined with EmMem.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>ReAct is applied by instructing the model to produce alternating reasoning traces and discrete actions; used with and without EmMem in LangSuit•E evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>LangSuit•E tasks (multiple)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same LangSuit•E tasks; ReAct supplies intermediate reasoning before each action to help plan under partial observability.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>navigation, manipulation, instruction following</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial+procedural+object-relational</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pre-training + in-context reasoning traces and immediate textual observations</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>chain-of-thought prompting with interleaved 'Thought:' and 'Act:' tokens</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>reasoning text tokens and the action token sequence; no explicit symbolic map beyond the natural-language trace</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>SR, Goal-SR, Avg Steps, Accuracy (IQA)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>ReAct often improved over Act-only in some high-level settings (e.g., Instruction Grounding high-level SR ~84% vs Act ~77%), but in some low-level IQA settings ReAct led to worse accuracy due to hallucination (paper notes a drop in IQA in low-level ReAct).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Helps break down plans into substeps and produces interpretable reasoning aligned with actions; useful in high-level planning.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Can increase hallucinations for environment-question tasks (IQA) under low-level action settings and long-horizon partial observability without explicit memory summarization.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared with Act-only and Reflexion; ReAct typically between Act and Reflexion in performance, and benefits from EmMem augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>ReAct+EmMem vs ReAct shows modest gains, especially for low-level action tasks where explicit memory helps mitigate lost-in-middle issues.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Interleaving reasoning and acting helps LLMs plan, but without explicit memory (EmMem) and multi-trial reflection (Reflexion) it can lead to hallucinations and struggles on precise perceptual QA.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LangSuit · E: Planning, Controlling and Interacting with Large Language Models in Embodied Text Environments', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e368.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e368.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflexion (multi-trial self-reflection)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting strategy where the agent performs trials and, upon failure, summarizes the last run and re-plans using that summary (one additional trial in the paper's experiments), used here with and without EmMem to improve stability and success rate.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Reflexion asks the model to reflect on failed trials, summarize mistakes, and re-plan; in LangSuit•E used typically with one additional trial and optionally augmented with EmMem summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>LangSuit•E tasks (multiple)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Used to attempt tasks repeatedly, leveraging failure summaries as additional context for re-planning under partial observability.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>navigation, manipulation, instruction following, multi-step planning</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial+procedural+object-relational</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pre-training + trial summaries (self-generated) + textual observations</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>multi-trial prompting with explicit failure summarization and re-planning</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>natural-language failure summaries and revised plan traces appended to prompt context as additional memory</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>SR, Goal-SR, Path-weighted SR, Avg Steps</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Reflexion consistently improved success rates over Act and ReAct in many tasks; e.g., Reflexion high-level Instruction Grounding SR ~86% and Reflexion+EmMem further improved or stabilized performance; Reflexion-EmMem produced notable gains on household path-weighted metrics (see Table 7), and multi-agent tasks saw high Goal-SR improvements when combined with EmMem (MA-WAH Goal-SR ~90.37%).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Better recognition and correction of prior mistakes, more stable planning across trials, improved success on tasks requiring multi-step correction.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Reflexion increases number of steps and computational cost; still insufficient to match SFT expert performance; occasional over-exploration causing step limits to be exceeded in low-level settings.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Outperforms Act and ReAct in many benchmarks; best results often achieved by Reflexion+EmMem.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Adding EmMem to Reflexion further improves success on many tasks, demonstrating complementary benefits between trial-based reflection and explicit embodied memory.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Multi-trial self-reflection helps LLM agents detect and correct planning mistakes; combining reflection with explicit embodied memory (EmMem) produces synergistic gains in partially-observable textual embodied tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LangSuit · E: Planning, Controlling and Interacting with Large Language Models in Embodied Text Environments', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>ReAct: Synergizing Reasoning and Acting in Language Models <em>(Rating: 2)</em></li>
                <li>Reflexion: An Agent That Learns from Verbal Self-Reflection <em>(Rating: 2)</em></li>
                <li>ALFWorld: Aligning Text and Embodied Environments for Language-Conditioned Action <em>(Rating: 2)</em></li>
                <li>TextWorld: A Learning Environment for Text-based Games <em>(Rating: 1)</em></li>
                <li>Vector-based navigation using grid-like representations in artificial agents <em>(Rating: 2)</em></li>
                <li>PaLM-E: An Embodied Multimodal Language Model <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-368",
    "paper_id": "paper-270702560",
    "extraction_schema_id": "extraction-schema-15",
    "extracted_data": [
        {
            "name_short": "EmMem",
            "name_full": "Embodied Memory (EmMem)",
            "brief_description": "A chain-of-thought prompting schema introduced in this paper that forces the LLM to explicitly summarize and predict its embodied state (position/orientation/inventory/action-history/subgoals) in natural language before planning the next action, yielding an emergent in-context embodied memory used for planning in partially-observable textual worlds.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo",
            "model_size": null,
            "model_description": "Chat-based transformer API model (gpt-3.5-turbo) used as the planner/controller. Runs with chain-of-thought style prompts (EmMem combined with ReAct or Reflexion) and varying temperature settings; no architectural modifications reported.",
            "task_name": "LangSuit•E (multiple tasks: Instruction Grounding, Household, Rearrangement, Interactive QA, MA-Household, MA-WAH)",
            "task_description": "A simulation-free textual embodied environment where agents receive only partial textual observations (what is in front/left/right within configurable view distance) and must execute sequences of low-level or high-level actions (move, turn, pick_up, put, open, etc.) to complete multi-step household, navigation and manipulation tasks under partial observability.",
            "task_type": "navigation + object manipulation + multi-step household planning (instruction following, multi-agent cooperation)",
            "knowledge_type": "spatial+procedural+object-relational",
            "knowledge_source": "pre-training on large text corpora (model weights) + in-context examples and the sequence of textual embodied observations provided online (prompted summaries)",
            "has_direct_sensory_input": false,
            "elicitation_method": "chain-of-thought style prompting / in-context summarization (EmMem) combined with ReAct or Reflexion prompting; few-shot examples included in some prompts",
            "knowledge_representation": "explicit natural-language embodied-state summaries stored in the prompt/history (agent position/orientation, inventory, observed objects, action history, subgoals) — an auto-regressive textual memory that the model conditions on for subsequent planning (analogy drawn to grid-cell integration)",
            "performance_metric": "Success Rate (SR), Goal-Conditioned SR (Goal-SR), Misplaced, Fixed Strict, Accuracy (for IQA), Path-weighted SR (path efficiency)",
            "performance_result": "EmMem combined with existing CoT strategies produced consistent improvements in many settings: e.g., small improvement in Instruction Grounding high-level SR (ReAct -&gt; ReAct+EmMem: ~84% -&gt; ~85%); Household path-weighted SR increased (Reflexion -&gt; Reflexion+EmMem: path-weighted SR from 28.23% -&gt; 34.45% reported in paper); multi-agent MA-WAH (Reflexion+EmMem) reached SR ~64% and Goal-SR ~90.37% (Table 4). Household high-level Reflexion-EmMem SR ≈ 40% (Table 5). Improvements and exact magnitudes depend on task and action granularity (low-level vs high-level).",
            "success_patterns": "Improved retention and use of action history and self-state (position/orientation/inventory) across turns; better low-level procedural control when EmMem explicitly summarized past observations; improved multi-step decomposition and subgoal tracking; helped with planning when perceptual input is limited to local textual observations.",
            "failure_patterns": "Does not close the gap to supervised fine-tuned (SFT) experts; CoT alone can increase hallucinations (noted drop in IQA performance in low-level ReAct setting); EmMem sometimes increases number of steps (longer plans) and can still fail on fine-grained manipulation or when detailed spatial precision is required; small open-source models (e.g., LLaMA-7B) failed regardless of prompting.",
            "baseline_comparison": "Compared to baseline prompting strategies: Act-only (~baseline), ReAct, Reflexion; Reflexion typically outperforms ReAct and Act, and EmMem variants (ReAct+EmMem, Reflexion+EmMem) further improve or stabilize performance. Example high-level numbers: Act IG SR ~77%, ReAct ~84%, ReAct+EmMem ~85%, Reflexion ~86%, Reflexion+EmMem ~86% (Table 3, high-level).",
            "ablation_results": "Ablating EmMem (i.e., ReAct vs ReAct+EmMem or Reflexion vs Reflexion+EmMem) generally reduces success rates or goal-conditioned success; paper shows EmMem is more beneficial in low-level action settings (larger relative gains) than in high-level actions. Example: Household path-weighted SR: Reflexion 28.23% -&gt; Reflexion+EmMem 34.45% (path-weighted metric; Table 7).",
            "key_findings": "Explicitly asking the LLM to produce and maintain a natural-language embodied memory (EmMem) helps the model encode and utilize spatial, procedural and object-relational knowledge when operating without direct sensory input: it increases self-awareness about position/orientation/inventory and action history, improves multi-step planning and low-level control, mitigates long-context 'lost-in-middle' issues by emphasizing middle context, and yields consistent gains when combined with multi-trial/reflexion strategies; however, CoT can increase hallucinations for certain perceptual QA tasks and open-source small models remain ineffective without further training.",
            "uuid": "e368.0",
            "source_info": {
                "paper_title": "LangSuit · E: Planning, Controlling and Interacting with Large Language Models in Embodied Text Environments",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "LangSuit•E",
            "name_full": "LangSuit•E testbed",
            "brief_description": "A simulation-free textual embodied-world benchmark introduced in this paper that exposes agents to partial textual observations and supports six representative embodied tasks (Instruction Grounding, Rearrangement, Interactive QA, Household, Multi-agent Teach, Multi-agent Watch-and-Help) to evaluate LLMs as embodied agents without visual perceptual noise.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLMs (evaluated primarily with gpt-3.5-turbo; GPT-4 mentioned as used in prior work but experiments here focus on gpt-3.5-turbo)",
            "model_size": null,
            "model_description": "LangSuit•E is not a model but a textual environment: it supplies configurable, partial natural-language observations, feedback on actions, and supports custom action spaces (low-level and high-level), multi-agent communication and human-in-the-loop interactions; it also includes an expert module to generate gold trajectories using A* for path planning.",
            "task_name": "LangSuit•E benchmark suite (6 embodied tasks)",
            "task_description": "Unified textual embodied environment where agents receive only local textual observations (what is visible in front/left/right within view distance) and must execute sequences of actions under physical constraints to accomplish tasks like moving objects, answering environment-dependent questions, and multi-agent cooperation; supports customization of view/manipulate distances and action sets.",
            "task_type": "navigation, object manipulation, instruction following, multi-agent cooperation, interactive QA",
            "knowledge_type": "spatial+procedural+object-relational",
            "knowledge_source": "textual descriptions generated by the environment + (optional) gold expert trajectories produced by an internal expert module; models rely on pre-trained knowledge plus in-context prompts",
            "has_direct_sensory_input": false,
            "elicitation_method": "prompting of LLMs with task instructions, examples, and step-wise textual observations; optionally asking for human help via ask actions",
            "knowledge_representation": "environmental state and percepts represented as natural-language observations (object lists, relative location info like 'on your left/ in front'), optional expert trajectories (sequences of actions), and agent-maintained textual memory (EmMem summaries) in prompt history",
            "performance_metric": "Success Rate, Goal-SR, Misplaced, Fixed Strict, Accuracy (IQA), Path-weighted SR, average steps",
            "performance_result": "Benchmark results show wide variability across tasks and prompting strategies; high-level actions are considerably easier (higher SRs) than low-level action settings; example ranges reported: Instruction Grounding SRs ~77–86% (high-level, depending on strategy), Household SRs ~10–40% depending on strategy and metric, multi-agent MA-WAH SRs up to ~64% with Reflexion+EmMem (Table 3–4).",
            "success_patterns": "Text-only partial observations suffice for LLMs to perform many high-level planning decisions when supplied with consistent history and prompts; agents can use pre-trained commonsense and the textual observation stream to find objects and follow instructions at a coarse level.",
            "failure_patterns": "Performance drops substantially in fine-grained low-level action settings and in IQA/counting/spatial-relationship questions when CoT increases hallucinations; small open-source LLMs failed without fine-tuning; long action sequences and missing global context remain challenging.",
            "baseline_comparison": "Compared approaches include Act-only, ReAct, Reflexion, and their EmMem-augmented variants; Reflexion+EmMem often achieves the best or competitive performance across tasks.",
            "ablation_results": "The paper shows comparisons between strategies with and without EmMem (and with/without multi-trial Reflexion); removing EmMem reduces performance especially in low-level action experiments and path-weighted metrics.",
            "key_findings": "A textual, simulation-free embodied testbed can reveal how LLMs form internal world models and plan without direct sensory input; EmMem-like prompting to maintain an explicit textual embodied memory is particularly beneficial for low-level control and multi-agent coordination under partial observability.",
            "uuid": "e368.1",
            "source_info": {
                "paper_title": "LangSuit · E: Planning, Controlling and Interacting with Large Language Models in Embodied Text Environments",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "gpt-3.5-turbo (as evaluated)",
            "name_full": "gpt-3.5-turbo (OpenAI chat model)",
            "brief_description": "A chat-optimized transformer LLM accessed via API and used as the primary planner/controller in experiments; evaluated under various prompting strategies (Act, ReAct, ReAct+EmMem, Reflexion, Reflexion+EmMem) and temperature settings on LangSuit•E tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo",
            "model_size": null,
            "model_description": "API-accessed chat transformer; used off-the-shelf (no fine-tuning) with chain-of-thought style prompts and in-context examples, temperature varied (0.5, 1.0, 1.5) in some experiments; no direct sensory modules attached (operates on textual descriptions only).",
            "task_name": "LangSuit•E tasks (Instruction Grounding, Household, Rearrangement, IQA, MA-Household, MA-WAH)",
            "task_description": "See LangSuit•E description: partially-observable text-based indoor scenarios requiring navigation, object manipulation and multi-step planning from textual inputs only.",
            "task_type": "navigation, object manipulation, multi-step instruction following, multi-agent cooperation",
            "knowledge_type": "spatial+procedural+object-relational",
            "knowledge_source": "implicit knowledge from pre-training on text corpora; task knowledge elicited via in-context examples and step-wise textual observations from LangSuit•E",
            "has_direct_sensory_input": false,
            "elicitation_method": "zero-shot / few-shot prompting, chain-of-thought prompting variants (ReAct, Reflexion) and EmMem in-context memory summarization",
            "knowledge_representation": "implicit knowledge in model weights plus explicit natural-language state/memory placed into prompt; actions are produced as textual tokens representing discrete actions",
            "performance_metric": "SR, Goal-SR, Misplaced, Fixed Strict, Accuracy (IQA), Path-weighted SR, Avg Steps",
            "performance_result": "Varied by task and prompt: high-level actions yielded much higher SRs (Instruction Grounding SRs ~77–86% across strategies); low-level actions had far lower SRs (single-digit to low-twenties percent range in many tasks); Household SR with Reflexion+EmMem around ~40% (Table 5); multi-agent tasks achieved higher Goal-SR with EmMem augmentations (see Table 4). Exact per-strategy numbers reported in Tables 3–7 of the paper.",
            "success_patterns": "Good at high-level planning, leveraging commonsense and step decomposition when given textual context and memory; multi-agent coordination via chat actions performs well because the environment occasionally supplies self-state feedback to agents.",
            "failure_patterns": "Struggles in fully low-level, fine-grained manipulation and long-step navigation without global context; chain-of-thought can worsen hallucination for some QA-style tasks; small open-source models performed poorly.",
            "baseline_comparison": "Compared to prompting baselines Act, ReAct, Reflexion; Reflexion (multi-trial, self-reflection) often improved performance over ReAct and Act; EmMem further improves or stabilizes performance in many cases.",
            "ablation_results": "Paper shows results across prompting ablations (with/without EmMem, with/without Reflexion) demonstrating EmMem's positive effect and trade-offs (more steps but higher SR); small LLMs without SFT fail even with prompting.",
            "key_findings": "A strong chat LLM like gpt-3.5-turbo can leverage pre-trained text knowledge and carefully structured prompts (EmMem + CoT strategies) to encode and use spatial, procedural and object-relational knowledge in purely textual embodied tasks, but performance depends strongly on action granularity, prompt design, and model capacity.",
            "uuid": "e368.2",
            "source_info": {
                "paper_title": "LangSuit · E: Planning, Controlling and Interacting with Large Language Models in Embodied Text Environments",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "ReAct",
            "name_full": "ReAct (Reason+Act)",
            "brief_description": "A chain-of-thought method that interleaves natural-language reasoning steps (Thought) with environment actions (Act) to improve decision-making; used here as one of the prompting baselines and combined with EmMem.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo",
            "model_size": null,
            "model_description": "ReAct is applied by instructing the model to produce alternating reasoning traces and discrete actions; used with and without EmMem in LangSuit•E evaluations.",
            "task_name": "LangSuit•E tasks (multiple)",
            "task_description": "Same LangSuit•E tasks; ReAct supplies intermediate reasoning before each action to help plan under partial observability.",
            "task_type": "navigation, manipulation, instruction following",
            "knowledge_type": "spatial+procedural+object-relational",
            "knowledge_source": "pre-training + in-context reasoning traces and immediate textual observations",
            "has_direct_sensory_input": false,
            "elicitation_method": "chain-of-thought prompting with interleaved 'Thought:' and 'Act:' tokens",
            "knowledge_representation": "reasoning text tokens and the action token sequence; no explicit symbolic map beyond the natural-language trace",
            "performance_metric": "SR, Goal-SR, Avg Steps, Accuracy (IQA)",
            "performance_result": "ReAct often improved over Act-only in some high-level settings (e.g., Instruction Grounding high-level SR ~84% vs Act ~77%), but in some low-level IQA settings ReAct led to worse accuracy due to hallucination (paper notes a drop in IQA in low-level ReAct).",
            "success_patterns": "Helps break down plans into substeps and produces interpretable reasoning aligned with actions; useful in high-level planning.",
            "failure_patterns": "Can increase hallucinations for environment-question tasks (IQA) under low-level action settings and long-horizon partial observability without explicit memory summarization.",
            "baseline_comparison": "Compared with Act-only and Reflexion; ReAct typically between Act and Reflexion in performance, and benefits from EmMem augmentation.",
            "ablation_results": "ReAct+EmMem vs ReAct shows modest gains, especially for low-level action tasks where explicit memory helps mitigate lost-in-middle issues.",
            "key_findings": "Interleaving reasoning and acting helps LLMs plan, but without explicit memory (EmMem) and multi-trial reflection (Reflexion) it can lead to hallucinations and struggles on precise perceptual QA.",
            "uuid": "e368.3",
            "source_info": {
                "paper_title": "LangSuit · E: Planning, Controlling and Interacting with Large Language Models in Embodied Text Environments",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Reflexion",
            "name_full": "Reflexion (multi-trial self-reflection)",
            "brief_description": "A prompting strategy where the agent performs trials and, upon failure, summarizes the last run and re-plans using that summary (one additional trial in the paper's experiments), used here with and without EmMem to improve stability and success rate.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo",
            "model_size": null,
            "model_description": "Reflexion asks the model to reflect on failed trials, summarize mistakes, and re-plan; in LangSuit•E used typically with one additional trial and optionally augmented with EmMem summaries.",
            "task_name": "LangSuit•E tasks (multiple)",
            "task_description": "Used to attempt tasks repeatedly, leveraging failure summaries as additional context for re-planning under partial observability.",
            "task_type": "navigation, manipulation, instruction following, multi-step planning",
            "knowledge_type": "spatial+procedural+object-relational",
            "knowledge_source": "pre-training + trial summaries (self-generated) + textual observations",
            "has_direct_sensory_input": false,
            "elicitation_method": "multi-trial prompting with explicit failure summarization and re-planning",
            "knowledge_representation": "natural-language failure summaries and revised plan traces appended to prompt context as additional memory",
            "performance_metric": "SR, Goal-SR, Path-weighted SR, Avg Steps",
            "performance_result": "Reflexion consistently improved success rates over Act and ReAct in many tasks; e.g., Reflexion high-level Instruction Grounding SR ~86% and Reflexion+EmMem further improved or stabilized performance; Reflexion-EmMem produced notable gains on household path-weighted metrics (see Table 7), and multi-agent tasks saw high Goal-SR improvements when combined with EmMem (MA-WAH Goal-SR ~90.37%).",
            "success_patterns": "Better recognition and correction of prior mistakes, more stable planning across trials, improved success on tasks requiring multi-step correction.",
            "failure_patterns": "Reflexion increases number of steps and computational cost; still insufficient to match SFT expert performance; occasional over-exploration causing step limits to be exceeded in low-level settings.",
            "baseline_comparison": "Outperforms Act and ReAct in many benchmarks; best results often achieved by Reflexion+EmMem.",
            "ablation_results": "Adding EmMem to Reflexion further improves success on many tasks, demonstrating complementary benefits between trial-based reflection and explicit embodied memory.",
            "key_findings": "Multi-trial self-reflection helps LLM agents detect and correct planning mistakes; combining reflection with explicit embodied memory (EmMem) produces synergistic gains in partially-observable textual embodied tasks.",
            "uuid": "e368.4",
            "source_info": {
                "paper_title": "LangSuit · E: Planning, Controlling and Interacting with Large Language Models in Embodied Text Environments",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "ReAct: Synergizing Reasoning and Acting in Language Models",
            "rating": 2,
            "sanitized_title": "react_synergizing_reasoning_and_acting_in_language_models"
        },
        {
            "paper_title": "Reflexion: An Agent That Learns from Verbal Self-Reflection",
            "rating": 2,
            "sanitized_title": "reflexion_an_agent_that_learns_from_verbal_selfreflection"
        },
        {
            "paper_title": "ALFWorld: Aligning Text and Embodied Environments for Language-Conditioned Action",
            "rating": 2,
            "sanitized_title": "alfworld_aligning_text_and_embodied_environments_for_languageconditioned_action"
        },
        {
            "paper_title": "TextWorld: A Learning Environment for Text-based Games",
            "rating": 1,
            "sanitized_title": "textworld_a_learning_environment_for_textbased_games"
        },
        {
            "paper_title": "Vector-based navigation using grid-like representations in artificial agents",
            "rating": 2,
            "sanitized_title": "vectorbased_navigation_using_gridlike_representations_in_artificial_agents"
        },
        {
            "paper_title": "PaLM-E: An Embodied Multimodal Language Model",
            "rating": 1,
            "sanitized_title": "palme_an_embodied_multimodal_language_model"
        }
    ],
    "cost": 0.020125999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>LangSuit•E: Planning, Controlling and Interacting with Large Language Models in Embodied Text Environments
24 Jun 2024</p>
<p>Zixia Jia 
Mengmeng Wang 
National Key Laboratory of General Artificial Intelligence
BIGAI</p>
<p>Baichen Tong 
National Key Laboratory of General Artificial Intelligence
BIGAI</p>
<p>Song-Chun Zhu 
National Key Laboratory of General Artificial Intelligence
BIGAI</p>
<p>Peking University</p>
<p>Tsinghua University</p>
<p>Zilong Zheng <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#122;&#108;&#122;&#104;&#101;&#110;&#103;&#64;&#98;&#105;&#103;&#97;&#105;&#46;&#97;&#105;">&#122;&#108;&#122;&#104;&#101;&#110;&#103;&#64;&#98;&#105;&#103;&#97;&#105;&#46;&#97;&#105;</a> 
National Key Laboratory of General Artificial Intelligence
BIGAI</p>
<p>Weize Chen 
Yusheng Su 
Jingwei Zuo 
Cheng Yang 
Chenfei Yuan 
Chen Qian 
Chi-Min Chan 
Yujia Qin 
Yaxi Lu 
Ruobing Xie 
Maxime Chevalier-Boisvert 
Dzmitry Bahdanau 
Salem Lahlou 
Lucas Willems 
Chitwan Saharia 
Huu Thien 
Yoshua Nguyen 
2019 Bengio 
Babyai 
Marc-Alexandre Côté 
Ákos Kádár 
Xingdi Yuan 
Ben Kybartas 
Tavian Barnes 
Emery Fine 
RuoJames Moore 
Yu Tao 
Matthew 2021 Hausknecht 
Layla El Asri 
Mahmoud Adada 
Wendy Tay 
Ishita Dasgupta 
Christine Kaeser-Chen 
Kenneth Marino 
Xiang Deng 
Yu Gu 
Boyuan Zheng 
Shijie Chen 
Danny Driess 
Fei Xia 
S M Mehdi 
Corey Sajjadi 
Lynch 
Kiana Ehsani 
Winson Han 
Alvaro Herrasti 
Eli Van- Derbilt 
Luca Weihs 
Eric Kolve 
Aniruddha Kemb- Havi 
Roozbeh 2021 Mottaghi 
Manipulathor 
Linxi Fan 
Guanzhi Wang 
Yunfan Jiang 
Ajay Man- Dlekar 
National Key Laboratory of General Artificial Intelligence
BIGAI</p>
<p>Yuncong Yang 
Haoyi Zhu 
Andrew Tang 
De-An Huang 
Yuke Zhu 
Anima Anandkumar 
Minedojo 
Daniel Gordon 
Aniruddha Kembhavi 
Mohammad Rastegari 
Joseph Redmon 
Wenlong Huang 
Ted Xiao 
Harris Chan 
Jacky Liang 
Pete Florence 
Andy Zeng 
Jonathan Tomp- Son 
Igor Mordatch 
Yevgen Chebotar 
Abhinav Gupta 
Ali 2017b Farhadi 
Nelson F Liu 
Kevin Lin 
John Hewitt 
Ashwin Paran- Jape 
Michele Bevilacqua 
Fabio Petroni 
Percy 2023a Liang 
Lost 
Xiao Liu 
Hao Yu 
Hanchen Zhang 
Yifan Xu 
Xuanyu Lei 
Hanyu Lai 
Hangliang Ding 
Kaiwen Men 
Kejuan Yang 
Shudan Zhang 
Ao- Han Zeng 
Zhengxiao Du 
Chenhui Zhang 
Sheng Shen 
Tianjun Zhang 
Huan Sun 
Minlie Huang 
Yuxiao Dong 
Xavier Puig 
Kevin Ra 
Marko Boben 
Jiaman Li 
Scott Reed 
Konrad Zolna 
Emilio Parisotto 
Ser- Gio Gomez Colmenarejo 
Alexander Novikov 
Gabriel Barth-Maron 
Mai Gimenez 
Yury Sulsky 
JostJackie Kay 
Tobias Springenberg 
Noah Shinn 
Federico Cassano 
Ashwin Gopinath 
Karthik R Narasimhan 
Shunyu 2023 Yao 
Mohit Shridhar 
Jesse Thomason 
Yonatan Bisk 
Luke Zettlemoyer 
Dieter 2020a Fox 
Alfred 
Luke 
Adam Trischler 
Alfworld 
Chan Hee Song 
Jiaman Wu 
Clayton Washington 
Brian M Sadler 
Wei-Lun Chao 
Mohit IyyerTu Vu 
Xuezhi Wang 
Noah Constant 
Jerry Wei 
Jason Wei 
Chris Tar 
Yun-Hsuan Sung 
Denny 
Shenzhi Wang 
Chang Liu 
Siyuan Qi 
Shuo Chen 
Qisen Yang 
Andrew Zhao 
Jason Wei 
Dale Schuurmans 
Maarten Bosma 
Ed Chi 
V Quoc 
Denny Le 
Zhou 
Howard Chen 
John Yang 
Karthik Preprint Narasimhan 
Webshop 
Jeffrey Zhao 
Dian Yu 
Nan Du 
Izhak Shafran 
Karthik Narasimhan 
Yuan 2022 Cao 
Shuyan Zhou 
Frank F Xu 
Xuhui Zhou 
Robert Lo 
Abishek Sridhar 
Xianyi Cheng 
Daniel Fried 
Uri Alon 
Wangchunshu Zhou 
Yuchen Eleanor Jiang 
Long Li 
Jia- Long Wu 
Tiannan Wang 
Shi Qiu 
Jintian Zhang 
Jing Chen 
Ruipu Wu 
Shuai Wang 
Xizhou Zhu 
Yuntao Chen 
Hao Tian 
Chenxin Tao 
Wei- Jie Su 
Chenyu Yang 
Gao Huang 
Bin Li 
Lewei Lu 
Brianna Zitkovich 
Tianhe Yu 
Sichun Xu 
Peng Xu 
Jialin Wu 
Paul Wohlhart 
Stefan Welker 
Ayzaan Wahid </p>
<p>Arun Ahuja
Sheila Babayan, Felix Hill</p>
<p>Samuel Stevens
Boshi Wang, Huan Sun, and Yu Su</p>
<p>Aakanksha Chowdhery
Jonathan Tompson
Brian Ichter, Ayzaan Wahid, Tianhe YuQuan Vuong</p>
<p>Neural Information Processing Systems (NeurIPS)
35:, Qiuyuan Huang, Xiaojian Ma, Hoi Vo, Yusuke Noda, Zilong Zheng, Song-Chun Zhu, Demetri Terzopoulos, Li Fei-Fei18343-18362, 2023Ran Gong, Zane Durante</p>
<p>Matt Deitke
Kiana Ehsani, Yuke Zhu, et al. 2017aDaniel Gordon</p>
<p>Tingwu Wang
Sanja Fidler</p>
<p>and Antonio Torralba</p>
<p>Shiji Song, and Gao Huang. 2023b
Chaofei Wang</p>
<p>LangSuit•E: Planning, Controlling and Interacting with Large Language Models in Embodied Text Environments
24 Jun 202432AD5BD8CB7A7B767ABEBD429DADDAEFarXiv:2406.16294v1[cs.CL]
Recent advances in Large Language Models (LLMs) have shown inspiring achievements in constructing autonomous agents that rely on language descriptions as inputs.However, it remains unclear how well LLMs can function as few-shot or zero-shot embodied agents in dynamic interactive environments.To address this gap, we introduce LangSuit•E, a versatile and simulation-free testbed featuring 6 representative embodied tasks in textual embodied worlds.Compared with previous LLM-based testbeds, LangSuit•E (i) offers adaptability to diverse environments without multiple simulation engines, (ii) evaluates agents' capacity to develop "internalized world knowledge" with embodied observations, and (iii) allows easy customization of communication and action strategies.To address the embodiment challenge, we devise a novel chain-of-thought (CoT) schema, EmMem, which summarizes embodied states w.r.t.history information.Comprehensive benchmark results illustrate challenges and insights of embodied planning.LangSuit•E represents a significant step toward building embodied generalists in the context of language models.</p>
<p>Introduction</p>
<p>Creating embodied agents that can perceive, think, and interact with simulated environments from the agents' own perspective is a long-standing yet imperative challenge within the realms of robots and artificial intelligence.Recent striking progress of Large Language Models (LLMs) has ushered promising avenues for constructing autonomous agents, wherein LLMs serve as their central planners or controllers, evidenced by several intriguing demos and benchmarks within simulated environments (Reed et al., 2022;Park et al., 2023;Chevalier-Boisvert et al., 2019) or even realistic physical worlds (Driess et al., 2023;Huang et al., 2022) across a diverse spectrum of domains, spanning from web browsing (Zhou et al., 2023a;Yao et al., preprint), language games (Park et al., 2023) and householding (Shridhar et al., 2021), etc.These language agents, reliant solely upon language descriptions as their inputs, can benefit from LLMs' exhibited commonsense knowledge, emergent semantic reasoning and problem-solving skills.Despite the promising advances of prior LLM-asagent tasks, it remains unclear that:</p>
<p>Without perception error caused by visual models, can LLMs behave as embodied agents to solve complicated tasks within dynamic environments?</p>
<p>Previous works mostly treat LLM agents as task planners, providing them with the global observations and all required information (e.g., entire history) in natural language to produce the next step, which contrasts the nature of chat-based language models that can potentially form their internal map and history across conversational turns.Moreover, the action spaces across tasks are specially designed and can only be evaluated within specific environments, which limits the potential capacity of evaluating the generalizability across tasks.In stark contrast, research on embodied agents and environments has gone through many successful attempts -numerous open-source embodied systems developed with realistic simulation engines, such as AI2Thor (Kolve et al., 2017b), VirtualHome (Puig et al., 2018), spurring the trend of learning embodied instruction-following challenges, such as navigation (Chattopadhyay et al., 2021), manipulations (Ehsani et al., 2021), question answering (Gordon et al., 2018),etc.Models learned within these environments, nevertheless, struggle with their dependency on simulation engines and are therefore limited to specific tasks.Other prominent research works such as PaLM-E (Driess et al., 2023) and RT (Zitkovich et al., 2023) sought to powerful vision-language models (VLMs) for embod-</p>
<p>[System]: You and your friend are two autonomus intelligent agents tasked with a shared household work.The task is to put a vase on the table.</p>
<p>[System][→Bot1]: You can see a table, two chairs in front.</p>
<p>[Bot1] Thought: To slove this task, we have to first find a vase.</p>
<p>[Bot1][→Bot2]: Search for a vase.[Bot2] Thought: Now I find two vases, I should know which one to pick up.</p>
<p>[System][→Bot2]: You can see a desk, a sofa in front.On the desk, there is a blue vase, a brown vase.</p>
<p>Interactive QA</p>
<p>How many vases are around here?I will go and explore the room...</p>
<p>Answer is [2]</p>
<p>Instruction Grounding</p>
<p>Go to the red box.</p>
<p>Sure.I can see a red box ahead.I will move ahead.</p>
<p>Multi-agent Teach</p>
<p>You are commander you have to complete a task with a follower: ...</p>
<p>Rearrangement</p>
<p>You have to recover the objects that are misplaced.The original states are ...</p>
<p>Household</p>
<p>Put a clean lettuce in dining table.</p>
<p>Sure.I will first find a lettuce...The cup_1 is on the sofa.It should be on the table.I will...</p>
<p>Multi-agent Coopeartion</p>
<p>You are hurry to finish a shared task with your friend together: ... ied scene perception and language models for lowlevel control.However, these works can only be applied to constrained environments and require large volumes of data for pre-training.
I
To fill the gap and step towards general embodied intelligence, we introduce LangSuit•E, a systematic and simulation-free testbed for evaluating LLMs' capabilities across different embodied tasks in textual embodied worlds.As listed in Tab. 1, LangSuit•E showcases 6 representative embodied tasks, with variants of the number of rooms, the number of agents, the action spaces of agents (whether they can communicate with each other or ask humans).Compared with prior LLMbased agent testbed, LangSuit•E presents superior benefits on building and benchmarking LLMs' embodied capabilities.First, due to the nature of textual worlds, LangSuit•E incorporates a general yet fully customizable indoor environment that can easily adapted to most simulated embodied environments, (e.g., AI2Thor (Kolve et al., 2017b), Vir-tualHome (Puig et al., 2018)), without resorting to variants of simulation engines and benchmark versions.Second, LangSuit•E only presents embodied observations to agents at each step.A typical example is shown in Figure 1, where only in-front observations within certain distances (unblocked by walls) are presented to agents at each step.Prior study (Huang et al., 2022) has shown that language models can potentially form "internalized world knowledge" through thinking in language.For in-stance, they shall potentially know that executing "turn_right by 90 degrees" four times shall remain in the same direction as the beginning.Such capacity could potentially enable LLM agents to reason over action histories and observations to plan for the next.Third, the agents in LangSuit•E are fully-customizable w.r.t.their action spaces and communicative capabilities, i.e., one can easily adapt the communication and acting strategy from one task to another.</p>
<p>To evaluate the current prevalent LLMs's performance on LangSuit•E, we derive a comprehensive benchmark across 6 different tasks.Of note, annotating or generating golden trajectories is commonly labor-intensive and time-consuming.In LangSuit•E, we can easily form a benchmark by leveraging and modifying existing annotations, a by-product benefit of pursuing such a general textual embodied world.An underlying hypothesized benefit is that knowledge learned from one task can also enhance the performance of the others.</p>
<p>Lastly, inspired by grid cells in neural science (Banino et al., 2018) and the "lost-in-middle" observation of long-context language models (Liu et al., 2023a), we propose EmMem, an Embodied planning strategy with an emergent embodied Memory.Specifically, the language model learns to summarize and predict their embodied states in an in-context manner given long-context trajectories and observations.Results on the Household task present the effectiveness of the EmMem strategy.In summary, this paper makes the following contributions: i) we deliver a systematic and useful testbed with 6 exemplar tasks for testing and training LLMs as embodied agents; ii) We introduce EmMem, a novel prompting strategy for embodied tasks; iii) We show substantive benchmark results and call for future exploration in LangSuit•E.</p>
<p>Related Work</p>
<p>Evaluating LLM as Autonomous Agents The concept of LLM-as-Agent has triggered the development of many systematic and interactive environments that allow for textual interactions and show well-aligned language groundings in visual or physical modalities.TextWorld (Côté et al., 2018) is one of the earliest testbeds that presented a text-based game sandbox for training and testing reinforcement learning (RL) agents.Based on this framework, ALFWorld (Shridhar et al., 2021) proposed an embodied challenge that aligns the ALFRED (Shridhar et al., 2020a) benchmark to TextWorld.However, this environment still requires a simulator to generate text descriptions and simplifies the embodied challenge by presenting full world observations to agents (Fig. 2).More recently, enhanced by harnessing the capacities of chat-based LLMs, a variety of language agents targeting a spectrum of different tasks have been demonstrated.For example, WebShop (Yao et al., preprint), Mind2Web (Deng et al., 2023) and WebArena (Zhou et al., 2023a) present environments on open-domain web shopping and browsing; Agents (Zhou et al., 2023b) design a library that supports long-short term memory; Agent-Bench (Liu et al., 2023b) presents a comprehensive evaluation of 25 different LLMs across 8 environments.In addition to single-agent settings, recent advances also show emergent behaviors in multiagent LLM systems (Gong et al., 2023;Park et al., 2023;Chen et al., 2023;Wang et al., 2023b).However, most of these tasks over-simplify the agent's observations and actions by presenting with full knowledge of the world's status.</p>
<p>LLMs for Embodied Tasks and Robotics The rise of LLMs has spurred many inspirations on conventional embodied and robotics systems.(Vemprala et al., 2023) presented an empirical success of using ChatGPT in zero-shot robotics controls.Say-Can (Ahn et al., 2022) and LLM-Planner (Song et al., 2022) employ visual navigation to collect information in the house for the challenging grounded plan generation.Another thread of works presents attempts on multimodal embodied environments by leveraging a visual-language pretraining model for world perception and LLMs for planning.For instance, many works leverage LLMs to enhance agent ability in the Minecraft environment (Wang et al., 2023a;Zhu et al., 2023;Reed et al., 2022;Baker et al., 2022;Fan et al., 2022).Wu et al. (2023b) construct a multimodal dataset containing triplets of indoor scenes, instructions, and action plans, where they employ the AI2-THOR simulator (Kolve et al., 2017a) to expand indoor scenes as environments and leverage GPT-3.5 to automatically annotating action plans.However, due to the imperfect multimodal understanding and dynamic scene environments, these tasks are limited to single-agent settings and simple manipulations and navigation in domain-specific environments.Differently, LangSuit•E presents a simulation-free and embodied framework that allows a variety of environments and agent settings.</p>
<p>Planning and Acting with LLMs Inspired by the success of chain-of-thought (CoT) on language reasoning tasks, many works propose to enhance the planning and acting capabilities of LLMs in a similar manner.ReAct (Yao et al., 2022) proposes to combine CoT reasoning and actions to enhance the efficiency of language models.Dasgupta et al. (2023)</p>
<p>High-level actions goto [loc]</p>
<p>Go to a location that is in current observation.... ...</p>
<p>Communicative actions chat [message]</p>
<p>Send messages to other agents.</p>
<p>ask [message]</p>
<p>Send queries to users.</p>
<p>Pan et al. ( 2023) transfers visual perception from the virtual environment to language descriptions, then prompts LLM to solve the specific navigation tasks only leveraging language descriptions.But different from them, we provide a unified embodied environment that is capable of real-time feedback and integrates various embodied tasks.</p>
<p>The LangSuit•E Testbed</p>
<p>The goal of LangSuit•E is to provide a unified testbed thereby evaluating the diversified ability of LLM-as-Agents in a dynamic embodied environment with textualized perceptual representation and realistic physical limitation.Moreover, by unifying various embodied tasks in one environment, agents could improve the ability of knowledge transfer and construct consistent world cognition from interactions with the environment to solve different kinds of tasks.</p>
<p>Generic Embodied Task Planning</p>
<p>LangSuit•E aims to construct a general textual embodied world by defining a lightweight and simulation-free environment system.The embodied task of LangSuit•E can be formulated as a partially observable Markov decision process (POMDP) (S, A, T , R, U, O), which encompasses embodied agent state space S, action space A, deterministic transition function T : S × A → S, reward function R, task instruction space U and embodied observation space O.</p>
<p>ALFWorld LangSuit•E</p>
<p>Global Description</p>
<p>Welcome !You are in the middle of the room.Looking around you, you see a diningtable, a stove, a microwave, and a cabinet.</p>
<p>Your task is to: Put a pan on the diningtable.</p>
<p>[1] goto the cabinet You arrive at the cabinet.</p>
<p>The cabinet is closed.</p>
<p>[2] open the cabinet</p>
<p>The cabinet is empty.……</p>
<p>Embodied Description</p>
<p>You  Embodied States The state refers to the representation of the environment that an agent is in at a given time.LangSuit•E encompasses diverse indoor scene layouts.The state of our embodied setting includes the indoor arrangement, object information, the history of the previous action decisions and executive outcomes, the overall target or programmatic sub-goals, the limitations that must be followed, and so on.</p>
<p>VS.</p>
<p>VS.</p>
<p>High-Level Actions Low-Level Actions</p>
<p>Embodied Observations To simulate real-world conditions, the environment is partially observable, which means that an agent is not capable of perceiving the entire world or environment states.Based on this, LangSuit•E provides the embodied observations of partial objects in the environment with full customization of each agent's view range.In particular, LangSuit•E includes a module dedicated to describing environmental objects that are perceptible within an agent's uniquely configured visual scope using natural language.For instance, the language expressions could be "You can see a red ball on your right" or "You can see nothing ahead".Embodied Actions LangSuit•E incorporates a wide spectrum of executable embodied actions, covering most actions typically encountered within various embodied environments.The agent decides to execute a specific action in each time step according to the embodied state and its internal world knowledge.Tab. 2 presents an enumeration of the action types that are currently accommodated in the LangSuit•E.Refer to Tab. 9 in Appendix for the complete action set.Furthermore, LangSuit•E exhibits substantial flexibility by allowing for the customized configuration of action scopes.For example, the movement distance associated with the move_ahead action can be readily modified by adjusting the relevant hyperparameters, thereby facilitating tailored adjustments to suit specific embodied task requirements.</p>
<p>Feedback LangSuit•E environment conveys feedback through the use of natural language descriptions.This feedback is automatically generated by our system by evaluating the feasibility of the action predicted by LLMs in the context of the current state.The evaluation takes into account physical constraints imposed by both the agent and the environment, such as the agent's grasping range or environmental obstacles, ensuring that the action aligns with the agent's physical capabilities and the environmental context; refer to Fig. 3 and Appendix for examples.</p>
<p>Communications in LangSuit•E</p>
<p>The platform additionally provides multi-agent and human-agent communication channels to enable multi-agent cooperation and human-in-the-loop tasks.The Communicative actions in Tab. 2 are additional actions to be appended for this capacity.</p>
<p>(Multi-)Agent Interactions Multi-agent cooperations are widely explored in many recent LLM-asagent tasks (Zhou et al., 2023b;Park et al., 2023).</p>
<p>In LangSuit•E, the agents are fully customizable regarding their action spaces and communicative capabilities, i.e., one can easily adapt the communication and acting strategy from one task to another.See Fig. 1 for a typical example.</p>
<p>Human Communication LangSuit•E supports human-agent interactions through two primary mechanisms: firstly, it empowers humans to control an agent or play the role of the agent to cooperate with other agents.Secondly, it affords agents the capability to seek clarification from humans in instances of environmental or instructional ambiguity, enabling humans to offer assistance by imparting pertinent information via a communication protocol, thus enhancing task completion.</p>
<p>Benchmark LangSuit•E Tasks</p>
<p>We integrate a diverse range of embodied tasks within the unified LangSuit•E testbed.Tab. 1 demonstrates the specific categories of tasks that have been accommodated within LangSuit•E.We carefully select the six tasks to illustrate how our testbed can be applied to single-agent embodied understanding and planning (Instruction Grounding, Interactive QA, Household and Rearrangement), multi-agent cooperation (MA-WAH), multi-agent teaching (MA-Household), each of which represents a significant challenge within the current embodied community.We re-implement all tasks in our embodied environments to reach the objective of a unified simulation-free testbed.Notably, LangSuit•E has an inherent capacity for facile expansion and adaptation to accommodate additional embodied tasks with minimal resource consumption, for the reason of the wealth meta-capabilities it possesses, encompassing easy customization of communication and action strategies and the ability to support interactions involving multiple agents, among others.</p>
<p>Task Instructions Task instruction provided to an agent focuses on the characteristics and constraints of tasks with a particular type, involving customized descriptions of the specific type, action spaces of tasks, and optional few-shot examples to show the diverse execution processes of some tasks under the same type.</p>
<p>LangSuit•E provides a template for the automated generation of diverse task instructions.By filling out customized task descriptions, definitions of action spaces pertinent to a particular task type, or the incorporation of few-shot illustrative examples, LangSuit•E is capable of generating distinct sets of instructions that serve as prompts for LLMs regarding different tasks.The tasks that the current version of LangSuit•E supports:</p>
<p>Task I. Instruction Grounding (IG) (Chevalier-Boisvert et al., 2019) provides synthetic naturallooking instructions (e.g., "put the red ball next to the box on your left"), requiring the agent to navigate the world (including unlocking doors) and move objects to specified locations.</p>
<p>Task II.Rearrangement (Weihs et al., 2021) involves moving and modifying (i.e., opening/closing) randomly placed objects within a room to obtain a goal configuration.</p>
<p>Task III.Interactive QA (IQA) (Gordon et al., 2018) is the task of answering questions that require the agent to interact with a dynamic environment, which contains three different question types: Existence questions ("Is there an apple in the kitchen?"),Counting questions ("How many forks are present in the scene?"), and Spatial Relationship, called Contains questions ("Is there lettuce in the fridge?").</p>
<p>Task IV.Household (Shridhar et al., 2020a) tasks contain both high-level goals like "Rinse off a mug and place it in the coffee maker."and low-level language instructions like "Walk to the coffee maker on the right."Task V. Multi-agent Teach (MA-Teach) (Padmakumar et al., 2022) tasks should be completed with the cooperation of two agents.A Commander with access to oracle information about a task communicates in natural language with a Follower.The Follower navigates through and interacts with the environment to complete tasks varying in complexity from "make coffee" to "prepare breakfast", asking questions, and getting additional information from the Commander.</p>
<p>Task VI.Multi-agent Coopeartion tasks focus on evaluating the cooperation and communication ability of multiple agents to accomplish shared goals.We follow C-WAH (Zhang et al., 2023) to use a watch-and-help (WAH) configuration in this task.Specifically, all agents shall negotiate with each other to break down the task based on their individual observations and capabilities.Different from C-WAH, all agents in this task can only perceive their own embodied observations.</p>
<p>Gold Trajectories Generation</p>
<p>We have implemented an extra expert module for task I-IV (the multi-agent settings require agent interactions), dedicated to generating gold trajectories and affording us the capability to construct training datasets that may be subsequently employed for the potential fine-tuning of LLMs.Given a task without gold trajectories, our expert module automatically produces a reasonable and executable trajectory, including every-step actions.Specifically, we first analyze a list of objects necessitating manipulation according to the answer or the final state of the task.Second, we construct a list of sub-tasks corresponding to each operation of an object and carefully devise the execution sequence of sub-tasks.Finally, we adopt the A-star algorithm to chart the shortest paths between the dynamic agent location (the agent's location after finishing the last sub-task) and the target object for each sub-task.As a consequence, our expert module outputs an exhaustive sequence of actions meticulously crafted to fulfill the task at hand.</p>
<p>Evaluation Metrics</p>
<p>We follow metric design schemas as in (Weihs et al., 2021;Shridhar et al., 2020a) to provide the following evaluation metrics:</p>
<p>• Success Rate (SR): the ratio of tasks in which an agent achieves the target goal or state.• Goal-Conditioned Success Rate (Goal-SR): the sub-goal completion ratio of a task, averaged across all the tasks of the same type.• Misplaced Rate (Misplaced): the misplaced objects in the Rearrangement task, which computes the ratio of misplaced objects at the end of the task, denoting better performance with a lower value.• Fixed Strict Rate (Fixed Strict): the correctly reset objects in the Rearrangement task, which computes the ratio of correctly reset objects at the end.• Accuracy (Acc): the proportion of correct answers compared to the total number of questions.</p>
<p>The metric solely evaluates on the IQA task.However, all these strategies failed to reveal satisfactory results in our embodied environments; refer to Fig. 3 for typical failures.We present EmMem, a novel embodied planning paradigm that reasons and summarizes the agent's current embodied memory in an in-context and</p>
<p>additional trail of verbal reflection). Because</p>
<p>Reflexion will try an additional trail if the task fails and the summarization of the whole last trail will be input to the new trail planning through the Task prompt, we do not show the case in this figure.More details and the prompt instructions for these strategies can be found in Appendix G. emergent manner.Specifically, the LLM agent has to explicitly predict and summarize their current embodied state before proceeding to the actual planning and action prediction step.We provide two underlying rationales for such prompts: First, as for mammals' embodied navigation, neurons like grid cells integrate path information and are thought to be critical in self-motion (Banino et al., 2018;Hafting et al., 2005).The EmMem, auto-regressively in principle, produces integrated embodied information as grid cells.Second, recent literature has observed the lost-in-middle performance for longcontext language models (Liu et al., 2023a).Our EmMem enhances the "middle" information in a residual-like approach (He et al., 2016).Fig. 3 illustrates a case study of EmMem and other strategies.</p>
<p>Experiments</p>
<p>Setup</p>
<p>We evaluate the ability of popular LLMs as agents to solve embodied tasks in our embodied textual environment.It is crucial to note that our testbed does not provide any global information, necessitating that agents mobilize more capabilities in the more realistic and complicated embodied environment to execute tasks, including comprehension of embodied description, retention of historical exploration within the environment, and selfawareness of its current state.We mainly use1 gpt-3.5-turbo(with temperature being 1) to evaluate on LangSuit•E.Notably, for API-cost economy, We evaluated 100 randomly sampled tasks for IG, Household, and Rearrangement, along with 50 randomly sampled tasks for each subtask of IQA (Exists, Contains, and Counts), MA-Household, and MA-WAH.We conduct experiments both in the low-level (fine-grained actions) and high-level action settings.The evaluation metrics adopted mainly are introduced in Section 3.4.</p>
<p>Main Results and Observations</p>
<p>In this section, we summarize the empirical results and focus on addressing the following questions: Q1: How do previous CoT strategies perform in LangSuit•E?Fig. 3 and Tab. 3 show the qualita-   et al., 2022).In LangSuit•E, we also incorporate the ask-for-help setting to obtain human preferences and selection, especially when it comes to ambiguous task instructions (a qualitative demonstration in Fig. 4).</p>
<p>Q4: Does the temperature highly influence the EmMem performance?We conducted experiments with different temperature settings for GPT-3.5 and reported the corresponding standard deviations in Tab. 5. We have observed consistent Task: Put the gold vase into the white safe.Obs: You see nothing.You can try to take action like ask, move_ahead, turn_left or turn_right …… [1] Thought: To complete the task, I need to find the gold vase and the white safe.Then I can put the gold vase into the white safe.I don't have any information about the location of the gold vase or the white safe.I should ask for help to find them.</p>
<p>[1] Act: ask [Where is the gold vase?] Feedback: Human response: On your left.</p>
<p>[2] Thought: Since the gold vase and the white safe are not in my current view, I need to explore the environment to find them.performance across various temperature settings.Our hypothesis is that the CoT strategy, coupled with exemplar cases, plays a crucial role in guiding LLMs to follow a consistent reasoning trajectory.This observation underscores the effectiveness of our approach in maintaining stability and reliability in the model's performance.Q5: How does the EmMem perform in the Multiagent settings?We only use high-level actions in multi-agent tasks to reduce task complexities.We observe that all API-based agents maintain the capacity of multi-agent cooperations with relatively high Goal-SRs.We did not report low-level results as there is barely a successful case.As demonstrated in Tab. 4, EmMem achieves consistent improvement, except for the Goal-SR metric in MA-Household with Reflexion.Note that GPT-3.5 achieves high performance in MA-WAH tasks in most of the prompt strategies, because the feedback of each action provided in MA-WAN contains the self-state of agents.It is consistent with our intuition for designing EmMem: explicitly predicting and summarizing agents' current embodied state before proceeding to the actual planning and action prediction step is beneficial.It is exciting that even with the self-state of agents the environment provided, EmMem inspiring the self-awareness of agents' current stat still further improves the performance.</p>
<p>Conclusion</p>
<p>We introduce LangSuit•E, a lightweight and simulation-free testbed in textual embodied worlds.Designed to be fully embodied, LangSuit•E aims to test LLM agent's embodied planning capacity without errors from visual perceptions.To further address the embodiment challenge, we devise a novel CoT schema, EmMem, which summarizes embodied states w.r.t.history information.Although benchmark results indicate the effectiveness of Em-Mem, there still remains a huge performance gap compared with SFT results on individual tasks.As such, we call for future exploration of embodied generalists on LangSuit•E.</p>
<p>Limitation</p>
<p>In this work, we endeavor to investigate how LLM performs and understands embodiment in a generic embodied textual setting.Nevertheless, our platform maintains the potential to be bridged with most simulators via additional visual language models (VLMs).Due to the page limitation and workload capacity, it remains unclear whether such linkage would be a successful attempt, even if we have obtained a powerful LLM embodied agent.We would like to leave it as a strong inclination for future research on generic embodied multimodal agents.</p>
<p>Moreover, the high demand of LLM API calls leads to a substantial querying price for experiments.Preliminary studies show that open-source language models, especially small models (e.g., LLAMA 2-7B), are unable to make appropriate embodied predictions in our environment, no matter what kind of prompting strategy is used.Nevertheless, it would be more than valuable to investigate how open-source language models can be trained or finetuned in the LangSuit•E environment.</p>
<p>Additionally, following previous works (Yao et al., 2022), we mainly use GPT-4 and gpt-3.5turbofor all experiments.Though we have observed performance boosts with a more powerful model, whether all observations and analyses are model-dependent remains unknown.Intuitively, our framework is model-agnostic and can be generalized to all embodied tasks.We are more than willing to experiment on more APIs once we have the access.</p>
<p>Lastly, we didn't go into too much on human communications and multi-agent tasks.Nevertheless, our platform maintains the capabilities to be generalized to multi-agent settings and human communicative settings (all you need is to update the configuration of agents).As such, learning and experimenting with multiple embodied agents in LangSuitE could also be a valuable future exploration trend</p>
<p>Ethics Statement</p>
<p>In this work, we aim to deliver a generic, lightweight and simluation-free testbed for general embodied planning.By leveraging large language models, the main purpose of this research is to benefit the embodied AI and language reasoning community.Moreover, we devise a novel reasoning paradigm, EmMem, which grants agents with emergent embodied memories.Going beyond embodied understanding, we believe the CoT paradigm is general and can be generalized to more reasoning and planning tasks.A The motivation of the design for LangSuit•E</p>
<p>First, we want to design our unified textual environment being general, embodied, interactive, and communicative, thereby containing most of the typical embodied task categories, such as navigation, rearrangement, human interaction, multi-agent cooperation, and so on.Therefore, we transfer typical embodied tasks, carefully selecting action sets that are both suitable for our environment and the original task.The differences between our benchmark and original embodied tasks are that our environment is fully embodied and the actions are lowlevel (fine-grained atom actions).Second, As the previous platform generally gives the global description of the environment, trained models in the simulated environment are difficult to apply to real environments.Our platform simulation more realistic situation that agent has a limited view scope and action scope.</p>
<p>Third, The simulation-free platform makes various embodied tasks conveniently and easily performed in a unified fashion.</p>
<p>B Data and Code Release</p>
<p>All code and data will be publicly available for use.The data and code are under MIT license.All resources can only be used for research purposes.More details of the construction process of LangSuit•E can be found in the codebase.</p>
<p>C Time complexity comparison</p>
<p>We compare the computational cost for one-step execution/rendering on simulated environments vs. LangSuit•E.The time interval (Time Int) counts from receiving one LLM inference results to the next query to LLM (thus the latency caused by APIcall is not counted).We run all experiments on an Intel Core i9 with 64GB RAM.Results are shown in Tab. 6.Our LangSuit•E demonstrates significant systemwide efficiency improvements.
p s = s × L * max(L * , L)(1)
where L is the number of actions the agent took in the episode, and L * is the number of actions in the expert demonstration.</p>
<p>As can be seen from the Tab. 7, after using the path weighted score, the performance on the Act strategy does not decline significantly.However, for other tasks, the agent is encouraged to think more, leading to an increase in the number of steps and consequently greater penalties, resulting in a more noticeable decline.</p>
<p>E Customized configuration of an agent.</p>
<p>LangSuit•E supports the customized configuration of agents.It allows the configuration of prompt strategies, the initialization position of agents, inventory capacity, focal length, manipulate distance and view distance.For more details, see Tab. 8.  Clean object with a basin, a sink or a bathtub.go_explore [room] Explore around the room.go_check [recep] Go to receptacle and check its inside.go_grab [obj] Go to object and pick it up.go_put [recep] Go to receptacle and put the object in hand into it.</p>
<p>Communicative actions chat [message]</p>
<p>Send messages to other agents.</p>
<p>ask [message]</p>
<p>Send queries to users.</p>
<p>F Pseudo-Code for Expert Trajectory Generation</p>
<p>Algorithm 1 shows the pseudo-code for generating expert trajectories as references.</p>
<p>G Examples of 5 prompt strategies for task Instruction Grounding</p>
<p>We show the detailed instruction for different prompt strategies: Act, ReAct, ReAct+EmMem, Reflexion, and Reflexion+EmMem.We highlight the pivotal instruction sentences.</p>
<p>G.4 Reflexion</p>
<p>Instruction:</p>
<p>You are an autonomous intelligent agent tasked with completing a minigrid game.These tasks will be accomplished through the use of specific actions you can issue.</p>
<p>Here's the information you'll have: Task: This is the task you are trying to accomplish.</p>
<p>[:</p>
<p>Bot2] Act: move_ahead [Bot2] Act: pick_up [blue vase] [Bot2][→User ]: I can see two vases in front, which one should I pick up?The blue one.</p>
<p>Figure 1 :
1
Figure 1: Task illustration of LangSuit•E.Top-Left: a typical example of a cooperative task between two agents.The agents are entirely blind (i.e., without visual perception) and can only obtain embodied information from the system.Around: Exemplar tasks supported by LangSuit•E.</p>
<p>Figure 2 :
2
Figure 2: Comparison of our Embodied Environment Description with ALFWorld (Shridhar et al., 2021).</p>
<p>Figure 3 :
3
Figure 3: A case study for four prompt strategies on Household.Act: The model predicts the next action only.Re-Act+EmMem: Our EmMem strategy with ReAct strategy (Reason+Act) (Yao et al., 2022).Reflexion+EmMem: Our EmMem strategy with Reflexion strategy (Shinn et al., 2023) (1 additional trail of verbal reflection).BecauseReflexion will try an additional trail if the task fails and the summarization of the whole last trail will be input to the new trail planning through the Task prompt, we do not show the case in this figure.More details and the prompt instructions for these strategies can be found in Appendix G.</p>
<p>[2] Act: turn_left Feedback: Action succeeded.Turn left by '90' degrees.Obs: On your left, you see a drawer_1 ……</p>
<p>Figure 4 :
4
Figure 4: Demonstration of human communications.</p>
<p>Table 1 :
1
Statistics of Tasks Integrated in LangSuit•E.We provide a unified and simulation-free environment that can easily adapt to most embodied tasks and benchmarks.
Task TypeSimulator/Env.# of Scenes # of Tasks # of Actions Multi-Room Multi-Agent CommunicativeInstruction Grounding (IG)Mini Grid/BabyAI (2019)1055006✓✗✗RearrangementAI2THOR/Rearrangement (2021)1205008✗✗✗Interactive QA (IQA)AI2THOR/IQA (2018)3030005✗✗✓HouseholdAI2THOR/ALFred (2020a)12050612✗✗✗Multi-agent Household (MA-Household) AI2THOR/TEACh (2022)12020013✗✓✓Multi-agent Watch-and-Help (MA-WAH) VirtualHome/Co-WaH (2023)2506✓✓✓</p>
<p>investigate a Planner-Actor-Reporter agent paradigm, where the Planner is equipped with a language model to perform reasoning.Wu et al. (2023a) propose a Plan, Eliminate and Track framework, aiming to mitigate constraints, such as finetuning inefficiency and bias from pre-training, of LLM with transformer architecture.Related to us,</p>
<p>Table 2 :
2
Examples of different action settings in LangSuit•E.Refer to Appendix for the complete set.
Action TypeDescriptionLow-level actionsmove_aheadMove ahead by 1 step.turn_leftTurn left by 90 degrees.turn_rightTurn right by 90 degrees.pick_up [obj]Pick up an object identified by [obj].put [obj, recep] Put the object [obj] into the receptacle[recep].open [obj]Open object [obj] if it is openable.close [obj]close object [obj] if it is closeable.stop [answer]Stop making actions and conclude thefinal answer in [answer].......</p>
<p>Table 3 :
3
Evaluation results on the test set of single-agent task types in LangSuit•E.The evaluation results on Household task in terms of Path Weighted Metrics are in Appendix D.
ModelSR.IG Steps.↓ AvgRearrangement SR. Misplaced.↓ Fixed Strict.Avg Steps.↓Exists Acc.IQA Contains Acc. Counts Acc.Avg Steps.↓Household SR. Goal-SR.Avg Steps.↓Low-Level Actions SettingAct13.028.28.0115.317.612.414.936.010.916.79.021.522.8ReAct12.025.35.0117.912.38.48.918.32.113.214.026.744.7ReAct-EmMem11.022.86.0108.313.79.548.058.06.014.810.026.328.5Reflexion20.029.98.099.314.313.762.072.038.023.719.034.628.6Reflexion-EmMem16.036.810.0100.415.712.560.086.072.020.714.032.035.1High-Level Actions SettingAct77.06.95.0112.317.27.024.076.030.010.323.037.816.2ReAct84.014.37.0111.215.26.944.088.018.015.122.042.021.4ReAct-EmMem85.016.96.0109.711.99.032.080.014.013.132.047.835.2Reflexion86.014.48.0102.717.56.544.090.022.09.933.053.027.7Reflexion-EmMem86.015.78.0100.2216.28.942.080.020.016.639.753.334.1</p>
<p>Table 4 :
4
Evaluation results on the test set of multi-agent task types in LangSuit•E.
ModelMA-Household SR. Goal-SR. SR. Goal-SR. MA-WAHHigh-Level Actions SettingAct18.0044.8036.0070.17ReAct18.0060.6338.0078.20ReAct-EmMem18.0064.7446.0076.13Reflexion18.0066.5856.0088.43Reflexion-EmMem20.0065.8164.0090.37ca-pability of considering the correct plans after theaction failed, but explores more steps, resultingin exceeding our step limit (in the low-level ac-tion setting on Household). Notably, ReAct dropsthe performance seriously in the IQA tasks in low-level action setting. A possible reason is that CoTcauses more severe hallucination problems, as illus-trated in Vu et al. (2023). The Reflexion, whichgoes through more trials and self-reflection pro-cess, showcases significantly better performanceon most task types. And equipped with EmMem,the performances are further improved in somecases. ➋ Low-level action vs. high-level actions:We compare the high-level action sets with low-
tive and numerical results of previous CoT methods on single-agent tasks.Our observations can be summarized as follows: ➊ Act-only vs. Re-Act vs. Reflexion: The Act can not strictly follow our desired format (it generates two actions in each step, where we only require the first one), and it predicts the stop action even though the task is not completed.Differently, ReAct has the level actions introduced in Tab. 2. In a high-level setting, an agent does not need to know how to perform low-level action sequences to achieve a goal place/object.Empirically and intuitively, we can observe that high-level actions make embodied tasks relatively easy to solve (fewer steps and higher SR.), which indicates the challenges of our fully embodied action setting.Although prompts with Thought (Reflexion, EmMem) increase the average execution steps of GPT-3.5, the performances of GPT-3.5 are improved in most cases.It is surprising that the Thought process drops the Fixed Strict scores in the low-level actions setting on Rearrangement.Q2: How does EmMem help in embodied planning tasks?As shown in Tab. 3 and case results Fig.3, with reasoning and summarizing the current embodied state in the ReAct+EmMem setting, an agent is more clear about the state of itself and the execution process of a task, hereby successfully carrying out the task; With both reflexion and memory (Reflexion+EmMem) strategies, an agent can recognize its mistakes quickly and reformulate following plans efficiently.We also observe that EmMem is more useful under the low-level action settings compared with high-level ones.Such observation is in line with the intuition that low-level settings can benefit more from embodied memory.Q3: How can human-in-the-loop be helpful?Previous works have shown that appropriate human-agent communication can substantially improve the efficiency in embodied grounding (Huang</p>
<p>Table 5 :
5
Evaluation results of varying temperatures (Temp) on Household using Reflexion-EmMem strategy in high-level actions setting.
Temp0.51.01.5SR.40.00±3.46 39.67±0.58 40.67±0.58Goal-SR. 52.47±3.71 53.30±2.43 52.86±1.13</p>
<p>Table 6 :
6
Time complexity comparison of simulated environments with LangSuit•E.Omni is the abbreviation of OmniGibson (OmniVerse) which is a physical-based simulator, and AI2Thor (Unity) takes only symbolic actions.
Env. (Engine)Omni AI2Thor LangSuit•ETime Int (ms) 67.907.811.89</p>
<p>Table 7 :
7
Evaluation results on Household task with Path Weighted Metrics.
ModelHousehold SR. Goal-SR.Low-Level Actions SettingAct6.6521.94ReAct9.5024.71ReAct-EmMem7.1921.13Reflexion14.8925.73Reflexion-EmMem11.1922.77High-Level Actions SettingAct22.3837.13ReAct18.6435.52ReAct-EmMem28.2743.02Reflexion28.2340.00Reflexion-EmMem34.4543.80D Path Weighted Metrics
We additionally utilized Path Weighted Metrics(Shridhar et al., 2020b)to calculate the path weighted score on Household task.This weighted score penalizes results that complete the task but take a longer number of steps.The introduction of Path Weighted Metrics allows the evaluation to focus not only on task completion but also on efficiency and path length, thereby providing a more comprehensive assessment of the model's performance.The path weighted score p s for metric s is given as:</p>
<p>Table 8 :
8
Customized configuration of an agent.
TypeDescriptionagent_typeWhether an agent has ReAct ability.agent_positionThe initial position of an agent.inventory_capacityThe number of objects an agent canhold.focal_lengthInversely proportional to viewing angle;shorter focal length corresponding tolarger view field.manipulate_distance The max distance that an agent's handcan reach.view_distanceThe max distance that an agent's viewcan reach.</p>
<p>Table 9 :
9
Action settings in LangSuit•E.
Action TypeDescriptionLow-level actionsmove_aheadMove ahead by 1 step.turn_leftTurn left by 90 degrees.turn_rightTurn right by 90 degrees.pan_leftMove left by 1 step.pan_rightMove right by 1 step.pick_up [obj]Pick up an object identified by [obj].drop [obj]Drop the object [obj] to the agent's cur-rent position.put [obj, recep]Put the object [obj] into the receptacle[recep].open [obj]Open object [obj] if it is openable.close [obj]close object [obj] if it is closeable.toggle_on [obj]Toggle on object [obj] if it is tog-gleable.toggle_off [obj]Toggle off object [obj] if it is tog-gleable.slice [obj]Slice object [obj] if it is sliceable.stop [answer]Stop making actions and conclude thefinal answer in [answer].High-level actionsgoto [loc]Go to a location that is in current obser-vation.heat [obj, heater] Heat object with a microwave or a stoveburner.cool [obj, cooler] Cool object with a fridge.clean [obj, sink]</p>
<p>This is the task you and your frend Alice are trying to accomplish.At each step, you can choose to think your plan or issue an action from one of the following: <code>chat [message].</code>:You can use this action to send a message to your friend Alice.<code>go_explore [room].</code>:Use this action to go to a room and explore around.<code>go_check [container].</code>:Use this action to go to a unchecked container and check its inside.You must have at least one free hands to check.<code>go_grab [object]</code>: Use this action to go to an object and grab it.<code>go_put [container].</code>:Use this action to go to the container and place the object in the container.You need to make sure you have the object(s) you need for the task.<code>stop [answer].</code>:Issue this action when you believe the task is complete.If you believe the task is impossible to complete, provide the answer as "N/A" in the bracket.To be successful, it is very important to follow the following rules: 1.You should only issue an action or your thought that is valid given the current observation 2.You should only issue one action at a time.3. Generate the action in the correct format starting with "Act: ".For example, "Act: go_grab [phone_0].".
Obs: These are the objects you can see .You can see at most { max_view_steps} step(s) in front of you, {side_steps} step(s) on your left, {side_steps} step(s) on turn_left, turn_right to appraoch it until you are facing it; 6. Issue stop action when you think you have achieved the objective. Don't generate anything after stop. Here are some examples: {example} Respond YES if you can play this game. Example: Task: go to the red box. Obs: You can see a blue key in front of you; You can see a red box on your right. Manipulable object: A blue key. &gt;Thought: To solve this task, I need to find a red box. There is a blue key just before me and blocked my way. The red box is on my right, so I should turn_right first to go to the red box. &gt;Act: turn_right. Task: Task: {task} Your memory from last trails is: { memory} Obs: {observation} What is your next step? Try to think before act. G.5 Reflexion+EmMem Instruction: You are an autonomous agent tasked with completing a minigrid game. These tasks will be accomplished through the use of specific actions you can issue. Here's the information you'll have: Task: This is the task you are trying to accomplish. Obs: These are the objects you can see 5. You can only operate the object in your sight, if the object is in front of you, on your left or right, take action like move_ahead, turn_left, turn_right to appraoch it until you it in your sight; 6. Issue stop action when you think you have achieved the objective.\ n7. Remember you are facing NORTH at the begining. Don't generate anything after stop. Here are some examples: {example} Respond YES if you can play this game. Example: Task: go to the red box. Obs: You can see nothing ahead. &gt;Thought: I see nothing, it could be I have reached the border or I am in a middle grid but there is nothing in front of me. I have taken 0 steps and I am facing NORTH now. I should remember what I saw, so that I can find them easily when necessary; I need to find a red box. &gt;OK. &gt;Act: move_ahead &gt;Feedback: Action failed. Can not move ahead, because there is an Obs: You see nothing ahead. &gt;Thought: I can't move ahead because of an obstacle but I saw nothing in front of me. so I have reached the border, I have taken 0 step and am facing NORTH now since last action failed. Next I will try to look around. &gt;OK. &gt;Act: turn_right &gt;Feedback: Action succeeded. Turned right by '90' degrees. Obs: You see a red box. &gt;Thought: I have reached the border left, 3 step(s) on your right. Manipulable object: The object that you are facing and you can take an operate action like pick_up [ object_name], toggle [object_name ] on it. Inventory: These are the objects you are holding, and you can take an action like drop on it. Feedback: Whether the action is succeed or not and why is it. At each step, you can choose to think your plan or execute an action from one of the following: <code>move_ahead</code>: it means you move ahead by 1 step. <code>turn_left</code>: it means you turn left 90 degrees. <code>turn_right</code>: it means you turn right 90 degrees. <code>pick_up [object_name]</code>: Use this action to pick up a pickupable and visible object directly in front of you without obstacle. <code>drop [object_name]</code>: it means you places the object in hand at the place just in front of you. <code>toggle [object_name]</code>: it means you toggle a toggleable and visible object in front of you without obstacle. <code>stop [answer]</code>: Issue this action when you believe the task is complete. If you believe the task is impossible to complete, provide the answer as "N/A" in the bracket. To be successful, it is very important to follow the following rules: 1. You should only issue a action or your thought that is valid given the current observation 2. You should only issue ONE action at a time, and ONLY action is enough. System: Feedback: Action failed. Failed to pick up red key. You can only pickup the object one step in front of you without obstacle. Obs: You can see a red key in front of you; You can see a green door,a blue key,a green box,a yellow key on your left; You can see a purple ball,a purple door on your right. Assistant: Thought: The red key is still in front of me, but I may need to move closer to pick it up. System: OK. Assistant: Act: move_ahead. System: Feedback: Action succeeded. Moved forward by 1 step. Obs: You can see a red key in front of you; You can see a green door,a blue key,a green box your left; You can see a purple ball on your right. Assistant: Thought: The red key is now directly in front of me. I can pick it up now. System: OK. Assistant: Act: pick_up [red key]. System: Feedback: Action failed. Failed to pick up red key. You can only pickup the object one step in front of you without obstacle. Obs: You can see a red key in front of you; You can see a green door,a blue key,a green box on your left; You can see a purple ball on your right. Assistant: Thought: I must have been too far to pick up the red key. I should move one step ahead to reach the key. System: OK. Assistant: Act: move_ahead. System: Feedback: Action succeeded. Moved forward by 1 step. Obs: You can see a red key in front of you; You can see a green door,a blue key on your left; Manipulable object: a red key. <code>drop [object_name]</code>: it means you place the object in hand at the place just in front of you. <code>open [object_name]</code>: it means you open an openable and visible object within 8.0 step(s). <code>close [object_name]</code>: it means you close a closeable and visible object. <code>stop [answer]</code>: Issue this action when you believe the task is complete. If you believe the task is impossible to complete, provide the answer as 'N/A' in the bracket. To be successful, it is very important to follow the following rules: 1. You should only issue ONE action or ONE thought that is valid given the current observation 2. When you issue a action, and ONLY ONE action is enough, and then wait for your feedback from system. 3. Generate the action in the correct format starting with "Act: " For example, "Act: pick_up [cup_0]" Do not add adjectives before object name. 4. State your thought or think process starting with "Thought: " 5. ONLY operate the object within your view distance. 6. If got action failed feedback, try to take other actions to proceed the task. 7. Issue stop action when you think you have achieved the objective. Don't generate anything after stop. Here are some examples: Original status: In the north of the room, there is a armchair_0; a pen_1; In the east of the room, cabinet_8; a cabinet_10; a cabinet_15; a cabinet_18; a countertop_2, in/on it you can see a bowl_0, a butterknife_0, a faucet_0, a faucet_1, a knife_0, a pan_0, a soapbottle_0, a toaster_0; a drawer_3. Obs: In front of you, You see a cabinet_0; a cabinet_4; a cabinet_9; a cabinet_11; an opened fridge_0, there is a egg_0 in/on it. On your left, you see a cabinet_2; a cabinet_10; a cabinet_13; a cabinet_18; a countertop_2; a bowl_0; a faucet_1 ; a knife_0; a pan_0; a toaster_0; a diningtable_0; a apple_0; a bread_0; a butterknife_0; a dishsponge_0; a lettuce_0; a potato_0; a saltshaker_0; a soapbottle_0; a spoon_0; a tomato_0; a drawer_3; a microwave_0; a sink_0; a sink_1; a stool_0; a window_0. On your right, you see a cabinet_3; a cabinet_22; a faucet_0. what is your next step? Assistant: Thought: As there are many objects in the room, I need to identify which objects have changed. Since I can see an egg in the fridge but the is open, I will close the fridge to reset it to its original state. Assistant: Act: close fridge_0. System: Feedback: Action succeeded. You closed fridge_0. Obs: In front of you, You see a cabinet_0; a cabinet_4; a cabinet_9; a cabinet_11; a fridge_0. On your left, you see a cabinet_2; a cabinet_10; a cabinet_13; a cabinet_18; a countertop_2; a bowl_0; a faucet_1 ; a knife_0; a pan_0; a toaster_0; a diningtable_0; a apple_0; a bread_0; a butterknife_0; a Assistant: Act: move_ahead. System: Feedback: Action succeeded. Moved forward by 1 step. Obs: In front of you, You see a diningtable_0; a window_0. On your left, you see a apple_0; a bread_0; a lettuce_0; a spoon_0; a stoveburner_0; a pot_0; a stoveburner_1; a stoveburner_2; a stoveknob_0. On your right, you see a cabinet_17; a countertop_1; a cup_0; a plate_0; a butterknife_0; a potato_0; a saltshaker_0; a soapbottle_0; a tomato_0; a drawer_2; a stool_0. Assistant: Thought: The sink_0 is not in my view. I need to turn to my right to find it. Assistant: Act: turn_right. System: Feedback: Action succeeded. Turned right by '90' degrees. Obs: In front of you, You see a cabinet_0; a cabinet_9; a cabinet_11; a fridge_0. On your left, you see a cabinet_2; a cabinet_4; a cabinet_10; a cabinet_13; a countertop_2; a bowl_0; a faucet_1; a knife_0; a pan_0; a toaster_0; a diningtable_0; a apple_0; a bread_0; a lettuce_0; a potato_0; a saltshaker_0; a soapbottle_0; a spoon_0; a tomato_0; a drawer_3; a microwave_0; a sink_0; a sink_1; a stool_0; a window_0. On your right, you see a cabinet_3; a cabinet_5; a cabinet_12; a cabinet_21; a cabinet_22; a faucet_0. Assistant: Thought: The sink_0 is in front of me. I need to move ahead and put the dishsponge back to the sink_0. Assistant: Act: move_ahead. System: Feedback: Action succeeded. take action like move_ahead, turn_left or turn_right to explore the room. Assistant: Act: move_ahead System: Feedback: Action succeeded. Moved forward by '0.25' meter(s). Obs: You see nothing. You can try to turn_left or turn_right to explore the room. Assistant: Act: turn_right System: Feedback: Action succeeded. Turn right by '90' degrees. Obs: On your left, you see a lightswitch_0; On your right, you see a bottle_1, a creditcard_1, a mug_1, a creditcard_1, a stool_1, Assistant: Act: move_ahead System: Feedback: Action succeeded. Moved forward by '0.25' meter(s). Obs: On your left, you see a lightswitch_0; On your right, you see a bottle_1, a creditcard_1, a stool_1, Assistant: Act: turn_right System: Feedback: Action succeeded. Turn right by '90' Obs: In front of you, You see a stoveknob_0; a stoveknob_1; a stoveknob_2; a cabinet_5; a stoveknob_3; a houseplant_1; a spoon_1; On your left, you see a bowl_5; a drawer_1; a drawer_2; a cabinet_0; a cabinet_1; a window_0 ; a sink_0; a drawer_6; a cabinet_6; a drawer_8; a toaster_1 ; a winebottle_1; a vase_3; a coffeemachine_1; a bowl_5; a lettuce_1; a stool_1; a faucet_0; On your right, you see a bowl_3, a bottle_1, a bottle_1, a knife_1, a soapbottle_2, a vase_1, a bowl_3, a creditcard_1, a pot_1, a papertowelroll_1, a fridge_0, a lightswitch_0, a mug_1, a creditcard_1, H.6 Multi-agent Cooperation System: You are an autonomous intelligent agent tasked with navigating a virtual home. Your name is Alice. You are hurry to finish a housework with your friend Bob together. There are kitchen, bedroom, bathroom. You will be given a household task. These tasks will be accomplished through the use of specific actions you and your friend can issue. You have two hands to pickup, hold, put and check objects. Here's the information you will have: Task: This is the task you and your frend Bob are trying to accomplish. At each step, you can choose to think your plan or issue an action from one of the following: <code>chat [message].</code>: You can use this action to send a message to your friend Bob. <code>go_explore [room].</code>: Use this action to go to a room and explore around. <code>go_check [container].</code>: Use this action to go to a unchecked container and check its inside. You must have at least one free hands to check. <code>go_grab [object]</code>: Use this action to go to an object and grab it. <code>go_put [container].</code>: Use this action to go to the container and place the object in the container. You need to make sure you have the object(s) you need for the task. <code>stop [answer].</code>: Issue this action when you believe the task is complete. If you believe the task is impossible to complete, provide Task: 4. State your thought or think process on it. Inventory: These are the objects you are holding, and you can take an action like drop on it. Feedback: Whether the action is succeed or not and why is it. At each step, you can choose to think your plan or execute an action from one of the following: <code>move_ahead</code>: it means you move ahead by 1 step. <code>turn_left</code>: it means you turn left 90 degrees. <code>turn_right</code>: it means you turn right 90 degrees. <code>pick_up [object_name]</code>: Use this action to pick up a pickupable and visible object directly in front of you without obstacle. <code>drop [object_name]</code>: it means you places the object in hand at the place just in front of you. <code>toggle [object_name]</code>: it means you toggle a toggleable and visible object directly in front of you without obstacle. <code>stop [answer]</code>: Issue this action when you believe the task is complete. If you believe the task is impossible to complete, provide the answer as \"N/A\" in the bracket. To be successful, it is very important to follow the following rules: 1. You should only issue a action or your thought that is valid given the current observation 2. You should only issue ONE action at a time, and ONLY action is enough. 3. Generate the action in the correct format starting with \"Act: \". For example, \"Act: pick_up [cup_0 ]\". 4. State your thought or think process Manipulable object: The object that you are facing and you can take an operate action like pick_up [ object_name], toggle [object_name] on it. Inventory: These are the objects you are holding, and you can take an action like drop on it. &gt;Feedback: Whether the action is succeed or not and why is it. At each step, you can choose to think your plan or execute an action from one of the following: <code>move_ahead</code>: it means you move ahead by 1 step. <code>turn_left</code>: it means you turn left 90 degrees. <code>turn_right</code>: it means you turn right 90 degrees. <code>pick_up [object_name]</code>: Use this action to pick up a pickupable and visible object directly in front of you without obstacle. <code>drop [object_name]</code>: it means you places the object in hand at the place just in front of you. <code>toggle [object_name]</code>: it means you toggle a toggleable and visible object directly in front of you without obstacle. <code>stop [answer]</code>: Issue this action when you believe the task is complete. If you believe the task is impossible to complete, provide the answer as \"N/A\" in the bracket. To be successful, it is very important to follow the following rules: 1. You should only issue a action or your thought that is valid given the current observation 2. You should only issue ONE action at a time, and ONLY action is enough. 3. Generate the action in the correct go to it. &gt;OK. &gt;Act: move_ahead &gt;Feedback: Action succeeded. Moved forward by 1 step. Obs: You can see a red box. &gt;Thought: I am moving along the border now, I have taken 3 steps and am still facing EAST after move ahead at last step. I need to go to the red box. &gt;OK. &gt;Act: move_ahead() &gt;Feedback: Action failed. Can not move ahead, because there is an obstacle ahead. Obs: You can see a red box. &gt;Thought: I see a red box and can not move ahead, I have taken 3 steps and am still facing EAST since last action failed. I have reached the red box. &gt;OK. &gt;Act: stop[]. Task: Task: {task} Your memory from last trails is: { memory} Obs: {observation} What is your next step? Try to summarize your status, recall what you have done and think before act. H Examples of different task prompts H.1 Instruction Grounding System: You are an autonomous intelligent agent tasked with completing a minigrid game. These tasks will be accomplished through the use of specific actions you can issue. Here's the information you'll have: 5. You can only operate the object you are facing, if the object is in front of you, on your left or right, take action like move_ahead, turn_left, turn_right to appraoch it until you are facing it; 6. Issue stop action when you think you have achieved the objective. Don't generate anything after stop. Here are some examples: Task: go to the red box. Obs: You can see a blue key in front of you; You can see a red box on your right. Manipulable object: A blue key. &gt;Thought: To solve this task, I need to find a red box. There is a blue key just before me and blocked my way. The red box is on my right, so I should turn_right first to go to the red box. &gt;Act: turn_right. Respond YES if you can play this game. Assistant: YES System: Task: pick up the red key Obs: You can see a yellow door, a yellow key in front of you; You can see a green door on your left; You can see a green box,a red key on your right. What is your next step? Assistant: Thought: To accomplish the task, I need to pick up the red key. The red key is on my right, so the first step is to turn right. System: OK. Assistant: Act: turn_right. System: Feedback: Action succeeded. Turned right by '90' degrees. Obs: You can see a red key in front of you; You can see a green door,a blue key,a green box,a yellow key on your left; You can see a purple ball,a purple door on your right. System: Feedback: Action succeeded. You picked a red key up. Inventory: You are now holding a red key. Obs: You can see a green door,a blue key on your left. System: [SUCCESS] You have completed the task. Congratulations! System: DONE! H.2 Rearrangement System: You are an autonomous intelligent agent tasked with rearranging a vitual home. System will randomly change between 1 to 5 objects in the room. Your goal is to identify which objects have changed and reset those objects to their original state. Here are the information you will have : Original status: The original room status that you should remember, pay your attention to the position and openness states of each of the objects. Then find out all the changed objects and reset them to their origianl states. Obs: These are the objects you can see . You can see at most 8.0 step(s) in front of you; 60 degrees on your left, 60 degrees on your right. Inventory: These are the objects you are holding. You can hold at most 1 objects. Feedback: Whether the action is succeed or not and why is it. At each step, you can choose to think your plan or execute an action from one of the following: <code>move_ahead</code>: it means you move ahead by 1 step. <code>turn_left</code>: it means you turn left 90 degrees. see a pen_1, because I am in the south of the room, so the pen_1 is in the south of the room, but I remember the pen_1 should be in the north of the room according to the origianl status. So I need to pick it up and put it back. OK. &gt; Act: move_ahead Respond YES if you can play this game. Rearrange Agent: YES System: Let's start. Original state: In the north of the room, there is a cabinet_2; a cabinet_3; a cabinet_13; a cabinet_16; a cabinet_19; a cabinet_22; a microwave_0; a sink_0; a sink_1, in/on it you can see a dishsponge_0. In the northeast of the room, there is a lightswitch_0. In the east of the room, there is a cabinet_7; a cabinet_20; a countertop_0, in/on it you can see a coffeemachine_0, a fork_0, a papertowelroll_0, a peppershaker_0, a saltshaker_0, a spatula_0, a spoon_0; a drawer_0; a drawer_1; a garbagecan_0; a stoveknob_2. In the southeast of the room, there is a cabinet_17; a countertop_1, in/on it you can see a cup_0, a plate_0; a drawer_2 ; a stoveburner_0; a stoveburner_1 ; a stoveburner_2, in/on it you can see a pot_0; a stoveburner_3; a stoveknob_0; a stoveknob_1; a stoveknob_3. In the south of the room, there is a diningtable_0, in /on it you can see a apple_0, a bread_0, a lettuce_0, a mug_0, a potato_0, a tomato_0; a stool_0; a window_0. In the southwest of the room, there is a cabinet_5; a cabinet_12; a cabinet_14; a cabinet_21. In the west of the room, there is a cabinet_0; a stool_0; a window_0. On your right, you see a cabinet_3; a cabinet_22; a faucet_0. Assistant: Thought: Now that the fridge is closed, I can see the original status of the room. I need to check if there are any changes in the room. Assistant: Act: turn_left. System: Feedback: Action succeeded. Turned left by '90' degrees. Obs: In front of you, You see a diningtable_0; a apple_0; a dishsponge_0; a spoon_0; a window_0. On your left, you see a bread_0; a butterknife_0; a lettuce_0; a potato_0; a saltshaker_0; a soapbottle_0; a tomato_0; a stool_0; a stoveburner_0; a pot_0; a stoveburner_3. On your right, you see a cabinet_17; a countertop_1; a cup_0; a plate_0; a drawer_2; a fridge_0; a stoveburner_1; a stoveburner_2; a stoveknob_0; a stoveknob_3. Assistant: Thought: I can see a dishsponge in the dining table which was not there before. So, I need to pick it up and put it back to its original position. Assistant: Act: pick_up dishsponge_0. System: Feedback: Action succeeded. You picked dishsponge_0 up. Inventory: dishsponge_0. Obs: In front of you, You see a diningtable_0; a apple_0; a spoon_0; a window_0. On your left, you see a bread_0; a butterknife_0; a lettuce_0; a potato_0; a saltshaker_0; a soapbottle_0; a tomato_0; a stool_0; a stoveburner_0; a pot_0; a stoveburner_3. On your right, you see a cabinet_17; a countertop_1; a cup_0; a plate_0; a drawer_2; a fridge_0; a H.4 Household System: You are an autonomous intelligent agent tasked with navigating a vitual home. You will be given a household task. These tasks will be accomplished through the use of specific actions you Here's the information you'll have: Task: This is the task you are trying to accomplish. Obs: These are the objects you can see . Inventory: These are the objects you are holding. Feedback: Whether the action is succeed or not and why is it. At each step, you can choose to think your plan or execute an action from one of the following: <code>move_ahead</code>: it means you move ahead by 1 step. <code>turn_left</code>: it means you turn left 90 degrees. <code>turn_right</code>: it means you turn right 90 degrees. <code>pick_up [object_name]</code>: Use this action to pick_up a pick_upable and visible object within your manipulation distance. <code>drop [object_name]</code>: it means you places the object in hand at the place just in front of it. <code>open [object_name]</code>: it means you open an openable and visible object within your manipulation distance. <code>close [object_name]</code>: it means you close a closeable and visible object. <code>toggle_on [object_name]</code>: it means you toggle a toggleable and visible object on. <code>toggle_off [object_name]</code>: it means 1. You should only issue an action or your thought that is valid given the current observation 2. You should only issue one action at a time. 3. Generate the action in the correct format starting with "Act: ". For example, "Act: go_grab [phone_0].". 4. State your thought or think process starting with "Thought: " 5. Issue stop action when you think you have achieved the objective. Don't generate anything after stop. 6. You should generate chats like this : chat [Hello, how can I help you ?], taking care to enclose them in square brackets. Respond YES. if you can play this game . System: You are an autonomous intelligent agent tasked with navigating a virtual home. Your name is Bob. You are hurry to finish a housework with your friend Alice together. There are four kinds of rooms, livingroom, kitchen, bedroom, bathroom. You will be given a household task. These tasks will be accomplished through the use of specific actions you and your friend can issue. You have two hands to pickup, hold, put and check objects. Here's the information you will have: starting with "Thought: " 5. Issue stop action when you think you have achieved the objective. Don't generate anything after stop. 6. You should generate chats like this : chat [Hello, how can I help you ?], taking care to enclose them in square brackets. Respond YES. if you can play this game . Alice: YES. System: Task: Find and put 1 wine onto the coffeetable_0. Obs: You have already found and put kitchencabinet_1, kitchencabinet_2, kitchencabinet_3, kitchencabinet_4, kitchencabinet_5, kitchencabinet_6, kitchencabinet_7, stove_0, dishwasher_0, fridge_0, fridge_1, microwave_0. You don't know where Bob is. The livingroom is unexplored. The bathroom is unexplored. What is your next step? Let's think and act. Bob: YES. System: Task: Find and put 1 wine onto the coffeetable_0. Obs: You have already found and put pudding_0, juice_0, juice_1 onto the coffeetable_0. You are holding nothing. You are in the bedroom, where you found an unchecked container cabinet_0. You don't know where Alice is. The livingroom is unexplored. The kitchen is unexplored. The bathroom is unexplored. What is your next step? Let's think and act. Alice: Thought: The task is to find and put 1 wine onto the coffeetable_0. I am currently in the kitchen with several unchecked containers. I will start by checking these containers as the wine could be in any of them. Alice: Act: go_check [kitchencabinet_0 ]. System: Feedback: Action succeeded. You opened kitchencabinet_0. Obs: In it you see nothing Bob: Thought: Since I need to find the wine and I am currently in an unexplored bedroom, I'll first check the unchecked container in the bedroom, the cabinet_0. If the wine is not there, I'll move to the next unexplored room. Alice: Thought: The first kitchen cabinet was empty. I'll check the next one. Alice: Act: go_check [kitchencabinet_1 ] System: Feedback: Action succeeded. You opened kitchencabinet_1. Obs: In it you see nothing cabinet_0 in the bedroom, so I need to explore other rooms. The kitchen is a good place to start as it is common to store wine there. Bob: Act: go_explore [kitchen] System: Feedback: Action succeeded. Go to kitchen_0. You have already found and put pudding_0, juice_0, juice_1 onto the coffeetable_0. You are holding nothing. You are in the kitchen, where you found unchecked containers kitchencabinet_2, kitchencabinet_3, kitchencabinet_4, kitchencabinet_5, kitchencabinet_6, kitchencabinet_7, stove_0, dishwasher_0, fridge_0, fridge_1, microwave_0. You also see Alice here in the kitchen, she is holding nothing. The livingroom unexplored. You found nothing in the bedroom. The bathroom is unexplored. Alice: Act: go_check [kitchencabinet_2 ] System: Feedback: Action succeeded. You opened kitchencabinet_2. Obs: In it you see book_1 Bob: Act: go_check [fridge_0] System: Feedback: Action succeeded. You opened fridge_0. Obs: In it you see wine_0 Alice: Act: go_check [kitchencabinet_3 ] System: Feedback: Action succeeded. You opened kitchencabinet_3. Obs: In it you see nothing the coffeetable_0. You are holding wine_0. You are in the kitchen, where you found unchecked containers kitchencabinet_4, kitchencabinet_5, kitchencabinet_6, kitchencabinet_7, stove_0, dishwasher_0, fridge_1, microwave_0. You also see Alice holding nothing. The livingroom is unexplored. You found nothing in the bedroom. The bathroom is unexplored. Alice: Act: go_check [kitchencabinet_4 ] System: Feedback: Action succeeded. You opened kitchencabinet_4. Obs: In it you see nothing Bob: Act: go_explore [livingroom] System: Feedback: Action succeeded. Go to livingroom_0. You have already found and put pudding_0, juice_0, juice_1 onto the coffeetable_0. You are holding wine_0. You are in the livingroom, where you found coffeetable_0. Last time you saw Alice was in the kitchen, she was holding nothing. You found unchecked containers kitchencabinet_5, kitchencabinet_6, kitchencabinet_7, stove_0, dishwasher_0, fridge_1, microwave_0 in the kitchen. You found nothing in the bedroom. The bathroom is unexplored. Alice: Act: go_check [kitchencabinet_5 ] System: Feedback: Action succeeded. You opened kitchencabinet_5. Obs: In it you see book_2 Bob: Act: go_put [coffeetable_0] System: Feedback: Action succeeded. You put wine_0 on coffeetable_0. Obs: You have already found and put pudding_0, juice_0, juice_1, wine_0 to the coffeetable_0. You are holding nothing. You are in kitchencabinet_5, kitchencabinet_6, kitchencabinet_7, stove_0, dishwasher_0, fridge_1, microwave_0 in the kitchen. You found nothing in the bedroom. The bathroom is unexplored. System: [SUCCESS] You have completed the task. Congratulations! Moved forward by 1 step. Obs: You see nothing. You can try to take action like move_ahead, can issue. four kinds of rooms, livingroom, unexplored. The bedroom is Bob: Thought: The wine is not in the here in the kitchen, she is System: DONE!your right. .You can see at most { when facing NORTH. I haven taken 2 3. Generate the action in the correct Assistant: Thought: I am now facing there is a chair_0;. dishsponge_0; a lettuce_0; a Assistant: Thought: Found it! the answer as "N/A" in the pudding_0, juice_0, juice_1 onto Bob: Act: go_grab [wine_0]starting with \"Thought: \" format starting with \"Act: \". Task: This is the task you are trying Assistant: Thought: Now that I'm <code>turn_right</code>: it means you turn right cabinet_4; a cabinet_9; a stoveburner_1; a stoveburner_2; a you toggle a toggleable and Bob: Act: go_check [cabinet_0]. the livingroom, where you foundManipulable object: The object that max_view_steps} step(s) in front steps and am facing EAST after format starting with "Act: ". For the red key and can pick it up. Obs: You can see nothing ahead. potato_0; a saltshaker_0; a Assistant: Act: answer [True] bracket. the coffeetable_0. You are holding System: Feedback: Action succeeded.5. You can only operate the object you For example, \"Act: pick_up [cup_0 to accomplish. facing the red key, I can pick it 90 degrees. cabinet_11; a fridge_0, in/on it stoveknob_0; a stoveknob_3. visible object off. System: Feedback: Action succeeded. coffeetable_0. Last time you sawyou are facing and you can take an of you, {side_steps} step(s) on turn right at last step. The example, "Act: pick_up [cup_0]". System: OK. &gt; Thought: To solve the task, I need soapbottle_0; a spoon_0; a System: Your are right! nothing. You are in the kitchen, You picked wine_0 up.are facing, if the object is in ]\". Obs: These are the objects you can see up to accomplish my task. <code>pick_up [object_name]</code>: Use this you can see a egg_0. In the Assistant: Thought: I need to put the <code>slice [object_name]</code>: it means you You opened cabinet_0. Alice was in the kitchen, she wasoperate action like pick_up [ your left, {side_steps} step(s) on border is on my leftside. Now I 4. State your thought or think process Assistant: Act: pick_up [red key]. to explore the room to find out tomato_0; a drawer_3; a System: DONE! To be successful, it is very important where you found unchecked Obs: You have already found and putfront of you, on your left or 4. State your thought or think process .You can see at most 7 step(s) in System: OK. action to pick up a pickupable and northwest of the room, there is a dishsponge back to the sink_0 slice a sliceable and visible Obs: In it you see book_3 holding nothing. You foundobject_name], toggle [object_name] your right. find the red box. Next I need to starting with "Thought: " what objects have been changed. I microwave_0; a sink_0; a sink_1; a to follow the following rules: containers kitchencabinet_0, pudding_0, juice_0, juice_1 ontoright, take action like move_ahead, starting with \"Thought: \" front of you, 3 step(s) on your Assistant: Act: pick_up [red key]. visible object within 8.0 step(s). cabinet_1; a cabinet_6; a where it was originally placed. object. unchecked containers
We did not fully test some open-source LLMs with small sizes, such as LLaMA, because we made a preliminary attempt to utilize LLaMA
-7B to solve Instruction Grounding and IQA tasks but found it failed in all tasks
Obs: In front of you, You see a cabinet_0; a cabinet_11; a fridge_0. On your left, you see a cabinet_1; a cabinet_2; a cabinet_4; a cabinet_6; a cabinet_9; a cabinet_10; a cabinet_18; a countertop_2; a bowl_0; a faucet_1; a knife_0; a pan_0; a toaster_0; a diningtable_0; a apple_0; a bread_0; a butterknife_0; a lettuce_0; a potato_0; a saltshaker_0; a soapbottle_0; a spoon_0; a tomato_0; a drawer_3; a stool_0; a window_0. On your right, you see a cabinet_3; a cabinet_5; a cabinet_12; a cabinet_13; a cabinet_14; a cabinet_21; a faucet_0; a microwave_0; a sink_0; a sink_1. Assistant: Thought: I am in front of the sink_0. I need to drop the dishsponge_0 to put it back to its original position. Assistant: Act: drop dishsponge_0 in sink_0. System: Feedback: Action succeeded. You dropped dishsponge_0. Inventory: Empty Obs: In front of you, You see a cabinet_0; a cabinet_11; a fridge_0; a dishsponge_0. On your left, you see a cabinet_1; a cabinet_2; a cabinet_4; a cabinet_6; a cabinet_9; a cabinet_10; a cabinet_18; a countertop_2; a bowl_0; a faucet_1 ; a knife_0; a pan_0; a toaster_0; a diningtable_0; a apple_0; a bread_0; a butterknife_0; a lettuce_0; a potato_0; a saltshaker_0; a soapbottle_0; a spoon_0; a tomato_0; a drawer_3; a stool_0; a window_0. On your right, you see a cabinet_3; a cabinet_5; a cabinet_12; a cabinet_13; a cabinet_14; a cabinet_21; a faucet_0; a microwave_0; a sink_0; a sink_1. Assistant: Thought: I have successfully reset the changed objects to their original states. Now, there is no change in the room. I can stop here. Assistant: Act: stop [] System: You stopped the game.
AcknowledgementsThe authors thank Dr. Wentao Ding for his great contributions in refining code repository and Mrs. Chen Zhen for her design of illustrative teasers.This work presented herein is supported by the National Natural Science Foundation of China (62376031).Zettlemoyer, and Dieter Fox. 2020b. Alfred: A benchmark for interpreting grounded instructions for everyday tasks.In Conference on Computer Vision and Pattern Recognition (CVPR), pages 10740-10749.System: {'misplaced': 0.0, 'fixed':1.0} System:[SUCCESS]
Task: Move the keys to the computer on the sofa. Obs: In front of you, You see a faucet_0; a sink_0; a sink_1. On your left, you see a cabinet_0</p>
<p>On your right, you see a countertop_0. a coffeemachine_0; a toaster_0; a cabinet_2; a fridge_0; a glassbottle_1. a butterknife_0; a bread_0; a knife_0; a apple_0; a saltshaker_0; a potato_0; a butterknife_1; a cabinet_4; a cabinet_6; a cabinet_7; a drawer_1; a cabinet_9; a spoon_1</p>
<p>Act: turn_right. Then Act: move_ahead. Feedback: Action succeeded. Obs: In front of you. You see ……</p>
<p>Feedback: Action succeeded. Obs: In front of you. Then Act: move_ahead. Then Act: put. Act: turn_left. You see a [11] Act: stop [keys are already put on the sofa</p>
<p>You stopped the game. failed to complete the task</p>
<p>Thought: I am facing north at the beginning. </p>
<p>Act: put [keychain_0, laptop_0] Feedback: Action succeeded. You put keychain_0 into/onto laptop_0. Obs: In front of you. </p>
<p>Thought: I have successfully placed the keys from the keychain_0 onto the laptop_0 on the sofa_0. The task is complete. </p>
<p>Act: put [keychain_0, computer_0] Feedback: Action failed. The object is not provided or not in sight. You can only manipulate the object in sight. </p>
<p>Thought: I need to be closer to the computer to put the keys on it. </p>
<p>Act: move_ahead Feedback: Action succeeded. Obs: You see nothing ……. </p>
<p>Act: turn_left. turn_left. Feedback: Action succeeded. Turn left by '90' degrees. </p>
<p>Thought: I have successfully moved forward and now I am next to the opened laptop. I can now put the keys on the laptop</p>
<p>Act: put [keychain_0, laptop_0] Feedback: Action failed. Object keychain_0 is not in your inventory. </p>
<p>Thought: Oops, I forgot to pick up the keys before trying to put them on the laptop. I need to pick up the keys first. </p>
<p>Act: pick_up [keychain_0] Feedback: Action succeeded. 0</p>
<p>Act: stop. The keys have been moved to the computer on the sofa.] You stopped the game [SUCCESS] You have completed the task</p>
<p>References Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, arXiv:2204.01691Do as i can, not as i say: Grounding language in robotic affordances. 2022arXiv preprint</p>
<p>Video pretraining (vpt): Learning to act by watching unlabeled online videos. Bowen Baker, Ilge Akkaya, Peter Zhokov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon Houghton, Raul Sampedro, Jeff Clune, Advances in Neural Information Processing Systems (NeurIPS). 202235</p>
<p>Vector-based navigation using grid-like representations in artificial agents. Andrea Banino, Caswell Barry, Benigno Uria, Charles Blundell, Timothy Lillicrap, Piotr Mirowski, Alexander Pritzel, Martin J Chadwick, Thomas Degris, Joseph Modayil, Nature. 55777052018</p>
<p>Robustnav: Towards benchmarking robustness in embodied navigation. Prithvijit Chattopadhyay, Judy Hoffman, Proceedings of the IEEE/CVF Interna. the IEEE/CVF Interna2021Roozbeh Mottaghi, and Aniruddha Kembhavi</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5012 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5012</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5012</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-107.html">extraction-schema-107</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <p><strong>Paper ID:</strong> paper-252760744</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2305.03742v1.pdf" target="_blank">Improved Logical Reasoning of Language Models via Differentiable Symbolic Programming</a></p>
                <p><strong>Paper Abstract:</strong> Pre-trained large language models (LMs) struggle to perform logical reasoning reliably despite advances in scale and compositionality. In this work, we tackle this challenge through the lens of symbolic programming. We propose DSR-LM, a Differentiable Symbolic Reasoning framework where pre-trained LMs govern the perception of factual knowledge, and a symbolic module performs deductive reasoning. In contrast to works that rely on hand-crafted logic rules, our differentiable symbolic reasoning framework efficiently learns weighted rules and applies semantic loss to further improve LMs. DSR-LM is scalable, interpretable, and allows easy integration of prior knowledge, thereby supporting extensive symbolic programming to robustly derive a logical conclusion. The results of our experiments suggest that DSR-LM improves the logical reasoning abilities of pre-trained language models, resulting in a significant increase in accuracy of over 20% on deductive reasoning benchmarks. Furthermore, DSR-LM outperforms a variety of competitive baselines when faced with systematic changes in sequence length.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5012.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5012.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DSR-LM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Differentiable Symbolic Reasoning for Language Models (DSR-LM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neuro-symbolic framework that tightly integrates a pretrained language model for perception (relation extraction) with a differentiable symbolic deductive reasoner (Scallop) that learns weighted logic rules and semantic loss end-to-end to improve strict logical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DSR-LM (framework with RoBERTa backbone)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Framework using a pretrained LM (RoBERTa-base by default) as a relation extractor M_theta and the Scallop differentiable deductive engine P_phi that learns weighted logical rules, propagates probabilities via approximated weighted model counting (WMC), and uses semantic loss from integrity constraints; trains two optimizers separately for LM fine-tuning and rule weights.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>127M</td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>CLUTRR (kinship reasoning) and DBpedia-INF (rule-based KB reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>CLUTRR: synthetic but natural-language-stylized multi-hop kinship reasoning where target relation is not explicit and requires composition over k facts; DBpedia-INF: synthetic passages with soft deductive rules mined from DBpedia to test rule-based multi-hop inference (k up to 5).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Tight neuro-symbolic integration: pretrained LM extracts probabilistic relational atoms from text; differentiable symbolic module (Scallop) performs probabilistic deductive inference using weighted learned rules; rule weights initialized via LM prompting and fine-tuned; semantic loss derived from integrity constraints regularizes learning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>On CLUTRR (trained on k in {2,3}): DSR-LM (RoBERTa backbone) achieves 60.98 ± 2.64% accuracy (Table cited). DSR-LM-with-Manual-Rule: 61.34 ± 1.56%. On DBpedia-INF: DSR-LM outperforms RuleBert by ~37% overall (paper reports 37% relative gain). DSR-LM degrades more slowly with increasing reasoning length k than pure neural baselines (figures reported).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Primary failure mode is relation-extraction errors cascading into incorrect deductions (example: misclassifying 'father-daughter dance' as husband-wife). Performance still drops as reasoning length k increases, because accumulated relation-extraction error reduces final accuracy. Rule initialization from LMs can be noisy (GPT-3 produced ~62% correct composition rules initially), requiring fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Outperforms a variety of baselines (pure neural LMs, structured models, and neuro-symbolic baselines) on CLUTRR and DBpedia-INF; notably outperforms RuleBert on DBpedia-INF by 37%. Compared to GPT-3 variants, DSR-LM achieves much higher systematic generalization on long reasoning chains. With ground-truth KB input (DSR-without-LM), framework achieves near-perfect reasoning, demonstrating that symbolic module itself is strong.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Ablations show: (1) Backbones—RoBERTa and DeBERTa give similar high performance; w2v+BiLSTM is weaker but still better than other baselines. (2) DSR-LM-with-Manual-Rule yields only marginal improvement over learned rules (61.34% vs 60.98%), implying effective rule learning. (3) Removing integrity constraints (DSR-LM-without-IC) degrades performance substantially (51.48 ± 0.57%). (4) Relation-extractor isolated accuracy ~84.69% (weak supervision) vs 85.32% when trained with intermediate labels. (5) DSR-without-LM (ground-truth KB input) reaches 98.81% vs GAT 39.05 and CTP 95.57, demonstrating strong symbolic reasoning when perception is perfect.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improved Logical Reasoning of Language Models via Differentiable Symbolic Programming', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5012.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5012.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DSR-without-LM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DSR (without LM) — ablation taking ground-truth KBs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ablated variant of DSR-LM that takes structured ground-truth entity-relation graphs as input so only the differentiable rule-learning and deductive reasoning components are trained; isolates symbolic reasoning capability from noisy perception.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DSR-without-LM</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same differentiable symbolic reasoning and rule-learning components as DSR-LM (Scallop-based), but uses ground-truth structured KBs as input instead of extracting relations from natural language; rule weights are learned (or randomized) during training.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>CLUTRR (kinship reasoning) — generalization to longer chains</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given ground-truth kinship graphs, learn/learned rules to deduce query relations for k in [4,10] despite training on short k in {2,3}.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Differentiable rule learning with WMC-based probabilistic inference in Scallop; no neural perception component.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Achieves 98.81% accuracy on CLUTRR (Table 7) when trained on k∈[2,3] and tested on k∈[4,10], outperforming GAT (39.05%) and CTP (95.57%) on the same setup.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Relies on access to ground-truth KBs (not realistic for text-only tasks); not reflective of perception errors which dominate full end-to-end pipeline failures.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Strongly outperforms structured baselines on KB reasoning (GAT and CTP); shows symbolic rule learning and inference are highly effective when perception is perfect.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Demonstrates the symbolic component's capability to generalize to longer chains even when rule weights are randomized or learned—confirming that major remaining errors in DSR-LM are due to relation extraction/perception.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improved Logical Reasoning of Language Models via Differentiable Symbolic Programming', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5012.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5012.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3 (variants)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 3 (Zero-shot, Few-shot, Fine-tuned, Zero-shot-CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Large autoregressive language model evaluated in multiple prompting/fine-tuning regimes (zero-shot, few-shot, fine-tuned, and chain-of-thought prompting) on CLUTRR; used as a competitive baseline for long-range logical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (175B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive transformer language model (GPT-3) evaluated in zero-shot (ZS), few-shot (5-shot), fine-tuned (FT), and zero-shot chain-of-thought (ZS-CoT) prompting variants; used both with and without inclusion of natural-language rules in the prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B</td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>CLUTRR (kinship reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Answer the kinship relation between two entities from a story; requires multi-hop compositional logical deduction over k facts.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Prompting strategies: Zero-shot, Few-shot, Fine-tuning on training data, Few-shot with rules (include hand-crafted rules in prompt), and zero-shot chain-of-thought prompting that requests step-by-step reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>GPT-3 Zero-Shot (ZS) achieved 28.6% accuracy on CLUTRR; Zero-Shot-CoT scored 25.6% (paper reports chain-of-thought didn't help and sometimes hurt); fine-tuned and few-shot variants show variability but generally lower systematic generalization than DSR-LM on long chains. GPT-3 often returns plausible but logically inconsistent chains.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Prone to preferring complicated or incorrect relation labels (e.g., 'stepdaughter' instead of 'daughter'); can produce logically inconsistent intermediate chains (correct final answer via incorrect reasoning); chain-of-thought prompting can fail on long-range kinship reasoning; performance relatively insensitive to reasoning length (ZS and ZS-CoT stable) but still low absolute accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Inferior to DSR-LM on CLUTRR long-range generalization; even GPT-3 fine-tuned variants and few-shot variants do not match DSR-LM's systematic generalization. GPT-3 FT and other neural methods drop in accuracy as k increases more than GPT-3 ZS variants, but absolute performance remains below DSR-LM.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Qualitative analysis shows GPT-3 makes systematic logical fallacies and inconsistent chains; inclusion of hand-crafted rules in the prompt can be attempted but does not achieve DSR-LM-level generalization. The paper reports GPT-3's prompted extraction of composition rules was only ~62% correct initially.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improved Logical Reasoning of Language Models via Differentiable Symbolic Programming', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5012.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5012.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RoBERTa</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RoBERTa-base (pretrained transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pretrained transformer-based encoder (RoBERTa-base) used as the primary LM backbone in DSR-LM for relation extraction and fine-tuned within the DSR-LM pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RoBERTa-base</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained transformer encoder used as the relation extraction backbone; embeddings of entity spans and windowed context are fed to an MLP classifier for relation prediction; fine-tuned with a low learning rate.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>123M</td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>CLUTRR / DBpedia-INF relation extraction component for downstream logical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Extract probabilistic relational atoms (p(subject,object)) from windowed context to be fed into symbolic reasoner; weak supervision from final QA labels.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Fine-tuned within DSR-LM with binary cross-entropy on probabilistic relation labels propagated via differentiable reasoning; uses concatenated embeddings of two entities plus window embedding into 2-layer MLP classifier.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>In isolation, the trained relation extractor achieves 84.69% accuracy on single-relation classification under weak supervision; fully supervised relation extractor reaches 85.32%. With RoBERTa backbone, DSR-LM reaches ~60.98% accuracy on CLUTRR overall.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Perception errors (misclassifying relations) are the dominant failure mode for the full pipeline; these errors accumulate over multi-hop chains causing accuracy drop as k increases.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>RoBERTa and DeBERTa backbones give similar performance in DSR-LM; both outperform word2vec+BiLSTM backbone. RoBERTa-based DSR-LM reached best performance within ~20 epochs while w2v+BiLSTM required ~40.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Ablation shows transformer backbones (RoBERTa, DeBERTa) yield higher final accuracy and faster convergence than word2vec+BiLSTM; relation extractor performance close to fully supervised despite weak supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improved Logical Reasoning of Language Models via Differentiable Symbolic Programming', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5012.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5012.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeBERTa</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeBERTa-base (pretrained transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pretrained transformer encoder (DeBERTa-base) used as an alternative backbone in DSR-LM ablations, showing comparable performance to RoBERTa.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeBERTa-base</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained transformer encoder used as a relation extraction backbone and fine-tuned inside DSR-LM (variant: DSR-LM-DeBERTa).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>CLUTRR relation extraction and downstream logical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same as RoBERTa: extract relations from text windows for input to symbolic reasoner.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Fine-tuned inside DSR-LM; same relation-classifier MLP architecture.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported to have on-par performance with RoBERTa in DSR-LM (table summary), converging within ~20 epochs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Same perception-driven limitations as other transformer backbones (relation-extraction errors can cascade).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Performs on-par with RoBERTa and both outperform word2vec+BiLSTM backbone in DSR-LM.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Ablation shows DeBERTa and RoBERTa give similar final accuracy and training speed advantages over word-embedding+encoder baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improved Logical Reasoning of Language Models via Differentiable Symbolic Programming', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5012.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5012.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>w2v-BiLSTM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>word2vec + BiLSTM (DSR-w2v-BiLSTM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A non-transformer baseline used as a relation-extraction backbone in DSR-LM ablations, combining 300-d word2vec embeddings with a BiLSTM encoder and an MLP classifier.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>w2v-google-news-300 + BiLSTM</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Word2vec (300-d pretrained Google News vectors) for token embeddings, encoded by a BiLSTM for context; used as the relation-extraction neural backbone in the DSR-LM-w2v-BiLSTM variant.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>CLUTRR relation extraction and downstream logical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Extract relations from text windows for symbolic reasoning; serves to test backbone sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Fine-tuned (encoder and classifier) within DSR-LM framework, but slower convergence and lower final performance than transformer backbones.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>DSR-w2v-BiLSTM attains lower final accuracy than transformer-backed DSR-LM variants but still outperforms many other baselines; requires ~40 epochs to peak vs ~20 for transformer backbones.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Weaker perception and slower training; lower final accuracy than transformer-based backbones.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Although worse than RoBERTa/DeBERTa-backed DSR-LM, w2v-BiLSTM variant still performs better than many purely neural or structured baselines (exact numbers in Table 5 of the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Demonstrates that choice of neural backbone matters: transformer backbones give higher accuracy and faster convergence.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improved Logical Reasoning of Language Models via Differentiable Symbolic Programming', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5012.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5012.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RuleBert</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RuleBert (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior neuro-symbolic baseline that fine-tunes language models with soft rules and facts to improve logical reasoning; used as a baseline on DBpedia-INF.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>RuleBert</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RuleBert</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A method that augments LM fine-tuning with soft rules and facts to encourage logical inference; evaluated on rule-based reasoning datasets like DBpedia-INF.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>DBpedia-INF (rule-based KB reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Deduce relations using synthetic passages and soft deductive rules mined from DBpedia; tests capacity to leverage soft rules during LM fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Fine-tuning with soft rules and facts (data augmentation / soft logical constraints) rather than a tightly integrated differentiable symbolic reasoner.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>On DBpedia-INF, DSR-LM outperforms RuleBert by ~37% (paper reports DSR-LM's substantial advantage), indicating RuleBert's approach is less robust to systematic generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Paper argues that augmenting data alone (fine-tuning with soft rules) does not effectively improve systematic generalization to longer chains compared to tight neuro-symbolic integration.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>DSR-LM shows much stronger generalization than RuleBert on DBpedia-INF, implying the differentiable symbolic integration and rule learning approach is more effective than RuleBert's fine-tuning with soft rules.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Paper reports RuleBert baseline results on DBpedia-INF and uses them to demonstrate DSR-LM's relative gain; no internal ablation for RuleBert reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improved Logical Reasoning of Language Models via Differentiable Symbolic Programming', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5012.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e5012.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GAT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph Attention Network (GAT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph-structured neural baseline that operates on structured entity-relation graphs (rather than raw text) and is compared in KB reasoning experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GAT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A graph neural network using attention over neighbors to perform relational reasoning over structured KB graphs; used as a baseline when structured KBs are available.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>CLUTRR KB reasoning (structured KB input)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Learn to answer kinship queries given the structured entity-relation graph; isolates graph-based relational inductive biases.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Supervised graph neural network trained on structured KBs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>On structured KB reasoning (trained on k∈[2,3], tested on k∈[4,10]) GAT achieves 39.05% accuracy, substantially below DSR-without-LM (98.81%).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Performs poorly on generalization to longer reasoning chains compared to differentiable rule-learning approach (DSR-without-LM).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Underperforms both DSR-without-LM and CTP on the KB reasoning benchmark used in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Used as a baseline to show benefit of differentiable symbolic rule learning versus GNN approaches on KB reasoning generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improved Logical Reasoning of Language Models via Differentiable Symbolic Programming', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5012.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e5012.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CTP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CTP (Closed-World Probabilistic reasoning model, baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neuro-symbolic/structured baseline that operates on structured KBs; included in comparisons for KB reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CTP</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A probabilistic/neuro-symbolic approach that reasons over structured KBs (specific implementation referenced in prior work Minervini et al., 2020), used as a high-performing baseline on KB reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>CLUTRR KB reasoning (structured KB input)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Deduce multi-hop kinship relations using structured KBs and rule-like reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Structured probabilistic/neuro-symbolic reasoning over KBs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>On structured KB reasoning (same setting), CTP achieves 95.57% accuracy, slightly below DSR-without-LM (98.81%) but much higher than GAT.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Less accurate than DSR-without-LM in this evaluation; details of failure modes not elaborated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Competitive with DSR approaches when given structured input, but DSR-without-LM still exceeds it in accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Serves as a strong baseline when the KB is structured; comparison highlights the power of the paper's differentiable rule-learning pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improved Logical Reasoning of Language Models via Differentiable Symbolic Programming', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5012.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e5012.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DSR-LM-without-IC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DSR-LM without Integrity Constraints (ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ablation of DSR-LM that removes integrity constraints and the semantic-loss term to assess the effect of logical integrity regularization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DSR-LM-without-IC</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same pipeline as DSR-LM but excludes integrity constraints and the semantic loss component that penalizes logical violations during training.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>CLUTRR (kinship reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Evaluate how removal of integrity constraints affects downstream multi-hop reasoning accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Ablation removing semantic loss derived from integrity constraints on predicted relations and rules.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported accuracy drops to 51.48 ± 0.57% on CLUTRR (Table 6), which is substantially worse than full DSR-LM (~60.98%).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Shows that integrity constraints are an essential component for robustness; without them, performance suffers significantly.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Performs worse than full DSR-LM and DSR-LM-with-Manual-Rule, demonstrating the importance of semantic loss regularization.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Confirms that semantic loss (integrity constraints) improves robustness and generalization of learned rules and relation extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improved Logical Reasoning of Language Models via Differentiable Symbolic Programming', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5012.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e5012.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DSR-LM-with-Manual-Rule</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DSR-LM with Manual Rules (ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A DSR-LM variant where hand-crafted logic rules (external KB) are provided instead of learning them, used to measure the benefit of learned versus manual rules.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DSR-LM-with-Manual-Rule</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>DSR-LM where 92 manually crafted kinship composition rules are injected and set to weight 1.0, disabling rule learning for those rules; otherwise same pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>CLUTRR (kinship reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Use gold/manual composition rules as the symbolic KB to drive deductive inference from extracted relations.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Inject human-crafted rules into Scallop program instead of initializing via LM prompting and fine-tuning; semantic loss and relation extraction remain in place.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported accuracy 61.34 ± 1.56% on CLUTRR, a modest 0.36% gain over learned-rule DSR-LM (60.98%), indicating learned rules are nearly as effective as manual ones.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Marginal gain implies human effort to hand-craft rules gives limited advantage when the rule-learning pipeline is effective.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Slightly better than learned-rule DSR-LM but not substantially; supports claim that rule learning via LM initialization plus fine-tuning is effective.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Shows that learned rules (initialized via LM prompts, fine-tuned) are nearly as good as manual rules; manual rules add only minor improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improved Logical Reasoning of Language Models via Differentiable Symbolic Programming', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>CLUTRR: A Diagnostic Benchmark for Compositional Generalization in Relational Reasoning <em>(Rating: 2)</em></li>
                <li>Scallop: From probabilistic deductive databases to scalable differentiable reasoning <em>(Rating: 2)</em></li>
                <li>Language Models are Few-Shot Learners <em>(Rating: 2)</em></li>
                <li>RuleBert <em>(Rating: 2)</em></li>
                <li>Large language models are zero-shot reasoners <em>(Rating: 2)</em></li>
                <li>Measuring and improving consistency in pretrained language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5012",
    "paper_id": "paper-252760744",
    "extraction_schema_id": "extraction-schema-107",
    "extracted_data": [
        {
            "name_short": "DSR-LM",
            "name_full": "Differentiable Symbolic Reasoning for Language Models (DSR-LM)",
            "brief_description": "A neuro-symbolic framework that tightly integrates a pretrained language model for perception (relation extraction) with a differentiable symbolic deductive reasoner (Scallop) that learns weighted logic rules and semantic loss end-to-end to improve strict logical reasoning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "DSR-LM (framework with RoBERTa backbone)",
            "model_description": "Framework using a pretrained LM (RoBERTa-base by default) as a relation extractor M_theta and the Scallop differentiable deductive engine P_phi that learns weighted logical rules, propagates probabilities via approximated weighted model counting (WMC), and uses semantic loss from integrity constraints; trains two optimizers separately for LM fine-tuning and rule weights.",
            "model_size": "127M",
            "logical_reasoning_task": "CLUTRR (kinship reasoning) and DBpedia-INF (rule-based KB reasoning)",
            "task_description": "CLUTRR: synthetic but natural-language-stylized multi-hop kinship reasoning where target relation is not explicit and requires composition over k facts; DBpedia-INF: synthetic passages with soft deductive rules mined from DBpedia to test rule-based multi-hop inference (k up to 5).",
            "method_or_approach": "Tight neuro-symbolic integration: pretrained LM extracts probabilistic relational atoms from text; differentiable symbolic module (Scallop) performs probabilistic deductive inference using weighted learned rules; rule weights initialized via LM prompting and fine-tuned; semantic loss derived from integrity constraints regularizes learning.",
            "performance": "On CLUTRR (trained on k in {2,3}): DSR-LM (RoBERTa backbone) achieves 60.98 ± 2.64% accuracy (Table cited). DSR-LM-with-Manual-Rule: 61.34 ± 1.56%. On DBpedia-INF: DSR-LM outperforms RuleBert by ~37% overall (paper reports 37% relative gain). DSR-LM degrades more slowly with increasing reasoning length k than pure neural baselines (figures reported).",
            "limitations_or_failure_cases": "Primary failure mode is relation-extraction errors cascading into incorrect deductions (example: misclassifying 'father-daughter dance' as husband-wife). Performance still drops as reasoning length k increases, because accumulated relation-extraction error reduces final accuracy. Rule initialization from LMs can be noisy (GPT-3 produced ~62% correct composition rules initially), requiring fine-tuning.",
            "comparison": "Outperforms a variety of baselines (pure neural LMs, structured models, and neuro-symbolic baselines) on CLUTRR and DBpedia-INF; notably outperforms RuleBert on DBpedia-INF by 37%. Compared to GPT-3 variants, DSR-LM achieves much higher systematic generalization on long reasoning chains. With ground-truth KB input (DSR-without-LM), framework achieves near-perfect reasoning, demonstrating that symbolic module itself is strong.",
            "ablation_or_analysis_results": "Ablations show: (1) Backbones—RoBERTa and DeBERTa give similar high performance; w2v+BiLSTM is weaker but still better than other baselines. (2) DSR-LM-with-Manual-Rule yields only marginal improvement over learned rules (61.34% vs 60.98%), implying effective rule learning. (3) Removing integrity constraints (DSR-LM-without-IC) degrades performance substantially (51.48 ± 0.57%). (4) Relation-extractor isolated accuracy ~84.69% (weak supervision) vs 85.32% when trained with intermediate labels. (5) DSR-without-LM (ground-truth KB input) reaches 98.81% vs GAT 39.05 and CTP 95.57, demonstrating strong symbolic reasoning when perception is perfect.",
            "uuid": "e5012.0",
            "source_info": {
                "paper_title": "Improved Logical Reasoning of Language Models via Differentiable Symbolic Programming",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "DSR-without-LM",
            "name_full": "DSR (without LM) — ablation taking ground-truth KBs",
            "brief_description": "An ablated variant of DSR-LM that takes structured ground-truth entity-relation graphs as input so only the differentiable rule-learning and deductive reasoning components are trained; isolates symbolic reasoning capability from noisy perception.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "DSR-without-LM",
            "model_description": "Same differentiable symbolic reasoning and rule-learning components as DSR-LM (Scallop-based), but uses ground-truth structured KBs as input instead of extracting relations from natural language; rule weights are learned (or randomized) during training.",
            "model_size": null,
            "logical_reasoning_task": "CLUTRR (kinship reasoning) — generalization to longer chains",
            "task_description": "Given ground-truth kinship graphs, learn/learned rules to deduce query relations for k in [4,10] despite training on short k in {2,3}.",
            "method_or_approach": "Differentiable rule learning with WMC-based probabilistic inference in Scallop; no neural perception component.",
            "performance": "Achieves 98.81% accuracy on CLUTRR (Table 7) when trained on k∈[2,3] and tested on k∈[4,10], outperforming GAT (39.05%) and CTP (95.57%) on the same setup.",
            "limitations_or_failure_cases": "Relies on access to ground-truth KBs (not realistic for text-only tasks); not reflective of perception errors which dominate full end-to-end pipeline failures.",
            "comparison": "Strongly outperforms structured baselines on KB reasoning (GAT and CTP); shows symbolic rule learning and inference are highly effective when perception is perfect.",
            "ablation_or_analysis_results": "Demonstrates the symbolic component's capability to generalize to longer chains even when rule weights are randomized or learned—confirming that major remaining errors in DSR-LM are due to relation extraction/perception.",
            "uuid": "e5012.1",
            "source_info": {
                "paper_title": "Improved Logical Reasoning of Language Models via Differentiable Symbolic Programming",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "GPT-3 (variants)",
            "name_full": "Generative Pre-trained Transformer 3 (Zero-shot, Few-shot, Fine-tuned, Zero-shot-CoT)",
            "brief_description": "Large autoregressive language model evaluated in multiple prompting/fine-tuning regimes (zero-shot, few-shot, fine-tuned, and chain-of-thought prompting) on CLUTRR; used as a competitive baseline for long-range logical reasoning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3 (175B)",
            "model_description": "Autoregressive transformer language model (GPT-3) evaluated in zero-shot (ZS), few-shot (5-shot), fine-tuned (FT), and zero-shot chain-of-thought (ZS-CoT) prompting variants; used both with and without inclusion of natural-language rules in the prompt.",
            "model_size": "175B",
            "logical_reasoning_task": "CLUTRR (kinship reasoning)",
            "task_description": "Answer the kinship relation between two entities from a story; requires multi-hop compositional logical deduction over k facts.",
            "method_or_approach": "Prompting strategies: Zero-shot, Few-shot, Fine-tuning on training data, Few-shot with rules (include hand-crafted rules in prompt), and zero-shot chain-of-thought prompting that requests step-by-step reasoning.",
            "performance": "GPT-3 Zero-Shot (ZS) achieved 28.6% accuracy on CLUTRR; Zero-Shot-CoT scored 25.6% (paper reports chain-of-thought didn't help and sometimes hurt); fine-tuned and few-shot variants show variability but generally lower systematic generalization than DSR-LM on long chains. GPT-3 often returns plausible but logically inconsistent chains.",
            "limitations_or_failure_cases": "Prone to preferring complicated or incorrect relation labels (e.g., 'stepdaughter' instead of 'daughter'); can produce logically inconsistent intermediate chains (correct final answer via incorrect reasoning); chain-of-thought prompting can fail on long-range kinship reasoning; performance relatively insensitive to reasoning length (ZS and ZS-CoT stable) but still low absolute accuracy.",
            "comparison": "Inferior to DSR-LM on CLUTRR long-range generalization; even GPT-3 fine-tuned variants and few-shot variants do not match DSR-LM's systematic generalization. GPT-3 FT and other neural methods drop in accuracy as k increases more than GPT-3 ZS variants, but absolute performance remains below DSR-LM.",
            "ablation_or_analysis_results": "Qualitative analysis shows GPT-3 makes systematic logical fallacies and inconsistent chains; inclusion of hand-crafted rules in the prompt can be attempted but does not achieve DSR-LM-level generalization. The paper reports GPT-3's prompted extraction of composition rules was only ~62% correct initially.",
            "uuid": "e5012.2",
            "source_info": {
                "paper_title": "Improved Logical Reasoning of Language Models via Differentiable Symbolic Programming",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "RoBERTa",
            "name_full": "RoBERTa-base (pretrained transformer)",
            "brief_description": "A pretrained transformer-based encoder (RoBERTa-base) used as the primary LM backbone in DSR-LM for relation extraction and fine-tuned within the DSR-LM pipeline.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "RoBERTa-base",
            "model_description": "Pretrained transformer encoder used as the relation extraction backbone; embeddings of entity spans and windowed context are fed to an MLP classifier for relation prediction; fine-tuned with a low learning rate.",
            "model_size": "123M",
            "logical_reasoning_task": "CLUTRR / DBpedia-INF relation extraction component for downstream logical reasoning",
            "task_description": "Extract probabilistic relational atoms (p(subject,object)) from windowed context to be fed into symbolic reasoner; weak supervision from final QA labels.",
            "method_or_approach": "Fine-tuned within DSR-LM with binary cross-entropy on probabilistic relation labels propagated via differentiable reasoning; uses concatenated embeddings of two entities plus window embedding into 2-layer MLP classifier.",
            "performance": "In isolation, the trained relation extractor achieves 84.69% accuracy on single-relation classification under weak supervision; fully supervised relation extractor reaches 85.32%. With RoBERTa backbone, DSR-LM reaches ~60.98% accuracy on CLUTRR overall.",
            "limitations_or_failure_cases": "Perception errors (misclassifying relations) are the dominant failure mode for the full pipeline; these errors accumulate over multi-hop chains causing accuracy drop as k increases.",
            "comparison": "RoBERTa and DeBERTa backbones give similar performance in DSR-LM; both outperform word2vec+BiLSTM backbone. RoBERTa-based DSR-LM reached best performance within ~20 epochs while w2v+BiLSTM required ~40.",
            "ablation_or_analysis_results": "Ablation shows transformer backbones (RoBERTa, DeBERTa) yield higher final accuracy and faster convergence than word2vec+BiLSTM; relation extractor performance close to fully supervised despite weak supervision.",
            "uuid": "e5012.3",
            "source_info": {
                "paper_title": "Improved Logical Reasoning of Language Models via Differentiable Symbolic Programming",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "DeBERTa",
            "name_full": "DeBERTa-base (pretrained transformer)",
            "brief_description": "A pretrained transformer encoder (DeBERTa-base) used as an alternative backbone in DSR-LM ablations, showing comparable performance to RoBERTa.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "DeBERTa-base",
            "model_description": "Pretrained transformer encoder used as a relation extraction backbone and fine-tuned inside DSR-LM (variant: DSR-LM-DeBERTa).",
            "model_size": null,
            "logical_reasoning_task": "CLUTRR relation extraction and downstream logical reasoning",
            "task_description": "Same as RoBERTa: extract relations from text windows for input to symbolic reasoner.",
            "method_or_approach": "Fine-tuned inside DSR-LM; same relation-classifier MLP architecture.",
            "performance": "Reported to have on-par performance with RoBERTa in DSR-LM (table summary), converging within ~20 epochs.",
            "limitations_or_failure_cases": "Same perception-driven limitations as other transformer backbones (relation-extraction errors can cascade).",
            "comparison": "Performs on-par with RoBERTa and both outperform word2vec+BiLSTM backbone in DSR-LM.",
            "ablation_or_analysis_results": "Ablation shows DeBERTa and RoBERTa give similar final accuracy and training speed advantages over word-embedding+encoder baselines.",
            "uuid": "e5012.4",
            "source_info": {
                "paper_title": "Improved Logical Reasoning of Language Models via Differentiable Symbolic Programming",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "w2v-BiLSTM",
            "name_full": "word2vec + BiLSTM (DSR-w2v-BiLSTM)",
            "brief_description": "A non-transformer baseline used as a relation-extraction backbone in DSR-LM ablations, combining 300-d word2vec embeddings with a BiLSTM encoder and an MLP classifier.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "w2v-google-news-300 + BiLSTM",
            "model_description": "Word2vec (300-d pretrained Google News vectors) for token embeddings, encoded by a BiLSTM for context; used as the relation-extraction neural backbone in the DSR-LM-w2v-BiLSTM variant.",
            "model_size": null,
            "logical_reasoning_task": "CLUTRR relation extraction and downstream logical reasoning",
            "task_description": "Extract relations from text windows for symbolic reasoning; serves to test backbone sensitivity.",
            "method_or_approach": "Fine-tuned (encoder and classifier) within DSR-LM framework, but slower convergence and lower final performance than transformer backbones.",
            "performance": "DSR-w2v-BiLSTM attains lower final accuracy than transformer-backed DSR-LM variants but still outperforms many other baselines; requires ~40 epochs to peak vs ~20 for transformer backbones.",
            "limitations_or_failure_cases": "Weaker perception and slower training; lower final accuracy than transformer-based backbones.",
            "comparison": "Although worse than RoBERTa/DeBERTa-backed DSR-LM, w2v-BiLSTM variant still performs better than many purely neural or structured baselines (exact numbers in Table 5 of the paper).",
            "ablation_or_analysis_results": "Demonstrates that choice of neural backbone matters: transformer backbones give higher accuracy and faster convergence.",
            "uuid": "e5012.5",
            "source_info": {
                "paper_title": "Improved Logical Reasoning of Language Models via Differentiable Symbolic Programming",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "RuleBert",
            "name_full": "RuleBert (baseline)",
            "brief_description": "A prior neuro-symbolic baseline that fine-tunes language models with soft rules and facts to improve logical reasoning; used as a baseline on DBpedia-INF.",
            "citation_title": "RuleBert",
            "mention_or_use": "mention",
            "model_name": "RuleBert",
            "model_description": "A method that augments LM fine-tuning with soft rules and facts to encourage logical inference; evaluated on rule-based reasoning datasets like DBpedia-INF.",
            "model_size": null,
            "logical_reasoning_task": "DBpedia-INF (rule-based KB reasoning)",
            "task_description": "Deduce relations using synthetic passages and soft deductive rules mined from DBpedia; tests capacity to leverage soft rules during LM fine-tuning.",
            "method_or_approach": "Fine-tuning with soft rules and facts (data augmentation / soft logical constraints) rather than a tightly integrated differentiable symbolic reasoner.",
            "performance": "On DBpedia-INF, DSR-LM outperforms RuleBert by ~37% (paper reports DSR-LM's substantial advantage), indicating RuleBert's approach is less robust to systematic generalization.",
            "limitations_or_failure_cases": "Paper argues that augmenting data alone (fine-tuning with soft rules) does not effectively improve systematic generalization to longer chains compared to tight neuro-symbolic integration.",
            "comparison": "DSR-LM shows much stronger generalization than RuleBert on DBpedia-INF, implying the differentiable symbolic integration and rule learning approach is more effective than RuleBert's fine-tuning with soft rules.",
            "ablation_or_analysis_results": "Paper reports RuleBert baseline results on DBpedia-INF and uses them to demonstrate DSR-LM's relative gain; no internal ablation for RuleBert reported here.",
            "uuid": "e5012.6",
            "source_info": {
                "paper_title": "Improved Logical Reasoning of Language Models via Differentiable Symbolic Programming",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "GAT",
            "name_full": "Graph Attention Network (GAT)",
            "brief_description": "A graph-structured neural baseline that operates on structured entity-relation graphs (rather than raw text) and is compared in KB reasoning experiments.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "GAT",
            "model_description": "A graph neural network using attention over neighbors to perform relational reasoning over structured KB graphs; used as a baseline when structured KBs are available.",
            "model_size": null,
            "logical_reasoning_task": "CLUTRR KB reasoning (structured KB input)",
            "task_description": "Learn to answer kinship queries given the structured entity-relation graph; isolates graph-based relational inductive biases.",
            "method_or_approach": "Supervised graph neural network trained on structured KBs.",
            "performance": "On structured KB reasoning (trained on k∈[2,3], tested on k∈[4,10]) GAT achieves 39.05% accuracy, substantially below DSR-without-LM (98.81%).",
            "limitations_or_failure_cases": "Performs poorly on generalization to longer reasoning chains compared to differentiable rule-learning approach (DSR-without-LM).",
            "comparison": "Underperforms both DSR-without-LM and CTP on the KB reasoning benchmark used in the paper.",
            "ablation_or_analysis_results": "Used as a baseline to show benefit of differentiable symbolic rule learning versus GNN approaches on KB reasoning generalization.",
            "uuid": "e5012.7",
            "source_info": {
                "paper_title": "Improved Logical Reasoning of Language Models via Differentiable Symbolic Programming",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "CTP",
            "name_full": "CTP (Closed-World Probabilistic reasoning model, baseline)",
            "brief_description": "A neuro-symbolic/structured baseline that operates on structured KBs; included in comparisons for KB reasoning tasks.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "CTP",
            "model_description": "A probabilistic/neuro-symbolic approach that reasons over structured KBs (specific implementation referenced in prior work Minervini et al., 2020), used as a high-performing baseline on KB reasoning.",
            "model_size": null,
            "logical_reasoning_task": "CLUTRR KB reasoning (structured KB input)",
            "task_description": "Deduce multi-hop kinship relations using structured KBs and rule-like reasoning.",
            "method_or_approach": "Structured probabilistic/neuro-symbolic reasoning over KBs.",
            "performance": "On structured KB reasoning (same setting), CTP achieves 95.57% accuracy, slightly below DSR-without-LM (98.81%) but much higher than GAT.",
            "limitations_or_failure_cases": "Less accurate than DSR-without-LM in this evaluation; details of failure modes not elaborated in this paper.",
            "comparison": "Competitive with DSR approaches when given structured input, but DSR-without-LM still exceeds it in accuracy.",
            "ablation_or_analysis_results": "Serves as a strong baseline when the KB is structured; comparison highlights the power of the paper's differentiable rule-learning pipeline.",
            "uuid": "e5012.8",
            "source_info": {
                "paper_title": "Improved Logical Reasoning of Language Models via Differentiable Symbolic Programming",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "DSR-LM-without-IC",
            "name_full": "DSR-LM without Integrity Constraints (ablation)",
            "brief_description": "An ablation of DSR-LM that removes integrity constraints and the semantic-loss term to assess the effect of logical integrity regularization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "DSR-LM-without-IC",
            "model_description": "Same pipeline as DSR-LM but excludes integrity constraints and the semantic loss component that penalizes logical violations during training.",
            "model_size": null,
            "logical_reasoning_task": "CLUTRR (kinship reasoning)",
            "task_description": "Evaluate how removal of integrity constraints affects downstream multi-hop reasoning accuracy.",
            "method_or_approach": "Ablation removing semantic loss derived from integrity constraints on predicted relations and rules.",
            "performance": "Reported accuracy drops to 51.48 ± 0.57% on CLUTRR (Table 6), which is substantially worse than full DSR-LM (~60.98%).",
            "limitations_or_failure_cases": "Shows that integrity constraints are an essential component for robustness; without them, performance suffers significantly.",
            "comparison": "Performs worse than full DSR-LM and DSR-LM-with-Manual-Rule, demonstrating the importance of semantic loss regularization.",
            "ablation_or_analysis_results": "Confirms that semantic loss (integrity constraints) improves robustness and generalization of learned rules and relation extraction.",
            "uuid": "e5012.9",
            "source_info": {
                "paper_title": "Improved Logical Reasoning of Language Models via Differentiable Symbolic Programming",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "DSR-LM-with-Manual-Rule",
            "name_full": "DSR-LM with Manual Rules (ablation)",
            "brief_description": "A DSR-LM variant where hand-crafted logic rules (external KB) are provided instead of learning them, used to measure the benefit of learned versus manual rules.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "DSR-LM-with-Manual-Rule",
            "model_description": "DSR-LM where 92 manually crafted kinship composition rules are injected and set to weight 1.0, disabling rule learning for those rules; otherwise same pipeline.",
            "model_size": null,
            "logical_reasoning_task": "CLUTRR (kinship reasoning)",
            "task_description": "Use gold/manual composition rules as the symbolic KB to drive deductive inference from extracted relations.",
            "method_or_approach": "Inject human-crafted rules into Scallop program instead of initializing via LM prompting and fine-tuning; semantic loss and relation extraction remain in place.",
            "performance": "Reported accuracy 61.34 ± 1.56% on CLUTRR, a modest 0.36% gain over learned-rule DSR-LM (60.98%), indicating learned rules are nearly as effective as manual ones.",
            "limitations_or_failure_cases": "Marginal gain implies human effort to hand-craft rules gives limited advantage when the rule-learning pipeline is effective.",
            "comparison": "Slightly better than learned-rule DSR-LM but not substantially; supports claim that rule learning via LM initialization plus fine-tuning is effective.",
            "ablation_or_analysis_results": "Shows that learned rules (initialized via LM prompts, fine-tuned) are nearly as good as manual rules; manual rules add only minor improvement.",
            "uuid": "e5012.10",
            "source_info": {
                "paper_title": "Improved Logical Reasoning of Language Models via Differentiable Symbolic Programming",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "CLUTRR: A Diagnostic Benchmark for Compositional Generalization in Relational Reasoning",
            "rating": 2,
            "sanitized_title": "clutrr_a_diagnostic_benchmark_for_compositional_generalization_in_relational_reasoning"
        },
        {
            "paper_title": "Scallop: From probabilistic deductive databases to scalable differentiable reasoning",
            "rating": 2,
            "sanitized_title": "scallop_from_probabilistic_deductive_databases_to_scalable_differentiable_reasoning"
        },
        {
            "paper_title": "Language Models are Few-Shot Learners",
            "rating": 2,
            "sanitized_title": "language_models_are_fewshot_learners"
        },
        {
            "paper_title": "RuleBert",
            "rating": 2
        },
        {
            "paper_title": "Large language models are zero-shot reasoners",
            "rating": 2,
            "sanitized_title": "large_language_models_are_zeroshot_reasoners"
        },
        {
            "paper_title": "Measuring and improving consistency in pretrained language models",
            "rating": 1,
            "sanitized_title": "measuring_and_improving_consistency_in_pretrained_language_models"
        }
    ],
    "cost": 0.01630375,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Improved Logical Reasoning of Language Models via Differentiable Symbolic Programming</p>
<p>Hanlin Zhang 
Carnegie Mellon University</p>
<p>Jiani Huang 
University of Pennsylvania</p>
<p>Ziyang Li 
University of Pennsylvania</p>
<p>Mayur Naik 
University of Pennsylvania</p>
<p>Eric Xing 
Carnegie Mellon University</p>
<p>Mohamed Bin Zayed
University of Artificial Intelligence</p>
<p>Petuum Inc</p>
<p>Improved Logical Reasoning of Language Models via Differentiable Symbolic Programming</p>
<p>Pre-trained large language models (LMs) struggle to perform logical reasoning reliably despite advances in scale and compositionality. In this work, we tackle this challenge through the lens of symbolic programming. We propose DSR-LM, a Differentiable Symbolic Reasoning framework where pretrained LMs govern the perception of factual knowledge, and a symbolic module performs deductive reasoning. In contrast to works that rely on hand-crafted logic rules, our differentiable symbolic reasoning framework efficiently learns weighted rules and applies semantic loss to further improve LMs. DSR-LM is scalable, interpretable, and allows easy integration of prior knowledge, thereby supporting extensive symbolic programming to robustly derive a logical conclusion. The results of our experiments suggest that DSR-LM improves the logical reasoning abilities of pretrained language models, resulting in a significant increase in accuracy of over 20% on deductive reasoning benchmarks. Furthermore, DSR-LM outperforms a variety of competitive baselines when faced with systematic changes in sequence length. 1</p>
<p>Introduction</p>
<p>Complex applications in natural language processing involve dealing with two separate challenges. On one hand, there is the richness, nuances, and extensive vocabulary of natural language. On the other hand, one needs logical connectives, long reasoning chains, and domain-specific knowledge to draw logical conclusions. The systems handling these two challenges are complementary to each other and are likened to psychologist Daniel Kahneman's human "system 1" and "system 2" (Kahneman, 2011): while the former makes fast and intuitive decisions, akin to neural networks, the latter * Equal contribution 1 Code available at https://github.com/moqingyan/dsr-lm  thinks more rigorously and methodically. Considering LMs as "system 1" and symbolic reasoners as "system 2", we summarize their respective advantages in Table 1. Although pre-trained LMs have demonstrated remarkable predictive performance, making them an effective "system 1", they fall short when asked to perform consistent logical reasoning (Kassner et al., 2020;Helwe et al., 2021;Creswell et al., 2022), which usually requires "system 2". In part, this is because LMs largely lack capabilities of systematic generalization (Elazar et al., 2021;Hase et al., 2021;Valmeekam et al., 2022).</p>
<p>In this work, we seek to incorporate deductive logical reasoning with LMs. Our approach has the same key objectives as neuro-symbolic programming (Chaudhuri et al., 2021): compositionality, consistency, interpretability, and easy integration of prior knowledge. We present DSR-LM, which tightly integrates a differentiable symbolic reasoning module with pre-trained LMs in an end-to-end fashion. With DSR-LM, the underlying LMs govern the perception of natural language and are finetuned to extract relational triplets with only weak supervision. To overcome a common limitation of symbolic reasoning systems, the reliance on human-crafted logic rules (Huang et al., 2021;Nye et al., 2021), we adapt DSR-LM to induce and finetune rules automatically. Further, DSR-LM allows incorporation of semantic loss obtained by logical integrity constraints given as prior knowledge, which substantially helps the robustness.</p>
<p>We conduct extensive experiments showing that DSR-LM can consistently improve the logical reasoning capability upon pre-trained LMs. Even if DSR-LM uses a RoBERTa backbone with much less parameters and does not explicitly take triplets as supervision, it can still outperform various baselines by large margins. Moreover, we show that DSR-LM can induce logic rules that are amenable to human understanding to explain decisions given only higher-order predicates. As generalization over long-range dependencies is a significant weakness of transformer-based language models (Lake and Baroni, 2018;Tay et al., 2020), we highlight that in systematic, long-context scenarios, where most pre-trained or neural approaches fail to generalize compositionally, DSR-LM can still achieve considerable performance gains.</p>
<p>Related Work</p>
<p>Logical reasoning with LMs. Pre-trained LMs have been shown to struggle with logical reasoning over factual knowledge (Kassner et al., 2020;Helwe et al., 2021;Talmor et al., 2020a). There is encouraging recent progress in using transformers for reasoning tasks (Zhou et al., 2020;Clark et al., 2021;Wei et al., 2022;Chowdhery et al., 2022;Zelikman et al., 2022) but these approaches usually require a significant amount of computation for re-training or human annotations on reasoning provenance (Camburu et al., 2018;Zhou et al., 2020;Nye et al., 2021;Wei et al., 2022). Moreover, their entangled nature with natural language makes it fundamentally hard to achieve robust inference over factual knowledge (Greff et al., 2020;Saparov and He, 2022;Zhang et al., 2022).</p>
<p>There are other obvious remedies for LMs' poor reasoning capability. Ensuring that the training corpus contains a sufficient amount of exemplary episodes of sound reasoning reduces the dependency on normative biases and annotation artifacts (Talmor et al., 2020b;Betz et al., 2020;Hase et al., 2021). Heuristics like data augmentation are also shown to be effective (Talmor et al., 2020b). But the above works require significant efforts for crowdsourcing and auditing training data. Our method handily encodes a few prototypes/templates of logic rules and is thus more efficient in terms of human effort. Moreover, our goal is fundamentally different from theirs in investigating the tight integration of neural and symbolic models in an end-to-end manner.</p>
<p>Neuro-symbolic reasoning. Neuro-symbolic approaches are proposed to integrate the perception of deep neural components and the reasoning of symbolic components. Representative works can be briefly categorized into regularization (Xu et al., 2018), program synthesis (Mao et al., 2018, and proof-guided probabilistic programming (Evans and Grefenstette, 2018;Rocktäschel and Riedel, 2017;Manhaeve et al., 2018;Zhang et al., 2019;Huang et al., 2021). To improve compositionality of LMs, previous works propose to parameterize grammatical rules (Kim, 2021;Shaw et al., 2021) but show that those hybrid models are inefficient and usually underperform neural counterparts. In contrast to the above works, DSR-LM focuses on improving LMs' reasoning over logical propositions with tight integration of their pre-trained knowledge in a scalable and automated way.</p>
<p>Methodology</p>
<p>Problem Formulation</p>
<p>Each question answering (QA) example in the dataset is a triplet containing input text x, query q, and the answer y. Figure 1 shows an instance that we will use as our running example. The input text x is a natural language passage within which there will be a set of entities, possibly referenced by 3rd person pronouns. The sentences hint at the relationships between entities. For example, "Dorothy went to her brother Rich's birthday party" implies that Rich is Dorothy's brother and Dorothy is Rich's sister. The query q is a tuple of two entities, representing the people with whom we want to infer the relation. The expected relation is stored in the answer y, which will be one of a confined set of possible relations R, allowing us to treat the whole problem as an R -way classification problem. We focus only on the problems where the desired relation is not explicitly stated in the context but need to be deduced through a sequence of reasoning.</p>
<p>Methodology Overview</p>
<p>The design of DSR-LM concerns tightly integrating a perceptive model for relation extraction with a symbolic engine for logical reasoning. While we apply LMs for low-level perception and relation extraction, we employ a symbolic reasoning module to consistently and logically reason about the extracted relations. With a recent surge in neurosymbolic methods, reasoning engines are made differentiable, allowing us to differentiate through </p>
<p>Question</p>
<p>How is Dorothy related to Anne? Figure 1: Overview of DSR-LM with a motivating example where "Anne is the niece of Dorothy" should be logically inferred from the context. We abbreviate the names with their first initials in the relational symbols.</p>
<p>Probabilistic Input Facts</p>
<p>the logical reasoning process. In particular, we employ Scallop (Huang et al., 2021) as our reasoning engine. We propose two add-ons to the existing neuro-symbolic methodology. First, some rules used for logical deduction are initialized using language models and further tuned by our end-to-end pipeline, alleviating human efforts. Secondly, we employ integrity constraints on the extracted relation graphs and the logical rules, to improve the logical consistency of LMs and the learned rules.</p>
<p>Based on this design, we formalize our method as follows. We adopt pretrained LMs to build relation extractors, denoted M θ , which take in the natural language input x and return a set of probabilistic relational symbols r. Next, we employ a differentiable deductive reasoning program, P φ , where φ represents the weights of the learned logic rules. It takes as input the probabilistic relational symbols and the query q and returns a distribution over R as the outputŷ. Overall, the deductive model is written aŝ
y = P φ (M θ (x), q).
(1)</p>
<p>Additionally, we have the semantic loss (sl) derived by another symbolic program P sl computing the probability of violating the integrity constraints:
l sl = P sl (M θ (x), φ)(2)
Combined, we aim to minimize the objective J over training set D with loss function L:
J(θ, φ) = 1 D (x,q,y)∈D w 1 L(P φ (M θ (x), q), y) + w 2 P sl (M θ (x), φ),(3)
where w 1 and w 2 are tunable hyper-parameters to balance the deduction loss and semantic loss.</p>
<p>Relation Extraction</p>
<p>Since pre-trained LMs have strong pattern recognition capabilities for tasks like Named-Entity-Recognition (NER) and Relation Extraction (RE) (Tenney et al., 2019; Soares et al., 2019), we adopt them as our neural components in DSR-LM. To ensure that LMs take in strings of similar length, we divide the whole context into multiple windows. The goal is to extract the relations between every pair of entities in each windowed context. Concretely, our relation extractor M θ comprises three components: 1) a Named-Entity Recognizer (NER) to obtain the entities in the input text, 2) a pretrained language model, to be fine-tuned, that converts windowed text into embeddings, and 3) a classifier that takes in the embedding of entities and predicts the relationship between them. The set of parameters θ contains the parameters of both the LM and the classifier.</p>
<p>We assume the relations to be classified come from a finite set of relations R. For example in CLUTRR (Sinha et al., 2019), we have 20 kinship relations including mother, son, uncle, fatherin-law, etc. In practice, we perform ( R + 1)way classification over each pair of entities, where the extra class stands for "n/a". The windowed contexts are split based on simple heuristics of "contiguous one to three sentences that contain at least two entities", to account for coreference resolution. The windowed contexts can be overlapping and we allow the reasoning module to deal with noisy and redundant data. Overall, assuming that there are m windows in the context x, we extract mn(n − 1)( R + 1) probabilistic relational symbols. Each symbol is denoted as an atom of the form p(s, o), where p ∈ R ∪ {n/a} is the relational predicate, and s, o are the two entities connected by the predicate. We denote the probability of such symbol extracted by the LM and relational classifier as Pr(p(s, o) θ). All these probabilities combined form the output vector r = M θ (x) ∈ R mn(n−1)( R +1) .</p>
<p>Differentiable Symbolic Inference</p>
<p>The symbolic inference modules P φ and P sl are responsible for processing the extracted relations to deduce 1) an expected output relation in R, and 2) a semantic loss encoding the probability of constraint violation. There are two main objectives for these modules. First, they need to logically reason about the output relation and the semantic loss based on the extracted relational symbols r, the query q, and the rule weights φ. Second, they need to compute the gradients ofŷ and l sl with respect to θ and φ, namely ∂ŷ ∂θ , ∂ŷ ∂φ , ∂l sl ∂φ , and ∂l sl ∂θ , in order for the fine-tuning and rule learning to happen.</p>
<p>Logical deduction. Logic rules can be applied to known facts to deduce new ones. For example, below is a horn clause, which reads "if b is a's brother and c is b's daughter, then c is a's niece":
niece(a, c) ← brother(a, b) ∧ daughter(b, c).
Note that the structure of the above rule can be captured by a higher-order logical predicate called "composite" (abbreviated as comp ). This allows us to express many other similarly structured rules with ease. For instance, we can have comp(brother, daughter, niece) and comp (father, mother, grandmother) . With this set of rules, we may derive more facts based on known kinship relations. In fact, composition is the only kind of rule we need for kinship reasoning. In general, there are many other useful higher-order predicates to reason over knowledge bases, which we list out in Table 2.</p>
<p>Predicate</p>
<p>Example transitive transitive(relative) symmetric symmetric(spouse) inverse inverse(husband, wife) implies implies(mother, parent) Probability propagation. We seek to have the deduced facts to also be associated with probabilities computed using probabilities predicted by the underlying relation extractor M θ . This is achieved by allowing the propagation of probabilities. For example, we have the proof tree with probabilities:
0.9 ∶∶ brother(D, R) 0.8 ∶∶ daughter(R, K) 0.72 ∶∶ niece(D, K)
In practice, there could be multiple steps in the proof tree (multi-hop) and one fact can be derived by multiple proof trees. We employ the inference algorithms based on approximated weighted model counting (WMC) presented in (Manhaeve et al., 2018) to account for probabilistic inference under complex scenarios. Since the WMC procedure is augmented for differentiation, we can obtain the gradient ∂ŷ ∂r . From here, we can obtain ∂ŷ ∂θ = ∂ŷ ∂r ∂r ∂θ , where the second part can be automatically derived from differentiating M θ .</p>
<p>Rule learning. Hand-crafted rules could be expensive or even impossible to obtain. To alleviate this issue, DSR-LM applies LMs to help automatically extract rules, and further utilizes the differentiable pipeline to fine-tune the rules. Each rule such as comp(brother, daughter, niece) is attached a weight, initialized by prompting an underlying LM. For example, the prompt we use for extracting comp(r,p,q) is "one's r's p is their <q:mask>". Given that the relations r, p, q ∈ R, DSR-LM automatically enumerates r and p from R while querying for LM to unmask the value of q. LM then returns a distribution of words, which we take an intersection with R. The probabilities combined form the initial rule weights φ. This type of rule extraction strategy is different from existing approaches in inductive logic programming since we are exploiting LMs for existing knowledge about relationships.</p>
<p>Note that LMs often make simple mistakes answering such prompt. In fact, with the above prompt, even GPT-3 can only produce 62% of composition rules correctly. While we can edit prompt to include few-shot examples, in this work we consider fine-tuning such rule weights φ within our differentiable reasoning pipeline. The gradient with respect to φ is also derived with the WMC procedure, giving us ∂ŷ ∂φ . In practice, we use two optimizers with different hyper-parameters to update the rule weights φ and the underlying model parameter θ, in order to account for optimizing different types of weights.</p>
<p>Semantic loss and integrity constraints. In general, learning with weak supervision label is hard, not to mention that the deductive rules are learnt as well. We thereby introduce an additional semantic loss during training. Here, semantic loss is derived by a set of integrity constraints used to regularize the predicted entity-relation graph as well as the learnt logic rules. In particular, we consider rules that detect violations of integrity constraints. For example, "if A is B's father, then B should be A's son or daughter" is an integrity constraint for relation extractor-if the model predicts a father relationship between A and B, then it should also predict a son or daughter relationship between B and A. Encoded in first order logic, it is a)). Through differentiable reasoning, we evaluate the probability of such constraint being violated, yielding our expected semantic loss. In practice, arbitrary number of constraints can be included, though too many interleaving ones could hinder learning.
∀a, b, father(a, b) ⇒ (son(b, a) ∨ daughter(b,</p>
<p>Experiments</p>
<p>We evaluate DSR-LM on both CLUTRR and DBpedia-INF. We show that DSR-LM has accurate and generalizable long-range reasoning capability.</p>
<p>Datasets</p>
<p>CLUTRR (Sinha et al., 2019) consists of kinship reasoning questions. Given a context that describes a family's routine activity, the goal is to deduce the relationship between two family members that is not explicitly mentioned in the story. Although the dataset is synthetic, the sentences are crowdsourced and hence there is a considerable amount of naturalness inside the dataset. The family kinship graph is synthetic and the names of the family members are randomized. For ablation study, we manually crafted 92 kinship composition rules as an external symbolic knowledge base. This yields the following symbolic information for each datapoint: 1) the full kinship graph corresponding to the story, 2) the symbolic knowledge base (KB), and 3) a query representing the question. The CLUTRR dataset is divided into different difficulties measured by k, the number of facts used in the reasoning chain. For training, we only have 10K data points with 5K k = 2 and another 5K k = 3, meaning that we can only receive supervision on data with short reasoning chains. The test set, on the other hand, contains 1.1K examples with k ∈ {2, . . . , 10}.</p>
<p>DBpedia-INF is a curated subset of the evaluation dataset used in RuleBert (Saeed et al., 2021). Similar to CLUTRR, it is generated synthetically to test the reasoning capability of LMs. Given a synthetic passage describing the relation between entities, and soft deductive logic rules, we aim to deduce the relationship between any two entities. The symbolic program of DBpedia-INF consists of 26 predicates, 161 soft rules mined from DBpedia, and 16 rules defining the negation and symmetricity between the predicates. The difficulty of the questions is represented in terms of reasoning length from k ∈ {0, . . . , 5}. 2 Larger k implies harder question. Compared to the exact dataset used in Rulebert, we clean it in order to ensure the question-answer pairs are logically consistent and probabilistically correct.</p>
<p>Experimental Setup</p>
<p>Implementation. We employ Scallop (Huang et al., 2021) as the differentiable symbolic inference module. We show the program used for CLUTRR reasoning task in Figure 2. It comprises relation type declarations, deductive rules for kinship reasoning, and integrity constraints for computing semantic loss (attached in the Appendix). The program used for DBpedia-INF is written in a similar manner with additional high-order predicates listed in Table 2.</p>
<p>Pre-trained LMs for fine-tuning. We used the HuggingFace (Wolf et al., 2019) pre-trained w2vgoogle-news-300, RoBERTa-base, and DeBERTabase as the pretrained language models. We finetune RoBERTa-base and DeBERTa-base during training with binary cross entropy loss. Our relation extraction module is implemented by adding an MLP classifier after the LM, accepting a concatenation of the embedding of the two entities and the embedding of the whole windowed context.</p>
<p>Our model. Our main model, DSR-LM, uses RoBERTa as the underlying LM. The relation classifier is a 2-layer fully connected MLP. For training, we initialize φ by prompting the LM. To accelerate the learning process, we use multinomial sampling to retrieve 150 rules for symbolic reasoning. During testing, we will instead pick the top 150 rules. We use two Adam optimizer to update θ and φ, with learning rate 10 −5 and 10 −2 respectively.</p>
<p>For ablation studies, we present a few other models. First, we ablate on back-bone LMs. Specifically, we have DSR-LM-DeBERTa which uses De-// Relation declaration type kinship(rela: String, subject: String, object: String) type query(subject: String, object: String) type composite(r1: String, r2: String, r3: String) // Rules to derive the final answer rel kinship(r3,a,c) = kinship(r1,a,b), kinship(r2,b,c), composite(r1,r2,r3), a != c rel answer(r) = query(s, o), derive(r, s, o) // Integrity constraints (6 for kinship graph and 2 for rule learning) rel violation(!r) = r := forall(a, b: kinship(FATHER, a, b) =&gt; kinship (SON, b, a) or kinship(DAUGHTER, b, a)) // Other constraints are omitted... Baseline setup. We highlight a few baselines we include for completeness but are treated as unfair comparison to us: GAT, CTP, and GPT-3 variants. All baselines other than GAT and CTP take as input natural language stories and the question to produce the corresponding answer. GAT and CTP, on the contrary, takes entity relation graph rather than natural language during training and testing. The model sizes are different across baselines as well. Model size generally depends on two parts, the backbone pre-trained LM, and the classifica-tion network built upon the LM. GPT-3 contains 175B parameters, and RoBERTa uses 123M parameters. The classification model of our method has 2.97M parameters (assuming using embeddings from RoBERTa). With extra 10K parameters for rule weights, our DSR-LM framework has around 127M parameters.</p>
<p>For GPT-3 variants, we conduct experiments on CLUTRR with GPT-3 under the Zero-Shot (GPT-3 ZS), GPT-3 Fine-Tuned (GPT-3 FT), and Few (5) </p>
<p>Experimental Results</p>
<p>DSR-LM systematically outperforms a wide range of baselines by a large margin. We evaluate DSR-LM and baselines on both CLUTRR and DBpedia-INF, as reported in Figure 3 and Table 3.</p>
<p>In the CLUTRR experiment, DSR-LM achieves the best performance among all the models (Figure 3). Next, we examine how models trained on stories generated from clauses of length k ≤ 3 and  Confidence Learnt Rules 1.154 mother(a,c) ← sister(a,b) ∧ mother(b,c) 1.152 daughter(a,c) ← daughter(a,b) ∧ sister(b,c) 1.125 sister(a,c) ← daughter(a,b) ∧ aunt(b,c) 1.125 father(a,c) ← brother(a,b) ∧ father(b,c) 1.123 granddaughter(a,c) ← grandson(a,b) ∧ sister(b,c) 1.120 brother(a,c) ← sister(a,b) ∧ brother(b,c) 1.117 brother(a,c) ← son(a,b) ∧ uncle(b,c) 1.105 brother(a,c) ← daughter(a,b) ∧ uncle(b,c) 1.104 daughter(a,c) ← wife(a,b) ∧ daughter(b,c) 1.102 mother(a,c) ← brother(a,b) ∧ mother(b,c) . . . . . . Table 4: The learnt top-10 confident logic rules over CLUTRR. evaluated on stories generated from larger clauses of length k ≥ 4. A fine-grained generalizability study reveals that although all models' performances decline as the reasoning length of the test sequence increases, pure neural-based models decrease the fastest (Figure 4a and 4b). It manifests the systematic issue that language models alone are still not robust for length generalization (Lake and Baroni, 2018). On the other hand, the performance of DSR-LM decreases much slower as test reasoning length increases and outperforms all the baselines when k ≥ 4.</p>
<p>In the DBpedia-INF experiment, DSR-LM outperforms RuleBert by 37% in terms of overall performance (Table 3), showing that DSR-LM has much more robust generalization. Recall that Rule-Bert aims to improve the logical reasoning of LMs by straightforward fine-tuning with soft rules and facts. Our results show that augmenting data alone for fine-tuning do not effectively improve systematicity. Meanwhile, DSR-LM imbues reasoning inductive biases throughout training and learns useful rules to generalize to longer reasoning lengths.</p>
<p>Learning interpretable logic rules. DSR-LM is capable of producing explicit logic rules as part of the learning process. For presentation, we show the top-10 rules learnt from DSR-LM model in Table 4. We compare the top-92 most likely prompted and fine-tuned rules against the 92 hand-crafted rules, and 70 of them match. Additionally, we find that our rule weight fine-tuning helps correct 11 of the incorrect rules produced by LM. Through this qualitative analysis, it is clear that DSR-LM provides an interface to probe and interpret the intermediate steps, enhancing the interpretability.</p>
<p>GPT-3 variants are inferior in long-range reasoning. Interestingly, ZS scores 28.6% accuracy on CLUTRR while ZS-CoT scores 25.6%, suggesting that the chain-of-thought prompting might not work in long-range reasoning (Figure 3). In fact, there are many cases where GPT-3 favors complication over simplicity: GPT-3 frequently answers "stepdaughter", "stepmother", and "adopted son", while the real answers are simply "daughter", "mother", and "son". Additionally, GPT-3 could derive the correct result for the wrong reason, e.g. "Jeffrey is Gabrielle's son, which would make William her grandson, and Jeffrey's brother." While we count the final answer to be correct (William is Jeffrey's brother), there is a clear inconsistency in the reasoning chain: William cannot be Gabrielle's grandson and Jeffrey's brother simultaneously, given that Jeffrey is Gabrielle's son. Lastly, we observe that, both GPT-3 FT and many other methods have an accuracy drop as k becomes larger (Figure 4b), ZS and ZS-CoT stay relatively consistent, suggesting that the size of context and the reasoning chain may have a low impact on GPT-3's performance.</p>
<p>Analyses and Ablation Studies</p>
<p>Symbolic reasoner consistently improves LMs and word embeddings. Since DSR-LM has a model agnostic architecture, we study how the choice of different LMs impacts the reasoning performance. As shown in Table 5, the two transformer-based models have on-par performance and outperform the word2vec one. However, note that the word2vec-based model still has better performance than all other baselines. Besides higher final accuracy, the pre-trained transformerbased language model also accelerates the training process. Both DSR-LM-RoBERTa and DSR-LM-DeBERTa reach their best performance within 20 epochs, while it takes DSR-w2v-BiLSTM 40 epochs to peak.</p>
<p>Model</p>
<p>Accuracy (  Incorporate domain knowledge. DSR-LM allows injecting domain specific knowledge. In DSR-LM-with-Rule, we manually crafted 92 rules for kinship reasoning to replace the learnt rules. As shown in Table 6, it obtained a 0.36% performance gain over DSR-LM. The fact that the improvement is marginal implies our method extracts useful rules to obtain on-par performance with manually crafted ones. DSR-LM-without-IC, our model without integrity constraints specified on predicted relations and rules, performs worse than DSR-LM, suggesting that logical integrity constraints are essential component for improving the model robustness.   Failure cases of DSR-LM. We showcase in Appendix Table 8 that even state-of-the-art large LMs are prone to logical fallacies. On the other hand, the failure case of our method usually occurs in the stage of relation extraction. For example, for the following sentence "Christopher and Guillermina are having a father-daughter dance", our RoBERTa based relation extractor fails to recognize the fatherdaughter relationship but rather thinks C and G have a husband-wife relationship. We require most of the relation extraction to be correct in order to avoid cascading error. As the error rate on individual relation extraction accumulates, it leads to the observed drop in accuracy as k becomes larger. Brenden Lake and Marco Baroni. 2018. Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks. In ICML.</p>
<p>Figure 2 :
2The Scallop program used in the CLUTRR reasoning task.BERTa as the back-bone LM. DSR-w2v-BiLSTM, on the other hand, uses as back-bone the word2vec(Mikolov et al., 2013)  model for word embedding and BiLSTM(Huang et al., 2015)  for sequential encoding. For DSR-LM-with-Manual-Rule we treat the logic rules as given, meaning that we provide 92 composition rules for CLUTRR and around 180 rules for DBpedia-INF. In this case, we set ground truth rules to have 1.0 weight and therefore φ is not learnt. Then, we have DSR-LM-without-IC which does not have integrity constraints and semantic loss. Lastly, we have DSR-without-LM that takes ground truth structured entity relation graph as input. This way, we do not need the underlying relation extractor and only φ needs to be learned.Baselines. We compare DSR-LM with a spectrum of baselines from purely neural to logically structured. The baselines include pretrained large language models (BERT (Kenton and Toutanova, 2019) and RoBERTa (Liu et al., 2019)), non-LM counterparts (BiLSTM(Hochreiter and Schmidhuber, 1997; Cho et al., 2014)  and BERT-LSTM), structured models(GAT (Veličković et al., 2018),RN (Santoro et al., 2017), and MAC (Hudson and  Manning, 2018)), and other neuro-symbolic models(CTP (Minervini et al., 2020),RuleBert (Saeed  et al., 2021)). The structured models include those models with relational inductive biases, while the neuro-symbolic model uses logic constraints.</p>
<p>Figure 3 :
3-Shot (GPT-3 5S)(Brown et al., 2020), as well as Zero-Shot-CoT (GPT-3 ZS-CoT) (Kojima et al., 2022a) settings. For fair comparison, we also include the ground truth kinship composition knowledge in GPT-3 zero shot (GPT-3 ZS w/ Rule), and 5 shot (GPT-3 5S w/ Rule). We include the prompts we used and additional details in Appendix A. DSR-LM's performance on CLUTRR compared with various baselines</p>
<p>Figure 4 :
4Systematic generalization performance comparison on CLUTRR dataset. Models except GPT-3-ZS*, GPT-3-FS are trained (or fine-tuned) on k ∈ {2, 3}. All models are tested on k ∈ {2, . . . , 10}.</p>
<p>Table 1 :
1Respective advantages of language models and symbolic reasoners.</p>
<p>Table 2 :
2Higher-order predicate examples.</p>
<p>Table 3 :
3DBpedia-INF generalization evalu-
ation under different test reasoning length. 
Models are trained on 10K reasoning length 
k = 0 sequences, and tested on sequences of 
reasoning length k = [0, 5]. </p>
<p>Table 5 :
5Ablation study about neural backbones of DSR-LM. We compare the CLUTRR performance of DSR-LM using different LMs.</p>
<p>The impact of the relation extractor. To understand what causes the failure case of DSR-LM, we study the performance of our relation classification LM-without-IC 51.48 ± 0.57 DSR-LM-with-Manual-Rule 61.34 ± 1.56Model 
Accuracy (%) 
DSR-LM 
60.98 ± 2.64 
DSR-</p>
<p>Table 6 :
6Ablation study. We compare our model's performance on CLUTRR with different setups. model separately. We isolate the trained relation extractor and found that it reaches 84.69% accuracy on the single relation classification task. For comparison, we train a relation extractor using all the intermediate labels in the training dataset, and it reaches 85.32% accuracy. It shows that even using only weak supervision (i.e., the final answers to multi-hop questions), our approach can reach onpar performance as supervised relation extraction.Reasoning over structured KBs. To understand the rule learning capability of our approach, we design our ablation model DSR-without-LM to take as input ground-truth KBs instead of natural language. In this case, rule weights are not initialized by LM but randomized. As shown inTable 7, our model outperforms GAT and CTP which also operates on structured KBs. It demonstrates that our differentiable rule learning paradigm learns rules to reason about KBs consistently.Model 
Accuracy (%) 
GAT 
39.05 
CTP 
95.57 
DSR-without-LM 
98.81 </p>
<p>Table 7 :
7DSR-without-LM compared against GAT and CTP on reasoning with ground truth KBs. For this comparison we train on k ∈ [2, 3] and test on k ∈ [4, 10].</p>
<p>Peter Clark, Oyvind Tafjord, and Kyle Richardson. 2021. Transformers as soft reasoners over language. In IJCAI. Creswell, Murray Shanahan, and Irina Higgins. 2022. Selection-inference: Exploiting large language models for interpretable logical reasoning. arXiv preprint arXiv:2205.09712. Elazar, Nora Kassner, Shauli Ravfogel, Abhilasha Ravichander, Eduard Hovy, Hinrich Schütze, and Yoav Goldberg. 2021. Measuring and improving consistency in pretrained language models. TACL. Evans and Edward Grefenstette. 2018. Learning explanatory rules from noisy data. Journal of Artificial Intelligence Research, 61:1-64. Klaus Greff, Sjoerd Van Steenkiste, and Jürgen Schmidhuber. 2020. On the binding problem in artificial neural networks. arXiv preprint arXiv:2012.05208. Hase, Mona Diab, Asli Celikyilmaz, Xian Li, Zornitsa Kozareva, Veselin Stoyanov, Mohit Bansal, and Srinivasan Iyer. 2021. Do language models have beliefs? methods for detecting, updating, and visualizing model beliefs. arXiv preprint arXiv:2111.13654. Helwe, Chloé Clavel, and Fabian M. Suchanek. 2021. Reasoning with transformer-based models: Deep learning, but shallow reasoning. In AKBC. Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory. Neural computation. Jiani Huang, Ziyang Li, Binghong Chen, Karan Samel, Mayur Naik, Le Song, and Xujie Si. 2021. Scallop: From probabilistic deductive databases to scalable differentiable reasoning. NeurIPS. Zhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidirectional lstm-crf models for sequence tagging. arXiv preprint arXiv:1508.01991. Drew A Hudson and Christopher D Manning. 2018. Compositional attention networks for machine reasoning. In ICLR. Kahneman. 2011. Thinking, fast and slow. Macmillan. Nora Kassner, Benno Krojer, and Hinrich Schütze. 2020. Are pretrained language models symbolic reasoners over knowledge? CoNLL. Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In NAACL. Yoon Kim. 2021. Sequence-to-sequence learning with latent neural grammars. NeurIPS. Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022a. Large language models are zero-shot reasoners. NeurIPS. Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022b. Large language models are zero-shot reasoners. NeurIPS.Antonia Yanai Richard Peter Chadi Sepp Daniel Takeshi Takeshi 
A length of 0 means that the hypothesis can be verified using the facts alone without using any rules.
Concluding RemarksWe investigate how to improve LMs' logical reasoning capability using differentiable symbolic reasoning. Through extensive experiments, we demonstrate the effectiveness of DSR-LM over challenging scenarios where widely deployed large LMs fail to reason reliably. We hope our work can lay the groundwork for exploring neuro-symbolic programming techniques to improve the robustness of LMs on reasoning problems.A Implementation DetailsHardware. We perform all the experiments on a server with two 20-core Intel Xeon CPUs, four GeForce RTX 2080 Ti GPUs, and 768 GB RAM.Reasoner details. The learning of rules and the fine-tuning of the underlying LM should happen separately with different learning rates -finetuning LM is an intricate process that requires a very small learning rate, while rules should be learned with larger learning rates since gradients are directly back-propagated onto the weights. This can be realized by employing two separate optimizers, one for fine-tuning and the other for rule learning. During training time, we rotate training the two parts by toggling one and the other optimizer for every 10 batches of data points.Rule learning training setup. For rule learning, we can initialize the transitivity tensor using the language model provided composite rules. Since the CLUTRR dataset consists of 20 different relations and a transitivity relationship is defined over 3 relations, there are 8K possible transitivity facts over these relations. Specifically, we give every predicted composite rule by the GPT with a 0.5 weight, while initializing the other rules with a range such as [0, 0.1], since otherwise, an insensible transitive fact may be getting a random high weight while it effectively does nothing for reasoning. The learning process encourages the rules that yield the correct query result and suppresses the rules that lead to wrong answers. To avoid the exponential blow-up caused by injecting all the 8K rules in the reasoning engine, we sample 200 rules according to their weights during the training time and deterministically use the top 200 learned rules during the test time. For the QA-No-Rule setup, the confidence score of rules, the MLP classifier for relation extraction, and the underlying LM are learned and updated simultaneously during training. To account for their difference, we employ two Adam optimizers A RL and A RE . A RE is used for optimizing models for relation extraction, and thus will take as parameters the MLP classifier and the underlying LM. It has a low learning rate 0.00001 since it needs to fine-tune LMs. A RL , on the other hand, will take as a parameter the confidence score tensor for the transitive rules, and is set to have a higher learning rate of 0.001. For the integrity constraints, we set the result integrity violation loss with the weight 0.1, and set the rule integrity constraint violation loss with the weight 0.01. We set the batch size to 16 and train for 20 epochs.To obtain the initial rule weights for the composition rule in our CLUTRR experiment, the prompt we use is "Mary's P's Q is her <mask>." where P and Q are enumerations of all possible relationships, and the unmasked value is treated as the answer R, producing composite(P, Q, R). For the other rule templates we used, the prompts are 1. transitive: "is R's R one's R? <mask>"; the probability of the unmasked word being "yes" is treated the rule weight for transitive(R). 2. symmetric: "does A is R of B means B is R of A? <mask>"; the probability of the unmasked word being "yes" is treated the rule weight for symmetric(R).inverse: "A is R of B means B is <mask> of A"; the unmasked value is treated as the answer P, producing inverse(R, P).4. implies: "does R imply P? <mask>"; the probability of unmasked value being "yes" is treated as the rule weight for implies(R, P).GPT-3 Prompt Setups.For Zero-Shot, we use the prompt "So B is A's:" for the query pair (A, B) to ask GPT-3 to complete the relationship between A and B. We pick the phrase in the first line or before the first period from the completed text and compare it directly with the ground truth relation. For the Few(5)-Shot setting, we randomly select 5 examples from the training dataset used for other models (k ∈ [2, 3]) to serve as examples. We use the same prompt for Few-Shot and Fine-Tuned as the Zero-Shot and the automated GPT-3 finetuning setup for our training dataset, trained for 4 epochs. To add in the transitive KB, we simply include 92 hand-crafted rules in natural language as a part of the prompt, and we performed Zero-shot with KB, and Few(5)-shot with KB experiments. For the Zero-Shot-CoT setting, we use the prompt "Who is B to A? Let's think step by step" to suggest GPT-3 to auto-complete while working out a reasoning chain. Under this setup, it is impossible to compare the answer to the ground truth automatically. Therefore, we manually check through the whole test dataset of CLUTRR.B Additional Experimental ResultsIn 1. Jami is Gino's sister. 2. Ethel is Gino's mother. 3. Therefore, Jami is Ethel's daughter-in-law.nieceTable 8: Qualitative analysis of GPT-3 Zero-Shot-CoT on the CLUTRR dataset. The novelty comes from the sentence marked in orange. Queries that are of interest are marked in blue. Correct answer in the output is marked green and incorrect ones are marked red.  // Integrity constraints on results rel violation(!r) = r := forall(a, b: kinship(GRANDFATHER, a, b) =&gt; (kinship(GRANDSON, b, a) or kinship(GRANDDAUGHTER, b, a))) rel violation(!r) = r := forall(a, b: kinship(GRANDMOTHER, a, b) =&gt; (kinship(GRANDSON, b, a) or kinship(GRANDDAUGHTER, b, a))) rel violation(!r) = r := forall(a, b: kinship(FATHER, a, b) =&gt; (kinship(SON, b, a) or kinship(DAUGHTER, b, a))) rel violation(!r) = r := forall(a, b: kinship(MOTHER, a, b) =&gt; (kinship(SON, b, a) or kinship(DAUGHTER, b, a))) rel violation(!r) = r := forall(a, b: kinship(HUSBAND, a, b) =&gt; kinship(WIFE, b, a)) rel violation(!r) = r := forall(a, b: kinship(BROTHER, a, b) =&gt; (kinship(SISTER, b, a) or kinship(BROTHER, b, a))) // Integrity constraints on rules rel violation(!r) = r := forall(r1, r2, r3: composite(r1, r2, r3) and gender(r2, g) =&gt; gender(r3, g)) rel violation(!r) = r := forall(r1, r2, r3: composite(r1, r2, r3) and gen(r1, g1) and gen(r2, g2) =&gt; gen(r3, g1 + g2))Confidence
Gregor Betz, Christian Voigt, Kyle Richardson, arXiv:2009.07185Critical thinking for language models. arXiv preprintGregor Betz, Christian Voigt, and Kyle Richardson. 2020. Critical thinking for language models. arXiv preprint arXiv:2009.07185.</p>            </div>
        </div>

    </div>
</body>
</html>
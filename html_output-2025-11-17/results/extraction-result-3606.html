<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3606 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3606</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3606</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-88.html">extraction-schema-88</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to estimate the probability or likelihood of future scientific discoveries or real-world events, including methods, results, and comparisons.</div>
                <p><strong>Paper ID:</strong> paper-268632153</p>
                <p><strong>Paper Title:</strong> The Potential of Neural Network Potentials</p>
                <p><strong>Paper Abstract:</strong> In the next half-century, physical chemistry will likely undergo a profound transformation, driven predominantly by the combination of recent advances in quantum chemistry and machine learning (ML). Specifically, equivariant neural network potentials (NNPs) are a breakthrough new tool that are already enabling us to simulate systems at the molecular scale with unprecedented accuracy and speed, relying on nothing but fundamental physical laws. The continued development of this approach will realize Paul Dirac’s 80-year-old vision of using quantum mechanics to unify physics with chemistry and providing invaluable tools for understanding materials science, biology, earth sciences, and beyond. The era of highly accurate and efficient first-principles molecular simulations will provide a wealth of training data that can be used to build automated computational methodologies, using tools such as diffusion models, for the design and optimization of systems at the molecular scale. Large language models (LLMs) will also evolve into increasingly indispensable tools for literature review, coding, idea generation, and scientific writing.</p>
                <p><strong>Cost:</strong> 0.009</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3606.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3606.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to estimate the probability or likelihood of future scientific discoveries or real-world events, including methods, results, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM future-trajectories speculation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Author's qualitative speculation about long-term trajectories of large language models (plateau vs continued exponential improvement)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper contains several qualitative statements about the likelihood of two broad long-term trajectories for LLMs: (1) continued exponential improvement possibly surpassing human capabilities within ~50 years, and (2) a developmental plateau with LLMs remaining powerful but not dominant. These are presented as author judgment calls rather than results of any LLM-based forecasting experiment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The Potential of Neural Network Potentials</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>prediction_task</strong></td>
                            <td>Estimating the likelihood that LLMs will (a) continue improving exponentially and surpass human capabilities within ~50 years, or (b) reach a developmental plateau and remain as powerful aids without fully supplanting human ingenuity.</td>
                        </tr>
                        <tr>
                            <td><strong>method_of_probability_estimation</strong></td>
                            <td>No empirical method described — purely qualitative reasoning by the author (textual argumentation citing three considerations: training-data limits, hallucination risk, and energy/training cost). No prompting, fine-tuning, calibration, or formal probabilistic forecasting procedure is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>None provided — no quantitative comparison to prediction markets, expert panels, or formal models; only qualitative discussion of plausibility and drivers.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Author-identified limitations underpinning uncertainty: (1) LLMs are trained on internet text limiting novelty generation; (2) persistent hallucinations / factual errors; (3) energy and computational costs of scaling models. The author explicitly states the likelihood of transformative change is "extremely hard to ascertain."</td>
                        </tr>
                        <tr>
                            <td><strong>notable_findings</strong></td>
                            <td>The paper does not present empirical findings but concludes that both trajectories are plausible; the author personally considers a prolonged plateau unlikely to last 50 years and expects some profound changes eventually, while acknowledging deep uncertainty and ethical/scientific implications if LLMs become dominant.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Potential of Neural Network Potentials', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3606.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3606.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to estimate the probability or likelihood of future scientific discoveries or real-world events, including methods, results, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM + simulation verification proposal</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Proposal to augment LLMs with access to molecular simulation/NNP-MD to validate generated knowledge and thus reduce hallucinations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The author proposes that one way to mitigate hallucinations and to generalize beyond training data is to give LLMs programmatic access to molecular simulation tools (e.g., neural-network-potential molecular dynamics and quantum chemistry) so the model could autonomously generate data and validate hypotheses; this is proposed as a route to enable LLMs to assess or improve the likelihood of scientific claims.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The Potential of Neural Network Potentials</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>prediction_task</strong></td>
                            <td>Enabling an LLM to generate hypotheses and then estimate/validate their likelihood by running molecular simulations (NNP-MD / quantum chemistry) to generate supporting data and thereby assess probability/credibility of scientific claims or discoveries.</td>
                        </tr>
                        <tr>
                            <td><strong>method_of_probability_estimation</strong></td>
                            <td>No concrete implementation provided — described conceptually: augmenting an LLM with programmatic access to simulation tools so it can generate validation data; no description of prompting methods, calibration, uncertainty quantification, or probabilistic output formats.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>None empirical; the idea is contrasted conceptually with vanilla LLMs that only rely on training data, but no quantitative baseline comparisons are given.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Challenges noted implicitly or explicitly: difficulty in ascertaining likelihood of such a transformative augmentation; persistent issues with hallucination may not be fully solved; computational cost and integration complexity; need for robust verification pipelines and ethical/societal implications if LLMs autonomously validate and act on hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_findings</strong></td>
                            <td>The paper offers this augmentation as a promising direction to improve LLM reliability for scientific claims but provides no experimental evidence, results, or benchmarks demonstrating that LLMs augmented in this way can accurately estimate probabilities of future discoveries or events.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Potential of Neural Network Potentials', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Autonomous Chemical Research with Large Language Models <em>(Rating: 2)</em></li>
                <li>The Future of Chemistry Is Language <em>(Rating: 1)</em></li>
                <li>De Novo Design of Protein Structure and Function with RFdiffusion <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3606",
    "paper_id": "paper-268632153",
    "extraction_schema_id": "extraction-schema-88",
    "extracted_data": [
        {
            "name_short": "LLM future-trajectories speculation",
            "name_full": "Author's qualitative speculation about long-term trajectories of large language models (plateau vs continued exponential improvement)",
            "brief_description": "The paper contains several qualitative statements about the likelihood of two broad long-term trajectories for LLMs: (1) continued exponential improvement possibly surpassing human capabilities within ~50 years, and (2) a developmental plateau with LLMs remaining powerful but not dominant. These are presented as author judgment calls rather than results of any LLM-based forecasting experiment.",
            "citation_title": "The Potential of Neural Network Potentials",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "prediction_task": "Estimating the likelihood that LLMs will (a) continue improving exponentially and surpass human capabilities within ~50 years, or (b) reach a developmental plateau and remain as powerful aids without fully supplanting human ingenuity.",
            "method_of_probability_estimation": "No empirical method described — purely qualitative reasoning by the author (textual argumentation citing three considerations: training-data limits, hallucination risk, and energy/training cost). No prompting, fine-tuning, calibration, or formal probabilistic forecasting procedure is reported.",
            "dataset_or_benchmark": null,
            "performance_metrics": null,
            "comparison_to_baselines": "None provided — no quantitative comparison to prediction markets, expert panels, or formal models; only qualitative discussion of plausibility and drivers.",
            "limitations_or_challenges": "Author-identified limitations underpinning uncertainty: (1) LLMs are trained on internet text limiting novelty generation; (2) persistent hallucinations / factual errors; (3) energy and computational costs of scaling models. The author explicitly states the likelihood of transformative change is \"extremely hard to ascertain.\"",
            "notable_findings": "The paper does not present empirical findings but concludes that both trajectories are plausible; the author personally considers a prolonged plateau unlikely to last 50 years and expects some profound changes eventually, while acknowledging deep uncertainty and ethical/scientific implications if LLMs become dominant.",
            "uuid": "e3606.0",
            "source_info": {
                "paper_title": "The Potential of Neural Network Potentials",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "LLM + simulation verification proposal",
            "name_full": "Proposal to augment LLMs with access to molecular simulation/NNP-MD to validate generated knowledge and thus reduce hallucinations",
            "brief_description": "The author proposes that one way to mitigate hallucinations and to generalize beyond training data is to give LLMs programmatic access to molecular simulation tools (e.g., neural-network-potential molecular dynamics and quantum chemistry) so the model could autonomously generate data and validate hypotheses; this is proposed as a route to enable LLMs to assess or improve the likelihood of scientific claims.",
            "citation_title": "The Potential of Neural Network Potentials",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "prediction_task": "Enabling an LLM to generate hypotheses and then estimate/validate their likelihood by running molecular simulations (NNP-MD / quantum chemistry) to generate supporting data and thereby assess probability/credibility of scientific claims or discoveries.",
            "method_of_probability_estimation": "No concrete implementation provided — described conceptually: augmenting an LLM with programmatic access to simulation tools so it can generate validation data; no description of prompting methods, calibration, uncertainty quantification, or probabilistic output formats.",
            "dataset_or_benchmark": null,
            "performance_metrics": null,
            "comparison_to_baselines": "None empirical; the idea is contrasted conceptually with vanilla LLMs that only rely on training data, but no quantitative baseline comparisons are given.",
            "limitations_or_challenges": "Challenges noted implicitly or explicitly: difficulty in ascertaining likelihood of such a transformative augmentation; persistent issues with hallucination may not be fully solved; computational cost and integration complexity; need for robust verification pipelines and ethical/societal implications if LLMs autonomously validate and act on hypotheses.",
            "notable_findings": "The paper offers this augmentation as a promising direction to improve LLM reliability for scientific claims but provides no experimental evidence, results, or benchmarks demonstrating that LLMs augmented in this way can accurately estimate probabilities of future discoveries or events.",
            "uuid": "e3606.1",
            "source_info": {
                "paper_title": "The Potential of Neural Network Potentials",
                "publication_date_yy_mm": "2024-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Autonomous Chemical Research with Large Language Models",
            "rating": 2,
            "sanitized_title": "autonomous_chemical_research_with_large_language_models"
        },
        {
            "paper_title": "The Future of Chemistry Is Language",
            "rating": 1,
            "sanitized_title": "the_future_of_chemistry_is_language"
        },
        {
            "paper_title": "De Novo Design of Protein Structure and Function with RFdiffusion",
            "rating": 1,
            "sanitized_title": "de_novo_design_of_protein_structure_and_function_with_rfdiffusion"
        }
    ],
    "cost": 0.008657999999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>The Potential of Neural Network Potentials
March 21, 2024</p>
<p>Timothy T Duignan 
The Potential of Neural Network Potentials
March 21, 2024E6C875B6640E8DBC481FC64F0E4C832C10.1021/acsphyschemau.4c00004Received: January 18, 2024 Revised: March 4, 2024 Accepted: March 5, 2024Molecular simulationmolecular dynamicsmachine learningartificial intelligencecoarse-graininglarge language modelsdiffusion models
In the next half-century, physical chemistry will likely undergo a profound transformation, driven predominantly by the combination of recent advances in quantum chemistry and machine learning (ML).Specifically, equivariant neural network potentials (NNPs) are a breakthrough new tool that are already enabling us to simulate systems at the molecular scale with unprecedented accuracy and speed, relying on nothing but fundamental physical laws.The continued development of this approach will realize Paul Dirac's 80-year-old vision of using quantum mechanics to unify physics with chemistry and providing invaluable tools for understanding materials science, biology, earth sciences, and beyond.The era of highly accurate and efficient firstprinciples molecular simulations will provide a wealth of training data that can be used to build automated computational methodologies, using tools such as diffusion models, for the design and optimization of systems at the molecular scale.Large language models (LLMs) will also evolve into increasingly indispensable tools for literature review, coding, idea generation, and scientific writing.</p>
<p>■ INTRODUCTION</p>
<p>Over the last year, a surge of excitement has arisen around the potentially transformative role of machine learning (ML) or artificial intelligence in many areas of technology, science, and society.This has been driven by the remarkable performance of large language models (LLMs) 1 and image generation diffusion models. 2iven the excitement around these technologies, there is a natural tendency to either assume they will profoundly transform every field or dismiss them as "overhyped".</p>
<p>A better approach is to be excited about the potential of these technologies while also thinking through as carefully as possible the potential obstructions that may arise to limit their potential, with the goal of identifying the most promising approaches and applications where they are likely to be most useful.</p>
<p>While the recent excitement around LLMs and diffusion models is justified, the potential for ML in science has long been recognized.A pioneering example is the demonstration in 2007 by Behler and Parrinello 3 that neural network potentials (NNPs), originally developed in the 1990s, 4,5 could be used to massively accelerate the simulation of liquids by learning the underlying potential energy surface from quantum chemistry calculations.</p>
<p>More recently, AlphaFold2 (AF2) 6,7 has dramatically demonstrated the ability of ML to predict the structure of folded proteins with remarkable accuracy.</p>
<p>In this article, I argue that neural network potentials, AF2, diffusion models, and even LLMs to some extent can all be understood as examples of one fundamental idea: learning and sampling from an underlying energy surface.The recursive application of this fundamental idea will have profound implications for physical chemistry and beyond over the next half-century.</p>
<p>■ DIRAC'S DREAM</p>
<p>Using quantum mechanics to predictively understand biology and chemistry has been a dream of physicists for nearly a century since Paul Dirac's famous statement:</p>
<p>The underlying physical laws necessary for the mathematical theory of a large part of physics and the whole of chemistry are thus completely known, and the difficulty is only that the exact application of these laws leads to equations that are much too complicated to be soluble.It therefore becomes desirable that approximate practical methods of applying quantum mechanics should be developed, which can lead to an explanation of the main features of complex atomic systems without too much computation. 8hysical chemistry is the natural discipline to home this dream, and much progress has been made on making it a reality.For example, accurate quantum chemistry algorithms, such as density functional theory (DFT) 9 and coupled-cluster (CC) with single, double, and perturbative triple excitations (CCSD-(T)), 10 and advanced simulation techniques have made it possible to build highly accurate models of water 11 and resolve challenging scientific questions 12 starting from nothing but Schrodinger's equation.This pioneering work convincingly demonstrates the feasibility of achieving Dirac's dream.Now the key scientific challenge becomes making this procedure efficient, automated, generalizable, and scalable.This is a task perfectly suited to ML. Achieving this dream, depicted in Figure 1, would be truly transformative, enabling the predictive understanding of the properties of a vast range of critically important systems throughout materials science, chemistry, biology, and beyond.</p>
<p>■ NEURAL NETWORK POTENTIALS</p>
<p>The most promising tool for doing this is neural network potentials (NNPs), which are a class of machine learning interatomic potentials (MLIPs).−18 The concept of NNPs is straightforward yet innovative.Instead of relying on traditional methods, such as specifying Lennard-Jones potentials and bonded terms, to define the potential energy surface (PES) as a function of atomic type and coordinates, NNPs employ more adaptable functions with a larger set of parameters.These parameters are optimized using machine learning algorithms facilitated by automatic differentiation.To avoid the need for large experimental data sets, these models are trained on data from quantum chemistry, predominantly at the DFT level, though increasingly higher levels of theory are being utilized. 19Traditional molecular dynamics (MD) algorithms can then be used to simulate the direct physical behaviors of gases, liquids, and solids using the trained NNPs.</p>
<p>While NNPs have been under development for some time, 14 recent advancements in ML have significantly enhanced their practical utility.−23 This feature, also pivotal in the success of AlphaFold2, 24 ensures that the neural network's features rotate in accordance with the rotations of the input coordinates, significantly enhancing model accuracy and reliability.A recent review provides a comprehensive and clear explanation. 25he success of equivariant models is very intuitive, as they use a natural extension of the traditional concepts of intermolecular modeling.More specifically, they use tensors to describe the atomic properties and tensor products for modeling the interactions.This approach mirrors the long-standing practice in physics, where molecular interactions have been described using the multipole expansion, i.e., using monopoles, dipoles, and quadrupoles, which are tensors, interacting via tensor products.</p>
<p>Additional exciting tools such as active and delta learning as well as improved quantum chemistry algorithms have also significantly improved the generalizability and usefulness of NNPs. 26−30 These more recent improvements have stepped up NNPs from being an occasionally useful tool to a potentially transformative one.As some recent perspective articles state, this field is undergoing "remarkable/breathtaking progress," achieving results that would be "unimaginable with conventional methods." 31,32he potential of neural network potentials is that they are a tool that provides access to a new spatial and temporal scale that has not yet been observed accurately.Such tool-driven revolutions are a recurring process in the history of science and are inevitably followed by a string of rapid and surprising discoveries.The telescope, the microscope, and the particle collider are famous examples.I believe that NNPs have the potential to approach a comparable level of significance.Initial examples of this are already beginning to emerge.For instance, important new insights into catalysis at high temperatures have already been discovered using NNP-MD simulations. 33Additionally, NNP-MD is apparently now the state of the art for protein−ligand binding affinity predictions. 34BREAKING THE ACCURACY−EFFICIENCY</p>
<p>TRADE-OFF</p>
<p>The promise of NNPs lies in their potential to transcend the long-standing trade-off between accuracy and efficiency in simulations.Specifically, they offer the possibility of achieving quantum mechanics (QM)-level accuracy at the computational cost of classical MD.This breakthrough implies that this technique will become increasingly prevalent.An important recent advance on this front has been the development of density corrected DFT methods, 9,35 where the computation of the energy and electron density are separated to achieve a better balance of accuracy.These methods have been demonstrated to provide an improved description of aqueous solutions 35−39 while maintaining affordable computational requirements.They are therefore well-suited to providing training data for NNPs.We have recently shown that accurate and efficient prediction of key electrolyte solution properties is possible by combining this method with an equivariant NNP. 40,41REMAINING CHALLENGES Although rapid progress is being made, several challenges remain toward fully realizing the potential of NNPs.One example accounts for long-range interactions.These are known to play an important role in many liquid phase phenomena. 42,43−52 Another is the generation of sufficiently diverse training data sets, where active learning plays a crucial role.−55 The use of structural databases generated at lower levels of theory is another promising option. 17,56he accuracy of the underlying quantum chemical method used to generate the training data is also an issue, but there is significant exciting progress on this front too, as discussed below.</p>
<p>Speed and scalability also present significant hurdles.Equivariant NNPs, while powerful, tend to be slower than classical MD and standard MLIPs.Although they can handle large systems, 57,58 this requires substantial computational resources.One solution is using equivariant NNPs to generate data for training faster non-equivariant architectures.There are many promising architectures from the more general family of MLIPs such as permutation invariant polynomials (PIPs) and Gaussian approximation potentials (GAPs), which generally appear to have similar performance to standard Behler− Parrinello-type NNPs and may be able to provide speed advantages. 59,60Data augmentation may be able to avoid the need for equivariant models as well. 61Another possibility is optimizing classical MD potentials against quantum data using automatic differentiation 62 or algorithmic improvements. 63,64RECURSIVE COARSE-GRAINING Despite the promise of current approaches to improving speed, there are inherent limitations to any simulation that attempts to keep track of the position and motion of every single atom in a system.A more transformative solution lies in coarse-graining, which involves focusing on only the essential degrees of freedom of a system while neglecting the others.This is the essence of physical modeling at all scales.For example, in modeling a macroscopic object, only the center of mass of each atom is considered.The core motivation is that most of the system's information is often irrelevant for predicting its properties.</p>
<p>In physical chemistry, details such as the exact positions of every solvent molecule are usually not critical for understanding the properties of interest.Thus, continuum or implicit solvent models are used, which are a form of coarse-graining.Currently, these models are developed in a somewhat empirical manner, and a systematic and rigorous approach is needed.−71 It is not surprising in retrospect that these tools are suitable for this task.The goal of coarse-graining is to find the potential of mean force (PMF) for a reduced set of degrees of freedom.This is just an energy surface, technically a free energy, and so this is a task well-suited for NNPs.Here NNPs have the key advantage over fitting a classical force field, as the free energy surfaces modeled by NNPs can capture a more complex-shaped landscape than the typical bonded or Lennard-Jones interactions used in a classical force field.In fact, fitting an NNP to the full PES is a form of coarsegraining itself.It is just the electronic positions that are the degrees of freedom that are being coarse-grained out.Protein folding models such as AF2 and RFdiffusion can actually be interpreted as an NNP that learns a coarse-grained free energy, as discussed below.</p>
<p>So the promising conclusion of this is that all atom equivariant NNPs trained on QM data can be used to recursively train coarse-grained NNPs. 41This could enable large-scale biological simulations starting solely from first principles.This process, illustrated in Figure 2, may have a transformative impact on biology and materials science if the remaining challenges can be overcome and may eventually supplant tools like AlphaFold2.We have demonstrated the feasibility of this process of recursively training a coarse-grained equivariant NNP on its own output in a recent preprint for simple electrolyte solutions. 41urrently, coarse-graining with an NNP is applied to relatively small-scale classical systems, such as simplifying water to a single site 72 or for proteins. 67,69,70,73However, they could potentially be iteratively applied to increasingly larger scales, perhaps one day encompassing complexes of multiple proteins, cell membranes, etc.</p>
<p>While the core principle of automatically creating reduceddimensional representations will likely remain constant, advancements may be required in determining which features to retain at a higher resolution, like binding sites, and which to coarse-grain; here again, ML will be a critical tool. 74his methodology may even be a general solution to the longstanding and central challenge of molecular simulation, which is how to connect scales, i.e., how to use information from the molecular scale to inform and understand meso-and macroscopic-scale phenomena.It is also possible to reproduce finegrained representation from the coarse-grained one. 75ne importance caveat is that coarse-graining does distort the underlying dynamics, but solutions to this problem are being developed. 76,77 SIMULATING LARGE-SCALE FROM THE</p>
<p>MOLECULAR SCALE</p>
<p>The goal of this methodology is both ambitious and thrilling.It opens the possibility of simulating large-scale complex systems starting solely from Schrodinger's equation.We can envisage a future where, through a simple web interface, we input a system of atoms and obtain a highly accurate depiction of their behavior at any desired scale.This would be a more general version of the way AF2 now enables the rapid prediction of the folded structure of a protein with a simple web interface.</p>
<p>Such a tool would be transformative for many areas of science.For a great many scientific problems, the scales at which the most critical and interesting processes occur are too small to observe experimentally either with direct microscopy or with more indirect methods, such as spectroscopy or X-ray diffraction.As a result, we are often left to rely on guess work, intuition, and indirect evidence to infer what is actually happening on the molecular scale.In other words, when it comes to the molecular scale, we are essentially operating in the dark.Accurate and efficient molecular-scale simulations could act as a computational microscope, shedding light on this critically important scale and opening a wealth of new otherwise hidden information by enabling us to simply directly observe the key phenomena that occur at this scale.The application areas span most of biology and much of chemical engineering, such as catalysis and electrochemical energy storage.</p>
<p>Additionally, we can use these simulations in combination with statistical mechanics to calculate key experimental properties.For example, free energies or activities can be directly estimated from molecular-scale simulations using tools such as the Kirkwood−Buff theory.Additionally, diffusivities and other kinetic properties can also be determined.These values can then be fed as parameters into simulations of largerscale systems, where these parameters are a key source of uncertainty.For example, the activity of ions in solution is a key challenge for predicting solubilities and chemical equilibria.Similarly, climate simulations are currently limited by the lack of information about molecular-scale processes, such as the stability and formation of aerosols.Molecular simulation models that can quickly predict the distribution of aerosol particles could be connected to larger-scale climate simulation models to improve their reliability.These tools therefore have the potential to have an impact on many different areas of science across many different scales of phenomena wherever physical-based modeling is important.Additionally, in combination with automated coarse-grained systems, these can potentially be scaled to simulate systems of ever-increasing size by selectively ignoring aspects of the system that are not considered practically important.</p>
<p>■ QUANTUM CHEMISTRY ACCURACY</p>
<p>A critical limitation of NNPs is the current accuracy of methods for solving Schrodinger's equation to generate data for training NNPs.However, the field of quantum chemistry is witnessing substantial progress with exciting developments like density corrected DFT, 35 ensemble DFT, 78 DLPNO−CCSD(T), 79 large-scale random phase approximation (RPA), 80 and the density matrix renormalization group. 81These innovations promise more precise descriptions of molecular-scale systems.Additionally, ML algorithms could also have potentially important applications here by using neural network ansatzes for wave function representation, for example, 82 or for reducing the cost of traditional methods. 83,84hile DFT functional accuracy is still an issue, a key advantage is that for a great many systems of practical interest, methods such as CCSD(T) can generate very high-quality data on small systems.This means that DFT functionals can be carefully validated and fine-tuned for specific tasks.This means that the problem of DFT functional accuracy is for many cases mainly a practical problem rather than a fundamental scientific challenge.Exciting new tools for automated development of new DFT functionals trained on data from higher-quality quantum chemistry methods is another promising recent development. 85owever, for some materials, we still lack good benchmark methods.For example, highly multireference states, where fundamental new tools are required.Multireference methods such as complete active space self-consistent field (CASSCF) can be used, but they come with their own set of challenges and limitations, particularly in terms of computational cost and complexity.</p>
<p>■ COMPUTATIONAL HARDWARE</p>
<p>Another potential avenue for advancement lies in the development of new, powerful hardware specifically designed for molecular simulation and quantum chemistry calculations.While quantum computers are often cited in this context, it is debatable whether using qubits will provide a practically relevant quantum chemistry acceleration. 86,87Other burgeoning technologies, such as thermodynamic computing, may prove useful instead.Additionally, the ongoing rapid development of new computational hardware for general computing and ML, such as large-memory graphics processing units (GPUs), should be sufficient to provide significant improvements in quantum chemistry and molecular simulation using existing algorithms.</p>
<p>The development of new computing architectures relies on components that are becoming increasingly small and will continue to approach the molecular scale.The design of these components could potentially be significantly advanced through highly accurate molecular simulations, which could be used to predict and optimize important properties such as heat generation, etc. Improvements in molecular simulation enabled by NNPs could therefore lead to a feedback process in which they enable improvements in computing.</p>
<p>■ DIFFUSION MODELS</p>
<p>Accurate molecular simulation is a powerful tool for understanding and predicting the properties of various systems.However, when it comes to generating and designing new systems, an additional new tool from ML is increasingly coming into play: diffusion models.These models, though initially complex to grasp, especially as often presented, are fundamentally about learning a generalized version of a potential energy surface.</p>
<p>To conceptualize diffusion models from a physical point of view, consider the pixel values to represent atomic coordinates.Then, just as certain coordinates have a high probability of being observed in an equilibrium simulation, certain pixel values have a high probability of having certain properties, i.e., being a face.</p>
<p>To generate these high-probability coordinates/pixel values, we evolve the coordinates/pixel values according to equations of motion with a force field, which is learned from the data via a denoising process.In fact, a diffusion model can be trained on molecular coordinates from an equilibrium distribution, which, in turn, reproduces the true underlying force field responsible for that distribution. 88Diffusion models then can be considered again as a special case of an NNP, where they are learning a coarse-grained free energy surface.An intriguing aspect of these models is that they can include "forces" for properties other than positions, such as atomic numbers, allowing them to sample from a variety of atomic types.This flexibility enables targeted sampling from specific regions of the probability distribution, akin to biasing MD simulations toward areas of the phase space of particular interest. 89,90Technically a time-dependent force is used to improve the convergence of the sampling, but this is a technicality.</p>
<p>This methodology is increasingly promising for the generation and design of new materials and molecules.It is already proving capable of protein design, leveraging databases like the Protein Data Bank (PDB) to identify new specific folded protein structures. 89</p>
<p>■ ALPHAFOLD2</p>
<p>One potential challenge to the importance of NNPs is AF2.AF2 represents a significant breakthrough in protein structure prediction.Its capability allows anyone to predict the atomically resolved structure of a protein, with accuracy now comparable to that obtained from nuclear magnetic resonance (NMR) techniques, 91 marking a paradigm shift in structural biology.While this is not traditionally considered a core problem of physical chemistry, there are many closely related problems, i.e., predicting the atomic structure of a chemical system in thermodynamic equilibrium.Most significantly, AF2 does not explicitly rely on physical notions of energy and MD.An intriguing question then arises as to whether this approach could be used to revolutionize biology and chemistry without the need for physics-based modeling.This is highly unlikely, in my opinion.The key to AF2's success is the extensive, high-quality Protein Data Bank (PDB). 92This immediately highlights a challenge to generalizing this approach.The PDB contains high-quality atomic structures of over 100 000 proteins with each one taking significant investment to obtain.This took many decades to build.There are vanishingly few problems of practical scientific interest, especially in physical chemistry, where these kinds of extremely large, high-quality databases exist.This becomes evident as soon as we leave the region of AF2's success.</p>
<p>For instance, AF2 cannot predict the folding pathway or kinetics of a protein due to its reliance on the static folded structures found in the database. 93Furthermore, the database lacks information on the distributions of many ions near the protein surfaces, limiting AF2's ability to predict properties influenced by electrolyte concentrations, such as protein agglomeration, ion conductivities, etc.</p>
<p>Additionally, AF2 has a deep conceptual relationship to NNPs.It has been demonstrated that AF2 can be interpreted as using a learned energy landscape to optimize the protein structure; 94 the main difference is that AF2 is trained on experimental obtained structures rather than quantum chemical data, but experimental structures still contain important information about the energy landscape: they must lie on its minima.As AF2 uses gradient descent to optimize the protein structures, it must have learned an energy landscape with the same energy minima as the true protein energy landscape.The true protein energy landscape can be rigorously described as the coarse-grained free energy surface where the solvent degrees of freedom have been integrated (or marginalized) out.This energy surface may look quite different from the true free energy surface far from a minimum, but this is not an issue for obtaining minimum energy structures, although it is for dynamic information.</p>
<p>For diffusion models trained on the PDB, the connection is even clearer.The score learned by the model is equivalent to the gradient of a coarse-grained free energy.If one could train the diffusion model on the true equilibrium distribution of states occupied by the protein, then one would learn the true coarsegrained free energy.The use of PDB training data means that one is effectively training on coordinates extracted at 0 K; so the off-equilibrium distribution will be wrong, but it will still have the correct minima.In fact, Langevin dynamics are used in diffusion models, which is a common algorithm to perform molecular simulations.Thus, something like RFdiffusion 89 is actually a coarse-grained MD simulation just on an approximation of the true coarse-grained free energy surface.</p>
<p>■ THE ONLY ALTERNATIVE?</p>
<p>The dependence of AF2 on extensive high-quality experimental data sets indicates that the same strategy will not work for predicting a broader range of important phenomena.This situation has a parallel to the development of AlphaGo, an ML algorithm that can play the board game Go, which was trained on data from human games accumulated over many decades.To improve the performance beyond what was learnable from human games, AlphaZero was developed, which does not require any human-generated training data and learns through self-play based only on the rules of the game. 95Similar recent breakthroughs in mathematical problem solving use a similar strategy. 96 natural question then arises: can we use this same strategy for solving scientific problems to avoid the need for large, handcurated experimental databases?Doing so would require one to know the fundamental rules of the game that nature obeys.Of course, this is exactly what quantum mechanics is.It can in principle predict the behavior of all biological and chemical systems as captured by Dirac's dream.NNPs are therefore the best option for using ML to have a broad and significant impact on physical chemistry and beyond.</p>
<p>■ ROBOTICS AND SELF-DRIVING LABORATORIES</p>
<p>Laboratories will also become increasingly automated as robotics continues to improve, reducing the need for humans in the loop experimentation and enabling automated active learning feedback where errors in computational methods are identified and rectified through iterative exploration of regions of high uncertainty.</p>
<p>■ END-TO-END DIFFERENTIABILITY Another critical advancement, which was also key to the success of AF2, is the concept of end-to-end differentiability.Essentially, this means that it should be possible to take derivatives of any parameter used in modeling a system with respect to the final loss function.This approach is exemplified by graph neural networks, which many modern NNP architectures successfully use.The idea is that rather than imposing arbitrary descriptors of the atomic structure, a flexible and general function can be used with many adjustable parameters that are then optimized using stochastic gradient descent just as the other parameters in a neural network are fitted. 97nd-to-end differentiability is also showing promise in applications to simulations themselves where gradients can be taken through simulations, allowing the automatic optimization of parameters with respect to output properties of molecular simulations.For example, this can be used to fit interatomic potentials to experimentally obtainable structural data. 98It is also an important component of modern generative models for drug discovery. 99Finally, this idea is also being applied to DFT functional development, where all of the key parameters used to define a DFT functional are treated as differentiable parameters that can be optimized with respect to some loss. 85LARGE LANGUAGE MODELS An obvious impact of ML in physical chemistry is through large language models (LLMs), e.g., ChatGPT, which have dramatically expanded their capabilities recently.These models can assist in various academic activities, including writing, brainstorming, literature searches, education, and even language translation for non-native English speakers.Their utility is expected to increase as they become more accurate and less prone to producing erroneous outputs ("hallucinations").Recent articles provide a detailed demonstration of the potential of LLMs in chemistry more generally. 1,100LMs will become increasingly integral to scientific meetings and conferences, offering real-time notes, suggestions, and relevant literature based on the extensive training data encoded in their weights.They will play a crucial role in reviewing and improving the scientific literature, making research more accessible and efficient.Moreover, their capability in condensing vast amounts of data will likely foster novel connections and insights in research.</p>
<p>Furthermore, LLMs will be instrumental in programming, aiding researchers with limited coding proficiency in translating their ideas into executable code or converting code across different programming languages.This will significantly enhance scientific productivity and democratize computational tools, making cutting-edge research accessible to a broader spectrum of scientists.</p>
<p>LLMs will help automate the generation of MD trajectories and the building and curation of large databases.LLMs will not only suggest improvements for optimizing system performance but may also autonomously manage simulations, drawing from vast data sets to propose novel research directions.</p>
<p>There is also an intriguing connection between NNPs and LLMs.LLMs use a Boltzmann distribution, via the softmax activation function, to compute probabilities from the output of the transformer during next word prediction.They can therefore be interpreted as predicting an energy for each token in the vocabulary as a function of the previous words in the sentence, i.e., as a series of NNPs.While this is a more tenuous connection than for diffusion models, it hints at the deep connections between statistical mechanics and deep learning, which remain to be fully uncovered.</p>
<p>■ LONGER TERM</p>
<p>As we look beyond the next decade, two potential trajectories emerge.One radical possibility is that LLMs continue to improve at the current exponential pace, potentially surpassing human capabilities in 50 years, leading to a future dominated by ML-driven science.Alternatively, these tools might reach a plateau, maintaining their status as invaluable aids but still leaving room for significant human contribution.This latter scenario presents an intriguing landscape for future speculation.</p>
<p>The likelihood of LLMs reaching a developmental plateau arises from three considerations.First, they are trained on Internet text, which, while extensive, inherently limits their capacity to generate novel knowledge.Second, the issue of "hallucinations" or generating false information remains a challenge.While scaling up models and refined training methods (such as reinforcement learning from human feedback) may mitigate this, this may be a more fundamental and challenging problem to resolve.The third reason that LLMs might plateau in performance may be the excessive energy demands of training ever larger models.Here again, advances in materials science to design new, more efficient forms of computing will be key, and improved molecular simulation enabled by NNPs will be a critically useful tool for this.</p>
<p>■ A MORE RADICAL FUTURE</p>
<p>One potential avenue to address the issues of hallucination and generalizing beyond the training data is to enable LLMs to verify or validate their generated knowledge.For example, in the area of physical chemistry, this could be enabled by augmenting an LLM by providing it access to molecular simulation tools such as NNP-MD and quantum chemistry.It could then automatically go about generating the necessary data and validating hypotheses it has generated autonomously.The likelihood of such a transformative leap in ML capabilities is extremely hard to ascertain, but if it does happen and continues to grow exponentially, it will be such a significant shift across all spheres of human society that speculation is only of limited use.In my personal view, while a temporary plateau in ML development might persist for decades, it seems improbable that it will extend for the next 50 years, and so I would expect at some point to witness profound changes in the way science and society operate.</p>
<p>This evolution raises profound and challenging questions both scientifically and ethically.A particularly sobering prospect is the potential redundancy of human involvement in scientific discovery courtesy of highly efficient LLMs or their successors.In such a scenario, the human role might be relegated to determining the tasks we would like ML to solve rather than engaging in direct scientific inquiry.</p>
<p>In the field of physical chemistry, many fundamental questions bear immense practical significance.It might be necessary to leverage ML's capabilities to address these.However, we could also consider one day preserving certain areas within science as "ML-free zones", allowing human scientists to explore them independently.Alternatively, we might use ML to develop technologies to enhance our quality of life while deliberately avoiding full comprehension of their workings to preserve a sense of purpose and discovery for ourselves.</p>
<p>While the prospect of ML solving complex scientific problems might initially seem disheartening, especially considering the potential obsolescence of human-led research, it is important to recognize the brighter side.In a world where ML resolves most material challenges, we could exist in a state of near-universal abundance.This shift might mitigate any disappointment stemming from the reduced role of human discovery.Moreover, even in such a world, humans could still find purpose in understanding and replicating ML-derived solutions, thus continuing the quest for knowledge and innovation.</p>
<p>■ CONCLUSIONS ML is poised to revolutionize physical chemistry, reshaping our approach to solving many long-standing problems.A pivotal advancement facilitated by ML combined with state-of-the-art quantum chemistry is the ability to conduct molecular-scale simulations with unprecedented accuracy and speed.While there are still challenges to be overcome, this is already beginning with equivariant neural network potentials (NNPs) trained on extensive data sets derived from solutions of Schrodinger's equation.To access increasingly larger scales, these models will be recursively trained to automatically generate good coarse-grained descriptions, i.e., reduced dimension representations, thereby solving the long-standing challenge of connecting the microscopic and macroscopic scales and dramatically widening the scope of applications where firstprinciples physical simulation plays a critical role.</p>
<p>LLMs will improve efficiency and knowledge discovery.Endto-end differentiable simulations will efficiently compute the thermodynamic and kinetic properties of diverse systems.Diffusion models will be used to automate the generation of new potential candidate molecules and materials with particular properties.</p>
<p>Eventually, upon finalizing a design, it will be synthesized in automated laboratories, robotically produced, and dispatched for testing and further optimization based on experimental data.This vision heralds a new era in our understanding and manipulation of chemical systems, offering boundless opportunities for discovery and innovation.</p>
<p>Finally, it is worth considering the possibility of continued exponential advancements in ML.The implications of such a development are challenging to predict, but they could potentially reduce the field of physical chemistry to a discipline where our primary role is to ask LLMs to teach us complex concepts.The timeline for such a shift, whether it spans decades or centuries, remains an intriguing and open question.</p>
<p>■ AUTHOR INFORMATION</p>
<p>Figure 1 .
1
Figure 1.DALL-E's depiction of the potential of neural network potentials.</p>
<p>Figure 2 .
2
Figure 2. Depiction of a method of connecting the microscopic scale to the macroscopic scale via the recursive application of equivariant neural network potentials.</p>
<p>https://doi.org/10.1021/acsphyschemau.4c00004 ACS Phys. Chem Au 2024, 4, 232−241
This article is licensed under CC-BY-NC-ND 4.0
Autonomous Chemical Research with Large Language Models. D A Boiko, R Macknight, B Kline, G Gomes, 10.1038/s41586-023-06792-0Nature. 20237992</p>
<p>J Ho, A Jain, P Abbeel, Denoising, 10.48550/arXiv.2006.11239?urlappend=%3Fref%3DPDF&amp;jav=VoR&amp;rel=cite-asDiffusion Probabilistic Models. 2020</p>
<p>Generalized Neural-Network Representation of High-Dimensional Potential-Energy Surfaces. J Behler, M Parrinello, 10.1103/PhysRevLett.98.146401No. 146401Phys. Rev. Lett. 14982007</p>
<p>Potential Energy Surfaces for Macromolecules. A Neural Network Technique. B G Sumpter, D W Noid, 10.1016/0009-2614(92)85498-YChem. Phys. Lett. 5−61992</p>
<p>Neural Network Models of Potential Energy Surfaces. T B Blank, S D Brown, A W Calhoun, D J Doren, 10.1063/1.469597J. Chem. Phys. 101995</p>
<p>Highly Accurate Protein Structure Prediction with AlphaFold. J Jumper, R Evans, A Pritzel, T Green, M Figurnov, O Ronneberger, K Tunyasuvunakool, R Bates, A ̌ídek, A Potapenko, A Bridgland, C Meyer, S A A Kohl, A J Ballard, A Cowie, B Romera-Paredes, S Nikolov, R Jain, J Adler, T Back, S Petersen, D Reiman, E Clancy, M Zielinski, M Steinegger, M Pacholska, T Berghammer, S Bodenstein, D Silver, O Vinyals, A W Senior, K Kavukcuoglu, P Kohli, D Hassabis, 10.1038/s41586-021-03819-2Nature. 20217873</p>
<p>The Impact of AlphaFold2 One Year On. D T Jones, J M Thornton, 10.1038/s41592-021-01365-3Nat. Methods. 192022</p>
<p>Quantum Mechanics of Many-Electron Systems. P A M Dirac, 10.1098/rspa.1929.0094Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences. the Royal Society A: Mathematical, Physical and Engineering SciencesRoyal Society1929</p>
<p>Improving Results by Improving Densities: Density-Corrected Density Functional Theory. E Sim, S Song, S Vuckovic, K Burke, 10.1021/jacs.1c11506?urlappend=%3Fref%3DPDF&amp;jav=VoR&amp;rel=cite-asJ. Am. Chem. Soc. 202215</p>
<p>A Fifth-Order Perturbation Comparison of Electron Correlation Theories. K Raghavachari, G W Trucks, J A Pople, M Head-Gordon, 10.1016/S0009-2614(89)87395-6Chem. Phys. Lett. 61989</p>
<p>Realistic Phase Diagram of Water from "First Principles" Data-Driven Quantum Simulations. S L Bore, F Paesani, 10.1038/s41467-023-38855-1Nat. Commun. 202313349</p>
<p>Toward an Understanding of the Specific Ion Effect Using Density Functional Theory. M D Baer, C J Mundy, 10.1021/jz200333b?urlappend=%3Fref%3DPDF&amp;jav=VoR&amp;rel=cite-asJ. Phys. Chem. Lett. 292011</p>
<p>Neural Network Potentials: A Concise Overview of Methods. E Kocer, T W Ko, J Behler, 10.1146/annurev-physchem-082720-034254Annu. Rev. Phys. Chem. 732022</p>
<p>Four Generations of High-Dimensional Neural Network Potentials. J Behler, 10.1021/acs.chemrev.0c00868?urlappend=%3Fref%3DPDF&amp;jav=VoR&amp;rel=cite-asChem. Rev. 202116</p>
<p>AIMNet2: A Neural Network Potential to Meet Your Neutral, Charged, Organic, and Elemental-Organic Needs. D Anstine, R Zubatyuk, O Isayev, 10.26434/chemrxiv-2023-296ch?urlappend=%3Fref%3DPDF&amp;jav=VoR&amp;rel=cite-asChemRxiv. 2023</p>
<p>Towards Predictive Design of Electrolyte Solutions by Accelerating Ab Initio Simulation with Neural Networks. J Zhang, J Pagotto, T T Duignan, 10.1039/D2TA02610DJ. Mater. Chem. A. 2022371019560−19571</p>
<p>High-Throughput Aqueous Electrolyte Structure Prediction Using IonSolvR and Equivariant Graph Neural Network Potentials. S Baker, J Pagotto, T T Duignan, A J Page, 10.1021/acs.jpclett.3c01783?urlappend=%3Fref%3DPDF&amp;jav=VoR&amp;rel=cite-asJ. Phys. Chem. Lett. 202342</p>
<p>Molecular Simulation Meets Machine Learning. R J Sadus, 10.1021/acs.jced.3c00553?urlappend=%3Fref%3DPDF&amp;jav=VoR&amp;rel=cite-asJ. Chem. Eng. Data. 20241</p>
<p>Coupled Cluster Molecular Dynamics of Condensed Phase Systems Enabled by Machine Learning Potentials: Liquid Water Benchmark. J Daru, H Forbert, J Behler, D Marx, 10.1103/PhysRevLett.129.226001No. 226001Phys. Rev. Lett. 202222129</p>
<p>Learning Rotationally Equivariant Features in Volumetric Data. M Weiler, M Geiger, M Welling, W Boomsma, T Cohen, Steerable Cnns, Advances in Neural Information Processing Systems. </p>
<p>. NeurIPS. 312018</p>
<p>E(3)-Equivariant Graph Neural Networks for Data-Efficient and Accurate Interatomic Potentials. S Batzner, A Musaelian, L Sun, M Geiger, J P Mailoa, M Kornbluth, N Molinari, T E Smidt, B Kozinsky, 10.1038/s41467-022-29939-5Nat. Commun. 24532022</p>
<p>Advancing Molecular Simulation with Equivariant Interatomic Potentials. S Batzner, A Musaelian, B Kozinsky, 10.1038/s42254-023-00615-xNat. Rev. Phys. 20238</p>
<p>T Maxson, T Szilvási, 10.48550/arXiv.2402.16204Transferable Water Potentials Using Equivariant Neural Networks. arXiv 2024. 1</p>
<p>Feature Articles Protein Structure Prediction by AlphaFold 2 : Are Attention and Symmetries All You Need ?. N Bouatta, P Sorger, M Alquraishi, 10.1107/S2059798321007531Structural Biology. 772021</p>
<p>A Hitchhiker's Guide to Geometric GNNs for 3D Atomic Systems. A Duval, S V Mathis, C K Joshi, V Schmidt, S Miret, F D Malliaros, T Cohen, P Lio, Y Bengio, M Bronstein, 10.48550/arXiv.2312.0751120231</p>
<p>Strategies for the Construction of Machine-Learning Potentials for Accurate and Efficient Atomic-Scale Simulations. A M Miksch, T Morawietz, J Kästner, A Urban, N Artrith, 10.1088/2632-2153/abfd96No. 031001Mach. Learn.: Sci. Technol. 20213</p>
<p>Leveraging Multitask Learning to Improve the Transferability of Machine Learned Force Fields. L Jacobson, J Stevenson, F Ramezanghorbani, S Dajnowicz, K Leswing, 10.26434/chemrxiv-2023-8n73720231</p>
<p>. I Batatia, P Benner, Y Chiang, A M Elena, D P Kovács, J Riebesell, X R Advincula, M Asta, W J Baldwin, N Bernstein, A Bhowmik, S M Blau, V Caȓare, J P Darby, S De, F Della Pia, V L Deringer, R Elijosǐus, Z El-Machachi, E Fako, A C Ferrari, A Genreith-Schriever, J George, R E A Goodall, C P Grey, S Han, W Handley, H H Heenen, K Hermansson, C Holm, J Jaafar, S Hofmann, K S Jakob, H Jung, V Kapil, A D Kaplan, N Karimitari, N Kroupa, J Kullgren, M C Kuner, D Kuryla, G Liepuoniute, J T Margraf, I.-B Magdaȗ, A Michaelides, J H Moore, A A Naik, S P Niblett, S W Norwood, N O'neill, C Ortner, K A Persson, K Reuter, A S Rosen, L L Schaaf, C Schran, E Sivonxay, T K Stenczel, V Svahn, C Sutton, C Van Der Oord, 10.48550/arXiv.2401.00096Varga-Umbrich1Wang, YWitt, W. C.; Zills, F.; Csányi, G. A Foundation Model for Atomistic Materials Chemistry. arXiv 2023</p>
<p>Scaling Deep Learning for Materials Discovery. A Merchant, S Batzner, S S Schoenholz, M Aykol, G Cheon, E D Cubuk, 10.1038/s41586-023-06735-9Nature. 6242023</p>
<p>Open Challenges in Developing Generalizable Large-Scale Machine-Learning Models for Catalyst Discovery. A Kolluru, M Shuaibi, A Palizhati, N Shoghi, A Das, B Wood, C L Zitnick, J R Kitchin, Z W Ulissi, 10.1021/acscatal.2c02291?urlappend=%3Fref%3DPDF&amp;jav=VoR&amp;rel=cite-asACS Catal. 12142022</p>
<p>Machine-Learned Interatomic Potentials: Recent Developments and Prospective Applications. V Eyert, J Wormald, W A Curtin, E Wimmer, 10.1557/s43578-023-01239-8J. Mater. Res. 202324</p>
<p>A Omranpour, P M De Hijes, J Behler, C Dellago, Perspective, 10.48550/arXiv.2401.17875Atomistic Simulations of Water and Aqueous Systems with Machine Learning Potentials. 20241</p>
<p>Reactant-Induced Dynamics of Lithium Imide Surfaces during the Ammonia Decomposition Process. M Yang, U Raucci, M Parrinello, 10.1038/s41929-023-01006-2Nat. Catal. 20239</p>
<p>F S Zariquiey, R Galvelis, E Gallicchio, J D Chodera, T E Markland, 10.48550/arXiv.2401.16062Fabritiis, G. Enhancing Protein-Ligand Binding Affinity Predictions Using Neural Network Potentials. arXiv 2024. 1</p>
<p>Extending Density Functional Theory with near Chemical Accuracy beyond Pure Water. S Song, S Vuckovic, Y Kim, H Yu, E Sim, K Burke, 10.1038/s41467-023-36094-yNat. Commun. 147992023</p>
<p>Consistent Density Functional Theory-Based Description of Ion Hydration Through Density-Corrected Many-Body Representations. E Palos, A Caruso, F Paesani, 10.26434/chemrxiv-2023-vp6nsChemRxiv. 20231</p>
<p>Assessing the Interplay between Functional-Driven and Density-Driven Errors in DFT Models of Water. E Palos, E Lambros, S Swee, J Hu, S Dasgupta, F Paesani, 10.1021/acs.jctc.2c00050?urlappend=%3Fref%3DPDF&amp;jav=VoR&amp;rel=cite-asJ. Chem. Theory Comput. 1862022</p>
<p>How Good Is the Density-Corrected SCAN Functional for Neutral and Ionic Aqueous Systems, and What Is so Right about the Hartree-Fock Density ?. S Dasgupta, C Shahi, P Bhetwal, J P Perdew, F Paesani, 10.1021/acs.jctc.2c00313?urlappend=%3Fref%3DPDF&amp;jav=VoR&amp;rel=cite-asJ. Chem. Theory Comput. 1882022</p>
<p>Radicals in Aqueous Solution: Assessment of Density-Corrected SCAN Functional. F Belleflamme, J Hutter, 10.1039/D3CP02517APhys. Chem. Chem. Phys. 202331</p>
<p>Predicting the Properties of Salt Water Using Neural Network Potentials and Continuum Solvent Theory. J Pagotto, J Zhang, T T Duignan, 10.26434/chemrxiv-2022-jndlx?urlappend=%3Fref%3DPDF&amp;jav=VoR&amp;rel=cite-asChemRxiv. 2022</p>
<p>J Zhang, J Pagotto, T Gould, T T Duignan, Accurate, 10.48550/arXiv.2310.12535?urlappend=%3Fref%3DPDF&amp;jav=VoR&amp;rel=cite-asFast and Generalisable First Principles Simulation of Aqueous Lithium Chloride. 2023</p>
<p>When Do Short-Range Atomistic Machine-Learning Models Fall Short?. S Yue, M C Muniz, M F Calegari Andrade, L Zhang, R Car, A Z Panagiotopoulos, 10.1063/5.0031215J. Chem. Phys. 1540341112021</p>
<p>Dilemma for a State-of-the-Art Neural Network Potential for Water: Reproducing Experimental Properties or the Physics of the Underlying Many-Body Interactions?. Y Zhai, A Caruso, S L Bore, Z Luo, F Paesani, Short, Blanket, 10.1063/5.0142843J. Chem. Phys. 2023084111</p>
<p>The TensorMol-0.1 Model Chemistry: A Neural Network Augmented with Long-Range Physics. K Yao, J E Herr, D W Toth, R Mckintyre, J Parkhill, 10.1039/C7SC04934JChem. Sci. 20188</p>
<p>Ewald-Based Long-Range Message Passing for Molecular Graphs. A Kosmala, J Gasteiger, N Gao, S Gunnemann, 10.48550/arXiv.2303.04791?urlappend=%3Fref%3DPDF&amp;jav=VoR&amp;rel=cite-as2023</p>
<p>Machine Learning Interatomic Potentials and Long-Range Physics. D M Anstine, O Isayev, 10.1021/acs.jpca.2c06778?urlappend=%3Fref%3DPDF&amp;jav=VoR&amp;rel=cite-asJ. Phys. Chem. A. 1272023. 2417</p>
<p>H Yu, L Hong, S Chen, X Gong, H Xiang, 10.48550/arXiv.2211.16684?urlappend=%3Fref%3DPDF&amp;jav=VoR&amp;rel=cite-asCapturing Long-Range Interaction with Reciprocal Space Neural Network. 2022</p>
<p>Incorporating Long-Range Physics in Atomic-Scale Machine Learning. A Grisafi, M Ceriotti, 10.1063/1.5128375J. Chem. Phys. 2041052019</p>
<p>Self-Consistent Determination of Long-Range Electrostatics in Neural Network Potentials. A Gao, R C Remsing, 10.1038/s41467-022-29243-2Nat. Commun. 1315722022</p>
<p>A Deep Potential Model with Long-Range Electrostatic Interactions. L Zhang, H Wang, M C Muniz, A Z Panagiotopoulos, R Car, E , W , 10.1063/5.0083669J. Chem. Phys. 2022124107</p>
<p>T W Ko, J A Finkler, S Goedecker, J Behler, 10.48550/arXiv.2305.10692Accurate Fourth-Generation Machine Learning Potentials by Electrostatic Embedding. 20231</p>
<p>Learning Intermolecular Forces at Liquid-Vapor Interfaces. S P Niblett, M Galib, D T Limmer, 10.1063/5.0067565J. Chem. Phys. 1552021. 164101</p>
<p>Active Learning of Uniformly Accurate Interatomic Potentials for Materials Simulation. L Zhang, D.-Y Lin, H Wang, R Car, E , W , 10.1103/PhysRevMaterials.3.023804Physical Review Materials. 32019. 023804</p>
<p>AL4GAP: Active Learning Workflow for Generating DFT-SCAN Accurate Machine-Learning Potentials for Combinatorial Molten Salt Mixtures. J Guo, V Woo, D Andersson, N Hoyt, M Williamson, I Foster, C Benmore, N Jackson, G Sivaraman, 10.26434/chemrxiv-2023-wzv3q20231</p>
<p>Uncertainty-Driven Dynamics for Active Learning of Interatomic Potentials. M Kulichenko, K Barros, N Lubbers, Y W Li, R Messerly, S Tretiak, J S Smith, B Nebgen, 10.1038/s43588-023-00406-5Nat. Comput. Sci. 20233</p>
<p>A Quantum Chemical Molecular Dynamics Repository of Solvated Ions. K P Gregory, G R Elliott, E J Wanless, G B Webber, A J Page, 10.1038/s41597-022-01527-8Scientific Data. 94302022</p>
<p>Extending the Limit of Molecular Dynamics with Ab Initio Accuracy to 10 Billion Atoms. Z Guo, D Lu, Y Yan, S Hu, R Liu, G Tan, N Sun, W Jiang, L Liu, Y Chen, L Zhang, M Chen, H Wang, W Jia, 10.48550/arXiv.2201.0144620221</p>
<p>Scaling the Leading Accuracy of Deep Equivariant Models to Biomolecular Simulations of Realistic Size. A Musaelian, A Johansson, S Batzner, B Kozinsky, 10.48550/arXiv.2304.1006120231</p>
<p>Comparison of Permutationally Invariant Polynomials, Neural Networks, and Gaussian Approximation Potentials in Representing Water Interactions through Many-Body Expansions. T T Nguyen, E Székely, G Imbalzano, J Behler, G Csányi, M Ceriotti, A W Götz, F Paesani, 10.1063/1.5024577J. Chem. Phys. 2417252018</p>
<p>P M De Hijes, C Dellago, R Jinnouchi, B Schmiedmayer, G Kresse, 10.48550/arXiv.2312.15213Comparing Machine Learning Potentials for Water: Kernel-Based Regression and Behler-Parrinello Neural Networks. arXiv 2023. 1</p>
<p>S N Pozdnyakov, M Ceriotti, Smooth, 10.48550/arXiv.2305.19302Exact Rotational Symmetrization for Deep Learning on Point Clouds. 20231</p>
<p>K Takaba, I Pulido, P K Behara, C E Cavender, A J Friedman, M M Henry, H Macdermott-Opeskin, C R Iacovella, A M Nagle, A M Payne, 10.48550/arXiv.2307.07085Espaloma-0.3.0: Machine-Learned Molecular Mechanics Force Field for the Simulation of Protein-Ligand Systems and Beyond. 20231</p>
<p>Learning Local Equivariant Representations for Large-Scale Atomistic Dynamics. A Musaelian, S Batzner, A Johansson, L Sun, C J Owen, M Kornbluth, B Kozinsky, 10.1038/s41467-023-36329-yNat. Commun. 5792023</p>
<p>R P Pelaez, G Simeon, R Galvelis, A Mirarchi, P Eastman, S Doerr, P Thölke, T E Markland, De Fabritiis, 10.48550/arXiv.2402.17660TorchMD-Net 2.0: Fast Neural Network Potentials for Molecular Simulations. 20241</p>
<p>Machine Learning of Coarse-Grained Molecular Dynamics Force Fields. J Wang, S Olsson, C Wehmeyer, A Pérez, N E Charron, G De Fabritiis, F Noé, C Clementi, 10.1021/acscentsci.8b00913?urlappend=%3Fref%3DPDF&amp;jav=VoR&amp;rel=cite-asACS Cent. Sci. 20195</p>
<p>Machine Learning Implicit Solvation for Molecular Dynamics. Y Chen, A Krämer, N E Charron, B E Husic, C Clementi, F Noé, 10.1063/5.0059915J. Chem. Phys. 1550841012021</p>
<p>Machine Learned Coarse-Grained Protein Force-Fields: Are We There Yet?. A E P Durumeric, N E Charron, C Templeton, F Musil, K Bonneau, A S Pasos-Trejo, Y Chen, A Kelkar, F Noé, C Clementi, 10.1016/j.sbi.2023.102533Curr. Opin. Struct. Biol. 792023. 102533</p>
<p>Anisotropic Molecular Coarse-Graining by Force and Torque Matching with Neural Networks. M O Wilson, D M Huang, 10.1063/5.0143724No. 024110J. Chem. Phys. 2023</p>
<p>Neural Potentials of Proteins Extrapolate beyond Training Data. G P Wellawatte, G M Hocky, A D White, 10.1063/5.0147240J. Chem. Phys. 2023085103</p>
<p>M Majewski, A Pérez, P Thölke, S Doerr, N E Charron, T Giorgino, B E Husic, C Clementi, F Noé, 10.1038/s41467-023-41343-1Machine Learning Coarse-Grained Potentials of Protein Thermodynamics. De Fabritiis20235739</p>
<p>Coarse-Graining with Equivariant Neural Networks: A Path Toward Accurate and Data-Efficient Models. T D Loose, P G Sahrmann, T S Qu, G A Voth, 10.1021/acs.jpcb.3c05928?urlappend=%3Fref%3DPDF&amp;jav=VoR&amp;rel=cite-asJ. Phys. Chem. B. 202349</p>
<p>Coarse-Graining with Equivariant Neural Networks: A Path Toward Accurate and Data-Efficient Models. T D Loose, P G Sahrmann, T S Qu, G A Voth, 10.1021/acs.jpcb.3c05928?urlappend=%3Fref%3DPDF&amp;jav=VoR&amp;rel=cite-asJ. Phys. Chem. B. 1272023. 10564</p>
<p>N E Charron, F Musil, A Guljas, Y Chen, K Bonneau, A S Pasos-Trejo, J Venturin, D Gusew, I Zaporozhets, A Krämer, C Templeton, A Kelkar, A E P Durumeric, S Olsson, A Pérez, M Majewski, B E Husic, A Patel, 10.48550/arXiv.2310.18278Noé, F.; Clementi, C. Navigating Protein Landscapes with a Machine-Learned Transferable Coarse-Grained Model. De Fabritiis20231</p>
<p>Graph Neural Network Based Coarse-Grained Mapping Prediction. Z Li, G P Wellawatte, M Chakraborty, H A Gandhi, C Xu, A D White, 10.1039/D0SC02458AChemical Science. 202035</p>
<p>W Wang, M Xu, C Cai, B K Miller, T Smidt, Y Wang, J Tang, R Gómez-Bombarelli, 10.48550/arXiv.2201.12176Generative Coarse-Graining of Molecular Conformations. 20221</p>
<p>Understanding Dynamics in Coarse-Grained Models. I. Universal Excess Entropy Scaling Relationship. J Jin, K S Schweizer, G A Voth, 10.1063/5.0116299J. Chem. Phys. 2023034103</p>
<p>Fast Protein Folding Is Governed by Memory-Dependent Friction. B A Dalton, C Ayaz, H Kiefer, A Klimek, L Tepper, R R Netz, 10.1073/pnas.2220068120No. e2220068120Proc. Natl. Acad. Sci. U.S.A. 202331</p>
<p>Ensemble Generalized Kohn−Sham Theory: The Good, the Bad, and the Ugly. T Gould, L Kronik, 10.1063/5.0040447J. Chem. Phys. 1540941252021</p>
<p>An Efficient and near Linear Scaling Pair Natural Orbital Based Local Coupled Cluster Method. C Riplinger, F Neese, 10.1063/1.4773581J. Chem. Phys. 1380341062013</p>
<p>Massively Parallel Implementation of Gradients within the Random Phase Approximation: Application to the Polymorphs of Benzene. F Stein, J Hutter, 10.1063/5.0180704No. 024120J. Chem. Phys. 2024</p>
<p>Ab Initio Quantum Chemistry Using the Density Matrix Renormalization Group. S R White, R L Martin, 10.1063/1.478295J. Chem. Phys. 11091999</p>
<p>J Hermann, J Spencer, K Choo, A Mezzacapo, W M C Foulkes, D Pfau, G Carleo, F Noé, 10.48550/arXiv.2208.12590Ab-Initio Quantum Chemistry with Neural-Network Wavefunctions. 20221</p>
<p>Two-Dimensional Frustrated J 1 − J 2 Model Studied with Neural Network Quantum States. K Choo, T Neupert, G Carleo, 10.1103/PhysRevB.100.125124No. 125124Phys. Rev. B. 121002019</p>
<p>How To Use Neural Networks To Investigate Quantum Many-Body Physics. J Carrasquilla, G Torlai, 10.1103/PRXQuantum.2.040201PRX Quantum. 20214</p>
<p>A Software Library for Machine Learning Enhanced Density Functional Theory. P A Casares, J S Baker, M Medvidovic, R D Reis, J M Arrazola, Graddft, 10.1063/5.0181037No. 062501J. Chem. Phys. 2024</p>
<p>S Lee, J Lee, H Zhai, Y Tong, A M Dalzell, A Kumar, P Helms, J Gray, Z.-H Cui, W Liu, M Kastoryano, R Babbush, J Preskill, D R Reichman, E T Campbell, E F Valeev, L Lin, G K Chan, -L , 10.48550/arXiv.2208.02199Is There Evidence for Exponential Quantum Advantage in Quantum Chemistry? arXiv 2022. 1</p>
<p>The Quantum House Of Cards. X Waintal, 10.1073/pnas.2313269120Proc. Natl. Acad. Sci. Natl. Acad. Sci</p>
<p>. U S A , No. e23132691202024121</p>
<p>Two for One: Diffusion Models and Force Fields for Coarse-Grained Molecular Dynamics. M Arts, V Garcia Satorras, C.-W Huang, D Zugner, M Federici, C Clementi, F Noé, R Pinsler, Van Den, R Berg, 10.1021/acs.jctc.3c00702?urlappend=%3Fref%3DPDF&amp;jav=VoR&amp;rel=cite-asJ. Chem. Theory Comput. 19182023</p>
<p>De Novo Design of Protein Structure and Function with RFdiffusion. J L Watson, D Juergens, N R Bennett, B L Trippe, J Yim, H E Eisenach, W Ahern, A J Borst, R J Ragotte, L F Milles, B I M Wicky, N Hanikel, S J Pellock, A Courbet, W Sheffler, J Wang, P Venkatesh, I Sappington, S V Torres, A Lauko, V De Bortoli, E Mathieu, S Ovchinnikov, R Barzilay, T S Jaakkola, F Dimaio, M Baek, D Baker, 10.1038/s41586-023-06415-8Nature. 20237976</p>
<p>C Zeni, R Pinsler, D Zugner, A Fowler, M Horton, X Fu, S Shysheya, J Crabbé, L Sun, J Smith, R Tomioka, T Xie, Mattergen, 10.48550/arXiv.2312.03687Generative Model for Inorganic Materials Design. 20231</p>
<p>The Accuracy of Protein Structures in Solution Determined by AlphaFold and NMR. N J Fowler, M P Williamson, 10.1016/j.str.2022.04.005Structure. 20227925</p>
<p>. S K Burley, H M Berman, C Bhikadiya, C Bi, L Chen, L D Costanzo, C Christie, J M Duarte, S Dutta, Z Feng, S Ghosh, D S Goodsell, R K Green, V Guranovic, D Guzenko, B P Hudson, Y Liang, R Lowe, E Peisach, I Periskova, C Randle, A Rose, M Sekharan, C Shao, Y.-P Tao, Y Valasatava, M Voigt, J Westbrook, J Young, C Zardecki, M Zhuravleva, G Kurisu, H Nakamura, Y Kengaku, H Cho, J Sato, J Y Kim, Y Ikegawa, A Nakagawa, R Yamashita, T Kudou, G.-J Bekker, H Suzuki, T Iwata, M Yokochi, N Kobayashi, T Fujiwara, S Velankar, G J Kleywegt, S Anyango, D R Armstrong, J M Berrisford, M J Conroy, J M Dana, M Deshpande, P Gane, R Gáborová, D Gupta, A Gutmanas, J Kocǎ, L Mak, S Mir, A Mukhopadhyay, N Nadzirin, S Nair, A Patwardhan, T Paysan-Lafosse, L Pravda, O Salih, D Sehnal, M Varadi, R Varěková, J L Markley, J C Hoch, P R Romero, K Baskaran, D Maziuk, E L Ulrich, J R Wedell, H Yao, Livny, 10.1093/nar/gky949Nucleic Acids Res. 47D12019Ioannidis, Y. E. Protein Data Bank: The Single Global Archive for 3D Macromolecular Structure Data</p>
<p>AlphaFold2 Has More to Learn about Protein Energy Landscapes. D Chakravarty, J W Schafer, E A Chen, J R Thole, L L Porter, 10.1101/2023.12.12.571380BioRxiv. 20231</p>
<p>State-of-the-Art Estimation of Protein Model Accuracy Using AlphaFold. J P Roney, S Ovchinnikov, 10.1103/PhysRevLett.129.238101Phys. Rev. Lett. 202223129238101</p>
<p>A General Reinforcement Learning Algorithm That Masters Chess, Shogi, and Go through Self-Play. D Silver, T Hubert, J Schrittwieser, I Antonoglou, M Lai, A Guez, M Lanctot, L Sifre, D Kumaran, T Graepel, T Lillicrap, K Simonyan, D Hassabis, 10.1126/science.aar6404Science. 36264192018</p>
<p>Solving Olympiad Geometry without Human Demonstrations. T H Trinh, Y Wu, Q V Le, H He, T Luong, 10.1038/s41586-023-06747-5Nature. 20247995</p>
<p>. P Reiser, M Neubert, A Eberhard, L Torresi, C Zhou, C Shao, H Metni, C Van Hoesel, H Schopmans, T Sommer, Friederich, 10.1038/s43246-022-00315-6P. Graph Neural Networks for Materials Science and Chemistry. Commun. Mater. 202293</p>
<p>Learning Pair Potentials Using Differentiable Simulations. W Wang, Z Wu, J C B Dietschreit, R Gómez-Bombarelli, 10.1063/5.0126475J. Chem. Phys. 2023044113</p>
<p>COATI: Multimodal Contrastive Pretraining for Representing and Traversing Chemical Space. B Kaufman, E C Williams, C Underkoffler, R Pederson, N Mardirossian, I Watson, J Parkhill, 10.1021/acs.jcim.3c01753?urlappend=%3Fref%3DPDF&amp;jav=VoR&amp;rel=cite-asJ. Chem. Inf. Model. 20244</p>
<p>The Future of Chemistry Is Language. A D White, 10.1038/s41570-023-00502-0Nature Reviews Chemistry. 72023</p>            </div>
        </div>

    </div>
</body>
</html>
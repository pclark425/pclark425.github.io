<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-790 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-790</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-790</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-21.html">extraction-schema-21</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <p><strong>Paper ID:</strong> paper-202538463</p>
                <p><strong>Paper Title:</strong> <a href="https://www.aclweb.org/anthology/D19-1063.pdf" target="_blank">Help, Anna! Visual Navigation with Natural Multimodal Assistance via Retrospective Curiosity-Encouraging Imitation Learning</a></p>
                <p><strong>Paper Abstract:</strong> Mobile agents that can leverage help from humans can potentially accomplish more complex tasks than they could entirely on their own. We develop “Help, Anna!” (HANNA), an interactive photo-realistic simulator in which an agent fulfills object-finding tasks by requesting and interpreting natural language-and-vision assistance. An agent solving tasks in a HANNA environment can leverage simulated human assistants, called ANNA (Automatic Natural Navigation Assistants), which, upon request, provide natural language and visual instructions to direct the agent towards the goals. To address the HANNA problem, we develop a memory-augmented neural agent that hierarchically models multiple levels of decision-making, and an imitation learning algorithm that teaches the agent to avoid repeating past mistakes while simultaneously predicting its own chances of making future progress. Empirically, our approach is able to ask for help more effectively than competitive baselines and, thus, attains higher task success rate on both previously seen and previously unseen environments.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e790.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e790.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HANNA agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Help, Anna! memory-augmented hierarchical navigation agent</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hierarchical, memory-augmented neural agent trained with imitation learning to request and interpret multimodal (language + image) assistance (ANNA) for object-finding in partially-observable photo-realistic indoor environments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>HANNA agent (navigation + help-request policies)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A dual-policy agent: a navigation policy π_nav and a help-request policy π_ask, each implemented as a hierarchical recurrent / attention-based network with three components — a text-encoding module (producing a text memory M_text for the current instruction), an inter-task module (h_inter) that represents the state of the current subtask and is reset when a new instruction/subtask begins, and an intra-task module (h_intra) that accumulates episode-level history. The model uses self-attention, a cosine-similarity attention to retrieve nearly-identical past states, ResNet-based time-encoding, and outputs probability distributions over panoramic navigation actions (adjacent graph node + camera angle changes) and a binary help-request decision. A reason classifier Φ predicts which help-request conditions (lost, uncertain, never-asked) hold. Training uses Imitation Learning with Indirect Intervention (I3L), a curiosity-encouraging loss that downweights repeating past non-reference actions, and a retrospective help-request teacher that labels help decisions after observing full episode futures.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>HANNA (Help, Anna!) on Matterport3D / Room-to-Room routes</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Photo-realistic indoor navigation benchmark built on Matterport3D; agent sees only first-person monocular RGB panoramic views (no absolute location sensors), navigates on a discrete graph of locations (nodes with 3D coords, edges weighted by meters), and may request help only when inside assistants' zones of attention. Challenges: partial observability (no ground-truth pose available to agent), long-range spatial reasoning, and interpreting noisy multimodal human-like assistance (natural language + target images).</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>Automatic Natural Navigation Assistants (ANNA) — simulated human assistants that act as an external tool: when queried they select a precomputed language-assisted route r (a path p_r and an instruction l_r and initial camera angles) and a departure node and return a multimedia subtask message (natural language instruction l_r, panoramic image I_vd of the departure location; if departing, an image I_g of the goal closest to the departure). During training the agent also uses a navigation teacher (π_nav) with full state access as an oracle for reference actions (shortest-path). The route system is constructed from Room-to-Room dataset instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Natural language instructions (text subtasks), panoramic RGB images of target/departure views (visual targets), an implicit subtask specification (route path p_r, start camera angles), and teacher-provided reference navigation actions (shortest-path next-node labels) when used as oracle.</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>The agent's belief/state is represented by learned memory vectors: text memory M_text (encoded representation of the current instruction), h_inter (inter-task state representing progress on the current subtask; reset when a new instruction arrives), and h_intra (intra-task / episode-level state) that is computed by combining a tentative current state with a weighted combination of past nearly-identical states via a learned gating β (β = σ(W_gate · [h_intra_t; h_intra_t~])). A reason-classifier Φ also predicts binary indicators for help-request conditions (lost, uncertain, never-asked). The navigation policy's action distribution (and its entropy) is part of the observation used by the help-request policy.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>When the agent requests help and receives ANNA's outputs (l_r, I_vd / I_g), it sets the current language instruction to l_r and the current target image to the provided panoramic image, re-encodes the language to form a new text memory M_text, and resets the inter-task state h_inter to zero (beginning a new subtask). The intra-task state h_intra is preserved but computed so as to incorporate past similar situations via cosine-similarity attention and the gating mechanism, enabling retention of episode-level context and preventing repetition of past mistakes. The help-request reason vector ρ (lost/uncertain/never-asked) is used as supervision to train the classifier Φ, which becomes part of the agent's internal estimate of conditions when deciding to ask for help.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Learned policy planning via imitation learning (I3L) with hierarchical recurrent/attention networks; teacher-provided reference actions are shortest-path steps (model-based oracle guidance) and ANNA provides precomputed subtask routes (route-selection itself minimizes shortest-path distance to goals). The agent's planner is a learned mapping from observation+memory to action distributions, guided during training by retrospective teacher labels and a curiosity-encouraging loss (to reduce repeating mistakes).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td>Graph-based navigation on the Matterport3D location graph: actions select an adjacent node plus camera angle change (panoramic action space). Reference navigation decisions are the next node on the shortest path to the current target; ANNA selects routes and departure nodes by minimizing shortest-path distance to goal locations. The agent executes learned policy during subtasks; 'perfect assistance interpretation' experiments execute teacher's navigation policy when following a subtask.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td>With learned help-request and ANNA multimodal assistance: success rate (SR) = 88.37% on TEST SEENENV (requests/task = 2.9) and SR = 47.45% on TEST UNSEENALL (requests/task = 5.8); using language+image assistance substantially increases success relative to no assistance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td>NOASK baseline (no assistance): SR = 17.21% on TEST SEENENV and SR = 8.10% on TEST UNSEENALL.</td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Access to external multimodal assistance (ANNA: language + target images) drastically improves navigation success, especially in unseen environments; language-based instructions provide more generalization gains than images alone. The agent explicitly incorporates tool outputs by re-encoding instructions and target images into its text memory and resetting inter-task state, while retaining episode history in a gated intra-task memory. A retrospective help-request teacher that reasons about the agent's future and a reason-prediction classifier enable more effective and interpretable help-request decisions. A curiosity-encouraging loss reduces repeating past navigation mistakes and improves success. ANNA's route selection uses shortest-path distances to pick useful subtasks, and the training oracle provides shortest-path reference actions to shape the learned navigation policy.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments <em>(Rating: 2)</em></li>
                <li>Look before you leap: Bridging model-free and model-based reinforcement learning for planned-ahead vision-and-language navigation <em>(Rating: 2)</em></li>
                <li>Speaker-follower models for vision-and-language navigation <em>(Rating: 2)</em></li>
                <li>Vision-and-dialog navigation <em>(Rating: 2)</em></li>
                <li>Mapping instructions to actions in 3d environments with visual goal prediction <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-790",
    "paper_id": "paper-202538463",
    "extraction_schema_id": "extraction-schema-21",
    "extracted_data": [
        {
            "name_short": "HANNA agent",
            "name_full": "Help, Anna! memory-augmented hierarchical navigation agent",
            "brief_description": "A hierarchical, memory-augmented neural agent trained with imitation learning to request and interpret multimodal (language + image) assistance (ANNA) for object-finding in partially-observable photo-realistic indoor environments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "HANNA agent (navigation + help-request policies)",
            "agent_description": "A dual-policy agent: a navigation policy π_nav and a help-request policy π_ask, each implemented as a hierarchical recurrent / attention-based network with three components — a text-encoding module (producing a text memory M_text for the current instruction), an inter-task module (h_inter) that represents the state of the current subtask and is reset when a new instruction/subtask begins, and an intra-task module (h_intra) that accumulates episode-level history. The model uses self-attention, a cosine-similarity attention to retrieve nearly-identical past states, ResNet-based time-encoding, and outputs probability distributions over panoramic navigation actions (adjacent graph node + camera angle changes) and a binary help-request decision. A reason classifier Φ predicts which help-request conditions (lost, uncertain, never-asked) hold. Training uses Imitation Learning with Indirect Intervention (I3L), a curiosity-encouraging loss that downweights repeating past non-reference actions, and a retrospective help-request teacher that labels help decisions after observing full episode futures.",
            "environment_name": "HANNA (Help, Anna!) on Matterport3D / Room-to-Room routes",
            "environment_description": "Photo-realistic indoor navigation benchmark built on Matterport3D; agent sees only first-person monocular RGB panoramic views (no absolute location sensors), navigates on a discrete graph of locations (nodes with 3D coords, edges weighted by meters), and may request help only when inside assistants' zones of attention. Challenges: partial observability (no ground-truth pose available to agent), long-range spatial reasoning, and interpreting noisy multimodal human-like assistance (natural language + target images).",
            "is_partially_observable": true,
            "external_tools_used": "Automatic Natural Navigation Assistants (ANNA) — simulated human assistants that act as an external tool: when queried they select a precomputed language-assisted route r (a path p_r and an instruction l_r and initial camera angles) and a departure node and return a multimedia subtask message (natural language instruction l_r, panoramic image I_vd of the departure location; if departing, an image I_g of the goal closest to the departure). During training the agent also uses a navigation teacher (π_nav) with full state access as an oracle for reference actions (shortest-path). The route system is constructed from Room-to-Room dataset instructions.",
            "tool_output_types": "Natural language instructions (text subtasks), panoramic RGB images of target/departure views (visual targets), an implicit subtask specification (route path p_r, start camera angles), and teacher-provided reference navigation actions (shortest-path next-node labels) when used as oracle.",
            "belief_state_mechanism": "The agent's belief/state is represented by learned memory vectors: text memory M_text (encoded representation of the current instruction), h_inter (inter-task state representing progress on the current subtask; reset when a new instruction arrives), and h_intra (intra-task / episode-level state) that is computed by combining a tentative current state with a weighted combination of past nearly-identical states via a learned gating β (β = σ(W_gate · [h_intra_t; h_intra_t~])). A reason-classifier Φ also predicts binary indicators for help-request conditions (lost, uncertain, never-asked). The navigation policy's action distribution (and its entropy) is part of the observation used by the help-request policy.",
            "incorporates_tool_outputs_in_belief": true,
            "belief_update_description": "When the agent requests help and receives ANNA's outputs (l_r, I_vd / I_g), it sets the current language instruction to l_r and the current target image to the provided panoramic image, re-encodes the language to form a new text memory M_text, and resets the inter-task state h_inter to zero (beginning a new subtask). The intra-task state h_intra is preserved but computed so as to incorporate past similar situations via cosine-similarity attention and the gating mechanism, enabling retention of episode-level context and preventing repetition of past mistakes. The help-request reason vector ρ (lost/uncertain/never-asked) is used as supervision to train the classifier Φ, which becomes part of the agent's internal estimate of conditions when deciding to ask for help.",
            "planning_approach": "Learned policy planning via imitation learning (I3L) with hierarchical recurrent/attention networks; teacher-provided reference actions are shortest-path steps (model-based oracle guidance) and ANNA provides precomputed subtask routes (route-selection itself minimizes shortest-path distance to goals). The agent's planner is a learned mapping from observation+memory to action distributions, guided during training by retrospective teacher labels and a curiosity-encouraging loss (to reduce repeating mistakes).",
            "uses_shortest_path_planning": true,
            "navigation_method": "Graph-based navigation on the Matterport3D location graph: actions select an adjacent node plus camera angle change (panoramic action space). Reference navigation decisions are the next node on the shortest path to the current target; ANNA selects routes and departure nodes by minimizing shortest-path distance to goal locations. The agent executes learned policy during subtasks; 'perfect assistance interpretation' experiments execute teacher's navigation policy when following a subtask.",
            "performance_with_tools": "With learned help-request and ANNA multimodal assistance: success rate (SR) = 88.37% on TEST SEENENV (requests/task = 2.9) and SR = 47.45% on TEST UNSEENALL (requests/task = 5.8); using language+image assistance substantially increases success relative to no assistance.",
            "performance_without_tools": "NOASK baseline (no assistance): SR = 17.21% on TEST SEENENV and SR = 8.10% on TEST UNSEENALL.",
            "has_tool_ablation": true,
            "key_findings": "Access to external multimodal assistance (ANNA: language + target images) drastically improves navigation success, especially in unseen environments; language-based instructions provide more generalization gains than images alone. The agent explicitly incorporates tool outputs by re-encoding instructions and target images into its text memory and resetting inter-task state, while retaining episode history in a gated intra-task memory. A retrospective help-request teacher that reasons about the agent's future and a reason-prediction classifier enable more effective and interpretable help-request decisions. A curiosity-encouraging loss reduces repeating past navigation mistakes and improves success. ANNA's route selection uses shortest-path distances to pick useful subtasks, and the training oracle provides shortest-path reference actions to shape the learned navigation policy.",
            "uuid": "e790.0"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments",
            "rating": 2,
            "sanitized_title": "visionandlanguage_navigation_interpreting_visuallygrounded_navigation_instructions_in_real_environments"
        },
        {
            "paper_title": "Look before you leap: Bridging model-free and model-based reinforcement learning for planned-ahead vision-and-language navigation",
            "rating": 2,
            "sanitized_title": "look_before_you_leap_bridging_modelfree_and_modelbased_reinforcement_learning_for_plannedahead_visionandlanguage_navigation"
        },
        {
            "paper_title": "Speaker-follower models for vision-and-language navigation",
            "rating": 2,
            "sanitized_title": "speakerfollower_models_for_visionandlanguage_navigation"
        },
        {
            "paper_title": "Vision-and-dialog navigation",
            "rating": 2,
            "sanitized_title": "visionanddialog_navigation"
        },
        {
            "paper_title": "Mapping instructions to actions in 3d environments with visual goal prediction",
            "rating": 1,
            "sanitized_title": "mapping_instructions_to_actions_in_3d_environments_with_visual_goal_prediction"
        }
    ],
    "cost": 0.011822,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Help, Anna! Visual Navigation with Natural Multimodal Assistance via Retrospective Curiosity-Encouraging Imitation Learning
November 3-7, 2019</p>
<p>Khanh Nguyen kxnguyen@umiacs.umd.edu 
Hal Daumé 
Iii ♥ </p>
<p>University of Maryland
College Park</p>
<p>Microsoft Research
New York ♥</p>
<p>Help, Anna! Visual Navigation with Natural Multimodal Assistance via Retrospective Curiosity-Encouraging Imitation Learning</p>
<p>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing
the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingHong Kong, ChinaNovember 3-7, 2019684
Mobile agents that can leverage help from humans can potentially accomplish more complex tasks than they could entirely on their own. We develop "Help, Anna!" (HANNA), an interactive photo-realistic simulator in which an agent fulfills object-finding tasks by requesting and interpreting natural languageand-vision assistance. An agent solving tasks in a HANNA environment can leverage simulated human assistants, called ANNA (Automatic Natural Navigation Assistants), which, upon request, provide natural language and visual instructions to direct the agent towards the goals. To address the HANNA problem, we develop a memory-augmented neural agent that hierarchically models multiple levels of decision-making, and an imitation learning algorithm that teaches the agent to avoid repeating past mistakes while simultaneously predicting its own chances of making future progress. Empirically, our approach is able to ask for help more effectively than competitive baselines and, thus, attains higher task success rate on both previously seen and previously unseen environments. We publicly release code and data at https://github. com/khanhptnk/hanna .</p>
<p>Introduction</p>
<p>The richness and generalizability of natural language makes it an effective medium for directing mobile agents in navigation tasks, even in environments they have never encountered before (Anderson et al., 2018b;Chen et al., 2019;Misra et al., 2018;de Vries et al., 2018;Qi et al., 2019). Nevertheless, even with language-based instructions, such tasks can be overly difficult for agents on their own, especially in unknown environments. To accomplish tasks that surpass their knowledge and skill levels, agents must be able to actively seek for and leverage assistance in the environment. Humans are rich external knowledge sources but, unfortunately, they may not be available all the time to provide guidance, or may be unwilling to help too frequently. To reduce the needed effort from human assistants, it is essential to design research platforms for teaching agents to request help mindfully.</p>
<p>In natural settings, human assistance is often: derived from interpersonal interaction (a lost tourist asks a local for directions); reactive to the situation of the receiver, based on the assistant's knowledge (the local may guide the tourist to the goal, or may redirect them to a different source of assistance); delivered via a multimodal communication channel (the local uses a combination of language, images, maps, gestures, etc.). We introduce the "Help, Anna!" (HANNA) problem ( § 3), in which a mobile agent has to navigate (without a map) to an object by interpreting its first-person visual perception and requesting help from Automatic Natural Navigation Assistants (ANNA). HANNA models a setting in which a human is not always available to help, but rather that human assistants are scattered throughout the environment and provide help upon request (modeling the interpersonal aspect). The assistants are not omniscient: they are only familiar with certain regions of the environment and, upon request, provide subtasks, expressed in language and images (modeling the multimodal aspect), for getting closer to the goal, not necessarily for fully completing the task (modeling the reactive aspect).</p>
<p>In HANNA, when the agent gets lost and becomes unable to make progress, it has the option of requesting assistance from ANNA. At test time, the agent must decide where to go and whether to request help from ANNA without additional supervision. At training time, we leverage imitation learning to learn an effective agent, both in terms of navigation, and in terms of being able to decide Figure 1: An example HANNA task. Initially, the agent stands in the bedroom at A and is requested by a human requester to "find a mug." The agent begins, but gets lost somewhere in the bathroom. It gets to the start location of route ( B ) to request help from ANNA. Upon request, ANNA assigns the agent a navigation subtask described by a natural language instruction that guides the agent to a target location, and an image of the view at that location. The agents follows the language instruction and arrives at C , where it observes a match between the target image and the current view, thus decides to depart route . After that, it resumes the main task of finding a mug. From this point, the agent gets lost one more time and has to query ANNA for another subtask that helps it follow route and enter the kitchen. The agent successfully fulfills the task it finally stops within meters of an instance of the requested object ( ). Here, the ANNA feedback is simulated using two pre-collected language-assisted routes ( and ).</p>
<p>when it is most worthwhile to request assistance. This paper has two primary contributions:</p>
<ol>
<li>Constructing the HANNA simulator by augmenting an indoor photo-realistic simulator with simulated human assistance, mimicking a scenario where a mobile agent finds objects by asking for directions along the way ( §3). 2. An effective model and training algorithm for the HANNA problem, which includes a hierarchical memory-augmented recurrent architecture that models human assistance as sub-goals ( § 5), and introduces an imitation learning objective that enhances exploration of the environment and interpretability of the agent's help-request decisions. ( §4).</li>
</ol>
<p>We embed the HANNA problem in the photorealistic Matterport3D environments (Chang et al., 2017) with no extra annotation cost by reusing the pre-existing Room-to-Room dataset (Anderson et al., 2018b). Empirical results ( § 7) show that our agent can effectively learn to request and interpret language and vision instructions, given a training set of 51 environments and less than 9,000 language instructions. Even in new environments, where the scenes and the language instructions are previously unseen, the agent successfully accomplishes 47% of its tasks. Our methods for training the navigation and help-request policies outperform competitive baselines by large margins.</p>
<p>Related work</p>
<p>Simulated environments provide an inexpensive platform for fast prototyping and evaluating new ideas before deploying them into the real world. Video-game and physics simulators are standard benchmarks in reinforcement learning (Todorov et al., 2012;Mnih et al., 2013;Kempka et al., 2016;Brockman et al., 2016;Vinyals et al., 2017). Nevertheless, these environments under-represent the complexity of the world. Realistic simulators play an important role in sim-to-real approaches, in which an agent is trained with arbitrarily many samples provided by the simulators, then transferred to real settings using sample-efficient transfer learning techniques (Kalashnikov et al., 2018;Andrychowicz et al., 2018;Karttunen et al., 2019). While modern techniques are capable of simulating images that can convince human perception (Karras et al., 2017(Karras et al., , 2018, simulating language interaction remains challenging. There are efforts in building complex interactive text-based worlds (Côté et al., 2018;Urbanek et al., 2019) but the lack of a graphical component makes them not suitable for visually grounded learning. On the other hand, experimentation on real humans and robots, despite expensive and time-consuming, are important for understanding the true complexity of real-world scenarios (Chai et al., 2018Rybski et al., 2007;Mohan and Laird, 2014;She et al., 2014).</p>
<p>Recent navigation tasks in photo-realistic simulators have accelerated research on teaching agents to execute human instructions. Nevertheless, modeling human assistance in these problems remains simplistic (Table 1): they either do not incorporate the ability to request additional help while executing tasks (Misra et al., 2014(Misra et al., , 2017Anderson et al., 2018b;Chen et al., 2019;Das et al., 2018;Misra et al., 2018;Wijmans et al., 2019;Qi et al., 2019), or mimic human verbal assistance with primitive, highly scripted language Chevalier-Boisvert et al., 2019). HANNA improves the realisticity of the VNLA setup  by using fully natural language instructions.</p>
<p>Imitation learning algorithms are a great fit for training agents in simulated environments: access to ground-truth information about the environments allows optimal actions to be computed in many situations. The "teacher" in standard imitation learning algorithms (Daumé III et al., 2009;Ross et al., 2011;Ross and Bagnell, 2014;Chang et al., 2015;Sun et al., 2017;Sharaf and Daumé III, 2017) does not take into consideration the agent's capability and behavior. He et al. (2012) present a coaching method where the teacher gradually increases the complexity of its demonstrations over time. Welleck et al. (2019) propose an "unlikelihood" objective, which, similar to our curiosity-encouraging objective, penalizes likelihoods of candidate negative actions to avoid mistake repetition. Our approach takes into account the agent's past and future behavior to determine actions that are most and least beneficial to them, combining the advantages of both model-based and progress-estimating methods (Wang et al., 2018;Ma et al., 2019a,b).</p>
<p>The HANNA Simulator</p>
<p>Problem. HANNA simulates a scenario where a human requester asks a mobile agent via language to find an object in an indoor environment. The task request is only a high-level command ("find [object(s)]"), modeling the general case when the requester does not need know how to accomplish a task when requesting it. We assume the task is always feasible: there is at least an instance of the requested object in the environment. Figure 1, to which references in this section will be made, illustrates an example where the agent is asked to "find a mug." The agent starts at a ran-   (Thomason et al., 2019b) contains natural conversations in which a human assistant aids another human in navigation tasks but offers limited language interaction simulation, as language assistance is not available when the agent deviates from the collected trajectories and tasks. HANNA simulates human assistants that provide language-and-vision instructions that adapt to the agent's current position and goal.</p>
<p>dom location ( A ), is given a task request, and is allotted a budget of T time steps to complete the task. The agent succeeds the if its final location is within success meters of the location of any instance of the requested object ( ). The agent is not given any sensors that help determine its location or the object's location and must navigate only with a monocular camera that captures its first-person view as an RGB image (e.g., image in the upper right of Figure 1). The only source of help the agent can leverage in the environment is assistants, who are present at both training and evaluation time. The assistants are not aware of the agent unless it enters their zones of attention, which include all locations within attn meters of their locations. When the agent is in one of these zones, it has an option to request help from the corresponding assistant. The assistant helps the agent by giving a subtask, described by a natural language instruction that guides the agent to a specific location, and an image of the view at that location.</p>
<p>In our example, at B , the assistant says "Enter the bedroom and turn left immediately. Walk straight to the carpet in the living room. Turn right, come to the coffee table." and provides an image of the destination in the living room. Executing the subtask may not fulfill the main task, but is guaranteed to get the agent to a location closer to a goal than where it was before (e.g., C ).</p>
<p>Photo-realistic Navigation Simulator. HANNA uses the Matterport3D simulator (Chang et al., 2017;Anderson et al., 2018b) to photorealistically emulate a first-person view while navigating in indoor environments. HANNA features 68 Matterport3D environments, each of which is a residential building consisting of multiple rooms and floors. Navigation is modeled as traversing an undirected graph G = (V, E), where each location corresponds to a node v ∈ V with 3D-coordinates x v , and edges are weighted by their lengths (in meters). The state of the agent is fully determined by its pose τ = (v, ψ, ω), where v is its location, ψ ∈ (0, 2π] is its heading (horizontal camera angle), and ω ∈ − π 6 , π 6 is its elevation (vertical camera angle). The agent does not know v, and the angles are constrained to multiples of π 6 . In each step, the agent can either stay at its current location, or it can rotate toward and go to a location adjacent to it in the graph 1 . Every time the agent moves (and thus changes pose), the simulator recalculates the image to reflect the new view.</p>
<p>Automatic Natural Navigation Assistants (ANNA). ANNA is a simulation of human assistants who do not necessarily know themselves how to optimally accomplish the agent's goal: they are only familiar with scenes along certain paths in the environment, and thus give advice to help the agent make partial progress. Specifically, the assistance from ANNA is modeled by a set of language-assisted routes R = {r 1 , r 2 , . . . , r |R| }. Each route r = (ψ r , ω r , p r , l r ) is defined by initial camera angles (ψ r , ω r ), a path p r in the environment graph, and a natural language instruction l r . A route becomes enterable when its start location is adjacent to and within attn meters of the agent's location. When the agent enters a route, it first adjusts its camera angles to (ψ r , ω r ), then attempts to interpret the language instructions l r to traverse along p r . At any time, the agent can depart the route by stopping following l r . An example of a route in Figure 1 is the combination of the initial camera angles at B , the path , and the language instruction "Enter the bedroom and turn left immediately. . . "</p>
<p>The set of all routes starting from a location simulates a human assistant who can recall scenes along these routes' paths. The zone of attention of the simulated human is the set of all locations from which the agent can enter one of the routes; when the agent is in this zone, it may ask the human for 1 We use the "panoramic action space" (Fried et al., 2018). help. Upon receiving a help request, the human selects a route r for the agent to enter (e.g., ), and a location v d on the route where it wants the agent to depart (e.g., C ). It then replies the agent with a multimedia message (l r , I v d ), where l r is the selected route's language instruction, and I v d is an image of the panoramic view at the departure location. The message describes a subtask which requires the agent to follow the direction described by l r and to stop if it reaches the location referenced by I v d . The route r and the departure node v d are selected to get the agent as close to a goal location as possible. Concretely, let R curr be the set of all routes associated with the requested human. The selected route minimizes the distance to the goal locations among all routes in R curr :
r = argmin r∈Rcurrd r, V goal (1) whered r, V goal def = min g∈V goal ,v∈p r d (g, v)
(2) d(., .) returns the (shortest-path) distance between two locations, and V goal is the set of all goal locations. The departure location minimizes the distance to the goal locations among all locations on the selected route:
v d = argmin g∈V goal ,v∈p r d (g, v) def =d r , V goal(3)
When the agent chooses to depart the route (not necessarily at the departure node), the human further assists it by providing I g , an image of the panoramic view at the goal location closest to the departure node:
g = argmin g∈V goal d (g, v d )(4)
The way the agent leverages ANNA to accomplish tasks is analogous to how humans travel using public transportation systems (e.g., bus, subway). For example, passengers of a subway system utilize fractions of pre-constructed routes to make progress toward a destination. They execute travel plans consisting of multiple subtasks, each of which requires entering a start stop, following a route (typically described by its name and last stop), and exiting at a departure stop (e.g., "Enter the Penn Station, hop on the Red line in the direction toward the South Ferry, get off at the World Trade Center" ). Occasionally, users walk short distances (at a lower speed) to switch routes. Our setup follows the same principle, but instead of having physical vehicles and railways, we employ low-level language-and-vision instructions as the "high-speed means" to accelerate travel. Constructing ANNA route system. Given a photo-realistic simulator, the primary cost for constructing the HANNA problem comes from crowdsourcing the natural language instructions. Ideally, we want to collect sufficient instructions to simulate humans in any location in the environment. Let N = |V | be the number of locations in the environment. Since each simulated human is familiar with at most N locations, in the worst case, we need to collect O(N 2 ) instructions to connect all location pairs. However, we theoretically prove that, assuming the agent executes instructions perfectly, it is possible to guide the agent between any location pair by collecting only Θ(N log N ) instructions. The key idea is using O(log N ) instead of a single instruction to connect each pair, and reusing an instruction for multiple routes.</p>
<p>Lemma 1. (proof in Appendix A) To guide the agent between any two locations using O(log N ) instructions, we need to collect instructions for Θ(N log N ) location pairs.</p>
<p>In our experiments, we leverage the pre-existing Room-to-room dataset (Anderson et al., 2018b) to construct the route system. This dataset contains 21,567 natural language instructions crowdsourced from humans and is originally intended to be used for the Vision-Language Navigation task (such as those in Figure 1), where an agent executes a language instruction to go to a location. We exclude instructions of the test split and their corresponding environments because ground-truth paths are not given. We use (on average) 211 routes to connect (on overage) 125 locations per environment. Even though the routes are selected randomly in the original dataset, our experiments show that they are sufficient for completing the tasks (assuming perfect assistance interpretation).</p>
<p>Retrospective Curiosity-Encouraging</p>
<p>Imitation Learning Agent Policies. Let s be a fully-observed state that contains ground-truth information about the environment and the agent (e.g., object locations, environment graph, agent parameters, etc.). Let o s be the corresponding observation given to the agent, which only encodes the current view, the current task, and extra information that the agent keeps track of (e.g., time, action history, etc.). The</p>
<p>Algorithm 1 Task episode, given agent helprequest policyπ ask and navigation policyπ nav 1: agent receives task request e 2: initialize the agent mode: m ← main task 3: initialize the language instruction: l0 ← e 4: initialize the target image: I tgt 0 ← None 5: for t = 1 . . . T do agent executesâ nav t to go to the next location 29: end for agent maintains two stochastic policies: a navigation policyπ nav and a help-request policyπ ask . Each policy maps an observation to a probability distribution over its action space. Navigation actions are tuples (v, ∆ψ, ∆ω), where v is a next location that is adjacent to the current location and (∆ψ, ∆ω) is the camera angle change. A special stop action is added to the set of navigation actions to signal that the agent wants to terminate the main task or a subtask (by departing a route). The action space of the help-request policy contains two actions: request help and do nothing. The request help action is only available when the agent is in a zone of attention. Alg 1 describes the effects of these actions during a task episode.</p>
<p>Imitation Learning Objective. The agent is trained with imitation learning to mimic behaviors suggested by a navigation teacher π nav and a help-request teacher π ask , who have access to the fully-observed states. In general, imitation learning (Daumé III et al., 2009;Ross et al., 2011;Ross and Bagnell, 2014;Chang et al., 2015;Sun et al., 2017) finds a policyπ that minimizes the expected imitation loss L with respect to a teacher policy π under the agent-induced state distribution Dπ:
min π E s∼Dπ <a href="5">L(s,π, π )</a>
We frame the HANNA problem as an instance of Imitation Learning with Indirect Intervention (I3L) . Under this framework, assistance is viewed as augmenting the current environment with new information. Interpreting the assistance is cast as finding the optimal acting policy in the augmented environment. Formally, I3L searches for policies that optimize:
min π ask ,πnav E s∼D statê πnav,E ,E∼D env π ask <a href="6">L(s)</a>
L(s) = L nav (s,π nav , π nav ) + L ask (s,π ask , π ask )</p>
<p>where L nav and L ask are the navigation and helprequest loss functions, respectively, D env π ask is the environment distribution induced byπ ask , and D statê πnav,E is the state distribution induced byπ nav in environment E. A common choice for the loss functions is the agent-estimated negative log likelihood of the reference action:
L NL (s,π, π ) = − logπ(a | o s ) (7)
where a is the reference action suggested by π . We introduce novel loss functions that enforce more complex behaviors than simply mimicking reference actions.</p>
<p>Reference Actions. The navigation teacher suggests a reference action a nav that takes the agent to the next location on the shortest path from its location to the target location. Here, the target location refers to the nearest goal location (if no target image is available), or the location referenced by the target image (provided by ANNA). If the agent is already at the target location, a nav = stop. To decide whether the agent should request help, the help-request teacher verifies the following conditions: 1. lost: the agent will not get (strictly) closer to the target location in the future; 2. uncertain wong: the entropy 2 of the navigation action distribution is greater than or equal to a threshold γ, and the highestprobability predicted navigation action is not suggested by the navigation teacher;</p>
<ol>
<li>never asked: the agent previously never requested help at the current location; If condition (1) or (2), and condition (3) are satisfied, we set a ask = request help; otherwise, a ask = do nothing.</li>
</ol>
<p>Curiosity-Encouraging Navigation Teacher.</p>
<p>In addition to a reference action, the navigation teacher returns A nav⊗ , the set of all non-reference actions that the agent took at the current location while executing the same language instruction:
A nav⊗ t = a ∈ A nav : ∃t &lt; t, v t = v t , (8) l t = l t , a = a nav t = a nav t
where A nav is the navigation action space. We devise a curiosity-encouraging loss L curious , which minimizes the log likelihoods of actions in A nav⊗ . This loss prevents the agent from repeating past mistakes and motivates it to explore untried actions. The navigation loss is:
L nav (s,π nav , π nav ) = L NL (s,πnav,π nav ) − logπ nav (a nav | o s ) (9) + α 1 |A nav⊗ | a∈A nav⊗ logπ nav (a | o s ) L curious (s,πnav,π nav )
where α ∈ [0, ∞) is a weight hyperparameter.</p>
<p>Retrospective Interpretable Help-Request</p>
<p>Teacher. In deciding whether the agent should ask for help, the help-request teacher must consider the agent's future situations. Standard imitation learning algorithms (e.g., DAgger) employ an online mode of interaction which queries the teacher at every time step. This mode of interaction is not suitable for our problem: the teacher must be able to predict the agent's future actions if it is queried when the episode is not finished. To overcome this challenge, we introduce a more efficient retrospective mode of interaction, which waits until the agent completes an episode and queries the teacher for reference actions for all time steps at once. With this approach, because the future actions at each time step are now fully observed, they can be taken into consideration when computing the reference action. In fact, we prove that the retrospective teacher is optimal for teaching the agent to determine the lost condition, which is the only condition that requires knowing the agent's future. Lemma 2. (proof in Appendix B) At any time step, the retrospective help-request teacher suggests the Inter-task Intra-task Encoder CosineSim Attention Figure 2: Our hierarchical recurrent model architecture (the navigation network). The help-request network is mostly similar except that the navigation action distribution is fed as an input to compute the "state features".</p>
<p>action that results in the agent getting closer to the target location in the future under its current navigation policy (if such an action exists).</p>
<p>To help the agent better justify its help-request decisions, we train a reason classifier Φ to predict which conditions are satisfied. To train this classifier, the teacher provides a reason vector ρ ∈ {0, 1} 3 , where ρ i = 1 indicates that the i-th condition is met. We formulate this prediction problem as multi-label binary classification and employ a binary logistic loss for each condition. Learning to predict the conditions helps the agent make more accurate and interpretable decisions. The helprequest loss is:</p>
<p>L ask (s,π ask , π ask ) = L NL (s,π ask ,π ask ) − logπ ask (a ask | o s ) (10)
− 1 3 3 i=1 [ρ i logρ i + (1 − ρ i ) log(1 −ρ i )]
Lreason(s,π ask ,π ask )</p>
<p>where a ask , ρ = π ask (s), andρ = Φ(o s ) is the agent-estimated likelihoods of the conditions.</p>
<p>Hierarchical Recurrent Architecture</p>
<p>We model the navigation policy and the helprequest policy as two separate neural networks. The two networks have similar architectures, which consists of three main components: the text-encoding component, the inter-task component, and the intra-task component (Figure 2). We use self-attention instead of recurrent neural networks to better capture long-term dependency, and develop novel cosine-similarity attention and ResNet-based time-encoding. Detail on the computations in each module is in the Appendix. The text-encoding component computes a text memory M text , which stores the hidden representation of the current language instruction. The</p>
<p>Split</p>
<p>Environments Tasks   inter-task module computes a vector h inter t representing the state of the current task's execution. During the episode, every time the current task is altered (due to the agent requesting help or departing a route), the agent re-encodes the new language instruction to generate a new text memory and resets the inter-task state to a zero vector. The intra-task module computes a vector h intra t representing the state of the entire episode. To compute this state, we first calculateh intra t , a tentative current state, andh intra t , a weighted combination of the past states at nearly identical situations. h intra t is computed as:
h intra t =h intra t − β ·h intra t (11) β = σ(W gate · [h intra t ;h intra t ])(12)
Eq 11 creates an context-sensitive dissimilarity between the current state and the past states at nearly identical situations. The scale vector β determines how large the dissimilarity is based on the inputs. This formulation incorporates past related information into the current state, thus enables the agent to optimize the curiosity-encouraging loss effectively. Finally, h intra t is passed through a softmax layer to produce an action distribution.</p>
<p>Experimental Setup</p>
<p>Dataset. We generate a dataset of object-finding tasks in the HANNA environments to train and evaluate our agent.  Table 3: Results on test splits. The agent with "perfect assistance interpretation" uses the teacher navigation policy (π nav ) to make decisions when executing a subtask from ANNA. Results of our final system are in bold.</p>
<p>language instruction vocabulary contains 2,332 words. The numbers of locations on the shortest paths to the requested objects are restricted to be between 5 and 15. With an average edge length of 2.25 meters, the agent has to travel about 9 to 32 meters to reach its goals. We evaluate the agent in environments that are seen during training (SEENENV), and in environments that are not seen (UNSEENALL). Even in the case of SEE-NENV, the tasks and the ANNA language instructions given during evaluation were never given in the same environments during training.</p>
<p>Hyperparameters. See Appendix.</p>
<p>Baselines and Skylines. We compare our agent against the following non-learning agents: 1. SHORTEST: uses the navigation teacher policy to make decisions (this is a skyline); 2. RAN-DOMWALK: randomly chooses a navigation action at every time step; 3. FORWARD10: navigates to the next location closest to the center of the current view to advance for 10 time steps. We compare our learned help-request policy with the following heuristics: 1. NOASK: does not request help; 2. RANDOMASK: randomly chooses to request help with a probabilty of 0.2, which is the average help-request ratio of our learned agent; 3. ASKEVERY5: requests help as soon as walking at least 5 time steps.</p>
<p>Evaluation metrics. Our main metrics are: success rate (SR), the fraction of examples on which the agent successfully solves the task; navigation error, the average (shortest-path) distance between the agent's final location and the nearest goal from that location; and SPL (Anderson et al., 2018a), which weights task success rate by travel distance as follows:
SPL = 1 N N i=1 S i L i max(P i , L i )(13)
where N is the number of tasks, S i indicates whether task i is successful, P i is the agent's travel distance, and L i is the shortest-path distance to the goal nearest to the agent's final location.</p>
<p>Results</p>
<p>Main results. From Table 3, we see that our problem is challenging: simple heuristic-based baselines such as RANDOMWALK and FOR-WARD10 attain success rates less than 7%. An agent that learns to accomplish tasks without additional assistance from ANNA succeeds only 17.21% of the time on TEST SEENENV, and 8.10% on TEST UNSEENALL. Leveraging help from ANNA dramatically boosts the success rate by 71.16% on TEST SEENENV and by 39.35% on TEST UNSEENALL over not requesting help. Given the small size of our dataset (e.g., the agent has fewer than 9,000 subtask instructions to learn from), it is encouraging that our agent is successful in nearly half of its tasks. On average, the agent takes paths that are 1.38 and 1.86 times longer than the optimal paths on TEST SEENENV and TEST UNSEENALL, respectively. In unseen environments, it issues on average twice as many requests to as it does in seen environments. To understand how well the agent interprets the ANNA instructions, we also provide results where our agent uses the optimal navigation policy to make decisions while executing subtasks. The large gaps on TEST SEENENV indicate there is still much room for improvement in the future, purely in learning to exe-   Does understanding language improve generalizability? Our agent is assisted with both language and visual instructions; similar to Thomason et al. (2019a), we disentangle the usefulness two these two modes of assistance. As seen in Table 4, the improvement from language on TEST UNSEENALL (+15.17%) is substantially more than that on TEST SEENENV (+3.42%), largely the agent can simply memorize the seen environments. This confirms that understanding language-based assistance effectively enhances the agent's capability of accomplishing tasks in novel environments.</p>
<p>Is learning to request help effective? Table 5 compares our learned help-request policies with baselines. We find that ASKEVERY5 provides a surprisingly strong baseline for this problem, leading to an improvement of +26.32% over not requesting help on TEST UNSEENALL. Nevertheless, our learned policy, with the ability to predict the future and access to the agent's uncertainty, outperforms all baselines by at least 10.40% in success rate on TEST UNSEENALL, while making less help requests. The small gap between the learned policy and ASKEVERY5 on TEST UN-SEENALL is expected because, on this split, the performance is mostly determined by the model's memorizing capability and is mostly insensitive to the help-request strategy.</p>
<p>Is proposed model architecture effective?</p>
<p>We implement an LSTM-based encoder-decoder model that is based on the architecture proposed  Table 6: Results on TEST UNSEENALL of our model, trained with and without curiosity-encouraging loss, and an LSTM-based encoder-decoder model (both models have about 15M parameters). "Navigation mistake repeat" is the fraction of time steps on which the agent repeats a non-optimal navigation action at a previously visited location while executing the same task. "Help-request repeat" is the fraction of help requests made at a previously visited location while executing the same task.</p>
<p>by (Wang et al., 2019). To incorporate the target image, we add an attention layer that uses the image's vector set as the attention memory. We train this model with imitation learning using the standard negative log likelihood loss (Eq 7), without the curiosity-encouraging and reason-prediction losses. As seen in Table 6, our hierarchical recurrent model outperforms this model by a large margin on TEST UNSEENALL (+28.2%).</p>
<p>Does the proposed imitation learning algorithm achieve its goals? The curiosity-encouraging training objective is proposed to prevent the agent from making the same mistakes at previously encountered situations. Table 6 shows that training with the curiosity-encouraging objective reduces the chance of the agent looping and making the same decisions repeatedly. As a result, its success rate is greatly boosted (+4.33% on TEST UN-SEENALL) over no curiosity-encouraging.</p>
<p>Conclusion</p>
<p>In this work, we present a photo-realistic simulator that mimics primary characteristics of real-life human assistance. We develop effective imitation learning techniques for learning to request and interpret the simulated assistance, coupled with a hierarchical neural network model for representing subtasks. Future work aims to provide more natural, linguistically realistic interaction between the agent and humans (e.g., providing the agent the ability ask a natural question rather than just signal for help), and to establish a theoretical framework for modeling human assistance. We are also exploring ways to deploy and evaluate our methods on real-world platforms.</p>
<p>be the current state, ot the current observation, and τt = (vt, ψt, ωt) : (r, I depart , I goal ) ← ANNA(st) navigation action: a nav t ← (p r 0 , ψ r − ψt, ω r − ωt), where p r 0 is the start location of route r</p>
<p>Table 1 :
1Comparing HANNA with other photo-realistic 
navigation problems. VLN (Anderson et al., 2018b) 
does not allow agent to request help. VNLA (Nguyen 
et al., 2019) models an advisor who is always present 
to help but speaks simple, templated language. CVDN </p>
<p>Table 2 :
2Data split.</p>
<p>Table 2 summarizes
2the dataset </p>
<p>Table 4 :
4Success rates (%) of agents on test splits with different types of assistance.SEENENV 
UNSEENALL </p>
<p>πask 
SR ↑ Requests/ 
SR ↑ Requests/ 
(%) 
task ↓ 
(%) 
task ↓ </p>
<p>NOASK 
17.21 
0.0 
8.10 
0.0 
RANDOMASK 82.71 
4.3 
37.05 
6.8 
ASKEVERY5 
87.39 
3.4 
34.42 
7.1 
Learned (ours) 88.37 
2.9 
47.45 
5.8 </p>
<p>Table 5 :
5Success rates (%) of different help-request policies on test splits. cute language instructions.</p>
<p>SR ↑ Nav. mistake ↓ Help-request ↓Model 
(%) 
repeat (%) 
repeat (%) </p>
<p>LSTM-ENCDEC 19.25 
31.09 
49.37 
Our model (α = 0) 43.12 
25.00 
40.17 
Our model (α = 1) 47.45 
17.85 
21.10 </p>
<p>Precisely, we use efficiency, or entropy of base |A nav | = 37, where A nav is the navigation action space.</p>
<p>Peter Anderson, Angel Chang, Devendra Singh Chaplot, Alexey Dosovitskiy, Saurabh Gupta, Vladlen Koltun, Jana Kosecka, Jitendra Malik, Roozbeh Mottaghi, Manolis Savva, arXiv:1807.06757On evaluation of embodied navigation agents. arXiv preprintPeter Anderson, Angel Chang, Devendra Singh Chap- lot, Alexey Dosovitskiy, Saurabh Gupta, Vladlen Koltun, Jana Kosecka, Jitendra Malik, Roozbeh Mottaghi, Manolis Savva, et al. 2018a. On evalu- ation of embodied navigation agents. arXiv preprint arXiv:1807.06757.</p>
<p>Visionand-language navigation: Interpreting visuallygrounded navigation instructions in real environments. Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko Sünderhauf, Ian Reid, Stephen Gould, Anton Van Den, Hengel, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionPeter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko Sünderhauf, Ian Reid, Stephen Gould, and Anton van den Hengel. 2018b. Vision- and-language navigation: Interpreting visually- grounded navigation instructions in real environ- ments. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3674-3683.</p>
<p>Marcin Andrychowicz, Bowen Baker, Maciek Chociej, Rafal Jozefowicz, Bob Mcgrew, Jakub Pachocki, Arthur Petron, Matthias Plappert, Glenn Powell, Alex Ray, arXiv:1808.00177Learning dexterous in-hand manipulation. arXiv preprintMarcin Andrychowicz, Bowen Baker, Maciek Chociej, Rafal Jozefowicz, Bob McGrew, Jakub Pachocki, Arthur Petron, Matthias Plappert, Glenn Powell, Alex Ray, et al. 2018. Learning dexterous in-hand manipulation. arXiv preprint arXiv:1808.00177.</p>
<p>. Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, Wojciech Zaremba, arXiv:1606.01540Openai gym. arXiv preprintGreg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wo- jciech Zaremba. 2016. Openai gym. arXiv preprint arXiv:1606.01540.</p>
<p>Collaborative language grounding toward situated human-robot dialogue. Y Joyce, Rui Chai, Changsong Fang, Lanbo Liu, She, 37AI MagazineJoyce Y Chai, Rui Fang, Changsong Liu, and Lanbo She. 2016. Collaborative language grounding to- ward situated human-robot dialogue. AI Magazine, 37(4):32-45.</p>
<p>Language to action: Towards interactive task learning with physical agents. Y Joyce, Qiaozi Chai, Lanbo Gao, Shaohua She, Sari Yang, Guangyue Saba-Sadiya, Xu, International Joint Conference on Artificial Intelligence. Joyce Y Chai, Qiaozi Gao, Lanbo She, Shaohua Yang, Sari Saba-Sadiya, and Guangyue Xu. 2018. Lan- guage to action: Towards interactive task learning with physical agents. In International Joint Confer- ence on Artificial Intelligence.</p>
<p>Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Niessner, Manolis Savva, Shuran Song, Andy Zeng, and Yinda Zhang. 2017. Mat-terport3D: Learning from RGB-D data in indoor environments. International Conference on 3D Vision (3DV). Angel Chang, Angela Dai, Thomas Funkhouser, Ma- ciej Halber, Matthias Niessner, Manolis Savva, Shu- ran Song, Andy Zeng, and Yinda Zhang. 2017. Mat- terport3D: Learning from RGB-D data in indoor en- vironments. International Conference on 3D Vision (3DV).</p>
<p>Learning to search better than your teacher. Kai-Wei Chang, Akshay Krishnamurthy, Alekh Agarwal, Hal Daume, Iii , John Langford, Proceedings of the International Conference of Machine Learning. the International Conference of Machine LearningKai-Wei Chang, Akshay Krishnamurthy, Alekh Agar- wal, Hal Daume III, and John Langford. 2015. Learning to search better than your teacher. In Pro- ceedings of the International Conference of Machine Learning.</p>
<p>Touchdown: Natural language navigation and spatial reasoning in visual street environments. Howard Chen, Alane Shur, Dipendra Misra, Noah Snavely, Ian Artzi, Stephen Yoav, Gould, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)and Anton van den HengelHoward Chen, Alane Shur, Dipendra Misra, Noah Snavely, Ian Artzi, Yoav, Stephen Gould, and An- ton van den Hengel. 2019. Touchdown: Natural language navigation and spatial reasoning in visual street environments. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recog- nition (CVPR).</p>
<p>Babyai: A platform to study the sample efficiency of grounded language learning. Maxime Chevalier-Boisvert, Dzmitry Bahdanau, Salem Lahlou, Lucas Willems, Chitwan Saharia, Yoshua Thien Huu Nguyen, Bengio, Proceedings of the International Conference on Learning Representations. the International Conference on Learning RepresentationsMaxime Chevalier-Boisvert, Dzmitry Bahdanau, Salem Lahlou, Lucas Willems, Chitwan Saharia, Thien Huu Nguyen, and Yoshua Bengio. 2019. Babyai: A platform to study the sample efficiency of grounded language learning. In Proceedings of the International Conference on Learning Representations.</p>
<p>Textworld: A learning environment for textbased games. Ákos Marc-Alexandre Côté, Xingdi Kádár, Ben Yuan, Tavian Kybartas, Emery Barnes, James Fine, Matthew Moore, Layla El Hausknecht, Mahmoud Asri, Wendy Adada, Adam Tay, Trischler, Computer Games Workshop at ICML/IJCAI. Marc-Alexandre Côté,Ákos Kádár, Xingdi Yuan, Ben Kybartas, Tavian Barnes, Emery Fine, James Moore, Matthew Hausknecht, Layla El Asri, Mah- moud Adada, Wendy Tay, and Adam Trischler. 2018. Textworld: A learning environment for text- based games. In Computer Games Workshop at ICML/IJCAI.</p>
<p>Embodied question answering. Abhishek Das, Samyak Datta, Georgia Gkioxari, Stefan Lee, Devi Parikh, Dhruv Batra, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionAbhishek Das, Samyak Datta, Georgia Gkioxari, Ste- fan Lee, Devi Parikh, and Dhruv Batra. 2018. Em- bodied question answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.</p>
<p>Search-based structured prediction. Hal Daumé, Iii , John Langford, Daniel Marcu, Machine learning. Springer75Hal Daumé III, John Langford, and Daniel Marcu. 2009. Search-based structured prediction. In Machine learning, volume 75, pages 297-325. Springer.</p>
<p>Speaker-follower models for vision-and-language navigation. Daniel Fried, Ronghang Hu, Volkan Cirik, Anna Rohrbach, Jacob Andreas, Louis-Philippe Morency, Taylor Berg-Kirkpatrick, Kate Saenko, Dan Klein, Trevor Darrell, Proceedings of Advances in Neural Information Processing Systems. Advances in Neural Information Processing SystemsDaniel Fried, Ronghang Hu, Volkan Cirik, Anna Rohrbach, Jacob Andreas, Louis-Philippe Morency, Taylor Berg-Kirkpatrick, Kate Saenko, Dan Klein, and Trevor Darrell. 2018. Speaker-follower models for vision-and-language navigation. In Proceedings of Advances in Neural Information Processing Sys- tems.</p>
<p>Imitation learning by coaching. He He, Jason Eisner, Hal Daumé, Iii , Proceedings of Advances in Neural Information Processing Systems. Advances in Neural Information Processing SystemsHe He, Jason Eisner, and Hal Daumé III. 2012. Imi- tation learning by coaching. In Proceedings of Ad- vances in Neural Information Processing Systems, pages 3149-3157.</p>
<p>Deep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recog- nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770- 778.</p>
<p>Qt-opt: Scalable deep reinforcement learning for vision-based robotic manipulation. Dmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian Ibarz, Alexander Herzog, Eric Jang, Deirdre Quillen, Ethan Holly, Mrinal Kalakrishnan, Vincent Vanhoucke, Proceedings of the Conference on Robot Learning. the Conference on Robot LearningDmitry Kalashnikov, Alex Irpan, Peter Pastor, Ju- lian Ibarz, Alexander Herzog, Eric Jang, Deirdre Quillen, Ethan Holly, Mrinal Kalakrishnan, Vincent Vanhoucke, et al. 2018. Qt-opt: Scalable deep rein- forcement learning for vision-based robotic manip- ulation. In Proceedings of the Conference on Robot Learning.</p>
<p>Progressive growing of gans for improved quality, stability, and variation. Tero Karras, Timo Aila, Samuli Laine, Jaakko Lehtinen, Proceedings of the International Conference on Learning Representations. the International Conference on Learning RepresentationsTero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. 2017. Progressive growing of gans for improved quality, stability, and variation. In Pro- ceedings of the International Conference on Learn- ing Representations.</p>
<p>A style-based generator architecture for generative adversarial networks. Tero Karras, Samuli Laine, Timo Aila, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionTero Karras, Samuli Laine, and Timo Aila. 2018. A style-based generator architecture for generative ad- versarial networks. In Proceedings of the IEEE Con- ference on Computer Vision and Pattern Recogni- tion.</p>
<p>From video game to real robot: The transfer between action spaces. Janne Karttunen, Anssi Kanervisto, Ville Hautamäki, Ville Kyrki, arXiv:1905.00741arXiv preprintJanne Karttunen, Anssi Kanervisto, Ville Hautamäki, and Ville Kyrki. 2019. From video game to real robot: The transfer between action spaces. arXiv preprint arXiv:1905.00741.</p>
<p>Vizdoom: A doom-based ai research platform for visual reinforcement learning. Michał Kempka, Marek Wydmuch, Grzegorz Runc, Jakub Toczek, Wojciech Jaśkowski, Computational Intelligence and Games (CIG), 2016 IEEE Conference on. IEEEMichał Kempka, Marek Wydmuch, Grzegorz Runc, Jakub Toczek, and Wojciech Jaśkowski. 2016. Viz- doom: A doom-based ai research platform for visual reinforcement learning. In Computational Intelli- gence and Games (CIG), 2016 IEEE Conference on, pages 1-8. IEEE.</p>
<p>. Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E Hin, arXiv:1607.06450ton. 2016. Layer normalization. arXiv preprintJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin- ton. 2016. Layer normalization. arXiv preprint arXiv:1607.06450.</p>
<p>Jointly learning grounded task structures from language instruction and visual demonstration. Changsong Liu, Shaohua Yang, Sari Saba-Sadiya, Nishant Shukla, Yunzhong He, Song-Chun, Joyce Zhu, Chai, Proceedings of Emperical Methods in Natural Language Processing. Emperical Methods in Natural Language ProcessingChangsong Liu, Shaohua Yang, Sari Saba-Sadiya, Nishant Shukla, Yunzhong He, Song-Chun Zhu, and Joyce Chai. 2016. Jointly learning grounded task structures from language instruction and visual demonstration. In Proceedings of Emperical Meth- ods in Natural Language Processing, pages 1482- 1492.</p>
<p>Self-monitoring navigation agent via auxiliary progress estimation. Chih-Yao Ma, Jiasen Lu, Zuxuan Wu, Ghassan Al-Regib, Zsolt Kira, Richard Socher, Caiming Xiong, Proceedings of the International Conference on Learning Representations. the International Conference on Learning RepresentationsChih-Yao Ma, Jiasen Lu, Zuxuan Wu, Ghassan Al- Regib, Zsolt Kira, Richard Socher, and Caiming Xiong. 2019a. Self-monitoring navigation agent via auxiliary progress estimation. In Proceedings of the International Conference on Learning Representa- tions.</p>
<p>The regretful agent: Heuristic-aided navigation through progress estimation. Chih-Yao Ma, Zuxuan Wu, Ghassan Alregib, Caiming Xiong, Zsolt Kira, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionChih-Yao Ma, Zuxuan Wu, Ghassan AlRegib, Caim- ing Xiong, and Zsolt Kira. 2019b. The regretful agent: Heuristic-aided navigation through progress estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6732-6740.</p>
<p>Mapping instructions to actions in 3d environments with visual goal prediction. Dipendra Misra, Andrew Bennett, Valts Blukis, Eyvind Niklasson, Max Shatkhin, Yoav Artzi, Proceedings of Emperical Methods in Natural Language Processing. Emperical Methods in Natural Language ProcessingAssociation for Computational LinguisticsDipendra Misra, Andrew Bennett, Valts Blukis, Eyvind Niklasson, Max Shatkhin, and Yoav Artzi. 2018. Mapping instructions to actions in 3d environments with visual goal prediction. In Proceedings of Em- perical Methods in Natural Language Processing, pages 2667-2678. Association for Computational Linguistics.</p>
<p>Mapping instructions and visual observations to actions with reinforcement learning. Dipendra Misra, John Langford, Yoav Artzi, Proceedings of Emperical Methods in Natural Language Processing. Emperical Methods in Natural Language ProcessingDipendra Misra, John Langford, and Yoav Artzi. 2017. Mapping instructions and visual observations to ac- tions with reinforcement learning. Proceedings of Emperical Methods in Natural Language Process- ing.</p>
<p>Tell me dave: Contextsensitive grounding of natural language to mobile manipulation instructions. K Dipendra, Jaeyong Misra, Kevin Sung, Ashutosh Lee, Saxena, Robotics: Science and Systems. Dipendra K Misra, Jaeyong Sung, Kevin Lee, and Ashutosh Saxena. 2014. Tell me dave: Contextsen- sitive grounding of natural language to mobile ma- nipulation instructions. In Robotics: Science and Systems.</p>
<p>Playing atari with deep reinforcement learning. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, Martin Riedmiller, NIPS Deep Learning Workshop. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. 2013. Playing atari with deep reinforcement learning. In NIPS Deep Learn- ing Workshop.</p>
<p>Learning goaloriented hierarchical tasks from situated interactive instruction. Shiwali Mohan, John Laird, Association for the Advancement of Artificial Intelligence. Shiwali Mohan and John Laird. 2014. Learning goal- oriented hierarchical tasks from situated interactive instruction. In Association for the Advancement of Artificial Intelligence.</p>
<p>Vision-based navigation with language-based assistance via imitation learning with indirect intervention. Khanh Nguyen, Debadeepta Dey, Chris Brockett, Bill Dolan, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionKhanh Nguyen, Debadeepta Dey, Chris Brockett, and Bill Dolan. 2019. Vision-based navigation with language-based assistance via imitation learning with indirect intervention. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 12527-12537.</p>
<p>Rerere: Remote embodied referring expressions in real indoor environments. Yuankai Qi, Qi Wu, Peter Anderson, Marco Liu, arXiv:1904.10151arXiv preprintChunhua Shen, and Anton van den HengelYuankai Qi, Qi Wu, Peter Anderson, Marco Liu, Chunhua Shen, and Anton van den Hengel. 2019. Rerere: Remote embodied referring expressions in real indoor environments. arXiv preprint arXiv:1904.10151.</p>
<p>Reinforcement and imitation learning via interactive noregret learning. Stephane Ross, Andrew Bagnell, arXiv:1406.5979arXiv preprintStephane Ross and J Andrew Bagnell. 2014. Rein- forcement and imitation learning via interactive no- regret learning. arXiv preprint arXiv:1406.5979.</p>
<p>A reduction of imitation learning and structured prediction to no-regret online learning. Stéphane Ross, Geoffrey Gordon, Drew Bagnell, Proceedings of Artificial Intelligence and Statistics. Artificial Intelligence and StatisticsStéphane Ross, Geoffrey Gordon, and Drew Bagnell. 2011. A reduction of imitation learning and struc- tured prediction to no-regret online learning. In Proceedings of Artificial Intelligence and Statistics, pages 627-635.</p>
<p>Imagenet large scale visual recognition challenge. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, International Journal of Computer Vision. 1153Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. 2015. Imagenet large scale visual recognition chal- lenge. International Journal of Computer Vision, 115(3):211-252.</p>
<p>Interactive robot task training through dialog and demonstration. Kevin Paul E Rybski, Jeremy Yoon, Manuela M Stolarz, Veloso, Proceedings of the ACM/IEEE international conference on Human-robot interaction. the ACM/IEEE international conference on Human-robot interactionACMPaul E Rybski, Kevin Yoon, Jeremy Stolarz, and Manuela M Veloso. 2007. Interactive robot task training through dialog and demonstration. In Pro- ceedings of the ACM/IEEE international conference on Human-robot interaction, pages 49-56. ACM.</p>
<p>Structured prediction via learning to search under bandit feedback. Amr Sharaf, Hal Daumé, Iii , Proceedings of the 2nd Workshop on Structured Prediction for Natural Language Processing. the 2nd Workshop on Structured Prediction for Natural Language ProcessingAmr Sharaf and Hal Daumé III. 2017. Structured pre- diction via learning to search under bandit feedback. In Proceedings of the 2nd Workshop on Structured Prediction for Natural Language Processing, pages 17-26.</p>
<p>Back to the blocks world: Learning new actions through situated human-robot dialogue. Lanbo She, Shaohua Yang, Yu Cheng, Yunyi Jia, Joyce Chai, Ning Xi, Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL). the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL)Lanbo She, Shaohua Yang, Yu Cheng, Yunyi Jia, Joyce Chai, and Ning Xi. 2014. Back to the blocks world: Learning new actions through situated human-robot dialogue. In Proceedings of the 15th Annual Meet- ing of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 89-97.</p>
<p>Deeply aggrevated: Differentiable imitation learning for sequential prediction. Wen Sun, Arun Venkatraman, Geoffrey J Gordon, Byron Boots, J Andrew Bagnell, Proceedings of the International Conference of Machine Learning. the International Conference of Machine LearningWen Sun, Arun Venkatraman, Geoffrey J Gordon, By- ron Boots, and J Andrew Bagnell. 2017. Deeply aggrevated: Differentiable imitation learning for se- quential prediction. In Proceedings of the Interna- tional Conference of Machine Learning.</p>
<p>Shifting the baseline: Single modality performance on visual navigation &amp; qa. Jesse Thomason, Daniel Gordan, Yonatan Bisk, Conference of the North American Chapter. Association for Computational LinguisticsJesse Thomason, Daniel Gordan, and Yonatan Bisk. 2019a. Shifting the baseline: Single modality per- formance on visual navigation &amp; qa. In Conference of the North American Chapter of the Association for Computational Linguistics.</p>
<p>Vision-and-dialog navigation. Jesse Thomason, Michael Murray, Maya Cakmak, Luke Zettlemoyer, Proceedings of the Conference on Robot Learning. the Conference on Robot LearningJesse Thomason, Michael Murray, Maya Cakmak, and Luke Zettlemoyer. 2019b. Vision-and-dialog navi- gation. In Proceedings of the Conference on Robot Learning.</p>
<p>Mujoco: A physics engine for model-based control. Emanuel Todorov, Tom Erez, Yuval Tassa, 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems. IEEEEmanuel Todorov, Tom Erez, and Yuval Tassa. 2012. Mujoco: A physics engine for model-based con- trol. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 5026-5033. IEEE.</p>
<p>Learning to speak and act in a fantasy text adventure game. Jack Urbanek, Angela Fan, Siddharth Karamcheti, Saachi Jain, Samuel Humeau, Emily Dinan, Tim Rocktäschel, Douwe Kiela, Arthur Szlam, Jason Weston, arXiv:1903.03094arXiv preprintJack Urbanek, Angela Fan, Siddharth Karamcheti, Saachi Jain, Samuel Humeau, Emily Dinan, Tim Rocktäschel, Douwe Kiela, Arthur Szlam, and Ja- son Weston. 2019. Learning to speak and act in a fantasy text adventure game. arXiv preprint arXiv:1903.03094.</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, Advances in neural information processing systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in neural information pro- cessing systems, pages 5998-6008.</p>
<p>Oriol Vinyals, Timo Ewalds, Sergey Bartunov, Petko Georgiev, Alexander Sasha Vezhnevets, Michelle Yeo, Alireza Makhzani, Heinrich Küttler, John Agapiou, Julian Schrittwieser, arXiv:1708.04782Starcraft ii: A new challenge for reinforcement learning. arXiv preprintOriol Vinyals, Timo Ewalds, Sergey Bartunov, Petko Georgiev, Alexander Sasha Vezhnevets, Michelle Yeo, Alireza Makhzani, Heinrich Küttler, John Aga- piou, Julian Schrittwieser, et al. 2017. Starcraft ii: A new challenge for reinforcement learning. arXiv preprint arXiv:1708.04782.</p>
<p>Kurt Harm De Vries, Dhruv Shuster, Devi Batra, Jason Parikh, Douwe Weston, Kiela, arXiv:1807.03367Talk the walk: Navigating new york city through grounded dialogue. arXiv preprintHarm de Vries, Kurt Shuster, Dhruv Batra, Devi Parikh, Jason Weston, and Douwe Kiela. 2018. Talk the walk: Navigating new york city through grounded dialogue. arXiv preprint arXiv:1807.03367.</p>
<p>Reinforced cross-modal matching and self-supervised imitation learning for vision-language navigation. Xin Wang, Qiuyuan Huang, Asli Celikyilmaz, Jianfeng Gao, Dinghan Shen, Yuan-Fang Wang, William Yang Wang, Lei Zhang, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionXin Wang, Qiuyuan Huang, Asli Celikyilmaz, Jian- feng Gao, Dinghan Shen, Yuan-Fang Wang, William Yang Wang, and Lei Zhang. 2019. Re- inforced cross-modal matching and self-supervised imitation learning for vision-language navigation. In Proceedings of the IEEE Conference on Com- puter Vision and Pattern Recognition.</p>
<p>Look before you leap: Bridging model-free and model-based reinforcement learning for planned-ahead vision-andlanguage navigation. Xin Wang, Wenhan Xiong, Hongmin Wang, William Yang Wang, Proceedings of the European Conference on Computer Vision. the European Conference on Computer VisionXin Wang, Wenhan Xiong, Hongmin Wang, and William Yang Wang. 2018. Look before you leap: Bridging model-free and model-based rein- forcement learning for planned-ahead vision-and- language navigation. In Proceedings of the Euro- pean Conference on Computer Vision.</p>
<p>Sean Welleck, Ilia Kulikov, Stephen Roller, Emily Dinan, Kyunghyun Cho, Jason Weston, arXiv:1908.04319Neural text generation with unlikelihood training. arXiv preprintSean Welleck, Ilia Kulikov, Stephen Roller, Emily Dinan, Kyunghyun Cho, and Jason Weston. 2019. Neural text generation with unlikelihood training. arXiv preprint arXiv:1908.04319.</p>
<p>Embodied question answering in photorealistic environments with point cloud perception. Erik Wijmans, Samyak Datta, Oleksandr Maksymets, Abhishek Das, Georgia Gkioxari, Stefan Lee, Irfan Essa, Devi Parikh, Dhruv Batra, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionErik Wijmans, Samyak Datta, Oleksandr Maksymets, Abhishek Das, Georgia Gkioxari, Stefan Lee, Irfan Essa, Devi Parikh, and Dhruv Batra. 2019. Em- bodied question answering in photorealistic environ- ments with point cloud perception. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6659-6668.</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-676 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-676</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-676</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-18.html">extraction-schema-18</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <p><strong>Paper ID:</strong> paper-1fdf449c96fbac0789cf8dfae15b788905407fd3</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/1fdf449c96fbac0789cf8dfae15b788905407fd3" target="_blank">Evaluating the Ripple Effects of Knowledge Editing in Language Models</a></p>
                <p><strong>Paper Venue:</strong> Transactions of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> This work proposes novel evaluation criteria that consider the implications of an edit on related facts, and constructs RippleEdits, a diagnostic benchmark of 5K factual edits, capturing various types of ripple effects.</p>
                <p><strong>Paper Abstract:</strong> Modern language models capture a large body of factual knowledge. However, some facts can be incorrectly induced or become obsolete over time, resulting in factually incorrect generations. This has led to the development of various editing methods that allow updating facts encoded by the model. Evaluation of these methods has primarily focused on testing whether an individual fact has been successfully injected, and if similar predictions for other subjects have not changed. Here we argue that such evaluation is limited, since injecting one fact (e.g., “Jack Depp is the son of Johnny Depp”) introduces a “ripple effect” in the form of additional facts that the model needs to update (e.g., “Jack Depp is the sibling of Lily-Rose Depp”). To address this, we propose novel evaluation criteria that consider the implications of an edit on related facts. Using these criteria, we then construct RippleEdits, a diagnostic benchmark of 5K factual edits, capturing various types of ripple effects. We evaluate prominent editing methods on RippleEdits, showing that they fail to introduce consistent changes in the model’s knowledge. In addition, we find that a simple in-context editing baseline obtains the best scores on our benchmark, suggesting a promising research direction for model editing.1</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e676.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e676.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Evaluation-scope mismatch (no ripple testing)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mismatch between standard knowledge-editing evaluation scope and required ripple-effect changes</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper identifies that common KE evaluation protocols (e.g., CounterFact, zsRE) describe and measure only direct edit success and unrelated-fact preservation, but their scope omits logically implied downstream changes ('ripple effects') that should also be updated after an edit.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Knowledge-editing evaluation protocols / benchmark specification</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Existing evaluation protocols for knowledge editing (e.g., CounterFact, zsRE style evaluations) that specify how to test whether an edit was applied and whether unrelated facts were preserved.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>research paper evaluation protocol / benchmark description</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>benchmark evaluation scripts and test-suite implementations used to compute accuracy on edits</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>incomplete specification / evaluation scope omission</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Prior evaluation descriptions specify checking that the edited triplet is produced and that unrelated facts remain unchanged, but they do not require checking for logically implied follow-up updates (ripple effects). As a result, the implemented evaluations treat successful local injection as sufficient, omitting tests for changes logically required by the edit (e.g., symmetric/transitive implications, compositions, alias propagation).</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>evaluation metrics and test-suite design (benchmark specification)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>conceptual analysis of existing benchmarks and empirical evaluation using the RippleEdits benchmark that implements 6 ripple-effect criteria</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Quantitative evaluation across six new criteria (Logical Generalization, Compositionality I & II, Subject Aliasing, Preservation, Relation Specificity); per-criterion accuracy measured and averaged per-edit and across dataset splits (Recent/Random/Popular).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Substantial: methods that pass standard checks often fail to propagate edits to implied facts; overall accuracy across ripple criteria was low (paper reports low average accuracies of ~38–66% across methods/models). This shows prior evaluations can overestimate real integration of edits.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Paper reports this as a general limitation of prior benchmarks; empirical evaluation on RippleEdits (5K edits) shows that failures to capture ripple effects are widespread across models and methods (low average accuracies across many settings).</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Evaluation descriptions were narrowly scoped (direct edit + unrelated-fact preservation) and omitted explicit logical propagation requirements; benchmark implementers therefore did not include tests for many implied relations.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Introduce RippleEdits: a benchmark and six concrete evaluation criteria that explicitly test ripple effects within a 2-hop neighborhood; provide NL templates and logical rules per relation and an evaluation pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Applying RippleEdits revealed weaknesses and enabled quantitative measurement; also motivated the in-context editing (ICE) baseline which yielded notably higher ripple-aware scores (ICE outperformed parametric methods by >10 points for GPT-NeoX and ~29 points for LLaMA on average across subsets).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning / natural language processing (knowledge editing in LMs)</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating the Ripple Effects of Knowledge Editing in Language Models', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e676.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e676.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KB-dependence and template mismatch</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mismatch due to reliance on external KB (WikiData) and NL template phrasing vs. implemented queries</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The authors note gaps arising from the data-generation choices: reliance on WikiData (which can be incomplete or outdated) and manual NL templates per relation can introduce mismatches between described/expected test queries and the model inputs actually used in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>RippleEdits data-generation pipeline (triplet-to-NL conversion)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Pipeline that samples triplets from WikiData, constructs edit targets, generates ripple-effect test triplets via hand-crafted logical rules, and converts triplets to natural-language queries with manually authored templates.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>benchmark data-generation specification / template documentation</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>data-generation scripts and NL templating code</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>incomplete specification / representation mismatch (KB incompleteness and template edge-cases)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>The pipeline depends on WikiData content and manually authored templates; WikiData incompleteness/outdatedness can cause missing or incorrect implied targets, and templating can produce non-grammatical or non-English entity renderings. While the paper validated a sample (100% semantic soundness, 98.5% grammaticality), these template and KB choices can still produce edge-case mismatches between the intended natural-language description of a test and the actual input text given to models.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>data preprocessing / test generation / prompt construction</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>manual data-quality audit: sampled 200 test queries and performed human checks for soundness and grammatical correctness; analysis of error cases and dataset statistics</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Human annotation of sampled queries: 100% soundness, 98.5% grammatical correctness reported; dataset-wide statistics reported (e.g., avg queries per edit).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Small but non-zero: 1.5% of sampled queries had grammatical issues (mostly non-English entity strings), which could affect model performance on those specific tests; more broadly, reliance on an incomplete KB limits coverage of ripple-effects and may bias evaluation toward relations present in WikiData.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>In the manual sample, grammatical issues were 1.5%; KB incompleteness is noted qualitatively as a systematic limitation but not quantified across the whole KB.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Implementation choices: use of a single external KB (WikiData) and manual NL templates; implicit assumptions that KB contains all required entailed facts and that templates generalize to all entity surface forms.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Manual validation of a sample; restrict relations to a fixed set with curated templates; suggest future work to use models' internal knowledge or additional KBs when KB incompleteness is a problem.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Manual validation showed high soundness and near-perfect grammaticality in sampled data (100% / 98.5%), indicating the mitigations work for most cases; however, the authors acknowledge remaining limitations and call for further work.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning / dataset curation / benchmark construction</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating the Ripple Effects of Knowledge Editing in Language Models', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e676.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e676.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Method-behavior mismatch (side effects & no propagation)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Discrepancy between intended edit semantics described in KE methods and observed implemented behavior (erroneous side effects, abstentions, or no propagation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper documents that state-of-the-art parametric editing methods (ROME, MEMIT, MEND) often do not propagate edits to logically implied facts and can introduce erroneous side effects or abstentions, diverging from the intended semantics described in prior method descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Knowledge-editing method implementations (ROME, MEMIT, MEND) and ICE baseline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Parametric editing algorithms that modify model weights or activations to inject factual edits (ROME, MEMIT, MEND) and a non-parametric in-context editing (ICE) baseline that conditions generation on a natural-language counterfactual prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>method specification in research papers (algorithm description / claimed guarantees)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>model-editing code (official implementations, modified implementations for specific models)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>different algorithm variant / incomplete effect (lack of implied-propagation) and unintended side-effects</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Although editing methods are described as modifying model knowledge to reflect new facts, implementations often only perform surface-level local changes and fail to implement full logical propagation. Empirically, edits frequently (a) introduce incorrect changes to other facts (noise), (b) cause the model to abstain, or (c) produce no effect on related queries. Error-type distributions (Table 7) show high prevalence of incorrect changes: e.g., for GPT-2 + ROME failures were 42% noise, 31% abstain, 27% no-effect; for ICE on GPT-Neo many failures were abstentions (65%).</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>model editing algorithm implementation / post-edit model behavior (inference)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Empirical evaluation using RippleEdits: run each editing method on many edits, then query the model on ripple-effect tests and categorize failures into no effect / abstain / noise; compute per-criterion accuracies.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Per-criterion accuracy metrics reported in Tables 3–5; error-type distributions from manual labelling of 200 failures per method/model (Table 7). Example aggregated metrics: overall per-method/model average accuracies reported in ranges ~38–66%; specific per-criterion accuracies vary widely.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Significant: many edits that are 'successful' under prior benchmarks do not produce consistent knowledge changes — this undermines claims of precise editing and reduces trust in downstream behavior. Quantitatively, parametric editors achieved low averages across ripple criteria (examples: ROME/MEMIT/MEND averaged 38–71 depending on model/subset), while ICE achieved substantially higher ripple-aware scores (e.g., ICE average up to ~90.3 on Random subset with GPT-3).</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Widespread across evaluated models and methods; empirical data across 5K edits shows these failures are common rather than exceptional.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Mechanistic limits of parametric editing approaches (they perform localized weight updates or low-rank modifications that do not ensure global logical propagation) and ambiguous evaluation targets that do not require propagation; additional causes include model-scale effects and entity popularity (edits to popular entities have higher severity and are harder to propagate).</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Propose comprehensive ripple-effect evaluation (RippleEdits) to surface these failures; explore non-parametric in-context editing (ICE) which conditions generation on the edited fact; suggest future work to design editing algorithms that ensure logical consistency and multi-hop propagation.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>ICE baseline showed improved propagation performance in experiments: ICE outperformed ROME by >10 points for GPT-NeoX and by ~29 points for LLaMA on average across dataset splits; however, authors note ample room for improvement and do not claim full mitigation.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning / NLP (knowledge editing in LMs)</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating the Ripple Effects of Knowledge Editing in Language Models', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Locating and editing factual associations in gpt <em>(Rating: 2)</em></li>
                <li>Fast model editing at scale <em>(Rating: 2)</em></li>
                <li>Mass-editing memory in a transformer <em>(Rating: 2)</em></li>
                <li>Detecting edit failures in large language models: An improved specificity benchmark <em>(Rating: 2)</em></li>
                <li>Mquake: Assessing knowledge editing in language models via multi-hop questions <em>(Rating: 2)</em></li>
                <li>Can we edit factual knowledge by in-context learning? <em>(Rating: 1)</em></li>
                <li>Measuring and manipulating knowledge representations in language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-676",
    "paper_id": "paper-1fdf449c96fbac0789cf8dfae15b788905407fd3",
    "extraction_schema_id": "extraction-schema-18",
    "extracted_data": [
        {
            "name_short": "Evaluation-scope mismatch (no ripple testing)",
            "name_full": "Mismatch between standard knowledge-editing evaluation scope and required ripple-effect changes",
            "brief_description": "The paper identifies that common KE evaluation protocols (e.g., CounterFact, zsRE) describe and measure only direct edit success and unrelated-fact preservation, but their scope omits logically implied downstream changes ('ripple effects') that should also be updated after an edit.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Knowledge-editing evaluation protocols / benchmark specification",
            "system_description": "Existing evaluation protocols for knowledge editing (e.g., CounterFact, zsRE style evaluations) that specify how to test whether an edit was applied and whether unrelated facts were preserved.",
            "nl_description_type": "research paper evaluation protocol / benchmark description",
            "code_implementation_type": "benchmark evaluation scripts and test-suite implementations used to compute accuracy on edits",
            "gap_type": "incomplete specification / evaluation scope omission",
            "gap_description": "Prior evaluation descriptions specify checking that the edited triplet is produced and that unrelated facts remain unchanged, but they do not require checking for logically implied follow-up updates (ripple effects). As a result, the implemented evaluations treat successful local injection as sufficient, omitting tests for changes logically required by the edit (e.g., symmetric/transitive implications, compositions, alias propagation).",
            "gap_location": "evaluation metrics and test-suite design (benchmark specification)",
            "detection_method": "conceptual analysis of existing benchmarks and empirical evaluation using the RippleEdits benchmark that implements 6 ripple-effect criteria",
            "measurement_method": "Quantitative evaluation across six new criteria (Logical Generalization, Compositionality I & II, Subject Aliasing, Preservation, Relation Specificity); per-criterion accuracy measured and averaged per-edit and across dataset splits (Recent/Random/Popular).",
            "impact_on_results": "Substantial: methods that pass standard checks often fail to propagate edits to implied facts; overall accuracy across ripple criteria was low (paper reports low average accuracies of ~38–66% across methods/models). This shows prior evaluations can overestimate real integration of edits.",
            "frequency_or_prevalence": "Paper reports this as a general limitation of prior benchmarks; empirical evaluation on RippleEdits (5K edits) shows that failures to capture ripple effects are widespread across models and methods (low average accuracies across many settings).",
            "root_cause": "Evaluation descriptions were narrowly scoped (direct edit + unrelated-fact preservation) and omitted explicit logical propagation requirements; benchmark implementers therefore did not include tests for many implied relations.",
            "mitigation_approach": "Introduce RippleEdits: a benchmark and six concrete evaluation criteria that explicitly test ripple effects within a 2-hop neighborhood; provide NL templates and logical rules per relation and an evaluation pipeline.",
            "mitigation_effectiveness": "Applying RippleEdits revealed weaknesses and enabled quantitative measurement; also motivated the in-context editing (ICE) baseline which yielded notably higher ripple-aware scores (ICE outperformed parametric methods by &gt;10 points for GPT-NeoX and ~29 points for LLaMA on average across subsets).",
            "domain_or_field": "machine learning / natural language processing (knowledge editing in LMs)",
            "reproducibility_impact": true,
            "uuid": "e676.0",
            "source_info": {
                "paper_title": "Evaluating the Ripple Effects of Knowledge Editing in Language Models",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "KB-dependence and template mismatch",
            "name_full": "Mismatch due to reliance on external KB (WikiData) and NL template phrasing vs. implemented queries",
            "brief_description": "The authors note gaps arising from the data-generation choices: reliance on WikiData (which can be incomplete or outdated) and manual NL templates per relation can introduce mismatches between described/expected test queries and the model inputs actually used in experiments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "RippleEdits data-generation pipeline (triplet-to-NL conversion)",
            "system_description": "Pipeline that samples triplets from WikiData, constructs edit targets, generates ripple-effect test triplets via hand-crafted logical rules, and converts triplets to natural-language queries with manually authored templates.",
            "nl_description_type": "benchmark data-generation specification / template documentation",
            "code_implementation_type": "data-generation scripts and NL templating code",
            "gap_type": "incomplete specification / representation mismatch (KB incompleteness and template edge-cases)",
            "gap_description": "The pipeline depends on WikiData content and manually authored templates; WikiData incompleteness/outdatedness can cause missing or incorrect implied targets, and templating can produce non-grammatical or non-English entity renderings. While the paper validated a sample (100% semantic soundness, 98.5% grammaticality), these template and KB choices can still produce edge-case mismatches between the intended natural-language description of a test and the actual input text given to models.",
            "gap_location": "data preprocessing / test generation / prompt construction",
            "detection_method": "manual data-quality audit: sampled 200 test queries and performed human checks for soundness and grammatical correctness; analysis of error cases and dataset statistics",
            "measurement_method": "Human annotation of sampled queries: 100% soundness, 98.5% grammatical correctness reported; dataset-wide statistics reported (e.g., avg queries per edit).",
            "impact_on_results": "Small but non-zero: 1.5% of sampled queries had grammatical issues (mostly non-English entity strings), which could affect model performance on those specific tests; more broadly, reliance on an incomplete KB limits coverage of ripple-effects and may bias evaluation toward relations present in WikiData.",
            "frequency_or_prevalence": "In the manual sample, grammatical issues were 1.5%; KB incompleteness is noted qualitatively as a systematic limitation but not quantified across the whole KB.",
            "root_cause": "Implementation choices: use of a single external KB (WikiData) and manual NL templates; implicit assumptions that KB contains all required entailed facts and that templates generalize to all entity surface forms.",
            "mitigation_approach": "Manual validation of a sample; restrict relations to a fixed set with curated templates; suggest future work to use models' internal knowledge or additional KBs when KB incompleteness is a problem.",
            "mitigation_effectiveness": "Manual validation showed high soundness and near-perfect grammaticality in sampled data (100% / 98.5%), indicating the mitigations work for most cases; however, the authors acknowledge remaining limitations and call for further work.",
            "domain_or_field": "machine learning / dataset curation / benchmark construction",
            "reproducibility_impact": true,
            "uuid": "e676.1",
            "source_info": {
                "paper_title": "Evaluating the Ripple Effects of Knowledge Editing in Language Models",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Method-behavior mismatch (side effects & no propagation)",
            "name_full": "Discrepancy between intended edit semantics described in KE methods and observed implemented behavior (erroneous side effects, abstentions, or no propagation)",
            "brief_description": "The paper documents that state-of-the-art parametric editing methods (ROME, MEMIT, MEND) often do not propagate edits to logically implied facts and can introduce erroneous side effects or abstentions, diverging from the intended semantics described in prior method descriptions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Knowledge-editing method implementations (ROME, MEMIT, MEND) and ICE baseline",
            "system_description": "Parametric editing algorithms that modify model weights or activations to inject factual edits (ROME, MEMIT, MEND) and a non-parametric in-context editing (ICE) baseline that conditions generation on a natural-language counterfactual prompt.",
            "nl_description_type": "method specification in research papers (algorithm description / claimed guarantees)",
            "code_implementation_type": "model-editing code (official implementations, modified implementations for specific models)",
            "gap_type": "different algorithm variant / incomplete effect (lack of implied-propagation) and unintended side-effects",
            "gap_description": "Although editing methods are described as modifying model knowledge to reflect new facts, implementations often only perform surface-level local changes and fail to implement full logical propagation. Empirically, edits frequently (a) introduce incorrect changes to other facts (noise), (b) cause the model to abstain, or (c) produce no effect on related queries. Error-type distributions (Table 7) show high prevalence of incorrect changes: e.g., for GPT-2 + ROME failures were 42% noise, 31% abstain, 27% no-effect; for ICE on GPT-Neo many failures were abstentions (65%).",
            "gap_location": "model editing algorithm implementation / post-edit model behavior (inference)",
            "detection_method": "Empirical evaluation using RippleEdits: run each editing method on many edits, then query the model on ripple-effect tests and categorize failures into no effect / abstain / noise; compute per-criterion accuracies.",
            "measurement_method": "Per-criterion accuracy metrics reported in Tables 3–5; error-type distributions from manual labelling of 200 failures per method/model (Table 7). Example aggregated metrics: overall per-method/model average accuracies reported in ranges ~38–66%; specific per-criterion accuracies vary widely.",
            "impact_on_results": "Significant: many edits that are 'successful' under prior benchmarks do not produce consistent knowledge changes — this undermines claims of precise editing and reduces trust in downstream behavior. Quantitatively, parametric editors achieved low averages across ripple criteria (examples: ROME/MEMIT/MEND averaged 38–71 depending on model/subset), while ICE achieved substantially higher ripple-aware scores (e.g., ICE average up to ~90.3 on Random subset with GPT-3).",
            "frequency_or_prevalence": "Widespread across evaluated models and methods; empirical data across 5K edits shows these failures are common rather than exceptional.",
            "root_cause": "Mechanistic limits of parametric editing approaches (they perform localized weight updates or low-rank modifications that do not ensure global logical propagation) and ambiguous evaluation targets that do not require propagation; additional causes include model-scale effects and entity popularity (edits to popular entities have higher severity and are harder to propagate).",
            "mitigation_approach": "Propose comprehensive ripple-effect evaluation (RippleEdits) to surface these failures; explore non-parametric in-context editing (ICE) which conditions generation on the edited fact; suggest future work to design editing algorithms that ensure logical consistency and multi-hop propagation.",
            "mitigation_effectiveness": "ICE baseline showed improved propagation performance in experiments: ICE outperformed ROME by &gt;10 points for GPT-NeoX and by ~29 points for LLaMA on average across dataset splits; however, authors note ample room for improvement and do not claim full mitigation.",
            "domain_or_field": "machine learning / NLP (knowledge editing in LMs)",
            "reproducibility_impact": true,
            "uuid": "e676.2",
            "source_info": {
                "paper_title": "Evaluating the Ripple Effects of Knowledge Editing in Language Models",
                "publication_date_yy_mm": "2023-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Locating and editing factual associations in gpt",
            "rating": 2
        },
        {
            "paper_title": "Fast model editing at scale",
            "rating": 2
        },
        {
            "paper_title": "Mass-editing memory in a transformer",
            "rating": 2
        },
        {
            "paper_title": "Detecting edit failures in large language models: An improved specificity benchmark",
            "rating": 2
        },
        {
            "paper_title": "Mquake: Assessing knowledge editing in language models via multi-hop questions",
            "rating": 2
        },
        {
            "paper_title": "Can we edit factual knowledge by in-context learning?",
            "rating": 1
        },
        {
            "paper_title": "Measuring and manipulating knowledge representations in language models",
            "rating": 1
        }
    ],
    "cost": 0.013714999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Evaluating the Ripple Effects of Knowledge Editing in Language Models</h1>
<p>Roi Cohen ${ }^{1}$ Eden Biran ${ }^{1}$ Ori Yoran ${ }^{1}$ Amir Globerson ${ }^{1,2}$ Mor Geva ${ }^{1,2, *}$<br>${ }^{1}$ Blavatnik School of Computer Science, Tel Aviv University, Israel ${ }^{2}$ Google Research, Israel<br>{roi1, edenbiran, oriy}@mail.tau.ac.il, {gamir, morgeva}@tauex.tau.ac.il</p>
<h4>Abstract</h4>
<p>Modern language models capture a large body of factual knowledge. However, some facts can be incorrectly induced or become obsolete over time, resulting in factually incorrect generations. This has led to the development of various editing methods that allow updating facts encoded by the model. Evaluation of these methods has primarily focused on testing whether an individual fact has been successfully injected, and if similar predictions for other subjects have not changed. Here we argue that such evaluation is limited, since injecting one fact (e.g., "Jack Depp is the son of Johnny Depp") introduces a "ripple effect" in the form of additional facts that the model needs to update (e.g., "Jack Depp is the sibling of Lily-Rose Depp"). To address this, we propose novel evaluation criteria that consider the implications of an edit on related facts. Using these criteria, we then construct RippleEdits, a diagnostic benchmark of 5 K factual edits, capturing various types of ripple effects. We evaluate prominent editing methods on RippleEdits, showing that they fail to introduce consistent changes in the model's knowledge. In addition, we find that a simple in-context editing baseline obtains the best scores on our benchmark, suggesting a promising research direction for model editing. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>Modern language models (LMs) capture a large volume of factual knowledge in their parameters, which can be effectively utilized in downstream tasks (Petroni et al., 2019; Roberts et al., 2020; Shin et al., 2020; Razniewski et al., 2021; Heinzerling and Inui, 2021; Kadavath et al., 2022; Cohen et al., 2023). However, factual beliefs captured by the model may be incorrect or become</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>outdated over time, potentially affecting the model's performance on downstream tasks, its reliability, and its usability (Dhingra et al., 2022; Lazaridou et al., 2021; Jang et al., 2022).</p>
<p>This limitation has prompted research on knowledge editing (KE) methods, which modify LMs to fix their factual errors (we provide a formal definition in $\S 2$ ). Knowledge editing work has focused on applying factual updates to LMs. Given an entity-relation-object triplet $(e, r, o)$ representing a fact (e.g., "Lionel Messi plays for the Inter Miami team'), recent work proposed various methods (Mitchell et al., 2022; Meng et al., 2022, 2023; Hernandez et al., 2023b; Si et al., 2023) to inject this fact into the parameters of a given LM, while "overriding" beliefs the model might have on $e$ and $r$ (e.g., that Messi plays for Paris Saint-Germain).</p>
<p>A key question with KE is how to evaluate the success of such editing operations. The most basic "sanity-check" is that the model correctly completes $(e, r, ?)$, as well as other paraphrases of this task, with $o$. However, this is not enough as an evaluation, since one needs to check that the model did not distort other facts. Indeed, the standard evaluation protocol (Mitchell et al., 2022; Meng et al., 2022, 2023) for KE focuses on these two aspects of correctly completing various paraphrases of the new fact, as well as ensuring that other unrelated facts have not been changed.</p>
<p>In this work, we argue that to evaluate model edits, one should go beyond the single fact that was edited and check that other facts that are logically derived from the edit were also changed accordingly. For example, if $z$ is the mother of $e$, then the children of $z$ are the siblings of $e$. Consequently, once we modify the belief of a certain model that $z \rightarrow z^{\prime}$ is the mother of $e$, then we should also ensure that the model's belief regarding the siblings of $e$ is also correct. Figure 1 illustrates another example, where editing the Team for which Lionel Messi plays modifies other related facts, such as his country of residence, while other facts</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Illustration of the evaluation scope of RippleEdits, compared to existing knowledge editing benchmarks. For a given factual edit, we consider the "ripple effect" of the edit on the model's knowledge.
should be retained. We refer to such changes that are implied by a factual edit as ripple effects.</p>
<p>To account for ripple effects in the evaluation of factual edits, we propose six concrete evaluation criteria (see $\S 3$, Figure 2), for testing which facts other than the edit itself should be modified or retained post-editing. Our tests evaluate how well the model integrates the edit with the rest of its knowledge, through queries that involve logical reasoning, complex composition of facts with the edit as an intermediate step, subject aliasing, and specificity across relations.</p>
<p>Building upon these criteria, we create RippleEdits, a new benchmark for comprehensive evaluation of KE of LMs (see §4). RippleEdits includes 5 K entries, each consisting of a factual edit, along with a set of test queries that check if the edit was successful in terms of its ripple effect. Moreover, RippleEdits contains meta-data for each edit, including information about the timestamp of the edit (i.e., recent versus old), and the popularity of the entities (i.e., head versus tail).</p>
<p>We use RippleEdits to evaluate three popular editing methods on five recent strong LMs (see §5). We find that, even though current KE methods are effective in modifying a particular fact, they often fail to capture the ripple effects entailed by that fact, and demonstrate poor performance on most of our evaluation criteria. Moreover, analyzing how editing performance varies across model sizes and entity frequencies, we find that (a) larger models handle ripple effects better, and (b) editing frequent entities results in more logical reasoning errors.</p>
<p>Last, we consider a simple in-context editing baseline for KE that leverages the casual attention mechanism rather than explicit parametric updates. While this method achieves the best results on our benchmark, outperforming current parametric KE methods, there is still ample room for improvement that calls for future research.</p>
<p>To conclude, our work makes multiple contributions: (a) it highlights key limitations of KE evaluation, specifically regarding ripple effects and introduces comprehensive evaluation criteria to mitigate those limitations, (b) it proposes RippleEdits, a benchmark inspired by these criteria, (c) it evaluates current methods for KE and shows that they do not perform well on this task, while demonstrating that in-context editing is a promising direction for KE. We release RippleEdits and our code to facilitate future work on KE.</p>
<h2>2 Problem Setting</h2>
<p>We consider editing of factual knowledge, where facts are expressed as triplets $(e, r, o)$ of a subject entity $e$ (e.g., Lionel Messi), a relation $r$ (e.g., Team), and an object $o$ (e.g., Inter Miami). We distinguish between two edit types, based on the knowledge encoded in the model before the edit: (a) modification of a fact that is already encoded in the model $(e, r, o) \rightarrow\left(e, r, o^{<em>}\right)$, that is, updating the object $o \rightarrow o^{</em>}$ for a given subject $e$ and relation $r$, and (b) injection of a new fact $\left(e, r, o^{<em>}\right)$ that is not captured by the model. Moreover, we note that for one-to-one relations like Date of birth, where there is a single object for a given subject, an injection edit can be viewed as populating an empty object $(e, r, \emptyset) \rightarrow\left(e, r, o^{</em>}\right)$. In contrast, for one-to-many relations, such as Sibling and Occupation, an injection edit augments the set of objects $(e, r$,</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: An illustration of our evaluation criteria, for an edit that simulates adding a sibling to the subject entity Prince, shown at the top of each graph with a bold arrow and an edit sign over the Sibling relation. For each criterion, the tested subject and target object are circles with dashed blue line and solid green line, respectively, and other nodes in dotted orange line. For Logical Generalization (A), the additional fact that needs to be inserted to the knowledge graph (KG) is presented with an edit sign next to the relation. We show the same node in different colors for completeness, as the tested subject is also the object in the edit that needs to be inserted. For Compositionality I, II (B, C), the model needs to hop over the edit to arrive at the target. In Subject Aliasing (D) we verify the edit also propagates to paraphrases of the input. In Preservation (E), we verify that other targets of the edited subject-relation are preserved. In Relation Specificity, we verify other relations for the subject are not modified.
$\left{o_{1}, . ., o_{n}\right}) \rightarrow\left(e, r,\left{o_{1}, . ., o_{n}, o^{*}\right}\right)$. Whether an edit is viewed as a modification or injection, depends on whether that information was captured in the model before the edit. Moreover, evaluating if a specific fact (before or after an edit) is encoded by a model is typically done by testing if the model predicts the object for various input queries that represent the subject and relation (see more details in $\S 3.2$ ).</p>
<h2>3 Ripple Effects of Factual Edits</h2>
<p>We focus on evaluating the downstream effect of a given edit, i.e., given an edit $(e, r, o) \rightarrow\left(e, r, o^{\prime}\right)$, we expect certain facts related to the edit to change as well. Consider, for example, the edit shown in Figure 1. Changing the team for which Messi plays might also affect the league he plays in and his country of residence. Formally, for a given model, assume a knowledge-graph $\mathcal{K}:=\left{\left(e_{i}, r_{i}, o_{i}\right)\right}_{i=1}^{N}$ of $N$ factual triplets, representing the model's knowledge, and let $\delta:(e, r, o) \rightarrow\left(e, r, o^{\prime}\right)$ be an edit request for $\mathcal{K}$. We define the ripple effect of $\delta$ on $\mathcal{K}$, as the set of triplets $\mathcal{R}(\delta)$ that the model
implicitly needs to inject, modify, or delete from $\mathcal{K}$ to reflect the world state after the edit.</p>
<p>Notably, different edits can cause ripple effects of varying magnitudes. For example, changing the country of Rome from Italy to France, will entail many follow-up changes, such as the country in which the Colosseum is located, the language spoken in Rome, and so forth. In contrast, updating the siblings of Prince (Figure 2) is both more realistic and should result in a more local effect. We refer to the number of facts affected by a single edit $\delta$ (i.e., $|\mathcal{R}(\delta)|$ ) as its severity. In general, editing popular entities that appeared frequently during training is likely to introduce more changes, and thus, editing their properties has a higher severity.</p>
<h3>3.1 Evaluation Criteria</h3>
<p>We wish to evaluate how well models capture the ripple effects of factual edits. However, given that ripple effects can potentially span a large number of implied edits, we focus on evaluating modified facts that are within a 2-hop distance from the</p>
<p>subject or object of the edit. Concretely, for an edit $\delta:(e, r, o) \rightarrow\left(e, r, o^{*}\right)$, we evaluate the ripple effect $\mathcal{R}(\delta)$, via the following evaluation criteria (examples are shown in Figure 2):</p>
<ol>
<li>Logical Generalization (LG): Relations in a knowledge graph satisfy certain logical constraints. For example, the relation Sibling is symmetric and therefore if ( $e$, Sibling, $o$ ) is true then ( $o$, Sibling, $e$ ) is also true, and vice versa (Figure 2A). Likewise, the relation Location is transitive so $(e$, Location, $o) \wedge(o$, Location, $z) \Rightarrow$ $(e$, Location, $z)$. We wish to check that such logical implications about the subject $e$, the original object $o$, and the new object $o^{*}$, hold after editing. We focus and elaborate on specific constraints in $\S 4$.</li>
<li>Compositionality I (CI): As $\delta$ alters one edge in a knowledge graph, we can check the composition of this edge with other edges. Namely, we test if the model can compose the edited fact with other facts about the target object. Let $\left(o, r^{\prime}, z\right)$ and $\left(o^{<em>}, r^{\prime}, z^{</em>}\right)$ be two facts of the same relation about $o$ and $o^{<em>}$, respectively. Also, denote by $r^{\prime \prime}=r \circ r^{\prime}$ the complex relation expressing the composition of $r$ and $r^{\prime}$ (e.g., $r^{\prime \prime}=$ Profession of sibling for $r=$ Sibling and $r^{\prime}=$ Profession). Then, after the edit $\delta$, we expect the following change $\left(e, r^{\prime \prime}, z\right) \rightarrow\left(e, r^{\prime \prime}, z^{</em>}\right)$. For example (Figure 2B), the professions of the siblings of Prince can be modified once a new sibling is injected.</li>
<li>Compositionality II (CII): We test if the model can compose the edited fact with facts about a different subject $e^{\prime} \neq e$. Formally, let $\left(e^{\prime}, r^{\prime}, e\right)$ be a fact about $e^{\prime}$ with $e$ as its object, and denote by $r^{\prime \prime}=r^{\prime} \circ r$ the complex relation expressing the composition of $r^{\prime}$ and $r$ (see an example in criterion 2). After the edit $\delta$, the following change is expected for the subject $e^{\prime}:\left(e^{\prime}, r^{\prime \prime}, o\right) \rightarrow\left(e^{\prime}, r^{\prime \prime}, o^{*}\right)$. For instance (Figure 2C), changing the siblings of Prince also modifies the siblings of the founder of Paisley Park Records (i.e., $r^{\prime \prime}$ is a complex relation expressing "siblings of the founder').</li>
<li>Subject Aliasing (SA): We test that editing a fact about $e$ induces the same edit to other
entities $e^{\prime}$ that are aliases of $e$, namely, $\left(e^{\prime}\right.$, $r, o) \rightarrow\left(e^{\prime}, r, o^{*}\right)$. For instance (Figure 2D), modifying the siblings of Prince should also modify the sibling of his alias, Prince Roger Nelson.</li>
<li>Preservation (PV): If $r$ is a one-to-many relation, then adding a new object should not affect the other objects encoded about $e$. Hence, in such cases, we expect that any existing triplet $\left(e, r, o^{\prime}\right)$ for an object $o^{\prime} \neq o^{*}$ would remain following the edit. For example (Figure 2E), after inserting the sibling Nicholas Carminowe for Prince, the fact that Tyka Nelson is also his sibling should be retained.</li>
<li>Relation Specificity (RS): We test that facts about $e$, with relations whose objects are not influenced by $o$, are indeed not affected by the edit. For example (Figure 2F), modifying the sibling of Prince should not change his Mother. Note that these facts complement those evaluated by Logical Generalization.</li>
</ol>
<p>In $\S 4.1$, we describe how we generate factual editing evaluations, based on the above criteria.</p>
<h3>3.2 Related Work</h3>
<p>Knowledge Editing Methods Several methods have been proposed to edit the factual knowledge encoded in a model. De Cao et al. (2021) and Mitchell et al. (2022) suggested using hypernetworks to update the model weights. In addition, Meng et al. $(2022,2023)$ proposed to modify encoded facts by updating the weights of MLP layers, following recent observations that these layers can be cast as key-value memories (Geva et al., 2021) that store factual knowledge (Dai et al., 2022). Other methods learn encodings that update the hidden representations created during model inference (Hernandez et al., 2023a), or augment the input context with edits (Zhong et al., 2023; Zheng et al., 2023). In $\S 5.1$, we discuss state-of-the-art KE methods used in this work in greater detail.</p>
<p>Separately from factual KE, recent work has also studied how to inject new facts into a model. Previous methods suggested unsupervised pretraining (Roberts et al., 2020; Zhang et al., 2021), semi-parametric methods, where external information is added from a knowledge-base (Zhang</p>
<p>et al., 2019; Peters et al., 2019; Lewis et al., 2020; Zhang et al., 2022), using adapters to store knowledge (Wang et al., 2021a), or extending the MLP layers (Yao et al., 2022).</p>
<p>Knowledge Editing Evaluation Recently, there has been a growing interest in KE evaluation (Yao et al., 2023). The prominent benchmarks for evaluating factual KE are the Zero-Shot Relation Extraction (zsRE) (Levy et al., 2017; De Cao et al., 2021) and CounterFact (Meng et al., 2022). zsRE is a question-answering dataset for relationspecific queries, which includes human generated paraphrases that are used to measure robustness to semantically equivalent inputs. For example, for the triplet (x, Country, y), zsRE contains queries such as "In which country is $x$ ?". CounterFact offers a more challenging setting, where edits are counterfactuals of a low probability, such as changing the City of The Louvre from Paris to Rome.</p>
<p>Evaluation in zsRE and CounterFact focuses on three primary aspects of (a) efficacy: checking that the model generates the target object postediting, (b) paraphrasing: testing robustness in generating the target for paraphrases of the input, and (c) specificity: verifying that facts not related to the edit are unaffected. In addition, CounterFact evaluates the generation quality of the edited model when prompted with the edit's subject, measuring: consistency, i.e., similarity with subjects that share the same property as the edited object, and fluency in terms of repetitiveness of the generated text. More broadly, previous work evaluated to which extent LMs have beliefs (Genin and Huber, 2022; Kassner et al., 2021; Hase et al., 2023), and Hase et al. (2023) examined if updating beliefs propagates to entailed facts, extending the Wikidata5m dataset (Wang et al., 2021b) to test editing specificity.</p>
<p>Recently, Onoe et al. (2023) introduce the task of entity knowledge propagation, aiming to examine the extent to which models are able to reason about emergent entities that did not appear in pretraining. In addition, Hoelscher-Obermaier et al. (2023) show that existing KE methods can have unwanted side effects and suffer from low specificity. A concurrent work by Zhong et al. (2023) introduces MQUAKE, a benchmark that tests the ability of models to perform multi-hop reasoning after edits. While each of these benchmarks focuses on a single consequence of editing, Ripple-</p>
<p>Edits provides a general framework for evaluating various types of edit ripple effects. Last, Gupta et al. (2023) focus on editing commonsense knowledge and introduce MEMIT-CSKPROBE, a dataset for semantic generalization of commonsense edits. RippleEdits is different from MEMITCSKPROBE as it evaluates editing of factual knowledge rather than commonsense knowledge.</p>
<h2>4 The RippleEdits Benchmark</h2>
<p>In this section, we describe a data generation pipeline $(\S 4.1)$ for factual edit requests and queries for evaluating their ripple effects. Then, we apply our pipeline to create the RippleEdits benchmark for comprehensive KE evaluation (§4.2), and validate the quality of the data (§4.3).</p>
<h3>4.1 Data Generation Pipeline</h3>
<p>We describe our data generation process (illustrated in Figure 3), that creates KE evaluation examples, each consisting of a factual edit request and a set of test queries that follow our criteria. Since the pipeline involves manual writing of templates and logical rules per relation, we restrict the edits and test queries to a fixed set of $N_{\text {rel }}$ basic relations. ${ }^{2}$</p>
<p>Step 1: Factual Triplet Collection The first step of the pipeline (Figure 3A) is to collect facts, from which we will later create edit requests. To this end, we use WikiData, a relational knowledge base consisting of facts that are expressed as triplets $(e, r, o)$, where $e$ is a subject entity, $r$ is a relation, and $o$ is an object. We collect triplets of three types:</p>
<ul>
<li>Recent: To create "real" plausible edit requests, we collect triplets that were inserted to WikiData only recently, and represent relatively new facts. Therefore, they can be used to create injection edit requests for models that were trained before these facts were introduced, to simulate cases of an out-of-date model that requires factual updates. We collect such facts by randomly sampling triplets that have been modified during a range of 250 days after July 2022.</li>
</ul>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Illustration of our data generation process. We start by sampling a fact from a KG (A), here (Bill Gates, Spouse, Melinda Gates). Then, we generate the target triplet for the edit (B), in this case, choosing an object (Ricciarda Cybo Malaspina) that shares the same type as the original object. Next, we generate test queries (C) by sampling new triplets from the KG that should be retained or modified post-editing. Last, we utilize pre-defined templates to translate the KG triplets to natural language phrases (D).</p>
<ul>
<li>Random: We collect triplets corresponding to random facts, for which we will later generate modification edits (similarly to Meng et al., 2022). These edits simulate factual edits that are meant to fix incorrect model predictions (e.g., predicting that the capital of Germany is Frankfurt). To this end, we divide the entities in WikiData into 10 uniform buckets, based on the number of triplets associated with them. Intuitively, this can be viewed as a popularity measure. Then, we sample $N_{\text {ent }}$ entities from each group and randomly choose one triplet for each entity.</li>
<li>Popular: The two previous triplet types are randomly sampled from the entire knowledge base, and most of them are likely to represent facts about tail entities (except perhaps for a small subset in the top bucket). Such entities are often not captured by models (Mallen et al., 2023), and therefore not suitable for testing modification edits. To address this, we sample triplets from WikiData with a subject that is a popular entity, namely, it appears in one of the top-viewed pages in Wikipedia. ${ }^{3}$ Importantly, these types of triplets allow controlling for the ripple effect severity ( $\S 3$ ), i.e., how models handle the ripple effects of popular entities versus tail entities.</li>
</ul>
<p>Step 2: Edit Generation Once we obtain factual triplets, we turn to generate edit requests for them (Figure 3B). For Recent, triplets represent new facts that are meant to be injected to the model, assuming that the latter was trained before these facts were introduced to the world. Hence, for</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Recent, the target triplet for injection is the triplet itself.</p>
<p>For Random and Popular triplets, we create an edit by generating a target triplet as follows. First, for every relation $r$, we create a set of candidate object entities $O_{r}$ by sampling $N_{\text {cand }}$ triplets $\left(e_{1}, r, o_{1}\right), \ldots,\left(e_{N_{\text {cand }}}, r, o_{N_{\text {cand }}}\right)$ with the relation $r$, and extracting their objects $O_{r}=\left{o_{1}\right.$, $\left.\ldots, o_{N_{\text {cand }}}\right}$. Then, for every triplet $(e, r, o)$ in Random and Popular, we sample a target object $o^{\prime} \neq o$ from $O_{r}$. Sampling the target object from triplets with the same relation makes the edit request technically consistent with the original triplet - the target object is of the same "type" as the original object (for example, a triplet with the relation Capital will get a new object of type City). The new triplet $\left(e, r, o^{\prime}\right)$ will thus result in a "fake" fact, since it attaches a wrong object $o^{\prime}$ to the pair $(e, r)$. For example, if RANDOM contains the triplet (France, Capital, Paris), its edit could be (France, Capital, London).</p>
<p>Step 3: Evaluation Test Generation The next step in the pipeline is to create ripple effect evaluations for the factual edits we collected (Figure 3C). To this end, we implement the evaluation criteria introduced in $\S 3.1$, and generate test queries for each criterion. Each test query corresponds to a triplet of subject and object entities and a possibly complex relation, that is expected to be true post-editing. In what follows, we provide details on our implementation, using objects from WikiData.</p>
<p>For an entity $e$, we denote by $\mathcal{S}(e)$ the set of triplets in WikiData in which $e$ is the subject, and by $\mathcal{T}(e)$ the set of triplets in which $e$ is the object. Moreover, for every relation $r$, we manually define a set $D_{r}$ of relations that semantically depend on it. Namely, for a given subject, changing</p>
<p>$r$ 's target object is expected to change the target objects for the relations $D_{r}$. For instance, the set $D_{r}$ for the relation $r=$ Mother, includes the relations Sibling, Sister, Brother, Aunt, and Uncle, among others. Then, for every relation $r^{\prime} \in D_{r}$, we craft a logical rule for obtaining the new target for that relation post-editing. For instance, for the relation $r=$ Sibling, we set a logical rule for $r^{\prime}=$ Mother such that if $\left(e, r, e^{\prime}\right)$ and $\left(e^{\prime}, r^{\prime}, z^{\prime}\right)$ are true for entities $e, e^{\prime}, z^{\prime}$, then $\left(e, r^{\prime}, z^{\prime}\right)$ should also be true.</p>
<p>Given an edit $(e, r, o) \rightarrow\left(e, r, o^{<em>}\right)$, we use $D_{r}$ to generate test queries for Logical Generalization and Relation Specificity. For Logical Generalization, we apply the rule corresponding to each relation $r^{\prime} \in D_{r}$ to obtain a set of test queries $\left(x, r^{\prime}, z^{\prime}\right)$ about $x \in\left{e, o, o^{</em>}\right}$, where $z^{\prime}$ is the target obtained from the logical rule. For Relation Specificity, we create a test query for every triplet in $\mathcal{S}(e)$ with a relation that is not in $D_{r}$ (but is in our set of $N_{\text {rel }}$ relations).</p>
<p>To generate text queries for Compositionality $I$, we iterate through $\mathcal{S}\left(o^{<em>}\right)$ and for each triplet $\left(o^{</em>}, r^{\prime}, z\right) \in \mathcal{S}\left(o^{<em>}\right)$, we construct a two-hop query $\left(e, r \circ r^{\prime}, z\right)$ about $e$, with $z$ as the answer. Similarly, for Compositionality II, we iterate through $\mathcal{T}(e)$ and for each triplet $\left(z, r^{\prime}, e\right) \in \mathcal{T}(e)$, we construct a two-hop query $\left(z, r^{\prime} \circ r, o^{</em>}\right)$ about $z$ with $o^{<em>}$ as the answer. For Subject Aliasing, we use information maintained by WikiData to create a test query $\left(e^{\prime}, r, o^{</em>}\right)$ for every alias $e^{\prime}$ of $e$. Last, for Preservation we create test triplets $\left(e, r, o_{1}\right), \ldots,\left(e, r, o_{n}\right)$ that check if the model retained the original objects $\left{o_{1}, \ldots, o_{n}\right}$ in addition to the new edited object $o^{*}$.</p>
<p>Step 4: Phrasing in Natural Language At this point (Figure 3D), we have factual edit requests and their corresponding test queries. To use them as inputs to LMs, we convert them from tripletform to natural language (NL). To this end, we manually craft a template NL phrase per relation (this is feasible since we use a fixed set of relations), and use it to convert all the triplets with this relation. For instance, the template ' 'The date of birth of $\langle e\rangle$ is' ' converts triplets with the relation $r=$ Date of Birth and a subject entity $e$.</p>
<p>For the Preservation triplets generated for an edit $\left(e, r,\left{o_{1}, \ldots, o_{n}\right}\right) \rightarrow\left(e, r,\left{o_{1}, \ldots, o_{n}, o^{<em>}\right}\right)$, where $o^{</em>}$ is a new object added to a set of possibly multiple $(n \geq 0)$ objects, we form a single NL</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: right;">Recent</th>
<th style="text-align: right;">Random</th>
<th style="text-align: right;">Popular</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"># of factual edits</td>
<td style="text-align: right;">2,000</td>
<td style="text-align: right;">1,000</td>
<td style="text-align: right;">1,000</td>
</tr>
<tr>
<td style="text-align: left;"># of queries per edit</td>
<td style="text-align: right;">26.8</td>
<td style="text-align: right;">18.8</td>
<td style="text-align: right;">25.6</td>
</tr>
<tr>
<td style="text-align: left;"># of queries per criterion</td>
<td style="text-align: right;">5.24</td>
<td style="text-align: right;">3.1</td>
<td style="text-align: right;">4.2</td>
</tr>
<tr>
<td style="text-align: left;"># of LG queries</td>
<td style="text-align: right;">2.5</td>
<td style="text-align: right;">3.6</td>
<td style="text-align: right;">2.6</td>
</tr>
<tr>
<td style="text-align: left;"># of CI queries</td>
<td style="text-align: right;">11.7</td>
<td style="text-align: right;">4.7</td>
<td style="text-align: right;">6.1</td>
</tr>
<tr>
<td style="text-align: left;"># of CII queries</td>
<td style="text-align: right;">5.1</td>
<td style="text-align: right;">5.1</td>
<td style="text-align: right;">3.9</td>
</tr>
<tr>
<td style="text-align: left;"># of SA queries</td>
<td style="text-align: right;">1.8</td>
<td style="text-align: right;">1.3</td>
<td style="text-align: right;">4.7</td>
</tr>
<tr>
<td style="text-align: left;"># of PV queries</td>
<td style="text-align: right;">0.6</td>
<td style="text-align: right;">0.4</td>
<td style="text-align: right;">0.5</td>
</tr>
<tr>
<td style="text-align: left;"># of RS queries</td>
<td style="text-align: right;">5.1</td>
<td style="text-align: right;">3.7</td>
<td style="text-align: right;">7.8</td>
</tr>
<tr>
<td style="text-align: left;">Subject triplets count</td>
<td style="text-align: right;">31.7</td>
<td style="text-align: right;">13.3</td>
<td style="text-align: right;">115.2</td>
</tr>
<tr>
<td style="text-align: left;">Subject page back-links</td>
<td style="text-align: right;">278.1</td>
<td style="text-align: right;">121.6</td>
<td style="text-align: right;">3934.5</td>
</tr>
<tr>
<td style="text-align: left;">Subject page views</td>
<td style="text-align: right;">189.6</td>
<td style="text-align: right;">67.91</td>
<td style="text-align: right;">7376.5</td>
</tr>
<tr>
<td style="text-align: left;">Object triplets count</td>
<td style="text-align: right;">192.4</td>
<td style="text-align: right;">46.4</td>
<td style="text-align: right;">39.5</td>
</tr>
<tr>
<td style="text-align: left;">Object page back-links</td>
<td style="text-align: right;">18634.2</td>
<td style="text-align: right;">3065.0</td>
<td style="text-align: right;">2136.0</td>
</tr>
<tr>
<td style="text-align: left;">Object page views</td>
<td style="text-align: right;">2852.4</td>
<td style="text-align: right;">1379.7</td>
<td style="text-align: right;">1176.7</td>
</tr>
</tbody>
</table>
<p>Table 1: Statistics per subset of RippleEdits, showing the average of different metrics. Breakdown by evaluation criteria shows the number of queries of each criterion per edit. For a given subject/object entity, triplets count is the number of WikiData facts it is associated with, page back-links is the number of Wikipedia pages with a link to the entity's page, and page views is the recent average daily view count of the entity's page.
query about other objects than the edited one, e.g., ' 'The award received by <e> which is not <o*> is''.</p>
<h3>4.2 Data Statistics</h3>
<p>We used our data generation pipeline to collect edits for 2,000 Recent facts, 1,000 Random facts, and 1,000 Popular facts, focusing on $N_{r e l}=54$ basic relations for which we manually crafted NL templates and logical rules. ${ }^{4}$ To obtain the RANDOM subset, we set $N_{\text {ent }}=200$ to sample 200 facts from each entity group in WikiData. For edit generation of Random and Popular, we set $N_{\text {cand }}=100,000$. We call our diagnostic benchmark RippleEdits, and publicly release it to the research community. Notably, RippleEdits focuses on ripple edits and is meant to complement existing benchmarks, and so it does not include previous evaluations, such as subject specificity and model consistency.</p>
<p>Statistics on RippleEdits are presented in Table 1, showing that our generation process resulted in 18-26 test queries per edit and over 3 queries per evaluation test, on average. Moreover,</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Most frequent relations and their frequency, in each subset of RippleEdits.</p>
<p>Popular edits contain more popular subjects (as intended), while Recent edits have more popular objects. Figure 4 shows the top relations and their frequency in each subset of RippleEdits, demonstrating the diversity of the generated facts.</p>
<h3>4.3 Data Quality</h3>
<p>We conducted a manual analysis to validate that our generation pipeline produces valid test queries. Concretely, we sampled 200 random test queries from RippleEdits and checked the following two requirements: (a) soundness: the triplet that represents a given test query should be semantically correct, namely, the entity type of the object should match the relation type and the relation type should match the entity type of the subject. For example, queries such as "The capital of Hilary Clinton is" or "The sibling of Lebron James is Los Angeles" would have been disqualified. (b) grammatically correct: we check that the phrasing of the test query in natural language is grammatical.</p>
<p>We found that $100 \%$ of the queries were sound (i.e., semantically clear and correct), showing that the data curating process was designed properly. Furthermore, $98.5 \%$ of the queries were grammatically correct, while the ones which were not contain entity representations in a non-English language. This shows that our templates are general enough to properly fit various entity names.</p>
<h2>5 Experiments</h2>
<p>We use RippleEdits to evaluate recent KE methods, and show that despite substantial progress
on existing benchmarks, current methods struggle to introduce consistent changes to the model's knowledge after an edit. Moreover, a simple incontext editing baseline that conditions the generation on the edited fact obtains better results, while leaving ample room for improvement for future research.</p>
<h3>5.1 Evaluation Setting</h3>
<p>Data To evaluate how well an editing method handles the ripple effects resulting from editing a given model, the data first needs to be adjusted such that (a) only cases of successful edits are evaluated, and (b) only test queries that the model answered correctly pre-editing are used for evaluation. Concretely, for an editing method $\mathcal{F}$ and a model $\mathcal{M}$, an edit request $x:(e, r, o) \rightarrow\left(e, r, o^{\prime}\right)$ is included in the evaluation if the following conditions are met when applying $\mathcal{F}$ to $\mathcal{M}$ and $x$ : (a) $\mathcal{M}$ successfully generates the original objects for the test queries before applying the edit, and (b) $\mathcal{M}$ successfully generates $o^{\prime}$ when queried about $e$ and $r$, namely, the edit has successfully been applied. For example, we verify that the model can predict the children of $o^{\prime}$ before asking about $e$ 's new siblings.</p>
<p>Editing Methods We evaluate three KE methods: MEND (Mitchell et al., 2022), ROME (Meng et al., 2022), and MEMIT (Meng et al., 2023). MEND trains a network that modifies gradients to produce local edits. ROME makes rank-one updates to the weights of the Transformer's MLP layers to modify specific factual associations, and</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: An example modification edit from our ICE baseline. The color code of the KG is similar to that described in Figure 2. We prepend the prefix 'Imagine that" to the input prompt, as counterfactuals can contradict knowledge embedded in a model's parameters.</p>
<p>MEMIT is an extension of ROME that is adjusted to editing many facts at once.</p>
<p>Baseline Motivated by the recent success of LMs to learn in-context and follow instructions (Brown et al., 2020; Ouyang et al., 2022; Liu et al., 2023), specifically for knowledge editing (Zhong et al., 2023; Zheng et al., 2023), we experiment with an in-context editing (ICE) baseline for factual editing. Unlike the above methods, it does not introduce changes to the model parameters, but rather generation is conditioned on the new fact. Concretely, given an edit $(e, r, o) \rightarrow\left(e, r, o^{<em>}\right)$ and a test query $q$, we use the following prompt to obtain an answer from the model: ''Imagine that $\left\langle o^{</em>}\right\rangle$ would have been $\left\langle P_{r}\right\rangle^{\prime \prime}$, where $P_{r}$ is a manually-written proposition of $r$, such as 'The mother of $\langle e\rangle$ '' when $r=$ Mother and $e$ is the subject. An example is depicted in Figure 5.</p>
<p>Models We use 4 recent auto-regressive decoder-only LMs of different sizes: GPT-2 XL (Radford et al., 2019) with 1.5B parameters, GPT-J (Chen et al., 2021) with 6B parameters, LLaMA with 7B parameters, (Touvron et al., 2023), and GPT-NeoX with 20B parameters (Black et al., 2022). In addition, as our baseline does not require access to the model parameters, we also evaluate it on the closed-source model GPT-3</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Recent</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Random</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Popular</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Edits</td>
<td style="text-align: center;">Tests</td>
<td style="text-align: center;">Edits</td>
<td style="text-align: center;">Tests</td>
<td style="text-align: center;">Edits</td>
<td style="text-align: center;">Tests</td>
</tr>
<tr>
<td style="text-align: left;">GPT-2</td>
<td style="text-align: center;">853</td>
<td style="text-align: center;">$29 \%$</td>
<td style="text-align: center;">689</td>
<td style="text-align: center;">$33 \%$</td>
<td style="text-align: center;">722</td>
<td style="text-align: center;">$71 \%$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-J</td>
<td style="text-align: center;">801</td>
<td style="text-align: center;">$33 \%$</td>
<td style="text-align: center;">717</td>
<td style="text-align: center;">$34 \%$</td>
<td style="text-align: center;">760</td>
<td style="text-align: center;">$76 \%$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-NEO</td>
<td style="text-align: center;">989</td>
<td style="text-align: center;">$45 \%$</td>
<td style="text-align: center;">801</td>
<td style="text-align: center;">$46 \%$</td>
<td style="text-align: center;">828</td>
<td style="text-align: center;">$86 \%$</td>
</tr>
<tr>
<td style="text-align: left;">LLAMA</td>
<td style="text-align: center;">847</td>
<td style="text-align: center;">$44 \%$</td>
<td style="text-align: center;">796</td>
<td style="text-align: center;">$49 \%$</td>
<td style="text-align: center;">784</td>
<td style="text-align: center;">$87 \%$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3</td>
<td style="text-align: center;">822</td>
<td style="text-align: center;">$55 \%$</td>
<td style="text-align: center;">760</td>
<td style="text-align: center;">$74 \%$</td>
<td style="text-align: center;">665</td>
<td style="text-align: center;">$94 \%$</td>
</tr>
</tbody>
</table>
<p>Table 2: (a) Number of edits considered in our evaluation (i.e., that have successfully applied), from each subset, averaged over ROME, MEMIT, and MEND, for the models: GPT-2, GPT-J, GPT-NEO and LLAMA, and the ICE baseline for GPT-3. (b) Portion of queries, on average, that were used in our evaluation.
text-davinci-003 with 175B parameters (Brown et al., 2020). However, for the baseline we do not include results for GPT-2 and GPT-J as the number of testable edits for these models is rather small ( $\leq 20 \%$ for each of the data subsets).</p>
<p>For all model-method combinations, except for ROME with LLAMA, we use the official implementation and hyperparameters from Meng et al. (2022). We adjust ROME to LLAMA by following the authors' method and codebase. Table 2 shows the number of edits and test queries left, for every model, after filtering out non-successful edits and inapplicable test queries (as described above).</p>
<p>Evaluation Each model-method pair is evaluated separately, on every subset of RippleEdits. For each evaluation criterion, we first compute the average accuracy over the test queries per example, and then average over all the examples. For a given test query, we let the model generate a maximum of 20 tokens. A generation is considered successful if one of the aliases of the target object appears in the text. In cases of multiple gold target objects (as in Preservation), we evaluate each target object separately and consider the generation as correct if it matches at least one object.</p>
<h3>5.2 Results</h3>
<p>Tables 3, 4, 5 show the evaluation results on the Recent, Random, and Popular subsets, respectively. Considering the average scores across all subsets, we observe that existing editing methods struggle to handle the ripple effect induced by editing facts, with low average accuracy of 38-66 across all models. This suggests that, while</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;">LG</th>
<th style="text-align: left;">CI</th>
<th style="text-align: left;">CII</th>
<th style="text-align: left;">SA</th>
<th style="text-align: left;">PV</th>
<th style="text-align: left;">RS</th>
<th style="text-align: left;">Avg.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">ROME</td>
<td style="text-align: left;">20.2</td>
<td style="text-align: left;">35.6</td>
<td style="text-align: left;">46.8</td>
<td style="text-align: left;">86.8</td>
<td style="text-align: left;">100</td>
<td style="text-align: left;">55.4</td>
<td style="text-align: left;">57.5</td>
</tr>
<tr>
<td style="text-align: left;">GPT-2</td>
<td style="text-align: left;">MEMIT</td>
<td style="text-align: left;">21.8</td>
<td style="text-align: left;">30.3</td>
<td style="text-align: left;">46.2</td>
<td style="text-align: left;">92.9</td>
<td style="text-align: left;">100</td>
<td style="text-align: left;">56.8</td>
<td style="text-align: left;">58.0</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">MEND</td>
<td style="text-align: left;">28.9</td>
<td style="text-align: left;">23.7</td>
<td style="text-align: left;">20.7</td>
<td style="text-align: left;">87.1</td>
<td style="text-align: left;">100</td>
<td style="text-align: left;">51.9</td>
<td style="text-align: left;">52.1</td>
</tr>
<tr>
<td style="text-align: left;">GPT-J</td>
<td style="text-align: left;">ROME</td>
<td style="text-align: left;">15.2</td>
<td style="text-align: left;">29.5</td>
<td style="text-align: left;">50.5</td>
<td style="text-align: left;">90.3</td>
<td style="text-align: left;">99.4</td>
<td style="text-align: left;">60.0</td>
<td style="text-align: left;">57.5</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">MEMIT</td>
<td style="text-align: left;">18.0</td>
<td style="text-align: left;">35.0</td>
<td style="text-align: left;">48.1</td>
<td style="text-align: left;">88.4</td>
<td style="text-align: left;">98.6</td>
<td style="text-align: left;">42.2</td>
<td style="text-align: left;">55.0</td>
</tr>
<tr>
<td style="text-align: left;">GPT-NEO</td>
<td style="text-align: left;">ROME</td>
<td style="text-align: left;">27.2</td>
<td style="text-align: left;">54.3</td>
<td style="text-align: left;">69.4</td>
<td style="text-align: left;">98.9</td>
<td style="text-align: left;">98.4</td>
<td style="text-align: left;">80.3</td>
<td style="text-align: left;">71.4</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">ICE</td>
<td style="text-align: left;">48.3</td>
<td style="text-align: left;">29.0</td>
<td style="text-align: left;">62.2</td>
<td style="text-align: left;">100</td>
<td style="text-align: left;">99.4</td>
<td style="text-align: left;">80.7</td>
<td style="text-align: left;">69.9</td>
</tr>
<tr>
<td style="text-align: left;">LLAMA</td>
<td style="text-align: left;">ROME</td>
<td style="text-align: left;">16.7</td>
<td style="text-align: left;">47.8</td>
<td style="text-align: left;">50.0</td>
<td style="text-align: left;">93.6</td>
<td style="text-align: left;">97.6</td>
<td style="text-align: left;">59.3</td>
<td style="text-align: left;">60.8</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">ICE</td>
<td style="text-align: left;">59.6</td>
<td style="text-align: left;">74.8</td>
<td style="text-align: left;">85.0</td>
<td style="text-align: left;">100</td>
<td style="text-align: left;">99.5</td>
<td style="text-align: left;">77.9</td>
<td style="text-align: left;">82.8</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3</td>
<td style="text-align: left;">ICE</td>
<td style="text-align: left;">33.3</td>
<td style="text-align: left;">100</td>
<td style="text-align: left;">91.3</td>
<td style="text-align: left;">100</td>
<td style="text-align: left;">100</td>
<td style="text-align: left;">73.1</td>
<td style="text-align: left;">82.8</td>
</tr>
</tbody>
</table>
<p>Table 3: Accuracy on the Recent subset, by MEND, ROME, MEMIT, and the ICE baseline, on GPT-2, GPT-J, GPT-NEO, LLAMA, and GPT-3.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;">LG</th>
<th style="text-align: left;">CI</th>
<th style="text-align: left;">CII</th>
<th style="text-align: left;">SA</th>
<th style="text-align: left;">PV</th>
<th style="text-align: left;">RS</th>
<th style="text-align: left;">Avg.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">ROME</td>
<td style="text-align: left;">53.6</td>
<td style="text-align: left;">31.6</td>
<td style="text-align: left;">44.4</td>
<td style="text-align: left;">94.9</td>
<td style="text-align: left;">9.9</td>
<td style="text-align: left;">38.9</td>
<td style="text-align: left;">45.5</td>
</tr>
<tr>
<td style="text-align: left;">GPT-2</td>
<td style="text-align: left;">MEMIT</td>
<td style="text-align: left;">58.4</td>
<td style="text-align: left;">30.5</td>
<td style="text-align: left;">49.8</td>
<td style="text-align: left;">100</td>
<td style="text-align: left;">20.0</td>
<td style="text-align: left;">36.2</td>
<td style="text-align: left;">49.1</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">MEND</td>
<td style="text-align: left;">62.5</td>
<td style="text-align: left;">16.7</td>
<td style="text-align: left;">14.6</td>
<td style="text-align: left;">91.3</td>
<td style="text-align: left;">17.7</td>
<td style="text-align: left;">30.1</td>
<td style="text-align: left;">38.8</td>
</tr>
<tr>
<td style="text-align: left;">GPT-J</td>
<td style="text-align: left;">ROME</td>
<td style="text-align: left;">53.8</td>
<td style="text-align: left;">40.8</td>
<td style="text-align: left;">49.9</td>
<td style="text-align: left;">93.8</td>
<td style="text-align: left;">15.2</td>
<td style="text-align: left;">39.4</td>
<td style="text-align: left;">48.8</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">MEMIT</td>
<td style="text-align: left;">53.0</td>
<td style="text-align: left;">35.7</td>
<td style="text-align: left;">48.2</td>
<td style="text-align: left;">95.6</td>
<td style="text-align: left;">18.2</td>
<td style="text-align: left;">39.9</td>
<td style="text-align: left;">48.4</td>
</tr>
<tr>
<td style="text-align: left;">GPT-NEO</td>
<td style="text-align: left;">ROME</td>
<td style="text-align: left;">61.6</td>
<td style="text-align: left;">49.4</td>
<td style="text-align: left;">57.1</td>
<td style="text-align: left;">100</td>
<td style="text-align: left;">30.8</td>
<td style="text-align: left;">50.7</td>
<td style="text-align: left;">58.3</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">ICE</td>
<td style="text-align: left;">78.6</td>
<td style="text-align: left;">90.0</td>
<td style="text-align: left;">55.6</td>
<td style="text-align: left;">100</td>
<td style="text-align: left;">100</td>
<td style="text-align: left;">61.9</td>
<td style="text-align: left;">81.0</td>
</tr>
<tr>
<td style="text-align: left;">LLAMA</td>
<td style="text-align: left;">ROME</td>
<td style="text-align: left;">54.3</td>
<td style="text-align: left;">35.5</td>
<td style="text-align: left;">49.5</td>
<td style="text-align: left;">96.0</td>
<td style="text-align: left;">17.8</td>
<td style="text-align: left;">38.9</td>
<td style="text-align: left;">48.7</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">ICE</td>
<td style="text-align: left;">71.1</td>
<td style="text-align: left;">73.8</td>
<td style="text-align: left;">80.3</td>
<td style="text-align: left;">100</td>
<td style="text-align: left;">100</td>
<td style="text-align: left;">69.6</td>
<td style="text-align: left;">82.5</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3</td>
<td style="text-align: left;">ICE</td>
<td style="text-align: left;">69.0</td>
<td style="text-align: left;">83.3</td>
<td style="text-align: left;">89.7</td>
<td style="text-align: left;">100</td>
<td style="text-align: left;">100</td>
<td style="text-align: left;">100</td>
<td style="text-align: left;">90.3</td>
</tr>
</tbody>
</table>
<p>Table 4: Accuracy on the Random subset, by MEND, ROME, MEMIT, and the ICE baseline, on GPT-2, GPT-J, GPT-NEO, LLAMA, and GPT-3.</p>
<p>KE methods demonstrate high capability in making local updates to the model's knowledge, these changes are mostly applied at a surface-level without propagating to other related facts. Moreover, we observe that our ICE baseline obtains the best overall results. Specifically, it outperforms ROME by more than 10 points for GPT-NEO and 29 points for LLAMA, on average across subsets. Although GPT-3 with ICE performs best on average, the 7B LLAMA is highly competitive, performing better or similarly on the Recent and Popular subsets.</p>
<p>Next, comparing results across evaluation criteria shows that some ripple effects are handled better than others. For example, while Subject Aliasing accuracy is consistently high ( $\geq 86.8$ across all settings), the accuracy on other criteria is generally lower and varies greatly between models, methods, and edits (e.g., Logical Generalization accuracy for ROME on GPT-J is 53.8 on</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;">LG</th>
<th style="text-align: left;">CI</th>
<th style="text-align: left;">CII</th>
<th style="text-align: left;">SA</th>
<th style="text-align: left;">PV</th>
<th style="text-align: left;">RS</th>
<th style="text-align: left;">Avg.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">ROME</td>
<td style="text-align: left;">5.7</td>
<td style="text-align: left;">46.4</td>
<td style="text-align: left;">21.8</td>
<td style="text-align: left;">100</td>
<td style="text-align: left;">100</td>
<td style="text-align: left;">18.5</td>
<td style="text-align: left;">48.7</td>
</tr>
<tr>
<td style="text-align: left;">GPT-2</td>
<td style="text-align: left;">MEMIT</td>
<td style="text-align: left;">6.7</td>
<td style="text-align: left;">45.2</td>
<td style="text-align: left;">21.2</td>
<td style="text-align: left;">100</td>
<td style="text-align: left;">100</td>
<td style="text-align: left;">24.3</td>
<td style="text-align: left;">49.6</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">MEND</td>
<td style="text-align: left;">25.9</td>
<td style="text-align: left;">10.7</td>
<td style="text-align: left;">5.4</td>
<td style="text-align: left;">100</td>
<td style="text-align: left;">100</td>
<td style="text-align: left;">21.2</td>
<td style="text-align: left;">43.9</td>
</tr>
<tr>
<td style="text-align: left;">GPT-J</td>
<td style="text-align: left;">ROME</td>
<td style="text-align: left;">5.5</td>
<td style="text-align: left;">44.1</td>
<td style="text-align: left;">21.0</td>
<td style="text-align: left;">98.6</td>
<td style="text-align: left;">99.0</td>
<td style="text-align: left;">22.3</td>
<td style="text-align: left;">48.4</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">MEMIT</td>
<td style="text-align: left;">7.0</td>
<td style="text-align: left;">45.9</td>
<td style="text-align: left;">23.7</td>
<td style="text-align: left;">100</td>
<td style="text-align: left;">100</td>
<td style="text-align: left;">24.8</td>
<td style="text-align: left;">50.2</td>
</tr>
<tr>
<td style="text-align: left;">GPT-NEO</td>
<td style="text-align: left;">ROME</td>
<td style="text-align: left;">36.4</td>
<td style="text-align: left;">29.4</td>
<td style="text-align: left;">41.6</td>
<td style="text-align: left;">100</td>
<td style="text-align: left;">100</td>
<td style="text-align: left;">50.8</td>
<td style="text-align: left;">59.7</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">ICE</td>
<td style="text-align: left;">37.5</td>
<td style="text-align: left;">92.4</td>
<td style="text-align: left;">40.1</td>
<td style="text-align: left;">100</td>
<td style="text-align: left;">100</td>
<td style="text-align: left;">74.4</td>
<td style="text-align: left;">74.1</td>
</tr>
<tr>
<td style="text-align: left;">LLAMA</td>
<td style="text-align: left;">ROME</td>
<td style="text-align: left;">22.0</td>
<td style="text-align: left;">37.4</td>
<td style="text-align: left;">16.2</td>
<td style="text-align: left;">100</td>
<td style="text-align: left;">100</td>
<td style="text-align: left;">20.6</td>
<td style="text-align: left;">49.4</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">ICE</td>
<td style="text-align: left;">57.2</td>
<td style="text-align: left;">85.1</td>
<td style="text-align: left;">67.6</td>
<td style="text-align: left;">100</td>
<td style="text-align: left;">100</td>
<td style="text-align: left;">78.0</td>
<td style="text-align: left;">81.3</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3</td>
<td style="text-align: left;">ICE</td>
<td style="text-align: left;">31.0</td>
<td style="text-align: left;">86.1</td>
<td style="text-align: left;">65.6</td>
<td style="text-align: left;">100</td>
<td style="text-align: left;">100</td>
<td style="text-align: left;">83.8</td>
<td style="text-align: left;">77.7</td>
</tr>
</tbody>
</table>
<p>Table 5: Accuracy on the Popular subset, by MEND, ROME, MEMIT, and the ICE baseline, on GPT-2, GPT-J, GPT-NEO, LLAMA, and GPT-3.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Accuracy averaged over evaluation criteria of ROME, as a function of the model's number of parameters, for the following models: GPT2-M, GPT2-L, GPT2-XL, GPT-J, LLAMA, and GPT-NEO.
the Random subset, compared to only 5.5 on the Popular subset).</p>
<p>Results Across Model Size We analyze how editing performance on RippleEdits is influenced by the model size. To this end, we further evaluate ROME on smaller versions of GPT-2 - with 345M (GPT2-M) and 762M (GPT2-L) parameters, and plot the average accuracy over the three subsets as a function of model size. Figure 6 presents the results, showing that editing performance increases with model size, with ROME obtaining substantially higher accuracy when applied to larger models. Nevertheless, our results (Tables 3, 4, 5) show that when using ICE, the 7B LLAMA is competitive with the much larger GPT-3, suggesting that simply scaling the model size may not be sufficient to fix the drawbacks of current editing methods.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">MEND</th>
<th style="text-align: center;">ROME</th>
<th style="text-align: center;">MEMIT</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Relation Specificity</td>
<td style="text-align: center;">34.4</td>
<td style="text-align: center;">37.6</td>
<td style="text-align: center;">39.1</td>
</tr>
<tr>
<td style="text-align: left;">Logical Generalization</td>
<td style="text-align: center;">39.1</td>
<td style="text-align: center;">26.5</td>
<td style="text-align: center;">29.0</td>
</tr>
<tr>
<td style="text-align: left;">Compositionality I</td>
<td style="text-align: center;">17.0</td>
<td style="text-align: center;">37.9</td>
<td style="text-align: center;">35.3</td>
</tr>
<tr>
<td style="text-align: left;">Compositionality II</td>
<td style="text-align: center;">13.6</td>
<td style="text-align: center;">37.7</td>
<td style="text-align: center;">39.1</td>
</tr>
</tbody>
</table>
<p>Table 6: Accuracy of MEND, ROME, and MEMIT, using GPT-2, averaged over the three RippleEdits splits - Recent, Random and Popular.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: The average accuracy of GPT-2 on different evaluation criteria in RippleEdits. Results are averaged over editing methods (ROME, MEMIT, and MEND); error bars indicate standard deviation.</p>
<p>Results Across Methods Table 6 shows the accuracy of MEND, ROME, and MEMIT, on GPT-2 across our evaluation criteria, averaged over the three subsets. Interestingly, MEND outperforms ROME and MEMIT in Logical Generalization, but is worse in Compositionality I and Compositionality II, suggesting that different methods might better capture different types of ripple effects.</p>
<p>Results Across Data Splits The subsets of RippleEdits differ in whether edited facts are counterfeit or real, and in the popularity of the edited entities. These differences allow us to control for the edit severity, as popular entities are expected to introduce larger ripple effects (see §3). In Figure 7, we show the accuracy on each subset and evaluation criterion, averaged over the different editing methods. Comparing Random and Popular, that differ in the popularity of the edited entities, we see that while Logical Generalization accuracy is substantially higher for Random, Preservation accuracy is higher for Popular. This suggests that, although retaining correct knowledge is easier for popular entities, modifying other facts that logically follow from an edit is harder for popular entities, which could be explained by the severity</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: center;">No effect</th>
<th style="text-align: center;">Abstaining</th>
<th style="text-align: center;">Noise</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT-2</td>
<td style="text-align: left;">ROME</td>
<td style="text-align: center;">$27 \%$</td>
<td style="text-align: center;">$31 \%$</td>
<td style="text-align: center;">$42 \%$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">ICE</td>
<td style="text-align: center;">$32 \%$</td>
<td style="text-align: center;">$27 \%$</td>
<td style="text-align: center;">$41 \%$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-Neo</td>
<td style="text-align: left;">ROME</td>
<td style="text-align: center;">$24 \%$</td>
<td style="text-align: center;">$40 \%$</td>
<td style="text-align: center;">$36 \%$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">ICE</td>
<td style="text-align: center;">$10 \%$</td>
<td style="text-align: center;">$65 \%$</td>
<td style="text-align: center;">$25 \%$</td>
</tr>
<tr>
<td style="text-align: left;">LLAMA</td>
<td style="text-align: left;">ROME</td>
<td style="text-align: center;">$20.5 \%$</td>
<td style="text-align: center;">$45 \%$</td>
<td style="text-align: center;">$34.5 \%$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">ICE</td>
<td style="text-align: center;">$11 \%$</td>
<td style="text-align: center;">$71 \%$</td>
<td style="text-align: center;">$18 \%$</td>
</tr>
</tbody>
</table>
<p>Table 7: Error type distribution on 200 failures of ROME and ICE, on GPT-2, GPT-NEO, and LLAMA.
of these edits (i.e., the high number of facts that are semantically related to them).</p>
<h3>5.3 Error Analysis</h3>
<p>ROME versus ICE We qualitatively analyze the effect induced by KE methods to the model's knowledge. To this end, for each of ROME and our ICE baseline and each of the models GPT-2, GPT-NEO, and LLAMA, we sample 200 test queries from RippleEdits on which the model fails post-editing. We then label these failures using three categories: (a) no effect, for cases when the model predicts the original object, i.e., the edit introduced no ripple effect, (b) abstaining, when the model abstains from answering by generating text like 'unknown' or 'a mystery', and (c) noise, when the model generates an incorrect object or unrelated text. Table 7 presents the results, showing that in most cases ( $\geq 68 \%$ across all settings) factual editing introduces erroneous changes to the model's knowledge rather than making no change. Interestingly, for both GPT-NEO and LLAMA, where editing performance is better than GPT-2, ROME introduces more incorrect changes while ICE causes the model to abstain from answering.</p>
<p>GPT-3 versus LLAMA using ICE We further looked into the performance on the LG tests, where applying ICE to GPT-3 is notably inferior to ICE on LLAMA (see Tables 3, 4, 5). Specifically, we collected responses from each of the models to 100 random LG queries, and analyzed them using the same categories as described above. We observed that GPT-3 abstains from answering the query much more often than LLAMA ( $49 \%$ of the cases for GPT-3 compared to only $28 \%$ in LLAMA), which could explain the lower performance of ICE on GPT-3 on these queries.</p>
<h2>6 Conclusion and Discussion</h2>
<p>We introduce the notion of ripple effects in knowledge editing, suggesting that editing a particular fact implies further updates of related facts. We additionally propose evaluation criteria for ripple effects and create RippleEdits, a diagnostic benchmark designed to evaluate how well KE methods handle the ripple effects of various edits. We evaluate prominent KE methods and show that they often fail to introduce consistent edits that capture the ripple effects of an edit, suggesting that future development of KE methods should consider those effects more carefully. Last, we show that a simple in-context editing method achieves the best results on RippleEdits, highlighting the potential of such editing approaches.</p>
<p>Notably, our benchmark covers a small fraction of all possible ripple-edits. For example, one could consider ripple effects that involve more than two hops, and explore the graph structure of different edits. In addition, while we focus on ripple effects of single edits, future work can consider the effect of editing multiple facts in a single batch. Finally, it would be interesting to consider cases where models succeed in capturing ripple-edits, and analyze how these are implemented mechanistically in the transformer architecture (Geva et al., 2023).</p>
<p>Limitations Our data generation pipeline relies on information from an existing knowledge-base (WikiData in our case), which could be incomplete or outdated. While RippleEdits does not aim to cover all the possible ripple-edits in WikiData, these concerns might be a major issue when seeking a comprehensive evaluation or considering domain-specific knowledge-bases, which often tend to be incomplete. A possible solution to explore in that case is to use LMs internal knowledge instead of an external knowledge-base (Cohen et al., 2023).</p>
<p>With RippleEdits focusing on the ripple effect of edits, it does not include tests, such as paraphrasing of the edit and subject specificity, that evaluate the edit itself and are covered by existing benchmarks (e.g., CounterFact). In addition, it does not verify that many other facts that are distantly related to the edit, i.e., triplets that are not included in the close neighbourhood of the edit, were retained post-editing. For example, we expect that editing the capital of France would not affect the population of Poland, yet this is not
explicitly checked. We note that building such an evaluation is hard, since there are many facts to consider and it is unclear how to determine automatically which triplets should and should not be affected by a certain edit.</p>
<h2>Acknowledgments</h2>
<p>We thank Maor Ivgi and Gal Elidan for valuable feedback and constructive suggestions. This work is supported in part by the Israeli Science Foundation.</p>
<h2>References</h2>
<p>Sidney Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, Usvsn Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach. 2022. GPT-NeoX-20B: An opensource autoregressive language model. In Proceedings of BigScience Episode #5 - Workshop on Challenges \&amp; Perspectives in Creating Large Language Models, pages 95-136, virtual+Dublin. Association for Computational Linguistics. https://doi.org/10 .18653/v1/2022.big Science-1.9</p>
<p>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy</p>
<p>Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. Evaluating large language models trained on code. ArXiv preprint, abs/2107.03374. https://arxiv.org/abs /2107.03374</p>
<p>Roi Cohen, Mor Geva, Jonathan Berant, and Amir Globerson. 2023. Crawling the internal knowledge-base of language models. In Findings of the Association for Computational Linguistics: EACL 2023, pages 1856-1869, Dubrovnik, Croatia. Association for Computational Linguistics. https://doi.org/10 .18653/v1/2023.findings-eacl. 139</p>
<p>Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and Furu Wei. 2022. Knowledge neurons in pretrained transformers. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8493-8502, Dublin, Ireland. Association for Computational Linguistics. https://doi.org/10 .18653/v1/2022.acl-long. 581</p>
<p>Nicola De Cao, Wilker Aziz, and Ivan Titov. 2021. Editing factual knowledge in language models. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6491-6506, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. https://doi.org/10 .18653/v1/2021.emnlp-main. 522</p>
<p>Bhuwan Dhingra, Jeremy R. Cole, Julian Martin Eisenschlos, Daniel Gillick, Jacob Eisenstein, and William W. Cohen. 2022. Time-aware language models as temporal knowledge bases. Transactions of the Association for Compu-
tational Linguistics, 10:257-273. https:// doi.org/10.1162/tacl_a_00459</p>
<p>Konstantin Genin and Franz Huber. 2022. Formal representations of belief. In Edward N. Zalta and Uri Nodelman, editors, The Stanford Encyclopedia of Philosophy, Fall 2022 edition. Metaphysics Research Lab, Stanford University.</p>
<p>Mor Geva, Jasmijn Bastings, Katja Filippova, and Amir Globerson. 2023. Dissecting recall of factual associations in auto-regressive language models. arXiv preprint arXiv:2304.14767. https://doi.org/10.18653/v1/2023 .emnlp-main. 751</p>
<p>Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. 2021. Transformer feed-forward layers are key-value memories. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 5484-5495, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. https://doi.org/10 .18653/v1/2021.emnlp-main. 446</p>
<p>Anshita Gupta, Debanjan Mondal, Akshay Krishna Sheshadri, Wenlong Zhao, Xiang Lorraine Li, Sarah Wiegreffe, and Niket Tandon. 2023. Editing commonsense knowledge in gpt. https://arxiv.org/abs /2305.14956</p>
<p>Peter Hase, Mona Diab, Asli Celikyilmaz, Xian Li, Zornitsa Kozareva, Veselin Stoyanov, Mohit Bansal, and Srinivasan Iyer. 2023. Methods for measuring, updating, and visualizing factual beliefs in language models. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 2714-2731, Dubrovnik, Croatia. Association for Computational Linguistics. https://doi.org/10 .18653/v1/2023.eacl-main. 199</p>
<p>Benjamin Heinzerling and Kentaro Inui. 2021. Language models as knowledge bases: On entity representations, storage capacity, and paraphrased queries. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 1772-1791, Online. Association for Computational Linguistics. https://doi.org/10.18653/v1 /2021.eacl-main. 153</p>
<p>Evan Hernandez, Belinda Z. Li, and Jacob Andreas. 2023a. Inspecting and editing knowledge representations in language models. https://arxiv.org/abs/2304.00740</p>
<p>Evan Hernandez, Belinda Z. Li, and Jacob Andreas. 2023b. Measuring and manipulating knowledge representations in language models. ArXiv preprint, abs/2304.00740.</p>
<p>Jason Hoelscher-Obermaier, Julia Persson, Esben Kran, Ioannis Konstas, and Fazl Barez. 2023. Detecting edit failures in large language models: An improved specificity benchmark. In Findings of the Association for Computational Linguistics: ACL 2023, pages 11548-11559, Toronto, Canada. Association for Computational Linguistics. https://doi.org/10 .18653/v1/2023.findings-acl. 733</p>
<p>Joel Jang, Seonghyeon Ye, Sohee Yang, Joongbo Shin, Janghoon Han, Gyeonghun Kim, Stanley Jungkyu Choi, and Minjoon Seo. 2022. Towards continual knowledge learning of language models. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net.</p>
<p>Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield Dodds, Nova DasSarma, Eli Tran-Johnson, Scott Johnston, Sheer El-Showk, Andy Jones, Nelson Elhage, Tristan Hume, Anna Chen, Yuntao Bai, Sam Bowman, Stanislav Fort, Deep Ganguli, Danny Hernandez, Josh Jacobson, Jackson Kernion, Shauna Kravec, Liane Lovitt, Kamal Ndousse, Catherine Olsson, Sam Ringer, Dario Amodei, Tom Brown, Jack Clark, Nicholas Joseph, Ben Mann, Sam McCandlish, Chris Olah, and Jared Kaplan. 2022. Language models (mostly) know what they know. ArXiv preprint, abs/2207.05221. https://arxiv .org/abs/2207.05221</p>
<p>Nora Kassner, Oyvind Tafjord, Hinrich Schütze, and Peter Clark. 2021. BeliefBank: Adding memory to a pre-trained language model for a systematic notion of belief. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 8849-8861, Online and Punta Cana,</p>
<p>Dominican Republic. Association for Computational Linguistics. https://doi.org/10 .18653/v1/2021.emnlp-main. 697</p>
<p>Angeliki Lazaridou, Adhiguna Kuncoro, Elena Gribovskaya, Devang Agrawal, Adam Liska, Tayfun Terzi, Mai Gimenez, Cyprien de Masson d'Autume, Tomás Kociský, Sebastian Ruder, Dani Yogatama, Kris Cao, Susannah Young, and Phil Blunsom. 2021. Mind the gap: Assessing temporal generalization in neural language models. In Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pages 29348-29363.</p>
<p>Omer Levy, Minjoon Seo, Eunsol Choi, and Luke Zettlemoyer. 2017. Zero-shot relation extraction via reading comprehension. In Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 333-342, Vancouver, Canada. Association for Computational Linguistics. https:// doi.org/10.18653/v1/K17-1034</p>
<p>Patrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2020. Retrievalaugmented generation for knowledge-intensive NLP tasks. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.</p>
<p>Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2023. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. ACM Computing Surveys, 55(9):1-35. https://doi.org/10.1145 /3560815</p>
<p>Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. 2023. When not to trust language models: Investigating effectiveness of parametric and non-parametric memories. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 9802-9822,</p>
<p>Toronto, Canada. Association for Computational Linguistics. https://doi.org/10 .18653/v1/2023.acl-long. 546</p>
<p>Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. 2022. Locating and editing factual associations in gpt. Advances in Neural Information Processing Systems, 35:17359-17372.</p>
<p>Kevin Meng, Arnab Sen Sharma, Alex J. Andonian, Yonatan Belinkov, and David Bau. 2023. Mass-editing memory in a transformer. In The Eleventh International Conference on Learning Representations.</p>
<p>Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, and Christopher D. Manning. 2022. Fast model editing at scale. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net.</p>
<p>Yasumasa Onoe, Michael Zhang, Shankar Padmanabhan, Greg Durrett, and Eunsol Choi. 2023. Can LMs learn new entities from descriptions? Challenges in propagating injected knowledge. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 5469-5485, Toronto, Canada. Association for Computational Linguistics. https://doi.org/10.18653 /v1/2023.acl-long. 300</p>
<p>Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. https://arxiv.org/abs /2203.02155</p>
<p>Matthew E. Peters, Mark Neumann, Robert Logan, Roy Schwartz, Vidur Joshi, Sameer Singh, and Noah A. Smith. 2019. Knowledge enhanced contextual word representations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),
pages 43-54, Hong Kong, China. Association for Computational Linguistics. https:// doi.org/10.18653/v1/D19-1005</p>
<p>Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. 2019. Language models as knowledge bases? In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2463-2473, Hong Kong, China. Association for Computational Linguistics. https://doi.org/10 .18653/v1/D19-1250</p>
<p>Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9.
Simon Razniewski, Andrew Yates, Nora Kassner, and Gerhard Weikum. 2021. Language models as or for knowledge bases. ArXiv preprint, abs/2110.04888. https://arxiv.org/abs /2110.04888</p>
<p>Adam Roberts, Colin Raffel, and Noam Shazeer. 2020. How much knowledge can you pack into the parameters of a language model? In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 5418-5426, Online. Association for Computational Linguistics. https://doi.org/10.18653/v1 /2020.emnlp-main. 437
Taylor Shin, Yasaman Razeghi, Robert L. Logan IV, Eric Wallace, and Sameer Singh. 2020. AutoPrompt: Eliciting knowledge from language models with automatically generated prompts. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4222-4235, Online. Association for Computational Linguistics. https://doi.org/10.18653/v1 /2020.emnlp-main. 346
Chenglei Si, Zhe Gan, Zhengyuan Yang, Shuohang Wang, Jianfeng Wang, Jordan Lee Boyd-Graber, and Lijuan Wang. 2023. Prompting GPT-3 to be reliable. In The Eleventh International Conference on Learning Representations.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux,</p>
<p>Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. Llama: Open and efficient foundation language models. ArXiv preprint, abs/2302.13971. https://arxiv .org/abs/2302.13971
Ruize Wang, Duyu Tang, Nan Duan, Zhongyu Wei, Xuanjing Huang, Jianshu Ji, Guihong Cao, Daxin Jiang, and Ming Zhou. 2021a. K-Adapter: Infusing knowledge into pre-trained models with adapters. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 1405-1418, Online. Association for Computational Linguistics. https://doi.org/10.18653/v1 /2021.findings-acl. 121
Xiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhengyan Zhang, Zhiyuan Liu, Juanzi Li, and Jian Tang. 2021b. KEPLER: A unified model for knowledge embedding and pre-trained language representation. Transactions of the Association for Computational Linguistics, 9:176-194. https://doi.org /10.1162/tacl_a_00360
Yunzhi Yao, Shaohan Huang, Li Dong, Furu Wei, Huajun Chen, and Ningyu Zhang. 2022. Kformer: Knowledge injection in transformer feed-forward layers. In Natural Language Processing and Chinese Computing, pages 131-143, Cham. Springer International Publishing. https://doi.org/10.1007 /978-3-031-17120-8_11
Yunzhi Yao, Peng Wang, Bozhong Tian, Siyuan Cheng, Zhoubo Li, Shumin Deng, Huajun Chen, and Ningyu Zhang. 2023. Editing large language models: Problems, methods, and opportunities. https://arxiv.org/abs /2305.13172</p>
<p>Ningyu Zhang, Shumin Deng, Xu Cheng, Xi Chen, Yichi Zhang, Wei Zhang, and Huajun Chen. 2021. Drop redundant, shrink irrelevant: Selective knowledge injection for language pretraining. In Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21, pages 4007-4014. International Joint Conferences on Artificial Intelligence Organization. https://doi.org /10.24963/ijcai.2021/552
Xikun Zhang, Antoine Bosselut, Michihiro Yasunaga, Hongyu Ren, Percy Liang, Christopher D. Manning, and Jure Leskovec. 2022. Greaselm: Graph reasoning enhanced language models. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net.</p>
<p>Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, and Qun Liu. 2019. ERNIE: Enhanced language representation with informative entities. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1441-1451, Florence, Italy. Association for Computational Linguistics. https://doi.org/10 .18653/v1/P19-1139
Ce Zheng, Lei Li, Qingxiu Dong, Yuxuan Fan, Zhiyong Wu, Jingjing Xu, and Baobao Chang. 2023. Can we edit factual knowledge by in-context learning? https://doi.org/10 .18653/v1/2023.emnlp-main. 296
Zexuan Zhong, Zhengxuan Wu, Christopher D. Manning, Christopher Potts, and Danqi Chen. 2023. Mquake: Assessing knowledge editing in language models via multi-hop questions. https://doi.org/10.18653/v1 /2023.emnlp-main. 971</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{4}$ We release the templates and rules in our codebase.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
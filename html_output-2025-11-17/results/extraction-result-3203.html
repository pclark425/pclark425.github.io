<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3203 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3203</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3203</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-74.html">extraction-schema-74</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-73290ecbec2f38d1d647ddef1ada69cee41725b3</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/73290ecbec2f38d1d647ddef1ada69cee41725b3" target="_blank">PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> Positional Skip-wisE (PoSE) training that smartly simulates long inputs using a fixed context window is proposed, and can potentially support infinite length, limited only by memory usage in inference.</p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) are trained with a pre-defined context length, restricting their use in scenarios requiring long inputs. Previous efforts for adapting LLMs to a longer length usually requires fine-tuning with this target length (Full-length fine-tuning), suffering intensive training cost. To decouple train length from target length for efficient context window extension, we propose Positional Skip-wisE (PoSE) training that smartly simulates long inputs using a fixed context window. This is achieved by first dividing the original context window into several chunks, then designing distinct skipping bias terms to manipulate the position indices of each chunk. These bias terms and the lengths of each chunk are altered for every training example, allowing the model to adapt to all positions within target length. Experimental results show that PoSE greatly reduces memory and time overhead compared with Full-length fine-tuning, with minimal impact on performance. Leveraging this advantage, we have successfully extended the LLaMA model to 128k tokens using a 2k training context window. Furthermore, we empirically confirm that PoSE is compatible with all RoPE-based LLMs and position interpolation strategies. Notably, our method can potentially support infinite length, limited only by memory usage in inference. With ongoing progress for efficient inference, we believe PoSE can further scale the context window beyond 128k.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3203.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3203.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PoSE-LLaMA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA fine-tuned via Positional Skip-wisE (PoSE) training</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A LLaMA-7B model fine-tuned using Positional Skip-wisE (PoSE), which simulates long inputs by partitioning the training context into chunks and adding sampled skipping bias terms to their position indices to extend effective context at inference without full-length fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LLaMA-7B (PoSE-extended)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Decoder-only Transformer (LLaMA-7B) with original RoPE position encoding; weights are fine-tuned using PoSE to adapt to much larger target context windows (e.g., 16k, 32k, up to 128k) while training still uses the original short context (2k).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>context window extension</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>PoSE manipulates RoPE position indices during fine-tuning by dividing the training context into N chunks (N=2 by default), sampling chunk lengths and non-overlapping skipping bias terms u_i to shift chunk position indices into the target range, and applying position interpolation; this enables the model to attend across a much larger effective context at inference (i.e., use the extended context as memory) without training on full-length inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Passkey retrieval (and long-context language modeling)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Passkey retrieval: recover a randomly placed short secret (five-digit passkey) embedded in a long document; tests whether a model can effectively attend to and recall tokens placed far away in the input. Also evaluated on long-document language modeling (GovReport, Proof-pile) to measure general long-range modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>retrieval / long-range language modeling</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Passkey retrieval: >=90% retrieval accuracy within the model's target context window (PoSE-16k and PoSE-32k models maintain >=90% accuracy within their respective context windows). Language modeling (GovReport): example perplexity at 16k = 4.60 PPL; (Proof-pile) 16k = 2.60 PPL.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Original LLaMA (2k context) retrieval accuracy drops to ~0 beyond 2k; perplexity explodes (>1e3) when evaluated naively at larger contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>PoSE-extended models can use the extended context window as effective memory for long-distance retrieval tasks (passkey retrieval) and achieve retrieval accuracy comparable to Full-length fine-tuning while requiring only the original short training context (major reduction in training memory/time). PoSE also attains language modeling perplexities at long context lengths very close to Full-length fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Performance still degrades as supported context length increases (trade-off between token quantity and attention granularity). The approach depends on RoPE and position interpolation strategies; inference memory usage remains the limiting factor for truly extreme lengths.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3203.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3203.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Full-length LLaMA (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Full-length fine-tuned LLaMA (target-length fine-tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline approach that fine-tunes the pre-trained LLaMA on inputs of the full target context length (e.g., 16k) using position interpolation, incurring quadratic compute/memory cost with sequence length.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LLaMA-7B (Full-length fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>LLaMA-7B model fine-tuned on inputs of the target (extended) context length (e.g., 16k) with position interpolation applied; requires full attention over the long sequences during training.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>context window extension</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Model is trained directly on full target-length sequences (after applying position interpolation) so the attention mechanism natively handles tokens across the extended window during both training and inference.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Passkey retrieval (and long-context language modeling)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same as PoSE: recovering randomly placed passkeys and evaluating long-document language modeling perplexity.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>retrieval / long-range language modeling</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Passkey retrieval: comparable high retrieval accuracy within target context (reported comparable to PoSE in the paper). Language modeling (GovReport) example: 16k perplexity = 4.59 PPL (Full-length 16k / 16k, Linear PI).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Full-length fine-tuning achieves strong long-context retrieval and language modeling performance (comparable to PoSE), but incurs much higher memory and time cost during training that grows rapidly with target length.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Training cost (memory and time) grows quadratically with target sequence length; impractical for extremely long target contexts (e.g., 32k+ on limited hardware).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3203.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3203.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Original LLaMA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA (original pre-trained model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The original LLaMA model pre-trained with a fixed context window (2k for LLaMA-7B), used as a baseline that lacks adaptations for much longer contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Llama: Open and efficient foundation language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Original LLaMA-7B</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Pre-trained LLaMA-7B decoder-only transformer trained with RoPE positional encodings and a 2k token context window; no fine-tuning for longer contexts in this baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>No additional memory beyond the pre-defined (short) context window; attention limited to original context length.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Passkey retrieval (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same passkey retrieval test; serves to show baseline behavior when tokens are placed beyond the model's native context window.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>retrieval / baseline</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Passkey retrieval accuracy rapidly drops to ~0 when prompt length exceeds 2k; language-model perplexity at longer evaluation windows becomes >1e3 without extension.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Original model cannot recover tokens placed beyond its native context window; performance collapses on long-context retrieval and modeling without adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Strict context-window limitation; cannot attend to tokens beyond the original trained length.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3203.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3203.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Transformer-XL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transformer-XL (recurrence-based)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recurrence-based Transformer that segments long inputs and reuses hidden states from previous segments to enable longer-range dependency modeling than fixed-length contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Transformer-xl: Attentive language models beyond a fixed-length context.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Transformer-XL</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A Transformer variant that introduces a segment-level recurrence mechanism (caching and reusing hidden states across segments) to extend effective context beyond a single attention window.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>recurrence-based memory (hidden-state cache)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Stores hidden states from previous segments and reuses them as memory for subsequent segments; this enables capturing longer-range dependencies but can suffer from information loss and limited random access.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Long-range language modeling / sequence modeling</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Improve modeling of dependencies that span beyond a single fixed-length attention window by recurrently passing hidden states between segments.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>language modeling / long-range dependency modeling</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Recurrence-based approaches enable longer-range dependency modeling but may lose information and have limited random access compared to full-attention over long contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Information loss over recurrences and limited random access to arbitrary distant tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3203.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3203.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Memorizing Transformers</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Memorizing Transformers (retrieval-based memory)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval-based approach that encodes prior sequences as discrete (key, value) memory items and retrieves them to augment transformer attention, enabling long-term memory beyond the attention window.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Memorizing transformers.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Memorizing Transformers</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Transformer augmented with a key-value memory store: past sequences are encoded into memory entries (keys and values) and a retrieval mechanism fetches relevant memory items for use during attention.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented memory (key-value store)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Encodes prior content into (key, value) pairs stored externally; during processing, a retriever selects relevant memory entries which are integrated into the model's attention computations to inform predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Long-context retrieval / memorization tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Supports retrieval of distant information by consulting an external memory store instead of relying solely on the fixed attention window.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>retrieval / long-range memory augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Retrieval-based memory enables storage and access to long-term information but may suffer from coarse segmentation and limited interaction between discrete segments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Discrete memory segmentation limits interaction across segments and may make integrating retrieved content with context challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3203.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3203.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Landmark Attention</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Landmark attention: Random-access infinite context length for transformers</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recent attention mechanism designed to facilitate random access to arbitrary chunks of extremely long inputs, enabling more flexible long-context processing.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Landmark attention: Random-access infinite context length for transformers</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Landmark attention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An attention variant that introduces landmark tokens or structures to allow random access to different chunks of the input, enabling more scalable access patterns over very long sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>random-access chunked memory / attention</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Organizes input into chunks and uses special landmark mechanisms to index and access arbitrary chunks at inference, improving random access compared to purely recurrent or segmented approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Random-access long-context retrieval and modeling</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Designed to permit transformers to access and reason about widely separated parts of very long inputs efficiently.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>retrieval / long-range attention</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Landmark attention improves random access to chunks compared to prior recurrence or retrieval systems, addressing some limitations of segmentation-based memory approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Implementation complexity and integration with standard transformer attention; paper-level trade-offs in memory/compute not exhaustively compared in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Transformer-xl: Attentive language models beyond a fixed-length context. <em>(Rating: 2)</em></li>
                <li>Recurrent memory transformer. <em>(Rating: 2)</em></li>
                <li>Memorizing transformers. <em>(Rating: 2)</em></li>
                <li>Augmenting language models with long-term memory. <em>(Rating: 2)</em></li>
                <li>Landmark attention: Random-access infinite context length for transformers <em>(Rating: 2)</em></li>
                <li>Randomized positional encodings boost length generalization of transformers. <em>(Rating: 1)</em></li>
                <li>Extending context window of large language models via positional interpolation. <em>(Rating: 1)</em></li>
                <li>Yarn: Efficient context window extension of large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3203",
    "paper_id": "paper-73290ecbec2f38d1d647ddef1ada69cee41725b3",
    "extraction_schema_id": "extraction-schema-74",
    "extracted_data": [
        {
            "name_short": "PoSE-LLaMA",
            "name_full": "LLaMA fine-tuned via Positional Skip-wisE (PoSE) training",
            "brief_description": "A LLaMA-7B model fine-tuned using Positional Skip-wisE (PoSE), which simulates long inputs by partitioning the training context into chunks and adding sampled skipping bias terms to their position indices to extend effective context at inference without full-length fine-tuning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "LLaMA-7B (PoSE-extended)",
            "agent_description": "Decoder-only Transformer (LLaMA-7B) with original RoPE position encoding; weights are fine-tuned using PoSE to adapt to much larger target context windows (e.g., 16k, 32k, up to 128k) while training still uses the original short context (2k).",
            "memory_used": true,
            "memory_type": "context window extension",
            "memory_mechanism_description": "PoSE manipulates RoPE position indices during fine-tuning by dividing the training context into N chunks (N=2 by default), sampling chunk lengths and non-overlapping skipping bias terms u_i to shift chunk position indices into the target range, and applying position interpolation; this enables the model to attend across a much larger effective context at inference (i.e., use the extended context as memory) without training on full-length inputs.",
            "task_name": "Passkey retrieval (and long-context language modeling)",
            "task_description": "Passkey retrieval: recover a randomly placed short secret (five-digit passkey) embedded in a long document; tests whether a model can effectively attend to and recall tokens placed far away in the input. Also evaluated on long-document language modeling (GovReport, Proof-pile) to measure general long-range modeling.",
            "task_type": "retrieval / long-range language modeling",
            "performance_with_memory": "Passkey retrieval: &gt;=90% retrieval accuracy within the model's target context window (PoSE-16k and PoSE-32k models maintain &gt;=90% accuracy within their respective context windows). Language modeling (GovReport): example perplexity at 16k = 4.60 PPL; (Proof-pile) 16k = 2.60 PPL.",
            "performance_without_memory": "Original LLaMA (2k context) retrieval accuracy drops to ~0 beyond 2k; perplexity explodes (&gt;1e3) when evaluated naively at larger contexts.",
            "has_performance_comparison": true,
            "key_findings": "PoSE-extended models can use the extended context window as effective memory for long-distance retrieval tasks (passkey retrieval) and achieve retrieval accuracy comparable to Full-length fine-tuning while requiring only the original short training context (major reduction in training memory/time). PoSE also attains language modeling perplexities at long context lengths very close to Full-length fine-tuning.",
            "limitations_or_challenges": "Performance still degrades as supported context length increases (trade-off between token quantity and attention granularity). The approach depends on RoPE and position interpolation strategies; inference memory usage remains the limiting factor for truly extreme lengths.",
            "uuid": "e3203.0",
            "source_info": {
                "paper_title": "PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Full-length LLaMA (baseline)",
            "name_full": "Full-length fine-tuned LLaMA (target-length fine-tuning)",
            "brief_description": "Baseline approach that fine-tunes the pre-trained LLaMA on inputs of the full target context length (e.g., 16k) using position interpolation, incurring quadratic compute/memory cost with sequence length.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "LLaMA-7B (Full-length fine-tuned)",
            "agent_description": "LLaMA-7B model fine-tuned on inputs of the target (extended) context length (e.g., 16k) with position interpolation applied; requires full attention over the long sequences during training.",
            "memory_used": true,
            "memory_type": "context window extension",
            "memory_mechanism_description": "Model is trained directly on full target-length sequences (after applying position interpolation) so the attention mechanism natively handles tokens across the extended window during both training and inference.",
            "task_name": "Passkey retrieval (and long-context language modeling)",
            "task_description": "Same as PoSE: recovering randomly placed passkeys and evaluating long-document language modeling perplexity.",
            "task_type": "retrieval / long-range language modeling",
            "performance_with_memory": "Passkey retrieval: comparable high retrieval accuracy within target context (reported comparable to PoSE in the paper). Language modeling (GovReport) example: 16k perplexity = 4.59 PPL (Full-length 16k / 16k, Linear PI).",
            "performance_without_memory": null,
            "has_performance_comparison": true,
            "key_findings": "Full-length fine-tuning achieves strong long-context retrieval and language modeling performance (comparable to PoSE), but incurs much higher memory and time cost during training that grows rapidly with target length.",
            "limitations_or_challenges": "Training cost (memory and time) grows quadratically with target sequence length; impractical for extremely long target contexts (e.g., 32k+ on limited hardware).",
            "uuid": "e3203.1",
            "source_info": {
                "paper_title": "PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Original LLaMA",
            "name_full": "LLaMA (original pre-trained model)",
            "brief_description": "The original LLaMA model pre-trained with a fixed context window (2k for LLaMA-7B), used as a baseline that lacks adaptations for much longer contexts.",
            "citation_title": "Llama: Open and efficient foundation language models.",
            "mention_or_use": "use",
            "agent_name": "Original LLaMA-7B",
            "agent_description": "Pre-trained LLaMA-7B decoder-only transformer trained with RoPE positional encodings and a 2k token context window; no fine-tuning for longer contexts in this baseline.",
            "memory_used": false,
            "memory_type": null,
            "memory_mechanism_description": "No additional memory beyond the pre-defined (short) context window; attention limited to original context length.",
            "task_name": "Passkey retrieval (baseline)",
            "task_description": "Same passkey retrieval test; serves to show baseline behavior when tokens are placed beyond the model's native context window.",
            "task_type": "retrieval / baseline",
            "performance_with_memory": null,
            "performance_without_memory": "Passkey retrieval accuracy rapidly drops to ~0 when prompt length exceeds 2k; language-model perplexity at longer evaluation windows becomes &gt;1e3 without extension.",
            "has_performance_comparison": true,
            "key_findings": "Original model cannot recover tokens placed beyond its native context window; performance collapses on long-context retrieval and modeling without adaptation.",
            "limitations_or_challenges": "Strict context-window limitation; cannot attend to tokens beyond the original trained length.",
            "uuid": "e3203.2",
            "source_info": {
                "paper_title": "PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Transformer-XL",
            "name_full": "Transformer-XL (recurrence-based)",
            "brief_description": "A recurrence-based Transformer that segments long inputs and reuses hidden states from previous segments to enable longer-range dependency modeling than fixed-length contexts.",
            "citation_title": "Transformer-xl: Attentive language models beyond a fixed-length context.",
            "mention_or_use": "mention",
            "agent_name": "Transformer-XL",
            "agent_description": "A Transformer variant that introduces a segment-level recurrence mechanism (caching and reusing hidden states across segments) to extend effective context beyond a single attention window.",
            "memory_used": true,
            "memory_type": "recurrence-based memory (hidden-state cache)",
            "memory_mechanism_description": "Stores hidden states from previous segments and reuses them as memory for subsequent segments; this enables capturing longer-range dependencies but can suffer from information loss and limited random access.",
            "task_name": "Long-range language modeling / sequence modeling",
            "task_description": "Improve modeling of dependencies that span beyond a single fixed-length attention window by recurrently passing hidden states between segments.",
            "task_type": "language modeling / long-range dependency modeling",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": false,
            "key_findings": "Recurrence-based approaches enable longer-range dependency modeling but may lose information and have limited random access compared to full-attention over long contexts.",
            "limitations_or_challenges": "Information loss over recurrences and limited random access to arbitrary distant tokens.",
            "uuid": "e3203.3",
            "source_info": {
                "paper_title": "PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Memorizing Transformers",
            "name_full": "Memorizing Transformers (retrieval-based memory)",
            "brief_description": "A retrieval-based approach that encodes prior sequences as discrete (key, value) memory items and retrieves them to augment transformer attention, enabling long-term memory beyond the attention window.",
            "citation_title": "Memorizing transformers.",
            "mention_or_use": "mention",
            "agent_name": "Memorizing Transformers",
            "agent_description": "Transformer augmented with a key-value memory store: past sequences are encoded into memory entries (keys and values) and a retrieval mechanism fetches relevant memory items for use during attention.",
            "memory_used": true,
            "memory_type": "retrieval-augmented memory (key-value store)",
            "memory_mechanism_description": "Encodes prior content into (key, value) pairs stored externally; during processing, a retriever selects relevant memory entries which are integrated into the model's attention computations to inform predictions.",
            "task_name": "Long-context retrieval / memorization tasks",
            "task_description": "Supports retrieval of distant information by consulting an external memory store instead of relying solely on the fixed attention window.",
            "task_type": "retrieval / long-range memory augmentation",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": false,
            "key_findings": "Retrieval-based memory enables storage and access to long-term information but may suffer from coarse segmentation and limited interaction between discrete segments.",
            "limitations_or_challenges": "Discrete memory segmentation limits interaction across segments and may make integrating retrieved content with context challenging.",
            "uuid": "e3203.4",
            "source_info": {
                "paper_title": "PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Landmark Attention",
            "name_full": "Landmark attention: Random-access infinite context length for transformers",
            "brief_description": "A recent attention mechanism designed to facilitate random access to arbitrary chunks of extremely long inputs, enabling more flexible long-context processing.",
            "citation_title": "Landmark attention: Random-access infinite context length for transformers",
            "mention_or_use": "mention",
            "agent_name": "Landmark attention",
            "agent_description": "An attention variant that introduces landmark tokens or structures to allow random access to different chunks of the input, enabling more scalable access patterns over very long sequences.",
            "memory_used": true,
            "memory_type": "random-access chunked memory / attention",
            "memory_mechanism_description": "Organizes input into chunks and uses special landmark mechanisms to index and access arbitrary chunks at inference, improving random access compared to purely recurrent or segmented approaches.",
            "task_name": "Random-access long-context retrieval and modeling",
            "task_description": "Designed to permit transformers to access and reason about widely separated parts of very long inputs efficiently.",
            "task_type": "retrieval / long-range attention",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": false,
            "key_findings": "Landmark attention improves random access to chunks compared to prior recurrence or retrieval systems, addressing some limitations of segmentation-based memory approaches.",
            "limitations_or_challenges": "Implementation complexity and integration with standard transformer attention; paper-level trade-offs in memory/compute not exhaustively compared in this work.",
            "uuid": "e3203.5",
            "source_info": {
                "paper_title": "PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training",
                "publication_date_yy_mm": "2023-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Transformer-xl: Attentive language models beyond a fixed-length context.",
            "rating": 2
        },
        {
            "paper_title": "Recurrent memory transformer.",
            "rating": 2
        },
        {
            "paper_title": "Memorizing transformers.",
            "rating": 2
        },
        {
            "paper_title": "Augmenting language models with long-term memory.",
            "rating": 2
        },
        {
            "paper_title": "Landmark attention: Random-access infinite context length for transformers",
            "rating": 2
        },
        {
            "paper_title": "Randomized positional encodings boost length generalization of transformers.",
            "rating": 1
        },
        {
            "paper_title": "Extending context window of large language models via positional interpolation.",
            "rating": 1
        },
        {
            "paper_title": "Yarn: Efficient context window extension of large language models",
            "rating": 1
        }
    ],
    "cost": 0.01471375,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>PoSE: Efficient Context Window Extension of LLMs via Positional SKIP-WISE TRAINING</h1>
<p>Dawei Zhu ${ }^{\circ}$ <em>Nan Yang</em> ${ }^{\circ}$ Liang Wang<em> Yifan Song ${ }^{\circ}$ Wenhao Wu ${ }^{\circ}$<br>Furu Wei</em> Sujian Li<em><br>School of Computer Science, Peking University<br>${ }^{\text {</em> }}$ National Key Laboratory for Multimedia Information Processing, Peking University<br>${ }^{\text {* }}$ Microsoft Corporation<br>https://github.com/dwzhu-pku/PoSE</p>
<h4>Abstract</h4>
<p>Large Language Models (LLMs) are trained with a pre-defined context length, restricting their use in scenarios requiring long inputs. Previous efforts for adapting LLMs to a longer length usually requires fine-tuning with this target length (Fulllength fine-tuning), suffering intensive training cost. To decouple train length from target length for efficient context window extension, we propose Positional Skip-wisE (PoSE) training that smartly simulates long inputs using a fixed context window. This is achieved by first dividing the original context window into several chunks, then designing distinct skipping bias terms to manipulate the position indices of each chunk. These bias terms and the lengths of each chunk are altered for every training example, allowing the model to adapt to all positions within target length. Experimental results show that PoSE greatly reduces memory and time overhead compared with Full-length fine-tuning, with minimal impact on performance. Leveraging this advantage, we have successfully extended the LLaMA model to 128 k tokens using a 2 k training context window. Furthermore, we empirically confirm that PoSE is compatible with all RoPE-based LLMs and position interpolation strategies. Notably, our method can potentially support infinite length, limited only by memory usage in inference. With ongoing progress for efficient inference, we believe PoSE can further scale the context window beyond 128 k .</p>
<h2>1 INTRODUCTION</h2>
<p>Large Language Models (LLMs) have revolutionized language modeling and demonstrated impressive abilities to perform various tasks (Brown et al., 2020). However, even with their remarkable capacity, these LLMs remain restricted by pre-defined context window sizes, suffering from notable performance decline when input tokens exceeds these limits. Nevertheless, numerous application scenarios demand extremely long input sequences, including long document summarization (Huang et al., 2021), in-context learning with numerous examples (Li et al., 2023), and long document retrieval (Zhou et al., 2022), etc. This naturally poses a significant challenge of context window extension: Extending the context window of a pre-trained LLM to accommodate longer sequences.</p>
<p>Naively fine-tuning LLMs on inputs of target length for window extension has received limited success due to the large disruption introduced by new position indices (Chen et al., 2023a; Han et al., 2023). Addressing this, Position Interpolation (Chen et al., 2023a; kaiokendev, 2023; Peng et al., 2023) propose to down-scale the position indices to match the original window size, yielding improved results for context extension. However, these methods still rely on Full-length fine-tuning, i.e., finetuning with context of target length, which is memory and time-intensive due to the computational complexity that increases quadratically with input length. For example, Chen et al. (2023a) use 32 A100 GPUs to extend LLaMA models from 2 k to 8 k context, and 128 A100 GPUs for even larger context. These overhead has made it impossible to extend context window to extreme lengths.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Position indices of Full-length fine-tuning v.s. PoSE fine-tuning for extending the context window size from 2,048 to 8,192 . At each iteration, the former directly takes 8,192 tokens for fine-tuning, while PoSE manipulates the position indices of 2,048 tokens to simulate longer inputs. For example, we partition the original context window of 2,048 tokens into two chunks, and adjust the position indices of the second chunk by adding a distinct skipping bias term. These bias terms, as well as the length of each chunk, are altered for each training example, so that the model can adapt to all relative positions of the target context window through fine-tuning.</p>
<p>In this paper, we introduce Positional Skip-wisE (PoSE) fine-tuning to decouple the fine-tuning length from the target context window length, unleashing the possibility of efficiently extending context window to an extreme size. The key idea of PoSE is to simulate long inputs by manipulating position indices within a fixed context window. As depicted in Figure 1, we partition the original context window into several chunks, and adjust the position indices of each chunk by adding a distinct skipping bias term. These bias terms, as well as the length of each chunk, are altered for each training example, so that the model can adapt to all positions (including both absolute and relative) within the target context window through fine-tuning. Meanwhile, by maintaining continuous position indices within each chunk, PoSE bears a close resemblance to pre-training. As a result, the model's pre-trained capacity for language modeling and comprehension is retained to the greatest degree.</p>
<p>The advantages of our PoSE are threefold: 1) Memory and Time Efficiency: By only requiring the original context size for fine-tuning, PoSE circumvents the quadratic increase in computational complexity with respect to target length during the fine-tuning stage, thereby significantly reducing memory and time overhead. 2) Potential for Extremely-Long Context: We manage to extend the context window of LLaMA (Touvron et al., 2023a) by up to 64 times ( $2 \mathrm{k} \rightarrow 128 \mathrm{k}, \mathrm{k}=1,024$ ) while preserving decent ability of language modeling and understanding. 3) Compatible with all RoPE-based LLMs and PI strategies: The effectiveness of PoSE has been empirically validated across several representative RoPE-based LLMs, including LLaMA, LLaMA2 (Touvron et al., 2023b), GPT-J (Wang \&amp; Komatsuzaki, 2021), and Baichuan (Baichuan, 2023). Additionally, PoSE has been demonstrated to be compatible with a variety of position interpolation methods, including Linear (Chen et al., 2023a), NTK (Peng \&amp; Quesnelle, 2023), and YaRN (Peng et al., 2023) interpolation.</p>
<p>Notably, by decoupling the fine-tuning and target length, PoSE can theoretically extend context window to an infinite length. The only constraint is the memory usage during the inference phase. Hopefully, with the continuous advancements in efficient inference techniques, including Flash Attention (Dao et al., 2022; Dao, 2023), xFormers (Lefaudeux et al., 2022), vLLM (Kwon et al., 2023), etc, we believe PoSE can promisingly push the context window size to a even larger scale.</p>
<h1>2 RELATED WORK</h1>
<p>Training Length-Extrapolatable Models. Length extrapolation requires the model to handle continually increasing input tokens, even beyond the context window size used for training (Press et al., 2021). To this end, a series of positional embedding schemes have been proposed, including ALibi (Press et al., 2021), xPos (Sun et al., 2023), NoPos (Haviv et al., 2022), etc.</p>
<p>Similar to our work, Ruoss et al. (2023) also attempted to simulate longer sequences during training time to mitigate out-of-distribution lengths. They proposed randomized positional encoding</p>
<p>(RandPos), which randomly selected an ordered subset of position indices from longer sequences. Our proposed method, PoSE, diverges from their approach in several key aspects: First, RandPos is a positional embedding scheme designed to pre-train encoder-only models from scratch for length extrapolation. In contrast, PoSE is a fine-tuning method aiming at efficiently extend the context window of pre-trained LLMs, which are majorly decoder-only models. Second, in RandPos, the position indices between adjacent tokens are not continuous. However, in PoSE, the position indices within each chunk are intentionally made continuous to resemble the pre-training phase, therefore reducing the risk of disrupting the language modeling abilities learned during pre-training.</p>
<p>Fine-tuning LLMs for Longer Context. Differing from length extrapolation, which primarily involves training a model from scratch to support lengths exceeding those it was initially trained for, context window extension focuses on extending the context window of a pre-trained LLM. Directly fine-tuning an existing LLM with a longer context window has been shown to progress slowly (Chen et al., 2023a). To expedite and stabilize training, Chen et al. (2023a) first down-scaled position indices to match original context size through Linear Position Interpolation. Subsequently, a range of Positional Interpolation (PI) strategies have been introduced, including NTK (Peng &amp; Quesnelle, 2023) and YaRN (Peng et al., 2023). More recently, LongLora (Chen et al., 2023b) propose shift short attention to approximate full attention. However, all these methods require Full-length fine-tuning, suffering computational cost that grows with target context size. By contrast, our method managed to decouple train / target length, requiring only the original context size for fine-tuning.</p>
<p>Memory Transformers. An alternative strategy for extremely long input sequences involves memory mechanisms. Typically, there are two lines of research for utilizing memory: the recurrence-based approach (Dai et al., 2019; Bulatov et al., 2022) and the retrieval-based approach (Wu et al., 2022; Wang et al., 2023; Tworkowski et al., 2023). The former segments long inputs and reuses the hidden states of preceding segments as memory, suffering from information loss and limited capacity for random access. The latter encodes prior sequences as (key, value) pairs and utilizes a memory retriever and reader to extract previously encoded information, primarily limited by the lack of interaction between discrete memory segments. More recently, Mohtashami &amp; Jaggi (2023) introduced landmark attention to facilitates random access to any chunk of the input. In contrast, our method achieves full access to the entire input without any modifications to the attention mechanism.</p>
<h1>3 Methodology</h1>
<h3>3.1 Preliminaries</h3>
<p>Rotary Position Embedding (RoPE). The use of RoPE (Su et al., 2021) has become pervasive in contemporary LLMs, including LLaMA (Touvron et al., 2023a), GPT-J (Wang &amp; Komatsuzaki, 2021), etc. It encodes position information of tokens with a rotation matrix that naturally incorporates explicit relative position dependency. To elucidate, given a hidden vector $\boldsymbol{h}=\left[h_{0}, h_{1}, \ldots, h_{d-1}\right]$, where $d$ is the hidden dimension, and a position index $m$, RoPE operates as follows:</p>
<p>$$
f(\boldsymbol{h}, m)=\left(\begin{array}{c}
h_{0} \
h_{1} \
h_{2} \
h_{3} \
\vdots \
h_{d-2} \
h_{d-1}
\end{array}\right) \otimes\left(\begin{array}{c}
\cos m \theta_{0} \
\cos m \theta_{0} \
\cos m \theta_{1} \
\cos m \theta_{1} \
\vdots \
\cos m \theta_{d / 2-1} \
\cos m \theta_{d / 2-1}
\end{array}\right)+\left(\begin{array}{c}
-h_{1} \
h_{0} \
-h_{3} \
h_{2} \
\vdots \
-h_{d-1} \
h_{d-2}
\end{array}\right) \otimes\left(\begin{array}{c}
\sin m \theta_{0} \
\sin m \theta_{0} \
\sin m \theta_{1} \
\sin m \theta_{1} \
\vdots \
\sin m \theta_{d / 2-1} \
\sin m \theta_{d / 2-1}
\end{array}\right)
$$</p>
<p>where $\theta_{j}=10000^{-2 j / d}, j \in{0,1, \ldots, d / 2-1}$. Unlike previous absolute position encodings that are directly applied to the input vector $\boldsymbol{x}$, RoPE is employed on the query and key vectors at each layer. Given a query $\boldsymbol{q}$ at position $m$ and a key $\boldsymbol{k}$ at position $n$, attention score $a(\boldsymbol{q}, \boldsymbol{k})$ is defined as:</p>
<p>$$
\begin{aligned}
a(\boldsymbol{q}, \boldsymbol{k}) &amp; =<f(\boldsymbol{q}, m), f(\boldsymbol{k}, n)> \
&amp; =\sum_{j=0}^{d / 2-1}\left[\left(q_{2 j} k_{2 j}+q_{2 j+1} k_{2 j+1}\right) \cos (m-n) \theta_{j}+\left(q_{2 j} k_{2 j+1}-q_{2 j+1} k_{2 j}\right) \sin (m-n) \theta_{j}\right] \
&amp; :=g(\boldsymbol{q}, \boldsymbol{k}, \boldsymbol{\theta}, m-n)
\end{aligned}
$$</p>
<p>Hence, RoPE encodes position information in a relative manner, as the attention score depends on the relative distances between positions rather than their absolute position values.</p>
<p>Problem Formulation. Given a Large Language Model pre-trained with a context window size of $L_{c}$, our objective is to extend this context size to a target length $L_{t}$, so that the model maintains good performance when processing input sequences containing a maximum of $L_{t}$ tokens.</p>
<p>Position Interpolation (PI). In contrast to directly extending the position indices to $L_{t}-1$ when dealing with an input text $\boldsymbol{x}=\left{x_{0}, x_{1}, \ldots, x_{L_{t}}\right}$, position interpolation down-scales the position indices to align with the original context window size $L_{c}$. This approach effectively mitigates the risk of encountering extreme values and has been empirically demonstrated to enhance stability during fine-tuning. Various interpolation strategies have been proposed, with $\alpha=L_{t} / L_{c}$ denoting the scaling factor:</p>
<ul>
<li>Linear Interpolation. As described by Chen et al. (2023a) and kaiokendev (2023), linear interpolation involves a proportional down-scaling of the position index $m$ to $m / \alpha$. Consequently, the attention score between a query $\boldsymbol{q}$ at position $m$ and a key $\boldsymbol{k}$ at position $n$ becomes $g(\boldsymbol{q}, \boldsymbol{k}, \boldsymbol{\theta},(m-n) / \alpha)$, as defined in Equation 2. Theoretical analysis has substantiated that the interpolated attention score exhibits significantly greater stability compared to the extrapolated counterpart.</li>
<li>Neural Tangent Kernel (NTK) Interpolation. In contrast to linear interpolation, NTK Interpolation alters the base of RoPE, effectively modifying the rotational "speed" of each dimension of RoPE (Peng \&amp; Quesnelle, 2023). Specifically, the original $\theta_{j}=10000^{-2 j / d}, j \in{0,1, \ldots, d / 2-1}$ in RoPE is transformed into $\theta_{j}^{\prime}=(10000 \lambda)^{-2 j / d}$, where $\lambda=\alpha^{d / d-2}$. It is noteworthy that the value of $\lambda$ is chosen to ensure that $m \theta_{d / 2-1}^{\prime}=(m / \alpha) \theta_{d / 2-1}$.</li>
<li>YaRN Interpolation. Different from Linear and NTK interpolation that treat each dimension of RoPE equally, YaRN (Peng et al., 2023) employs a ramp function to combine Linear and NTK interpolation at varying proportions across different dimensions. Simultaneously, it introduces a temperature factor to mitigate distribution shift of attention matrix caused by long inputs.</li>
</ul>
<h1>3.2 Proposed Approach: Positional Skip-wise Training (PoSE)</h1>
<p>Although position interpolation effectively addresses out-of-distribution position indices, extending to an extreme length by fine-tuning on context window of this size remains impractical, owing to the quadratic growth in computational complexity of attention as sequence length increases. Instead, we explore to train within the original context window $L_{c}$ and achieve context window extension via manipulating position indices to simulate longer inputs.</p>
<p>There are two designing desiderata for this endeavor: First, to avoid out-of-distribution positions during inference, the relative distance of manipulated position indices should comprehensively cover the range of $\left{1, \ldots, L_{t}-1\right}$. Second, fine-tuning with the manipulated position indices should not harm the original abilities of LLMs, so the structure of manipulated position indices should closely adhere to the original structure to the greatest extent possible.</p>
<p>Initially, we randomly divide the original context window $L_{c}$ into $N$ chunks $c_{0}, c_{1}, \ldots, c_{N-1}$, each with lengths $l_{0}, l_{1}, \ldots, l_{N-1}$, where $\sum_{i=0}^{N-1} l_{i}=L_{c}$. We introduce the starting index $s t_{i}$ for each chunk $c_{i}$, which facilitates the formulation of its position indices as follows:</p>
<p>$$
\operatorname{Pos}\left(c_{i}\right)=\left{s t_{i}, s t_{i}+1, \ldots, s t_{i}+l_{i}-1\right}, \quad s t_{i}=\sum_{j=0}^{i-1} l_{j}
$$</p>
<p>Subsequently, we employ the discrete uniform distribution $\mathcal{U}(S)$ to sample a skipping bias term $u_{i} \sim \mathcal{U}\left(\left{u_{i-1}, \ldots, L_{t}-L_{c}\right}\right)$ for each chunk $c_{i}$. This bias term is applied to the corresponding chunk to transform the original position indices into:</p>
<p>$$
\operatorname{PoSE}\left(c_{i}\right)=\left{u_{i}+s t_{i}, u_{i}+s t_{i}+1, \ldots, u_{i}+s t_{i}+l_{i}-1\right}
$$</p>
<p>Note that the constraint of $u_{i} \geq u_{i-1}$ is applied to prevent position index overlaps between chunks.
Intuitively, the introduction of skipping bias terms exposes model to a more diverse range of relative positions. To achieve comprehensive coverage of the target context window, we re-sample both the</p>
<p>length and skipping bias term of every chunk for each training example. Moreover, the continuity of position indices within each chunk closely resembles the structure employed during pre-training. Consequently, fine-tuning the model on these new position indices for language modeling does not compromise its original capabilities.</p>
<p>Concerning the text contained within each chunk, a similar procedure is followed to select continuous spans of tokens from the input text $\boldsymbol{x}=\left{x_{0}, x_{1}, \ldots, x_{L_{x}}\right}$. To elaborate, we begin by sampling a bias term $v_{i} \sim \mathcal{U}\left(\left{v_{i-1}, \ldots, L_{x}-L_{c}\right)\right.$ followed by assigning the content of chunk $c_{i}$ as below:</p>
<p>$$
c_{i}=\boldsymbol{x}\left[v_{i}+s t_{i}: v_{i}+s t_{i}+l_{i}\right]
$$</p>
<p>Notably, we have also explored other assigning strategy of $v_{i}$, including scenarios where $v_{i}=0$, which results in genuinely continuous content for the chunks, or $v_{i}=u_{i}$, aligning the manipulated position indices with actual positions in the original text. However, we observe that these variations have relatively little impact on the outcomes of fine-tuning.</p>
<p>After position indices and content for each chunk are settled, we perform position interpolation for stabilized fine-tuning. For simplicity, We set the initial bias terms $u_{0}$ and $v_{0}$ to 0 . In terms of chunk number $N$, we view it as an trade-off between efficiency and effectiveness. Because an increase in the number of chunks will further deviates from the position structure of pre-training, which may harm the ability acquired during pre-training. Hence, in this paper we set $N$ to 2 , exposing the models to a wider range of relative positions, while adhering as close to the original position structure as possible. (See Appendxi A and B for further discussion of $v_{i}$ and $N$.)</p>
<h1>4 EXPERIMENTS</h1>
<p>In this section, we conduct experiments to verify the effectiveness of PoSE for context window extension. Our method demonstrates impressive results on context lengths of both 16 k and 32 k for language modeling as well as passkey retrieval. Other advantages of PoSE are discussed in Section 5.</p>
<h3>4.1 SETUPS</h3>
<p>Training Procedure. For each setting in the main experiments, we train LLaMA-7B with the next token prediction objective. This training process comprises 1,000 steps, employing a global batch size of 64 on 8 V100 GPUs using Deepspeed ZeRO stage 3 (Rajbhandari et al., 2020). We use learning rate $2 e^{-5}$ and a linear scheduler, with 10 warmup steps. We use AdamW optimizer with its default hyperparameters setup. The fine-tuning dataset is sourced from The Pile (Gao et al., 2020), with a minimum length requirement of 2,048 tokens. Our default choice for interpolation strategies is linear interpolation. For evaluation, we use a single A100 GPU. Flash Attention V2 (Dao, 2023) is applied, making it possible to evaluate long documents of up to 128 k tokens $(\mathrm{k}=1,024)$
Evaluation Tasks and Datasets. We examine the ability of long text modeling on two tasks: language modeling and passkey retrieval. The language modeling task is a fundamental task that reflects the overall capability of a model in handling long text. Passkey retrieval, on the other hand, can effectively measure the maximum distance that a token can attend to during the inference stage. We evaluate language modeling on GovReport (Huang et al., 2021) and Proof-pile (Zhangir et al., 2022) datasets. For passkey retrieval, we follow Mohtashami \&amp; Jaggi (2023) to construct synthetic prompts for evaluation.</p>
<p>Baseline Methods. We compare our PoSE training method against following baselines:</p>
<ul>
<li>Full-length fine-tuning takes input tokens of target length for fine-tuning. For this method, computation complexity scales quadratically with target context window size. Following Chen et al. (2023a) and Peng et al. (2023), we perform PI before fine-tuning LLMs on inputs of target length.</li>
<li>RandPos (Ruoss et al., 2023) is initially designed to train an encoder-only model from scratch for length extrapolation. However, since it shares similar idea of simulating longer sequences via changing position indices, we include it for a comprehensive comparison. Given the original / target context window length $L_{c} / L_{t}$, it uniquely samples $L_{c}$ positions from the set $\left{0, \ldots, L_{t}-1\right}$, arranges them in ascending order, and employs them as new position indices for training. For fair comparison, we also apply PI for this method.</li>
</ul>
<p>Table 1: Perplexity of models trained with different methods. We conduct evaluation on the GovReport and Proof-pile datasets, varying evaluation context window size from 2 k to 32 k . Our PoSE, with a fixed training window size of 2 k , effectively extended to a target context size of $16 \mathrm{k} / 32 \mathrm{k}$ for inference while receiving only minimal performance degradation compared to Full-length.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Context size</th>
<th style="text-align: center;">GovReport</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Proof-pile</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Train / Target</td>
<td style="text-align: center;">2k</td>
<td style="text-align: center;">4k</td>
<td style="text-align: center;">8k</td>
<td style="text-align: center;">16k</td>
<td style="text-align: center;">32k</td>
<td style="text-align: center;">2k</td>
<td style="text-align: center;">4k</td>
<td style="text-align: center;">8k</td>
<td style="text-align: center;">16k</td>
<td style="text-align: center;">32k</td>
</tr>
<tr>
<td style="text-align: center;">Original</td>
<td style="text-align: center;">- / -</td>
<td style="text-align: center;">4.74</td>
<td style="text-align: center;">$&gt;10^{3}$</td>
<td style="text-align: center;">$&gt;10^{3}$</td>
<td style="text-align: center;">$&gt;10^{3}$</td>
<td style="text-align: center;">$&gt;10^{3}$</td>
<td style="text-align: center;">2.83</td>
<td style="text-align: center;">$&gt;10^{3}$</td>
<td style="text-align: center;">$&gt;10^{3}$</td>
<td style="text-align: center;">$&gt;10^{3}$</td>
<td style="text-align: center;">$&gt;10^{3}$</td>
</tr>
<tr>
<td style="text-align: center;">Full-length</td>
<td style="text-align: center;">$16 \mathrm{k} / 16 \mathrm{k}$</td>
<td style="text-align: center;">4.87</td>
<td style="text-align: center;">4.70</td>
<td style="text-align: center;">4.61</td>
<td style="text-align: center;">4.59</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">2.93</td>
<td style="text-align: center;">2.71</td>
<td style="text-align: center;">2.58</td>
<td style="text-align: center;">2.53</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">RandPos</td>
<td style="text-align: center;">$2 \mathrm{k} / 16 \mathrm{k}$</td>
<td style="text-align: center;">11.63</td>
<td style="text-align: center;">11.17</td>
<td style="text-align: center;">11.54</td>
<td style="text-align: center;">15.16</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">7.26</td>
<td style="text-align: center;">6.83</td>
<td style="text-align: center;">6.76</td>
<td style="text-align: center;">7.73</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$2 \mathrm{k} / 32 \mathrm{k}$</td>
<td style="text-align: center;">93.43</td>
<td style="text-align: center;">95.85</td>
<td style="text-align: center;">91.79</td>
<td style="text-align: center;">93.22</td>
<td style="text-align: center;">97.57</td>
<td style="text-align: center;">60.74</td>
<td style="text-align: center;">63.54</td>
<td style="text-align: center;">60.56</td>
<td style="text-align: center;">63.15</td>
<td style="text-align: center;">66.47</td>
</tr>
<tr>
<td style="text-align: center;">PoSE (Ours)</td>
<td style="text-align: center;">$2 \mathrm{k} / 16 \mathrm{k}$</td>
<td style="text-align: center;">4.84</td>
<td style="text-align: center;">4.68</td>
<td style="text-align: center;">4.60</td>
<td style="text-align: center;">4.60</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">2.95</td>
<td style="text-align: center;">2.74</td>
<td style="text-align: center;">2.61</td>
<td style="text-align: center;">2.60</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$2 \mathrm{k} / 32 \mathrm{k}$</td>
<td style="text-align: center;">4.91</td>
<td style="text-align: center;">4.76</td>
<td style="text-align: center;">4.68</td>
<td style="text-align: center;">4.64</td>
<td style="text-align: center;">4.66</td>
<td style="text-align: center;">3.01</td>
<td style="text-align: center;">2.78</td>
<td style="text-align: center;">2.66</td>
<td style="text-align: center;">2.60</td>
<td style="text-align: center;">2.59</td>
</tr>
</tbody>
</table>
<h1>4.2 LANGUAGE MODELING</h1>
<p>First, we investigate the impacts of different fine-tuning methods on long sequence language modeling using the GovReport and Proof-pile datasets. GovReport is a summarization dataset comprising 19,402 reports published by the Congress and the U.S. Government, with an average document length of 7,866 tokens. We randomly select 50 reports containing more than 32,768 tokens for evaluation. Similarly, Proof-pile is a 13GB mathematical dataset of long mathematical documents. In line with the approach taken for GovReport, we choose 50 samples from Proof-pile that contain more than 32,768 tokens for evaluation.</p>
<p>Table 1 presents the results of scaling to 16 k and 32 k using Full-length, RandPos, and PoSE training method, each with linear interpolation (See Appendix C for results of NTK and YaRN). For each scaled model, as well as the Original LLaMA model, we report perplexity scores at various evaluation context window sizes, ranging from 2 k to 32 k , employing the sliding window approach proposed by Press et al. (2021). For evaluation efficiency, we set the stride of the sliding window to 1,024 .</p>
<p>First, we observe an overall decreasing trend of perplexity for both models scaled to 16 k and 32 k via PoSE as evaluation context window size increases, proving their abilities to leverage longer context. Second, with significantly shorter context length during fine-tuning, our PoSE achieves comparable results with Full-length, consolidating its effectiveness. Third, our method achieves much stronger results than RandPos. We suppose it is because our manipulated position indices closely resembles that of pre-training, hereby preserving the pre-trained language modeling ability to the greatest extent.</p>
<p>We also notice that all the scaling methods suffers certain performance degradation as the supported context length increases. We perceive this as a trade-off between the quantity of tokens the model can process and the level of granularity in the attention the model can pay to each individual token.</p>
<h3>4.3 PASSKEY RETRIEVAL FOR EFFECTIVE CONTEXT WINDOW</h3>
<p>To effectively measure the maximum distance that a token can attend to during the inference stage, we adopt the passkey retrieval test proposed by Mohtashami \&amp; Jaggi (2023). In this test, models are tasked with recovering a random passkey hidden within a lengthy document. Prompt template used for this task is presented in Figure 2a.</p>
<p>Specifically, we compare the original LLaMA model with the PoSE-extended versions for 16 k and 32 k context. For each model, we vary the prompt length from 2 k to 32 k . For each length, we conduct the passkey retrieval test for 50 times, with a random passkey of 5 digits generated and placed at a random position inside the prompt. We also include results from Full-length, RandPos, and PI-only (position interpolation without fine-tuning). Figure 2b illustrates the results. For the Original, PI-only, and RandPos models, their retrieval accuracy rapidly drop to 0 when the context exceeds 2 k . In contrast, both PoSE-16k / 32k models managed to maintain a high retrieval accuracy ( $\geq 90 \%$ ) within their respective target context window, comparable to Full-length. This indicates that models trained via PoSE genuinely possess the capability to attend to all tokens within the extended context windows.</p>
<p>There is an important info hidden inside a lot of irrelevant text. Find it and memorize them. I will quiz you about the important information there.</p>
<p>The grass is green. The sky is blue. The sun is yellow. Here we go. There and back again. (repeat x times)</p>
<p>The pass key is 81501. Remember it. 81501 is the pass key.
The grass is green. The sky is blue. The sun is yellow. Here we go. There and back again. (repeat y times)</p>
<p>What is the pass key? The pass key is
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: (a) Prompt template used for passkey retrieval; (b) Retrieval accuracy for the PoSE-extended 16k / 32k models, compared with other baselines. Both PoSE-extended models maintain a high retrieval accuracy $(\geq 90 \%)$ within their respective context window.</p>
<h1>5 ANALYSIS</h1>
<p>In this section, we analyze the advantages of PoSE, including 1) memory and time efficiency; 2) compatibility with all RoPE-based LLMs and diverse interpolation strategies; 3) potential for extremely-long context. In Section 5.4, We also verify that model performance within the original context window only receives minimal degradation.</p>
<h3>5.1 MEMORY AND TIME EFFICIENCY</h3>
<p>We study the memory and time efficiency of PoSE compared with Full-length fine-tuning. For each method, we scale LLaMA-7B to $4 \mathrm{k} / 8 \mathrm{k} / 16 \mathrm{k}$ through 1,000 training steps with a global batch size of 16 on 8 V100 GPUs. Experiment results are demonstrated in Figure 3. Figure 3(a) and (b) respectively illustrates memory and time consumption for 1,000 steps of Full-length versus PoSE. While the training cost of Full-length increases rapidly with target window length, PoSE only requires a fixed quota of memory and time for context extension, which is significantly lower. Figure 3(c) further compares model perplexity of the two training methods at different steps on GovReport. Notably, both models achieve relatively low perplexity levels within the initial 100 training steps. Moreover, at each step, our proposed PoSE, while requiring only a training context size of 2 k tokens, exhibits very close language modeling ability to Full-length fine-tuning, which requires an extended training context of 16 k . We did not experiment with context window of 32 k or above, because V100 machines cannot afford full fine-tuning of these lengths. But it can be expected that the overhead ration between Full-leng and PoSE will become more exaggerated as target length increases. Consequently, we can confidently assert that our proposed approach is both memory and time-efficient.</p>
<h3>5.2 COMPATIBILITY WITH ROPE-BASED LLMS AND DIVERSE INTERPOLATION STRATEGIES</h3>
<p>We also delve into the effectiveness of PoSE when applied to different RoPE-based LLMs, as well as various interpolation strategies. Specifically, we employ PoSE on four distinct models: LLaMA-7B, LLaMA2-7B, GPT-J-6B, and Baichuan2-7B, all of which encompasses RoPE in their architectures. The original context size of LLaMA-7B and GPT-J-6B is 2 k , while that of LLaMA2-7B and Baichuan2-7B is 4 k . For each model, we examine the integration with Linear, NTK, and YaRN interpolation, as well as the original version for comparative purposes. The same GovReport dataset as described in Section 4.2 is utilized. The test set is truncated to the first 1 k to 16 k tokens for plotting the perplexity curve, as depicted in Figure 4. First, it is evident that PoSE is effective across all four models and three interpolation strategies, as evidenced by the low perplexities achieved by all 12 combinations in comparison to the 4 original model. Second, we observe that NTK and YaRN interpolation generally yields superior results compared to Linear interpolation. However, it is noteworthy that NTK exhibits a significant increase in perplexity after a certain turning point, which occurs prior to reaching the target context length. This behavior is consistent with previous findings, indicating that for a given scaling factor $\alpha$, NTK cannot genuinely expand the context window by $\alpha$ times (Peng \&amp; Quesnelle, 2023; Quesnelle, 2023; Peng et al., 2023).</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Full-length fine-tuning v.s. PoSE in terms of (a) Memory and (b) Time consumption for extending LLaMA-7B from 2 k to $4 \mathrm{k} / 8 \mathrm{k} / 16 \mathrm{k}$ context, each finishing 1000 training steps. (c) Perplexity of both 16 k -context models at every training steps. We show that PoSE takes a constantly reduced time and memory for context extension, while attaining a comparable level of PPL performance with Full-length fine-tuning at each step.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Perplexity of LLaMA-7B, LLaMA2-7B, GPT-J-6B, Baichuan2-7B extended to 16k via PoSE with Linear / NTK / YaRN interpolation, along with the Original model. The consistently low perplexity observed across all nine combinations serves as an indication of the effectiveness of our method across RoPE-based LLMs and diverse interpolation strategies.</p>
<h1>5.3 Potential for Extremely-Long Context</h1>
<p>Because PoSE only takes a fixed context window at training stage to extend to target context window size, we can promisingly extend LLMs to support infinite input lengths using this method. In this section, we extend context window size to 96 k and 128 k to explore PoSE's potential for extreme context window extension. Given the need to evaluate on extremely long documents, we have opted to employ two book datasets, namely Books3 (Presser, 2020) and Gutenberg (PG-19) (Rae et al., 2019). Both of these datasets consist of extensive collections of literary works, rendering them well-suited subjects for the assessment of long-range modeling. For our evaluation, we randomly selected 20 books from each dataset, each containing more than 128 k tokens.</p>
<p>Fine-tuning LLaMA models using PoSE, we experimented with Linear / NTK / YaRN interpolation for both the 96 k and 128 k models. To calculate perplexity, we adhere to the sliding window strategy adopted in Section 4.2, with an increased sliding window step of 16 k to enhance evaluation efficiency. The outcomes of these experiments are detailed in Table 2. It is observe that, PoSE successfully extends the model's context window to 96 k when coupled with Linear interpolation, and further extends the context window to 128 k when paired with YaRN. These promising results consolidates the effectiveness of PoSE for extreme context window extension.</p>
<h3>5.4 Evaluation of Capability on Original Context Window</h3>
<p>In this section, we examine the capabilities of the PoSE-extended models on the original context window using standard benchmarks. We combine the Hugging Face Open LLM Leaderboard (Face, 2023)</p>
<p>Table 2: Perplexity of models extended to extreme context size via PoSE on PG-19 and Books3. We show that our training method can effectively extend context window size to 128 k when combined with YaRN interpolation.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Gutenberg (PG-19)</th>
<th></th>
<th></th>
<th></th>
<th>Books3</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>32k</td>
<td>64k</td>
<td>96k</td>
<td>128k</td>
<td>32k</td>
<td>64k</td>
<td>96k</td>
<td>128k</td>
</tr>
<tr>
<td>PoSE-Linear-96k</td>
<td>10.18</td>
<td>11.11</td>
<td>13.57</td>
<td>-</td>
<td>9.98</td>
<td>10.90</td>
<td>13.42</td>
<td>-</td>
</tr>
<tr>
<td>PoSE-NTK-96k</td>
<td>7.98</td>
<td>20.39</td>
<td>38.73</td>
<td>-</td>
<td>8.29</td>
<td>20.82</td>
<td>40.39</td>
<td>-</td>
</tr>
<tr>
<td>PoSE-YaRN-96k</td>
<td>8.31</td>
<td>8.65</td>
<td>9.36</td>
<td>-</td>
<td>8.90</td>
<td>9.40</td>
<td>10.38</td>
<td>-</td>
</tr>
<tr>
<td>PoSE-Linear-128k</td>
<td>16.90</td>
<td>22.47</td>
<td>26.77</td>
<td>31.18</td>
<td>26.20</td>
<td>43.62</td>
<td>57.08</td>
<td>70.87</td>
</tr>
<tr>
<td>PoSE-NTK-128k</td>
<td>8.04</td>
<td>14.84</td>
<td>29.48</td>
<td>34.80</td>
<td>8.34</td>
<td>16.04</td>
<td>31.42</td>
<td>37.00</td>
</tr>
<tr>
<td>PoSE-YaRN-128k</td>
<td>9.32</td>
<td>10.36</td>
<td>10.77</td>
<td>11.33</td>
<td>10.56</td>
<td>12.30</td>
<td>13.07</td>
<td>13.81</td>
</tr>
</tbody>
</table>
<p>Table 3: Performance of PoSE-extended LLaMA model on standard benchmarks in comparison with Full-length fine-tuning and the original LLaMA. We show that PoSE-extended models exhibit only marginal performance degradation compared with Full-length fine-tuning and the original version.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Zero-Shot</th>
<th></th>
<th></th>
<th></th>
<th>Few-Shot</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>BoolQ</td>
<td>PIQA</td>
<td>WinoGrande</td>
<td>TruthfulQA</td>
<td>ARC-C</td>
<td>HellaSwag</td>
</tr>
<tr>
<td>Original LLaMA</td>
<td>75.11</td>
<td>78.67</td>
<td>69.85</td>
<td>34.08</td>
<td>51.19</td>
<td>77.75</td>
</tr>
<tr>
<td>Full-Linear-16k</td>
<td>70.95</td>
<td>77.64</td>
<td>69.06</td>
<td>31.89</td>
<td>48.55</td>
<td>74.19</td>
</tr>
<tr>
<td>Full-NTK-16k</td>
<td>75.80</td>
<td>78.08</td>
<td>68.98</td>
<td>33.83</td>
<td>48.81</td>
<td>76.57</td>
</tr>
<tr>
<td>Full-YaRN-16k</td>
<td>73.88</td>
<td>77.64</td>
<td>68.15</td>
<td>34.12</td>
<td>50.60</td>
<td>77.18</td>
</tr>
<tr>
<td>PoSE-Linear-16k</td>
<td>74.50</td>
<td>78.13</td>
<td>68.59</td>
<td>32.05</td>
<td>48.29</td>
<td>75.56</td>
</tr>
<tr>
<td>PoSE-NTK-16k</td>
<td>74.28</td>
<td>78.24</td>
<td>68.90</td>
<td>33.89</td>
<td>49.83</td>
<td>76.82</td>
</tr>
<tr>
<td>PoSE-YaRN-16k</td>
<td>74.28</td>
<td>78.02</td>
<td>69.06</td>
<td>34.00</td>
<td>49.23</td>
<td>77.04</td>
</tr>
<tr>
<td>PoSE-Linear-128k</td>
<td>67.71</td>
<td>76.22</td>
<td>67.56</td>
<td>36.16</td>
<td>39.93</td>
<td>66.04</td>
</tr>
<tr>
<td>PoSE-NTK-128k</td>
<td>75.35</td>
<td>78.18</td>
<td>68.98</td>
<td>32.71</td>
<td>49.66</td>
<td>76.19</td>
</tr>
<tr>
<td>PoSE-YaRN-128k</td>
<td>73.61</td>
<td>77.80</td>
<td>70.01</td>
<td>34.47</td>
<td>48.46</td>
<td>75.54</td>
</tr>
</tbody>
</table>
<p>with a subset of LLaMA benchmarks to assess zero-shot and few-shot performance. For zero-shot evaluation, we employ BoolQ (Clark et al., 2019), PIQA (Bisk et al., 2020), WinoGrande (Keisuke et al., 2019), and TruthfulQA (Lin et al., 2022). For few-shot evaluation, we utilize 25-shot ARCChallenge (Clark et al., 2018) and 10-shot HellaSwag (Zellers et al., 2019). Our evaluation metrics are benchmark-specific: for BoolQ, PIQA, and WinoGrande, we report accuracy; for TruthfulQA, we report mc2; and for ARC-C and HellaSwag, we report normalized accuracy.</p>
<p>Table 3 summarizes the results. It is observed that, PoSE-extended models exhibit only marginal performance degradation compared with Full-length fine-tuning and the original LLaMA, with the only exception of the 128 k model employing linear interpolation. This indicates that while extending context window size, PoSE effectively preserves original language comprehension ability.</p>
<h1>6 CONCLUSION</h1>
<p>In this paper, we introduce Positional Skip-wisE (PoSE) training to efficiently extend the context window of Large Language Models. PoSE simulates long inputs by manipulating position indices, thereby requiring only the original context window for fine-tuning, successfully decoupling train length and target length. Experiments have shown that, compared with fine-tuning on the full length, PoSE greatly reduces memory and time overhead. Taking advantage of this, we have managed to extend LLaMA model to 128 k on 8 V100 GPUs, observing only minimal performance degradation on standard benchmarks. We have also empirically verified that PoSE is compatible with all RoPE-based LLMs and position interpolation strategies.</p>
<h1>7 ACKNOWLEDGEMENT</h1>
<p>We thank all the anonymous reviewers for their helpful comments on this paper. We thank Xueguang Ma, Yang Ouyang, Pengyun Yue, Hanyu Li, Fangwei Zhu for the thoughtful discussion. This work was partially supported by the Okawa Research Grant.</p>
<h2>REFERENCES</h2>
<p>Baichuan. Baichuan 2: Open large-scale language models. arXiv preprint arXiv:2309.10305, 2023. URL https://arxiv.org/abs/2309.10305.</p>
<p>Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning about physical commonsense in natural language. In Thirty-Fourth AAAI Conference on Artificial Intelligence, 2020.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.</p>
<p>Aydar Bulatov, Yury Kuratov, and Mikhail Burtsev. Recurrent memory transformer. Advances in Neural Information Processing Systems, 35:11079-11091, 2022.</p>
<p>Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of large language models via positional interpolation. ArXiv, abs/2306.15595, 2023a.</p>
<p>Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. Longlora: Efficient fine-tuning of long-context large language models. arXiv preprint arXiv:2309.12307, 2023b.</p>
<p>Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. In NAACL, 2019.</p>
<p>Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018.</p>
<p>Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G Carbonell, Quoc Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 2978-2988, 2019.</p>
<p>Tri Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. 2023.
Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R. FlashAttention: Fast and memory-efficient exact attention with IO-awareness. In Advances in Neural Information Processing Systems, 2022.</p>
<p>Hugging Face. Open llm leaderboard. https://huggingface.co/spaces/HuggingFaceH4/open_ llm_leaderboard, 2023.</p>
<p>Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The Pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020.</p>
<p>Chi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng Ji, and Sinong Wang. Lm-infinite: Simple on-the-fly length generalization for large language models. arXiv preprint arXiv:2308.16137, 2023.</p>
<p>Adi Haviv, Ori Ram, Ofir Press, Peter Izsak, and Omer Levy. Transformer language models without positional encodings still learn positional information. In Findings of the Association for Computational Linguistics: EMNLP 2022, pp. 1382-1390, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-emnlp. 99.</p>
<p>Luyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, and Lu Wang. Efficient attentions for long document summarization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 1419-1436, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main. 112 .
kaiokendev. Things i'm learning while training superhot. https://kaiokendev.github.io/til# extending-context-to-8k, 2023.</p>
<p>Sakaguchi Keisuke, Le Bras Ronan, Bhagavatula Chandra, and Choi Yejin. Winogrande: An adversarial winograd schema challenge at scale. 2019.</p>
<p>Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention, 2023.</p>
<p>Benjamin Lefaudeux, Francisco Massa, Diana Liskovich, Wenhan Xiong, Vittorio Caggiano, Sean Naren, Min Xu, Jieru Hu, Marta Tintore, Susan Zhang, Patrick Labatut, and Daniel Haziza. xformers: A modular and hackable transformer modelling library. https://github.com/ facebookresearch/xformers, 2022.</p>
<p>Mukai Li, Shansan Gong, Jiangtao Feng, Yiheng Xu, Jun Zhang, Zhiyong Wu, and Lingpeng Kong. In-context learning with many demonstration examples. arXiv preprint arXiv:2302.04931, 2023.</p>
<p>Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human falsehoods. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 3214-3252, 2022.</p>
<p>Amirkeivan Mohtashami and Martin Jaggi. Landmark attention: Random-access infinite context length for transformers, 2023.</p>
<p>Bowen Peng and Jeffrey Quesnelle. Ntk-aware scaled rope allows llama models to have extended ( $8 \mathrm{k}+$ ) context size without any fine-tuning and minimal perplexity degradation. https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_ rope_allows_llama_models_to_have, 2023.</p>
<p>Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window extension of large language models, 2023.</p>
<p>Ofir Press, Noah A Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input length extrapolation. arXiv preprint arXiv:2108.12409, 2021.</p>
<p>Shawn Presser. https://twitter.com/theshawwn/status/1320282149329784833, 2020.
Jeffrey Quesnelle. Dynamically scaled rope further increases performance of long context llama with zero fine-tuning. https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/ dynamically_scaled_rope_further_increases/, 2023.</p>
<p>Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, Chloe Hillier, and Timothy P Lillicrap. Compressive transformers for long-range sequence modelling. arXiv preprint, 2019.</p>
<p>Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training trillion parameter models. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, pp. 1-16. IEEE, 2020.</p>
<p>Anian Ruoss, Grgoire Deltang, Tim Genewein, Jordi Grau-Moya, Rbert Csords, Mehdi Bennani, Shane Legg, and Joel Veness. Randomized positional encodings boost length generalization of transformers. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 1889-1903, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-short. 161.</p>
<p>Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. arXiv preprint arXiv:2104.09864, 2021.</p>
<p>Yutao Sun, Li Dong, Barun Patra, Shuming Ma, Shaohan Huang, Alon Benhaim, Vishrav Chaudhary, Xia Song, and Furu Wei. A length-extrapolatable transformer. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 14590-14604, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long. 816 .</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothe Lacroix, Baptiste Rozire, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b.</p>
<p>Szymon Tworkowski, Konrad Staniszewski, Mikoaj Pacek, Yuhuai Wu, Henryk Michalewski, and Piotr Mio. Focused transformer: Contrastive training for context scaling. arXiv preprint arXiv:2307.03170, 2023.</p>
<p>Ben Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax, May 2021.</p>
<p>Weizhi Wang, Li Dong, Hao Cheng, Xiaodong Liu, Xifeng Yan, Jianfeng Gao, and Furu Wei. Augmenting language models with long-term memory. arXiv preprint arXiv:2306.07174, 2023.</p>
<p>Yuhuai Wu, Markus N Rabe, DeLesley Hutchins, and Christian Szegedy. Memorizing transformers. arXiv preprint arXiv:2203.08913, 2022.</p>
<p>Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 4791-4800, 2019.</p>
<p>Azerbayev Zhangir, Ayers Edward, and Bartosz Piotrowski. Proof-pile. https://github.com/ zhangir-azerbayev/proof-pile, 2022.</p>
<p>Yucheng Zhou, Tao Shen, Xiubo Geng, Chongyang Tao, Guodong Long, Can Xu, and Daxin Jiang. Fine-grained distillation for long document retrieval. arXiv preprint arXiv:2212.10423, 2022.</p>
<p>Table 4: Comparison of different methods for choosing $v_{i}$. We report perplexity with evaluation context window ranging from 2 k to 16 k . We show that these variations have relatively little impact on the outcomes of fine-tuning.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>GovReport</th>
<th></th>
<th></th>
<th></th>
<th>Proof-pile</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>$\mathbf{2 k}$</td>
<td>$\mathbf{4 k}$</td>
<td>$\mathbf{8 k}$</td>
<td>$\mathbf{1 6 k}$</td>
<td>$\mathbf{2 k}$</td>
<td>$\mathbf{4 k}$</td>
<td>$\mathbf{8 k}$</td>
<td>$\mathbf{1 6 k}$</td>
</tr>
<tr>
<td>$v_{i} \sim \mathcal{U}(\ldots)$</td>
<td>4.84</td>
<td>4.68</td>
<td>4.60</td>
<td>4.60</td>
<td>2.95</td>
<td>2.74</td>
<td>2.61</td>
<td>2.60</td>
</tr>
<tr>
<td>$v_{i}=0$</td>
<td>4.85</td>
<td>4.72</td>
<td>4.64</td>
<td>4.68</td>
<td>2.96</td>
<td>2.75</td>
<td>2.63</td>
<td>2.61</td>
</tr>
<tr>
<td>$v_{i}=u_{i}$</td>
<td>4.84</td>
<td>4.68</td>
<td>4.60</td>
<td>4.60</td>
<td>2.95</td>
<td>2.73</td>
<td>2.60</td>
<td>2.56</td>
</tr>
</tbody>
</table>
<h1>A Ablation of Text Contained within Each Chunk</h1>
<p>PoSE divide the original context window into several chunks, and modify the position indices of each chunk to cover a wider range of relative positions in a fixed window. However, it does not impose a particular constraint on the text contained within each chunk. Recall that in Equation 5, we assign the content of chunk $c_{i}$ as below:</p>
<p>$$
c_{i}=\boldsymbol{x}\left[v_{i}+s t_{i}: v_{i}+s t_{i}+l_{i}\right]
$$</p>
<p>In this section, we explore several strategies for determining $v_{i}$ : 1) sampling from uniform distribution, $v_{i} \sim \mathcal{U}\left(\left{v_{i-1}, \ldots, L_{x}-L_{c}\right)\right.$, which is the one used in PoSE; 2) $v_{i}=0$, which results in genuinely continuous content for the chunks; 3) $v_{i}=u_{i}$, aligning the manipulated position indices with actual positions in the original text. We use the same test setting as Section 4.2, extending LLaMA-7B from 2 k to 16 k context. As can be seen in Table 4, we show that these variations have relatively little impact on the outcomes of fine-tuning.</p>
<h2>B Analysis of Chunk Number N</h2>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Coverage probability for each relative position in a single training example ( $2 \mathrm{k}-&gt;16 \mathrm{k}$ ). Utilizing multiple chunks reduces coverage probability within the original $[0,2,048]$ context window, while enhancing the coverage likelihood of relative positions in the range of $[2,048,16,383]$. Probability of coverage increases with the number of chunks. Pushing the chunk number to the limit is RandPos, utilizing 2048 chunks, capable of covering every relative position in each training example by expectation.</p>
<p>PoSE achieves coverage of all positions within the target context window by randomly sampling the chunk sizes and skipping bias terms for each training example. In this section, we explore the probability of each relative position being covered by a training example, using a context extension of 2,048 to 16,384 as an example. For the unextended original version, the probability of a relative position within 2048 being covered is 1 , and the probability of a relative position above 2,048 being covered is 0 . For the cases where the number of chunks is 2,3 , or 2,048 (i.e., RandPos), we use the</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Python Code used for calculating coverage probability of each relative position in Figure 5.
Table 5: Comparison of different chunk numbers. We report perplexity with evaluation context window ranging from 2 k to 16 k . By increasing chunk number, relative positions in $[2,048,16,383]$ receive an increased chance of being trained, rendering better results for context extension. However, extremely large chunk number also damages model performance.</p>
<table>
<thead>
<tr>
<th>Chunk number</th>
<th>Proof-pile</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>2k</td>
<td>4k</td>
<td>8k</td>
</tr>
<tr>
<td>1</td>
<td>2.83</td>
<td>$&gt;10^{3}$</td>
<td>$&gt;10^{3}$</td>
</tr>
<tr>
<td>2</td>
<td>2.95</td>
<td>2.74</td>
<td>2.61</td>
</tr>
<tr>
<td>3</td>
<td>2.93</td>
<td>2.72</td>
<td>2.60</td>
</tr>
<tr>
<td>2048</td>
<td>7.26</td>
<td>6.83</td>
<td>6.76</td>
</tr>
</tbody>
</table>
<p>Monte Carlo method to estimate this coverage probability. The code used is demonstrated in Figure 6. The estimated results are shown in Figure 5. It can be seen that PoSE reduces the coverage probability of positions within the original context window, while all relative positions in $[2,048,16,383]$ receives a certain increase in chance of being covered, and the probability of coverage increases as the number of chunks increases. For the case where the number of chunks is equal to 2,048, the probability of each relative position being covered is close to 1 . With this observation, we further compare the impact of chunk number on language modeling capability, as presented in Table 5. Increasing chunk number efficiently renders better results for context extension. However, extremely large chunk number also damages model performance, due to the severe deviation from the position encoding structure used in pre-training phase. We believe that the choice of the number of chunks is a trade-off between training efficiency and performance.</p>
<h1>C Sliding Window PPL from Linear / NTK / YARN Interpolation</h1>
<p>Evaluation results in Table 1 are based on Linear interpolation. In this section, we comprehensively provide experiment results with all three PI strategies, and compare four scenarios for each: Fulllength fine-tuning, PoSE, PI-only, and Original. Note that PI-only means we only apply position interpolation without fine-tuning, while Original means the original LLaMA model with neither PI nor fine-tuning. For testing data and sliding window stride, we use the same setup used for Table 1. From Table 6, We can see that for all strategies, the performance follows the same trend: Full-length $\approx \mathrm{PoSE}&gt;\mathrm{PI}$-only $\gg$ Original. We also notice that the NTK method suffers from a significant increase in ppl at 16 k , mainly because NTK cannot effectively extend the context window by a scaling factor $\alpha$ (Peng \&amp; Quesnelle, 2023; Quesnelle, 2023; Peng et al., 2023). YaRN alleviates this issue, achieving progressively decreasing ppl as the context window grows.</p>
<p>Table 6: Perplexity of models trained with different methods using Linear / NTK / YaRN interpolation. It is observed that for all interpolation strategies, the performance follows same trend: Full-length $\approx$ PoSE $&gt;$ PI-only $\gg$ Original.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Context size <br> Train / Target</th>
<th style="text-align: center;">GovReport</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Proof-pile</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">2k</td>
<td style="text-align: center;">4k</td>
<td style="text-align: center;">8k</td>
<td style="text-align: center;">16k</td>
<td style="text-align: center;">2k</td>
<td style="text-align: center;">4k</td>
<td style="text-align: center;">8k</td>
<td style="text-align: center;">16k</td>
</tr>
<tr>
<td style="text-align: center;">Original</td>
<td style="text-align: center;">$-/-$</td>
<td style="text-align: center;">4.74</td>
<td style="text-align: center;">$&gt;10^{3}$</td>
<td style="text-align: center;">$&gt;10^{3}$</td>
<td style="text-align: center;">$&gt;10^{3}$</td>
<td style="text-align: center;">2.83</td>
<td style="text-align: center;">$&gt;10^{3}$</td>
<td style="text-align: center;">$&gt;10^{3}$</td>
<td style="text-align: center;">$&gt;10^{3}$</td>
</tr>
<tr>
<td style="text-align: center;">Linear Interpolation</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">PI-only</td>
<td style="text-align: center;">- / 16k</td>
<td style="text-align: center;">43.80</td>
<td style="text-align: center;">43.35</td>
<td style="text-align: center;">45.89</td>
<td style="text-align: center;">54.33</td>
<td style="text-align: center;">25.32</td>
<td style="text-align: center;">24.20</td>
<td style="text-align: center;">24.88</td>
<td style="text-align: center;">29.59</td>
</tr>
<tr>
<td style="text-align: center;">Full-length</td>
<td style="text-align: center;">$16 \mathrm{k} / 16 \mathrm{k}$</td>
<td style="text-align: center;">4.87</td>
<td style="text-align: center;">4.70</td>
<td style="text-align: center;">4.61</td>
<td style="text-align: center;">4.59</td>
<td style="text-align: center;">2.93</td>
<td style="text-align: center;">2.71</td>
<td style="text-align: center;">2.58</td>
<td style="text-align: center;">2.53</td>
</tr>
<tr>
<td style="text-align: center;">PoSE (Ours)</td>
<td style="text-align: center;">$2 \mathrm{k} / 16 \mathrm{k}$</td>
<td style="text-align: center;">4.84</td>
<td style="text-align: center;">4.68</td>
<td style="text-align: center;">4.60</td>
<td style="text-align: center;">4.60</td>
<td style="text-align: center;">2.95</td>
<td style="text-align: center;">2.74</td>
<td style="text-align: center;">2.61</td>
<td style="text-align: center;">2.60</td>
</tr>
<tr>
<td style="text-align: center;">NTK Interpolation</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">PI-only</td>
<td style="text-align: center;">- / 16k</td>
<td style="text-align: center;">5.62</td>
<td style="text-align: center;">5.61</td>
<td style="text-align: center;">5.80</td>
<td style="text-align: center;">550</td>
<td style="text-align: center;">3.27</td>
<td style="text-align: center;">3.15</td>
<td style="text-align: center;">3.19</td>
<td style="text-align: center;">517</td>
</tr>
<tr>
<td style="text-align: center;">Full-length</td>
<td style="text-align: center;">$16 \mathrm{k} / 16 \mathrm{k}$</td>
<td style="text-align: center;">4.78</td>
<td style="text-align: center;">4.63</td>
<td style="text-align: center;">4.57</td>
<td style="text-align: center;">7.24</td>
<td style="text-align: center;">2.93</td>
<td style="text-align: center;">2.71</td>
<td style="text-align: center;">2.61</td>
<td style="text-align: center;">5.66</td>
</tr>
<tr>
<td style="text-align: center;">PoSE (Ours)</td>
<td style="text-align: center;">$2 \mathrm{k} / 16 \mathrm{k}$</td>
<td style="text-align: center;">4.79</td>
<td style="text-align: center;">4.63</td>
<td style="text-align: center;">4.57</td>
<td style="text-align: center;">7.24</td>
<td style="text-align: center;">2.92</td>
<td style="text-align: center;">2.71</td>
<td style="text-align: center;">2.60</td>
<td style="text-align: center;">4.37</td>
</tr>
<tr>
<td style="text-align: center;">YaRN Interpolation</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">PI-only</td>
<td style="text-align: center;">- / 16k</td>
<td style="text-align: center;">5.57</td>
<td style="text-align: center;">5.51</td>
<td style="text-align: center;">5.57</td>
<td style="text-align: center;">5.83</td>
<td style="text-align: center;">3.17</td>
<td style="text-align: center;">2.97</td>
<td style="text-align: center;">2.87</td>
<td style="text-align: center;">2.89</td>
</tr>
<tr>
<td style="text-align: center;">Full-length</td>
<td style="text-align: center;">$16 \mathrm{k} / 16 \mathrm{k}$</td>
<td style="text-align: center;">4.78</td>
<td style="text-align: center;">4.62</td>
<td style="text-align: center;">4.54</td>
<td style="text-align: center;">4.53</td>
<td style="text-align: center;">2.90</td>
<td style="text-align: center;">2.68</td>
<td style="text-align: center;">2.56</td>
<td style="text-align: center;">2.52</td>
</tr>
<tr>
<td style="text-align: center;">PoSE (Ours)</td>
<td style="text-align: center;">$2 \mathrm{k} / 16 \mathrm{k}$</td>
<td style="text-align: center;">4.79</td>
<td style="text-align: center;">4.63</td>
<td style="text-align: center;">4.55</td>
<td style="text-align: center;">4.55</td>
<td style="text-align: center;">2.91</td>
<td style="text-align: center;">2.69</td>
<td style="text-align: center;">2.57</td>
<td style="text-align: center;">2.53</td>
</tr>
</tbody>
</table>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*Work done during Dawei's internship at MSRA. Sujian Li is the corresponding author.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
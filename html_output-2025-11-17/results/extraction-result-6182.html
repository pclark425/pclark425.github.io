<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6182 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6182</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6182</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-121.html">extraction-schema-121</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <p><strong>Paper ID:</strong> paper-a6a7724763d8adba466519489b0b9d209e7f2d15</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/a6a7724763d8adba466519489b0b9d209e7f2d15" target="_blank">BARTScore: Evaluating Generated Text as Text Generation</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> This work conceptualizes the evaluation of generated text as a text generation problem, modeled using pre-trained sequence-to-sequence models, and proposes a metric BARTScore with a number of variants that can be flexibly applied in an unsupervised fashion to evaluation of text from different perspectives.</p>
                <p><strong>Paper Abstract:</strong> A wide variety of NLP applications, such as machine translation, summarization, and dialog, involve text generation. One major challenge for these applications is how to evaluate whether such generated texts are actually fluent, accurate, or effective. In this work, we conceptualize the evaluation of generated text as a text generation problem, modeled using pre-trained sequence-to-sequence models. The general idea is that models trained to convert the generated text to/from a reference output or the source text will achieve higher scores when the generated text is better. We operationalize this idea using BART, an encoder-decoder based pre-trained model, and propose a metric BARTScore with a number of variants that can be flexibly applied in an unsupervised fashion to evaluation of text from different perspectives (e.g. informativeness, fluency, or factuality). BARTScore is conceptually simple and empirically effective. It can outperform existing top-scoring metrics in 16 of 22 test settings, covering evaluation of 16 datasets (e.g., machine translation, text summarization) and 7 different perspectives (e.g., informativeness, factuality). Code to calculate BARTScore is available at https://github.com/neulab/BARTScore, and we have released an interactive leaderboard for meta-evaluation at http://explainaboard.nlpedia.ai/leaderboard/task-meval/ on the ExplainaBoard platform, which allows us to interactively understand the strengths, weaknesses, and complementarity of each metric.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6182.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6182.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BARTScore</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BARTScore (evaluation metric based on BART sequence-to-sequence generation probabilities)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An unsupervised automated evaluation metric that scores a candidate text by the (weighted) log-probability a pretrained seq2seq model (BART) assigns to generating that text conditioned on another text (source or reference); supports multiple directions (s->h, r->h, h->r) and token weighting. Designed to evaluate multiple perspectives (informativeness, fluency, factuality, etc.) by changing inputs/outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Compute the sum of token log-probabilities (weighted equally by default) under a pretrained BART seq2seq model for generating target text given a conditioning text; instantiate as different conditional generation tasks (source->hypothesis for faithfulness, reference->hypothesis for precision, hypothesis->reference for recall, and arithmetic mean for F-score).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Correlations to human judgments across evaluation perspectives: Informativeness, Relevance, Fluency, Coherence, Factuality, Semantic Coverage (pyramid), Adequacy; numerical meta-evaluation via Spearman, Pearson, Kendall's Tau, and Accuracy (for pairwise factuality).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>BART (pretrained seq2seq), with optional fine-tuned variants (BART-CNN, BART-CNN-PARA).</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Natural Language Processing â€” automated evaluation of generated text (machine translation, summarization, data-to-text).</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Not a scientific theory; rather, a generative-probability-based metric to evaluate the quality of model-generated texts from multiple perspectives by treating evaluation as a conditional text-generation task.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>BARTScore (vanilla and variants) outperforms or matches top existing metrics in many settings: overall best performance in 16 of 22 test settings reported; robust correlations with human judgments across 16 datasets and 7 perspectives. Examples: BARTSCORE + CNN achieved high performance on factuality and summarization tasks; vanilla BARTScore already substantially outperformed embedding/matching baselines on many summarization settings.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Used across multiple datasets: WMT19 (MT language pairs de-en, fi-en, gu-en, kk-en, lt-en, ru-en, zh-en), REALSumm, SummEval, NeR18, Rank19, QAGS (CNN and XSUM), NEWSROOM, CNNDM (for fine-tuning), DARR (WMT), BAGEL, SFHOT, SFRES (data-to-text), ParaBank2 (for paraphrase fine-tuning).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Meta-evaluation: correlation (Spearman/Pearson/Kendall) between BARTScore and human judgments; in some factuality settings BARTScore+CNN matched or nearly matched human baselines (e.g., Rank19 accuracy 0.836 vs human 0.839). Overall, BARTScore variants produce high correlation with human scores and often outperform other unsupervised metrics and approach supervised methods on some tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Designed for text-generation evaluation, not specifically for evaluating scientific theories; may be biased toward abstractive-generation setups (less effective on extractive systems), prompting helps semantic-overlap evaluation but shows inconsistent or negative effects on factuality, and fine-tuning choices (e.g., paraphrase fine-tuning) can harm factuality-oriented evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BARTScore: Evaluating Generated Text as Text Generation', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6182.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6182.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BARTScore-Prompt</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BARTScore with Prompting (BARTSCORE-PROMPT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A variant of BARTScore that prepends or appends short textual prompts to the source or target to bring the evaluation task closer to pretraining and improve metric effectiveness; can ensemble scores over multiple prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Add a short prompt phrase to the source or target text (prefix or suffix) before computing log-probabilities under BART; average the generation scores over a set of prompts (prompt ensembling).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Same correlation-based criteria vs human judgments (Spearman, Pearson, Kendall); measured per perspective to assess whether prompting improves semantic overlap, fluency, or factuality.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>BART (pretrained seq2seq), same as base BARTScore.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>NLP text-generation evaluation (summarization, MT, data-to-text).</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Not a scientific theory; a protocol to modify inputs/outputs of the metric via natural-language prompts to improve evaluation alignment with human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Prompting improves semantic-overlap/informativeness evaluations consistently (prompt ensembling improves Spearman correlations on SummEval and NeR18); a single chosen prompt ('Such as') improved Kendall's Tau by 0.033 on de-en MT, in one case enabling BARTScore to surpass supervised metrics (BLEURT/COMET) on that language pair. However, prompts often do not improve and may degrade factuality scores.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Evaluated using the prompt set across SummEval, REALSumm, NeR18, and WMT (development WMT18 used to search for best prompt 'Such as').</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Prompt-augmented scores show higher correlations with human judgments for semantic-overlap perspectives; inconsistent for factuality and linguistic quality.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Prompt choice is dataset/task sensitive; many prompts help semantic overlap but few help factuality; prompt search strategies can be expensive and discovered prompts may not generalize.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BARTScore: Evaluating Generated Text as Text Generation', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6182.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6182.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BARTScore-CNN / BARTScore-CNN-PARA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BARTScore fine-tuned on CNN/DailyMail (CNN) and further on ParaBank2 (CNN-PARA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Variants of BARTScore where the underlying BART model is fine-tuned on downstream generation tasks: (1) fine-tuned on CNN/DailyMail summarization improves summarization-related correlations; (2) further fine-tuning on paraphrase data (ParaBank2) can help some tasks but may hurt factuality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Replace base BART parameters with those fine-tuned on summarization (CNNDM) and/or paraphrase data, then compute BARTScore as usual to better align metric with evaluation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Same correlation-based metrics vs human judgments (Spearman/Pearson/Kendall) across summarization, factuality, and data-to-text tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>BART fine-tuned on CNNDM (BART-CNN) and optionally further fine-tuned on ParaBank2 (BART-CNN-PARA).</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>NLP text-generation evaluation (summarization, factuality, data-to-text).</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Not a scientific theory; method adapts base metric by tuning the underlying model to a related generation task to improve metric-task alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Fine-tuning on CNNDM consistently boosts correlations (e.g., improved Spearman on summarization and data-to-text). BARTSCORE-CNN achieved top results on factuality datasets (Rank19 and QAGS), e.g., Rank19 accuracy 0.836 (significantly better than other unsupervised metrics). Further paraphrase fine-tuning improved some data-to-text correlations (e.g., BAGEL up to +0.083) but reduced performance on factuality datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>CNNDM (used for fine-tuning), ParaBank2 (paraphrase fine-tuning), evaluated on REALSumm, SummEval, NeR18, Rank19, QAGS, BAGEL, SFRES, SFHOT.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Fine-tuned variants increase correlation with human judgments, sometimes matching or approaching human baselines on factuality tasks (Rank19).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Choice of fine-tuning data matters: paraphrase fine-tuning can reduce factuality performance (because summaries and documents are not paraphrases); model specialization may reduce generality across perspectives.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BARTScore: Evaluating Generated Text as Text Generation', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6182.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6182.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Evaluation Perspectives (Criteria)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human evaluation perspectives: Informativeness, Relevance, Fluency, Coherence, Factuality, Semantic Coverage, Adequacy</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Set of human-judgment axes used as gold standards for meta-evaluation of automated metrics; each perspective measures a distinct facet of generated text quality (e.g., factuality = entailment from source; informativeness = key idea coverage).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Human annotators provide judgments per perspective (e.g., Likert-scale ratings, pyramid recall, pairwise correctness), which are treated as gold-standard labels for meta-evaluation of automated metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>The seven explicit criteria: Informativeness (INFO), Relevance (REL), Fluency (FLU), Coherence (COH), Factuality (FAC), Semantic Coverage (Cov/pyramid), Adequacy (ADE).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>NLP human evaluation protocols for generated text.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Human-annotated judgments defining ground-truth evaluations used to compute correlations for automated metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>All automated metrics are evaluated by correlation to these human judgments using Spearman, Pearson, Kendall's Tau, and accuracy (for pairwise factual correctness). The paper reports detailed correlations per perspective on datasets like SummEval (Coherence, Factuality, Fluency, Informativeness), REALSumm (pyramid coverage), and Rank19/QAGS (factuality).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>SummEval, REALSumm, NeR18, Rank19, QAGS, NEWSROOM used as human-judgment sources.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Human judgments are the gold standard; automated metrics are compared by correlation measures; in some cases automated metrics approach human-level agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Human annotation is expensive and perspective definitions vary across datasets (e.g., 'consistency' vs 'factuality'); some perspectives (factuality) are harder for prompt-based metric improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BARTScore: Evaluating Generated Text as Text Generation', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6182.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6182.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Baseline Metrics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Baseline automatic evaluation metrics (BLEU, ROUGE, CHRF, BERTScore, MoverScore, PRISM, BLEURT, COMET, FactCC, QAGS)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Collection of widely used metrics for evaluating generated text: n-gram match (BLEU, ROUGE, CHRF), embedding-based matching (BERTScore, MoverScore), paraphrase/translation-based (PRISM), and supervised learned metrics (BLEURT, COMET); also factuality-specific methods (FactCC, QAGS).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Varies by metric: token/character n-gram overlaps, embedding similarity and matching, paraphrase probability, supervised regression/ranking trained on human judgments, and question-answering based factuality checks.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Typically aim to measure adequacy/semantic overlap (BLEU, ROUGE), semantic similarity (BERTScore, MoverScore), paraphrastic equivalence (PRISM), or learned regressors correlated with human scores (BLEURT, COMET); factuality tools detect contradictions or verify statements against source (FactCC, QAGS).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>NLP automated evaluation metrics for MT, summarization, data-to-text.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Baselines that BARTScore is compared against in meta-evaluation experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>BARTScore often outperforms unsupervised baselines (BERTScore, MoverScore, PRISM) across many settings and sometimes matches or surpasses supervised baselines (BLEURT, COMET) when using prompting/fine-tuning on specific language pairs or datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Compared across WMT19, REALSumm, SummEval, NeR18, Rank19, QAGS, BAGEL, SFRES, SFHOT.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Baselines evaluated by correlation to human judgments; supervised metrics trained on human judgments sometimes outperform unsupervised ones, but BARTScore narrows the gap.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Each baseline has limited applicability to certain perspectives/tasks; supervised ones require human-labeled data; PRISM trained from scratch on parallel data whereas BARTScore uses pre-trained models; performance varies by language pair and task.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BARTScore: Evaluating Generated Text as Text Generation', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6182.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6182.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Benchmarks / Datasets</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Evaluation datasets and benchmarks used in meta-evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Concrete datasets and shared tasks used as testbeds for metric meta-evaluation, spanning machine translation (WMT DARR), summarization (REALSumm, SummEval, NeR18, NEWSROOM, CNNDM), factuality (Rank19, QAGS), and data-to-text (BAGEL, SFHOT, SFRES).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Use human-annotated judgments or task-specific gold standards from these datasets to compute metric-to-human correlations (Spearman/Pearson/Kendall) and accuracy for pairwise factuality.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Dataset-specific gold standards include pyramid recall (REALSumm), per-aspect human ratings (SummEval, NeR18), pairwise factual correctness (Rank19), question-answering based factual consistency (QAGS), and MT system rankings (WMT19).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>NLP evaluation: MT, summarization, data-to-text, factuality.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Datasets provide human judgments and system outputs used to measure automated metrics' alignment with humans.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>BARTScore and variants were evaluated across these datasets; main reported outcomes include per-dataset Spearman/Pearson/Kendall correlations and accuracy; e.g., BARTScore variants achieved top results across many of these benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>WMT19 (DARR), WMT18 (development for prompt search), REALSumm, SummEval, NeR18, Rank19, QAGS (CNN and XSUM), NEWSROOM, CNNDM (for fine-tuning), BAGEL, SFHOT, SFRES, ParaBank2 (fine-tuning data).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Human annotations in these datasets are treated as gold standards; metric performance is reported as correlation to those annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Dataset differences (e.g., number of systems, annotation schemes) impact metric comparability; some datasets have few systems (NeR18) making gains harder to measure; prompt search done on WMT18 may not generalize.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BARTScore: Evaluating Generated Text as Text Generation', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6182.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6182.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Meta-evaluation Measures</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Statistical measures for metric meta-evaluation and significance testing</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard statistical measures used to quantify agreement between automated metrics and human judgments (Spearman correlation, Pearson correlation, Kendall's Tau, Accuracy), plus bootstrap-based pairwise significance testing.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Compute Spearman (rank-order monotonic), Pearson (linear), Kendall's Tau (ordinal association) correlations between metric scores and human judgments; compute accuracy for pairwise factual comparisons; use bootstrap resampling to test pairwise significance (p < 0.05).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Correlation coefficients and statistical significance (bootstrapped p-values) indicate how well metrics match human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Statistical evaluation and meta-evaluation methodology for NLP metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Procedures and significance tests used to validate comparison of automated metrics to human annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Reported correlations and significance markings across tables (e.g., Kendall's Tau on WMT19, Spearman on summarization/data-to-text); used bootstrapping for pairwise tests with p < 0.05.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Applied to WMT19, SummEval, REALSumm, NeR18, Rank19, QAGS, BAGEL, SFHOT, SFRES.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>These measures quantify agreement between automated metric outputs and human judgments (gold standards).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Correlation coefficients summarize agreement but may mask per-system or per-instance failures; significance testing depends on dataset size and variance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BARTScore: Evaluating Generated Text as Text Generation', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6182.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e6182.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Key empirical findings</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Summary of key experimental results and numeric benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Concise summary of the major quantitative outcomes: BARTScore variants achieve top performance in the majority of tested settings, prompting and fine-tuning can materially improve correlations in specific scenarios, and BARTScore is robust to high-quality system outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Empirical meta-evaluation across datasets using correlations to human judgments and accuracy on factuality; ablations over prompting and fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Spearman/Pearson/Kendall correlations and pairwise accuracy; significance via bootstrapping.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>BART and BART fine-tuned variants (BART-CNN, BART-CNN-PARA).</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>NLP text-generation evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Not a scientific theory; empirical benchmarks demonstrating metric performance.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Aggregate: BARTScore outperforms existing top-scoring metrics in 16 of 22 test settings. Examples: On WMT19 average Kendall's Tau improved with BARTSCORE+CNN+Para to ~0.331 and further to 0.337 with prompting; on SummEval/REALSumm/NeR18 BARTSCORE variants yield the highest average Spearman correlations (e.g., average ~0.518 with best prompt/ensembling). On Rank19 factuality BARTSCORE+CNN achieves accuracy 0.836 (ddagger: significantly better than other unsupervised metrics), and QAGS Pearson up to 0.735. Prompt 'Such as' gave +0.033 Kendall's Tau on de-en MT. Data-to-text: BARTSCORE+CNN+Para+Prompt achieved average Spearman ~0.270, outperforming baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>WMT19, WMT18 (prompt dev), SummEval, REALSumm, NeR18, Rank19, QAGS, BAGEL, SFRES, SFHOT.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Reports direct correlations to human judgments; in some cases variant metrics approach human agreement levels (e.g., Rank19 human 0.839 vs BART+CNN 0.836).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Performance gains are dataset- and perspective-dependent; prompting and fine-tuning can help or hurt depending on whether the perspective is semantic overlap vs factuality; biases found (better discrimination for abstractive than extractive systems).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BARTScore: Evaluating Generated Text as Text Generation', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6182.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e6182.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bias & limitations analysis</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bias analysis and noted limitations of BARTScore-based evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical analyses showing BARTScore's relative weaknesses and potential biases: less effective discrimination for extractive summarizers, inconsistent prompt effects on factuality, sensitivity to fine-tuning data choices.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Rank-difference analysis on REALSumm systems and per-bucket breakdowns (top-k systems, reference length buckets); prompt-effect percentage analysis per perspective; dataset-specific ablations for fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Rank difference versus human ranks; correlations per bucket; percent of prompts that improve performance per perspective.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>BART and variants used for analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>NLP evaluation reliability and bias analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Not a scientific theory; identifies where a probability-based metric may systematically under- or over-rank systems.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>BARTScore is less effective distinguishing extractive systems (rank differences biased) and is more robust for high-quality (top-k) systems; prompt ensembling greatly helps semantic-overlap measures but rarely helps factuality; paraphrase fine-tuning harms factuality evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>REALSumm (bias/rank differences), SummEval, NeR18 (prompt analyses), Rank19, QAGS (factuality ablations).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Bias analysis compares BARTScore system rankings to human-derived rankings and highlights systematic disparities (especially on extractive systems).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Bias toward abstractive generation; difficulty improving factuality via prompting; dependency on fine-tuning data selection; not validated for domains like scientific-theory evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BARTScore: Evaluating Generated Text as Text Generation', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Automatic machine translation evaluation in many languages via zero-shot paraphrasing <em>(Rating: 2)</em></li>
                <li>BLEURT: Learning robust metrics for text generation <em>(Rating: 2)</em></li>
                <li>COMET: A neural framework for MT evaluation <em>(Rating: 2)</em></li>
                <li>Bertscore: Evaluating text generation with bert <em>(Rating: 2)</em></li>
                <li>MoverScore: Text generation evaluating with contextualized embeddings and earth mover distance <em>(Rating: 2)</em></li>
                <li>Asking and answering questions to evaluate the factual consistency of summaries <em>(Rating: 2)</em></li>
                <li>Evaluating the factual consistency of abstractive text summarization <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6182",
    "paper_id": "paper-a6a7724763d8adba466519489b0b9d209e7f2d15",
    "extraction_schema_id": "extraction-schema-121",
    "extracted_data": [
        {
            "name_short": "BARTScore",
            "name_full": "BARTScore (evaluation metric based on BART sequence-to-sequence generation probabilities)",
            "brief_description": "An unsupervised automated evaluation metric that scores a candidate text by the (weighted) log-probability a pretrained seq2seq model (BART) assigns to generating that text conditioned on another text (source or reference); supports multiple directions (s-&gt;h, r-&gt;h, h-&gt;r) and token weighting. Designed to evaluate multiple perspectives (informativeness, fluency, factuality, etc.) by changing inputs/outputs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Compute the sum of token log-probabilities (weighted equally by default) under a pretrained BART seq2seq model for generating target text given a conditioning text; instantiate as different conditional generation tasks (source-&gt;hypothesis for faithfulness, reference-&gt;hypothesis for precision, hypothesis-&gt;reference for recall, and arithmetic mean for F-score).",
            "evaluation_criteria": "Correlations to human judgments across evaluation perspectives: Informativeness, Relevance, Fluency, Coherence, Factuality, Semantic Coverage (pyramid), Adequacy; numerical meta-evaluation via Spearman, Pearson, Kendall's Tau, and Accuracy (for pairwise factuality).",
            "llm_model_name": "BART (pretrained seq2seq), with optional fine-tuned variants (BART-CNN, BART-CNN-PARA).",
            "theory_domain": "Natural Language Processing â€” automated evaluation of generated text (machine translation, summarization, data-to-text).",
            "theory_description": "Not a scientific theory; rather, a generative-probability-based metric to evaluate the quality of model-generated texts from multiple perspectives by treating evaluation as a conditional text-generation task.",
            "evaluation_results": "BARTScore (vanilla and variants) outperforms or matches top existing metrics in many settings: overall best performance in 16 of 22 test settings reported; robust correlations with human judgments across 16 datasets and 7 perspectives. Examples: BARTSCORE + CNN achieved high performance on factuality and summarization tasks; vanilla BARTScore already substantially outperformed embedding/matching baselines on many summarization settings.",
            "benchmarks_or_datasets": "Used across multiple datasets: WMT19 (MT language pairs de-en, fi-en, gu-en, kk-en, lt-en, ru-en, zh-en), REALSumm, SummEval, NeR18, Rank19, QAGS (CNN and XSUM), NEWSROOM, CNNDM (for fine-tuning), DARR (WMT), BAGEL, SFHOT, SFRES (data-to-text), ParaBank2 (for paraphrase fine-tuning).",
            "comparison_to_human": "Meta-evaluation: correlation (Spearman/Pearson/Kendall) between BARTScore and human judgments; in some factuality settings BARTScore+CNN matched or nearly matched human baselines (e.g., Rank19 accuracy 0.836 vs human 0.839). Overall, BARTScore variants produce high correlation with human scores and often outperform other unsupervised metrics and approach supervised methods on some tasks.",
            "limitations_or_challenges": "Designed for text-generation evaluation, not specifically for evaluating scientific theories; may be biased toward abstractive-generation setups (less effective on extractive systems), prompting helps semantic-overlap evaluation but shows inconsistent or negative effects on factuality, and fine-tuning choices (e.g., paraphrase fine-tuning) can harm factuality-oriented evaluations.",
            "uuid": "e6182.0",
            "source_info": {
                "paper_title": "BARTScore: Evaluating Generated Text as Text Generation",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "BARTScore-Prompt",
            "name_full": "BARTScore with Prompting (BARTSCORE-PROMPT)",
            "brief_description": "A variant of BARTScore that prepends or appends short textual prompts to the source or target to bring the evaluation task closer to pretraining and improve metric effectiveness; can ensemble scores over multiple prompts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Add a short prompt phrase to the source or target text (prefix or suffix) before computing log-probabilities under BART; average the generation scores over a set of prompts (prompt ensembling).",
            "evaluation_criteria": "Same correlation-based criteria vs human judgments (Spearman, Pearson, Kendall); measured per perspective to assess whether prompting improves semantic overlap, fluency, or factuality.",
            "llm_model_name": "BART (pretrained seq2seq), same as base BARTScore.",
            "theory_domain": "NLP text-generation evaluation (summarization, MT, data-to-text).",
            "theory_description": "Not a scientific theory; a protocol to modify inputs/outputs of the metric via natural-language prompts to improve evaluation alignment with human judgments.",
            "evaluation_results": "Prompting improves semantic-overlap/informativeness evaluations consistently (prompt ensembling improves Spearman correlations on SummEval and NeR18); a single chosen prompt ('Such as') improved Kendall's Tau by 0.033 on de-en MT, in one case enabling BARTScore to surpass supervised metrics (BLEURT/COMET) on that language pair. However, prompts often do not improve and may degrade factuality scores.",
            "benchmarks_or_datasets": "Evaluated using the prompt set across SummEval, REALSumm, NeR18, and WMT (development WMT18 used to search for best prompt 'Such as').",
            "comparison_to_human": "Prompt-augmented scores show higher correlations with human judgments for semantic-overlap perspectives; inconsistent for factuality and linguistic quality.",
            "limitations_or_challenges": "Prompt choice is dataset/task sensitive; many prompts help semantic overlap but few help factuality; prompt search strategies can be expensive and discovered prompts may not generalize.",
            "uuid": "e6182.1",
            "source_info": {
                "paper_title": "BARTScore: Evaluating Generated Text as Text Generation",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "BARTScore-CNN / BARTScore-CNN-PARA",
            "name_full": "BARTScore fine-tuned on CNN/DailyMail (CNN) and further on ParaBank2 (CNN-PARA)",
            "brief_description": "Variants of BARTScore where the underlying BART model is fine-tuned on downstream generation tasks: (1) fine-tuned on CNN/DailyMail summarization improves summarization-related correlations; (2) further fine-tuning on paraphrase data (ParaBank2) can help some tasks but may hurt factuality.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Replace base BART parameters with those fine-tuned on summarization (CNNDM) and/or paraphrase data, then compute BARTScore as usual to better align metric with evaluation tasks.",
            "evaluation_criteria": "Same correlation-based metrics vs human judgments (Spearman/Pearson/Kendall) across summarization, factuality, and data-to-text tasks.",
            "llm_model_name": "BART fine-tuned on CNNDM (BART-CNN) and optionally further fine-tuned on ParaBank2 (BART-CNN-PARA).",
            "theory_domain": "NLP text-generation evaluation (summarization, factuality, data-to-text).",
            "theory_description": "Not a scientific theory; method adapts base metric by tuning the underlying model to a related generation task to improve metric-task alignment.",
            "evaluation_results": "Fine-tuning on CNNDM consistently boosts correlations (e.g., improved Spearman on summarization and data-to-text). BARTSCORE-CNN achieved top results on factuality datasets (Rank19 and QAGS), e.g., Rank19 accuracy 0.836 (significantly better than other unsupervised metrics). Further paraphrase fine-tuning improved some data-to-text correlations (e.g., BAGEL up to +0.083) but reduced performance on factuality datasets.",
            "benchmarks_or_datasets": "CNNDM (used for fine-tuning), ParaBank2 (paraphrase fine-tuning), evaluated on REALSumm, SummEval, NeR18, Rank19, QAGS, BAGEL, SFRES, SFHOT.",
            "comparison_to_human": "Fine-tuned variants increase correlation with human judgments, sometimes matching or approaching human baselines on factuality tasks (Rank19).",
            "limitations_or_challenges": "Choice of fine-tuning data matters: paraphrase fine-tuning can reduce factuality performance (because summaries and documents are not paraphrases); model specialization may reduce generality across perspectives.",
            "uuid": "e6182.2",
            "source_info": {
                "paper_title": "BARTScore: Evaluating Generated Text as Text Generation",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "Evaluation Perspectives (Criteria)",
            "name_full": "Human evaluation perspectives: Informativeness, Relevance, Fluency, Coherence, Factuality, Semantic Coverage, Adequacy",
            "brief_description": "Set of human-judgment axes used as gold standards for meta-evaluation of automated metrics; each perspective measures a distinct facet of generated text quality (e.g., factuality = entailment from source; informativeness = key idea coverage).",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_method": "Human annotators provide judgments per perspective (e.g., Likert-scale ratings, pyramid recall, pairwise correctness), which are treated as gold-standard labels for meta-evaluation of automated metrics.",
            "evaluation_criteria": "The seven explicit criteria: Informativeness (INFO), Relevance (REL), Fluency (FLU), Coherence (COH), Factuality (FAC), Semantic Coverage (Cov/pyramid), Adequacy (ADE).",
            "llm_model_name": null,
            "theory_domain": "NLP human evaluation protocols for generated text.",
            "theory_description": "Human-annotated judgments defining ground-truth evaluations used to compute correlations for automated metrics.",
            "evaluation_results": "All automated metrics are evaluated by correlation to these human judgments using Spearman, Pearson, Kendall's Tau, and accuracy (for pairwise factual correctness). The paper reports detailed correlations per perspective on datasets like SummEval (Coherence, Factuality, Fluency, Informativeness), REALSumm (pyramid coverage), and Rank19/QAGS (factuality).",
            "benchmarks_or_datasets": "SummEval, REALSumm, NeR18, Rank19, QAGS, NEWSROOM used as human-judgment sources.",
            "comparison_to_human": "Human judgments are the gold standard; automated metrics are compared by correlation measures; in some cases automated metrics approach human-level agreement.",
            "limitations_or_challenges": "Human annotation is expensive and perspective definitions vary across datasets (e.g., 'consistency' vs 'factuality'); some perspectives (factuality) are harder for prompt-based metric improvements.",
            "uuid": "e6182.3",
            "source_info": {
                "paper_title": "BARTScore: Evaluating Generated Text as Text Generation",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "Baseline Metrics",
            "name_full": "Baseline automatic evaluation metrics (BLEU, ROUGE, CHRF, BERTScore, MoverScore, PRISM, BLEURT, COMET, FactCC, QAGS)",
            "brief_description": "Collection of widely used metrics for evaluating generated text: n-gram match (BLEU, ROUGE, CHRF), embedding-based matching (BERTScore, MoverScore), paraphrase/translation-based (PRISM), and supervised learned metrics (BLEURT, COMET); also factuality-specific methods (FactCC, QAGS).",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_method": "Varies by metric: token/character n-gram overlaps, embedding similarity and matching, paraphrase probability, supervised regression/ranking trained on human judgments, and question-answering based factuality checks.",
            "evaluation_criteria": "Typically aim to measure adequacy/semantic overlap (BLEU, ROUGE), semantic similarity (BERTScore, MoverScore), paraphrastic equivalence (PRISM), or learned regressors correlated with human scores (BLEURT, COMET); factuality tools detect contradictions or verify statements against source (FactCC, QAGS).",
            "llm_model_name": null,
            "theory_domain": "NLP automated evaluation metrics for MT, summarization, data-to-text.",
            "theory_description": "Baselines that BARTScore is compared against in meta-evaluation experiments.",
            "evaluation_results": "BARTScore often outperforms unsupervised baselines (BERTScore, MoverScore, PRISM) across many settings and sometimes matches or surpasses supervised baselines (BLEURT, COMET) when using prompting/fine-tuning on specific language pairs or datasets.",
            "benchmarks_or_datasets": "Compared across WMT19, REALSumm, SummEval, NeR18, Rank19, QAGS, BAGEL, SFRES, SFHOT.",
            "comparison_to_human": "Baselines evaluated by correlation to human judgments; supervised metrics trained on human judgments sometimes outperform unsupervised ones, but BARTScore narrows the gap.",
            "limitations_or_challenges": "Each baseline has limited applicability to certain perspectives/tasks; supervised ones require human-labeled data; PRISM trained from scratch on parallel data whereas BARTScore uses pre-trained models; performance varies by language pair and task.",
            "uuid": "e6182.4",
            "source_info": {
                "paper_title": "BARTScore: Evaluating Generated Text as Text Generation",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "Benchmarks / Datasets",
            "name_full": "Evaluation datasets and benchmarks used in meta-evaluation",
            "brief_description": "Concrete datasets and shared tasks used as testbeds for metric meta-evaluation, spanning machine translation (WMT DARR), summarization (REALSumm, SummEval, NeR18, NEWSROOM, CNNDM), factuality (Rank19, QAGS), and data-to-text (BAGEL, SFHOT, SFRES).",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_method": "Use human-annotated judgments or task-specific gold standards from these datasets to compute metric-to-human correlations (Spearman/Pearson/Kendall) and accuracy for pairwise factuality.",
            "evaluation_criteria": "Dataset-specific gold standards include pyramid recall (REALSumm), per-aspect human ratings (SummEval, NeR18), pairwise factual correctness (Rank19), question-answering based factual consistency (QAGS), and MT system rankings (WMT19).",
            "llm_model_name": null,
            "theory_domain": "NLP evaluation: MT, summarization, data-to-text, factuality.",
            "theory_description": "Datasets provide human judgments and system outputs used to measure automated metrics' alignment with humans.",
            "evaluation_results": "BARTScore and variants were evaluated across these datasets; main reported outcomes include per-dataset Spearman/Pearson/Kendall correlations and accuracy; e.g., BARTScore variants achieved top results across many of these benchmarks.",
            "benchmarks_or_datasets": "WMT19 (DARR), WMT18 (development for prompt search), REALSumm, SummEval, NeR18, Rank19, QAGS (CNN and XSUM), NEWSROOM, CNNDM (for fine-tuning), BAGEL, SFHOT, SFRES, ParaBank2 (fine-tuning data).",
            "comparison_to_human": "Human annotations in these datasets are treated as gold standards; metric performance is reported as correlation to those annotations.",
            "limitations_or_challenges": "Dataset differences (e.g., number of systems, annotation schemes) impact metric comparability; some datasets have few systems (NeR18) making gains harder to measure; prompt search done on WMT18 may not generalize.",
            "uuid": "e6182.5",
            "source_info": {
                "paper_title": "BARTScore: Evaluating Generated Text as Text Generation",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "Meta-evaluation Measures",
            "name_full": "Statistical measures for metric meta-evaluation and significance testing",
            "brief_description": "Standard statistical measures used to quantify agreement between automated metrics and human judgments (Spearman correlation, Pearson correlation, Kendall's Tau, Accuracy), plus bootstrap-based pairwise significance testing.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_method": "Compute Spearman (rank-order monotonic), Pearson (linear), Kendall's Tau (ordinal association) correlations between metric scores and human judgments; compute accuracy for pairwise factual comparisons; use bootstrap resampling to test pairwise significance (p &lt; 0.05).",
            "evaluation_criteria": "Correlation coefficients and statistical significance (bootstrapped p-values) indicate how well metrics match human judgments.",
            "llm_model_name": null,
            "theory_domain": "Statistical evaluation and meta-evaluation methodology for NLP metrics.",
            "theory_description": "Procedures and significance tests used to validate comparison of automated metrics to human annotations.",
            "evaluation_results": "Reported correlations and significance markings across tables (e.g., Kendall's Tau on WMT19, Spearman on summarization/data-to-text); used bootstrapping for pairwise tests with p &lt; 0.05.",
            "benchmarks_or_datasets": "Applied to WMT19, SummEval, REALSumm, NeR18, Rank19, QAGS, BAGEL, SFHOT, SFRES.",
            "comparison_to_human": "These measures quantify agreement between automated metric outputs and human judgments (gold standards).",
            "limitations_or_challenges": "Correlation coefficients summarize agreement but may mask per-system or per-instance failures; significance testing depends on dataset size and variance.",
            "uuid": "e6182.6",
            "source_info": {
                "paper_title": "BARTScore: Evaluating Generated Text as Text Generation",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "Key empirical findings",
            "name_full": "Summary of key experimental results and numeric benchmarks",
            "brief_description": "Concise summary of the major quantitative outcomes: BARTScore variants achieve top performance in the majority of tested settings, prompting and fine-tuning can materially improve correlations in specific scenarios, and BARTScore is robust to high-quality system outputs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Empirical meta-evaluation across datasets using correlations to human judgments and accuracy on factuality; ablations over prompting and fine-tuning.",
            "evaluation_criteria": "Spearman/Pearson/Kendall correlations and pairwise accuracy; significance via bootstrapping.",
            "llm_model_name": "BART and BART fine-tuned variants (BART-CNN, BART-CNN-PARA).",
            "theory_domain": "NLP text-generation evaluation.",
            "theory_description": "Not a scientific theory; empirical benchmarks demonstrating metric performance.",
            "evaluation_results": "Aggregate: BARTScore outperforms existing top-scoring metrics in 16 of 22 test settings. Examples: On WMT19 average Kendall's Tau improved with BARTSCORE+CNN+Para to ~0.331 and further to 0.337 with prompting; on SummEval/REALSumm/NeR18 BARTSCORE variants yield the highest average Spearman correlations (e.g., average ~0.518 with best prompt/ensembling). On Rank19 factuality BARTSCORE+CNN achieves accuracy 0.836 (ddagger: significantly better than other unsupervised metrics), and QAGS Pearson up to 0.735. Prompt 'Such as' gave +0.033 Kendall's Tau on de-en MT. Data-to-text: BARTSCORE+CNN+Para+Prompt achieved average Spearman ~0.270, outperforming baselines.",
            "benchmarks_or_datasets": "WMT19, WMT18 (prompt dev), SummEval, REALSumm, NeR18, Rank19, QAGS, BAGEL, SFRES, SFHOT.",
            "comparison_to_human": "Reports direct correlations to human judgments; in some cases variant metrics approach human agreement levels (e.g., Rank19 human 0.839 vs BART+CNN 0.836).",
            "limitations_or_challenges": "Performance gains are dataset- and perspective-dependent; prompting and fine-tuning can help or hurt depending on whether the perspective is semantic overlap vs factuality; biases found (better discrimination for abstractive than extractive systems).",
            "uuid": "e6182.7",
            "source_info": {
                "paper_title": "BARTScore: Evaluating Generated Text as Text Generation",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "Bias & limitations analysis",
            "name_full": "Bias analysis and noted limitations of BARTScore-based evaluation",
            "brief_description": "Empirical analyses showing BARTScore's relative weaknesses and potential biases: less effective discrimination for extractive summarizers, inconsistent prompt effects on factuality, sensitivity to fine-tuning data choices.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Rank-difference analysis on REALSumm systems and per-bucket breakdowns (top-k systems, reference length buckets); prompt-effect percentage analysis per perspective; dataset-specific ablations for fine-tuning.",
            "evaluation_criteria": "Rank difference versus human ranks; correlations per bucket; percent of prompts that improve performance per perspective.",
            "llm_model_name": "BART and variants used for analysis.",
            "theory_domain": "NLP evaluation reliability and bias analysis.",
            "theory_description": "Not a scientific theory; identifies where a probability-based metric may systematically under- or over-rank systems.",
            "evaluation_results": "BARTScore is less effective distinguishing extractive systems (rank differences biased) and is more robust for high-quality (top-k) systems; prompt ensembling greatly helps semantic-overlap measures but rarely helps factuality; paraphrase fine-tuning harms factuality evaluations.",
            "benchmarks_or_datasets": "REALSumm (bias/rank differences), SummEval, NeR18 (prompt analyses), Rank19, QAGS (factuality ablations).",
            "comparison_to_human": "Bias analysis compares BARTScore system rankings to human-derived rankings and highlights systematic disparities (especially on extractive systems).",
            "limitations_or_challenges": "Bias toward abstractive generation; difficulty improving factuality via prompting; dependency on fine-tuning data selection; not validated for domains like scientific-theory evaluation.",
            "uuid": "e6182.8",
            "source_info": {
                "paper_title": "BARTScore: Evaluating Generated Text as Text Generation",
                "publication_date_yy_mm": "2021-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Automatic machine translation evaluation in many languages via zero-shot paraphrasing",
            "rating": 2
        },
        {
            "paper_title": "BLEURT: Learning robust metrics for text generation",
            "rating": 2
        },
        {
            "paper_title": "COMET: A neural framework for MT evaluation",
            "rating": 2
        },
        {
            "paper_title": "Bertscore: Evaluating text generation with bert",
            "rating": 2
        },
        {
            "paper_title": "MoverScore: Text generation evaluating with contextualized embeddings and earth mover distance",
            "rating": 2
        },
        {
            "paper_title": "Asking and answering questions to evaluate the factual consistency of summaries",
            "rating": 2
        },
        {
            "paper_title": "Evaluating the factual consistency of abstractive text summarization",
            "rating": 2
        }
    ],
    "cost": 0.019645,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>BARTSCORE: Evaluating Generated Text as Text Generation</h1>
<p>Weizhe Yuan<br>Carnegie Mellon University<br>weizhey@cs.cmu.edu<br>Graham Neubig<br>Carnegie Mellon University<br>gneubig@cs.cmu.edu<br>Pengfei Liu *<br>Carnegie Mellon University<br>pliu3@cs.cmu.edu</p>
<h4>Abstract</h4>
<p>A wide variety of NLP applications, such as machine translation, summarization, and dialog, involve text generation. One major challenge for these applications is how to evaluate whether such generated texts are actually fluent, accurate, or effective. In this work, we conceptualize the evaluation of generated text as a text generation problem, modeled using pre-trained sequence-to-sequence models. The general idea is that models trained to convert the generated text to/from a reference output or the source text will achieve higher scores when the generated text is better. We operationalize this idea using BART [32], an encoder-decoder based pre-trained model, and propose a metric BARTSCORE with a number of variants that can be flexibly applied in an unsupervised fashion to evaluation of text from different perspectives (e.g. informativeness, fluency, or factuality). BARTSCORE is conceptually simple and empirically effective. It can outperform existing top-scoring metrics in 16 of 22 test settings, covering evaluation of 16 datasets (e.g., machine translation, text summarization) and 7 different perspectives (e.g., informativeness, factuality). Code to calculate BARTScore is available at https://github.com/neulab/BARTScore, and we have released an interactive leaderboard for meta-evaluation at http: //explainaboard.nlpedia.ai/leaderboard/task-meval/ on the EXPLAINABOARD platform [38], which allows us to interactively understand the strengths, weaknesses, and complementarity of each metric.</p>
<h2>1 Introduction</h2>
<p>One defining feature of recent NLP models is the use of neural representations trained on raw text, using unsupervised objectives such as language modeling [6,54], or denoising autoencoding [9,32,55]. By learning to predict the words or sentences in natural text, these models simultaneously learn to extract features that not only benefit mainstream NLP tasks such as information extraction [23,37], question answering $[1,26]$, text summarization $[40,78]$ but also have proven effective in development of automatic metrics for evaluation of text generation itself [63,66]. For example, BERTScore [76] and MoverScore [77] take features extracted by BERT [9] and apply unsupervised matching functions to compare system outputs against references. Other works build supervised frameworks that use the extracted features to learn to rank [57] or regress [63] to human evaluation scores.</p>
<p>However, in the context of generation evaluation, one may note that there is a decided disconnect between how models are pre-trained using text generation objectives and how they are used as down-stream feature extractors. This leads to potential under-utilization of the pre-trained model parameters. For example, the output prediction layer is not used at all in this case. This disconnect is particularly striking because of the close connection between the pre-training objectives and the generation tasks we want to evaluate.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>In this paper, we instead argue for a formulation of evaluation of generated text as a text generation problem, directly evaluating text through the lens of its probability of being generated from or generating other textual inputs and outputs. This is a better match with the underlying pre-training tasks and allows us to more fully take advantage of the parameters learned during the pre-training phase. We solve the modeling problem with a pre-trained sequence-to-sequence (seq2seq) model, specifically BART [32], and devise a metric named BARTSCORE, which has the following characteristics: (1) BARTSCORE is parameter- and data-efficient. Architecturally there are no extra parameters beyond those used in pre-training itself, and it is an unsupervised metric that doesnâ€™t require human judgments to train. (2) BARTSCORE can better support evaluation of generated text from different perspectives (e.g., informativeness, coherence, factuality, Â§4) by adjusting the inputs and outputs of the conditional text generation problem, as we demonstrate in Â§3.2. This is in contrast to most previous work, which mostly examines correlation of the devised metrics with output quality from a limited number of perspectives. (3) BARTSCORE can be further enhanced by (i) providing textual prompts that bring the evaluation task closer to the pre-training task, or (ii) updating the underlying model by fine-tuning BART based on downstream generation tasks (e.g., text summarization).</p>
<p>Experimentally, we evaluate different variants of BARTSCORE from 7 perspectives on 16 datasets. BARTSCORE achieves the best performance in 16 of 22 test settings against existing top-scoring metrics. Empirical results also show the effectiveness of the prompting strategy supported by BARTSCORE. For example, simply adding the phrase "such as" to the translated text when using BARTSCORE can lead to a $3$ point absolute improvement in correlation on "German-English" machine translation (MT) evaluation. Additional analysis shows that BARTSCORE is more robust when dealing with high-quality texts generated by top-performing systems.</p>
<h1>2 Preliminaries</h1>
<h3>2.1 Problem Formulation</h3>
<p>As stated above, our goal is to assess the quality of generated text [3,47]. In this work, we focus on conditional text generation (e.g., machine translation), where the goal is to generate a hypothesis $\left(\boldsymbol{h}=h_{1}, \cdots, h_{m}\right)$ based on a given source text $\left(\boldsymbol{s}=s_{1}, \cdots, s_{n}\right)$. Commonly, one or multiple human-created references $\left(\boldsymbol{r}=r_{1}, \cdots, r_{l}\right)$ are provided to aid this evaluation.</p>
<h3>2.2 Gold-standard Human Evaluation</h3>
<p>In general, the gold-standard method for evaluating such texts is still human evaluation, where human annotators assess the generated textsâ€™ quality. This evaluation can be done from perspectives, and we list a few common varieties below (all are investigated in Â§4):</p>
<ol>
<li>Informativeness (INFO): How well the generated hypothesis captures the key ideas of the source text [18].</li>
<li>Relevance (REL): How consistent the generated hypothesis is with respect to the source text [19].</li>
<li>Fluency (FLU): Whether the text has no formatting problems, capitalization errors or obviously ungrammatical sentences (e.g., fragments, missing components) that make the text difficult to read [13].</li>
<li>Coherence (COH): Whether the text builds from sentence to sentence to a coherent body of information about a topic [7].</li>
<li>Factuality (FAC): Whether the generated hypothesis contains only statements entailed by the source text [30].</li>
<li>Semantic Coverage (Cov): How many semantic content units from reference texts are covered by the generated hypothesis [50].</li>
<li>Adequacy (ADE): Whether the output conveys the same meaning as the input sentence, and none of the message is lost, added, or distorted [29].</li>
</ol>
<p>Most existing evaluation metrics were designed to cover a small subset of these perspectives. For example, BLEU [51] aims to capture the adequacy and fluency of translations, while ROUGE [36] was designed to match the semantic coverage metric. Some metrics, particularly trainable ones, can perform evaluation from different perspectives but generally require maximizing correlation with each type of judgment separately [8].</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Evaluation metrics as different tasks, where $s_{i}, h_{i}$ and $r_{j}$ represent source, hypothesis and reference words respectively.</p>
<p>As we describe more in $\S 4$, BARTSCORE can evaluate text from the great majority of these perspectives, significantly expanding its applicability compared to these metrics.</p>
<h1>2.3 Evaluation as Different Tasks</h1>
<p>There is a recent trend that leverages neural models for automated evaluation in different ways, as shown in Fig. 1. We first elaborate on their characteristics by highlighting differences in task formulation and evaluation perspectives.</p>
<p>T1: Unsupervised Matching. Unsupervised matching metrics aim to measure the semantic equivalence between the reference and hypothesis by using a token-level matching functions in distributed representation space, such as BERTScore [76], MoverScore [77] or discrete string space like ROUGE [35], BLEU [51], CHRF [53]. Although similar matching functions can be used to assess the quality beyond semantic equivalence (e.g, factuality, a relationship between source text and hypothesis), to our knowledge prior research has not attested to the capability of unsupervised matching methods in this regard; we explore this further in our experiments (Tab. 5).</p>
<p>T2: Supervised Regression. Regression-based models introduce a parameterized regression layer, which would be learned in a supervised fashion to accurately predict human judgments. Examples include recent metrics BLEURT [63], COMET [57] and traditional metrics like $S^{3}$ [52], VRM [21].</p>
<p>T3: Supervised Ranking. Evaluation can also be conceived as a ranking problem, where the main idea is to learn a scoring function that assigns a higher score to better hypotheses than to worse ones. Examples include COMET [57] and BEER [65], where COMET focuses the machine translation task and relies on human judgments to tune parameters in ranking or regression layers, and BEER combines many simple features in a tunable linear model of MT evaluation metrics.</p>
<p>T4: Text Generation. In this work, we formulate evaluating generated text as a text generation task from pre-trained language models. The basic idea is that a high-quality hypothesis will be easily generated based on source or reference text or vice-versa. This has not been covered as extensively in previous work, with one notable exception being PRISM [66]. Our work differs from PRISM in several ways: (i) PRISM formulates evaluation as a paraphrasing task, whose definition that two texts are with the same meaning limits its applicable scenarios, like factuality evaluation in text summarization that takes source documents and generated summaries as input whose semantic space are different. (ii) PRISM trained a model from scratch on parallel data while BARTSCORE is based on open-sourced pre-trained seq2seq models. (iii) BARTSCORE supports prompt-based learning $[60,64]$ which hasn't been examined in PRISM.</p>
<h2>3 BARTScore</h2>
<h3>3.1 Sequence-to-Sequence Pre-trained Models</h3>
<p>Although pre-trained models differ along different axes, one of the main axes of variation is the training objective, with two main variants: language modeling objectives (e.g., masked language modeling [9]) and seq2seq objectives [55]. In particular, seq2seq pre-trained models are particularly well-suited to conditioned generation tasks since they consist of both an encoder and a decoder, and predictions are made auto-regressively [32]. In this work, we operationalize our idea by using</p>
<p>BART [32] as our backbone due to its superior performance in text generation [12, 42, 72]. We also report preliminary experiments comparing BART with T5 [55] and PEGASUS [75] in the Appendix.</p>
<p>Given a seq2seq model parameterized by $\theta$, a source sequence containing $n$ tokens $\mathbf{x}=\left{x_{1}, \cdots, x_{n}\right}$ and a target sequence containing $m$ tokens $\mathbf{y}=\left{y_{1}, \cdots, y_{m}\right}$. We can factorize the generation probability of $\mathbf{y}$ conditioned on $\mathbf{x}$ as follows:</p>
<p>$$
p(\mathbf{y} \mid \mathbf{x}, \theta)=\prod_{t=1}^{m} p\left(\mathbf{y}<em _t="&lt;t">{t} \mid \mathbf{y}</em>, \theta\right)
$$}, \mathbf{x</p>
<p>By exploring these probabilities, we design metrics that can gauge the quality of the generated text.</p>
<h1>3.2 BARTScore</h1>
<p>The most general form of our proposed BARTSCORE is shown in Eq. 2, where we use the weighted $\log$ probability of one text $\mathbf{y}$ given another text $\mathbf{x}$. The weights are used to put different emphasis on different tokens, which can be instantiated using different methods like Inverse Document Frequency (IDF) [25] etc. In our work, we weigh each token equally. ${ }^{2}$</p>
<p>$$
\operatorname{BARTSCORE}=\sum_{t=1}^{m} \omega_{t} \log p\left(\mathbf{y}<em _t="&lt;t">{t} \mid \mathbf{y}</em>, \theta\right)
$$}, \mathbf{x</p>
<p>Due to its generation task-based formulation and ability to utilize the entirety of BART's pre-trained parameters, BARTSCORE can be flexibly used in different evaluation scenarios. We specifically present four methods for using BARTSCORE based on different generation directions, which are,</p>
<ul>
<li>Faithfulness $(\boldsymbol{s} \rightarrow \boldsymbol{h})$ : from source document to hypothesis $p(\boldsymbol{h} \mid \boldsymbol{s}, \theta)$. This direction measures how likely it is that the hypothesis could be generated based on the source text. Potential application scenarios are factuality and relevance introduced in $\S 2.2$. This measure can also be used for estimating measures of the quality of only the target text, such as coherence and fluency (Â§2.2).</li>
<li>Precision $(\boldsymbol{r} \rightarrow \boldsymbol{h})$ : from reference text to system-generated text $p(\boldsymbol{h} \mid \boldsymbol{r}, \theta)$. This direction assesses how likely the hypothesis could be constructed based on the gold reference and is suitable for the precision-focused scenario.</li>
<li>Recall $(\boldsymbol{h} \rightarrow \boldsymbol{r})$ : from system-generated text to reference text $p(\boldsymbol{r} \mid \boldsymbol{h}, \theta)$. This version quantifies how easily a gold reference could be generated by the hypothesis and is suitable for pyramid-based evaluation (i.e., semantic coverage introduced in $\S 2.2$ ) in summarization task since pyramid score measures fine-grained Semantic Content Units (SCUs) [50] covered by system-generated texts.</li>
<li>$\mathcal{F}$ score $(\boldsymbol{r} \leftrightarrow \boldsymbol{h})$ : Consider both directions and use the arithmetic average of Precision and Recall ones. This version can be broadly used to evaluate the semantic overlap (informativeness, adequacy detailed in $\S 2.2$ ) between reference texts and generated texts.</li>
</ul>
<h3>3.3 BARTScore Variants</h3>
<p>We also investigate two extensions to BARTSCORE: (i) changing $\mathbf{x}$ and $\mathbf{y}$ through prompting, which can bring the evaluation task closer to the pre-training task. (ii) changing $\theta$ by considering different fine-tuning tasks, which can bring the pre-training domain closer to the evaluation task.</p>
<h3>3.3.1 Prompt</h3>
<p>Prompting is a practice of adding short phrases to the input or output to encourage pre-trained models to perform specific tasks, which has been proven effective in several other NLP scenarios [24,58,59,61,64]. The generative formulation of BARTSCORE makes it relatively easy to incorporate these insights here as well; we name this variant BARTSCORE-PROMPT.</p>
<p>Given a prompt of $l$ tokens $\mathbf{z}=\left{z_{1}, \cdots, z_{l}\right}$, we can either (i) append it to the source text, in which case we get $\mathbf{x}^{\prime}=\left{x_{1}, \cdots, x_{n}, z_{1}, \cdots, z_{l}\right}$, and calculate the score based on this new source text using Eq.2. or (ii) prepend it to the target text, getting $\mathbf{y}^{\prime}=\left{z_{1}, \cdots, z_{l}, y_{1}, \cdots, y_{m}\right}$. Then we can also use Eq. 2 given the new target text.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>3.3.2 Fine-tuning Task</h1>
<p>Different from BERT-based metrics, which typically use classification-based tasks (e.g., natural language inference) [68] to fine-tune, BARTSCORE can be fine-tuned using generation-based tasks, which will make the pre-training domain closer to the evaluation task. In this paper, we explore two downstream tasks. (1) Summarization. We use BART fine-tuned on CNNDM dataset [20], which is available off-the-shelf in Huggingface Transformers [71]. (2) Paraphrasing. We continue fine-tuning BART from (1) on ParaBank2 dataset [22], which contains a large paraphrase collection. We used a random subset of 30,000 data and fine-tuned for one epoch with a batch size of 20 and a learning rate of $5 e^{-5}$. We used two 2080Ti GPUs, and the training time is less than one hour.</p>
<h2>4 Experiment</h2>
<p>This section aims to evaluate the reliability of different automated metrics, which is commonly achieved by quantifying how well different metrics correlate with human judgments using measures (e.g., Spearman Correlation [73]) defined below (Â§4.1.2).</p>
<h3>4.1 Baselines and Datasets</h3>
<h3>4.1.1 Evaluation Metrics</h3>
<p>We comprehensively examine metrics outlined in $\S 2.3$, which either require human judgments to train (i.e., supervised metrics): COMET [57], BLEURT [63], or are human judgment-free (i.e., unsupervised): BLEU [51] ROUGE-1 and ROUGE-2, ROUGE-L, CHRF [53], PRISM [66], MoverScore [77], BERTScore [76]. The detailed comparisons of those metrics can be found in Appendix. We use the official code for each metric.</p>
<h3>4.1.2 Measures for Meta Evaluation</h3>
<p>Pearson Correlation [15] measures the linear correlation between two sets of data. Spearman Correlation [73] assesses the monotonic relationships between two variables. Kendall's Tau [27] measures the ordinal association between two measured quantities. Accuracy, in our experiments, measures the percentage of correct ranking between factual texts and non-factual texts. We follow previous works in the choices of measures for different datasets to make a fair comparison.</p>
<h3>4.1.3 Datasets</h3>
<p>The datasets we use are summarized in Tab. 1. We consider three different tasks: summarization (SUM), machine translation (MT), and data-to-text (D2T).
Machine Translation We obtain the source language sentences, machine-translated texts and reference texts from the WMT19 metrics shared task [44]. We use the DARR corpus and consider 7 language pairs, which are de-en, fi-en, gu-en, kk-en, lt-en, ru-en, zh-en.
Text Summarization (1) REALSumm [4] is a metaevaluation dataset for text summarization which measures pyramid recall of each system-generated summary. (2) SummEval [13] is a collection of human judgments of model-generated summaries on the CNNDM dataset annotated by both expert judges and crowd-source workers. Each system generated summary is gauged through the lens of coherence, consistency, fluency and relevance. ${ }^{3}$ (3) NeR18 The</p>
<p>Table 1: A summary of tasks, datasets, and evaluation perspectives that we have covered in our experiments. Explanation of evaluation perspectives can be found in $\S 2.2$.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Tasks</th>
<th style="text-align: center;">Datasets</th>
<th style="text-align: center;">Eval. Perspectives</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">SUM</td>
<td style="text-align: center;">REALSUM</td>
<td style="text-align: center;">Cov</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SummEval</td>
<td style="text-align: center;">CoH Fac Flu Info</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">NeR18</td>
<td style="text-align: center;">CoH Flu Rel Info</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Rank19 <br> QAGS-C <br> QAGS-X</td>
<td style="text-align: center;">FAC</td>
</tr>
<tr>
<td style="text-align: center;">MT</td>
<td style="text-align: center;">DE FI GU KK <br> IT RU ZH</td>
<td style="text-align: center;">ADE Flu</td>
</tr>
<tr>
<td style="text-align: center;">D2T</td>
<td style="text-align: center;">BAGEL <br> SFHOT <br> SFRES</td>
<td style="text-align: center;">Info</td>
</tr>
</tbody>
</table>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>NEWSROOM dataset [18] contains 60 articles with summaries generated by 7 different methods are annotated with human scores in terms of coherence, fluency, informativeness, relevance.</p>
<p>Factuality (1) Rank19 [14] is used to meta-evaluate factuality metrics. It is a collection of 373 triples of a source sentence with two summary sentences, one correct and one incorrect. (2) QAGS20 [67] collected 235 test outputs on CNNDM dataset from [16] and 239 test outputs on XSUM dataset [48] from BART fine-tuned on XSUM. Sentences in each summary are annotated with correctness scores w.r.t. factuality.</p>
<p>Data to Text We consider the following datasets which target utterance generation for spoken dialogue systems. (1) BAGEL [45] provides information about restaurants. (2) SFHOT [70] provides information about hotels in San Francisco. (3) SFRES [70] provides information about restaurants in San Francisco. They contain 202, 398, and 581 samples respectively, each sample consists of one meaning representation, multiple references, and utterances generated by different systems.</p>
<h1>4.2 Setup</h1>
<h3>4.2.1 Prompt Design</h3>
<p>To perform prompting, we first need to find proper prompts within a search space. Instead of considering a large discrete search space [64] ${ }^{4}$ or continuous search space [34], we use simple heuristics to narrow our search space. In particular, we use manually devised seed prompts and gather paraphrases to construct our prompt set. ${ }^{5}$ The seed prompts and some examples of paraphrased prompts are shown in Tab. 2. Details are listed in the Appendix.</p>
<p>Table 2: Seed prompts and examples of final prompts. "Number" denotes the size of our final prompt set that was acquired from the seed prompts.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Usage</th>
<th style="text-align: center;">Number</th>
<th style="text-align: center;">Seed</th>
<th style="text-align: center;">Example</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$\boldsymbol{s} \rightarrow \boldsymbol{h}$</td>
<td style="text-align: center;">70</td>
<td style="text-align: center;">in summary</td>
<td style="text-align: center;">in short, in a word, to sum up</td>
</tr>
<tr>
<td style="text-align: center;">$\boldsymbol{h} \leftrightarrow \boldsymbol{r}$</td>
<td style="text-align: center;">34</td>
<td style="text-align: center;">in other words</td>
<td style="text-align: center;">to rephrase it, that is to say, i.e.</td>
</tr>
</tbody>
</table>
<h3>4.2.2 Settings</h3>
<p>Variants. We consider four variants of BARTSCORE, which are (1) BARTSCORE, which uses the vanilla BART; (2) BARTSCORE-CNN, which uses the BART fine-tuned on the summarization dataset CNNDM; (3) BARTSCORE-CNN-PARA, where BART is first fine-tuned on CNNDM, then fine-tuned on ParaBank2. (4) BARTSCORE-PROMPT, which is enhanced by adding prompts.
Selection of Prompts. For the summarization and data-to-text tasks, we use all entries (either all prompts designed for $\boldsymbol{s} \rightarrow \boldsymbol{h}$ or all prompts designed for $\boldsymbol{h} \leftrightarrow \boldsymbol{r}$ depending on the BARTScore usage chosen) in the prompt set by prefixing the decoder input and getting different generation scores (calculated by Eq.2) for each hypothesis based on different prompts. We finally get the score for one hypothesis by taking the average of all its generation scores using different prompts ( [24]; details about prompt ensembling can be found in the Appendix). For the machine translation task, due to the more expensive computational cost brought by larger text sets, we first use WMT18 [43] as a development set to search for one best prompt and obtain the phrase "Such as", which is then used for the test language pairs.
Selection of BARTScore Usage. Although BARTSCORE can be used in different ways (shown in Â§3.2)), in different tasks, they can be chosen based on how targeted evaluation perspectives are defined (described in $\S 2.2$ ) as well as the types of tasks. Specifically, (i) For those datasets whose gold standard human evaluation are obtained based on recall-based pyramid method, we adopt recall-based BARTSCORE $(\boldsymbol{h} \rightarrow \boldsymbol{r})$. (ii) For those datasets whose human judgments focus on linguistic quality (coherence, fluency) and factual correctness (factuality), or the source and hypothesis texts are in the same modality (i.e., language), we use faithfulness-based BARTSCORE $(\boldsymbol{s} \rightarrow \boldsymbol{h})$. (iii) For data-to-text and machine translation tasks, to make a fair comparison, we use BARTSCORE with the F-score version that other existing works [66] have adopted when evaluating generated texts.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 3: Kendall's Tau correlation of different metrics on WMT19 dataset. The highest correlation for each language pair achieved by unsupervised method is bold, and the highest correlation overall is underlined. Avg. denotes the average correlation achieved by a metric across all language pairs.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">de-en</th>
<th style="text-align: left;">fi-en</th>
<th style="text-align: left;">gu-en</th>
<th style="text-align: left;">kk-en</th>
<th style="text-align: left;">lt-en</th>
<th style="text-align: left;">ru-en</th>
<th style="text-align: left;">zh-en</th>
<th style="text-align: left;">Avg.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">SUPERVISED METHODS</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">BLEURT</td>
<td style="text-align: left;">0.174</td>
<td style="text-align: left;">$\underline{0.374}$</td>
<td style="text-align: left;">0.313</td>
<td style="text-align: left;">0.372</td>
<td style="text-align: left;">0.388</td>
<td style="text-align: left;">0.220</td>
<td style="text-align: left;">0.436</td>
<td style="text-align: left;">$\underline{0.325}$</td>
</tr>
<tr>
<td style="text-align: left;">COMET</td>
<td style="text-align: left;">0.219</td>
<td style="text-align: left;">0.369</td>
<td style="text-align: left;">0.316</td>
<td style="text-align: left;">$\underline{0.378}$</td>
<td style="text-align: left;">$\underline{0.405}$</td>
<td style="text-align: left;">$\underline{0.226}$</td>
<td style="text-align: left;">$\underline{0.462}$</td>
<td style="text-align: left;">$\underline{0.339}$</td>
</tr>
<tr>
<td style="text-align: left;">UnSUPERVISED METHODS</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">BLEU</td>
<td style="text-align: left;">0.054</td>
<td style="text-align: left;">0.236</td>
<td style="text-align: left;">0.194</td>
<td style="text-align: left;">0.276</td>
<td style="text-align: left;">0.249</td>
<td style="text-align: left;">0.115</td>
<td style="text-align: left;">0.321</td>
<td style="text-align: left;">0.206</td>
</tr>
<tr>
<td style="text-align: left;">CHRF</td>
<td style="text-align: left;">0.123</td>
<td style="text-align: left;">0.292</td>
<td style="text-align: left;">0.240</td>
<td style="text-align: left;">0.323</td>
<td style="text-align: left;">0.304</td>
<td style="text-align: left;">0.177</td>
<td style="text-align: left;">0.371</td>
<td style="text-align: left;">0.261</td>
</tr>
<tr>
<td style="text-align: left;">PRISM</td>
<td style="text-align: left;">0.199</td>
<td style="text-align: left;">0.366</td>
<td style="text-align: left;">$\underline{0.320}$</td>
<td style="text-align: left;">0.362</td>
<td style="text-align: left;">0.382</td>
<td style="text-align: left;">0.220</td>
<td style="text-align: left;">0.434</td>
<td style="text-align: left;">0.326</td>
</tr>
<tr>
<td style="text-align: left;">BERTScore</td>
<td style="text-align: left;">0.190</td>
<td style="text-align: left;">0.354</td>
<td style="text-align: left;">0.292</td>
<td style="text-align: left;">0.351</td>
<td style="text-align: left;">0.381</td>
<td style="text-align: left;">$\mathbf{0 . 2 2 1}$</td>
<td style="text-align: left;">0.430</td>
<td style="text-align: left;">0.317</td>
</tr>
<tr>
<td style="text-align: left;">BARTSCORE</td>
<td style="text-align: left;">0.156</td>
<td style="text-align: left;">0.335</td>
<td style="text-align: left;">0.273</td>
<td style="text-align: left;">0.324</td>
<td style="text-align: left;">0.322</td>
<td style="text-align: left;">0.167</td>
<td style="text-align: left;">0.389</td>
<td style="text-align: left;">0.281</td>
</tr>
<tr>
<td style="text-align: left;">+ CNN</td>
<td style="text-align: left;">0.190</td>
<td style="text-align: left;">0.365</td>
<td style="text-align: left;">0.300</td>
<td style="text-align: left;">0.348</td>
<td style="text-align: left;">0.384</td>
<td style="text-align: left;">0.208</td>
<td style="text-align: left;">0.425</td>
<td style="text-align: left;">0.317</td>
</tr>
<tr>
<td style="text-align: left;">+ CNN + Para</td>
<td style="text-align: left;">$0.205 \dagger$</td>
<td style="text-align: left;">$0.370 \dagger$</td>
<td style="text-align: left;">0.316</td>
<td style="text-align: left;">$\underline{0.378} \dagger$</td>
<td style="text-align: left;">$\mathbf{0 . 3 8 6} \dagger$</td>
<td style="text-align: left;">0.219</td>
<td style="text-align: left;">$0.442 \dagger$</td>
<td style="text-align: left;">0.331</td>
</tr>
<tr>
<td style="text-align: left;">+ CNN + Para + Prompt</td>
<td style="text-align: left;">$\underline{0.238}$</td>
<td style="text-align: left;">$\underline{0.374}$</td>
<td style="text-align: left;">0.318</td>
<td style="text-align: left;">$0.376 \dagger$</td>
<td style="text-align: left;">$\mathbf{0 . 3 8 6} \dagger$</td>
<td style="text-align: left;">0.219</td>
<td style="text-align: left;">$\mathbf{0 . 4 4 7} \dagger$</td>
<td style="text-align: left;">$\mathbf{0 . 3 3 7}$</td>
</tr>
</tbody>
</table>
<p>Table 4: Spearman correlation of different metrics on three human judgement datasets. For promptbased learning, we consider adding prompts to the best-performing BARTSCORE ( $\Omega$ ) on each dataset. The highest correlation overall for each aspect on each dataset is bold.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">REALSumm</th>
<th style="text-align: center;">SummEval</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">NeR18</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Avg.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Cov</td>
<td style="text-align: center;">CoH</td>
<td style="text-align: center;">FAC</td>
<td style="text-align: center;">FLU</td>
<td style="text-align: center;">INFO</td>
<td style="text-align: center;">CoH</td>
<td style="text-align: center;">FLU</td>
<td style="text-align: center;">INFO</td>
<td style="text-align: center;">REL</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">ROUGE-1</td>
<td style="text-align: center;">0.498</td>
<td style="text-align: center;">0.167</td>
<td style="text-align: center;">0.160</td>
<td style="text-align: center;">0.115</td>
<td style="text-align: center;">0.326</td>
<td style="text-align: center;">0.095</td>
<td style="text-align: center;">0.104</td>
<td style="text-align: center;">0.130</td>
<td style="text-align: center;">0.147</td>
<td style="text-align: center;">0.194</td>
</tr>
<tr>
<td style="text-align: center;">ROUGE-2</td>
<td style="text-align: center;">0.423</td>
<td style="text-align: center;">0.184</td>
<td style="text-align: center;">0.187</td>
<td style="text-align: center;">0.159</td>
<td style="text-align: center;">0.290</td>
<td style="text-align: center;">0.026</td>
<td style="text-align: center;">0.048</td>
<td style="text-align: center;">0.079</td>
<td style="text-align: center;">0.091</td>
<td style="text-align: center;">0.165</td>
</tr>
<tr>
<td style="text-align: center;">ROUGE-L</td>
<td style="text-align: center;">0.488</td>
<td style="text-align: center;">0.128</td>
<td style="text-align: center;">0.115</td>
<td style="text-align: center;">0.105</td>
<td style="text-align: center;">0.311</td>
<td style="text-align: center;">0.064</td>
<td style="text-align: center;">0.072</td>
<td style="text-align: center;">0.089</td>
<td style="text-align: center;">0.106</td>
<td style="text-align: center;">0.164</td>
</tr>
<tr>
<td style="text-align: center;">BERTScore</td>
<td style="text-align: center;">0.440</td>
<td style="text-align: center;">0.284</td>
<td style="text-align: center;">0.110</td>
<td style="text-align: center;">0.193</td>
<td style="text-align: center;">0.312</td>
<td style="text-align: center;">0.147</td>
<td style="text-align: center;">0.170</td>
<td style="text-align: center;">0.131</td>
<td style="text-align: center;">0.163</td>
<td style="text-align: center;">0.217</td>
</tr>
<tr>
<td style="text-align: center;">MoverScore</td>
<td style="text-align: center;">0.372</td>
<td style="text-align: center;">0.159</td>
<td style="text-align: center;">0.157</td>
<td style="text-align: center;">0.129</td>
<td style="text-align: center;">0.318</td>
<td style="text-align: center;">0.161</td>
<td style="text-align: center;">0.120</td>
<td style="text-align: center;">0.188</td>
<td style="text-align: center;">0.195</td>
<td style="text-align: center;">0.200</td>
</tr>
<tr>
<td style="text-align: center;">PRISM</td>
<td style="text-align: center;">0.411</td>
<td style="text-align: center;">0.249</td>
<td style="text-align: center;">0.345</td>
<td style="text-align: center;">0.254</td>
<td style="text-align: center;">0.212</td>
<td style="text-align: center;">0.573</td>
<td style="text-align: center;">0.532</td>
<td style="text-align: center;">0.561</td>
<td style="text-align: center;">0.553</td>
<td style="text-align: center;">0.410</td>
</tr>
<tr>
<td style="text-align: center;">BARTSCORE</td>
<td style="text-align: center;">0.441</td>
<td style="text-align: center;">$0.322 \dagger$</td>
<td style="text-align: center;">0.311</td>
<td style="text-align: center;">0.248</td>
<td style="text-align: center;">0.264</td>
<td style="text-align: center;">$0.679 \dagger$</td>
<td style="text-align: center;">$0.670 \dagger$</td>
<td style="text-align: center;">$0.646 \dagger$</td>
<td style="text-align: center;">$0.604 \dagger$</td>
<td style="text-align: center;">0.465</td>
</tr>
<tr>
<td style="text-align: center;">+ CNN</td>
<td style="text-align: center;">0.475</td>
<td style="text-align: center;">$\mathbf{0 . 4 4 8} \dagger$</td>
<td style="text-align: center;">$0.382 \dagger$</td>
<td style="text-align: center;">$0.356 \dagger$</td>
<td style="text-align: center;">$0.356 \dagger$</td>
<td style="text-align: center;">$0.653 \dagger$</td>
<td style="text-align: center;">$0.640 \dagger$</td>
<td style="text-align: center;">$0.616 \dagger$</td>
<td style="text-align: center;">0.567</td>
<td style="text-align: center;">0.499</td>
</tr>
<tr>
<td style="text-align: center;">+ CNN + Para</td>
<td style="text-align: center;">0.471</td>
<td style="text-align: center;">$0.424 \dagger$</td>
<td style="text-align: center;">$\mathbf{0 . 4 0 1} \dagger$</td>
<td style="text-align: center;">$\mathbf{0 . 3 7 8} \dagger$</td>
<td style="text-align: center;">0.313</td>
<td style="text-align: center;">$0.657 \dagger$</td>
<td style="text-align: center;">$0.652 \dagger$</td>
<td style="text-align: center;">$0.614 \dagger$</td>
<td style="text-align: center;">0.562</td>
<td style="text-align: center;">0.497</td>
</tr>
<tr>
<td style="text-align: center;">$+\Omega+$ Prompt</td>
<td style="text-align: center;">0.488</td>
<td style="text-align: center;">$0.407 \dagger$</td>
<td style="text-align: center;">$0.378 \dagger$</td>
<td style="text-align: center;">0.338</td>
<td style="text-align: center;">$\mathbf{0 . 3 6 8} \dagger$</td>
<td style="text-align: center;">$\mathbf{0 . 7 0 1}$</td>
<td style="text-align: center;">$\mathbf{0 . 6 7 9} \dagger$</td>
<td style="text-align: center;">$\mathbf{0 . 6 8 6} \dagger$</td>
<td style="text-align: center;">$\mathbf{0 . 6 2 0} \dagger$</td>
<td style="text-align: center;">$\mathbf{0 . 5 1 8}$</td>
</tr>
</tbody>
</table>
<p>Significance Tests. To perform rigorous analysis, we adopt the bootstrapping method (p-value &lt; 0.05) [28] for pair-wise significance tests. In all tables, we use $\dagger$ on BARTSCORE if it significantly $(p&lt;0.05)$ outperforms other unsupervised metrics excluding BARTSCORE variants. We use $\ddagger$ on BARTSCORE if it significantly outperforms all other unsupervised metrics including BARTSCORE variants.</p>
<h1>4.3 Experimental Results</h1>
<h3>4.3.1 Machine Translation</h3>
<p>Tab. 3 illustrates Kendall's Tau correlation of diverse metrics on different language pairs. We can observe that: (1) BARTSCORE enhanced by fine-tuning tasks (CNN+Para) can significantly outperform all other unsupervised methods on five language pairs and achieve comparable results on the other two. (2) The performance of BARTSCORE can be further improved by simply adding a prompt (i.e., such as) without any other overhead. Notably, on the language pair de-en, using the prompt results in a 0.033 improvement, which even significantly surpasses existing state-of-the-art supervised metrics BLEURT and COMET. This suggests a promising future direction for metric design: searching for proper prompts to better leverage knowledge stored in pre-trained language models instead of training on human judgment data [31].</p>
<h1>4.3.2 Text Summarization</h1>
<p>Tab. 4 shows the meta-evaluation results of different metrics on the summarization task. We can observe that: (1) Simply vanilla BARTSCORE can outperform BERTScore and MoverScore by a large margin on 8 settings except the INFO perspective on SummEval. Strikingly, it achieves improvements of 0.251 and 0.265 over BERTScore and MoverScore respectively. (2) The improvement on REALSum and SummEval datasets can be further improved when introducing fine-tuning tasks. However, fine-tuning does not improve on the NeR18 dataset, likely because this dataset only contains 7 systems with easily distinguishable quality, and vanilla BARTSCORE can already achieve a high level of correlation ( $&gt;0.6$ on average). (3) Our prompt combination strategy can consistently improve the performance on informativeness, up to 0.072 Spearman correlation on the NeR18 dataset
and 0.055 on SummEval. However, the performance from other perspectives such as fluency and factuality do not show consistent improvements, which we will elaborate on later (Â§4.4.2).
Analysis on Factuality Datasets The goal of these datasets is to judge whether a short generated summary is faithful to the original long documents. As shown in Tab. 5, we observe that (1) BARTSCORE + CNN can almost match human baseline on Rank19 and outperform all other metrics, including the most recent top-performing factuality metrics FactCC and QAGS by a large margin. (2) Using paraphrase as a fine-tuning task will reduce BARTSCORE's performance, which is reasonable since these two texts (i.e., the summary and document) shouldn't maintain the paraphrased relationship in general. (3) Introducing prompts does not bring an improvement, even resulting in a performance decrease.</p>
<h3>4.3.3 Data-to-text</h3>
<p>The experiment results on data-to-text datasets are shown in Tab. 6. We observe that (1) finetuning on the CNNDM dataset can consistently boost the correlation, for example, up to 0.056 gain on BAGEL. (2) Additionally, further finetuning on paraphrase datasets results in even higher performance compared to the version without any fine-tuning, up to 0.083 Spearman correlation on BAGEL dataset. These results surpass all existing top-performing metrics. (3) Our proposed prompt combination strategy can consistently improve correlation, on average 0.028 Spearman correlation. This is consistent with the findings in $\S 4.3 .2$ that we can improve the aspect of informativeness through proper prompting.</p>
<p>Table 5: Results on Rank19 and QAGS datasets. where "Q" represents QAGS. Metrics achieve highest correlation are bold.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Rank19</th>
<th style="text-align: center;">Q-CNN</th>
<th style="text-align: center;">Q-XSUM</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Acc.</td>
<td style="text-align: center;">Pearson</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">ROUGE-1</td>
<td style="text-align: center;">0.568</td>
<td style="text-align: center;">0.338</td>
<td style="text-align: center;">-0.008</td>
</tr>
<tr>
<td style="text-align: left;">ROUGE-2</td>
<td style="text-align: center;">0.630</td>
<td style="text-align: center;">0.459</td>
<td style="text-align: center;">0.097</td>
</tr>
<tr>
<td style="text-align: left;">ROUGE-L</td>
<td style="text-align: center;">0.587</td>
<td style="text-align: center;">0.357</td>
<td style="text-align: center;">0.024</td>
</tr>
<tr>
<td style="text-align: left;">BERTScore</td>
<td style="text-align: center;">0.713</td>
<td style="text-align: center;">0.576</td>
<td style="text-align: center;">0.024</td>
</tr>
<tr>
<td style="text-align: left;">MoverScore</td>
<td style="text-align: center;">0.713</td>
<td style="text-align: center;">0.414</td>
<td style="text-align: center;">0.054</td>
</tr>
<tr>
<td style="text-align: left;">PRISM</td>
<td style="text-align: center;">0.780</td>
<td style="text-align: center;">0.479</td>
<td style="text-align: center;">0.025</td>
</tr>
<tr>
<td style="text-align: left;">FactCC [30]</td>
<td style="text-align: center;">0.700</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">QAGS [67]</td>
<td style="text-align: center;">0.721</td>
<td style="text-align: center;">0.545</td>
<td style="text-align: center;">0.175</td>
</tr>
<tr>
<td style="text-align: left;">Human [14]</td>
<td style="text-align: center;">0.839</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">BARTSCORE</td>
<td style="text-align: center;">0.684</td>
<td style="text-align: center;">$0.661 \dagger$</td>
<td style="text-align: center;">0.009</td>
</tr>
<tr>
<td style="text-align: left;">+ CNN</td>
<td style="text-align: center;">$\mathbf{0 . 8 3 6} \ddagger$</td>
<td style="text-align: center;">$\mathbf{0 . 7 3 5} \ddagger$</td>
<td style="text-align: center;">$\mathbf{0 . 1 8 4} \ddagger$</td>
</tr>
<tr>
<td style="text-align: left;">+ CNN + Para</td>
<td style="text-align: center;">0.788</td>
<td style="text-align: center;">$0.680 \dagger$</td>
<td style="text-align: center;">0.074</td>
</tr>
<tr>
<td style="text-align: left;">+ CNN + Prompt</td>
<td style="text-align: center;">0.796</td>
<td style="text-align: center;">$0.719 \dagger$</td>
<td style="text-align: center;">0.094</td>
</tr>
</tbody>
</table>
<p>Table 6: Results on data-to-text datasets. We report Spearman correlation. Metrics achieve highest correlation are bold.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">BAGEL</th>
<th style="text-align: center;">SFRES</th>
<th style="text-align: center;">SFHOT</th>
<th style="text-align: center;">Avg.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ROUGE-1</td>
<td style="text-align: center;">0.234</td>
<td style="text-align: center;">0.115</td>
<td style="text-align: center;">0.118</td>
<td style="text-align: center;">$\mathbf{0 . 1 5 6}$</td>
</tr>
<tr>
<td style="text-align: left;">ROUGE-2</td>
<td style="text-align: center;">0.199</td>
<td style="text-align: center;">0.116</td>
<td style="text-align: center;">0.088</td>
<td style="text-align: center;">0.134</td>
</tr>
<tr>
<td style="text-align: left;">ROUGE-L</td>
<td style="text-align: center;">0.189</td>
<td style="text-align: center;">0.103</td>
<td style="text-align: center;">0.110</td>
<td style="text-align: center;">0.134</td>
</tr>
<tr>
<td style="text-align: left;">BERTScore</td>
<td style="text-align: center;">0.289</td>
<td style="text-align: center;">0.156</td>
<td style="text-align: center;">0.135</td>
<td style="text-align: center;">0.193</td>
</tr>
<tr>
<td style="text-align: left;">MoverScore</td>
<td style="text-align: center;">0.284</td>
<td style="text-align: center;">0.153</td>
<td style="text-align: center;">0.172</td>
<td style="text-align: center;">0.203</td>
</tr>
<tr>
<td style="text-align: left;">PRISM</td>
<td style="text-align: center;">0.305</td>
<td style="text-align: center;">0.155</td>
<td style="text-align: center;">0.196</td>
<td style="text-align: center;">0.219</td>
</tr>
<tr>
<td style="text-align: left;">BARTSCORE</td>
<td style="text-align: center;">0.247</td>
<td style="text-align: center;">$0.164 \dagger$</td>
<td style="text-align: center;">0.158</td>
<td style="text-align: center;">0.190</td>
</tr>
<tr>
<td style="text-align: left;">+ CNN</td>
<td style="text-align: center;">0.303</td>
<td style="text-align: center;">$0.191 \dagger$</td>
<td style="text-align: center;">0.190</td>
<td style="text-align: center;">0.228</td>
</tr>
<tr>
<td style="text-align: left;">+ CNN + Para</td>
<td style="text-align: center;">$0.330 \dagger$</td>
<td style="text-align: center;">$0.185 \dagger$</td>
<td style="text-align: center;">$0.211 \dagger$</td>
<td style="text-align: center;">0.242</td>
</tr>
<tr>
<td style="text-align: left;">+ $\mathrm{TT}+$ Prompt</td>
<td style="text-align: center;">$\mathbf{0 . 3 3 6} \ddagger$</td>
<td style="text-align: center;">$\mathbf{0 . 2 3 8} \ddagger$</td>
<td style="text-align: center;">$\mathbf{0 . 2 3 5} \ddagger$</td>
<td style="text-align: center;">$\mathbf{0 . 2 7 0}$</td>
</tr>
</tbody>
</table>
<h3>4.4 Analysis</h3>
<p>We design experiments to better understand the mechanism by which BARTSCORE obtains these promising results, specifically asking three questions: Q1: Compared to other unsupervised metrics, where does BARTSCORE outperform them? Q2: How does adding prompts benefit evaluation? Q3: Will BARTScore introduce biases in unpredictable ways?</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Fine-grained analysis (a,b) and prompt analysis (c). In (a, b), BE, PR, BL, CO, BA represent BERTScore, PRISM, BLEURT, COMET and BARTSCORE respectively. In (c), SEM, Lin, Fac denote semantic overlap, linguistic quality and factual correctness respectively.</p>
<h1>4.4.1 Fine-grained Analysis</h1>
<p>To answer Q1, we choose the MT task and break down the performance of each metric into different buckets based on different axes.</p>
<p>Top-k Systems We report the average correlation across all language pairs achieved by each metric given only translations from top- $k$ systems. We vary the number of $k$, and the results are shown in Fig. 2-(a). We can see that BARTSCORE can outperform all other metrics (including one supervised metric BLEURT) except the existing state-of-the-art supervised metric COMET for different $k$, and the decrease in correlation becomes smoother than others when considering top-scoring systems. This indicates that BARTSCORE is robust to high-quality generated texts.</p>
<p>Reference Length We break down each test set into four buckets based on the reference length, which are $[15,25),[25,35),[35,45),[45,54]$ and compute the Kendall's Tau average correlation of different metrics across all language pairs within each bucket. ${ }^{6}$ The results are shown in Fig. 2-(b). We observe that BARTSCORE can outperform or tie with other unsupervised metrics over different reference lengths. Also, its correlation with human judgments is more stable compared to all other metrics. This indicates its robustness to different input lengths. More other analyses can be found in Appendix.</p>
<h3>4.4.2 Prompt Analysis</h3>
<p>For Q2, we choose the summarization and data-to-text tasks for analysis where we used all prompts from our prompt set. We first group all the evaluation perspectives into three categories: (1) semantic overlap (informativeness, pyramid score, and relevance) (2) linguistic quality (fluency, coherence) (3) factual correctness (factuality). We then calculate the percentage of prompts that result in performance improvements for each perspective within a dataset. Finally, we compute the average percentage of prompts that can lead to performance gains for each category. The results are shown in Tab. 2-(c). We can see that for semantic overlap, almost all prompts can lead to the performance increase, while for factuality only a few prompts can improve the performance. This also explains the results in $\S 4.3 .2$ where we found that combining the results of different prompts can lead to consistent increases in semantic overlap but worse performance in factuality. Regarding linguistic quality, the effect of adding a prompt is not that predictive, which is also consistent with our findings in $\S 4.3 .2$.</p>
<h3>4.4.3 Bias Analysis</h3>
<p>To answer Q3, we conduct bias analysis. Bias would indicate that the scores are too high or too low compared to the scores they are given by human annotators. Therefore, to see whether such biases exist, we inspected the rank differences given by human annotators and BARTScore (fine-tuned on CNNDM dataset) on the REALSumm dataset where 24 systems are considered, including both abstractive models and extractive models as well as models based on pre-trained models and models</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Bias analysis of BARTScore. The "Rank Difference" is the rank obtained using human judgements minus the rank got from BARTScore. Systems beginning with letter "E" are extractive systems while systems beginning with letter "A" are abstractive systems.
that are trained from scratch. We list all the systems below. And the resulting rank difference is shown in Fig. 3.</p>
<p>Extractive Systems E1: BanditSum [11]; E2: Refresh [49]; E3: NeuSum [81]; E4: LSTM-PN-RL [80]; E5: BERT-TF-SL [80]; E6: BERT-TF-PN [80]; E7: BERT-LSTM-PN-RL [80]; E8: BERT-LSTM-PN [80]; E9: HeterGraph [69]; E10: MatchSum [79].</p>
<p>Abstractive Systems A1: Ptr-Gen [62]; A2: Bottom-up [17]; A3: Fast-Abs-RL [5]; A4: Two-stageRL [74]; A5: BERT-Ext-Abs [41]; A6: BERT-Abs [41]; A7: Trans-Abs [41]; A8: UniLM-1 [10]; A9: UniLM-2 [2]; A10: T5-base [56]; A11: T5-large [56]; A12: T5-11B [56]; A13: BART [33]; A14: SemSim [42].
As shown in Fig. 3, BARTScore is less effective at distinguishing the quality of extractive summarization systems while much better at distinguishing the quality of abstractive summarization systems. However, given that there is a trend for using abstractive systems as more and more pre-trained sequence-to-sequence models being proposed, BARTScore's weaknesses on extractive systems will be mitigated.</p>
<h1>5 Implications and Future Directions</h1>
<p>In this paper, we proposed a metric BARTSCORE that formulates evaluation of generated text as a text generation task, and empirically demonstrated its efficacy. Without the supervision of human judgments, BARTSCORE can effectively evaluate texts from 7 perspectives and achieve the best performance on 16 of 22 settings against existing top-scoring metrics. We highlight potential future directions based on what we have learned.
Prompt-augmented metrics As an easy-to-use but powerful method, prompting [39] has achieved impressive performance particularly on semantic overlap-based evaluation perspectives. However, its effectiveness in factuality and linguistic quality-based perspectives has not been fully demonstrated in this paper. In the future, more works can explore how to make better use of prompts for these and other evaluation scenarios.</p>
<p>Co-evolving evaluation metrics and systems BARTSCORE builds the connection between metric design and system design, which allows them to share their technological advances, thereby progressing together. For example, a better BART-based summarization system may be directly used as a more reliable automated metric for evaluating summaries, and this work makes them connected.</p>
<h2>Acknowledgments</h2>
<p>The authors would like to thank the anonymous reviewers for their insightful comments and suggestions. The authors also thank Wei Zhao for assisting with reproducing baseline results. This work was supported by the Air Force Research Laboratory under agreement number FA8750-19-2-0200. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the Air Force Research Laboratory or the U.S. Government.</p>
<h1>References</h1>
<p>[1] Akari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi, Richard Socher, and Caiming Xiong. Learning to retrieve reasoning paths over wikipedia graph for question answering. arXiv preprint arXiv:1911.10470, 2019.
[2] Hangbo Bao, Li Dong, Furu Wei, Wenhui Wang, Nan Yang, Xiaodong Liu, Yu Wang, Jianfeng Gao, Songhao Piao, Ming Zhou, and Hsiao-Wuen Hon. Unilmv2: Pseudo-masked language models for unified language model pre-training. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 642-652. PMLR, 2020.
[3] Manik Bhandari, Pranav Narayan Gour, Atabak Ashfaq, and Pengfei Liu. Metrics also disagree in the low scoring range: Revisiting summarization evaluation metrics. In Proceedings of the 28th International Conference on Computational Linguistics, pages 5702-5711, Barcelona, Spain (Online), December 2020. International Committee on Computational Linguistics.
[4] Manik Bhandari, Pranav Narayan Gour, Atabak Ashfaq, Pengfei Liu, and Graham Neubig. Re-evaluating evaluation in text summarization. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 9347-9359, Online, November 2020. Association for Computational Linguistics.
[5] Yen-Chun Chen and Mohit Bansal. Fast abstractive summarization with reinforce-selected sentence rewriting. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 675-686, Melbourne, Australia, July 2018. Association for Computational Linguistics.
[6] Andrew M Dai and Quoc V Le. Semi-supervised sequence learning. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 28. Curran Associates, Inc., 2015.
[7] Hoa Trang Dang. Overview of duc 2005. In Proceedings of the document understanding conference, volume 2005, pages 1-12, 2005.
[8] Michael Denkowski and Alon Lavie. Extending the METEOR machine translation evaluation metric to the phrase level. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 250-253, Los Angeles, California, June 2010. Association for Computational Linguistics.
[9] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, 2019.
[10] Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon. Unified language model pre-training for natural language understanding and generation. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'AlchÃ©-Buc, Emily B. Fox, and Roman Garnett, editors, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 13042-13054, 2019.
[11] Yue Dong, Yikang Shen, Eric Crawford, Herke van Hoof, and Jackie Chi Kit Cheung. BanditSum: Extractive summarization as a contextual bandit. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3739-3748, Brussels, Belgium, October-November 2018. Association for Computational Linguistics.
[12] Zi-Yi Dou, Pengfei Liu, Hiroaki Hayashi, Zhengbao Jiang, and Graham Neubig. GSum: A general framework for guided neural abstractive summarization. In Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL), Mexico City, June 2021.
[13] A. R. Fabbri, Wojciech Kryscinski, Bryan McCann, R. Socher, and Dragomir Radev. Summeval: Re-evaluating summarization evaluation. Trans. Assoc. Comput. Linguistics, 9:391-409, 2021.
[14] Tobias Falke, Leonardo FR Ribeiro, Prasetya Ajie Utama, Ido Dagan, and Iryna Gurevych. Ranking generated summaries by correctness: An interesting but challenging application for natural language inference. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2214-2220, 2019.</p>
<p>[15] David Freedman, Robert Pisani, and Roger Purves. Statistics (international student edition). Pisani, R. Purves, 4th edn. WW Norton \&amp; Company, New York, 2007.
[16] Sebastian Gehrmann, Yuntian Deng, and Alexander Rush. Bottom-up abstractive summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4098-4109, 2018.
[17] Sebastian Gehrmann, Yuntian Deng, and Alexander Rush. Bottom-up abstractive summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4098-4109, Brussels, Belgium, October-November 2018. Association for Computational Linguistics.
[18] Max Grusky, Mor Naaman, and Yoav Artzi. Newsroom: A dataset of 1.3 million summaries with diverse extractive strategies. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 708-719, New Orleans, Louisiana, June 2018. Association for Computational Linguistics.
[19] Max Grusky, Mor Naaman, and Yoav Artzi. Newsroom: A dataset of 1.3 million summaries with diverse extractive strategies. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), volume 1, pages 708-719, 2018.
[20] K. Hermann, TomÃ¡s KociskÃ½, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and P. Blunsom. Teaching machines to read and comprehend. In NIPS, 2015.
[21] Tsutomu Hirao, Manabu Okumura, Norihito Yasuda, and Hideki Isozaki. Supervised automatic evaluation for summarization with voted regression model. Information Processing \&amp; Management, 43(6):1521-1535, 2007.
[22] J. Edward Hu, Abhinav Singh, Nils Holzenberger, Matt Post, and Benjamin Van Durme. Largescale, diverse, paraphrastic bitexts via sampling and clustering. In Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL), pages 44-54, Hong Kong, China, November 2019. Association for Computational Linguistics.
[23] Sarthak Jain, Madeleine van Zuylen, Hannaneh Hajishirzi, and Iz Beltagy. Scirex: A challenge dataset for document-level information extraction. arXiv preprint arXiv:2005.00512, 2020.
[24] Zhengbao Jiang, Frank F. Xu, Jun Araki, and Graham Neubig. How can we know what language models know? Transactions of the Association for Computational Linguistics, 8:423-438, 2020.
[25] Karen Sparck Jones. A statistical interpretation of term specificity and its application in retrieval. Journal of documentation, 1972.
[26] Vladimir Karpukhin, Barlas OÄŸuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2004.04906, 2020.
[27] M. G. Kendall. A new measure of rank correlation. Biometrika, 30(1/2):81-93, 1938.
[28] Philipp Koehn. Statistical significance tests for machine translation evaluation. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, pages 388-395, Barcelona, Spain, July 2004. Association for Computational Linguistics.
[29] Philipp Koehn. Statistical machine translation. Cambridge University Press, 2009.
[30] Wojciech Kryscinski, Bryan McCann, Caiming Xiong, and Richard Socher. Evaluating the factual consistency of abstractive text summarization. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 9332-9346, Online, November 2020. Association for Computational Linguistics.
[31] Teven Le Scao and Alexander Rush. How many data points is a prompt worth? In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2627-2636, Online, June 2021. Association for Computational Linguistics.
[32] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461, 2019.</p>
<p>[33] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871-7880, Online, July 2020. Association for Computational Linguistics.
[34] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. ArXiv, abs/2101.00190, 2021.
[35] Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. Text Summarization Branches Out, 2004.
[36] Chin-Yew Lin and Eduard Hovy. Automatic evaluation of summaries using n-gram cooccurrence statistics. In Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, pages 150-157, 2003.
[37] Ying Lin, Heng Ji, Fei Huang, and Lingfei Wu. A joint neural model for information extraction with global features. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7999-8009, 2020.
[38] Pengfei Liu, Jinlan Fu, Yang Xiao, Weizhe Yuan, Shuaicheng Chang, Junqi Dai, Yixin Liu, Zihuiwen Ye, and Graham Neubig. Explainaboard: An explainable leaderboard for nlp. arXiv preprint arXiv:2104.06387, 2021.
[39] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing, 2021.
[40] Yang Liu and Mirella Lapata. Text summarization with pretrained encoders. arXiv preprint arXiv:1908.08345, 2019.
[41] Yang Liu and Mirella Lapata. Text summarization with pretrained encoders. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3730-3740, Hong Kong, China, November 2019. Association for Computational Linguistics.
[42] Yixin Liu and Pengfei Liu. Simcls: A simple framework for contrastive learning of abstractive summarization. arXiv preprint arXiv:2106.01890, 2021.
[43] Qingsong Ma, OndÅ™ej Bojar, and Yvette Graham. Results of the WMT18 metrics shared task: Both characters and embeddings achieve good performance. In Proceedings of the Third Conference on Machine Translation: Shared Task Papers, pages 671-688, Belgium, Brussels, October 2018. Association for Computational Linguistics.
[44] Qingsong Ma, Johnny Wei, OndÅ™ej Bojar, and Yvette Graham. Results of the WMT19 metrics shared task: Segment-level and strong MT systems pose big challenges. In Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1), pages 62-90, Florence, Italy, August 2019. Association for Computational Linguistics.
[45] FranÃ§ois Mairesse, Milica Gasic, Filip Jurcicek, Simon Keizer, Blaise Thomson, Kai Yu, and Steve Young. Phrase-based statistical language generation using graphical models and active learning. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1552-1561, 2010.
[46] Chaitanya Malaviya, Graham Neubig, and Patrick Littell. Learning language representations for typology prediction. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2529-2535, Copenhagen, Denmark, September 2017. Association for Computational Linguistics.
[47] Nitika Mathur, Timothy Baldwin, and Trevor Cohn. Tangled up in BLEU: Reevaluating the evaluation of automatic machine translation evaluation metrics. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4984-4997, Online, July 2020. Association for Computational Linguistics.
[48] Shashi Narayan, Shay B Cohen, and Mirella Lapata. Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1797-1807, 2018.</p>
<p>[49] Shashi Narayan, Shay B. Cohen, and Mirella Lapata. Ranking sentences for extractive summarization with reinforcement learning. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1747-1759, New Orleans, Louisiana, June 2018. Association for Computational Linguistics.
[50] Ani Nenkova and Rebecca Passonneau. Evaluating content selection in summarization: The pyramid method. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics: HLT-NAACL 2004, pages 145-152, Boston, Massachusetts, USA, May 2 - May 7 2004. Association for Computational Linguistics.
[51] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311-318, Philadelphia, Pennsylvania, USA, July 2002. Association for Computational Linguistics.
[52] Maxime Peyrard, Teresa Botschen, and Iryna Gurevych. Learning to score system summaries for better content selection evaluation. In Proceedings of the Workshop on New Frontiers in Summarization, pages 74-84, Copenhagen, Denmark, September 2017. Association for Computational Linguistics.
[53] Maja PopoviÄ‡. chrF: character n-gram F-score for automatic MT evaluation. In Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 392-395, Lisbon, Portugal, September 2015. Association for Computational Linguistics.
[54] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.
[55] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1-67, 2020.
[56] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21:140:1-140:67, 2020.
[57] Ricardo Rei, Craig Stewart, Ana C Farinha, and Alon Lavie. COMET: A neural framework for MT evaluation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2685-2702, Online, November 2020. Association for Computational Linguistics.
[58] Laria Reynolds and Kyle McDonell. Prompt programming for large language models: Beyond the few-shot paradigm. In Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems, pages 1-7, 2021.
[59] Timo Schick and Hinrich SchÃ¼tze. Exploiting cloze questions for few-shot text classification and natural language inference. arXiv preprint arXiv:2001.07676, 2020.
[60] Timo Schick and Hinrich SchÃ¼tze. Few-shot text generation with pattern-exploiting training. arXiv preprint arXiv:2012.11926, 2020.
[61] Timo Schick and Hinrich SchÃ¼tze. It's not just size that matters: Small language models are also few-shot learners. arXiv preprint arXiv:2009.07118, 2020.
[62] Abigail See, Peter J. Liu, and Christopher D. Manning. Get to the point: Summarization with pointer-generator networks. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1073-1083, Vancouver, Canada, July 2017. Association for Computational Linguistics.
[63] Thibault Sellam, Dipanjan Das, and Ankur Parikh. BLEURT: Learning robust metrics for text generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7881-7892, Online, July 2020. Association for Computational Linguistics.
[64] Taylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace, and Sameer Singh. Autoprompt: Eliciting knowledge from language models with automatically generated prompts. arXiv preprint arXiv:2010.15980, 2020.</p>
<p>[65] MiloÅ¡ StanojeviÄ‡ and Khalil Sima'an. BEER: BEtter evaluation as ranking. In Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 414-419, Baltimore, Maryland, USA, June 2014. Association for Computational Linguistics.
[66] Brian Thompson and Matt Post. Automatic machine translation evaluation in many languages via zero-shot paraphrasing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 90-121, Online, November 2020. Association for Computational Linguistics.
[67] Alex Wang, Kyunghyun Cho, and M. Lewis. Asking and answering questions to evaluate the factual consistency of summaries. In ACL, 2020.
[68] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 353-355, Brussels, Belgium, November 2018. Association for Computational Linguistics.
[69] Danqing Wang, Pengfei Liu, Yining Zheng, Xipeng Qiu, and Xuanjing Huang. Heterogeneous graph neural networks for extractive document summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6209-6219, Online, July 2020. Association for Computational Linguistics.
[70] Tsung-Hsien Wen, Milica Gasic, Nikola Mrksic, Pei-Hao Su, David Vandyke, and Steve Young. Semantically conditioned lstm-based natural language generation for spoken dialogue systems. arXiv preprint arXiv:1508.01745, 2015.
[71] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38-45, Online, October 2020. Association for Computational Linguistics.
[72] Weizhe Yuan, Pengfei Liu, and Graham Neubig. Can we automate scientific reviewing? arXiv preprint arXiv:2102.00176, 2021.
[73] Jerrold H Zar. Spearman rank correlation. Encyclopedia of Biostatistics, 7, 2005.
[74] Haoyu Zhang, Jingjing Cai, Jianjun Xu, and Ji Wang. Pretraining-based natural language generation for text summarization. In Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL), pages 789-797, Hong Kong, China, November 2019. Association for Computational Linguistics.
[75] Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter Liu. Pegasus: Pre-training with extracted gap-sentences for abstractive summarization. In International Conference on Machine Learning, pages 11328-11339. PMLR, 2020.
[76] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation with bert. In International Conference on Learning Representations, 2020.
[77] Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Christian M. Meyer, and Steffen Eger. MoverScore: Text generation evaluating with contextualized embeddings and earth mover distance. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 563-578, Hong Kong, China, November 2019. Association for Computational Linguistics.
[78] Ming Zhong, Pengfei Liu, Yiran Chen, Danqing Wang, Xipeng Qiu, and Xuanjing Huang. Extractive summarization as text matching. arXiv preprint arXiv:2004.08795, 2020.
[79] Ming Zhong, Pengfei Liu, Yiran Chen, Danqing Wang, Xipeng Qiu, and Xuanjing Huang. Extractive summarization as text matching. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6197-6208, Online, July 2020. Association for Computational Linguistics.
[80] Ming Zhong, Pengfei Liu, Danqing Wang, Xipeng Qiu, and Xuanjing Huang. Searching for effective neural extractive summarization: What works and what's next. In Proceedings of</p>
<p>the 57th Annual Meeting of the Association for Computational Linguistics, pages 1049-1058, Florence, Italy, July 2019. Association for Computational Linguistics.
[81] Qingyu Zhou, Nan Yang, Furu Wei, Shaohan Huang, Ming Zhou, and Tiejun Zhao. Neural document summarization by jointly learning to score and select sentences. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 654-663, Melbourne, Australia, July 2018. Association for Computational Linguistics.</p>
<h1>A Appendix</h1>
<h2>A. 1 Summary of Commonly Used Metrics for Text Generation</h2>
<p>Table 7: Summary of commonly used metrics for text generation. $(S, H)$ represents whether a metric has a setting that uses source text and hypothesis text. $(R, H)$ denotes whether a metric has a setting that uses reference text and hypothesis text. $(S, R, H)$ indicates whether a metric has a setting that uses source text, hypothesis text and reference text. We use the following abbreviations for different tasks: SUM - Summarization, MT - Machine Translation, MUL - Multiple tasks, FAC - Factuality. For settings and tasks, we only list the ones justified by the original paper for each metric.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Metrics</th>
<th style="text-align: left;">Supervised</th>
<th style="text-align: left;">Paradigm</th>
<th style="text-align: center;">$(S, H)$</th>
<th style="text-align: center;">$(R, H)$</th>
<th style="text-align: center;">$(S, R, H)$</th>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Support FAC</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ROUGE</td>
<td style="text-align: left;">$\boldsymbol{x}$</td>
<td style="text-align: left;">Match</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">SUM</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
</tr>
<tr>
<td style="text-align: left;">BLEU</td>
<td style="text-align: left;">$\boldsymbol{x}$</td>
<td style="text-align: left;">Match</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">MT</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
</tr>
<tr>
<td style="text-align: left;">CHRF</td>
<td style="text-align: left;">$\boldsymbol{x}$</td>
<td style="text-align: left;">Match</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">MT</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
</tr>
<tr>
<td style="text-align: left;">BERTScore</td>
<td style="text-align: left;">$\boldsymbol{x}$</td>
<td style="text-align: left;">Match</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">MUL</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
</tr>
<tr>
<td style="text-align: left;">MoverScore</td>
<td style="text-align: left;">$\boldsymbol{x}$</td>
<td style="text-align: left;">Match</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">MUL</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
</tr>
<tr>
<td style="text-align: left;">PRISM</td>
<td style="text-align: left;">$\boldsymbol{x}$</td>
<td style="text-align: left;">Paraphrase</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">MT</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
</tr>
<tr>
<td style="text-align: left;">BLEURT</td>
<td style="text-align: left;">$\checkmark$</td>
<td style="text-align: left;">Regress</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">MT</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
</tr>
<tr>
<td style="text-align: left;">S3</td>
<td style="text-align: left;">$\checkmark$</td>
<td style="text-align: left;">Regress</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">SUM</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
</tr>
<tr>
<td style="text-align: left;">VRM</td>
<td style="text-align: left;">$\checkmark$</td>
<td style="text-align: left;">Regress</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">SUM</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
</tr>
<tr>
<td style="text-align: left;">COMET</td>
<td style="text-align: left;">$\checkmark$</td>
<td style="text-align: left;">Regress, Rank</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">MT</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
</tr>
<tr>
<td style="text-align: left;">BEER</td>
<td style="text-align: left;">$\checkmark$</td>
<td style="text-align: left;">Rank</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">MT</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
</tr>
<tr>
<td style="text-align: left;">BARTScore</td>
<td style="text-align: left;">$\boldsymbol{x}$</td>
<td style="text-align: left;">Generation</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">MUL</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
</tbody>
</table>
<h2>A. 2 Pre-trained Model Selection</h2>
<p>Besides BART, we also tried T5 and PEGASUS as our sequence-to-sequence model to get generation scores. We conduct experiments on WMT19, and the results are shown in Tab. 8. We don't observe improvements in using PEGASUS or T5 over BART.</p>
<p>Table 8: Experiment results for PEGASUS and T5 on the WMT19 dataset. The highest correlations are bold.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">de-en</th>
<th style="text-align: left;">fi-en</th>
<th style="text-align: left;">gu-en</th>
<th style="text-align: left;">kk-en</th>
<th style="text-align: left;">lt-en</th>
<th style="text-align: left;">ru-en</th>
<th style="text-align: left;">zh-en</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">PEGASUS-large</td>
<td style="text-align: left;">0.124</td>
<td style="text-align: left;">0.297</td>
<td style="text-align: left;">0.237</td>
<td style="text-align: left;">0.205</td>
<td style="text-align: left;">0.252</td>
<td style="text-align: left;">0.148</td>
<td style="text-align: left;">0.311</td>
</tr>
<tr>
<td style="text-align: left;">PEGASUS-large-cnn</td>
<td style="text-align: left;">0.174</td>
<td style="text-align: left;">0.361</td>
<td style="text-align: left;">0.297</td>
<td style="text-align: left;">0.337</td>
<td style="text-align: left;">0.373</td>
<td style="text-align: left;">0.215</td>
<td style="text-align: left;">0.415</td>
</tr>
<tr>
<td style="text-align: left;">T5-base</td>
<td style="text-align: left;">0.170</td>
<td style="text-align: left;">0.357</td>
<td style="text-align: left;">$\mathbf{0 . 3 0 0}$</td>
<td style="text-align: left;">0.339</td>
<td style="text-align: left;">0.348</td>
<td style="text-align: left;">$\mathbf{0 . 2 0 8}$</td>
<td style="text-align: left;">0.378</td>
</tr>
<tr>
<td style="text-align: left;">T5-large</td>
<td style="text-align: left;">0.168</td>
<td style="text-align: left;">0.353</td>
<td style="text-align: left;">0.287</td>
<td style="text-align: left;">0.332</td>
<td style="text-align: left;">0.335</td>
<td style="text-align: left;">0.193</td>
<td style="text-align: left;">0.383</td>
</tr>
<tr>
<td style="text-align: left;">T5-base-cnn</td>
<td style="text-align: left;">0.177</td>
<td style="text-align: left;">0.364</td>
<td style="text-align: left;">0.295</td>
<td style="text-align: left;">0.342</td>
<td style="text-align: left;">0.347</td>
<td style="text-align: left;">0.207</td>
<td style="text-align: left;">0.402</td>
</tr>
<tr>
<td style="text-align: left;">BART</td>
<td style="text-align: left;">0.156</td>
<td style="text-align: left;">0.335</td>
<td style="text-align: left;">0.273</td>
<td style="text-align: left;">0.324</td>
<td style="text-align: left;">0.322</td>
<td style="text-align: left;">0.167</td>
<td style="text-align: left;">0.389</td>
</tr>
<tr>
<td style="text-align: left;">BART-cnn</td>
<td style="text-align: left;">$\mathbf{0 . 1 9 0}$</td>
<td style="text-align: left;">$\mathbf{0 . 3 6 5}$</td>
<td style="text-align: left;">$\mathbf{0 . 3 0 0}$</td>
<td style="text-align: left;">$\mathbf{0 . 3 4 8}$</td>
<td style="text-align: left;">$\mathbf{0 . 3 8 4}$</td>
<td style="text-align: left;">$\mathbf{0 . 2 0 8}$</td>
<td style="text-align: left;">$\mathbf{0 . 4 2 5}$</td>
</tr>
</tbody>
</table>
<h2>A. 3 Prompt Set</h2>
<p>In Tab. 9, we list the full prompt set for both $\boldsymbol{s} \rightarrow \boldsymbol{h}$ direction and $\boldsymbol{h} \leftrightarrow \boldsymbol{r}$ direction.</p>
<h2>A. 4 Prompt Combination</h2>
<p>Given a source sequence $\mathbf{x}$, a target sequence $\mathbf{y}$ and a set of prompts $\mathbf{z}<em 2="2">{1}, \mathbf{z}</em>}, \cdots \mathbf{z<em i="i">{n}$. We denote the prompted target sequence as $\left[\mathbf{y}: \mathbf{z}</em>$. Under the sequence-to-sequence model}\right]$ for any prompt $\mathbf{z}_{i</p>
<p>Table 9: Full prompt set for both $\boldsymbol{s} \rightarrow \boldsymbol{h}$ and $\boldsymbol{h} \leftrightarrow \boldsymbol{r}$</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Prompt Set</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$\boldsymbol{s} \rightarrow \boldsymbol{h}$</td>
<td style="text-align: center;">Last</td>
<td style="text-align: center;">Tersely</td>
<td style="text-align: center;">Succinctly</td>
<td style="text-align: center;">In summation</td>
<td style="text-align: center;">To put it succinctly</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">After</td>
<td style="text-align: center;">In brief</td>
<td style="text-align: center;">All in all</td>
<td style="text-align: center;">To summarize</td>
<td style="text-align: center;">Bringing up the rear</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Behind</td>
<td style="text-align: center;">In short</td>
<td style="text-align: center;">In outline</td>
<td style="text-align: center;">In a nutshell</td>
<td style="text-align: center;">To come to the point</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Lastly</td>
<td style="text-align: center;">Concisely</td>
<td style="text-align: center;">In closing</td>
<td style="text-align: center;">In conclusion</td>
<td style="text-align: center;">In the final analysis</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">In sum</td>
<td style="text-align: center;">In precis</td>
<td style="text-align: center;">In passing</td>
<td style="text-align: center;">In winding up</td>
<td style="text-align: center;">Without wasting words</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">To end</td>
<td style="text-align: center;">In a word</td>
<td style="text-align: center;">To conclude</td>
<td style="text-align: center;">Last in order</td>
<td style="text-align: center;">At the end of the day</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Curtly</td>
<td style="text-align: center;">Compactly</td>
<td style="text-align: center;">Summarising</td>
<td style="text-align: center;">In a few words</td>
<td style="text-align: center;">Without waste of words</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Crisply</td>
<td style="text-align: center;">Summarily</td>
<td style="text-align: center;">In the rear</td>
<td style="text-align: center;">As a final point</td>
<td style="text-align: center;">Finally yet importantly</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">At last</td>
<td style="text-align: center;">To sum up</td>
<td style="text-align: center;">Summarizing</td>
<td style="text-align: center;">Not least of all</td>
<td style="text-align: center;">To put it in a nutshell</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Pithily</td>
<td style="text-align: center;">Basically</td>
<td style="text-align: center;">Laconically</td>
<td style="text-align: center;">To put it briefly</td>
<td style="text-align: center;">When all is said and done</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Shortly</td>
<td style="text-align: center;">In the end</td>
<td style="text-align: center;">At the rear</td>
<td style="text-align: center;">Not to mince words</td>
<td style="text-align: center;">To cut a long story short</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">In fine</td>
<td style="text-align: center;">At the end</td>
<td style="text-align: center;">To be brief</td>
<td style="text-align: center;">Last but not least</td>
<td style="text-align: center;">Not to beat about the bush</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Finally</td>
<td style="text-align: center;">In essence</td>
<td style="text-align: center;">Last of all</td>
<td style="text-align: center;">Just as importantly</td>
<td style="text-align: center;">In drawing things to a close</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Briefly</td>
<td style="text-align: center;">Ultimately</td>
<td style="text-align: center;">Elliptically</td>
<td style="text-align: center;">To put it concisely</td>
<td style="text-align: center;">Not to put too fine a point on it</td>
</tr>
<tr>
<td style="text-align: center;">$\boldsymbol{h} \leftrightarrow \boldsymbol{r}$</td>
<td style="text-align: center;">As</td>
<td style="text-align: center;">To wit</td>
<td style="text-align: center;">As it were</td>
<td style="text-align: center;">Case in point</td>
<td style="text-align: center;">As an illustration</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">sc.</td>
<td style="text-align: center;">That is</td>
<td style="text-align: center;">Especially</td>
<td style="text-align: center;">That is to say</td>
<td style="text-align: center;">To give an example</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">i.e.</td>
<td style="text-align: center;">Such as</td>
<td style="text-align: center;">For example</td>
<td style="text-align: center;">To rephrase it</td>
<td style="text-align: center;">To give an instance</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Like</td>
<td style="text-align: center;">Scilicet</td>
<td style="text-align: center;">Particularly</td>
<td style="text-align: center;">To be specific</td>
<td style="text-align: center;">To put it another way</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Viz.</td>
<td style="text-align: center;">Videlicet</td>
<td style="text-align: center;">Specifically</td>
<td style="text-align: center;">In plain English</td>
<td style="text-align: center;">By way of explanation</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Namely</td>
<td style="text-align: center;">Expressly</td>
<td style="text-align: center;">For instance</td>
<td style="text-align: center;">Take for example</td>
<td style="text-align: center;">By way of illustration</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">id est</td>
<td style="text-align: center;">Specially</td>
<td style="text-align: center;">To illustrate</td>
<td style="text-align: center;">Strictly speaking</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>parameterized by $\theta$, we combine the generation scores using different prompts as follows:</p>
<p>$$
\text { BARTSCORE-PROMPT }=\frac{1}{n} \sum_{i=1}^{n} \frac{1}{m_{i}} \sum_{t=1}^{m_{i}} \log p\left(\left[\mathbf{y}: \mathbf{z}<em t="t">{i}\right]</em>, \theta\right)
$$} \mid\left[\mathbf{y}: \mathbf{z}_{i}\right]&lt;t, \mathbf{x</p>
<p>Where $n$ is the number of prompts considered, $m_{i}$ is the target length after adding the $i$-th prompt.</p>
<h1>A. 5 Robustness to Language Pair Distance</h1>
<p>Translations between different language pairs contain different variances. Here we aim to measure how the performance of a metric will change considering the distance between a language pair. We use language vectors to measure the distance between two languages [46], and consider 6 distances, which are syntactic, geographic, phonological, genetic, inventory and featural distances. We plot the Pearson correlation heatmap in Fig. 4. We observe that the correlation doesn't change much w.r.t. different distances across metrics. And the results show that all metrics have a significant correlation with genetic distance. This indicates that metrics are good at measuring translation quality from genetically different languages. This may be because the translation from similar languages is easier than dissimilar languages, making translation systems less distinguishable.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Pearson correlation between language pair distance and correlation with human metrics.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{6}$ In each bucket, we remove the language pairs that do not contain over 500 samples. This results in the removal of kk-en in $[35,45)$ and the removal of gu-en, kk-en, lt-en in $[45,54]$.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
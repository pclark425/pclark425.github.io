<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1405 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1405</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1405</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-27.html">extraction-schema-27</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <p><strong>Paper ID:</strong> paper-2fec20377bc947ec1df003b4aedcb4d7f25ac934</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/2fec20377bc947ec1df003b4aedcb4d7f25ac934" target="_blank">TransDreamer: Reinforcement Learning with Transformer World Models</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> A transformer-based MBRL agent, called TransDreamer, is proposed that outperforms Dreamer in these complex tasks and is applied to 2D visual RL and 3D first-person visual RL tasks both requiring long-range memory access for memory-based reasoning.</p>
                <p><strong>Paper Abstract:</strong> The Dreamer agent provides various benefits of Model-Based Reinforcement Learning (MBRL) such as sample efficiency, reusable knowledge, and safe planning. However, its world model and policy networks inherit the limitations of recurrent neural networks and thus an important question is how an MBRL framework can benefit from the recent advances of transformers and what the challenges are in doing so. In this paper, we propose a transformer-based MBRL agent, called TransDreamer. We first introduce the Transformer State-Space Model, a world model that leverages a transformer for dynamics predictions. We then share this world model with a transformer-based policy network and obtain stability in training a transformer-based RL agent. In experiments, we apply the proposed model to 2D visual RL and 3D first-person visual RL tasks both requiring long-range memory access for memory-based reasoning. We show that the proposed model outperforms Dreamer in these complex tasks.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1405.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1405.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TSSM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transformer State-Space Model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A stochastic transformer-based state-space world model that replaces the RNN deterministic path with a transformer to enable direct access to past stochastic states and parallel computation during training, while maintaining stochastic priors for imagination.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer State-Space Model (TSSM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Stochastic latent world model where the deterministic state h_t is produced by a transformer attending over past stochastic states and actions h_t = transformer(z_{1:t-1}, a_{1:t-1}), the posterior representation is approximated myopically as q(z_t | x_t) (removing dependence on h_t) enabling parallel computation of inputs, and the prior p(z_t | z_{1:t-1}, a_{1:t-1}) (or p(z_t | h_t)) is used for autoregressive imagination in latent space.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (stochastic transformer-based state-space model)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Visual model-based reinforcement learning (2D & 3D partially observable navigation tasks called Hidden Order Discovery, also evaluated on DMC and Atari)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Image reconstruction/generation MSE (foreground and overall), reward prediction accuracy (classification for +3 reward within ±0.3 and zero-reward within ±0.01), and qualitative imagined-trajectory correctness</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Quantitative improvements vs DreamerV2 on Hidden Order (examples): Foreground MSE (4-Ball Dense, context=60): TSSM 211.2 vs DreamerV2 281.9; Overall MSE (4-Ball Dense, context=60): 458.0 vs 577.8. Reward (non-zero) accuracy (4-Ball Dense, context=60): TSSM 46.9% vs DreamerV2 28.2%; (context=80) TSSM 73.2% vs DreamerV2 50.5%.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Predominantly a black-box neural latent model; interpretability is limited to inspecting generated images/trajectories and prediction errors (MSE, reward accuracy). The model does not claim explicit disentangled or symbolic latent semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Visualization of imagined trajectories and predicted frames; quantitative inspection of foreground vs overall MSE and reward-prediction accuracy; no attention-map or latent-factor probing analysis reported.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Higher memory footprint than RNN-based RSSM (authors state 'increased memory requirements'); used transformer variants: 6-layer transformer with identity-map reordering for Hidden Order tasks; for DMC/Atari used 2-layer transformer, 10 attention heads; hidden dims: 200 (DMC) / 600 (Atari). Training required reducing number of imagined trajectories per batch (cannot imagine from every sampled state) — e.g., K reduced, and for DMC/Atari used 3 imagined trajectories per sample.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Training allows parallel computation of state updates (removes sequential RNN dependence) resulting in parallelizable world-model training; however, in practice TSSM uses more memory and is slower to saturate on short-term tasks (DMC/Atari) than Dreamer/RSSM and requires reducing imagined trajectories to fit memory. The paper reports slower early convergence but comparable final performance on short-term tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>On long-term memory Hidden Order Discovery tasks, TransDreamer (TSSM + Dreamer policy framework) outperforms Dreamer: e.g., 2D 4-Ball average episode reward ~7 vs ~4 and success rate 23% vs 7%; 3D tasks also show consistent improvement. On DMC and Atari (short-term tasks), TransDreamer converges to comparable returns but typically more slowly.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Higher fidelity (better image generation MSE and reward prediction accuracy) correlates with improved task performance on long-horizon, memory-intensive tasks: TSSM's better reward prediction and more accurate long-range imagination appears to yield higher success rates and returns in Hidden Order Discovery. For short-horizon tasks, the extra capacity/compute does not provide clear runtime advantages and slows early learning.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>TSSM vs RSSM/Dreamer: improved long-term fidelity and task utility for memory-heavy tasks at the cost of higher memory use, increased computational requirements, and slower convergence on short-horizon tasks. Design enabling parallel training (q(z_t|x_t) posterior) trades off some posterior expressivity for parallelizability. Also, the transformer must be held fixed during policy training to avoid stability issues (trading off end-to-end adaptability).</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Key choices: replace RNN deterministic state with transformer over past stochastic states/actions; approximate posterior as q(z_t|x_t) to enable parallel per-step posterior computation; use stochastic prior p(z_t|h_t) for imagination; hold TSSM parameters fixed during policy learning for stability; use prioritized replay and reduced number of imagined trajectories K to manage reward-sparsity and memory constraints; optional use of identity-map reordering/gating.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared directly to RSSM/DreamerV2: TSSM produces lower image MSE and higher reward-prediction accuracy in long-term Hidden Order tasks and yields higher task success rates and episode returns; compared to RSSM, TSSM supports direct access to historical states and parallelizable training but costs more memory and slower early policy learning on short-term tasks. Compared to model-free baselines (not the study's focus), TSSM-based MBRL retains sample-efficiency benefits of imagination-based training.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper recommendations/insights: Use TSSM when tasks require long-range memory and complex temporal reasoning; employ q(z_t|x_t) posterior to enable parallel training; hold the transformer (world model) fixed during policy learning to avoid instability; use prioritized replay for sparse-reward environments and limit the number of imagined trajectories per batch to fit memory. Transformer depth/heads should be tuned per domain (authors used 6-layer for Hidden Order, 2-layer for DMC/Atari).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TransDreamer: Reinforcement Learning with Transformer World Models', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1405.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1405.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RSSM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Recurrent State-Space Model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A stochastic recurrent latent dynamics model (deterministic recurrent state h_t plus stochastic latent z_t) used as the core world model in Dreamer; updates sequentially via an RNN (GRU/LSTM) and supports latent-space rollouts for imagination.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning latent dynamics for planning from pixels</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Recurrent State-Space Model (RSSM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Latent state s_t = [h_t, z_t] where deterministic h_t = gru(h_{t-1}, z_{t-1}, a_{t-1}) and stochastic z_t sampled from a posterior q(z_t | h_t, x_t) and prior p(z_t | h_t). Observation and reward predictors decode from s_t. Designed for sequential latent rollouts and ELBO-based training.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (stochastic recurrent state-space model)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Visual model-based reinforcement learning (Dreamer experiments across DMC, Atari, and Hidden Order baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Image reconstruction/generation MSE (foreground and overall), reward prediction accuracy (classification around true reward values), ELBO components / KL used during training</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Baseline numbers (DreamerV2 / RSSM variants): Foreground MSE (4-Ball Dense, context=60): 281.9 (DreamerV2) vs TSSM 211.2; Overall MSE (4-Ball Dense, context=60): 577.8. Reward (non-zero) accuracy (4-Ball Dense, context=60): DreamerV2 28.2% vs TSSM 46.9%. (These are reported baseline performance values in paper tables for RSSM/DreamerV2.)</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Black-box neural latent model; interpretability limited to inspecting reconstructions/imagination and error metrics; no claim of interpretable latent factors in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Qualitative visualization of imagined frames and quantitative metrics (MSE, reward-accuracy). No attention-based explanations (RNN-based).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Sequential computation due to RNN update (h_t depends on h_{t-1}) so training is inherently sequential in time; lower memory footprint than transformer-based TSSM enabling imagining from every state sampled in a batch (Dreamer imagines from all sampled states); overall more memory- and compute-efficient per-batch compared to TSSM.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Compared to TSSM, RSSM has lower memory requirements, can generate imagined trajectories from every sampled start state (higher effective number of imagined rollouts per batch), and typically learns faster on short-horizon tasks; however, it lacks the transformer advantages for direct long-range access and parallelization.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Per paper: RSSM/DreamerV2 achieves competitive results on short-term DMC and Atari tasks and is outperformed by TSSM-based TransDreamer on long-horizon Hidden Order tasks (example success rates: 2D 4-Ball DreamerV2 success 7% vs TransDreamer 23%).</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>RSSM's inductive bias towards recent history makes it efficient for tasks requiring short-term dynamics modeling, enabling faster early convergence and efficient rollouts, but this inductive bias limits performance on tasks requiring long-range memory and complex relational reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>RSSM gains computational and sample-efficiency for short-horizon tasks due to sequential, compact recurrence and ability to imagine from many start points, but sacrifices ability to utilize long contexts and direct access to entire history, yielding lower fidelity on long-horizon imagination and reward prediction compared to TSSM.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Uses an RNN (GRU/LSTM) for deterministic h_t updates; posterior conditioned on h_t and x_t q(z_t | h_t, x_t); stochastic z_t concatenated with h_t to form state s_t; ELBO-based training with reconstruction and KL terms; allows efficient latent-space rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to TSSM: RSSM is less capable at leveraging long contexts and produces worse long-horizon imagination and reward prediction in the studied tasks, but is more memory-efficient and faster to train on short-term tasks. Compared to model-free methods, RSSM (in Dreamer) provides sample efficiency via imagined rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>For short-term, near-Markovian control tasks, RSSM (as used in Dreamer) is recommended because of its inductive bias and computational efficiency; the paper suggests transformer replacement (TSSM) when tasks require direct long-term memory access.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TransDreamer: Reinforcement Learning with Transformer World Models', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1405.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1405.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DreamerV2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DreamerV2 (Dreamer with discrete world model variants)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An improved Dreamer agent variant that uses discrete latent states and other training enhancements (straight-through gradients, KL balancing) to achieve strong performance, notably on Atari; used as the main baseline in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mastering atari with discrete world models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DreamerV2 world model (RSSM-based with discrete latent states)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>World model variant that uses discrete stochastic latent states instead of continuous ones, straight-through gradients for differentiating through discrete states/actions, and KL balancing; based on the RSSM/deterministic recurrent backbone but with discrete latent representations for improved performance on certain domains (e.g., Atari).</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (discrete stochastic recurrent state-space model)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Atari benchmark and other visual control tasks; used here as the baseline on Hidden Order, DMC, and Atari evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Image generation MSE (foreground and overall) and reward prediction accuracy for +3 reward timesteps and zero-reward timesteps (as in paper tables)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Reported baseline numbers: Foreground MSE (4-Ball Dense, context=60): 281.9; Overall MSE (4-Ball Dense, context=60): 577.8. Reward (non-zero) accuracy (4-Ball Dense, context=60): 28.2%. Performance is consistently lower than TSSM on long-horizon tasks in these metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Similar to RSSM: primarily black-box latent model; analysis in the paper limited to reconstructed frames and reward-prediction accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Evaluation by image/trajectory visualization and MSE/reward-accuracy metrics; no explicit interpretability methods reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Comparable to RSSM (recurrent computation); lower memory footprint than TSSM; DreamerV2 imagines from all sampled states in a batch (unlike TransDreamer which reduces imagined trajectories due to TSSM memory costs).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>More computationally efficient per-batch imagination than TSSM because it can produce imagined rollouts from every sampled state; tends to converge faster on short-horizon DMC/Atari tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Per experiments in paper: DreamerV2 attains competitive performance on short-term tasks and acts as the primary baseline; on Hidden Order (long-term) tasks it underperforms compared to TransDreamer (examples: 2D 4-Ball success 7% vs 23%).</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Discrete-latent RSSM variants like DreamerV2 are effective for many control tasks and Atari due to stable training tricks (discrete latents, straight-through grads), but their ability to leverage very long contexts is limited compared to transformer-based TSSM.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Discrete latent + recurrent design yields good short-term sample efficiency and practical training stability at lower memory cost, but reduced long-range context usage and lower long-horizon fidelity than TSSM.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Uses discrete latent variables, straight-through gradient estimators, KL-balancing; imagines from all sampled states in replay to maximize data for policy learning; follows Dreamer framework.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to TSSM: DreamerV2 is more memory-efficient, learns faster on short-term tasks, but produces worse long-horizon imagination fidelity and reward prediction; compared to model-free baselines DreamerV2 has sample-efficiency benefits.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper implication: DreamerV2/RSSM-style models are optimal or practical choices for short-horizon or near-Markov tasks where lower memory and faster convergence matter; for long-range memory tasks consider transformer-based models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TransDreamer: Reinforcement Learning with Transformer World Models', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1405.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1405.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>World Models (Ha & Schmidhuber)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>World Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Early work demonstrating the use of learned generative models (VAE + RNN) as compact world models for policy learning via imagination and planning; cited as foundational MBRL work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>World models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>World Models (VAE + RNN generative model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pixel-level generative model combining a VAE encoder for observations and an RNN (MDN-RNN / recurrent dynamics) to model temporal dynamics and generate imagined trajectories for policy learning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (pixel-level generative plus recurrent dynamics)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Simulated control and video prediction domains (foundational experiments in learned world models for control)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Image reconstruction / likelihood, agent performance on downstream tasks</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Not quantified in this paper beyond citation; referenced as foundational prior work demonstrating benefits of learned world models.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Not assessed in this paper; original work uses generative decoders enabling inspection of reconstructed frames, but latent interpretability not emphasized here.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Visualization of generated frames (as in prior work), but no further interpretability analysis in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>RNN-based generative models with VAE encoder — typically lower memory than transformer-based alternatives; specifics not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Mentioned as part of the lineage motivating latent world models and imagination-based policy training; not directly compared empirically in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Cited as seminal demonstration that learned world models can be useful for policy learning; not experimentally evaluated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Used to motivate the utility of world models for sample-efficient RL and reusability of knowledge, but no new analysis presented here.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Paper cites RNN-based world models' limitations for long-range dependencies motivating transformer-based TSSM, implying that World Models' RNN approach may struggle with long-term memory.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>VAE for compact observations plus recurrent dynamics for temporal modeling; pixel-level decoding for visualization and interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared conceptually: RNN/VAEs enable latent imagination with less memory than transformers but are weaker at long-range direct memory access compared to transformer-based designs.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Not discussed in this paper; referenced as prior art motivating latent-space imagination.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TransDreamer: Reinforcement Learning with Transformer World Models', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1405.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1405.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MuZero</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MuZero</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model-based RL approach that learns task-specific models (value, policy, and dynamics) for planning with Monte-Carlo Tree Search; cited as an alternative MBRL approach focusing on task-specific planning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mastering atari, go, chess and shogi by planning with a learned model</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MuZero (task-specific learned model for planning)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Learns a compact model of environment dynamics, value, and policy tailored to a specific task and uses MCTS for planning, with models trained end-to-end to support planning rather than full observation reconstruction.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>task-specific learned model for planning (hybrid between latent model and planning algorithm)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Atari, board games (go, chess, shogi), planning-heavy tasks</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Task reward / game score, planning performance (MCTS results); not measured in this paper beyond citation</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Not reported in this paper; cited as state-of-the-art planning method using learned task-specific models.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Model is primarily optimized for planning performance rather than interpretable latent reconstructions; no interpretability analysis in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>MuZero relies on search (MCTS) which can be computationally intensive at inference/planning time; paper references MuZero conceptually but does not compare compute directly.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Referenced as a model-based planning approach that is task-specific and uses search; contrasted conceptually with latent world models (like RSSM/TSSM) that enable efficient latent rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Not quantified in this paper; referenced for its planning strengths.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Cited to highlight different philosophies in MBRL: task-specific planning models (MuZero) vs task-agnostic world models (Dreamer/TSSM) that aim for reusable knowledge and imagination-based policy learning.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>MuZero's task-specific planning yields strong task performance but trades off model generality; used here as a conceptual contrast to learned latent world models.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Learning dynamics/prediction modules specifically to support planning (value/policy/dynamics) and coupling with MCTS for decision making.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Conceptually compared: MuZero emphasizes task-specific learned models and planning (MCTS), while Dreamer/TSSM emphasize compact latent generative world models for imagination-based policy learning; no empirical comparison in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Not discussed in this paper; included as related work illustrating planning-focused learned models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TransDreamer: Reinforcement Learning with Transformer World Models', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Learning latent dynamics for planning from pixels <em>(Rating: 2)</em></li>
                <li>Dream to control: Learning behaviors by latent imagination <em>(Rating: 2)</em></li>
                <li>Mastering atari with discrete world models <em>(Rating: 2)</em></li>
                <li>World models <em>(Rating: 2)</em></li>
                <li>Mastering atari, go, chess and shogi by planning with a learned model <em>(Rating: 1)</em></li>
                <li>Stabilizing transformers for reinforcement learning <em>(Rating: 1)</em></li>
                <li>The monte carlo transformer: a stochastic self-attention model for sequence prediction <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1405",
    "paper_id": "paper-2fec20377bc947ec1df003b4aedcb4d7f25ac934",
    "extraction_schema_id": "extraction-schema-27",
    "extracted_data": [
        {
            "name_short": "TSSM",
            "name_full": "Transformer State-Space Model",
            "brief_description": "A stochastic transformer-based state-space world model that replaces the RNN deterministic path with a transformer to enable direct access to past stochastic states and parallel computation during training, while maintaining stochastic priors for imagination.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Transformer State-Space Model (TSSM)",
            "model_description": "Stochastic latent world model where the deterministic state h_t is produced by a transformer attending over past stochastic states and actions h_t = transformer(z_{1:t-1}, a_{1:t-1}), the posterior representation is approximated myopically as q(z_t | x_t) (removing dependence on h_t) enabling parallel computation of inputs, and the prior p(z_t | z_{1:t-1}, a_{1:t-1}) (or p(z_t | h_t)) is used for autoregressive imagination in latent space.",
            "model_type": "latent world model (stochastic transformer-based state-space model)",
            "task_domain": "Visual model-based reinforcement learning (2D & 3D partially observable navigation tasks called Hidden Order Discovery, also evaluated on DMC and Atari)",
            "fidelity_metric": "Image reconstruction/generation MSE (foreground and overall), reward prediction accuracy (classification for +3 reward within ±0.3 and zero-reward within ±0.01), and qualitative imagined-trajectory correctness",
            "fidelity_performance": "Quantitative improvements vs DreamerV2 on Hidden Order (examples): Foreground MSE (4-Ball Dense, context=60): TSSM 211.2 vs DreamerV2 281.9; Overall MSE (4-Ball Dense, context=60): 458.0 vs 577.8. Reward (non-zero) accuracy (4-Ball Dense, context=60): TSSM 46.9% vs DreamerV2 28.2%; (context=80) TSSM 73.2% vs DreamerV2 50.5%.",
            "interpretability_assessment": "Predominantly a black-box neural latent model; interpretability is limited to inspecting generated images/trajectories and prediction errors (MSE, reward accuracy). The model does not claim explicit disentangled or symbolic latent semantics.",
            "interpretability_method": "Visualization of imagined trajectories and predicted frames; quantitative inspection of foreground vs overall MSE and reward-prediction accuracy; no attention-map or latent-factor probing analysis reported.",
            "computational_cost": "Higher memory footprint than RNN-based RSSM (authors state 'increased memory requirements'); used transformer variants: 6-layer transformer with identity-map reordering for Hidden Order tasks; for DMC/Atari used 2-layer transformer, 10 attention heads; hidden dims: 200 (DMC) / 600 (Atari). Training required reducing number of imagined trajectories per batch (cannot imagine from every sampled state) — e.g., K reduced, and for DMC/Atari used 3 imagined trajectories per sample.",
            "efficiency_comparison": "Training allows parallel computation of state updates (removes sequential RNN dependence) resulting in parallelizable world-model training; however, in practice TSSM uses more memory and is slower to saturate on short-term tasks (DMC/Atari) than Dreamer/RSSM and requires reducing imagined trajectories to fit memory. The paper reports slower early convergence but comparable final performance on short-term tasks.",
            "task_performance": "On long-term memory Hidden Order Discovery tasks, TransDreamer (TSSM + Dreamer policy framework) outperforms Dreamer: e.g., 2D 4-Ball average episode reward ~7 vs ~4 and success rate 23% vs 7%; 3D tasks also show consistent improvement. On DMC and Atari (short-term tasks), TransDreamer converges to comparable returns but typically more slowly.",
            "task_utility_analysis": "Higher fidelity (better image generation MSE and reward prediction accuracy) correlates with improved task performance on long-horizon, memory-intensive tasks: TSSM's better reward prediction and more accurate long-range imagination appears to yield higher success rates and returns in Hidden Order Discovery. For short-horizon tasks, the extra capacity/compute does not provide clear runtime advantages and slows early learning.",
            "tradeoffs_observed": "TSSM vs RSSM/Dreamer: improved long-term fidelity and task utility for memory-heavy tasks at the cost of higher memory use, increased computational requirements, and slower convergence on short-horizon tasks. Design enabling parallel training (q(z_t|x_t) posterior) trades off some posterior expressivity for parallelizability. Also, the transformer must be held fixed during policy training to avoid stability issues (trading off end-to-end adaptability).",
            "design_choices": "Key choices: replace RNN deterministic state with transformer over past stochastic states/actions; approximate posterior as q(z_t|x_t) to enable parallel per-step posterior computation; use stochastic prior p(z_t|h_t) for imagination; hold TSSM parameters fixed during policy learning for stability; use prioritized replay and reduced number of imagined trajectories K to manage reward-sparsity and memory constraints; optional use of identity-map reordering/gating.",
            "comparison_to_alternatives": "Compared directly to RSSM/DreamerV2: TSSM produces lower image MSE and higher reward-prediction accuracy in long-term Hidden Order tasks and yields higher task success rates and episode returns; compared to RSSM, TSSM supports direct access to historical states and parallelizable training but costs more memory and slower early policy learning on short-term tasks. Compared to model-free baselines (not the study's focus), TSSM-based MBRL retains sample-efficiency benefits of imagination-based training.",
            "optimal_configuration": "Paper recommendations/insights: Use TSSM when tasks require long-range memory and complex temporal reasoning; employ q(z_t|x_t) posterior to enable parallel training; hold the transformer (world model) fixed during policy learning to avoid instability; use prioritized replay for sparse-reward environments and limit the number of imagined trajectories per batch to fit memory. Transformer depth/heads should be tuned per domain (authors used 6-layer for Hidden Order, 2-layer for DMC/Atari).",
            "uuid": "e1405.0",
            "source_info": {
                "paper_title": "TransDreamer: Reinforcement Learning with Transformer World Models",
                "publication_date_yy_mm": "2022-02"
            }
        },
        {
            "name_short": "RSSM",
            "name_full": "Recurrent State-Space Model",
            "brief_description": "A stochastic recurrent latent dynamics model (deterministic recurrent state h_t plus stochastic latent z_t) used as the core world model in Dreamer; updates sequentially via an RNN (GRU/LSTM) and supports latent-space rollouts for imagination.",
            "citation_title": "Learning latent dynamics for planning from pixels",
            "mention_or_use": "use",
            "model_name": "Recurrent State-Space Model (RSSM)",
            "model_description": "Latent state s_t = [h_t, z_t] where deterministic h_t = gru(h_{t-1}, z_{t-1}, a_{t-1}) and stochastic z_t sampled from a posterior q(z_t | h_t, x_t) and prior p(z_t | h_t). Observation and reward predictors decode from s_t. Designed for sequential latent rollouts and ELBO-based training.",
            "model_type": "latent world model (stochastic recurrent state-space model)",
            "task_domain": "Visual model-based reinforcement learning (Dreamer experiments across DMC, Atari, and Hidden Order baselines)",
            "fidelity_metric": "Image reconstruction/generation MSE (foreground and overall), reward prediction accuracy (classification around true reward values), ELBO components / KL used during training",
            "fidelity_performance": "Baseline numbers (DreamerV2 / RSSM variants): Foreground MSE (4-Ball Dense, context=60): 281.9 (DreamerV2) vs TSSM 211.2; Overall MSE (4-Ball Dense, context=60): 577.8. Reward (non-zero) accuracy (4-Ball Dense, context=60): DreamerV2 28.2% vs TSSM 46.9%. (These are reported baseline performance values in paper tables for RSSM/DreamerV2.)",
            "interpretability_assessment": "Black-box neural latent model; interpretability limited to inspecting reconstructions/imagination and error metrics; no claim of interpretable latent factors in this paper.",
            "interpretability_method": "Qualitative visualization of imagined frames and quantitative metrics (MSE, reward-accuracy). No attention-based explanations (RNN-based).",
            "computational_cost": "Sequential computation due to RNN update (h_t depends on h_{t-1}) so training is inherently sequential in time; lower memory footprint than transformer-based TSSM enabling imagining from every state sampled in a batch (Dreamer imagines from all sampled states); overall more memory- and compute-efficient per-batch compared to TSSM.",
            "efficiency_comparison": "Compared to TSSM, RSSM has lower memory requirements, can generate imagined trajectories from every sampled start state (higher effective number of imagined rollouts per batch), and typically learns faster on short-horizon tasks; however, it lacks the transformer advantages for direct long-range access and parallelization.",
            "task_performance": "Per paper: RSSM/DreamerV2 achieves competitive results on short-term DMC and Atari tasks and is outperformed by TSSM-based TransDreamer on long-horizon Hidden Order tasks (example success rates: 2D 4-Ball DreamerV2 success 7% vs TransDreamer 23%).",
            "task_utility_analysis": "RSSM's inductive bias towards recent history makes it efficient for tasks requiring short-term dynamics modeling, enabling faster early convergence and efficient rollouts, but this inductive bias limits performance on tasks requiring long-range memory and complex relational reasoning.",
            "tradeoffs_observed": "RSSM gains computational and sample-efficiency for short-horizon tasks due to sequential, compact recurrence and ability to imagine from many start points, but sacrifices ability to utilize long contexts and direct access to entire history, yielding lower fidelity on long-horizon imagination and reward prediction compared to TSSM.",
            "design_choices": "Uses an RNN (GRU/LSTM) for deterministic h_t updates; posterior conditioned on h_t and x_t q(z_t | h_t, x_t); stochastic z_t concatenated with h_t to form state s_t; ELBO-based training with reconstruction and KL terms; allows efficient latent-space rollouts.",
            "comparison_to_alternatives": "Compared to TSSM: RSSM is less capable at leveraging long contexts and produces worse long-horizon imagination and reward prediction in the studied tasks, but is more memory-efficient and faster to train on short-term tasks. Compared to model-free methods, RSSM (in Dreamer) provides sample efficiency via imagined rollouts.",
            "optimal_configuration": "For short-term, near-Markovian control tasks, RSSM (as used in Dreamer) is recommended because of its inductive bias and computational efficiency; the paper suggests transformer replacement (TSSM) when tasks require direct long-term memory access.",
            "uuid": "e1405.1",
            "source_info": {
                "paper_title": "TransDreamer: Reinforcement Learning with Transformer World Models",
                "publication_date_yy_mm": "2022-02"
            }
        },
        {
            "name_short": "DreamerV2",
            "name_full": "DreamerV2 (Dreamer with discrete world model variants)",
            "brief_description": "An improved Dreamer agent variant that uses discrete latent states and other training enhancements (straight-through gradients, KL balancing) to achieve strong performance, notably on Atari; used as the main baseline in the paper.",
            "citation_title": "Mastering atari with discrete world models",
            "mention_or_use": "use",
            "model_name": "DreamerV2 world model (RSSM-based with discrete latent states)",
            "model_description": "World model variant that uses discrete stochastic latent states instead of continuous ones, straight-through gradients for differentiating through discrete states/actions, and KL balancing; based on the RSSM/deterministic recurrent backbone but with discrete latent representations for improved performance on certain domains (e.g., Atari).",
            "model_type": "latent world model (discrete stochastic recurrent state-space model)",
            "task_domain": "Atari benchmark and other visual control tasks; used here as the baseline on Hidden Order, DMC, and Atari evaluations.",
            "fidelity_metric": "Image generation MSE (foreground and overall) and reward prediction accuracy for +3 reward timesteps and zero-reward timesteps (as in paper tables)",
            "fidelity_performance": "Reported baseline numbers: Foreground MSE (4-Ball Dense, context=60): 281.9; Overall MSE (4-Ball Dense, context=60): 577.8. Reward (non-zero) accuracy (4-Ball Dense, context=60): 28.2%. Performance is consistently lower than TSSM on long-horizon tasks in these metrics.",
            "interpretability_assessment": "Similar to RSSM: primarily black-box latent model; analysis in the paper limited to reconstructed frames and reward-prediction accuracy.",
            "interpretability_method": "Evaluation by image/trajectory visualization and MSE/reward-accuracy metrics; no explicit interpretability methods reported in this paper.",
            "computational_cost": "Comparable to RSSM (recurrent computation); lower memory footprint than TSSM; DreamerV2 imagines from all sampled states in a batch (unlike TransDreamer which reduces imagined trajectories due to TSSM memory costs).",
            "efficiency_comparison": "More computationally efficient per-batch imagination than TSSM because it can produce imagined rollouts from every sampled state; tends to converge faster on short-horizon DMC/Atari tasks.",
            "task_performance": "Per experiments in paper: DreamerV2 attains competitive performance on short-term tasks and acts as the primary baseline; on Hidden Order (long-term) tasks it underperforms compared to TransDreamer (examples: 2D 4-Ball success 7% vs 23%).",
            "task_utility_analysis": "Discrete-latent RSSM variants like DreamerV2 are effective for many control tasks and Atari due to stable training tricks (discrete latents, straight-through grads), but their ability to leverage very long contexts is limited compared to transformer-based TSSM.",
            "tradeoffs_observed": "Discrete latent + recurrent design yields good short-term sample efficiency and practical training stability at lower memory cost, but reduced long-range context usage and lower long-horizon fidelity than TSSM.",
            "design_choices": "Uses discrete latent variables, straight-through gradient estimators, KL-balancing; imagines from all sampled states in replay to maximize data for policy learning; follows Dreamer framework.",
            "comparison_to_alternatives": "Compared to TSSM: DreamerV2 is more memory-efficient, learns faster on short-term tasks, but produces worse long-horizon imagination fidelity and reward prediction; compared to model-free baselines DreamerV2 has sample-efficiency benefits.",
            "optimal_configuration": "Paper implication: DreamerV2/RSSM-style models are optimal or practical choices for short-horizon or near-Markov tasks where lower memory and faster convergence matter; for long-range memory tasks consider transformer-based models.",
            "uuid": "e1405.2",
            "source_info": {
                "paper_title": "TransDreamer: Reinforcement Learning with Transformer World Models",
                "publication_date_yy_mm": "2022-02"
            }
        },
        {
            "name_short": "World Models (Ha & Schmidhuber)",
            "name_full": "World Models",
            "brief_description": "Early work demonstrating the use of learned generative models (VAE + RNN) as compact world models for policy learning via imagination and planning; cited as foundational MBRL work.",
            "citation_title": "World models",
            "mention_or_use": "mention",
            "model_name": "World Models (VAE + RNN generative model)",
            "model_description": "Pixel-level generative model combining a VAE encoder for observations and an RNN (MDN-RNN / recurrent dynamics) to model temporal dynamics and generate imagined trajectories for policy learning.",
            "model_type": "latent world model (pixel-level generative plus recurrent dynamics)",
            "task_domain": "Simulated control and video prediction domains (foundational experiments in learned world models for control)",
            "fidelity_metric": "Image reconstruction / likelihood, agent performance on downstream tasks",
            "fidelity_performance": "Not quantified in this paper beyond citation; referenced as foundational prior work demonstrating benefits of learned world models.",
            "interpretability_assessment": "Not assessed in this paper; original work uses generative decoders enabling inspection of reconstructed frames, but latent interpretability not emphasized here.",
            "interpretability_method": "Visualization of generated frames (as in prior work), but no further interpretability analysis in this paper.",
            "computational_cost": "RNN-based generative models with VAE encoder — typically lower memory than transformer-based alternatives; specifics not discussed here.",
            "efficiency_comparison": "Mentioned as part of the lineage motivating latent world models and imagination-based policy training; not directly compared empirically in this paper.",
            "task_performance": "Cited as seminal demonstration that learned world models can be useful for policy learning; not experimentally evaluated in this paper.",
            "task_utility_analysis": "Used to motivate the utility of world models for sample-efficient RL and reusability of knowledge, but no new analysis presented here.",
            "tradeoffs_observed": "Paper cites RNN-based world models' limitations for long-range dependencies motivating transformer-based TSSM, implying that World Models' RNN approach may struggle with long-term memory.",
            "design_choices": "VAE for compact observations plus recurrent dynamics for temporal modeling; pixel-level decoding for visualization and interpretability.",
            "comparison_to_alternatives": "Compared conceptually: RNN/VAEs enable latent imagination with less memory than transformers but are weaker at long-range direct memory access compared to transformer-based designs.",
            "optimal_configuration": "Not discussed in this paper; referenced as prior art motivating latent-space imagination.",
            "uuid": "e1405.3",
            "source_info": {
                "paper_title": "TransDreamer: Reinforcement Learning with Transformer World Models",
                "publication_date_yy_mm": "2022-02"
            }
        },
        {
            "name_short": "MuZero",
            "name_full": "MuZero",
            "brief_description": "A model-based RL approach that learns task-specific models (value, policy, and dynamics) for planning with Monte-Carlo Tree Search; cited as an alternative MBRL approach focusing on task-specific planning.",
            "citation_title": "Mastering atari, go, chess and shogi by planning with a learned model",
            "mention_or_use": "mention",
            "model_name": "MuZero (task-specific learned model for planning)",
            "model_description": "Learns a compact model of environment dynamics, value, and policy tailored to a specific task and uses MCTS for planning, with models trained end-to-end to support planning rather than full observation reconstruction.",
            "model_type": "task-specific learned model for planning (hybrid between latent model and planning algorithm)",
            "task_domain": "Atari, board games (go, chess, shogi), planning-heavy tasks",
            "fidelity_metric": "Task reward / game score, planning performance (MCTS results); not measured in this paper beyond citation",
            "fidelity_performance": "Not reported in this paper; cited as state-of-the-art planning method using learned task-specific models.",
            "interpretability_assessment": "Model is primarily optimized for planning performance rather than interpretable latent reconstructions; no interpretability analysis in this paper.",
            "interpretability_method": "Not discussed in this paper.",
            "computational_cost": "MuZero relies on search (MCTS) which can be computationally intensive at inference/planning time; paper references MuZero conceptually but does not compare compute directly.",
            "efficiency_comparison": "Referenced as a model-based planning approach that is task-specific and uses search; contrasted conceptually with latent world models (like RSSM/TSSM) that enable efficient latent rollouts.",
            "task_performance": "Not quantified in this paper; referenced for its planning strengths.",
            "task_utility_analysis": "Cited to highlight different philosophies in MBRL: task-specific planning models (MuZero) vs task-agnostic world models (Dreamer/TSSM) that aim for reusable knowledge and imagination-based policy learning.",
            "tradeoffs_observed": "MuZero's task-specific planning yields strong task performance but trades off model generality; used here as a conceptual contrast to learned latent world models.",
            "design_choices": "Learning dynamics/prediction modules specifically to support planning (value/policy/dynamics) and coupling with MCTS for decision making.",
            "comparison_to_alternatives": "Conceptually compared: MuZero emphasizes task-specific learned models and planning (MCTS), while Dreamer/TSSM emphasize compact latent generative world models for imagination-based policy learning; no empirical comparison in this paper.",
            "optimal_configuration": "Not discussed in this paper; included as related work illustrating planning-focused learned models.",
            "uuid": "e1405.4",
            "source_info": {
                "paper_title": "TransDreamer: Reinforcement Learning with Transformer World Models",
                "publication_date_yy_mm": "2022-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Learning latent dynamics for planning from pixels",
            "rating": 2
        },
        {
            "paper_title": "Dream to control: Learning behaviors by latent imagination",
            "rating": 2
        },
        {
            "paper_title": "Mastering atari with discrete world models",
            "rating": 2
        },
        {
            "paper_title": "World models",
            "rating": 2
        },
        {
            "paper_title": "Mastering atari, go, chess and shogi by planning with a learned model",
            "rating": 1
        },
        {
            "paper_title": "Stabilizing transformers for reinforcement learning",
            "rating": 1
        },
        {
            "paper_title": "The monte carlo transformer: a stochastic self-attention model for sequence prediction",
            "rating": 1
        }
    ],
    "cost": 0.0181525,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>TRANSDRFAMER: REINFORCEMENT LEARNING WITH TRANSFORMER WORLD MODELS</h1>
<p>Chang Chen ${ }^{11}$, Yi-Fu Wu ${ }^{11}$, Jaesik Yoon ${ }^{11}$, Sungjin Ahn ${ }^{1,2}$<br>${ }^{1}$ Rutgers University \&amp; ${ }^{2}$ KAIST</p>
<h4>Abstract</h4>
<p>The Dreamer agent provides various benefits of Model-Based Reinforcement Learning (MBRL) such as sample efficiency, reusable knowledge, and safe planning. However, its world model and policy networks inherit the limitations of recurrent neural networks and thus an important question is how an MBRL framework can benefit from the recent advances of transformers and what the challenges are in doing so. In this paper, we propose a transformer-based MBRL agent, called TransDreamer. We first introduce the Transformer State-Space Model, a world model that leverages a transformer for dynamics predictions. We then share this world model with a transformer-based policy network and obtain stability in training a transformer-based RL agent. In experiments, we apply the proposed model to 2D visual RL and 3D first-person visual RL tasks both requiring long-range memory access for memory-based reasoning. We show that the proposed model outperforms Dreamer in these complex tasks.</p>
<h2>1 INTRODUCTION</h2>
<p>Model-based reinforcement learning (MBRL) (Sutton, 1991) provides a solution for many problems of current reinforcement learning. Its imagination-based training provides sample efficiency by fully leveraging the interaction experience via the world model (Ha \&amp; Schmidhuber, 2018). The world model can be considered a form of task-agnostic general knowledge enabling reusability of the knowledge about the environment in many downstream tasks (Sekar et al., 2020), and finally, the dynamics model makes planning possible (Schrittwieser et al., 2020; Hafner et al., 2018) for accurate and safe decisions (Berkenkamp et al., 2017; Kidambi et al., 2020; Lu et al., 2020).</p>
<p>Among the recent advances in MBRL, a particularly notable one is the Dreamer agent (Hafner et al., 2019; 2020). Learning a world model in latent representation space, Dreamer is the first visual MBRL model that achieves performance and sample efficiency better than model-free approaches such as Rainbow (Hessel et al., 2018) and IQN (Dabney et al., 2018).</p>
<p>To deal with partial observability (Kaelbling et al., 1998), the dynamics models in MBRL have been implemented using recurrent neural networks (RNNs) (Hafner et al., 2019; 2020; Schrittwieser et al., 2020; Kaiser et al., 2019). However, Transformers (Vaswani et al., 2017; Dai et al., 2019) have shown to be more effective than RNNs in many domains requiring long-term dependency and direct access to memory for a form of memory-based reasoning (Ritter et al., 2020; Banino et al., 2020). Also, it has been shown that training complex policy networks based on transformers using only rewards is difficult (Parisotto et al., 2020), so learning a transformer-based world model where the training signal is more diverse may facilitate learning. Therefore, it is important to investigate how to make an MBRL agent using a transformer-based world model and to analyze the benefits and challenges in doing so.</p>
<p>In this paper, we propose a transformer-based MBRL agent, called TransDreamer. As implied by the name, the proposed model inherits from the Dreamer framework, but aims to bring the benefits of transformers into it. Seemingly, it may seem like a simple plug-in task to replace an RNN with a transformer. However, there are a few critical challenges to make it work. First of all, we need to develop a new transformer-based world model that supports effective stochastic action-conditioned transitions in the latent space to implement the prior and posterior of the transition model. At the</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>same time, this model should also preserve the parallel trainability of transformers for computational efficiency. To the best of our knowledge, there is no such model yet. Also, as shown in Parisotto et al. (2020), finding an architecture, hyperparameters, and other design choices to make a working model is particularly challenging for Transformer-based RL models.</p>
<p>The main contribution of this paper is the first transformer-based MBRL agent. We introduce the Transformer State-Space Model (TSSM) as the first transformer-based stochastic world model. Using this world model in the Dreamer framework, we propose TransDreamer, a fully transformerbased MBRL framework. In experiments, we show that TransDreamer outperforms Dreamer on tasks that require long-term and complex memory interactions, and the world model of TransDreamer is better than Dreamer at predicting rewards and future frames for imagination. Furthermore, we also show that the performance of TransDreamer is comparable to Dreamer on a few simple DMC (Tassa et al., 2018) and Atari (Bellemare et al., 2013) tasks that do not require long-term memory.</p>
<h1>2 Preliminaries</h1>
<p>Our model builds on top of Dreamer (Hafner et al., 2019; 2020), a model-based reinforcement learning framework (Sutton, 1991) for visual control in a partially observable Markov decision process (POMDP) (Kaelbling et al., 1998). Dreamer consists of three main steps: (1) world model learning, (2) policy learning, and (3) environment interaction. These steps are cycled until convergence. Specifically, a dynamics model of the environment (i.e., world model) and a reward function are learned to fit the data in an experience replay buffer. Then, an actor-critic policy is trained on imagined experience, i.e., hypothetical trajectories generated by simulating the learned world model. Lastly, to provide fresh experiences to the replay buffer, the agent collects trajectory data by executing the trained policy in the real environment.</p>
<h3>2.1 World Model in DreamER</h3>
<p>The backbone of the world model used in Dreamer is a stochastic recurrent neural network, called the Recurrent State-Space Model (RSSM) (Hafner et al., 2018). Depicted in Figure 1a, the RSSM represents a latent state $s_{t}$ by the concatenation of a stochastic state $z_{t}$ and a deterministic state $h_{t}$ which are updated by $z_{t} \sim p\left(z_{t} \mid h_{t}\right)$ and $h_{t}=f\left(h_{t-1}, z_{t-1}, a_{t-1}\right)$, respectively. Here, $a_{t-1}$ is an action and the deterministic update $f$ is the state update rule of a recurrent neural network (RNN) such as an LSTM (Hochreiter \&amp; Schmidhuber, 1997) or GRU (Chung et al., 2014). While the deterministic path helps to model the temporal dependency in the world model, it also inherits the limitations of RNNs, particularly when compared to the benefits of transformers. The stochastic state makes it possible to capture the stochastic nature of the world, e.g., for imagining multiple hypothetical future trajectories. Crucially, using the above models, rollouts can be executed efficiently in a compact latent space without the need to generate observation images.</p>
<p>Learning the RSSM is via the maximization of evidence lower bound (Jordan et al., 1999). The representation model $z_{t} \sim q\left(z_{t} \mid h_{t}, x_{t}\right)$ infers the stochastic state given an observation $x_{t}$. Whenever a new observation is provided, the current state is updated by the representation model. The observation model $p\left(x_{t} \mid s_{t}\right)$ and the reward model $p\left(r_{t} \mid s_{t}\right)$ are then used for the reconstruction of observation $x_{t}$ and reward $r_{t}$ from the latent state. All component models are listed in Table 1.</p>
<h3>2.2 Policy Learning in DreamER</h3>
<p>After updating the world model for a number of iterations, Dreamer updates its policy $\pi_{\phi}\left(a_{t} \mid s_{t}\right)$. The policy learning is done without interaction with the actual environment; it uses imagined trajectories obtained by simulating the learned world model. Specifically, from each state $s_{t}$ obtained from a batch sampled from the replay buffer, it generates a future trajectory of length $H$ using the RSSM world model and the current policy as the behavior policy for the imagination. Then, for each state in the trajectory, the rewards $p_{\theta}\left(r_{t} \mid s_{t}\right)$ and the values $v_{\psi}\left(s_{t}\right)$ are estimated. This allows us to compute the value estimate $V\left(s_{t}\right)$, e.g., by the discounted sum of the predicted rewards and the bootstrapped value $v\left(s_{t+H}\right)$ at the end of the trajectory. See Hafner et al. (2019) for more details and other options about the value estimation.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: RSSM and TSSM. The red arrow in RSSM makes sequential computational necessary. In TSSM, we replace this by a transformer. In addition, the purple arrow should also be removed in TSSM because it also prevents parallelizing the updates of all time steps.</p>
<p>Learning the policy in Dreamer means updating two models, the policy $\pi_{\phi}\left(a_{t} \mid s_{t}\right)$ and the value model $v_{\psi}\left(s_{t}\right)$. For updating the policy, Dreamer uses the sum of the value estimates of the simulated trajectories, $\sum_{\tau=t}^{t+T} V\left(s_{\tau}\right)$, to construct the objective function. While we can compute the gradient of this objective w.r.t. the parameters of the policy $\phi$ via a policy gradient method such as REINFORCE (Williams, 1992), Dreamer also takes advantage of the differentiability of the learned world model by directly backpropagating from the value function to the world model, and to the parameters of the policy network. This provides gradients of lower variance than that of REINFORCE. Updating the value model parameter $\psi$ is done via temporal difference learning with the value estimate $V\left(s_{t}\right)$ as the target.</p>
<h1>3 TransDreamer</h1>
<p>Transformers have been shown to outperform RNNs in many tasks in both NLP and computer vision. In particular, their ability to directly access historical states and to learn complex interactions among them, has been shown to excel in tasks that require complex long-term temporal dependencies such as memory-based reasoning (Ritter et al., 2020; Banino et al., 2020). Furthermore, they have been shown to be effective for temporal generation in both language and visual domains. Observing that both of these abilities are important and desirable properties for a robust world model, we hypothesize that a model-based agent based on transformers can outperform the RNN-based Dreamer agent for tasks requiring complex and long-term memory dependency.</p>
<h3>3.1 Transformer State Space Model (TSSM)</h3>
<p>In the design of a transformer-based world model, we aim to achieve the following desiderata; (i) to directly access past states, (ii) to update the states of each time step in parallel during training, (iii) to be able to roll out sequentially for trajectory imagination at test time, and (iv) to be a stochastic latent variable model. To our knowledge, no such model is available. The RSSM does not satisfy (i) and (ii). Although we can consider a simple modification of the RSSM, a memory-augmented RSSM, by introducing direct attention to the past states in the RNN state update (Ke et al., 2018) and make (i) satisfied as well, it still does not satisfy (ii) and thus remains computationally inefficient. Traditional transformers are deterministic and thus do not satisfy (iv). This motivates us to introduce the Transformer State-Space Model (TSSM).</p>
<p>The TSSM is a stochastic transformer-based state-space model. Figure 1 illustrates the architectures of TSSM in comparison to RSSM. In the RSSM, the main source of the sequentially-dependent computation is the RNN-based state update $h_{t}=f_{\text {gru }}\left(h_{t-1}, z_{t-1}, a_{t-1}\right)$, depicted in red arrows in Figure 1a. This means that all component models of the RSSM are computed sequentially because they all take the hidden state as input. To remove this sequential computation and enable direct access to and complex interaction of the historical states, we propose employing a transformer as a replacement for the RNN. Unlike the RSSM which accesses the past indirectly only via a compression $h_{t-1}$, the transformer is allowed to directly access the sequence of stochastic states and actions</p>
<p>Table 1: Comparison of the Component Models of RSSM and TSSM.</p>
<table>
<thead>
<tr>
<th></th>
<th>RSSM</th>
<th>TSSM</th>
</tr>
</thead>
<tbody>
<tr>
<td>Deterministic state model</td>
<td>$h_{t}=\text{gru}(h_{t-1},z_{t-1},a_{t-1})$</td>
<td>$h_{t}=\text{transformer}(z_{1:t-1},a_{1:t-1})$</td>
</tr>
<tr>
<td>Representation model</td>
<td>$z_{t}\sim q(z_{t}</td>
<td>h_{t},x_{t})$</td>
</tr>
<tr>
<td>Stochastic state model</td>
<td>$\hat{z}<em t="t">{t}\sim p(\hat{z}</em></td>
<td>h_{t})$</td>
</tr>
<tr>
<td>Image predictor</td>
<td>$\hat{x}<em t="t">{t}\sim p(\hat{x}</em></td>
<td>h_{t},z_{t})$</td>
</tr>
<tr>
<td>Reward predictor</td>
<td>$\hat{r}<em t="t">{t}\sim p(\hat{r}</em></td>
<td>h_{t},z_{t})$</td>
</tr>
<tr>
<td>Discount predictor</td>
<td>$\hat{\gamma}<em t="t">{t}\sim p(\hat{\gamma}</em></td>
<td>h_{t},z_{t})$</td>
</tr>
</tbody>
</table>
<p>of the past at every time step, i.e., $h_{t}=f_{\text{transformer}}\left(z_{1: t-1}, a_{1: t-1}\right)$. If all $h_{t}$ can be computed in parallel in this way, then all components taking $h_{t}$ as input can also be computed in parallel.</p>
<p>Myopic Representation Model. This hope, however, is broken due to the representation model $q\left(z_{t} \mid h_{t}, x_{t}\right)$. This is because unlike other component models listed in Table 1, the output of the representation model is used as the input to the transformer. Since a transformer should not use an output also as an input to achieve parallel training, the representation model should not be conditioned on $h_{t}$. That is, the purple arrows in Figure 1 should be removed. To this end, we propose approximating the posterior representation model by $q\left(z_{t} \mid x_{t}\right)$, removing $h_{t}$. Since the posterior can now be computed independently for each time step, we can compute all of the inputs $z_{1: t-1}$ simultaneously. Then, with a single forward pass through the transformer, we can obtain $h_{1: t}$.</p>
<p>One may argue that removing $h_{t}$ and thereby ignoring all the history $z_{1: t-1}$ in the representation model may result in a poor approximation. This can be true if we use only $z_{t}$ as the full state of our model. However, like the RSSM, the full state of our model is a concatenation of the stochastic state $z_{t}$ and the deterministic state $h_{t}$. Therefore, we hypothesize that encoding temporal information in the stochastic states may not be essential for model performance because that information is provided by the deterministic states. Furthermore, the trajectory imagination does not require the representation model but only requires the stochastic state model $p\left(z_{t} \mid h_{t}\right)$ that can still use $h_{t}$. We observe that if this hypothesis is correct, a modified Dreamer using $q\left(z_{t} \mid x_{t}\right)$ instead of $q\left(z_{t} \mid x_{t}, h_{t}\right)$ would perform similarly to the original Dreamer and the plot on the right confirms this. Another possible yet more complex choice for the representation model is to condition the posterior representation directly on all the past observations using another transformer layer, i.e., $q\left(z_{t} \mid f_{\text {transformer }}\left(x_{1: t}, a_{1: t}\right)\right)$. However, due to its increased complexity, we do not consider this model.</p>
<p>Imagination. During imagination, we use the prior stochastic state $\hat{z}<em t="t">{t} \sim p(\hat{z} \mid h</em>)$ as the input to the transformer to autoregressively generate future states as shown in Figure 6 in the Appendix. This allows the agent to imagine future states completely in the latent space but with more direct access to the historical states than the RSSM. In this way, the TSSM achieves all the desiderata discussed above. Table 1 highlights the key differences between the RSSM and the TSSM.</p>
<p>The loss function is almost the same as that of the RSSM. The difference is that we approximate the representation posterior $p\left(z_{1: t} \mid x_{1: t}\right)$ by $\prod_{\tau=1}^{t} q_{\phi}\left(z_{\tau} \mid x_{\tau}\right)$ instead of $\prod_{\tau=1}^{t} q_{\phi}\left(z_{\tau} \mid z_{1: \tau-1}, x_{1: \tau}\right)$ used in the RSSM. The loss function can be found in Appendix A. 2 with the derivation of the ELBO.</p>
<h1>3.2 Policy Learning and Implementation Details</h1>
<p>Policy Learning. The policy learning in TransDreamer inherits the general framework of Dreamer described in Section 2.2. The main difference is that we replace the RSSM with the TSSM. The component models are thus based on the states from the TSSM which can capture the long-term and complex temporal dependency better. Since the TSSM is fully differentiable, we can similarly use both REINFORCE and dynamics backpropagation to train the policy. The TSSM parameters are held fixed when training the agent.</p>
<p>Training Stability. Transformers have notably had stability issues when used in RL settings, especially in cases where the rewards are sparse. GTrXL (Parisotto et al., 2020), in particular, adds GRU gating layers to try to alleviate this problem. In TransDreamer, however, since the transformer</p>
<p>parameters are held fixed during agent training and only trained to predict images, rewards, and discounts, we find that we do not encounter similar stability issues even without any additional gating.</p>
<p>Prioritized Replay. Since the training signal for agent training is based on only the rewards the agent receives, the reward prediction in the world model is especially important for learning a good agent. Learning a good reward predictor, on the other hand, relies on the agent having a good enough policy so that it can collect trajectories with reward, especially in environments with sparse rewards. To facilitate this, we optionally weight the replay buffer so that trajectories with higher rewards are more likely to be sampled. In particular, we sample only from nonzero-reward trajectories $\alpha$ percentage of the time where $\alpha \in[0,1]$. The remaining trajectories are sampled uniformly across the replay buffer.</p>
<p>Number of Imagination Trajectories. Due to the increased memory requirements of transformers compared with RNNs, we find that it is not feasible to generate imagined trajectories from every state sampled from the replay buffer, as is done in Dreamer. Instead, we randomly choose a smaller subset of starting states of size $K$ to generate imagined trajectories from. While this effectively reduces the number of trajectories the agent can learn from in any given iteration, we find that we are still able to achieve performance comparable or better than Dreamer.</p>
<h1>4 Related Works</h1>
<p>Transformers in RL. Transformers have been used successfully in diverse domains including NLP (Vaswani et al., 2017; Devlin et al., 2019; Radford \&amp; Narasimhan, 2018; Radford et al., 2019; Brown et al., 2020), computer vision (Chen et al., 2020; Dosovitskiy et al., 2021; Parmar et al., 2018), and video generation (Weissenborn et al., 2020; Yan et al., 2021). Parisotto et al. (2020) address the problem of using transformers in RL and showed that adding gating layers on top of the transformers layers can stabilize training. Subsequent works addressed the increased computational load of using a transformer for an agent's policy (Irie et al., 2021; Parisotto \&amp; Salakhutdinov, 2021). Chen et al. (2021); Janner et al. (2021) take a different approach by modeling the RL problem as a sequence modeling problem and use a transformer to predict actions without additional networks for an actor or critic. Several recent works also explore long-term video generation with transformers which is related to building world models using transformer-based architectures (Wu et al., 2021; Creswell et al., 2021).</p>
<p>Stochastic Transformers. Stochasticity has been added to several transformer-based architectures in the context of response generation (Lin et al., 2020), sign language translation (Voskou et al., 2021), story completion (Wang \&amp; Wan, 2019), and layout generation (Arroyo et al., 2021). Martin et al. (2020) introduce the Sequential Monte Carlo Transformer which adds stochastic hidden states to the network architecture and outputs a distribution of predictions allowing for uncertainty quantification. To our knowledge, no previous work investigates stochastic transformers in the context of world models and MBRL.</p>
<p>Model-based RL. Dyna (Sutton, 1991) introduced a general framework for MBRL that our model is based on. SimPle (Kaiser et al., 2019) adopts this framework by making predictions at the pixel level and training a PPO agent on that model. Our work mainly builds off of the Dreamer (Hafner et al., 2019; 2020) framework. The RSSM is first introduced in PlaNet (Hafner et al., 2018) where it is used for planning in the latent space. Ha \&amp; Schmidhuber (2018) use a VAE with an RNN as the world model and learns a policy with an evolution strategy. MuZero (Schrittwieser et al., 2020) uses task-specific rewards to build a model and Monte-Carlo Tree Search to solve RL tasks.</p>
<h2>5 EXPERIMENTS</h2>
<p>In this section, we compare TransDreamer and Dreamer on a variety of tasks, from tasks that require long-term memory and reasoning to tasks that can be solved with only short-term memory. We try to answer the following three questions: 1) How do TransDreamer and Dreamer perform in tasks that require long-term memory and reasoning? 2) How do the learned world models of TransDreamer and Dreamer compare? 3) Can TransDreamer also work comparably to Dreamer in environments that require short-term memory?</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Hidden Order Discovery Environments.
To answer the first question, we created a new set of tasks called Hidden Order Discovery that is inspired by the Numpad task (Humplik et al., 2019; Parisotto et al., 2020). We create both 2D and 3D versions of this task. The 2D environment, built with the Minigrid (Chevalier-Boisvert et al., 2018) framework, provides a top-down view of the agent navigating a room while the 3D environment, built with Unity (Juliani et al., 2020), provides a more partially observable and visually rich firstperson view of the agent. These tasks require long-term memory and reasoning to solve. To answer the second question, we thoroughly analyze the quality of the world model that is learned in solving these tasks both quantitatively and qualitatively. Lastly, to answer the third question, we compared TransDreamer and Dreamer on some tasks in DeepMind Control Suite (DMC) (Tassa et al., 2018) and Atari (Bellemare et al., 2013). These tasks are almost fully observable and do not require longterm memory and reasoning to solve.</p>
<h1>5.1 Hidden Order Discovery in 2D Object Room</h1>
<p>To evaluate our model on the tasks that require long-term memory, we created a new task called Hidden Order Discovery inspired by the NumPad task (Humplik et al., 2019; Parisotto et al., 2020). In this task, there are several color balls in a 2D grid, as shown in Fig. 2a. The agent (illustrated by the red triangle) can only see the highlighted area in front of it, so this is a partially observable environment. For each episode, there is a hidden order of balls and the agent must collect the balls in the correct order. If the agent fails, the map is reset, and the agent needs to start from the first ball. Note that when the map is reset, the agent position and the hidden order are not changed but all the balls are reset to their initial positions. Therefore, to find the hidden ball order efficiently, the agent can benefit from remembering what it has tried in the past in the current episode. When a ball is collected in the correct order, a reward of +3 is given, but if the map is reset due to the agent collecting the incorrect ball, the rewards for balls visited in previous tries are 0 . This prevents the agent from collecting a high reward from just constantly revisiting the first ball in the sequence. When the agent successfully collects every ball in the hidden order, the map and rewards are reset. The hidden order and ball positions are randomized per episode.</p>
<p>We evaluate in environments with 4, 5, and 6 balls where the grid size is 8 x 8 cells, and the agent is given 100 time steps to collect as much reward as possible. The results are shown in Figure 3. We see that TransDreamer outperforms Dreamer in all of these configurations. Since the reward for correctly collecting one ball is +3 , an average reward of 3 means that on average the agent collects the first ball correctly, and an average reward of 6 means that on average the agent collects the first two balls correctly, and so on. For the 4-Ball configuration, TransDreamer reaches an episode reward of around 7 while Dreamer's episode reward is around 4. This means that in the 4-Balls setting, TransDreamer averages collecting over two balls in the correct order, whereas Dreamer only collects one ball in the correct order.</p>
<p>To obtain further understanding beyond the averaged behavior, we also measure the success rate of each agent, defined as the percentage of trajectories where an agent collects all balls in the correct order at least once. For the 4-Ball configuration, we find that TransDreamer has a success ratio of $23 \%$ and Dreamer has a success ratio of only $7 \%$, providing further evidence that TransDreamer can better solve this task than Dreamer. A full comparison of the success ratio is reported in Appendix Table 3. The difficulty of this task increases as the number of balls increases since with more balls, there are more combinations for the agent to try before determining the hidden order. Thus, we see the performance for both degrade as the number of balls increases.</p>
<p>We emphasize the difficulty of this task. Because the hidden order is randomized in each episode, in the worst-case scenario, discovering the first ball in the given order would require searching through all 4 balls. Then, determining the second ball in the sequence would require searching among the remaining 3 balls, while always remembering what has happened before in order not to waste</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Comparison between DreamerV2 and TransDreamer in Hidden Order Discovery tasks
time by visiting balls already known to be incorrect. The higher scores for TransDreamer provides some evidence that the transformer-based architecture is effective in tasks that require this long-term memory and reasoning.</p>
<h1>5.2 Hidden Order Discovery in 3D Object Room</h1>
<p>To evaluate this task in a more realistic environment, we also implemented a 3D version of Hidden Order Discovery in Unity (Juliani et al., 2020). The reward structure is the same as the 2D task, but the agent view is a 3D first-person view. Figure 2b shows an overview view of the different configurations and Figure 2c shows the agent's first-person view. Compared to the 2D environment, since the environment is larger, it takes more steps to navigate to the balls, especially in the sparse setting. Therefore, with the 3D environment, we can evaluate how TransDreamer can handle longterm dependency and complex reasoning more clearly.
We implemented 3 settings for this task. The 4-Ball and 5-Ball Dense environments have 4 and 5 balls, separated by at least one ball-length each. The 4-Ball Sparse environment tests longer-term memory by increasing the distance between balls to three ball-lengths so the agent needs to navigate a longer distance between balls. The results are shown in Figure 3. Even if the 3D environment provides more severe partial observability and longer-term dependency than the 2D environment due to its degree of freedom in exploring a larger environment with a first-person view, we see that TransDreamer obtains similar outperforming results as we obtained in the 2D Object Room. Next, we compare the quality of the trajectories imagined by the TSSM and the RSSM by measuring the generation performance quantitatively and qualitatively.</p>
<h3>5.3 World Model - Quantitative Results</h3>
<p>We measure the Mean Squared Error (MSE) of the predicted images and the reward prediction accuracy during the action-conditioned generation in the 3D 5-Ball Dense configuration. Even though the image is not directly used for policy training, the quality of the predicted image can serve as a proxy for measuring latent state prediction accuracy. Reward prediction accuracy, on the other hand, is directly related to policy training, and may provide some insights into why TransDreamer performs better than Dreamer in the above environments.</p>
<p>Image Generation. For a fair comparison, we separately trained the TSSM and the RSSM with the same set of trajectories without any policy training. Given a trajectory of 100 time steps, we measure the generation quality for several different context lengths and measure the per image MSE in the remaining generated steps. This allows us to measure the generation quality given different amounts of historical context and analyze how the different models utilize this context. The reported MSE is for the foreground objects (i.e., the balls), since that is where the most important information for this task is and more than $60 \%$ of the gap in overall MSE can be attributed to the foreground, despite the balls only occupying a small portion of the image most of the time (see Appendix Table 4 for overall</p>
<p>Table 2: World Model Quantitative Comparison
(a) Image Generation Foreground MSE
(b) Reward Prediction Accuracy</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Context</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">60</td>
<td style="text-align: center;">70</td>
</tr>
<tr>
<td style="text-align: center;">4-Ball Dense</td>
<td style="text-align: center;">TransDreamer</td>
<td style="text-align: center;">211.2</td>
<td style="text-align: center;">133.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DreamerV2</td>
<td style="text-align: center;">281.9</td>
<td style="text-align: center;">194.2</td>
</tr>
<tr>
<td style="text-align: center;">4-Ball Sparse</td>
<td style="text-align: center;">TransDreamer</td>
<td style="text-align: center;">195.5</td>
<td style="text-align: center;">115.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DreamerV2</td>
<td style="text-align: center;">215.8</td>
<td style="text-align: center;">138.6</td>
</tr>
<tr>
<td style="text-align: center;">5-Ball Dense</td>
<td style="text-align: center;">TransDreamer</td>
<td style="text-align: center;">245.2</td>
<td style="text-align: center;">163.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DreamerV2</td>
<td style="text-align: center;">300.9</td>
<td style="text-align: center;">217.0</td>
</tr>
</tbody>
</table>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Imagined trajectories comparison between DreamerV2 and TransDreamer
MSE results). The results are shown in Table 2a. We see that TransDreamer generally achieves lower or comparable MSE when compared with Dreamer. As expected, more steps given in the context results in lower MSE since the agents have more opportunity to see the entire environment before making predictions. In the 4-Ball Sparse setting, the MSE between TransDreamer and Dreamer are very comparable. This may be because when the environment is sparse, the agent sees the foreground objects less frequently.
Reward Prediction. Since the reward is zero for most time steps and both models can predict zero reward time steps well (see Appendix Table 5), we focus on the reward prediction accuracy for the nonzero +3 reward time steps. Since the reward is a continuous value, in order to obtain an accuracy, we classify rewards as positive if they are predicted in the range $3 \pm 0.3$. The results are shown in Table 2b. We once again see that TransDreamer generally achieves more accurate reward prediction than Dreamer, with longer contexts resulting in higher accuracy. For the 5-Ball Dense environments, however, Dreamer's reward prediction does not improve much as the context increases. This can indicate that Dreamer is not fully taking advantage of the additional context when predicting rewards in these more complex settings. TransDreamer, in contrast, does continue to improve when given more context, showing that the transformer architecture can take advantage of the increased context in making more accurate predictions. A full version including zero-reward prediction accuracy is reported in Appendix Table 5.</p>
<h1>5.4 World Model - Qualitative Comparison</h1>
<p>In Figure 4, we show the imagined trajectories from TransDreamer and Dreamer in the 5-Ball Dense environment. We provide context timesteps from each agent's trained policy up to when all the balls are collected for the first time. After this, the agent and balls reset to their original positions (frame 48 for TransDreamer and frame 58 for Dreamer). We then imagine the rest of the trajectory up to 100 total steps for each agent. Since the context timesteps contain information about the correct order of balls, ideally the agent would revisit the balls in this order during the imagination timesteps and correctly predict the rewards when the balls are collected. Note that the context frames for Dreamer and TransDreamer are different since they are based on trajectories from their own policies. This is necessary because the world model is trained from the trajectories of each agent's policy. Providing context that is not from the agent's policy would not necessarily be in the training distribution of the respective world model. See Figure 8 in the Appendix for an example of when the same context is given to both agents. Despite this, however, we can still see some clear differences between the quality of the imagination steps as well as the reward predictions.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Comparison between Dreamer and TransDreamer on a few DMC (upper row) and Atari tasks (bottom row) for short-term memory test. As expected, TransDreamer converges to a similar return value but slowly.</p>
<p>In particular, TransDreamer is able to predict the appearance of the environment correctly as well as the collection and subsequent disappearance of the balls in frames $54,62,73,90$, and 98 . It also accurately predicts the reward at these timesteps of around +3 (highlighted in red). For timesteps where there is no reward, it correctly predicts a reward around 0 . Dreamer, on the other hand, predicts images that are blurrier than TransDreamer. Furthermore, the imagined trajectories are incorrect. While it does predict the collection of the red ball and the reward in frame 67, this color is incorrect since the first ball should be purple. When it subsequently predicts the collection of the purple ball in frame 78, it is again the wrong color and no reward is predicted. This error seems to compound as the dark green ball it predicts at the end of the trajectory is not even one of the possible colors in the environment. This shows that the quality of the world model is better in TransDreamer than Dreamer, especially in the later steps of imagination where the long-term memory is more important. This can be a reason why TransDreamer outperforms Dreamer in these tasks.</p>
<h1>5.5 Short-Term Memory Tasks in DMC and Atari</h1>
<p>As the final validation, we perform a sanity check by testing the proposed model on a few simple DMC and Atari tasks. We note that it is expected that these tasks may favor Dreamer over TransDreamer, because solving these tasks does not require long-term and complex memory interactions, but modeling the dynamics of just the last few steps can suffice ${ }^{1}$. Specifically, we expect that RNNbased models can learn faster than transformer-based models because the former has the specific inductive bias to focus on the near-term history. Nevertheless, TransDreamer is supposed to converge eventually to an accuracy similar to that of Dreamer, and it is an important step to see whether these expectations are met.</p>
<p>We follow the configurations used by the authors in Dreamer (Hafner et al., 2019) for DMC and DreamerV2 (Hafner et al., 2020) for Atari. The only difference is that Dreamer uses the imagined trajectory from all states sampled from the replay buffer, whereas TransDreamer uses a few randomly selected states. Configurations for Transformer are described in Appendix A.3. As shown in Fig. 5, Dreamer and TransDreamer eventually reach comparable performance as expected, but TransDreamer is slower to saturate in general than Dreamer except for a few tasks. Interestingly, TransDreamer shows slightly better performance and faster convergence for the DMC Cheetah Run.</p>
<h2>6 CONCLUSION</h2>
<p>We proposed TransDreamer, a transformer-based MBRL agent, and the Transformer State-Space Model (TSSM), the first transformer-based stochastic world model. TransDreamer shows comparable performance with Dreamer on DMC and Atari tasks that do not require long-term memory, and outperforms Dreamer on Hidden Order Discovery tasks that require long-term complex memory interactions. We also show that image generation and reward prediction of TSSM is better than</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Dreamer qualitatively and quantitatively. Future work may involve validating our model on more complex tasks such as Crafter (Hafner, 2021).</p>
<h1>ACKNOWLEDGMENTS</h1>
<p>This work is supported by Brain Pool Plus Program (No. 2021H1D3A2A03103645) through the National Research Foundation of Korea (NRF) funded by the Ministry of Science and ICT.</p>
<h2>REFERENCES</h2>
<p>Diego Martín Arroyo, Janis Postels, and Federico Tombari. Variational transformer networks for layout generation. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021, pp. 13642-13652. Computer Vision Foundation / IEEE, 2021. URL https://openaccess.thecvf.com/content/CVPR2021/html/ Arroyo_Variational_Transformer_Networks_for_Layout_Generation_ CVPR_2021_paper.html.</p>
<p>Andrea Banino, Adrià Puigdomènech Badia, Raphael Köster, Martin J Chadwick, Vinicius Zambaldi, Demis Hassabis, Caswell Barry, Matthew Botvinick, Dharshan Kumaran, and Charles Blundell. Memo: A deep network for flexible combination of episodic memories. arXiv preprint arXiv:2001.10913, 2020.</p>
<p>Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47: 253-279, 2013.</p>
<p>Yoshua Bengio, Nicholas Léonard, and Aaron Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.</p>
<p>Felix Berkenkamp, Matteo Turchetta, Angela P Schoellig, and Andreas Krause. Safe model-based reinforcement learning with stability guarantees. arXiv preprint arXiv:1705.08551, 2017.</p>
<p>Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020.</p>
<p>Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. arXiv preprint arXiv:2106.01345, 2021.</p>
<p>Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever. Generative pretraining from pixels. In International Conference on Machine Learning, pp. 16911703. PMLR, 2020.</p>
<p>Maxime Chevalier-Boisvert, Lucas Willems, and Suman Pal. Minimalistic gridworld environment for openai gym. https://github.com/maximecb/gym-minigrid, 2018.</p>
<p>Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014.</p>
<p>Antonia Creswell, Rishabh Kabra, Christopher Burgess, and Murray Shanahan. Unsupervised object-based transition models for 3d partially observable environments. CoRR, abs/2103.04693, 2021. URL https://arxiv.org/abs/2103.04693.</p>
<p>Will Dabney, Georg Ostrovski, David Silver, and Rémi Munos. Implicit quantile networks for distributional reinforcement learning. In International conference on machine learning, pp. 10961105. PMLR, 2018.</p>
<p>Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context. arXiv preprint arXiv:1901.02860, 2019.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171-4186, 2019.</p>
<p>Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https://openreview.net/forum? id=YicbFdNTTy.</p>
<p>David Ha and Jürgen Schmidhuber. World models. arXiv preprint arXiv:1803.10122, 2018.
Danijar Hafner. Benchmarking the spectrum of agent capabilities. arXiv preprint arXiv:2109.06780, 2021.</p>
<p>Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James Davidson. Learning latent dynamics for planning from pixels. arXiv preprint arXiv:1811.04551, 2018.</p>
<p>Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning behaviors by latent imagination. arXiv preprint arXiv:1912.01603, 2019.</p>
<p>Danijar Hafner, Timothy Lillicrap, Mohammad Norouzi, and Jimmy Ba. Mastering atari with discrete world models. arXiv preprint arXiv:2010.02193, 2020.</p>
<p>Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan Horgan, Bilal Piot, Mohammad Azar, and David Silver. Rainbow: Combining improvements in deep reinforcement learning. In Thirty-Second AAAI Conference on Artificial Intelligence, 2018.</p>
<p>Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8): $1735-1780,1997$.</p>
<p>Jan Humplik, Alexandre Galashov, Leonard Hasenclever, Pedro A Ortega, Yee Whye Teh, and Nicolas Heess. Meta reinforcement learning as task inference. arXiv preprint arXiv:1905.06424, 2019.</p>
<p>Kazuki Irie, Imanol Schlag, Róbert Csordás, and Jürgen Schmidhuber. Going beyond linear transformers with recurrent fast weight programmers. arXiv preprint arXiv:2106.06295, 2021.</p>
<p>Michael Janner, Qiyang Li, and Sergey Levine. Reinforcement learning as one big sequence modeling problem. arXiv preprint arXiv:2106.02039, 2021.</p>
<p>Michael I Jordan, Zoubin Ghahramani, Tommi S Jaakkola, and Lawrence K Saul. An introduction to variational methods for graphical models. Machine learning, 37(2):183-233, 1999.</p>
<p>Arthur Juliani, Vincent-Pierre Berges, Ervin Teng, Andrew Cohen, Jonathan Harper, Chris Elion, Chris Goy, Yuan Gao, Hunter Henry, Marwan Mattar, and Danny Lange. Unity: A general platform for intelligent agents, 2020.</p>
<p>Leslie Pack Kaelbling, Michael L Littman, and Anthony R Cassandra. Planning and acting in partially observable stochastic domains. Artificial intelligence, 101(1-2):99-134, 1998.</p>
<p>Lukasz Kaiser, Mohammad Babaeizadeh, Piotr Milos, Blazej Osinski, Roy H Campbell, Konrad Czechowski, Dumitru Erhan, Chelsea Finn, Piotr Kozakowski, Sergey Levine, et al. Model-based reinforcement learning for atari. arXiv preprint arXiv:1903.00374, 2019.</p>
<p>Nan Rosemary Ke, Anirudh Goyal, Olexa Bilaniuk, Jonathan Binas, Michael C Mozer, Chris Pal, and Yoshua Bengio. Sparse attentive backtracking: Temporal creditassignment through reminding. arXiv preprint arXiv:1809.03702, 2018.</p>
<p>Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims. Morel: Modelbased offline reinforcement learning. arXiv preprint arXiv:2005.05951, 2020.</p>
<p>Zhaojiang Lin, Genta Indra Winata, Peng Xu, Zihan Liu, and Pascale Fung. Variational transformers for diverse response generation. CoRR, abs/2003.12738, 2020. URL https://arxiv.org/ abs/2003.12738.</p>
<p>Kevin Lu, Aditya Grover, Pieter Abbeel, and Igor Mordatch. Reset-free lifelong learning with skillspace planning. arXiv preprint arXiv:2012.03548, 2020.</p>
<p>Alice Martin, Charles Ollion, Florian Strub, Sylvain Le Corff, and Olivier Pietquin. The monte carlo transformer: a stochastic self-attention model for sequence prediction. CoRR, abs/2007.08620, 2020. URL https://arxiv.org/abs/2007.08620.</p>
<p>Emilio Parisotto and Ruslan Salakhutdinov. Efficient transformers in reinforcement learning using actor-learner distillation. arXiv preprint arXiv:2104.01655, 2021.</p>
<p>Emilio Parisotto, Francis Song, Jack Rae, Razvan Pascanu, Caglar Gulcehre, Siddhant Jayakumar, Max Jaderberg, Raphael Lopez Kaufman, Aidan Clark, Seb Noury, et al. Stabilizing transformers for reinforcement learning. In International Conference on Machine Learning, pp. 7487-7498. PMLR, 2020.</p>
<p>Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Image transformer. In Jennifer G. Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine Learning Research, pp. 40524061. PMLR, 2018. URL http://proceedings.mlr.press/v80/parmar18a.html.</p>
<p>Alec Radford and Karthik Narasimhan. Improving language understanding by generative pretraining. 2018.</p>
<p>Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.</p>
<p>Sam Ritter, Ryan Faulkner, Laurent Sartran, Adam Santoro, Matt Botvinick, and David Raposo. Rapid task-solving in novel environments. arXiv preprint arXiv:2006.03662, 2020.</p>
<p>Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering atari, go, chess and shogi by planning with a learned model. Nature, 588(7839):604-609, 2020.</p>
<p>Ramanan Sekar, Oleh Rybkin, Kostas Daniilidis, Pieter Abbeel, Danijar Hafner, and Deepak Pathak. Planning to explore via self-supervised world models. In International Conference on Machine Learning, pp. 8583-8592. PMLR, 2020.</p>
<p>Richard S Sutton. Dyna, an integrated architecture for learning, planning, and reacting. ACM Sigart Bulletin, 2(4):160-163, 1991.</p>
<p>Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Budden, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, et al. Deepmind control suite. arXiv preprint arXiv:1801.00690, 2018.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pp. 5998-6008, 2017.</p>
<p>Andreas Voskou, Konstantinos P Panousis, Dimitrios Kosmopoulos, Dimitris N Metaxas, and Sotirios Chatzis. Stochastic transformer networks with linear competing units: Application to end-to-end sl translation. arXiv preprint arXiv:2109.13318, 2021.</p>
<p>Tianming Wang and Xiaojun Wan. T-CVAE: transformer-based conditioned variational autoencoder for story completion. In Sarit Kraus (ed.), Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI 2019, Macao, China, August 10-16, 2019, pp. 52335239. ijcai.org, 2019. doi: 10.24963/ijcai.2019/727. URL https://doi.org/10.24963/ ijcai.2019/727.</p>
<p>Dirk Weissenborn, Oscar Täckström, and Jakob Uszkoreit. Scaling autoregressive video models. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/forum?id= rJgsskrFwH.</p>
<p>Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8(3-4):229-256, 1992.</p>
<p>Yi-Fu Wu, Jaesik Yoon, and Sungjin Ahn. Generative video transformer: Can objects be the words? In International Conference on Machine Learning, pp. 11307-11318. PMLR, 2021.</p>
<p>Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas. Videogpt: Video generation using VQ-VAE and transformers. CoRR, abs/2104.10157, 2021. URL https://arxiv.org/abs/ 2104.10157.</p>
<h1>A APPENDIX</h1>
<h2>A. 1 DREAMERV2</h2>
<p>Hafner et al. (2020) makes several additional changes to the framework that are found to improve performance on the Atari environment. First, instead of using a continuous stochastic hidden state, a discrete state is used. Second, straight-through gradients (Bengio et al., 2013) are used to differentiate through the discrete states and actions. Due to the bias induced by the straight-through estimator, REINFORCE gradient or a mixed gradient of REINFORCE and the dynamics backpropagation is used. Lastly, they use KL balancing, separately scaling the prior cross entropy and the posterior entropy in the KL loss.</p>
<h2>A. 2 TransDreamer Loss Function</h2>
<p>We optimize the following objective, which is the negative ELBO of the action conditioned model with additional terms for predicting the reward and discount,</p>
<p>$$
\begin{aligned}
\mathcal{L}<em t="1">{\mathrm{TSSM}}(\phi)=\sum</em>}^{T}( &amp; \mathbb{E<em _tau="1">{\prod</em>\right)\right] \
&amp; \left.+\mathbb{E}}^{t} q_{\phi\left(x_{\tau} \mid x_{\tau}\right)}}\left[-\eta_{x} \ln p_{\phi}\left(x_{t} \mid h_{t}, z_{t}\right)-\eta_{r} \ln p_{\phi}\left(r_{t} \mid h_{t}, z_{t}\right)-\eta_{\gamma} \ln p_{\phi}\left(\gamma_{t} \mid h_{t}, z_{t<em _tau="1">{\prod</em>\right)\right]\right]\right)
\end{aligned}
$$}^{t-1} q_{\phi}\left(z_{\tau} \mid x_{\tau}\right)}\left[D_{\mathrm{KL}}\left[q_{\phi}\left(z_{t} \mid x_{t}\right) | p_{\phi}\left(z_{t} \mid z_{1: t-1}, a_{1: t-1</p>
<p>Here, $\eta_{x}, \eta_{r}$, and $\eta_{\gamma}$ are hyperparameters used to scale the loss terms. The derivation of the ELBO can be found in the below.</p>
<h2>A.2.1 ELBO</h2>
<p>The generative model is $p\left(o_{t}, z_{1: T} \mid a_{1: T}\right)=\prod_{t} p\left(o_{t} \mid h_{t}, z_{t}\right) p\left(z_{t} \mid z_{1: t-1}, a_{1: t-1}\right)$ where $o_{t}=$ $\left(x_{t}, r_{t}, \gamma_{t}\right)$ and $h_{t}=f_{\text {transformer }}\left(z_{1: t-1}, a_{1: t-1}\right)$. By approximating the posterior by $q\left(z_{t} \mid x_{t}\right)$, a variational posterior is $q\left(z_{1: T} \mid o_{1: T}, a_{1: T}\right)=\prod_{t} q\left(z_{t} \mid x_{t}\right)$. By the importance weighting and Jensen's inequality, we can write as follows:</p>
<p>$$
\begin{aligned}
\ln p\left(o_{1: T} \mid a_{1: T}\right)= &amp; \ln \mathbb{E}<em 1:="1:" T="T">{p\left(z</em>\right)\right] \
= &amp; \ln \mathbb{E}} \mid o_{1: T}, a_{1: T}\right)}\left[\prod_{t=1}^{T} p\left(o_{t} \mid h_{t}, z_{t<em 1:="1:" T="T">{q\left(z</em>\right)\right] \
\geq &amp; \mathbb{E}} \mid o_{1: T}, a_{1: T}\right)}\left[\prod_{t=1}^{T} p\left(o_{t} \mid h_{t}, z_{t}\right) p\left(z_{t} \mid z_{1: t-1}, a_{1: t-1}\right) / q\left(z_{t} \mid x_{t<em t="1">{\prod</em>\right)\right] \
= &amp; \sum_{t=1}^{T}\left(\mathbb{E}}^{T} q\left(z_{t} \mid x_{t}\right)}\left[\sum_{t=1}^{T} \ln p\left(o_{t} \mid h_{t}, z_{t}\right)+\ln p\left(z_{t} \mid z_{1: t-1}, a_{1: t-1}\right)-\ln q\left(z_{t} \mid x_{t<em _tau="1">{\prod</em>\right)\right]\right. \
&amp; \left.-\mathbb{E}}^{t-1} q\left(z_{\tau} \mid x_{\tau}\right)}\left[\ln p\left(o_{t} \mid h_{t}, z_{t<em _tau="1">{\prod</em>\right)\right]\right]\right) \
= &amp; \sum_{t=1}^{T}\left(\mathbb{E}}^{t-1} q\left(z_{\tau} \mid x_{\tau}\right)}\left[D_{\mathrm{KL}}\left[q\left(z_{t} \mid x_{t}\right) | p\left(z_{t} \mid z_{1: t-1}, a_{1: t-1<em _tau="1">{\prod</em>\right)\right]\right. \
&amp; \left.-\mathbb{E}}^{t-1} q\left(z_{\tau} \mid x_{\tau}\right)}\left[\ln p\left(x_{t} \mid h_{t}, z_{t}\right)+\ln p\left(r_{t} \mid h_{t}, z_{t}\right)+\ln p\left(\gamma_{t} \mid h_{t}, z_{t<em _tau="1">{\prod</em>\right)\right]\right]\right)
\end{aligned}
$$}^{t-1} q\left(z_{\tau} \mid x_{\tau}\right)}\left[D_{\mathrm{KL}}\left[q\left(z_{t} \mid x_{t}\right) | p\left(z_{t} \mid z_{1: t-1}, a_{1: t-1</p>
<p>where $p\left(o_{t} \mid h_{t}, z_{t}\right)=p\left(x_{t} \mid h_{t}, z_{t}\right) p\left(r_{t} \mid h_{t}, z_{t}\right) p\left(\gamma_{t} \mid h_{t}, z_{t}\right)$.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Transfomer-Based Trajectory Rollout for Actor Critic Learning.</p>
<h1>A. 3 DMC and Atari</h1>
<p>As written in Sec. 5.5, we used almost identical configurations with Dreamer and DreamerV2 by referring to the configuration file in https://github.com/danijar/dreamerv2 (e.g., action repeat and training World Model and policy every 5 steps for DMC). The one configuration we did modify is the number of imagined trajectories, which is not configurable in Dreamer, but is necessary in TransDreamer because imagining from every state in the batch requires too much computational resources. We control this through a hyperparamter that limits the number of imagined trajectories per training sample. For DMC and Atari, we use 3 imagined trajectories per sample.
Several other hyperparmeters are specific to the TSSM. These include: whether or not to use gating or identity map reording as is done in GTrXL (Parisotto et al., 2020), the number of layers and heads to use for the transformer, the size of the hidden state for the MLP in the transformer, and whether or not to use relational positional embedding (Dai et al., 2019). For DMC and Atari, we generally use 2-layer Transformer (Vaswani et al., 2017) without dropout, gating, or identity map reordering. One exception is for Atari Pong where we did find identity map reordering performed better. We use 10 heads in the Multihead Attention and the dimensions of the hidden state for the MLP and Attention are 200 for DMC and 600 for Atari. These are the same as the dimensions used in the deterministic state in DreamerV2.</p>
<h2>A. 4 Hidden Order Discovery</h2>
<p>For 2D and 3D Hidden Order Discovery tasks, we measure the model in two aspects, the ability to deal with complex memory-based reasoning and the ability to extract long-term knowledge. Thus we design tasks either with an increased number of objects or the distances between any two objects or both. Specifically, on 2D tasks, we increase the number of balls from 4 to 6 , while not changing the distance. The distance here is measured as the number of cells between any two balls. We sum the absolute difference along the $x$-axis and $y$-axis as the distance between any two balls. To control the long-term dependency, a threshold of 2 is applied to the distance of balls, i.e. the minimum distance between any two balls should not be less than 2 cells. For 3D tasks, we tested not only the reasoning complexity but also the ability to capture long-term dependency. For reasoning complexity, we compare 5-Ball Dense with 4-Ball Dense. For long term dependency, we compare 4-Ball sparse against 4-Ball Dense. The sparse setting has a larger distance between any two balls. We use the Euclidean distance as a measure of distance. Any two balls have a distance at least 4 units in the sparse setting, while in the dense setting, it is 2 units. 1 unit equals 1 ball size (diameter). Thus, in sparse setting, the distance between any two balls is at least 3 ball-size. For each task, the maximum steps for an episode is set as 100 .
We implemented a 6-layer transformer with identity map reordering as TSSM for both 2D and 3D Hidden Order Discovery tasks. During imagination, only one state was randomly sampled as the starting state for imagination. We imagined till the trajectory's max step was reached. Empirically we found concatenating the intermediate output of attention layers together as $h_{4}$ accelerates the converge speed, so we applied this during experiments. Other hyperparameter configurations are kept the same as DreamerV2 crafter configuration, see https://github.com/danijar/crafter/issues/1</p>
<p>for details. For DreamerV2, we use the same configuration as DreamerV2 for crafter, except that we imagined 30 steps for agent learning.</p>
<p>As explained in the paper, to help train the world model better, we used a prioritized replay buffer for Dreamer and TransDreamer with $\alpha=0.5$. The sampling probability for each trajectory is set at the return of this trajectory divided by the overall return of the whole data buffer collected till that time, thus a trajectory with higher rewards will have a higher chance to be sampled. The rest $50 \%$ of the batch are sampled uniformly from the whole data buffer.</p>
<p>Table 3 shows the ratio of successfully completing at least one round of the hidden order on all the 2D and 3D tasks. We deploy the well-trained agent for each task to collect 1000 trajectories and count the ratio in the whole trajectories. As we can see, TransDreamer performs better than Dreamer by this metric. Note that the episode length is limited to 100, and succeeding in this task in 100 steps is difficult. For example, in the 4-Ball case, the chance of guessing the right order is 0.04 (1 over 4!), and the agent needs to start collecting from the first ball when it collects an incorrect ball. Therefore the agent needs to start over again and again during exploration. When increasing the number of balls from 4 to 5 , the chance of randomly guessing the order decreases by a factor of 5. We can observe this relation approximately on TransDreamer's performance, $23 \% \rightarrow 5 \%$, while Dreamer nearly fails.</p>
<p>Table 3: Success Rate for Complete Order Visitation</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">2D Object Room</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">3D Object Room</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">4-Ball</td>
<td style="text-align: center;">5-Ball</td>
<td style="text-align: center;">6-Ball</td>
<td style="text-align: center;">4-Ball Dense</td>
<td style="text-align: center;">4-Ball Sparse</td>
<td style="text-align: center;">5-Ball Dense</td>
</tr>
<tr>
<td style="text-align: center;">TransDreamer</td>
<td style="text-align: center;">23\%</td>
<td style="text-align: center;">5\%</td>
<td style="text-align: center;">1\%</td>
<td style="text-align: center;">18\%</td>
<td style="text-align: center;">11\%</td>
<td style="text-align: center;">4\%</td>
</tr>
<tr>
<td style="text-align: center;">DreamerV2</td>
<td style="text-align: center;">7\%</td>
<td style="text-align: center;">0\%</td>
<td style="text-align: center;">0\%</td>
<td style="text-align: center;">10\%</td>
<td style="text-align: center;">1\%</td>
<td style="text-align: center;">0 \%</td>
</tr>
</tbody>
</table>
<h1>A.4.1 Full Quantitative Results</h1>
<p>To compute the foreground MSE, we use Unity (Juliani et al., 2020) to render a foreground image, Figure 7, and from the foreground image, we infer a binary foreground mask to filter out the background from the predicted image. The full MSE result is reported in Table 4. As can be seen, more than half of the overall MSE gap between TransDreamer and Dreamer happens in the foreground. For example, in the 4-Ball Dense, 60 context setting, the overall MSE gap
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Image from Unity Foreground Camera
between TransDreamer and Dreamer is 119.8, while 70.7 of the error occurs in the foreground.
Table 5 shows the reward prediction accuracy on both zero-reward and nonzero-reward timesteps for the 3D tasks. As mentioned in the paper, to measure prediction accuracy for +3 reward case, we classify it by labeling $3 \pm 0.3$ as positive. For 0 reward case, we classify it by labeling $\pm 0.01$ as positive. We can see that TransDreamer outperforms Dreamer by a large gap on nonzero-reward in the 4-Ball Dense and the 5-Ball Dense. Both models perform well generally on zero-reward. In 4-Ball Sparse setting, The gap is smaller, we hypothesis that it is because in the sparse setting, the foreground balls are seen less frequently.</p>
<p>Table 4: Image Generation MSE</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">60 contexts / 40 targets</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">70 contexts / 30 targets</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">80 contexts / 20 targets</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Overall</td>
<td style="text-align: center;">Foreground</td>
<td style="text-align: center;">Overall</td>
<td style="text-align: center;">Foreground</td>
<td style="text-align: center;">Overall</td>
<td style="text-align: center;">Foreground</td>
</tr>
<tr>
<td style="text-align: center;">4-Ball Dense</td>
<td style="text-align: center;">TransDreamer</td>
<td style="text-align: center;">458.0</td>
<td style="text-align: center;">211.2</td>
<td style="text-align: center;">281.9</td>
<td style="text-align: center;">133.1</td>
<td style="text-align: center;">146.0</td>
<td style="text-align: center;">69.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DreamerV2</td>
<td style="text-align: center;">577.8</td>
<td style="text-align: center;">281.9</td>
<td style="text-align: center;">380.0</td>
<td style="text-align: center;">194.2</td>
<td style="text-align: center;">206.2</td>
<td style="text-align: center;">110.8</td>
</tr>
<tr>
<td style="text-align: center;">4-Ball Sparse</td>
<td style="text-align: center;">TransDreamer</td>
<td style="text-align: center;">448.8</td>
<td style="text-align: center;">195.5</td>
<td style="text-align: center;">261.4</td>
<td style="text-align: center;">115.2</td>
<td style="text-align: center;">128.1</td>
<td style="text-align: center;">56.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DreamerV2</td>
<td style="text-align: center;">462.6</td>
<td style="text-align: center;">215.8</td>
<td style="text-align: center;">279.7</td>
<td style="text-align: center;">138.6</td>
<td style="text-align: center;">145.1</td>
<td style="text-align: center;">72.4</td>
</tr>
<tr>
<td style="text-align: center;">5-Ball Dense</td>
<td style="text-align: center;">TransDreamer</td>
<td style="text-align: center;">516.0</td>
<td style="text-align: center;">245.2</td>
<td style="text-align: center;">329.9</td>
<td style="text-align: center;">163.1</td>
<td style="text-align: center;">167.4</td>
<td style="text-align: center;">85.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DreamerV2</td>
<td style="text-align: center;">605.1</td>
<td style="text-align: center;">300.9</td>
<td style="text-align: center;">413.8</td>
<td style="text-align: center;">217.0</td>
<td style="text-align: center;">231.6</td>
<td style="text-align: center;">124.9</td>
</tr>
</tbody>
</table>
<p>Table 5: Reward Prediction Accuracy</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">60 contexts / 40 targets</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">70 contexts / 30 targets</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">80 contexts / 20 targets</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Zero</td>
<td style="text-align: center;">Non-zero</td>
<td style="text-align: center;">Zero</td>
<td style="text-align: center;">Non-zero</td>
<td style="text-align: center;">Zero</td>
<td style="text-align: center;">Non-zero</td>
</tr>
<tr>
<td style="text-align: center;">4-Ball Dense</td>
<td style="text-align: center;">Transdreamer</td>
<td style="text-align: center;">94.9</td>
<td style="text-align: center;">46.9</td>
<td style="text-align: center;">94.7</td>
<td style="text-align: center;">53.2</td>
<td style="text-align: center;">95.4</td>
<td style="text-align: center;">73.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DreamerV2</td>
<td style="text-align: center;">93.7</td>
<td style="text-align: center;">28.2</td>
<td style="text-align: center;">93.6</td>
<td style="text-align: center;">34.6</td>
<td style="text-align: center;">94.2</td>
<td style="text-align: center;">50.5</td>
</tr>
<tr>
<td style="text-align: center;">4-Ball Sparse</td>
<td style="text-align: center;">Transdreamer</td>
<td style="text-align: center;">96.4</td>
<td style="text-align: center;">32.4</td>
<td style="text-align: center;">96.0</td>
<td style="text-align: center;">36.5</td>
<td style="text-align: center;">96.6</td>
<td style="text-align: center;">48.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DreamerV2</td>
<td style="text-align: center;">95.6</td>
<td style="text-align: center;">32.0</td>
<td style="text-align: center;">96.2</td>
<td style="text-align: center;">33.3</td>
<td style="text-align: center;">95.5</td>
<td style="text-align: center;">42.3</td>
</tr>
<tr>
<td style="text-align: center;">5-Ball Dense</td>
<td style="text-align: center;">Transdreamer</td>
<td style="text-align: center;">92.5</td>
<td style="text-align: center;">17.7</td>
<td style="text-align: center;">93.2</td>
<td style="text-align: center;">18.1</td>
<td style="text-align: center;">93.3</td>
<td style="text-align: center;">32.35</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DreamerV2</td>
<td style="text-align: center;">91.1</td>
<td style="text-align: center;">9.8</td>
<td style="text-align: center;">92.3</td>
<td style="text-align: center;">6.2</td>
<td style="text-align: center;">92.4</td>
<td style="text-align: center;">15.3</td>
</tr>
</tbody>
</table>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Imagined trajectories comparison between DreamerV2 and TransDreamer given same context</p>
<h1>A. 5 World Model Imagination With Same Context</h1>
<p>Different from Figure 4, in Figure 8, we illustrated imagined trajectories from TransDreamer and Dreamer given the same contexts. This 5-Ball Dense sample is collected from Dreamer's agent learning process, so for TransDreamer, it is an out of distribution sample. This is the same context given to Dreamer in Figure 4. Despite being an out of distribution sample, TransDreamer can still correctly imagine the balls and predicts rewards for the purple and white balls (Green box) correctly. However, it does make a mistake predicting the reward for the green ball (blue box). Nevertheless, even in this setting, the quality of imagination in TransDreamer is better than Dreamer.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ Some Atari games require long-term memory along with a good exploration policy. We do not choose these games as we do not address the exploration problem in this work.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
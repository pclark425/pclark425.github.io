<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-244 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-244</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-244</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-14.html">extraction-schema-14</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <p><strong>Paper ID:</strong> paper-266055397</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2312.04333v4.pdf" target="_blank">Is Bigger and Deeper Always Better? Probing LLaMA Across Scales and Layers</a></p>
                <p><strong>Paper Abstract:</strong> This paper presents an in-depth analysis of Large Language Models (LLMs), focusing on LLaMA, a prominent open-source foundational model in natural language processing. Instead of assessing LLaMA through its generative output, we design multiple-choice tasks to probe its intrinsic understanding in high-order tasks such as reasoning and computation. We examine the model horizontally, comparing different sizes, and vertically, assessing different layers. We unveil several key and uncommon findings based on the designed probing tasks: (1) Horizontally, enlarging model sizes almost could not automatically impart additional knowledge or computational prowess. Instead, it can enhance reasoning abilities, especially in math problem solving, and helps reduce hallucinations, but only beyond certain size thresholds; (2) In vertical analysis, the lower layers of LLaMA lack substantial arithmetic and factual knowledge, showcasing logical thinking, multilingual and recognitive abilities, with top layers housing most computational power and real-world knowledge.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e244.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e244.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-2-7B (arithmetic)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA 2 - 7B parameter model (arithmetic probing in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of LLaMA 2 (7B) on a suite of arithmetic tasks (addition, subtraction, multiplication, division, and multi-step arithmetic) using multiple-choice, few-shot in-context probing; shows decent integer performance but weaker floating-point and multi-step performance concentrated in upper layers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA 2</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>addition, subtraction, multiplication, division, multi-step arithmetic (2-step and 3-step mixed operations)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>Integers and floating-point (up to 3 decimal places); integer ranges: 1–100, 100–10,000, 10,000–100,000 (some tasks up to 1,000,000); also 2- and 3-operation complex expressions</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Few-shot in-context prompting (6-shot for calculation tasks); multiple-choice probing; distractor answers created by adding/subtracting a small float (± up to 20) to correct answer to test sensitivity</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Integer arithmetic ≈ 67.9% accuracy; floating-point arithmetic ≈ 52.5% accuracy. Strong on addition and subtraction; multiplication/division accuracy degrades with increasing digit count; comparable to larger sizes on floating-point tasks but lags slightly on integer tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>No low-level mechanistic algorithm (e.g., explicit carry-tracking) identified; paper reports that pure computational ability is nearly absent in lower layers and emerges sharply in middle-to-upper layers, peaking in the final few (but not always the absolute last) layers — suggesting arithmetic is represented/processed in upper layers rather than memorized uniformly across the network.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Arithmetic performance shows little improvement compared to larger LLaMA sizes (13B, 70B); floating-point accuracy nearly identical across sizes (~52.5%), integer accuracy slightly lower than larger models. Larger model size does not materially improve basic computation ability under same pretraining data volume.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Performance declines with number-of-digits and operation complexity; multiplication and division degrade markedly with larger digit counts; floating-point operations are harder than integers; models struggle as number of reasoning steps grows (sharp drop for 7+ step problems).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared across model sizes (7B vs 13B vs 70B) and across internal layers (first to last layer). Few-shot prompting used because zero-shot was unstable.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>LLaMA-2-7B can do basic arithmetic reasonably (better on integers/add/sub) but struggles on larger-digit and floating-point multiplication/division; pure computational ability is concentrated in upper layers and does not improve much simply by scaling from 7B to larger sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Is Bigger and Deeper Always Better? Probing LLaMA Across Scales and Layers', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e244.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e244.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-2-13B (arithmetic)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA 2 - 13B parameter model (arithmetic probing in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of LLaMA 2 (13B) on the same arithmetic suite; shows marginally better integer performance than 7B but nearly identical floating-point performance, with computation localized to upper layers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA 2</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>addition, subtraction, multiplication, division, multi-step arithmetic (2-step and 3-step mixed operations)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>Integers and floating-point (up to 3 decimal places); integer ranges: 1–100, 100–10,000, 10,000–100,000 (some tasks up to 1,000,000); also 2- and 3-operation complex expressions</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Few-shot in-context prompting (6-shot for calculation tasks); multiple-choice probing with subtle distractors (± up to 20 float perturbations)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Integer arithmetic ≈ 70.6% accuracy; floating-point arithmetic ≈ 52.6% accuracy. Similar pattern to 7B: good on addition/subtraction, multiplication/division degrade with digit count, floats harder than integers.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Same layer-wise pattern: very little pure computational capability in lower layers; significant emergence of computation in middle-to-top layers with peaks in the last several (often penultimate) layers. No evidence of explicit algorithmic modules (e.g., dedicated carry-heads) was provided.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Only marginal improvement over 7B in integer tasks; floating-point accuracy nearly unchanged — indicates arithmetic capability is not strongly scaling between 7B and 13B given same training data budget.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Struggles with larger-digit multiplication/division and floating-point arithmetic; multi-step arithmetic performance drops as number of operations increases.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to 7B and 70B sizes and across layers; same few-shot prompting and multiple-choice evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Increasing to 13B yields modest integer gains but does not substantially improve floating-point calculation; arithmetic competence remains concentrated in upper layers rather than distributed throughout the network.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Is Bigger and Deeper Always Better? Probing LLaMA Across Scales and Layers', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e244.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e244.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-2-70B (arithmetic)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA 2 - 70B parameter model (arithmetic probing in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of LLaMA 2 (70B) showing roughly similar base computational abilities to smaller models but substantially improved multi-step math reasoning and slightly higher integer accuracy; arithmetic processing remains primarily in upper layers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA 2</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>addition, subtraction, multiplication, division, multi-step arithmetic (2-step to 6-step mixed operations)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>Integers and floating-point (up to 3 decimal places); integer ranges: 1–100, 100–10,000, 10,000–100,000 (some tasks up to 1,000,000); tasks include 1–6 step reasoning problems and 5–6 digit numeric calculations</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Few-shot in-context prompting (6-shot for calculation tasks); multiple-choice probing; MPS datasets (GSM8K-derived) used for multi-step problems; distractor crafting via small numeric perturbations</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Integer arithmetic ≈ 70.8% accuracy; floating-point arithmetic ≈ 52.9% accuracy. Notably better performance on math problem solving (MPS-Cal ~48.3% vs ~28–30% for smaller models), with improvements concentrated on problems requiring 3–6 reasoning steps; pure arithmetic accuracy remains close to smaller models.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Arithmetic and factual knowledge primarily encoded in upper layers; computational ability shows a sharp emergence in middle-to-top layers and often peaks a few layers before the final layer. The paper does not identify circuit-level mechanisms (e.g., specialized heads) but infers layer specialization: lower layers retain abstract/multilingual features while computation/knowledge are in top layers.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Basic arithmetic (especially floating-point) scales poorly with size — minimal gains from 7B→70B. However, multi-step mathematical reasoning shows emergent improvement at 70B (not present at 7B/13B), indicating reasoning abilities benefit from scale even when raw computation does not.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Same as smaller models: multiplication/division degrade with digit count; floating-point performance poor relative to integers; all models (including 70B) fail substantially on very long multi-step problems (7+ steps).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Direct comparisons across 7B, 13B, and 70B and layer-wise (first..last layers). Few-shot prompting used; MPS tasks include both computational-error (MPS-Cal) and reasoning-error (MPS-Rea) derived distractors.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Scaling to 70B yields clear gains in multi-step mathematical reasoning but not in basic arithmetic computation — arithmetic accuracy (particularly floating-point) remains nearly unchanged and computational capability is localized to upper layers.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Is Bigger and Deeper Always Better? Probing LLaMA Across Scales and Layers', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Emergent abilities of large language models <em>(Rating: 2)</em></li>
                <li>Training verifiers to solve math word problems <em>(Rating: 2)</em></li>
                <li>PAL: Program-aided language models <em>(Rating: 2)</em></li>
                <li>GSM8K: A dataset for grade school math (Cobbe et al., 2021) <em>(Rating: 2)</em></li>
                <li>GPT-4 Technical Report <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-244",
    "paper_id": "paper-266055397",
    "extraction_schema_id": "extraction-schema-14",
    "extracted_data": [
        {
            "name_short": "LLaMA-2-7B (arithmetic)",
            "name_full": "LLaMA 2 - 7B parameter model (arithmetic probing in this paper)",
            "brief_description": "Evaluation of LLaMA 2 (7B) on a suite of arithmetic tasks (addition, subtraction, multiplication, division, and multi-step arithmetic) using multiple-choice, few-shot in-context probing; shows decent integer performance but weaker floating-point and multi-step performance concentrated in upper layers.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA 2",
            "model_size": "7B",
            "model_architecture": "decoder-only transformer",
            "arithmetic_operation_type": "addition, subtraction, multiplication, division, multi-step arithmetic (2-step and 3-step mixed operations)",
            "number_range_or_complexity": "Integers and floating-point (up to 3 decimal places); integer ranges: 1–100, 100–10,000, 10,000–100,000 (some tasks up to 1,000,000); also 2- and 3-operation complex expressions",
            "method_or_intervention": "Few-shot in-context prompting (6-shot for calculation tasks); multiple-choice probing; distractor answers created by adding/subtracting a small float (± up to 20) to correct answer to test sensitivity",
            "performance_result": "Integer arithmetic ≈ 67.9% accuracy; floating-point arithmetic ≈ 52.5% accuracy. Strong on addition and subtraction; multiplication/division accuracy degrades with increasing digit count; comparable to larger sizes on floating-point tasks but lags slightly on integer tasks.",
            "mechanistic_insight": "No low-level mechanistic algorithm (e.g., explicit carry-tracking) identified; paper reports that pure computational ability is nearly absent in lower layers and emerges sharply in middle-to-upper layers, peaking in the final few (but not always the absolute last) layers — suggesting arithmetic is represented/processed in upper layers rather than memorized uniformly across the network.",
            "performance_scaling": "Arithmetic performance shows little improvement compared to larger LLaMA sizes (13B, 70B); floating-point accuracy nearly identical across sizes (~52.5%), integer accuracy slightly lower than larger models. Larger model size does not materially improve basic computation ability under same pretraining data volume.",
            "failure_modes": "Performance declines with number-of-digits and operation complexity; multiplication and division degrade markedly with larger digit counts; floating-point operations are harder than integers; models struggle as number of reasoning steps grows (sharp drop for 7+ step problems).",
            "comparison_baseline": "Compared across model sizes (7B vs 13B vs 70B) and across internal layers (first to last layer). Few-shot prompting used because zero-shot was unstable.",
            "key_finding": "LLaMA-2-7B can do basic arithmetic reasonably (better on integers/add/sub) but struggles on larger-digit and floating-point multiplication/division; pure computational ability is concentrated in upper layers and does not improve much simply by scaling from 7B to larger sizes.",
            "uuid": "e244.0",
            "source_info": {
                "paper_title": "Is Bigger and Deeper Always Better? Probing LLaMA Across Scales and Layers",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "LLaMA-2-13B (arithmetic)",
            "name_full": "LLaMA 2 - 13B parameter model (arithmetic probing in this paper)",
            "brief_description": "Evaluation of LLaMA 2 (13B) on the same arithmetic suite; shows marginally better integer performance than 7B but nearly identical floating-point performance, with computation localized to upper layers.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA 2",
            "model_size": "13B",
            "model_architecture": "decoder-only transformer",
            "arithmetic_operation_type": "addition, subtraction, multiplication, division, multi-step arithmetic (2-step and 3-step mixed operations)",
            "number_range_or_complexity": "Integers and floating-point (up to 3 decimal places); integer ranges: 1–100, 100–10,000, 10,000–100,000 (some tasks up to 1,000,000); also 2- and 3-operation complex expressions",
            "method_or_intervention": "Few-shot in-context prompting (6-shot for calculation tasks); multiple-choice probing with subtle distractors (± up to 20 float perturbations)",
            "performance_result": "Integer arithmetic ≈ 70.6% accuracy; floating-point arithmetic ≈ 52.6% accuracy. Similar pattern to 7B: good on addition/subtraction, multiplication/division degrade with digit count, floats harder than integers.",
            "mechanistic_insight": "Same layer-wise pattern: very little pure computational capability in lower layers; significant emergence of computation in middle-to-top layers with peaks in the last several (often penultimate) layers. No evidence of explicit algorithmic modules (e.g., dedicated carry-heads) was provided.",
            "performance_scaling": "Only marginal improvement over 7B in integer tasks; floating-point accuracy nearly unchanged — indicates arithmetic capability is not strongly scaling between 7B and 13B given same training data budget.",
            "failure_modes": "Struggles with larger-digit multiplication/division and floating-point arithmetic; multi-step arithmetic performance drops as number of operations increases.",
            "comparison_baseline": "Compared to 7B and 70B sizes and across layers; same few-shot prompting and multiple-choice evaluation.",
            "key_finding": "Increasing to 13B yields modest integer gains but does not substantially improve floating-point calculation; arithmetic competence remains concentrated in upper layers rather than distributed throughout the network.",
            "uuid": "e244.1",
            "source_info": {
                "paper_title": "Is Bigger and Deeper Always Better? Probing LLaMA Across Scales and Layers",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "LLaMA-2-70B (arithmetic)",
            "name_full": "LLaMA 2 - 70B parameter model (arithmetic probing in this paper)",
            "brief_description": "Evaluation of LLaMA 2 (70B) showing roughly similar base computational abilities to smaller models but substantially improved multi-step math reasoning and slightly higher integer accuracy; arithmetic processing remains primarily in upper layers.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA 2",
            "model_size": "70B",
            "model_architecture": "decoder-only transformer",
            "arithmetic_operation_type": "addition, subtraction, multiplication, division, multi-step arithmetic (2-step to 6-step mixed operations)",
            "number_range_or_complexity": "Integers and floating-point (up to 3 decimal places); integer ranges: 1–100, 100–10,000, 10,000–100,000 (some tasks up to 1,000,000); tasks include 1–6 step reasoning problems and 5–6 digit numeric calculations",
            "method_or_intervention": "Few-shot in-context prompting (6-shot for calculation tasks); multiple-choice probing; MPS datasets (GSM8K-derived) used for multi-step problems; distractor crafting via small numeric perturbations",
            "performance_result": "Integer arithmetic ≈ 70.8% accuracy; floating-point arithmetic ≈ 52.9% accuracy. Notably better performance on math problem solving (MPS-Cal ~48.3% vs ~28–30% for smaller models), with improvements concentrated on problems requiring 3–6 reasoning steps; pure arithmetic accuracy remains close to smaller models.",
            "mechanistic_insight": "Arithmetic and factual knowledge primarily encoded in upper layers; computational ability shows a sharp emergence in middle-to-top layers and often peaks a few layers before the final layer. The paper does not identify circuit-level mechanisms (e.g., specialized heads) but infers layer specialization: lower layers retain abstract/multilingual features while computation/knowledge are in top layers.",
            "performance_scaling": "Basic arithmetic (especially floating-point) scales poorly with size — minimal gains from 7B→70B. However, multi-step mathematical reasoning shows emergent improvement at 70B (not present at 7B/13B), indicating reasoning abilities benefit from scale even when raw computation does not.",
            "failure_modes": "Same as smaller models: multiplication/division degrade with digit count; floating-point performance poor relative to integers; all models (including 70B) fail substantially on very long multi-step problems (7+ steps).",
            "comparison_baseline": "Direct comparisons across 7B, 13B, and 70B and layer-wise (first..last layers). Few-shot prompting used; MPS tasks include both computational-error (MPS-Cal) and reasoning-error (MPS-Rea) derived distractors.",
            "key_finding": "Scaling to 70B yields clear gains in multi-step mathematical reasoning but not in basic arithmetic computation — arithmetic accuracy (particularly floating-point) remains nearly unchanged and computational capability is localized to upper layers.",
            "uuid": "e244.2",
            "source_info": {
                "paper_title": "Is Bigger and Deeper Always Better? Probing LLaMA Across Scales and Layers",
                "publication_date_yy_mm": "2023-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Emergent abilities of large language models",
            "rating": 2,
            "sanitized_title": "emergent_abilities_of_large_language_models"
        },
        {
            "paper_title": "Training verifiers to solve math word problems",
            "rating": 2,
            "sanitized_title": "training_verifiers_to_solve_math_word_problems"
        },
        {
            "paper_title": "PAL: Program-aided language models",
            "rating": 2,
            "sanitized_title": "pal_programaided_language_models"
        },
        {
            "paper_title": "GSM8K: A dataset for grade school math (Cobbe et al., 2021)",
            "rating": 2,
            "sanitized_title": "gsm8k_a_dataset_for_grade_school_math_cobbe_et_al_2021"
        },
        {
            "paper_title": "GPT-4 Technical Report",
            "rating": 1,
            "sanitized_title": "gpt4_technical_report"
        }
    ],
    "cost": 0.0106015,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Is Bigger and Deeper Always Better? Probing LLaMA Across Scales and Layers
9 Jan 2024</p>
<p>Nuo Chen 
Hong Kong University of Science and Technology (Guangzhou) Hong Kong University of Science and Technology</p>
<p>Ning Wu 
Hong Kong University of Science and Technology (Guangzhou) Hong Kong University of Science and Technology</p>
<p>Shining Liang 
Hong Kong University of Science and Technology (Guangzhou) Hong Kong University of Science and Technology</p>
<p>Ming Gong 
Hong Kong University of Science and Technology (Guangzhou) Hong Kong University of Science and Technology</p>
<p>Linjun Shou 
Hong Kong University of Science and Technology (Guangzhou) Hong Kong University of Science and Technology</p>
<p>Dongmei Zhang 
Hong Kong University of Science and Technology (Guangzhou) Hong Kong University of Science and Technology</p>
<p>Jia Li 
Hong Kong University of Science and Technology (Guangzhou) Hong Kong University of Science and Technology</p>
<p>Is Bigger and Deeper Always Better? Probing LLaMA Across Scales and Layers
9 Jan 20247C471941DE0CC02F59636A4D16C13E33arXiv:2312.04333v4[cs.CL]Where were the 1992 Olympics held? A: The 1992 Olympics were held in Barcelona, Spain.
This paper presents an in-depth analysis of Large Language Models (LLMs), focusing on LLaMA, a prominent open-source foundational model in natural language processing.Instead of assessing LLaMA through its generative output, we design multiple-choice tasks to probe its intrinsic understanding in high-order tasks such as reasoning and calculation.We examine the model horizontally, comparing different sizes, and vertically, assessing different layers.We unveil several key and uncommon findings based on the designed probing tasks: (1) Horizontally, enlarging model sizes almost could not automatically impart additional knowledge or computational prowess.Instead, it can enhance reasoning abilities, especially in math problem solving, and helps reduce hallucinations, but only beyond certain size thresholds;(2) In vertical analysis, the lower layers of LLaMA lack substantial arithmetic and factual knowledge, showcasing logical thinking, multilingual and recognitive abilities, with top layers housing most computational power and real-world knowledge.These findings provide new observations into LLaMA's capabilities, offering insights into the current state of LLMs.To reproduce our results and access datasets, please refer to https://github.com/nuochenpku/LLaMA_Analysis.</p>
<p>Introduction</p>
<p>Large language models (LLMs) (OpenAI, 2023;Scao et al., 2022;Chen, 2023;Yao et al., 2022;Chen et al., 2023c) have shown significant potential in numerous high-order open-generation tasks such as mathematical and logical reasoning.LLaMA (Touvron et al., 2023b), an open-source, state-ofthe-art foundational large language model has been designed to facilitate research in natural language processing communities.In a relatively brief period, LLaMA has garnered significant attention.This prominence can be attributed to its inherent accessibility and demonstrated efficacy across a diverse array of text-generation tasks (Hu et al., 2021;Chen et al., 2022bChen et al., , 2023a;;Gao et al., 2023).Beyond LLaMA's impressive generative capabilities, can we further uncover its intrinsic understanding abilities?Does bigger and deeper always lead to better performances in its advanced capabilities such as computational and reasoning sensitivity?Addressing this question is not only instrumental in comprehending the foundations of its success, but it also facilitates an understanding of its inherent limitations.This, in turn, can guide future advancements in the architecture and training optimization of LLMs.</p>
<p>In this paper, we conduct a series of experiments to probe the nature of LLaMA on five higher-order tasks under in-context learning, including calculation, math problem solving (MPS), logical reasoning, truthfulness, and factual knowledge detection.</p>
<p>The latter two are considered as the important symbols of hallucination.In these tasks, we probe the model's capabilities from two distinct perspectives: 1) Horizontally: Comparing the model's abilities across different sizes (Scaling Law); 2) Vertically: Comparing the different layers capabilities of the same size model (Layer-wise).Instead of directly testing LLMs via their open-text generation abilities, as is usually done, we prob LLaMA with a set of challenging multiple-choice questions.The primary considerations for this design are: Firstly, it offers controlled and efficient evaluation, with clear, quantifiable results that reduce ambiguity.This approach allows for directly targeted testing of specific knowledge areas and reasoning skills, as well as validating models' sensitivity to correct or incorrect answers; Secondly, our experimental observations reveal a tendency for LLaMA's lower layers to produce repetitive words rather than coherent sequences, which would lead to an unfair layer-wise comparison.</p>
<p>In the context of our experiments corroborating each other, we draw the following conclusions:</p>
<p>Horizontally: (1) The primary benefit of increasing model size lies in the enhanced reasoning abilities of the models, most notably in their improved capacity in MPS.This increase in size also tends to reduce the occurrence of hallucinations.However, these improvements are only evident when certain LLM size thresholds are surpassed, known as emergent abilities (Wei et al., 2022).For instance, models ranging from 7B to 13B show comparable performance across all probing tasks.It's only when the model size increases from 13B to 70B parameters that a noticeable improvement in reasoning capabilities and a reduction in hallucination issues can be observed, as shown in Figure 1; (2) The pure arithmetic capabilities and inherent factual knowledge of LLaMAs with different parameter sizes are remarkably similar.In other words, increasing the model size does not necessarily impart additional factual knowledge or significantly enhance computational capabilities, especially when the same volume of pre-training corpus is used.</p>
<p>Vertically: (1) We find that the lower and middle layers of LLaMA have almost negligible pure arithmetic and factual knowledge capabilities.As the network layers deepen, there is a noticeable leap in performance.Contrarily, even at the very lowest layers, LLaMA possesses logical thinking and recognitive abilities, such as in mathematics, logical reasoning, and avoiding hallucinations.While these abilities do enhance slightly with deeper layers, the improvement remains quite limited.This implies that current LLMs predominantly house computational power and real-world knowledge in their upper layers, while the lower layers are geared towards relevant abstract thinking but lack substantial real-world knowledge and computational skills.(2) Interestingly, in our layer-bylayer performance comparisons, we observe that the model's optimal performance in MPS and computational abilities is not always at the final layer.More often, these peak capabilities are found in several layers before the last.However, in contrast, for representing factual knowledge, the final layer of the model proves to be exceptionally crucial.</p>
<p>Further, we extend the mathematical probing tasks to the cross-lingal reasoning context.Specifically, we maintain the questions and incorrect options unchanged and translate the correct answers into other languages to assess the LLaMA's multilingual proficiency.In this setting, our layer-wise experiments show an effect completely opposite to monolingual reasoning: models' performance gradually decreases as the layers deepen.This indicates that LLaMA's earlier layers are responsible for preserving general multilingual features.</p>
<p>Of note, the results presented in our experiments do not necessarily equate directly to the generative capabilities of LLaMA.Rather, in this paper, we provide a novel and comprehensive perspective for observing the natural performance of LLaMA, giving insights to understand the current LLMs better.</p>
<p>LLaMA</p>
<p>LLaMA (Touvron et al., 2023a,c) is a series of foundation large language models, released by META, has becomes the most popular open-source LLMs in NLP communities.LLaMA is built on transformer layers (Vaswani et al., 2017), trained on trillion of tokens with the language modeling objective, showing powerful abilities down-stream tasks.The contextualized representations are optimized by predicting the next token based on the input sequences.</p>
<p>In this work, we probe LLaMA 2 series LLMs with our designed tasks, ranging from 7B to 70B parameters in in-context learning.Concretely, LLaMA 2-7B, 13B and 70B consist of 32, 40 and 80 transformer layers with 4096, 5120 and 8192 hidden embedding sizes, separately.Table 1: Test data statistics in our arithmetic tasks.</p>
<p>Probing Tasks</p>
<p>Probing tasks are generally utilized to explore the inherent knowledge and linguistic features within deep learning models.Previously, Jawahar et al.</p>
<p>(2019a) employed a series of probing tasks to examine the internal representations of BERT.However, with the rapid advancement of LLMs, there currently lacks comprehensive research that deeply analyzes the relationship between the higher-order capabilities of contemporary LLMs and factors such as model size and network layers.</p>
<p>To bridge this gap, we use probing tasks to access LLaMA in their ability to encode different types of features across two views: model size and individual layers.Specifically, we devise five high-order tasks: calculation, math problem solving (MPS), logical reasoning, truthfulness, and factual knowledge detection.We include the latter two as the hallucination detecting in the following.Besides, we also probe LLaMA efficiency in multilingual mathematical reasoning.In this section, we will illustrate them sequentially.</p>
<p>Calculation</p>
<p>In this paper, we focus on testing LLMs in basic arithmetic tasks, including four simple arithmetic expressions: addition (+), subtraction (-), multiplication (×) and division (÷):</p>
<p>• Add of two elements within 1∼100, 100∼10000, 10000∼100000, separately.</p>
<p>• Subtract of two elements within 1∼100, 100∼10000, 10000∼1000000, separately.</p>
<p>• Multiply of two elements within 1∼100, 100∼10000, 10000∼1000000, separately.</p>
<p>• Division of two elements within 1∼100, 100∼10000, 10000∼1000000, separately.</p>
<p>• Complex arithmetic operations that require performing two operations of addition, subtraction, multiplication, or division.</p>
<p>• Complex arithmetic operations that require performing three operations of addition, subtraction, multiplication, or division.</p>
<p>Of note, the elements used in the above arithmetic operations include integers and floatingpoint numbers (with precision up to three decimal places), separately.Table 1 shows the corresponding data statistics.Since we probe the computational abilities of LLaMA through the multiplechoice question answering task, to increase the difficulty and test the model's sensitivity to minor differences in computational results, we randomly add or subtract a floating-point number within ±20 (except 0) to the correct answer to create three different but indistinct incorrect options.</p>
<p>This design of our test set allows for an intuitive and fine-grained comparison of 1) the model's relative strengths and weaknesses in addition, subtraction, multiplication, and division operations; 2) the model's performance patterns when faced with complex calculations; 3) the variations in the model's computational abilities when dealing with floating-point numbers and integers, 1-2 digit, 3-4 digit, 5-6 digit numbers respectively.Our data are constructed by calling python random.randint()and random.uniform()functions.</p>
<p>Math Problem Solving</p>
<p>Besides validating LLaMA in arithmetic tasks, we also test the model in MPS tasks to comprehensively review its math reasoning abilities.</p>
<p>We select GSM8K (Cobbe et al., 2021) as our source data to construct challenging and misleading options that effectively fool the model.Our strategy involves the following steps:</p>
<p>• We first fine-tune the LLaMA 2-13B model on GSM8K, and then perform rejection sampling via inference 100 times to generate various reasoning paths based on the resulting model.</p>
<p>• Next, we extract all the formulas in each reasoning path and validate their accuracy.We use the erroneous reasoning paths to construct our probing task data:</p>
<p>-If a reasoning path only contains computational errors, meaning the correct answer can be obtained by recalculating, we retain it as part of our MPS-Cal probing test set.</p>
<p>MPS-Cal</p>
<p>Query: Peyton has 3 children and they each get a juice box in their lunch, 5 days a week.The school year is 25 weeks long.How many juices boxes will she need for the entire school year for all of her children?</p>
<p>Options: Peyton needs 25 weeks x 5 days x 3 children = 375 juice boxes (✓); 25 weeks x 5 days x 3 children = 75 juice boxes; Given the conditions of the problem, 3 children, 5 days a week, 25 weeks long, that's 3<em>5</em>25 = 105 juice boxes needed.</p>
<p>MPS-Rea</p>
<p>Query: A family of 12 monkeys collected 10 piles of bananas.6 piles had 9 hands, with each hand having 14 bananas, while the remaining piles had 12 hands, with each hand having 9 bananas.How many bananas would each monkey get if they divide the bananas equally amongst themselves?</p>
<p>Options: The first 6 bunches had 6 x 9 x 14 = 756 bananas.There were 10 -6 = 4 remaining bunches.The 4 remaining bunches had 4 x 12 x 9 = 432 bananas.All together, there were 756 + 432 = 1188 bananas.Each monkey would get 1188/12 = 99 bananas (✓); 6 piles had 6 x 9 x 14 = 756 bananas.The remaining 6 piles had 6 x 12 x 9 = 648 bananas.All together, there were 756 + 720 = 1476 bananas.Each monkey would get 1476/12 = 123.0bananas; 6 piles had 6 x 9 x 14 = 756 bananas.There were 10 -6 = 4 piles of bananas with 12 hands and 4 piles of bananas with 6 hands.The 4 piles of bananas with 12 hands had 4 x 12 x 9 = 432 bananas.The 4 piles of bananas with 6 hands had 4 x 6 x 9 = 216 bananas.There were 756 + 432 + 240 = 1428 bananas.Every monkey will get 1428/12 = 119.0bananas  -If all computations in a reasoning path are correct, but the final conclusion is wrong, indicating a reasoning error, we use it for our MPS-Rea test set.</p>
<p>The MPS-Cal focuses on assessing the model's sensitivity to computational results in solving mathematical problems.Conversely, MPS-Rea emphasizes evaluating the model's ability to discern correct from incorrect reasoning paths, requiring a superior level of understanding and reasoning capabilities.Table 2 shows several examples in MPS and calculation tasks.</p>
<p>Logical Reasoning</p>
<p>As a key indicator of the advanced capabilities of contemporary LLMs, logical reasoning stands out for its importance in examining, analyzing, and critically assessing arguments in natural language.</p>
<p>In our study, we employ Reclor (Yu et al., 2020) as a testing platform to evaluate the logical reasoning skills of these large models.Reclor comprises a dataset derived from logical reasoning questions found in standardized tests for graduate admissions.</p>
<p>Each sample from Reclor contains one context, one corresponding question and four options.</p>
<p>Hallucination Detecting</p>
<p>Hallucination, which means generating content that deviates from real-world facts observed during pretraining, is considered one of the most challenging issues in LLMs.In order to further investigate the relationship between hallucination and model layers and size, we conduct tests from two aspects: 1) Measure whether a language model is truthful in generating answers to questions, also known as truthfulness; 2) Test the model's internal factual knowledge.We use TruthfulQA MC tasks (Lin et al., 2022) and LAMA (Petroni et al., 2019) as test beds for these two aspects, respectively.It is important to note that in TruthfulQA, there may be more than one correct answer, accompanied by 4-5 incorrect options.As for LAMA, we randomly extract a subset containing 3070 questions along with their 9-10 corresponding options.</p>
<p>Cross-Lingual Math Problem Solving</p>
<p>In this study, we delve further into LLaMA's multilingual abilities.We translate the correct answers from the collected two datasets: MPS-Cal and MPS-Rea in Section 3.2 into four additional languages: Chinese, French, Spanish, and Thai, while keeping the questions and other incorrect options as they are, the new resulting test sets named xMPS-Cal and xMPS-Rea.This setting offers several advantages: Firstly, it tests the model's capacity in cross-lingual reasoning transfer, demonstrating its proficiency in not just recognizing but also reasoning in multiple languages.Secondly, by mixing incorrect choices with correct answers in different languages, we robustly assess the model's adaptability and comprehension across linguistic barriers.This unique setup challenges the model's ability to process and integrate multilingual information, not only evaluates the model's languagespecific capabilities but also its overall versatility in cross-lingual understanding and reasoning.</p>
<p>Test Setting</p>
<p>Consider a probing dataset D = {Q, C, O}, where Q, C and O denote a set of questions, contexts (only exits for LAMA), and answer options.For each question q ∈ Q, there is a corresponding set of answer choices, denoted as o ∈ O, where o = {o 1 , o 2 , ..., o n−1 , a}, n is the number of answer choices, and a refers to the correct answer.The model's task is to identify the correct answer from the set o for each question q.It need to assign the highest log-probability of completion following the question, independent of the other answer choices (Chuang et al., 2023).This selection process can be mathematically represented as:
o * i = argmax log P (o i |q) (1) Acc = 1, if a * &gt; (o * 1 , ..o * n−1 ), 0, otherwise.
(2)</p>
<p>Where log P (o i |q) is the log-probability that the choice o i to question q, as evaluated by the model.</p>
<p>Experimental Settings</p>
<p>We select LLaMA 2 from 7B to 70B as our experimental subject.Observing that LLaMA 2 exhibits significant instability in zero-shot testing, we choose to implement few-shot prompting in our probing tasks to optimize the model's performance.</p>
<p>In TruthfulQA and LAMA, we respectively employ 6-shot (Table 7) and 4-shot (Table 8) approaches.</p>
<p>For reasoning tasks, we consistently use 4-shot for both (x) MPS (Table 10) and logical reasoning (Table 9).In calculation tasks, we use 6-shot examples (Table 6).Detailed prompts are presented in Appendix A.</p>
<p>Experiments on Probing Model Size</p>
<p>In this section, we are dedicated to presenting a comparison of the results from LLaMA of different sizes on our probing tasks, as shown in Table 4 1 .</p>
<p>In Table 5, we showcase the detailed performance of models under different arithmetic rules and digit counts.Combining these two tables, we can draw the following conclusions:</p>
<p>Increasing model size hardly enhance the model's internal knowledge.From Table 4, we can see that the performance of LLAMA 2-7B and 13B on LAMA is identical , and even increasing the model size to 70B results in only a slight improvement (58.7% vs. 57.9%).This indicates that only increasing model size is difficult to improve the model's ability to remember and understand knowledge present in the training corpus, provided the training data remains the same.</p>
<p>Increasing model size does not significantly boost fundamental computational ability.Similarly, in our computational tasks, models of different sizes also show comparable computational abilities.Even though the 7B model lags a bit in Table 5: Detailed results of different operations in our probing arithmetic tasks.M-2/3 refers to arithmetic expression that requires 2/3 times mix operations.integer operations compared to 13B and 70B, it still performs similarly in floating-point operations (52.5% vs. 52.6% vs. 52.9%).Obviously, the computational abilities of 13B and 70B models are nearly identical.</p>
<p>Larger models show a relative improvement in reasoning ability and truthfulness.In MPS-Cal, which requires not just computational ability but also the understanding and reasoning of mathematical problems, the 70B model significantly outperforms the 7B and 13B models (48.3% vs 30.2%, 28.7%); MPS-Rea demands a clear discernment between correct and incorrect reasoning paths, further challenging the model's reasoning capabilities.</p>
<p>Here, the LLaMA 2-70B still shows considerable improvement.Considering that three LLMs show similar computational performances, we argue that such superior improvements could contribute to its better mathematical reasoning of 70B model.Figure 2 further indicate that all sizes of models perform well on mathematical problems requiring 1-2 steps of reasoning, with relative minimal differences between them.The enhancement of mathematical capabilities in the 70B model, relative to the 7B and 13B models, is primarily concentrated on problems requiring 3-6 steps of reasoning.The above findings demonstrate that the LLaMA series models all possess elementary reasoning capabilities.However, the ability to solve more complex reasoning problems only appears to emerge as the certain model size thresholds are surpassed.Moreover, when faced with problems requiring 7 steps of reasoning, the performance of all models rapidly declines and shows little difference, indicating that even LLaMA 2-70B is still at a "moderate intelligence" level, lacking strong reasoning skills.</p>
<p>In calculation, LLaMA's performance declines with increasing operation and numbers complexity.LLaMA possesses strong addition and subtraction capabilities, but its multiplication and division abilities noticeably decrease with increasing digit count, as seen in Table 5.Compared to floating-point operations, LLaMA is better at integer operations.Interestingly, in integer operations, LLaMA shows better capability in multiplication, but this strength significantly diminishes when dealing with floating-point numbers.</p>
<p>Experiments on Probing Layer-Wise</p>
<p>In this section, we focus on evaluating each layer of LLaMA across our different probing tasks.We present comprehensive results of all layers across three size models in Appendix.</p>
<p>Computational ability primarily exists in the upper layers of the model.First, in Figure 3, we present the performance of different layers of models ranging from 7B to 70B in conducting 5-6 digit integer and floating-point number calculations.From the figure, it is evident that almost no pure computational ability exists in the lower layers of any size model.However, as the number of layers increases, there is a significant leap in computational ability, peaking in the final few layers.The above results aptly explain why, in the MPS-cal probing task, the model's performance significantly improves with the increasing depth of layers, as shown in Figure 4. Notably, in most cases, the last layer of the model does not necessarily represent the best computational proficiency, especially in complex arithmetic expressions.For instance, layers 28-29 of the 7B model exhibit better computational skills than the last layer.</p>
<p>Models predominantly embed rich factual knowledge within their top layers.As depicted in Figure 4, for both the 7B and 70B models, the performances on LAMA suggest the factual knowledge learned by the LLaMA is also mainly located in the upper layers.In contrast, the lower layers exhibit a notable deficiency in retaining this knowledge.Yet, with the increase in layer depth, the model exhibits a substantial enhancement in its ability to process and retain factual information.Remarkably, it is observed that the LLaMA's ultimate layer harbors the greatest amount of factual knowledge.This finding stands in contrast to other probing tasks where the model's peak performance is typically manifested in the penultimate layers, rather than in the absolute final layer.</p>
<p>The abstract thinking and cognitive abilities of LLaMAs are consistently present across all layers.A comparative observation of the model's performance across various layers in tasks such as MPS-Rea, TFQA, and Reclor reveals that even in the model's lowest layers (e.g., the first layer), there is a certain degree of reasoning and cognitive capabilities, particularly in mathematical reasoning, which is evidenced by the results in MPS-Rea.</p>
<p>While the top layers still exhibit the best performance for the corresponding probing tasks, the improvement is relatively limited.We speculate that the reason for the small performance gap from the bottom to the top layer in the MPS-Rea probing task for the LLaMA 2-7B model might be due to: 1) A lack of related mathematical task corpus in the pre-training phase, leading to insufficient training; 2) The MPS-Rea task demands a high level of mathematical reasoning ability, which the current LLaMA2-7B model, even at its highest layer, does not possess strong capabilities in.</p>
<p>Earlier layers across different model scales show similar abilities.In our probing tasks, the lower layers (such as the first 15 layers) of models of different scales exhibit almost identical performances, despite having different hidden embedding sizes and attention heads in their transformer layers.This suggests that as the contextual information progresses through LLaMA's middle-top layers, it begins to specialize, leading to an increase in highorder capacities.</p>
<p>Experiments on Probing xMPS</p>
<p>In this section, we further to probe the multilingual proficiency of LLaMA models.The Figure 5 shows the performance of three models in our designed xMPS probing tasks across four languages.</p>
<p>From the comparison with Figure 3, we first observe that the LLAMA series models show a notable decrease in performance in languages other than English, particularly in the low-resource language Thai.Both 7B and 13B LLaMAs still show very similar performance in this domain, indicating their comparable multilingual abilities, yet the 70B model consistently outperforms them.Additionally, the lower layers of the models exhibit comparable performance across languages, with their effectiveness in French and Spanish being on par with English.This similarity is likely due to their Latin language family roots and inclusion in the LLaMA pre-training corpus.</p>
<p>However, unlike the results in all previous probing tasks, the performance of models in these languages decreases with deeper layers, a trend especially pronounced in the 13B model.Although there is a slight recovery in the top layers, the topmost layer still under-perform than lower layers.Given that prior experiments have indicated substantial mathematical reasoning abilities across all layers of the LLaMA series, it appears that the lower layers are primarily responsible for retaining multilingual abilities.This trait, however, diminishes with increasing layer depth, impacting the their ability to correctly interpret answers in other languages in the xMPS tasks.This phenomenon, however, is significantly less pronounced in the upper layers of the 70B model, indicating that en- hancing model size or the number of network layers could be an effective approach to bolstering the multilingual capabilities of LLMs.</p>
<p>To further analyze the above phenomenon, we perform 2D T-SNE visualizations of the embedding representations of LLaMA's first and last layers in the xMPS-Rea task across different languages.These visualizations show that at the model's top layer, distinct separations between the representations of different languages exist.Conversely, at the model's bottom layer, representations of different languages, particularly English, French, and Spanish, are relatively close and almost blend together, indicating that the lower layers primarily preserve language-agnostic features.The pronounced distinction of Chinese and Thai from other languages mainly stems from the lack of Chinese and Thai pre-training data in LLaMA's corpus.</p>
<p>Similar phenomenon also could observe in our xMPS-Cal probing task, where we present corresponding results in Appendix B, Figure 8 7 Related Works</p>
<p>The interpretability of neural networks (Peters et al., 2018;Goldberg, 2019), especially language models, has recently garnered significant attention from scholars in the field of Natural Language Processing (NLP).Over the last few years, much of this research has centered on BERT (Devlin et al., 2019), exploring how language models capture textual semantics across different layers (Tenney et al., 2019;Jawahar et al., 2019b;Liu et al., 2019;Chen et al., 2023b;Chuang et al., 2023).For instance, Tenney et al. (2019) introduced an innovative edge probing task to assess how contextual word representations encode sentence structures, covering a spectrum of syntactic, semantic, local, and long-range phenomena.Their findings suggest that language models trained on tasks like language modeling and machine translation robustly encode syntactic structures.Similarly, Jawahar et al. (2019b) employed a series of probing tasks within BERT, deduced that the lower layers of BERT capture phrase-level semantic features, mid-layers apprehend syntactic grammatical semantics, and upper layers comprehend sentence-level content, thereby laying a linguistic foundation for the tailored application of language models in specific contexts.</p>
<p>Currently, large language models (LLMs) (Ope-nAI, 2023;Scao et al., 2022;Chen, 2023;Yao et al., 2022;Touvron et al., 2023a,c), with their expansive parameter sizes, high-quality and extensive pre-training corpus, have exhibited astounding capabilities in various generative tasks (Brown et al., 2020), thereby gaining immense popularity.Particularly in advanced tasks such as mathematical reasoning and computation (Chen et al., 2023c), these LLMs surpass their predecessors by a large margin, including smaller-sized language models like BERT, Roberta (Chen et al., 2022a).Among these, LLaMA (Touvron et al., 2023a,c), notable for its open-source nature and efficiency, has rapidly emerged as a leading model in the realm of open-source LLMs.In this evolving landscape, several questions still remain to be explored, such as the interpretability of current LLMs, their intrinsic understanding abilities in high-order tasks, how their performance varies with changes in model size, and whether the highest layer of the model always represents its best performance?</p>
<p>Answering these questions could help understand the LLMs behaviour, model transparency and design more effective LLMs, etc.Unfortunately, there are currently no related research findings on LLMs.To facilitate the study of this field, we test LLaMA series models in five probing tasks from the perspective of model scales and layer-wise, unveiling their success and inherent limitations.</p>
<p>Conclusion</p>
<p>Beyond generation, we utilize several welldesigned and find-grained probing tasks to probe the intrinsic high-order capacities in LLaMA across the model scales and layers.Our results reveal that LLaMA models have nearly identical computational abilities and factual knowledge regardless of different scales while increasing size could benefit reasoning abilities.We also show that lower layers of LLaMA contain multilingual features and reasoning abilities while has hardly computational abilities and real-world knowledge.We have shown that LLaMA posses abstract thinking and cognitive abilities in their all layers.We expect that our study could contribute to build more powerful LLMs and given insights to help explain the results of LLMs in specific domain.</p>
<p>Limitation</p>
<p>The learning dynamics of neural networks, especially in LLMs can be quite intricate.Though, we have tried to explain the reasons behind our experimental findings, there still some question remain explored and hard to explain:</p>
<p>• Why LLaMA obtains optimal performances in their last 2-7 layers rather than the absolute final layer in some tasks like computation?</p>
<p>We guess the reason of this phenomenon is that models lack sufficient pre-training cor-pos related to these tasks while there is none straight way to prove this claim.</p>
<p>• Why the penultimate layer of LLaMA perform much better than the last layer in xMPS tasks?</p>
<p>• We also observe a remarkable phenomenon: essentially all LLaMA models begin to show significant performance improvements starting from their mid-layer networks.What contribute to this phenomenon?</p>
<p>Table 7: Prompts of the base setting in our experiments.</p>
<p>A Appendix: Prompts</p>
<p>In this section, we present prompts used in our probing tasks with few-shot examples.tasks, which are used in our all calculation related experiments, including 1-2bit, 3-4bit and 5-6bit.</p>
<p>For truthful QA tasks, we follow (Chuang et al., 2023) use the same the 6-shot prompts in Table 7.</p>
<p>Table 8 presents 4-shot prompts in factural knowledge detection probing tasks, where few-shot examples are randomly selected from the LAMA training dataset.</p>
<p>Table 9 illustrate 3-shot prompts used in logical reasoning tasks, where few-shot examples are randomly selected from the Reclor training dataset.</p>
<p>Table 10 illustrate 4-shot prompts used in MPS tasks, which are both used in MPS-Rea and MPS-Cal sub-tasks.Of note, as proved in (Chen et al., 2023c), English CoT prompts could contribute to better performances in multilingual reasoning tasks.Hence, we use the same prompt for xMPS tasks.</p>
<p>B Appendix: Layer-Wise Results</p>
<p>In this section, we provide detailed layer-wise results of LLaMA 2-7B, 13B and 70B models in our five designed probing tasks: MPS-Cal, LAMA, Reclor, TFQA and MPS-Rea, as presented in Table 11, Table 12 and Table 13, separately.</p>
<p>Figure 7 shows performances of each size LLaMA 2 model dealing with 1-2bit and 3-4bit integer and floating-point calculation tasks.Please answer the logical question based on the passage.</p>
<p>P: In rheumatoid arthritis, the body' s immune system misfunctions by attacking healthy cells in the joints causing the release of a hormone that in turn causes pain and swelling.This hormone is normally activated only in reaction to injury or infection.A new arthritis medication will contain a protein that inhibits the functioning of the hormone that causes pain and swelling in the joints.Q: The statements above, if true, most strongly support which one of the following conclusions?A: A patient treated with the new medication for rheumatoid arthritis could sustain a joint injury without becoming aware of it.</p>
<p>P: Patient: Pharmacists maintain that doctors should not be permitted to sell the medicine that they prescribe because doctors would then be tempted to prescribe unnecessary medicines in order to earn extra income.But pharmacists have a financial interest in having a monopoly on the sale of prescription medicines, so their objection to the sale of medicines by doctors cannot be taken seriously.Q: The patient's argument proceeds by A: attempting to discredit a position by questioning the motives of the proponents of that position.</p>
<p>P: Paula will visit the dentist tomorrow morning only if Bill goes golfing in the morning.Bill will not go golfing unless Damien agrees to go golfing too.However, Damien has decided not to go golfing.Ttherefore, Paula will not be visiting the dentist tomorrow morning.Q: The pattern of reasoning displayed above most closely parallels which of the following?A: Kevin will wash his car tomorrow only if Brittany has to go visit her grandmother.Unless Aunt Susan has to run errands, Brittany will not have to go visit her grandmother.Since Aunt Susan does not have to run errands, Kevin will not wash his car tomorrow.</p>
<p>Input: P: <Context> Q: <logical Question> A:</p>
<p>Table 9: Prompts of the logical reasoning tasks in our experiments.</p>
<p>Prompt of MPS and xMPS Tasks.(4-shot)</p>
<p>Give the answer to the math question step by step.</p>
<p>Q: Carly collected 7 starfish with 5 arms each and one seastar with 14 arms.How many arms do the animals she collected have in total?A: She has 7 * 5 + 14 = 49.</p>
<p>Q: Manny had 3 birthday cookie pies to share with his 24 classmates and his teacher, Mr. Keith.If each of the cookie pies were cut into 10 slices and Manny, his classmates, and Mr. Keith all had 1 piece, how many slices are left?A: Manny has 3 x 10 = «3*10=30»30 cookie pieces in total.He will have 30 -24 -1 -1 = 4 cookie pieces left.</p>
<p>Q: A new program had 60 downloads in the first month.</p>
<p>The number of downloads in the second month was three times as many as the downloads in the first month, but then reduced by 30% in the third month.How many downloads did the program have total over the three months?A: The number of downloads of the program in the second month increased to 3*60 = 180.</p>
<p>In the first two months, the total number of downloads of the program was 180+60 = 240.</p>
<p>In the third month, the number of downloads of the program reduced by 30/100*180 = 54 There were 180-54 = 126 downloads in the third month.</p>
<p>In the three months, the total number of downloads of the program was 126+240 = 366.</p>
<p>The answer is 366.In the three months, the total number of downloads of the program was 126+240 = 366.The answer is 366.Q: Michael had 58 golf balls.On tuesday, he lost 23 golf balls.On Wednesday, he lost 2 more.How many golf balls did he have at the end of wednesday?A: Michael started with 58 golf balls.</p>
<p>After losing 23 on Tuesday, he had 58 -23 = 35.After losing 2 more, he had 35 -2 = 33 golf balls.The answer is 33.</p>
<p>Input: Q: <Math Question> A:</p>
<p>Figure 1 :
1
Figure 1: Overall Comparison with LLaMA 2 7B-70B in our probing tasks.Detailed introduction of each task include in Section 3. Dashed lines represent the first layer of each model, while solid lines represent the last layer of the model.</p>
<p>Figure 2 :
2
Figure 2: Overall comparison between LLaMA 2 7B to 70B dealing with different reasoning steps problems in our probing MPS-Rea tasks.</p>
<p>Figure 3 :Figure 4 :
34
Figure3: Overall comparison between LLaMA 2 7B to 70B dealing with 5-6 bit calculations in our probing arithmetic tasks.We present more detailed results of 1-2 and 3-4 bit calculations in the Appendix B, Figure7.</p>
<p>Figure 5 :
5
Figure 5: Overall Comparison with LLaMA 2-7B to 70B in our xMPS-Rea probing tasks.ES, FR, ZH and TH refer to Spanish, French, Chinese and Thai.</p>
<p>Figure 6 :
6
Figure 6: 2D T-SNE plot of language embeddings computed from the first and last layers of LLaMA 2 7B-70B on the xMPS-Rea probing task.</p>
<p>Figure 7 :Figure 8 :
78
Figure 7: Overall Comparison with LLaMA 2-7B to 70B in our probing calculation tasks.Here, we show layer-wise results of each model in 2-bit and 4-bit integer and floating-point arithmetic expression, seaprately.</p>
<p>Table 2 :
2
Testing examples in our designed calculation and MPS probing tasks.
Avg. Ground-truth11113.51Avg. Candidates4434.97.69.7Total Queries360050071210008173070
TasksArithmetic-Int/Float Reclor (x) MPS-Cal (x) MPS-Rea TruthfulQA LAMA *</p>
<p>Table 3 :
3
Overall Test data statistics in our probing tasks.LAMA</p>
<ul>
<li>refers to we only use s subset of original corpus.</li>
</ul>
<p>Table 4 :
4
Table 3 presents detailed data statistics in our probing tasks.Overall performances of each size LLaMA 2 model in our probing tasks.LAMA * refers to we only use s subset of original corpus.MC3 accuracy means the normalized total probability assigned to all true answers among candidates in TruthfulQA.
Model Size LAMA  *ReclorMPS-Cal MPS-ReaTruthfulQAArithmetic(Fact)(Logical)MC1 MC3IntFloat7B57.920.028.747.028.620.7 67.9 52.513B57.923.730.246.629.120.7 70.6 52.670B58.726.448.351.937.327.1 70.8 52.9</p>
<p>Table 6 shows our 6-shot prompts of arithmetic
Prompt of factural knowledge Tasks. (4-shot)Please complete the following text so that it is factuallycorrect.Q: G20 consists of <mask>.A: CanadaQ: kerosene is a subclass of <mask>.A: petroleumQ: sundial is a subclass of <mask>.A: clockQ: Bordeaux and <mask> are twin cities.A: CasablancaInput:Q: <Sentence with a masked term>A:</p>
<p>Table 8 :
8
Prompts of the factual knowledge detection probing task used in our experiments.</p>
<p>Table 10 :
10
Prompts of MPS and xMPS tasks in our experiments.
Layers1234567891011121314151617181920Layers212223242526272829303132MPS-Cal 21.91 23.17 23.88 22.7523.624.58 23.882524.16 25.28 30.34 28.65LAMA39.86 42.95 45.4947.548.9849.850.350.76 51.42 50.53 48.12 57.87Reclor18.619.420.620.42021.419.62020.22020.220TFQA28.03 27.66 27.42 27.78 28.64 27.54 27.05 28.03 26.93 25.46 26.44 28.64MPS-Rea49.350.551.150.851.352.15249.549.245.842.247</p>
<p>Table 11 :
11
Layer-wise Results of LLaMA 2-7B on five probing tasks.
Layers1234567891011121314151617181920</p>
<p>Table 12 :
12
Layer-wise Results of LLaMA 2-13B on five probing tasks.
Layers1234567891011121314151617181920
Of note, we use the model last layer to count its performances in this section.
Q: What is human life expectancy in the United States?A: Human life expectancy in the United States is 78 years.Q: Who was president of the United States in 1955?A: Dwight D. Eisenhower was president of the United States in 1955.
Language models are few-shot learners. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, CoRR, abs/2005.14165Ilya Sutskever, and Dario Amodei. Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford2020</p>
<p>Orca: A few-shot benchmark for chinese conversational machine reading comprehension. Nuo Chen, Hongguang Li, Yinan Bao, Junqing He, Xinshi Lin, Qi Yang, Jianfeng Liu, Ruyi Gan, Jiaxing Zhang, Baoyuan Wang, arXiv:2302.136192023aarXiv preprint</p>
<p>Bridging the gap between language models and cross-lingual sequence labeling. Nuo Chen, Linjun Shou, Ming Gong, Jian Pei, Daxin Jiang, 10.18653/v1/2022.naacl-main.139Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesSeattle, United StatesAssociation for Computational Linguistics2022a</p>
<p>Alleviating over-smoothing for unsupervised sentence representation. Nuo Chen, Linjun Shou, Jian Pei, Ming Gong, Bowen Cao, Jianhui Chang, Jia Li, Daxin Jiang, 10.18653/v1/2023.acl-long.197Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics2023b1</p>
<p>What would harry say? building dialogue agents for characters in a story. Nuo Chen, Yan Wang, Haiyun Jiang, Deng Cai, Ziyang Chen, Jia Li, arXiv:2211.068692022barXiv preprint</p>
<p>Nuo Chen, Zinan Zheng, Ning Wu, Linjun Shou, Ming Gong, Yangqiu Song, Dongmei Zhang, Jia Li, arXiv:2310.20246Breaking language barriers in multilingual mathematical reasoning: Insights and observations. 2023carXiv preprint</p>
<p>Large language models are few(1)-shot table reasoners. Wenhu Chen, 10.18653/v1/2023.findings-eacl.83Findings of the Association for Computational Linguistics: EACL 2023. Dubrovnik, CroatiaAssociation for Computational Linguistics2023</p>
<p>Yung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon Kim, James Glass, Pengcheng He, arXiv:2309.03883Dola: Decoding by contrasting layers improves factuality in large language models. 2023arXiv preprint</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, CoRR, abs/2110.141682021</p>
<p>BERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 10.18653/v1/N19-1423Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics20191</p>
<p>Pal: Program-aided language models. Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, Graham Neubig, International Conference on Machine Learning. PMLR2023</p>
<p>Assessing bert's syntactic abilities. Yoav Goldberg, 2019</p>
<p>J Edward, Yelong Hu, Phillip Shen, Zeyuan Wallis, Yuanzhi Allen-Zhu, Shean Li, Lu Wang, Weizhu Wang, Chen, arXiv:2106.09685Lora: Low-rank adaptation of large language models. 2021arXiv preprint</p>
<p>What does BERT learn about the structure of language. Ganesh Jawahar, Benoît Sagot, Djamé Seddah, ACL (1). Association for Computational Linguistics2019a</p>
<p>What does BERT learn about the structure of language?. Ganesh Jawahar, Benoît Sagot, Djamé Seddah, 10.18653/v1/P19-1356Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational Linguistics2019b</p>
<p>Truthfulqa: Measuring how models mimic human falsehoods. Stephanie Lin, Jacob Hilton, Owain Evans, ACL (1). Association for Computational Linguistics2022</p>
<p>Linguistic knowledge and transferability of contextual representations. Nelson F Liu, Matt Gardner, Yonatan Belinkov, Matthew E Peters, Noah A Smith, 10.18653/v1/N19-1112Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics20191</p>
<p>Gpt-4 technical report. 2023OpenAI</p>
<p>Dissecting contextual word embeddings: Architecture and representation. Matthew E Peters, Mark Neumann, Luke Zettlemoyer, Wen-Tau Yih, 10.18653/v1/D18-1179Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational Linguistics2018</p>
<p>Language models as knowledge bases?. Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, S H Patrick, Anton Lewis, Yuxiang Bakhtin, Alexander H Wu, Miller, EMNLP/IJCNLP (1). 2019Association for Computational Linguistics</p>
<p>Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, arXiv:2211.05100Bloom: A 176b-parameter open-access multilingual language model. 2022arXiv preprint</p>
<p>What do you learn from context? probing for sentence structure in contextualized word representations. Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, Thomas Mccoy, Najoung Kim, Benjamin Van Durme, Sam Bowman, Dipanjan Das, Ellie Pavlick, International Conference on Learning Representations. 2019</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Hambro, arXiv:2302.13971Faisal Azhar, et al. 2023a. Llama: Open and efficient foundation language models. arXiv preprint</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, ; Jian, Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Pushkar Mishra, Igor Molybog. Andrew Nie, Jeremy Poulton, Rashi Reizenstein, Kalyan Rungta, Alan Saladi, Ruan Schelten, Eric Michael Silva, Ranjan Smith, Xiaoqing Subramanian, Ellen Tan, Binh Tang, Ross Taylor, Adina Williams,; Angela Fan, Melanie Kambadur, Sharan Narang; Robert Stojnic, Sergey EdunovAurelien Rodriguezand Thomas Scialom. 2023b. Llama 2: Open foundation and fine-tuned chat models</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.09288Llama 2: Open foundation and fine-tuned chat models. 2023carXiv preprint</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, Illia Polosukhin, NIPS. 2017</p>
<p>Emergent abilities of large language models. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, William Fedus, Trans. Mach. Learn. Res. 2022. 2022</p>
<p>Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, arXiv:2210.03629React: Synergizing reasoning and acting in language models. 2022arXiv preprint</p>
<p>Reclor: A reading comprehension dataset requiring logical reasoning. Weihao Yu, Zihang Jiang, Yanfei Dong, Jiashi Feng, ICLR. OpenReview.net2020</p>
<p>. MPS-Rea. 448</p>
<p>. MPS-Rea. 449</p>
<p>. MPS-Rea. 489</p>
<p>. MPS-Rea. 447</p>
<p>. MPS-Rea. 446</p>
<p>. MPS-Rea. 473</p>
<p>Layer-wise Results of LLaMA 2-70B on five probing tasks. Table. 13</p>            </div>
        </div>

    </div>
</body>
</html>
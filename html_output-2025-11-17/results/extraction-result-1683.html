<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1683 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1683</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1683</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-32.html">extraction-schema-32</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <p><strong>Paper ID:</strong> paper-269005006</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2404.05051v1.pdf" target="_blank">Skill Transfer and Discovery for Sim-to-Real Learning: A Representation-Based Viewpoint</a></p>
                <p><strong>Paper Abstract:</strong> We study sim-to-real skill transfer and discovery in the context of robotics control using representation learning. We draw inspiration from spectral decomposition of Markov decision processes. The spectral decomposition brings about representation that can linearly represent the state-action value function induced by any policies, thus can be regarded as skills. The skill representations are transferable across arbitrary tasks with the same transition dynamics. Moreover, to handle the sim-to-real gap in the dynamics, we propose a skill discovery algorithm that learns new skills caused by the sim-to-real gap from real-world data. We promote the discovery of new skills by enforcing orthogonal constraints between the skills to learn and the skills from simulators, and then synthesize the policy using the enlarged skill sets. We demonstrate our methodology by transferring quadrotor controllers from simulators to Crazyflie 2.1 quadrotors. We show that we can learn the skill representations from a single simulator task and transfer these to multiple different real-world tasks including hovering, taking off, landing and trajectory tracking. Our skill discovery approach helps narrow the sim-to-real gap and improve the real-world controller performance by up to 30.2%.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1683.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1683.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>STEADY</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Skill TransfEr And DiscoverY (STEADY)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A representation-based sim-to-real algorithm that (1) learns task-independent spectral skill representations in simulation, (2) discovers residual skills from limited real data by enforcing orthogonality to simulator skills, and (3) synthesizes real-world policies by combining simulator and discovered skills with a stabilized policy update.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Crazyflie 2.1 quadrotor</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>A small nano quadrotor (STM32F405 microcontroller) used for agile flight tasks; capable of position/attitude control via onboard motors and communicates via Crazyswarm with OptiTrack external motion capture for state estimation.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>robotics (aerial quadrotor control)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>gym-pybullet-drones (PyBullet)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>A physics-based simulator built on the Bullet physics engine that simulates rigid-body 3D dynamics of a quadrotor, propeller-generated forces, and basic motor/actuation mappings.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>mid-fidelity physics-based dynamics (PyBullet) — preserves rigid-body dynamics and motor force mappings but not full aerodynamics</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>rigid-body 3D dynamics, propeller force distribution, rotor forces (converted from desired motor forces), state variables (position, velocity, orientation, angular rates), conversion of motor force to PWM mapping approximated</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>aerodynamic effects (ground effect, downwash) and complex aerodynamic interactions, detailed motor response dynamics and delays, certain sensor and network noise characteristics, defective/unequal motor behavior and extra payload effects (these were not fully modeled and constitute the sim-to-real gap)</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Experiments on Crazyflie 2.1 using OptiTrack motion capture for positioning; additional hardware decks (microSD, LED) added 6–8 g payload; one tested vehicle had a defective motor requiring higher PWM; real-world noise sources included motion-capture noise and network communication fluctuations.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>General trajectory-tracking policy (trained in simulator) and transferable spectral skill representations; downstream transfer tasks in the real world: take-off, hovering (7s at 1 m), landing, and following an '8'-shaped trajectory.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Reinforcement learning with representation learning: spectral decomposition / SPEDER-style representation learning in simulator, Soft Actor-Critic-like policy improvement with linear Q parametrization over learned skill representations; policy parameterized via differentiable Mellinger controller (controller parameters learned via policy gradients).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Average position tracking error (reported as ×10^-1 m) and cumulative reward along trajectories; also qualitative metrics such as oscillation/stability during hover and roll/pitch angle variance.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td>Average tracking error (STEADY): 0.982 ×10^-1 m (≈0.0982 m); Cumulative reward: -1241 (unitless as defined by reward sum); comparative results: STEADY improved average tracking error by 11.9% vs Skill Transfer ablation and by 30.2% vs Simulator zero-shot, and improved cumulative reward by 9.1% vs Skill Transfer, 10.9% vs Built-in PID, and 22.7% vs Simulator zero-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Extra payload weight (6–8 g decks), defective motor requiring different PWM (motor mismatch/unequal thrust), unmodeled aerodynamics (ground effect, downwash), motor response delays and unmodeled actuator dynamics, motion-capture and network noise/latency.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Learning task-independent spectral skill representations in simulator and reusing them; discovering residual (sim-to-real) skills from limited real data with orthogonality constraints that enforce linear independence from simulator skills; initializing real-world policy from simulator policy and weights; penalizing KL divergence to the simulator policy during policy updates to avoid large policy shifts; using a robust differentiable Mellinger controller parameterization to improve stability on real hardware.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>Paper identifies that simulator must capture the transition dynamics structure sufficiently to learn base skill representations, but also emphasizes that unmodeled dynamics (aerodynamics, motor response mismatches, delays) necessitate real-world residual skill discovery; no numeric fidelity thresholds were specified.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td>Real-world data collection and representation discovery were performed during deployment (skill discovery step). Example: policy improvement shown after training with a small number of real trajectories (e.g., figure caption indicates improvement after 20 take-off/landing trajectories); skill discovery optimized via constrained spectral conditional density estimation (empirical loss with orthogonality penalties) and concurrent policy evaluation/improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Representation-based spectral skills learned in simulation are transferable across multiple real-world quadrotor tasks; augmenting simulator skill sets by learning orthogonal residual skills from limited real data (STEADY) narrows the sim-to-real gap and improves real-world performance (up to 30.2% improvement reported); orthogonality constraints prevent redundant re-learning and encourage discovery of distinct residual dynamics; initializing from simulator policy and constraining policy updates (KL penalty) stabilizes transfer to hardware.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Skill Transfer and Discovery for Sim-to-Real Learning: A Representation-Based Viewpoint', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1683.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1683.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Domain Randomization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Domain randomization for sim-to-real transfer</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A widely used sim-to-real technique that trains policies under randomized simulator parameters so the learned policy is robust to a distribution of transition dynamics and thus more likely to succeed in the real world.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>generic simulated robotic systems</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>Applies to many robotic agents (e.g., quadrotors, manipulators) where physics parameters can be randomized during simulation to produce robust policies.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>sim-to-real transfer for robotics</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>simulators (general — e.g., PyBullet, MuJoCo) with randomized parameters</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Simulators in which physical parameters (masses, frictions, motor constants, etc.) are randomly perturbed across episodes to create a distribution of transition dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>approximate / distributionally robust simulation (trains over a randomized ensemble rather than increasing per-simulation fidelity)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>Parametric variations of physical properties (mass, inertia, friction coefficients, motor constants); in literature can include sensor noise models and simple actuator variability.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>Cannot reliably capture complex unmodeled phenomena such as aerodynamics (ground effect/downwash), defective actuators, unmodeled delays, or complex contact effects that are not parameterized in the randomization.</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Policies trained to be robust over parameter distributions (generalization across tasks/environments that fall within randomized distribution).</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Reinforcement learning in simulation with randomized physical parameters (domain randomization).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Typically real-world task success rate or task performance under real dynamics; in this paper domain randomization is discussed conceptually and not used experimentally.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>Discussed as a baseline sim-to-real approach in related work; the authors state domain randomization can handle parameterizable gaps (e.g., inexact physical parameters) but not complex unmodeled effects like aerodynamics, contact effects, or response delays; the STEADY experiments did not apply domain randomization.</td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Limitations arise when the gap is due to phenomena not parameterized in the randomized simulator: aerodynamics, motor response nonlinearities/delays, defective actuators, contact phenomena.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Works when the true real-world dynamics lie within the randomized distribution or when relevant parameters are identified and randomized sufficiently during training.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>Authors note domain randomization only fixes gaps that can be modeled/parameterized; no numeric fidelity requirements provided.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Domain randomization is convenient when simulator parameters can be randomized to cover real-world variability, but it cannot handle unmodeled complex dynamics (aerodynamics, motor defects, response delays), motivating residual dynamics learning and the STEADY representation-based discovery approach.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Skill Transfer and Discovery for Sim-to-Real Learning: A Representation-Based Viewpoint', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Domain randomization for transferring deep neural networks from simulation to the real world <em>(Rating: 2)</em></li>
                <li>Sim-to-Real Transfer of Robotic Control with Dynamics Randomization <em>(Rating: 2)</em></li>
                <li>Residual Reinforcement Learning for Robot Control <em>(Rating: 2)</em></li>
                <li>Data-efficient control policy search using residual dynamics learning <em>(Rating: 2)</em></li>
                <li>Closing the Sim-to-Real Loop: Adapting Simulation Randomization with Real World Experience <em>(Rating: 2)</em></li>
                <li>Sim-to-(multi)-real: Transfer of low-level robust control policies to multiple quadrotors <em>(Rating: 2)</em></li>
                <li>Neural Lander: Stable Drone Landing Control Using Learned Dynamics <em>(Rating: 2)</em></li>
                <li>Neural-fly enables rapid learning for agile flight in strong winds <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1683",
    "paper_id": "paper-269005006",
    "extraction_schema_id": "extraction-schema-32",
    "extracted_data": [
        {
            "name_short": "STEADY",
            "name_full": "Skill TransfEr And DiscoverY (STEADY)",
            "brief_description": "A representation-based sim-to-real algorithm that (1) learns task-independent spectral skill representations in simulation, (2) discovers residual skills from limited real data by enforcing orthogonality to simulator skills, and (3) synthesizes real-world policies by combining simulator and discovered skills with a stabilized policy update.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "Crazyflie 2.1 quadrotor",
            "agent_system_description": "A small nano quadrotor (STM32F405 microcontroller) used for agile flight tasks; capable of position/attitude control via onboard motors and communicates via Crazyswarm with OptiTrack external motion capture for state estimation.",
            "domain": "robotics (aerial quadrotor control)",
            "virtual_environment_name": "gym-pybullet-drones (PyBullet)",
            "virtual_environment_description": "A physics-based simulator built on the Bullet physics engine that simulates rigid-body 3D dynamics of a quadrotor, propeller-generated forces, and basic motor/actuation mappings.",
            "simulation_fidelity_level": "mid-fidelity physics-based dynamics (PyBullet) — preserves rigid-body dynamics and motor force mappings but not full aerodynamics",
            "fidelity_aspects_modeled": "rigid-body 3D dynamics, propeller force distribution, rotor forces (converted from desired motor forces), state variables (position, velocity, orientation, angular rates), conversion of motor force to PWM mapping approximated",
            "fidelity_aspects_simplified": "aerodynamic effects (ground effect, downwash) and complex aerodynamic interactions, detailed motor response dynamics and delays, certain sensor and network noise characteristics, defective/unequal motor behavior and extra payload effects (these were not fully modeled and constitute the sim-to-real gap)",
            "real_environment_description": "Experiments on Crazyflie 2.1 using OptiTrack motion capture for positioning; additional hardware decks (microSD, LED) added 6–8 g payload; one tested vehicle had a defective motor requiring higher PWM; real-world noise sources included motion-capture noise and network communication fluctuations.",
            "task_or_skill_transferred": "General trajectory-tracking policy (trained in simulator) and transferable spectral skill representations; downstream transfer tasks in the real world: take-off, hovering (7s at 1 m), landing, and following an '8'-shaped trajectory.",
            "training_method": "Reinforcement learning with representation learning: spectral decomposition / SPEDER-style representation learning in simulator, Soft Actor-Critic-like policy improvement with linear Q parametrization over learned skill representations; policy parameterized via differentiable Mellinger controller (controller parameters learned via policy gradients).",
            "transfer_success_metric": "Average position tracking error (reported as ×10^-1 m) and cumulative reward along trajectories; also qualitative metrics such as oscillation/stability during hover and roll/pitch angle variance.",
            "transfer_performance_sim": null,
            "transfer_performance_real": "Average tracking error (STEADY): 0.982 ×10^-1 m (≈0.0982 m); Cumulative reward: -1241 (unitless as defined by reward sum); comparative results: STEADY improved average tracking error by 11.9% vs Skill Transfer ablation and by 30.2% vs Simulator zero-shot, and improved cumulative reward by 9.1% vs Skill Transfer, 10.9% vs Built-in PID, and 22.7% vs Simulator zero-shot.",
            "transfer_success": true,
            "domain_randomization_used": false,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": "Extra payload weight (6–8 g decks), defective motor requiring different PWM (motor mismatch/unequal thrust), unmodeled aerodynamics (ground effect, downwash), motor response delays and unmodeled actuator dynamics, motion-capture and network noise/latency.",
            "transfer_enabling_conditions": "Learning task-independent spectral skill representations in simulator and reusing them; discovering residual (sim-to-real) skills from limited real data with orthogonality constraints that enforce linear independence from simulator skills; initializing real-world policy from simulator policy and weights; penalizing KL divergence to the simulator policy during policy updates to avoid large policy shifts; using a robust differentiable Mellinger controller parameterization to improve stability on real hardware.",
            "fidelity_requirements_identified": "Paper identifies that simulator must capture the transition dynamics structure sufficiently to learn base skill representations, but also emphasizes that unmodeled dynamics (aerodynamics, motor response mismatches, delays) necessitate real-world residual skill discovery; no numeric fidelity thresholds were specified.",
            "fine_tuning_in_real_world": true,
            "fine_tuning_details": "Real-world data collection and representation discovery were performed during deployment (skill discovery step). Example: policy improvement shown after training with a small number of real trajectories (e.g., figure caption indicates improvement after 20 take-off/landing trajectories); skill discovery optimized via constrained spectral conditional density estimation (empirical loss with orthogonality penalties) and concurrent policy evaluation/improvement.",
            "comparison_across_fidelity_levels": false,
            "fidelity_comparison_results": null,
            "key_findings": "Representation-based spectral skills learned in simulation are transferable across multiple real-world quadrotor tasks; augmenting simulator skill sets by learning orthogonal residual skills from limited real data (STEADY) narrows the sim-to-real gap and improves real-world performance (up to 30.2% improvement reported); orthogonality constraints prevent redundant re-learning and encourage discovery of distinct residual dynamics; initializing from simulator policy and constraining policy updates (KL penalty) stabilizes transfer to hardware.",
            "uuid": "e1683.0",
            "source_info": {
                "paper_title": "Skill Transfer and Discovery for Sim-to-Real Learning: A Representation-Based Viewpoint",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Domain Randomization",
            "name_full": "Domain randomization for sim-to-real transfer",
            "brief_description": "A widely used sim-to-real technique that trains policies under randomized simulator parameters so the learned policy is robust to a distribution of transition dynamics and thus more likely to succeed in the real world.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_system_name": "generic simulated robotic systems",
            "agent_system_description": "Applies to many robotic agents (e.g., quadrotors, manipulators) where physics parameters can be randomized during simulation to produce robust policies.",
            "domain": "sim-to-real transfer for robotics",
            "virtual_environment_name": "simulators (general — e.g., PyBullet, MuJoCo) with randomized parameters",
            "virtual_environment_description": "Simulators in which physical parameters (masses, frictions, motor constants, etc.) are randomly perturbed across episodes to create a distribution of transition dynamics.",
            "simulation_fidelity_level": "approximate / distributionally robust simulation (trains over a randomized ensemble rather than increasing per-simulation fidelity)",
            "fidelity_aspects_modeled": "Parametric variations of physical properties (mass, inertia, friction coefficients, motor constants); in literature can include sensor noise models and simple actuator variability.",
            "fidelity_aspects_simplified": "Cannot reliably capture complex unmodeled phenomena such as aerodynamics (ground effect/downwash), defective actuators, unmodeled delays, or complex contact effects that are not parameterized in the randomization.",
            "real_environment_description": null,
            "task_or_skill_transferred": "Policies trained to be robust over parameter distributions (generalization across tasks/environments that fall within randomized distribution).",
            "training_method": "Reinforcement learning in simulation with randomized physical parameters (domain randomization).",
            "transfer_success_metric": "Typically real-world task success rate or task performance under real dynamics; in this paper domain randomization is discussed conceptually and not used experimentally.",
            "transfer_performance_sim": null,
            "transfer_performance_real": null,
            "transfer_success": null,
            "domain_randomization_used": false,
            "domain_randomization_details": "Discussed as a baseline sim-to-real approach in related work; the authors state domain randomization can handle parameterizable gaps (e.g., inexact physical parameters) but not complex unmodeled effects like aerodynamics, contact effects, or response delays; the STEADY experiments did not apply domain randomization.",
            "sim_to_real_gap_factors": "Limitations arise when the gap is due to phenomena not parameterized in the randomized simulator: aerodynamics, motor response nonlinearities/delays, defective actuators, contact phenomena.",
            "transfer_enabling_conditions": "Works when the true real-world dynamics lie within the randomized distribution or when relevant parameters are identified and randomized sufficiently during training.",
            "fidelity_requirements_identified": "Authors note domain randomization only fixes gaps that can be modeled/parameterized; no numeric fidelity requirements provided.",
            "fine_tuning_in_real_world": false,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": false,
            "fidelity_comparison_results": null,
            "key_findings": "Domain randomization is convenient when simulator parameters can be randomized to cover real-world variability, but it cannot handle unmodeled complex dynamics (aerodynamics, motor defects, response delays), motivating residual dynamics learning and the STEADY representation-based discovery approach.",
            "uuid": "e1683.1",
            "source_info": {
                "paper_title": "Skill Transfer and Discovery for Sim-to-Real Learning: A Representation-Based Viewpoint",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Domain randomization for transferring deep neural networks from simulation to the real world",
            "rating": 2,
            "sanitized_title": "domain_randomization_for_transferring_deep_neural_networks_from_simulation_to_the_real_world"
        },
        {
            "paper_title": "Sim-to-Real Transfer of Robotic Control with Dynamics Randomization",
            "rating": 2,
            "sanitized_title": "simtoreal_transfer_of_robotic_control_with_dynamics_randomization"
        },
        {
            "paper_title": "Residual Reinforcement Learning for Robot Control",
            "rating": 2,
            "sanitized_title": "residual_reinforcement_learning_for_robot_control"
        },
        {
            "paper_title": "Data-efficient control policy search using residual dynamics learning",
            "rating": 2,
            "sanitized_title": "dataefficient_control_policy_search_using_residual_dynamics_learning"
        },
        {
            "paper_title": "Closing the Sim-to-Real Loop: Adapting Simulation Randomization with Real World Experience",
            "rating": 2,
            "sanitized_title": "closing_the_simtoreal_loop_adapting_simulation_randomization_with_real_world_experience"
        },
        {
            "paper_title": "Sim-to-(multi)-real: Transfer of low-level robust control policies to multiple quadrotors",
            "rating": 2,
            "sanitized_title": "simtomultireal_transfer_of_lowlevel_robust_control_policies_to_multiple_quadrotors"
        },
        {
            "paper_title": "Neural Lander: Stable Drone Landing Control Using Learned Dynamics",
            "rating": 2,
            "sanitized_title": "neural_lander_stable_drone_landing_control_using_learned_dynamics"
        },
        {
            "paper_title": "Neural-fly enables rapid learning for agile flight in strong winds",
            "rating": 1,
            "sanitized_title": "neuralfly_enables_rapid_learning_for_agile_flight_in_strong_winds"
        }
    ],
    "cost": 0.0116025,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Skill Transfer and Discovery for Sim-to-Real Learning: A Representation-Based Viewpoint
7 Apr 2024</p>
<p>Haitong Ma 
Zhaolin Ren 
Bo Dai 
Na Li 
Skill Transfer and Discovery for Sim-to-Real Learning: A Representation-Based Viewpoint
7 Apr 20241925BC5F2B7C05BA6F32D6D53FC56BF0arXiv:2404.05051v1[cs.LG]
We study sim-to-real skill transfer and discovery in the context of robotics control using representation learning.We draw inspiration from spectral decomposition of Markov decision processes.The spectral decomposition brings about representation that can linearly represent the state-action value function induced by any policies, thus can be regarded as skills.The skill representations are transferable across arbitrary tasks with the same transition dynamics.Moreover, to handle the sim-to-real gap in the dynamics, we propose a skill discovery algorithm that learns new skills caused by the sim-to-real gap from real-world data.We promote the discovery of new skills by enforcing orthogonal constraints between the skills to learn and the skills from simulators, and then synthesize the policy using the enlarged skill sets.We demonstrate our methodology by transferring quadrotor controllers from simulators to Crazyflie 2.1 quadrotors.We show that we can learn the skill representations from a single simulator task and transfer these to multiple different real-world tasks including hovering, taking off, landing and trajectory tracking.Our skill discovery approach helps narrow the sim-to-real gap and improve the real-world controller performance by up to 30.2%.</p>
<p>I. INTRODUCTION</p>
<p>Reinforcement learning (RL) has demonstrated superior performance in many robotic simulators [1]- [3].However, transferring the controllers learned in simulators to real robots has long been a very challenging question in the RL community.One difficulty of sim-to-real transfer is that the learned policies are highly specific to the dynamics and tasks in the simulators, making them difficult to generalize to many real-world tasks, in which sim-to-real gaps exist.In the existing practices of sim-to-real transfer methods [4]- [7], the controllers learned in the simulator are usually difficult to generalize across various tasks and dynamic environments.</p>
<p>Recent studies in the theoretical RL community on the spectral decomposition of Markov decision processes (MDPs) [8]- [12] reveal the idea of task-independent representations for RL.The results of spectral decomposition is the spectral functions of the transition dynamics in MDPs.The spectral functions can linearly represent the stateaction value function, i.e., the Q-function, induced by any policy.Therefore, we say the spectral functions are taskindependent representation of skills, because the spectral functions include information needed to accomplish any tasks.These task-independent skill representations are shared across arbitrary tasks, thus are reusable and transferable.</p>
<p>Haitong Ma, Zhaolin Ren, Na Li are with School of Engineering and Applied Sciences, Harvard University.Bo Dai is with School of Computational Science and Engineering, Georgia Institute of Technology.Email: {haitongma, zhaolinren}@g.harvard.edu,bodai@google.com,nali@seas.harvard.edu.The work is supported under NSF AI institute: 2112085, NSF CNS: 2003111, NSF ECCS: 2328241</p>
<p>The Github link for codes, videos, and full report is available at https://congharvard.github.io/steady-sim-to-real/Meanwhile, the representation can also be used to synthesize policies.Given the representation-based skill sets and a specific task, we can perform sample-efficient planning upon the skill sets to synthesize the optimal policy.When the representations are unknown, sample-efficient representation learning methods have been proposed including maximum likelihood estimation [11], contrastive learning [13], spectral conditional density estimation [12], or variational inference [14].</p>
<p>Nevertheless, these representation-based skill learning are still designed for specific transition dynamics.When it applies to sim-to-real transfer, the sim-to-real gap, which will induce new skills different from the simulator skill sets, has not been investigated yet.Learning the sim-to-real gap from real-world data, also called residual dynamics learning [5], [15]- [17], naturally aligns with our representation learning viewpoint.However, naively learning the representations of residual dynamics might lead us to relearn redundant skills that are linearly dependent with the existing simulator skill sets.Therefore, we need additional incentives to discover new skills that enable us to bridge the sim-to-real gap.</p>
<p>To further leverage the transferability of the representation-based skill sets and discover new skills induced by the sim-to-real gap, we proposed the Skill TransfEr And DiscoverY (STEADY) for sim-to-real representation learning algorithm.We show that recent theoretical representation learning algorithms for spectral decomposition of MDPs, such as [12], [18], can apply to learning transferable representations of real-world robots.Moreover, we handle the sim-to-real gap by augmenting distinct representation-based skills learned from the simto-real gap, which we refer to as skill discovery.During the learning process, orthogonal constraints between the newly discovered skill sets and the simulator skill sets are enforced to fill the sim-to-real gap.In this way, we ensure that the skills necessary for the real robots are also included in our augmented skill sets, upon which the planning can be handled in a more complete space efficiently.Meanwhile, to ensure the policy transferring smoothly from the simulator to the real-world, we also designed a mechanism to characterize the policy shift in our algorithm.</p>
<p>We demonstrate our proposed algorithm by transferring the learned quadrotor control policies to Crazyflie 2.1 quadrotors.First, we learn the representations and train a tracking controller on the simulator.Then, we collect taskspecific data, including hovering, taking-off, landing, and trajectory tracking in the real world for skill transfer and discovery.The results show that our representation-based skills are easily transferred to the real world and generalizable to different tasks, and that our method has improved the realworld tracking performance by up to 30.2%.</p>
<p>II. RELATED WORKS</p>
<p>A. Sim-to-Real Transfer</p>
<p>Sim-to-real transfer aims to transfer knowledge from simulator to real-world robots and overcome the sim-to-real gap.We focus on sim-to-real gaps in the dynamics, which might arise from measurement noises, defective actuators, inexact physical parameters, contact or fluid effects, response delays, etc. Representative sim-to-real transfer techniques include domain randomization and residual dynamics learning.</p>
<p>1) Domain randomization: Domain randomization in RL refers to randomly perturbing the physical parameters of the simulators [4]- [6], [19]- [21].Then the RL agents aim to learn a policy performing well under a distribution of transition dynamics.The trained policies are directly applied to the real world and no knowledge will be learned from real-world data.These methods are convenient since simulator data are usually cheap to sample.However, domain randomization can only fix the sim-to-real gaps that can be modeled, such as inexact physical parameters.It cannot handle other sim-toreal gaps like aerodynamics, contact effects, response delays, etc.</p>
<p>2) Residual dynamics learning: Learning the sim-to-real gap, or the residual dynamics, appears in many recent simto-real transfer studies [5], [15]- [17], [22], [23].Unlike the domain randomization, during the real-world implementation stage, real-world data are collected to learn the sim-toreal gap and improve the policies.Existing practices have considered different models, including Gaussian mixture model [22], Gaussian process [15], [24], k-nearest neighbors [5], or deep neural networks [16], [17], [23].Then the learned sim-to-real gap is integrated with the prior simulator knowledge as external disturbance [17], [23], [24] or additive controllers [16].</p>
<p>We also follow the residual dynamics learning idea to fill the sim-to-real gap, but through a novel representation view.We use the knowledge of the simulator skill sets in the residual dynamics learning process in the way that enforces orthogonal constraints between simulator skill sets and the new skill sets, expanding skill sets while preventing redundancy of learning skills that are already in the simulator skill sets.</p>
<p>B. Representation-Based Knowledge Transfer</p>
<p>Representation-based knowledge transfer is commonly seen for computer vision models and visual-input control tasks [25], [26].For dynamics control tasks, [27] decomposed the policy networks into task-specific and robotspecific modules and show that the modules are transferable.[28] proposed transfer learning by matching the transfer functions, where transfer functions can be regarded as another type of representation.However, the algorithms are only limited to single-input-single-output dynamical systems.Our approach applies to general nonlinear dynamical systems and the transferability is justified rigorously by theoretical analysis.For theoretical representation learning, [29] has proved that there are provable benefits on sampling complexity for transferring the representations.However, no experimental results on simulators or in real world has been reported.</p>
<p>C. Representation Learning via Spectral Decomposition</p>
<p>The spectral structrue in the dynamics of MDP paves the way for sample-efficient planning on MDPs with continuous state and action spaces [8], [10].However, the analysis relies on the known representation, i.e., the spectral decompositions of transition dynamics.But in practice, the representations are intractable for general dynamics.Then [10], [11] proposed conceptual algorithms upon some computational oracle that simultaneously learns the representations and solves the optimal policy.Later [12], [14] further push the representation learning practical, using techniques like spectral conditional density estimation and variational learning, which provides us with practical algorithms for real-world robotic systems.</p>
<p>III. PRELIMINARIES A. Notations and Sim-to-Real Problem Setting</p>
<p>Markov Decision Processes (MDPs) are a standard sequential decision-making model for RL, and can be described as a tuple M = (S, A, r, P, ρ, γ), where S is the state space, A is the action space, r : S × A → R is the reward function, P : S × A → ∆(S) is the transition operator with ∆(S) as the family of distributions over S, ρ ∈ ∆(S) is the initial distribution and γ ∈ (0, 1) is the discount factor.The goal of RL is to find a policy π : S → ∆(A) that maximizes the infinite-horizon cumulative discounted reward
E s0∼ρ,π ∞ i=0 γ i r (s i , a i ) | s 0
by interacting with the MDP.The value function under transition dynamics P and policy π is defined as
V π P (s) = E π ∞ i=0 γ i r (s i , a i ) | s 0 = s ,
and the state-action value function under transition dynamics P is
Q π P (s, a) = E π ∞ i=0 γ i r (s i , a i ) | s 0 = s,
a 0 = a .When doing offpolicy learning we slightly abuse the notation of B to denote any data distribution sampled from the off-policy data set of replay buffer.</p>
<p>The sim-to-real problem indicates that we have a simulator of the real-world MDP M, which is also an MDP M • = (S, A, r • , P • , ρ • , γ).Notations with superscript • means they are related to the simulator.The two MDPs might differ in the transition dynamics, P • and P , initial distributions ρ • , ρ and rewards r • , r.The simulator is cheap to query and the real world is expensive.Therefore, we split the learning procedure into the simulator stage and the real-world transfer stage.In the simulator stage, the agent interacts with M • for sufficiently many transitions to obtain knowledge from the simulators.Then in the real-world stage, the agent transfers knowledge learned in the simulator to solve the optimal policy of M, while only collecting a limited number of transitions by interacting with M.</p>
<p>B. Spectral Decomposition and Skills in Markov Decision Processes</p>
<p>The formal definition of spectral decomposition of MDPs refers to the following structures on the transition dynamics and rewards, Definition 1 (Spectral decomposition of MDPs, [8], [10]).The spectral decomposition of an MDP M • with transition dynamics P • (s ′ | s, a) means there exists representations ϕ
• : S × A → R d and µ • : S → R d such that P • (s ′ | s, a) = ⟨ϕ • (s, a), µ • (s ′ )⟩ , r(s, a) = ⟨ϕ • (s, a), θ r ⟩
where θ r ∈ R d and ⟨•, •⟩ denotes the vector inner product.</p>
<p>The spectral decomposition enables that the representation ϕ can linearly represent the state-action value function Q π P • for any policy π,
Q π P • (s, a) = ⟨ϕ • (s, a), w π ⟩(1)
where
w π = θ r +γ S V π P • (s ′ ) µ (s ′ ) ds ′ .
The linear structure can be obtained by the recursive relationship between the
Q π P • and V π P • , Q π P • (s, a) =r(s, a) + γ S P • (s ′ |s, a)V π P • (s ′ )ds ′ = ⟨ϕ • (s, a), θ r ⟩ + γ S ⟨ϕ • (s, a), µ • (s ′ )⟩ V π P • (s ′ )ds ′ = ϕ • (s, a), θ r + γ S V π P • (s ′ ) µ • (s ′ ) ds ′ w π .
Representation-based skills.Note that the linear structure of Q-function holds for any policies under dynamics P • .We argue that ϕ • can be regarded as the skill sets under dynamics P • , since it includes the information of constructing arbitrary policy.Therefore, we interpret the representation ϕ • as the task-independent skill sets, which includes the information of all the skills needed for the model P • .Formally, given a Q-function, it induces the max-entropy policy as
π Q (a | s) := exp Q(s,a) τ a∈A exp Q(s,a) τ (2) = arg max π(•|s)∈∆(A) E π [Q(s, a)] + τ H(π)(3)
where π Q is the greedy max-entropy policy given a Qfunction, H(π
) := a∈A π(a | s) log π(a | s). Therefore, if we know the skill sets ϕ • , we can construct the max-entropy policies π(a | s) ∝ exp(w ⊤ ϕ • (s, a)) from skills ϕ • (s, a).
Practical Implementations.For practical implementations, we can parameterize the Q-function as Q(s, a; w) = w ⊤ ϕ • (s, a).Then, the policy evaluation can be conducted by minimizing the temporal-difference error w.r.t. the parameter w, i.e.,
min w E (s,a,r,s ′ )∼B,a ′ ∼π(•|s ′ ) [(r + γQ(s ′ , a ′ ) − w ⊤ ϕ • (s, a)) 2 ] (4
) where B is the data distribution in the replay buffer, and Q is the target Q-function commonly used in the target network trick.We emphasize that comparing to the deep Qlearning [30], the policy evaluation optimization is in the linear space spanned by the learned skill sets, therefore, is more stable.The policy improvement is the same as in other off-policy algorithms that optimize (3) by policy gradient.Practical implementations like soft actor-critic uses the reparameterization trick to simplify the calculations of (3) [2].We will discuss our policy parameterization in the experimental setup in Section V-A.</p>
<p>C. Skill Learning by Spectral Conditional Density Estimation</p>
<p>If the representation ϕ • , µ • is unknown, we need to learn these representations from data.The objective is to make ⟨ϕ • (s, a), µ • (s ′ )⟩ as close as P • (s ′ | s, a) as possible.There are multiple statistical learning methods to solve the problem [12]- [14].We follow the spectral conditional density estimation method, which optimizes the following loss function,
min ϕ • ,µ • E (s,a)∼B,s ′ ∼P • (•|s,a) ∥P • (s ′ | s, a)−⟨ϕ • (s, a), µ • (s ′ )⟩∥ 2 2
(5) where B denotes the offline dataset distribution.However, ( 5) is usually intractable since we usually do not know the exact value of P • (s ′ | s, a).This necessitates the use of samplingbased algorithms.For example, [12] uses a surrogate loss function that is equivalent with (5):
L feat (ϕ • , µ • ) :=C − 2E (s,a)∼B,s ′ ∼P (s ′ |s,a) ϕ(s, a) ⊤ µ (s ′ ) + E (s,a)∼B S ϕ(s, a) ⊤ µ (s ′ ) 2 ds ′ ,(6)
where C is a constant independent of ϕ • , µ • .For practical implementations, we can parameterize the ϕ • , µ • both as neural networks (with matching output layer dimensions) and doing gradient descent on L feat .</p>
<p>IV. STEADY: SKILL TRANSFER AND DISCOVERY FOR</p>
<p>SIM-TO-REAL LEARNING In the following two sections, we introduce our methodology of skill transfer and discovery (STEADY) for sim-to-real learning.The whole process includes the following steps:</p>
<p>(i) Learning skill sets and policy in the simulator;</p>
<p>(ii) Discovery of new skills from real-world data;</p>
<p>(iii) Policy synthesis from skill sets.An overview of the whole framework is shown in Figure 1.</p>
<p>A. Learning Skill Sets in the Simulator</p>
<p>We can leverage the previously mentioned representation learning techniques to learn the simulator skill sets ϕ • and µ • , as well as policy upon the skill sets.In this paper, we follow a similar procedure to the spectral decomposition representation learning (SPEDER) algorithm in [12], which is summarized in Algorithm 1.</p>
<p>B. Skill Discovery from Real-World Data</p>
<p>After the simulator stage, we transfer learned simulator representations/skill sets, namely ϕ • , to real robots.However, simply applying the policies π • learned from the simulator to the real world, like in zero-shot sim-to-real transfer, might lead to problems due to the sim-to-real gap.Therefore, the major motivation for skill discovery is learning new skills induced by the sim-to-real gap.</p>
<p>Following a similar procedure of assuming the decomposition structure on P • , we can assume that the sim-to-real gap also admits a spectral decomposition.</p>
<p>The spectral decomposition allows us to efficiently learn the gap from expensive real-world data, and then a good policy (and potentially more realistic simulators) given current knowledge of the existing simulator.We then learn the residual dynamics by formulating and solving the following least-square style optimization problem.
min ϕ,µ E (s,a)∼ρ0 ∥P (• | s, a) − P • (s ′ | s, a) − ⟨ϕ(s, a), µ(s ′ )⟩∥ 2 2 . (8)
Enforce Skill Discovery by Constraints.Before we proceed to solve the (8), we need to make sure that we are discovering new skills, which means that the skills learned from the realworld data are different from the previous simulator skills.In mathematical terms, having different skills means that the newly learned representations must be linearly independent with all the previous representations.Otherwise, we could simply represent the new skills by a linear combination of the previous skills.In such an undesirable case, the new skills will be redundant when we use the representations to linearly represent the Q-function in (1).To prevent this and enforce linear independence between the new and simulator skill sets, we add the following orthogonal constraints ⟨ϕ • i , ϕ j ⟩ = 0, ∀i ∈ {1, 2, . . ., d} and j ∈ {1, 2, . . ., s} (9) where the inner product is defined as
⟨ϕ • i , ϕ j ⟩ = E (s,a)∼B [ϕ • i (s, a)ϕ j (s, a)]
The orthogonal constraints in (9) enforce the linear independence between simulator skill sets ϕ • and newly learned skill sets ϕ.Attaching constraints (9), we have the following optimization problem for skill discovery:
min ϕ,µ E (s,a)∼ρ0 P (• | s, a) − P • (• | s, a) − ϕ(s, a) ⊤ µ(•) 2 2 s.t. ⟨ϕ • i , ϕ j ⟩ = 0, ∀i ∈ {1, 2, . . . , d}, ∀j ∈ {1, 2, . . . , s}.(10
) Practical Implementations.For the practical implementation of the skill discovery in (10), the minimization problem (ignoring the constraints) is similar to the problem in ( 5), but apart from estimating P , we also need to estimate P • .For P • , we can use the learned representation ⟨ϕ • , µ • ⟩ to replace P • .Then we follow a similar idea in (6) to minimize
L disc (ϕ, µ) := E (s,a)∼ρ0 P (• | s, a) − ϕ • (s, a) ϕ(s, a) ⊤ µ • (•) µ(•) 2 2 = C − 2E (s,a)∼B,s ′ ∼P (s ′ |s,a) ϕ • (s, a) ϕ(s, a) ⊤ µ • (•) µ(•) + E (s,a)∼B   S ϕ • (s, a) ϕ(s, a) ⊤ µ • (•) µ(•) 2 ds ′   ,(11)
where the simulator skill sets ϕ • , µ • are fixed in the skill discovery stage.We transfer the constraints in (11) to soft constraints by penalty methods for training stability, which leads to the following empirical loss function,
min ϕ,µ L disc (ϕ, µ) + λ i,j |⟨ϕ • i , ϕ j ⟩| , (12)
where λ is a hyperparameter penalizing the constraint violations.</p>
<p>C. Policy Synthesis for Real-World Tasks</p>
<p>Real-world Policy Evaluation.After we learn the representations ϕ, µ, we can leverage the augmented skill sets, [ϕ • , ϕ], to synthesize the policies for specific real-world tasks by the MDP planning algorithm such as policy iteration.</p>
<p>Here, we also follow a similar policy iteration algorithm to the one used in the simulator stage, while changing the skill sets/representations to be [ϕ
E (s,a,r,s ′ )∼B,a ′ ∼π(•|s ′ ) [(r + γQ(s ′ , a ′ ) − w ⊤ 1 ϕ • (s, a) − w ⊤ 2 ϕ(s, a)) 2 ]
(13) Then we can optimize the policy similar to (3) with the new linear parameterization of Q-function.Policy Synthesis.When initializing the real-world stage, we initialize the policy by simulator policy π • and weights w 1 by the simulator learned weights w π • so that we do not need to learn the Q-function from scratch.The skill discovery and policy synthesis are conducted simultaneously, and the algorithm is listed in Algorithm 2. To avoid the instability caused by changing from the simulator to the real world, we penalize the KL-divergence between the simulator policy and the updated policy to (3) when solving the real-world policy improvement,
max π E s∼B,a∼π(•|s) [Q(s, a)] − τ π DKL (π∥π • )(14)
where DKL (π∥π
• ) = E s [D KL (π(• | s)∥π • (• | s))],
τ π is a hyperparameter penalizing policy update.Practical Implementations.In practical implementations, we add several modifications to the soft actor-critic (SAC) [2] algorithm.First, we change the Q-function parameterization to the linear representation w ⊤ 1 ϕ • (s, a) + w ⊤ 2 ϕ(s, a).The representations ϕ, µ, ϕ • , µ • are all parameterized by fullyconnected neural networks.Second, we use a special parameterization called differentiable Mellinger controllers to parameterize the mean of stochastic actor.The differentiable Mellinger controllers are inspired by the commonly used Mellinger controller for quadrotors [31], which has great robustness on real Crazyflies.Details can be found in Section V-A.The Mellinger controller is a deterministic policy.We parameterized the variance of the stochastic actor using a neural network.The variance network will not be transferred to Crazyflies.</p>
<p>V. EXPERIMENTS A. Experimental Setup</p>
<p>We conduct experiments on the quadrotors.Quadrotor is a representative agile and safety-critical dynamical system, which is sensitive to controller malfunctions.Controller failure will immediately cause noticeable oscillations or even falling from the air.Moreover, there are multiple sim-to-real gaps in the quadrotor dynamics that are difficult to model, like motor response [6], [32] and aerodynamics including ground effect and downwash [17], [23].Therefore, quadrotors are perfect for demonstrating the sim-to-real transfer, which has been widely used in the sim-to-real transfer experiments [5]- [7], [17], [33].</p>
<p>Policy Parameterization.Our transfer learning approaches focus on representing the value function, which supports arbitrary policy parameterization.Although neuralnetwork-based controllers have been implemented on the Crazyflies [6], [7], [32], we found that they are usually unstable and not robust to external disturbance such as Algorithm 2 STEADY: Skill Transfer and Discovery for Sim-to-Real Learning
Input: Simulator MDP M • , real world MDP M 1: # Simulator stage. 2: ϕ • , µ • , π • , w π • = SPEDER(M • ) (See Algorithm 1) 3: # Real-world stage. 4: Initialization: π 0 = π • , w 0 1 = w π • 5: for episode k = 1, 2, . . . , N do 6:
Collect trajectory of (s, a, s ′ ) in real world, following current policy π k−1 .</p>
<p>7:</p>
<p>Skill discovery to learn ϕ, µ by optimizing (12), given simulator skills ϕ • , µ • .</p>
<p>8:</p>
<p>Policy evaluation by solving Eq. ( 13).Policy improvement by doing policy gradient with respect to (14).10: end for Output: Final policy π N observation noise, response delays, motor dynamics, etc.To improve controller stability and make the experiments more reproducible, we use a differentiable version of Mellinger controller [31] as our policy parameterization.The Mellinger controller can be regarded as a modified hierarchical PID controller, which is one of the most commonly used feedback controllers for quadrotors.The policy gradients are applied on the parameters of the Mellinger controllers, which can be found here.Major parameters of Mellinger controller include the PID gain of position, rotation and angular rate error.</p>
<p>Simulator and real-word tasks.We first train the policies in the simulator for general trajectory tracking tasks.We fix a goal position and randomly initialize the drone states (position, rotation, velocity, angular and positional velocity, propeller rpm) to track the reference.Then for the downstream real-world transfer stage, we consider three different tasks, (1) taking off, hovering, and landing; (2) following a trajectory in the shape of "8".We will show that the skill sets learned from the general trajectory tracking tasks are transferable to these specific tasks, and the skill discovery will improve the real-world controller performance.</p>
<p>Baselines and ablation studies.To demonstrate the effectiveness of the proposed controller, we show the comparison between the controller learned by the Algorithm 2 and baselines and ablations.The baseline is the builtin controller in the Crazyflie firmware (labeled as "Builtin").Ablation studies including simulator policy (labeled as "Simulator") and skill transfer policy (labeled as "Skill Transfer").Simulator policy means that we directly use the controller learned in the simulator by Algorithm 1.For the skill transfer policy, we only synthesize policies from the simulator skill sets.The skill discovery in line 7 of Algorithm 2 is skipped, and the Q-function parameterization is the same as the simulator stage.</p>
<p>B. Simulators Setup</p>
<p>The dynamical systems of a quadcopter can be regarded as a 3D motion of a rigid body, with four propellers placed at each corner generating force and torque.We use the  gym-pybullet-drones simulator [34] based on the bullet physical simulator.The simulator frequency and control frequency is both 240 Hz, and the maximum episode length is 480 steps, which is 2s in simulation time.</p>
<p>Observations, actions, and reward designs.The observations of the system include relative position p − p goal , rotations/altitude (quaternion q and roll, pitch, yaw angles {r, p, y}), velocity v, angular velocity/altitude rate ω, and last-step inputs u last , integral and differences of position, altitude, and altitude rate error, rotor PWM inputs.Note that we add the integral and difference of position, altitude, and altitude rate error because the Mellinger controller uses this information.The reward r is designed as
r(s, a) =2 − 2.5∥p − p goal ∥ − 1.5||[r, p]|| − 0.05∥v∥ − 0.05∥ω∥ − 0.1∥u∥(15)
The initial distribution and other detailed simulator settings can be found in Appendix B of our online report [35].Domain randomization and curriculum learning.Some existing sim-to-real transfer on quadrotors have used domain randomization [6] on the physical parameters and used curriculum learning to speed up the training [32].Curriculum learning means adaptively changing the reward weights in (15) to improve performance.These aspects are not the major focus of our paper, and for crazyflie the physical parameters has been identified by existing papers.Therefore, we did not add these methodologies to our simulators.</p>
<p>C. Real-World Robots Setup</p>
<p>We show the effectiveness of our sim-to-real learning on the Crazyflie 2.1 Quadrotor with an STM32F405 microcontroller clocked at 168 MHz.We transfer the learned controller parameters to Crazyflie and handle the communication using Crazyswarm [36].We use the Optitrack motion capture system with single-marker configuration to provide location information for the Crazyflie, shown in Figure 3(a).The Crazyflie carries an additional micro SD card deck to record data or a LED ring deck to highlight itself shown in Figure 3(b) and 3(c), respectively.</p>
<p>Causes of sim-to-real gaps.The sim-to-real gaps include several factors.(1) Extra weight from the decks carried.The additional decks weigh 6-8g each, which is a noticeable weight change compared to the original weight of Crazyflie, 27g.(2) For the taking-off, hovering, and landing tasks, we found out that our Crazyflie used for these specific tasks has a defective motor that requires higher PWM input to balance shown in Figure 3(d).(3) Other sim-to-real gaps that are not captured by models like motion capture noise and network fluctuations.</p>
<p>D. Experimental Results</p>
<p>1) Simulation Stage:</p>
<p>The features ϕ • , ϕ, µ • , µ are parameterized by fully connected neural networks with two hidden layers with 256 neurons.The dimensions of all the representations are 256.For the simulation stage, we train the algorithm with 1.6×10 6 transitions and the expected return during the training process are shown in Figure 4. 2) Real-world Stage: Taking off, hovering and landing.First we show the experimental results on the real-world task of taking off, hovering for 7 seconds at 1m and landing.We compare the 3D trajectories in Figure 5 and roll and pitch angle in Figure 6 with skill transfer and discovery policy (labeled as "STEADY"), skill transfer policy (labeled as "Skill Tranfer") and simulator policy (labeled as "Simulator Zero-Shot", Zero-Shot means that no learning from realworld data is implemented.).Two sequences of snapshots comparing Simulator policy and the policy improved by STEADY are also shown in Figure 2, and the full video can be found at https://congharvard.github.io/steady-sim-to-real/. Results show that after learning from the real-world data using the STEADY framework, the controller can maintain at the target height very stably without oscillations even with a defective motor and extra weight carried.The stability of the STEADY controller can also be verified from the flat roll and pitch angle curves in Figure 6.For the controllers from ablation studies, the simulator policy oscillate heavily and cannot maintain the height.The skill transfer policy stay in the air longer than the simulator policy but still fails to maintain stably at the target position.Trajectory tracking.For the trajectory tracking task, the drone needs to follow a trajectory in the shape of the number "8", which is commonly seen in related studies [32], [33].The trajectories is (x(t), y(t), z(t)) = (sin(t), 1  2 sin(2t), 1.0) and the shape is shown as orange dash lines in Figure 7. Figure 7 shows the trajectory tracking results with different controllers compared to reference trajectories.The performance is compared in Table I.We calculate two metrics for the tracking error, the average position tracking error and the cumulative rewards.The average position tracking error averages ∥p − p goal ∥ over the trajectory.The cumulative rewards sums up rewards defined in (15) over the trajectory (removing the initial constant term of 2).</p>
<p>VI. CONCLUDING REMARKS</p>
<p>In this paper, we proposed the STEADY framework, which utilizes skill transfer and discovery for sim-to-real learning.Inspired by the concept of representations as skill sets when considering the spectral decompositions of MDPs, we show that we can learn the skill sets from simulators and then transfer them to the real world.The representation-based skill sets can also help sim-to-real transfer.By enforcing orthogonal constraints between the simulator skill sets and the skill sets induced by the sim-to-real gap, we promote the discovery of useful and distinct new skills.Building on the enlarged skill sets comprising these new skill sets and the existing simulator skill sets, STEADY facilitates more efficient and effective sim-to-real transfer smoothly.</p>
<p>A-A Differentiable Mellinger controllers.</p>
<p>Notations.In the rigid body dynamics, we need to be careful with the reference frame.We use W to denote the world frame, B to denote the body frame of the crazyflie.We use W R B to denote the rotation matrix of frame B with respect to frame W . the x, y, z axes of B frame are denoted by x B , y B , z B .We also use Z − X − Y Euler angles to define the roll, pitch, and yaw angles (ϕ, θ, and ψ) as a local coordinate system.The angular velocity of the robot is denoted by ω BW , denoting the angular velocity of frame B in the frame W , with components p, q, and r in the body frame:
ω BW = px B + qy B + rz B .
Mellinger Controller.The Mellinger controller was proposed in [31], which is a modified version of the PID controller.The design is similar to a hierarchical PID controller.We will briefly introduce the Mellinger controller here and then discuss our modification to the Mellinger controller to make it as our policy parameterizations.The Mellinger controller starts from the position and velocity error,
e p = p − r T , e v = v − ṙT
where r T is the 3D vector of the positions of the reference trajectory.Then we compute the desired force vector by
F des = −K p e p − K v e v + mgz W + mr T
where K p and K v are positive definite gain matrices.Next we project the desired force to the z-axis of the current body frame, which is the first input, desired thrust,
u 1 = F des • z B
To determine the other three inputs, we must consider the rotation errors.First, we observe that the desired z B direction is along the desired thrust vector:
z B,des = F des ∥F des ∥ .
Thus if e 3 = [0, 0, 1] T , the desired rotation W R B denoted by R des for brevity is given by: R e 3 = z B,des .</p>
<p>Knowing the specified yaw angle along the trajectory, ψ T (t), we compute x B,des and y B,des as in the previous section:</p>
<p>x C,des = [cos ψ T , sin ψ T , 0]</p>
<p>T , and y B,des = z B,des × x C,des ∥z B,des × x C,des ∥ , x B,des = y B,des × z B,des , provided x C,des × z B,des ̸ = 0.This defines the desired rotation matrix R des .While mathematically this singularity is a single point in SO(3), this computation results in large changes in the unit vectors in the neighborhood of the singularity.To fix this problem, we observe that −x B, des and −y B,des are also consistent with the desired yaw angle and body frame z axis.In practice we simply check which one of the solutions is closer to the actual orientation of the quadrotor in order to calculate the desired orientation, R des .</p>
<p>Next we define the error on orientation:
e R = 1 2 R T des W R B − W R T B R des ∨
where ∨ represents the vee map which takes elements of so(3) to R 3 .The angular velocity error is simply the difference between the actual and desired angular velocity in body frame coordinates:
e ω = B [ω BW ] − B [ω BW,T ] .
Now the desired moments and the three remaining inputs are computed as follows:
[u 2 , u 3 , u 4 ] T = −K R e R − K ω e ω ,
where K R and K ω are diagonal gain matrices.This allows unique gains to be used for roll, pitch, and yaw angle tracking.</p>
<p>To summarize, the parameters in the Mellinger controller are the feedback gains, K p , K v , K R , K ω .</p>
<p>Modification to differentiable version.In practice, we use the PID to replace the simple linear feedback, so for each feedback gain (take K p as an example), we have the proportional term gain K p,p , integral gain K p,i and difference term gain K p,d .For the feedback matrices, we usually use diagonal matrices for simplicity.The first two diagonal elements are for x, y axes, and we put the same gains for the x, y axes.Therefore, take K p,p as example, denote the xy axes gain as K xy p,p , and K p,p = diag(K xy p,p , K xy p,p , K z p,p ).Therefore, for each set of feedback gain matrices, we have 2 feedback gains for proportional, integral, and difference matrices, respectively.These parameters sums up to 2 × 3 × 4 = 24 trainable parameters.APPENDIX B DETAILED SIMULATOR SETUP Initial distribution and simulator action design.</p>
<p>In the simulator, we set the tracking target p goal = [0.0,0.0, 1.0] and randomly initialize the position p ∼ N ([0.0, 0.0, 1.0], diag([0.02,0.02, 0.02])).The initial roll, pitch, and yaw angle are randomly sampled from N (0.0 • , 5.0 • ).For the simulator action space, we reformulate the action space.The action space is 4-dimensional, including desired vertical thrust F z and rotational forces F r , F p , F y .Then these desired forces are converted to desired propeller forces by the power distribution rules
         F 1 = F z − F r /2 + F p /2 + F y F 2 = F z − F r /2 − F p /2 − F y F 3 = F z + F r /2 − F p /2 + F y F 4 = F z + F r /2 + F p /2 − F y
where F i is the desired force of i th propeller.Compared to the original actions in gym-pybullet-drones, which is the rotor RPM, we change it to the normalized desired motor force.We move the motor force to PWM onboard the crazyflies.The motor force to PWM is extensively studied by previous work, like [17].</p>
<p>Fig. 1 : 3 :Assumption 1 (
131
Fig.1: Overview of the STEADY framework for sim-to-real learning.More information can be found on our project website.</p>
<p>(a) Simulator policy.(b) Policy improved by STEADY after training with 20 taking-off and landing trajectories.</p>
<p>Fig. 2 :Fig. 3 :
23
Fig. 2: Snapshots of taking-off, hovering 7 seconds then landing with the simulator policy and policy improved by .Yellow dash lines indicate the target hovering height (1m).The Crazyflies are highlighted with red boxes.The snapshots are taken every 0.8 seconds.Figure 2(a) shows the simulator policy and Figure 2(b) shows the policy learned by the proposed STEADY algorithm.</p>
<p>Fig. 4 :
4
Fig. 4: Episodic return to training samples during the training process with 5 random seeds.The shaded region implies 95% confidential interval over 10 evaluation episodes for every 2000 samples of the five random seeds.</p>
<p>Fig. 5 :
5
Fig. 5: Trajectories of taking-off, hovering and landing trajectories.</p>
<p>• , ϕ].Similarly, in the policy evaluation stage, we parameterize the Q-function by a linear combination of the enlarged skill sets [ϕ • , ϕ], Q(s, a; w) = w ⊤ 1 ϕ • (s, a) + w ⊤ 2 ϕ(s, a).We can minimize the TD error, min
w1,w2</p>
<p>Table I
IUROODQJOHLQGHJUHHSLWFKDQJOHLQGHJUHH67($'&lt;6NLOO7UDQVIHU6LPXODWRU=HUR6KRWWLPHVWLPHVFig. 6: Roll and pitch angles during taking off, hovering and landingexperiments.that the STEADY controller achieves the smallest trajec-tory tracking error and highest accumulative rewards. Thetracking error is comparable to the built-in controllers andimproved by 11.9% and 30.2% compared to ablation skilltransfer controller and ablation simulator controller, respec-tively. For the cumulative reward, the STEADY controlleroutperforms all three baseline controllers, improving on theablation skill transfer controller by 9.1%, the Built-in PIDcontroller by 10.9%, and the ablation simulator controllerby 22.7%.7UDMHFWRU\WUDFNLQJYLVXDOL]DWLRQ67($'&lt;SROLF\SURSRVHG6NLOO7UDQVIHUSROLF\$EODWLRQ\P\P67($'&lt;5HI7UDM6NLOO7UDQVIHU5HI7UDM[P[P6LPXODWRUSROLF\$EODWLRQ%XLOWLQSROLF\%DVHOLQH\P\P6LPXODWRU5HI7UDM%XLOWLQ5HI7UDM[P[Pshows
Fig. 7: Trajectory tracking visualization.</p>
<p>TABLE I :
I
Tracking performance comparison.
Average Tracking CumulativeControllerError (×10 −1 m)RewardsSTEADY0.982-1241Skill Transfer1.115-1365Simulator1.406-1584Built-in PID0.983-1392</p>
<p>Mastering the game of Go without human knowledge. D Silver, J Schrittwieser, K Simonyan, I Antonoglou, A Huang, A Guez, T Hubert, L Baker, M Lai, A Bolton, Y Chen, T Lillicrap, F Hui, L Sifre, G Van Den Driessche, T Graepel, D Hassabis, Nature. 5507676Oct. 2017Nature Publishing Groupnumber: 7676 Publisher</p>
<p>Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor. T Haarnoja, A Zhou, P Abbeel, S Levine, Proceedings of the 35th International Conference on Machine Learning. the 35th International Conference on Machine LearningPMLRJul. 2018</p>
<p>Outracing champion Gran Turismo drivers with deep reinforcement learning. P R Wurman, S Barrett, K Kawamoto, J Macglashan, K Subramanian, T J Walsh, R Capobianco, A Devlic, F Eckert, F Fuchs, L Gilpin, P Khandelwal, V Kompella, H Lin, P Macalpine, D Oller, T Seno, C Sherstan, M D Thomure, H Aghabozorgi, L Barrett, R Douglas, D Whitehead, P Dürr, P Stone, M Spranger, H Kitano, Nature. 6027896Feb. 2022Nature Publishing Groupnumber: 7896 Publisher</p>
<p>Learning dexterous in-hand manipulation. O M Andrychowicz, B Baker, M Chociej, R Józefowicz, B Mc-Grew, J Pachocki, A Petron, M Plappert, G Powell, A Ray, J Schneider, S Sidor, J Tobin, P Welinder, L Weng, W Zaremba, The International Journal of Robotics Research. 391Jan. 2020SAGE Publications Ltd STM</p>
<p>Champion-level drone racing using deep reinforcement learning. E Kaufmann, L Bauersfeld, A Loquercio, M Müller, V Koltun, D Scaramuzza, number: 7976 PublisherNature. 6207976Aug. 2023Nature Publishing Group</p>
<p>Sim-to-(multi)-real: Transfer of low-level robust control policies to multiple quadrotors. A Molchanov, T Chen, W Hönig, J A Preiss, N Ayanian, G S Sukhatme, 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2019</p>
<p>Comparing Quadrotor Control Policies for Zero-Shot Reinforcement Learning under Uncertainty and Partial Observability. S Gronauer, D Stümke, K Diepold, 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). Detroit, MI, USAIEEEOct. 2023</p>
<p>Provably efficient reinforcement learning with linear function approximation. C Jin, Z Yang, Z Wang, M I Jordan, Conference on learning theory. PMLR2020</p>
<p>Reinforcement learning in feature space: Matrix bandit, kernels, and regret bound. L Yang, M Wang, International Conference on Machine Learning. PMLR2020</p>
<p>Flambe: Structural complexity and representation learning of low rank mdps. A Agarwal, S Kakade, A Krishnamurthy, W Sun, Advances in neural information processing systems. 202033</p>
<p>Representation learning for online and offline rl in low-rank mdps. M Uehara, X Zhang, W Sun, arXiv:2110.046522021arXiv preprint</p>
<p>Spectral decomposition representation for reinforcement learning. T Ren, T Zhang, L Lee, J E Gonzalez, D Schuurmans, B Dai, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Making linear mdps practical via contrastive representation learning. T Zhang, T Ren, M Yang, J Gonzalez, D Schuurmans, B Dai, International Conference on Machine Learning. PMLR202226466</p>
<p>Latent variable representation for reinforcement learning. T Ren, C Xiao, T Zhang, N Li, Z Wang, D Schuurmans, B Dai, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Data-efficient control policy search using residual dynamics learning. M Saveriano, Y Yin, P Falco, D Lee, 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). Sep. 2017</p>
<p>Residual Reinforcement Learning for Robot Control. T Johannink, S Bahl, A Nair, J Luo, A Kumar, M Loskyll, J A Ojea, E Solowjow, S Levine, 2019 International Conference on Robotics and Automation (ICRA). Montreal, QC, CanadaIEEEMay 2019</p>
<p>Neural Lander: Stable Drone Landing Control Using Learned Dynamics. G Shi, X Shi, M O'connell, R Yu, K Azizzadenesheli, A Anandkumar, Y Yue, S.-J Chung, 2019 International Conference on Robotics and Automation (ICRA). May 2019</p>
<p>Stochastic nonlinear control via finite-dimensional spectral dynamic embedding. T Ren, Z Ren, N Li, B Dai, 2023 62nd IEEE Conference on Decision and Control (CDC). IEEE2023</p>
<p>Domain randomization for transferring deep neural networks from simulation to the real world. J Tobin, R Fong, A Ray, J Schneider, W Zaremba, P Abbeel, 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). Sep. 2017</p>
<p>Closing the Sim-to-Real Loop: Adapting Simulation Randomization with Real World Experience. Y Chebotar, A Handa, V Makoviychuk, M Macklin, J Issac, N Ratliff, D Fox, 2019 International Conference on Robotics and Automation (ICRA). May 2019</p>
<p>Simto-Real Transfer of Robotic Control with Dynamics Randomization. X B Peng, M Andrychowicz, W Zaremba, P Abbeel, 2018 IEEE International Conference on Robotics and Automation (ICRA). May 2018</p>
<p>Learning Neural Network Policies with Guided Policy Search under Unknown Dynamics. S Levine, P Abbeel, Advances in Neural Information Processing Systems. Curran Associates, Inc201427</p>
<p>Neural-Swarm: Decentralized Close-Proximity Multirotor Control Using Learned Interactions. G Shi, W Hönig, Y Yue, S.-J Chung, 2020 IEEE International Conference on Robotics and Automation (ICRA). May 2020</p>
<p>A General Safety Framework for Learning-Based Control in Uncertain Robotic Systems. J F Fisac, A K Akametalu, M N Zeilinger, S Kaynama, J Gillula, C J Tomlin, IEEE Transactions on Automatic Control. 647Jul. 2019</p>
<p>Auto-Tuned Sim-to-Real Transfer. Y Du, O Watkins, T Darrell, P Abbeel, D Pathak, 2021 IEEE International Conference on Robotics and Automation (ICRA). Xi'an, ChinaIEEEMay 2021</p>
<p>DIRL: Domain-Invariant Representation Learning for Sim-to-Real Transfer. A Tanwani, Proceedings of the 2020 Conference on Robot Learning. PMLR. the 2020 Conference on Robot Learning. PMLROct. 2021</p>
<p>Learning modular neural network policies for multi-task and multi-robot transfer. C Devin, A Gupta, T Darrell, P Abbeel, S Levine, 2017 IEEE International Conference on Robotics and Automation (ICRA). May 2017</p>
<p>Multi-robot transfer learning: A dynamical system perspective. M K Helwa, A P Schoellig, 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). Sep. 2017</p>
<p>Provable Benefits of Representational Transfer in Reinforcement Learning. A Agarwal, Y Song, W Sun, K Wang, M Wang, X Zhang, Proceedings of Thirty Sixth Conference on Learning Theory. Thirty Sixth Conference on Learning TheoryPMLRJul. 2023</p>
<p>Human-level control through deep reinforcement learning. V Mnih, K Kavukcuoglu, D Silver, A A Rusu, J Veness, M G Bellemare, A Graves, M Riedmiller, A K Fidjeland, G Ostrovski, nature. 51875402015</p>
<p>Minimum snap trajectory generation and control for quadrotors. D Mellinger, V Kumar, May 2011IEEEShanghai, China</p>
<p>Learning to fly in seconds. J Eschmann, D Albani, G Loianno, 2023</p>
<p>Neural-fly enables rapid learning for agile flight in strong winds. M O'connell, G Shi, X Shi, K Azizzadenesheli, A Anandkumar, Y Yue, S.-J Chung, Science Robotics. 76665972022</p>
<p>Learning to fly-a gym environment with pybullet physics for reinforcement learning of multi-agent quadcopter control. J Panerati, H Zheng, S Zhou, J Xu, A Prorok, A P Schoellig, 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2021</p>
<p>Skill transfer and discovery for sim-to-real learning: A representation-based viewpoint. H Ma, Z Ren, B Dai, N Li, </p>
<p>Crazyswarm: A large nano-quadcopter swarm. J A Preiss, W Honig, G S Sukhatme, N Ayanian, 2017 IEEE International Conference on Robotics and Automation (ICRA). IEEE2017</p>            </div>
        </div>

    </div>
</body>
</html>
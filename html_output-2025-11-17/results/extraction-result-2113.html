<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2113 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2113</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2113</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-54.html">extraction-schema-54</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <p><strong>Paper ID:</strong> paper-281310172</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2509.09915v1.pdf" target="_blank">The (R)evolution of Scientific Workflows in the Agentic AI Era: Towards Autonomous Science</a></p>
                <p><strong>Paper Abstract:</strong> Modern scientific discovery increasingly requires coordinating distributed facilities and heterogeneous resources, forcing researchers to act as manual workflow coordinators rather than scientists. Advances in AI leading to AI agents show exciting new opportunities that can accelerate scientific discovery by providing intelligence as a component in the ecosystem. However, it is unclear how this new capability would materialize and integrate in the real world. To address this, we propose a conceptual framework where workflows evolve along two dimensions which are intelligence (from static to intelligent) and composition (from single to swarm) to chart an evolutionary path from current workflow management systems to fully autonomous, distributed scientific laboratories. With these trajectories in mind, we present an architectural blueprint that can help the community take the next steps towards harnessing the opportunities in autonomous science with the potential for 100x discovery acceleration and transformational scientific workflows.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2113.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2113.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Berkeley A-lab</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Berkeley Autonomous Laboratory (Berkeley A‑lab)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An autonomous laboratory platform reported to run high-throughput experimental cycles for materials discovery, achieving large increases in sample throughput and producing new synthesized materials in short timeframes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>An autonomous laboratory for the accelerated synthesis of novel materials</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Berkeley A‑lab</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>High-throughput autonomous experimental laboratory that integrates robotics and experimental characterization to run synthesis-characterization cycles at rates much higher than human operators; used in materials discovery campaigns.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Materials science / experimental materials synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>experimental</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Validation is reported as real experimental synthesis and characterization: the paper cites Berkeley A‑lab as having processed 50–100× more samples than humans daily and synthesizing 41 novel materials in 17 days. The paper itself does not reproduce experimental protocols or characterization pipelines but cites that real wet‑lab synthesis and characterization were performed in the referenced work.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_sufficiency</strong></td>
                            <td>The paper treats experimental synthesis/characterization as sufficient for domain validation; domain norms in materials science require experimental (wet‑lab / instrument) validation of synthesized materials and characterization data to claim discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_accuracy</strong></td>
                            <td>No quantitative accuracy metrics (e.g., measurement error, agreement to standards) are provided in this paper; only throughput and counts (e.g., 41 novel materials in 17 days, 50–100× sample throughput) are reported from the cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_details</strong></td>
                            <td>The paper reports (by citation) that Berkeley A‑lab performed physical synthesis and characterization at scale (50–100× sample throughput; 41 novel materials in 17 days). This manuscript does not list specific experimental protocols, instrumentation details, or characterization methodologies — those details are delegated to the cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_comparison</strong></td>
                            <td>No direct comparison (e.g., experiment vs simulation or baseline human performance metrics beyond throughput) is provided except throughput claims; the paper uses these examples to illustrate successful experimental validation in autonomous labs.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_failures</strong></td>
                            <td>The paper does not report specific validation failures for Berkeley A‑lab; it does, however, emphasize general risks of irreversible experiments and the need for uncertainty quantification and safeguards in physical autonomous systems.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_cases</strong></td>
                            <td>Cited success: reported synthesis of 41 novel materials in 17 days and significantly higher sample throughput versus humans — called out as an example where experimental validation confirmed discoveries at scale.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_comparison</strong></td>
                            <td>No discussion of comparison to external ground truth standards or benchmarked measurements is provided in this manuscript for the cited experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_replication</strong></td>
                            <td>The paper does not report independent replication of the Berkeley A‑lab results; it recommends establishing robust testbeds and benchmarks for validating progressive levels of autonomy to enable reproducibility.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>The cited example emphasizes accelerated time-to-discovery (e.g., 41 materials in 17 days) and greatly increased throughput (50–100×), but no detailed resource cost accounting (equipment time, reagent cost) is provided in this manuscript.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_norms</strong></td>
                            <td>The paper states that experimental (wet‑lab) validation is the domain norm for materials and experimental sciences; reproducibility, provenance, and instrument-backed measurements are required to claim discoveries.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>The paper highlights that uncertainty quantification is critical for physical autonomous systems but does not provide details on how Berkeley A‑lab quantified uncertainty in the cited results.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Limitations stated generally: irreversible / expensive experiments, precious samples, equipment failure risk; the paper notes these constraints make validation both critical and challenging for autonomous physical systems.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_details</strong></td>
                            <td></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2113.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2113.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Ada (self-driving lab)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ada self-driving laboratory (thin-film optimization)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A self-driving laboratory system used for thin-film materials optimization that integrates automated experiments and closed-loop optimization to improve material properties.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-driving laboratory for accelerated discovery of thin-film materials</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Ada (self-driving laboratory)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Autonomous experimental platform focused on thin-film materials that plans and executes experiments (synthesis/processing/characterization) and applies closed-loop optimization to improve target properties.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Materials science / thin-film optimization / experimental</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>experimental</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>The paper cites Ada as an example of a self-driving lab that performs real experiments (thin-film synthesis and characterization) in closed-loop optimization; however, the current manuscript does not provide procedural details, metrics of characterization fidelity, or protocols.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_sufficiency</strong></td>
                            <td>Treated as experimentally validated in the domain; paper emphasizes that experimental validation is necessary for physical scientific claims and that self-driving labs like Ada provide such validation in their original reports.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_accuracy</strong></td>
                            <td>No numeric accuracy or error/uncertainty figures are reported here; the manuscript only references the published success of Ada in optimizing thin films.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_details</strong></td>
                            <td>The manuscript references the experimental optimization of thin films by Ada but does not give experimental protocols, instruments, or quantitative characterization results — those are in the cited study.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_comparison</strong></td>
                            <td>No direct comparison between simulation and experimental validation is given for Ada in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_failures</strong></td>
                            <td>No specific failures are reported here; the paper discusses general risks of physical experiments under autonomous control (irreversibility, sample scarcity) as open challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_cases</strong></td>
                            <td>Cited as a representative success of self-driving experimental platforms in materials optimization (thin films).</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_comparison</strong></td>
                            <td>No explicit ground-truth benchmarking details provided in this manuscript for Ada.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_replication</strong></td>
                            <td>Not discussed for Ada in this manuscript; the paper advocates for testbeds and benchmarks to enable reproducible validation of autonomous systems.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Paper emphasizes accelerated cycles (qualitatively) with autonomous labs but does not provide Ada-specific cost/time accounting beyond being cited as an example of accelerated experimental loops.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_norms</strong></td>
                            <td>Reiterates that wet‑lab experimental validation is standard; autonomous labs must meet such experimental standards.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>No Ada‑specific uncertainty quantification details in this paper; the manuscript calls out uncertainty quantification as a general necessity.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Paper flags general limitations for physical autonomous systems (e.g., brittleness, equipment heterogeneity) that apply to labs like Ada.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_details</strong></td>
                            <td></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2113.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2113.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChemChow</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChemChow (GPT-4 + chemistry tools integration)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that augments a large language model (GPT‑4) with a suite of chemistry tools to autonomously plan chemical syntheses and associated procedures.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Augmenting large language models with chemistry tools</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ChemChow (GPT‑4 + chemistry tools)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>LLM-augmented planning system that uses GPT‑4 together with ~18 chemistry tools to generate synthesis plans and orchestrate tasks in chemical workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Chemistry / synthesis planning</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>none</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>This manuscript mentions ChemChow as an example of LLM+tool integration but does not describe how results produced by ChemChow are validated (no experimental or simulation validation details provided here).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_sufficiency</strong></td>
                            <td>Not discussed in this manuscript; the paper raises the general requirement that scientific AI must be validated with domain‑appropriate evidence (typically experimental confirmation for chemistry), but does not state whether ChemChow's outputs were experimentally validated.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_accuracy</strong></td>
                            <td>No accuracy or reliability metrics for ChemChow are provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_details</strong></td>
                            <td>No experimental protocol or validation outcomes are given in this manuscript for ChemChow; the paper simply cites it as an example of LLM integration in chemistry.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_comparison</strong></td>
                            <td>None provided in this manuscript.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_failures</strong></td>
                            <td>No failures are reported here; paper generally warns that LLM-based tools may lack causal understanding and require domain‑specific validation.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_cases</strong></td>
                            <td>Not specified in this manuscript.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_comparison</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_replication</strong></td>
                            <td>Not discussed for ChemChow in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Not discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_norms</strong></td>
                            <td>Paper reiterates that chemistry typically requires experimental wet‑lab validation; highlights gap between LLM outputs and domain validation requirements.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>No detail; the paper stresses need for uncertainty quantification generally but does not describe ChemChow's handling.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Manuscript notes that LLMs excel at text correlation but lack causal understanding required for irreversible experiments — a limitation relevant to LLM-based chemistry planning systems like ChemChow.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_details</strong></td>
                            <td></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2113.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2113.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChemOS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChemOS orchestration software</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An orchestration framework cited as a system that democratizes autonomous discovery by coordinating experimental tasks, instruments, and analysis pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ChemOS: An orchestration software to democratize autonomous discovery</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ChemOS</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Orchestration software designed to manage distributed autonomous experimentation campaigns, coordinating robots, instruments, and software components in chemistry experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Chemistry / autonomous experimentation orchestration</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>none</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>This manuscript cites ChemOS as an orchestration example but does not provide details of how ChemOS-initiated experiments are validated or whether ChemOS includes built-in validation procedures.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_sufficiency</strong></td>
                            <td>Not discussed here. The paper argues that orchestration alone is insufficient without domain‑specific validation pipelines and provenance tracking.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_accuracy</strong></td>
                            <td>No accuracy or reliability metrics are provided in this manuscript for ChemOS.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_details</strong></td>
                            <td>The manuscript does not include experimental protocols or validation outcomes specific to ChemOS; it references ChemOS as an existing orchestration tool in the literature.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_comparison</strong></td>
                            <td>No comparisons provided.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_failures</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_cases</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_comparison</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_replication</strong></td>
                            <td>Not discussed for ChemOS within this manuscript.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Not discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_norms</strong></td>
                            <td>Paper emphasizes that orchestration systems must integrate with validation, provenance, and experimental standards to be useful in science.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Paper notes that orchestration frameworks without integrated validation and provenance risk producing non-reproducible or untrusted results.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_details</strong></td>
                            <td></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2113.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2113.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Autonomous materials campaigns</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Autonomous materials discovery campaigns (generic)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>General class of automated/high-throughput campaigns that evaluate large numbers of candidate materials using combinations of computation, experiment, and optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Autonomous materials discovery campaigns (generic)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Campaigns that combine automated synthesis/characterization, simulation, and optimization to evaluate large candidate pools (the paper cites examples evaluating >1e6 candidates and iterative experiment-model loops).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Materials science / high-throughput discovery</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>none</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>The manuscript notes that such campaigns have 'evaluated over one million candidate compounds' but does not provide specifics here about whether those evaluations were validated experimentally, validated via simulation, or both; the paper emphasizes the need for experimental follow-up and provenance.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_sufficiency</strong></td>
                            <td>The paper stresses that domain norms demand experimental validation for material claims and that computational evaluation alone is generally insufficient to establish new materials without experimental confirmation.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_accuracy</strong></td>
                            <td>No numeric accuracy metrics are provided in this manuscript for these campaigns.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_details</strong></td>
                            <td>Not specified in this manuscript — the paper references large-scale evaluation as evidence of scale but explicitly calls for robust validation testbeds and domain-appropriate validation practices.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_comparison</strong></td>
                            <td>No comparisons between modes of validation are given here for these campaigns.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_failures</strong></td>
                            <td>Not reported here; manuscript warns about risks of overreliance on pattern recognition and lack of causal understanding in AI-driven results.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_cases</strong></td>
                            <td>Not specified in detail here; the paper uses aggregate evaluation counts to illustrate scale rather than validated discoveries.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_comparison</strong></td>
                            <td>No ground-truth comparisons provided in this manuscript for those large-scale evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_replication</strong></td>
                            <td>Paper recommends establishing benchmarks, testbeds, and provenance mechanisms to enable reproducibility but does not report replication results for the cited campaigns.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Paper emphasizes scale (evaluating >1e6 candidates) as a potential time-accelerating factor but does not detail resource or cost breakdown.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_norms</strong></td>
                            <td>Paper reiterates experimental follow-up and provenance as domain norms; computational screening typically requires experimental confirmation before claims are accepted.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>The paper calls out the importance of uncertainty quantification for these campaigns but does not describe specific practices used in the cited large-scale evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Main limitations: lack of detailed experimental follow-up in narrative here, potential for false positives from purely computational screening, and practical constraints (samples, equipment).</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_details</strong></td>
                            <td></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2113.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2113.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PROV-AGENT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PROV-AGENT: Unified Provenance for Tracking AI Agent Interactions in Agentic Workflows</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A provenance framework intended to capture and link AI agent actions, decision traces, and data provenance in agentic workflows to support reproducibility, auditing, and validation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>PROV-AGENT: Unified Provenance for TrackingAI Agent Interactions in Agentic Workflows</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>PROV-AGENT</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Provenance schema / framework extended to represent agent interactions, reasoning chains, and emergent swarm behavior to support traceability, accountability, and validation of autonomous workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Workflow provenance / computational reproducibility (cross-domain support)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>none</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>PROV-AGENT is described as a provenance mechanism to record AI reasoning chains, interactions, and decisions; by capturing these artifacts it supports post-hoc validation, auditing, and reproducibility rather than performing validation itself.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_sufficiency</strong></td>
                            <td>The paper argues provenance is necessary but not sufficient — provenance enables reproducibility and auditing which are prerequisites for trusted validation in scientific domains.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_accuracy</strong></td>
                            <td>Not applicable — PROV-AGENT is a provenance/tracking system and does not itself produce accuracy metrics for scientific claims.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_details</strong></td>
                            <td>PROV-AGENT records metadata and decision traces to enable later validation; the manuscript does not claim that PROV-AGENT executes experimental validation.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_comparison</strong></td>
                            <td>The paper positions provenance as complementary to experimental and simulation validation; no empirical comparisons are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_failures</strong></td>
                            <td>No concrete failures of PROV-AGENT are reported; the manuscript notes that provenance alone cannot solve issues like uncertainty quantification or physical validation.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_cases</strong></td>
                            <td>Not provided in this manuscript as an empirical demonstration; PROV-AGENT is cited as an enabling infrastructure for validation and reproducibility.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_comparison</strong></td>
                            <td>Not applicable directly; provenance supports comparisons by preserving data and decision history required to reproduce comparisons externally.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_replication</strong></td>
                            <td>The paper emphasizes that richer provenance (e.g., via PROV-AGENT) is necessary to reproduce agentic workflows but does not document independent replications here.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Overhead of capturing fine-grained provenance is noted in general but no cost/time numbers are supplied for PROV-AGENT in this manuscript.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_norms</strong></td>
                            <td>Paper states that provenance is a core requirement across domains to meet reproducibility and validation norms; agentic workflows require provenance of decisions, learned models, and context.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>PROV-AGENT is framed as complementary to uncertainty quantification; it captures artifacts needed to assess and interpret uncertainty but does not itself quantify uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Limitations: provenance capture does not replace experimental validation; provenance requires standards and tooling to be effective across heterogeneous facilities.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_details</strong></td>
                            <td></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2113.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2113.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Simulation/Testbeds</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Simulation environments and validation testbeds (conceptual)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Simulation-based environments and controlled testbeds proposed to validate agent behaviors and autonomous workflows prior to deployment in live physical experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Simulation environments / validation testbeds</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Software and controlled physical testbeds used to exercise, evaluate, and validate agent policies and workflows in silico or in constrained physical settings before full deployment on costly/irreversible infrastructure.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Cross-domain (general infrastructure for validating autonomous agents in science)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>simulated</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>The manuscript advocates for simulation environments and shared testbeds to validate progressive levels of autonomy: run agent behaviors in simulated scenarios, characterize failure modes, and quantify uncertainty prior to physical deployment; specifics of simulation tools or protocols are not described here.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td>Not specified; the paper notes the need for fidelity appropriate to the domain (ranging from low‑fidelity scenario models to higher‑fidelity physics simulators) and emphasizes that fidelity limitations constrain transferability to real experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_sufficiency</strong></td>
                            <td>Paper argues simulation/testbed validation is necessary but not sufficient for physical sciences; domain norms typically require experimental confirmation after simulation-based validation.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_accuracy</strong></td>
                            <td>No numeric accuracy metrics provided; the manuscript cautions that simulation fidelity and model mismatch can lead to failures when agents transfer to the real world.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_details</strong></td>
                            <td>Paper recommends using simulation/testbeds as preparatory steps before experimental deployment; it does not describe specific experimental validations performed within such testbeds in this manuscript.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_comparison</strong></td>
                            <td>The manuscript contrasts simulated/testbed validation (useful for safety checks and iteration) with experimental validation (required for scientific claims), but does not provide empirical comparisons or quantitative tradeoffs.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_failures</strong></td>
                            <td>Paper warns of simulation-to-reality gaps, model mismatch, and the potential for agents to overfit to simulated environments, leading to failures in real experiments — no case studies with quantified failures are provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_cases</strong></td>
                            <td>No specific success cases with quantitative results are provided in this manuscript for simulation/testbed approaches, though the paper positions testbeds as necessary infrastructure to enable safe deployment.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_comparison</strong></td>
                            <td>Paper suggests testbeds enable ground-truth comparisons for agent behaviors under controlled conditions, but provides no concrete examples or outcomes in this manuscript.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_replication</strong></td>
                            <td>The paper recommends shared testbeds and benchmarks to enable reproducible validation of agentic systems across institutions.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Paper notes that simulation/testbeds reduce risk and resource cost relative to direct physical experiments, but also calls for investments in infrastructure; no specific cost/time figures are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_norms</strong></td>
                            <td>Paper emphasizes that simulation/testbed validation is an accepted step but domain validation norms in experimental sciences ultimately require wet‑lab or instrumented experimental confirmation.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>The manuscript stresses the importance of uncertainty quantification in simulations and for validating agent behaviors, but does not prescribe specific methods.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Limitations highlighted: fidelity gaps, risk of overfitting to simulated scenarios, and inability of simulations alone to prove experimental realizability.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_details</strong></td>
                            <td>The paper recommends a hybrid pipeline: simulation/testbed validation for safety and iteration followed by experimental validation for scientific claims; it advocates establishing testbeds and benchmarks to operationalize such hybrid validation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2113.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2113.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Workflow WMS examples</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>FireWorks / Pegasus / Parsl (workflow management systems)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Established workflow management systems that provide robust orchestration, provenance, and reproducibility for computational pipelines but have limited built‑in learning or autonomous validation features.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>FireWorks, Pegasus, Parsl (representative WMS)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Traditional workflow management systems offering static/pipeline orchestration, conditional execution, fault tolerance, and provenance for HPC/cloud tasks; used as the current baseline for scientific workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computational workflows across domains (HPC, materials, bioinformatics, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>none</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>The paper describes these WMSs as providing reproducible execution and provenance for computational tasks, but they are not described as performing domain scientific validation themselves (they record provenance and orchestrate validated computational workflows).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_sufficiency</strong></td>
                            <td>These systems support reproducibility of computational experiments (a component of validation) but do not substitute for domain‑specific validation (e.g., experimental confirmation of physical hypotheses).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_accuracy</strong></td>
                            <td>Not applicable — these systems provide orchestration and reproducibility, not validation accuracy metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_details</strong></td>
                            <td>WMSs orchestrate experiments but do not intrinsically validate scientific claims; the paper highlights the need to integrate validation frameworks with such WMSs.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_comparison</strong></td>
                            <td>The manuscript contrasts the deterministic reproducibility of traditional WMSs with the adaptive, learning-driven validation needs of autonomous systems.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_failures</strong></td>
                            <td>The paper notes limitations: static DAGs and limited adaptive features hinder responsive validation and near‑real‑time experiment steering.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_cases</strong></td>
                            <td>These WMSs are recognized as successful for reproducible orchestration of computational workloads and data pipelines (baseline for reproducible computing).</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_comparison</strong></td>
                            <td>Not applicable directly.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_replication</strong></td>
                            <td>The paper credits WMSs with enabling reproducible computational pipelines but calls for extended provenance and validation integrations to handle learning-enabled workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Not detailed here; WMSs are framed as mature tooling that reduces manual orchestration overhead.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_norms</strong></td>
                            <td>Paper suggests WMSs meet computational reproducibility norms but must be extended to meet experimental validation norms in agentic workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Traditional WMSs do not typically provide uncertainty quantification; the paper states this is needed for autonomous scientific systems.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Limitations: static execution models, inability to capture learned decision logic, and limited support for validating adaptive/autonomous behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_details</strong></td>
                            <td></td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-driving laboratory for accelerated discovery of thin-film materials <em>(Rating: 2)</em></li>
                <li>An autonomous laboratory for the accelerated synthesis of novel materials <em>(Rating: 2)</em></li>
                <li>ChemOS: An orchestration software to democratize autonomous discovery <em>(Rating: 2)</em></li>
                <li>Augmenting large language models with chemistry tools <em>(Rating: 2)</em></li>
                <li>PROV-AGENT: Unified Provenance for TrackingAI Agent Interactions in Agentic Workflows <em>(Rating: 2)</em></li>
                <li>FireWorks: a dynamic workflow system designed for high-throughput applications <em>(Rating: 1)</em></li>
                <li>The future of self-driving laboratories: from human in the loop interactive AI to gamification <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2113",
    "paper_id": "paper-281310172",
    "extraction_schema_id": "extraction-schema-54",
    "extracted_data": [
        {
            "name_short": "Berkeley A-lab",
            "name_full": "Berkeley Autonomous Laboratory (Berkeley A‑lab)",
            "brief_description": "An autonomous laboratory platform reported to run high-throughput experimental cycles for materials discovery, achieving large increases in sample throughput and producing new synthesized materials in short timeframes.",
            "citation_title": "An autonomous laboratory for the accelerated synthesis of novel materials",
            "mention_or_use": "mention",
            "system_name": "Berkeley A‑lab",
            "system_description": "High-throughput autonomous experimental laboratory that integrates robotics and experimental characterization to run synthesis-characterization cycles at rates much higher than human operators; used in materials discovery campaigns.",
            "scientific_domain": "Materials science / experimental materials synthesis",
            "validation_type": "experimental",
            "validation_description": "Validation is reported as real experimental synthesis and characterization: the paper cites Berkeley A‑lab as having processed 50–100× more samples than humans daily and synthesizing 41 novel materials in 17 days. The paper itself does not reproduce experimental protocols or characterization pipelines but cites that real wet‑lab synthesis and characterization were performed in the referenced work.",
            "simulation_fidelity": null,
            "validation_sufficiency": "The paper treats experimental synthesis/characterization as sufficient for domain validation; domain norms in materials science require experimental (wet‑lab / instrument) validation of synthesized materials and characterization data to claim discovery.",
            "validation_accuracy": "No quantitative accuracy metrics (e.g., measurement error, agreement to standards) are provided in this paper; only throughput and counts (e.g., 41 novel materials in 17 days, 50–100× sample throughput) are reported from the cited work.",
            "experimental_validation_performed": true,
            "experimental_validation_details": "The paper reports (by citation) that Berkeley A‑lab performed physical synthesis and characterization at scale (50–100× sample throughput; 41 novel materials in 17 days). This manuscript does not list specific experimental protocols, instrumentation details, or characterization methodologies — those details are delegated to the cited work.",
            "validation_comparison": "No direct comparison (e.g., experiment vs simulation or baseline human performance metrics beyond throughput) is provided except throughput claims; the paper uses these examples to illustrate successful experimental validation in autonomous labs.",
            "validation_failures": "The paper does not report specific validation failures for Berkeley A‑lab; it does, however, emphasize general risks of irreversible experiments and the need for uncertainty quantification and safeguards in physical autonomous systems.",
            "validation_success_cases": "Cited success: reported synthesis of 41 novel materials in 17 days and significantly higher sample throughput versus humans — called out as an example where experimental validation confirmed discoveries at scale.",
            "ground_truth_comparison": "No discussion of comparison to external ground truth standards or benchmarked measurements is provided in this manuscript for the cited experiments.",
            "reproducibility_replication": "The paper does not report independent replication of the Berkeley A‑lab results; it recommends establishing robust testbeds and benchmarks for validating progressive levels of autonomy to enable reproducibility.",
            "validation_cost_time": "The cited example emphasizes accelerated time-to-discovery (e.g., 41 materials in 17 days) and greatly increased throughput (50–100×), but no detailed resource cost accounting (equipment time, reagent cost) is provided in this manuscript.",
            "domain_validation_norms": "The paper states that experimental (wet‑lab) validation is the domain norm for materials and experimental sciences; reproducibility, provenance, and instrument-backed measurements are required to claim discoveries.",
            "uncertainty_quantification": "The paper highlights that uncertainty quantification is critical for physical autonomous systems but does not provide details on how Berkeley A‑lab quantified uncertainty in the cited results.",
            "validation_limitations": "Limitations stated generally: irreversible / expensive experiments, precious samples, equipment failure risk; the paper notes these constraints make validation both critical and challenging for autonomous physical systems.",
            "hybrid_validation_approach": null,
            "hybrid_validation_details": "",
            "uuid": "e2113.0"
        },
        {
            "name_short": "Ada (self-driving lab)",
            "name_full": "Ada self-driving laboratory (thin-film optimization)",
            "brief_description": "A self-driving laboratory system used for thin-film materials optimization that integrates automated experiments and closed-loop optimization to improve material properties.",
            "citation_title": "Self-driving laboratory for accelerated discovery of thin-film materials",
            "mention_or_use": "mention",
            "system_name": "Ada (self-driving laboratory)",
            "system_description": "Autonomous experimental platform focused on thin-film materials that plans and executes experiments (synthesis/processing/characterization) and applies closed-loop optimization to improve target properties.",
            "scientific_domain": "Materials science / thin-film optimization / experimental",
            "validation_type": "experimental",
            "validation_description": "The paper cites Ada as an example of a self-driving lab that performs real experiments (thin-film synthesis and characterization) in closed-loop optimization; however, the current manuscript does not provide procedural details, metrics of characterization fidelity, or protocols.",
            "simulation_fidelity": null,
            "validation_sufficiency": "Treated as experimentally validated in the domain; paper emphasizes that experimental validation is necessary for physical scientific claims and that self-driving labs like Ada provide such validation in their original reports.",
            "validation_accuracy": "No numeric accuracy or error/uncertainty figures are reported here; the manuscript only references the published success of Ada in optimizing thin films.",
            "experimental_validation_performed": true,
            "experimental_validation_details": "The manuscript references the experimental optimization of thin films by Ada but does not give experimental protocols, instruments, or quantitative characterization results — those are in the cited study.",
            "validation_comparison": "No direct comparison between simulation and experimental validation is given for Ada in this paper.",
            "validation_failures": "No specific failures are reported here; the paper discusses general risks of physical experiments under autonomous control (irreversibility, sample scarcity) as open challenges.",
            "validation_success_cases": "Cited as a representative success of self-driving experimental platforms in materials optimization (thin films).",
            "ground_truth_comparison": "No explicit ground-truth benchmarking details provided in this manuscript for Ada.",
            "reproducibility_replication": "Not discussed for Ada in this manuscript; the paper advocates for testbeds and benchmarks to enable reproducible validation of autonomous systems.",
            "validation_cost_time": "Paper emphasizes accelerated cycles (qualitatively) with autonomous labs but does not provide Ada-specific cost/time accounting beyond being cited as an example of accelerated experimental loops.",
            "domain_validation_norms": "Reiterates that wet‑lab experimental validation is standard; autonomous labs must meet such experimental standards.",
            "uncertainty_quantification": "No Ada‑specific uncertainty quantification details in this paper; the manuscript calls out uncertainty quantification as a general necessity.",
            "validation_limitations": "Paper flags general limitations for physical autonomous systems (e.g., brittleness, equipment heterogeneity) that apply to labs like Ada.",
            "hybrid_validation_approach": null,
            "hybrid_validation_details": "",
            "uuid": "e2113.1"
        },
        {
            "name_short": "ChemChow",
            "name_full": "ChemChow (GPT-4 + chemistry tools integration)",
            "brief_description": "An approach that augments a large language model (GPT‑4) with a suite of chemistry tools to autonomously plan chemical syntheses and associated procedures.",
            "citation_title": "Augmenting large language models with chemistry tools",
            "mention_or_use": "mention",
            "system_name": "ChemChow (GPT‑4 + chemistry tools)",
            "system_description": "LLM-augmented planning system that uses GPT‑4 together with ~18 chemistry tools to generate synthesis plans and orchestrate tasks in chemical workflows.",
            "scientific_domain": "Chemistry / synthesis planning",
            "validation_type": "none",
            "validation_description": "This manuscript mentions ChemChow as an example of LLM+tool integration but does not describe how results produced by ChemChow are validated (no experimental or simulation validation details provided here).",
            "simulation_fidelity": null,
            "validation_sufficiency": "Not discussed in this manuscript; the paper raises the general requirement that scientific AI must be validated with domain‑appropriate evidence (typically experimental confirmation for chemistry), but does not state whether ChemChow's outputs were experimentally validated.",
            "validation_accuracy": "No accuracy or reliability metrics for ChemChow are provided in this paper.",
            "experimental_validation_performed": null,
            "experimental_validation_details": "No experimental protocol or validation outcomes are given in this manuscript for ChemChow; the paper simply cites it as an example of LLM integration in chemistry.",
            "validation_comparison": "None provided in this manuscript.",
            "validation_failures": "No failures are reported here; paper generally warns that LLM-based tools may lack causal understanding and require domain‑specific validation.",
            "validation_success_cases": "Not specified in this manuscript.",
            "ground_truth_comparison": "Not provided here.",
            "reproducibility_replication": "Not discussed for ChemChow in this paper.",
            "validation_cost_time": "Not discussed.",
            "domain_validation_norms": "Paper reiterates that chemistry typically requires experimental wet‑lab validation; highlights gap between LLM outputs and domain validation requirements.",
            "uncertainty_quantification": "No detail; the paper stresses need for uncertainty quantification generally but does not describe ChemChow's handling.",
            "validation_limitations": "Manuscript notes that LLMs excel at text correlation but lack causal understanding required for irreversible experiments — a limitation relevant to LLM-based chemistry planning systems like ChemChow.",
            "hybrid_validation_approach": null,
            "hybrid_validation_details": "",
            "uuid": "e2113.2"
        },
        {
            "name_short": "ChemOS",
            "name_full": "ChemOS orchestration software",
            "brief_description": "An orchestration framework cited as a system that democratizes autonomous discovery by coordinating experimental tasks, instruments, and analysis pipelines.",
            "citation_title": "ChemOS: An orchestration software to democratize autonomous discovery",
            "mention_or_use": "mention",
            "system_name": "ChemOS",
            "system_description": "Orchestration software designed to manage distributed autonomous experimentation campaigns, coordinating robots, instruments, and software components in chemistry experiments.",
            "scientific_domain": "Chemistry / autonomous experimentation orchestration",
            "validation_type": "none",
            "validation_description": "This manuscript cites ChemOS as an orchestration example but does not provide details of how ChemOS-initiated experiments are validated or whether ChemOS includes built-in validation procedures.",
            "simulation_fidelity": null,
            "validation_sufficiency": "Not discussed here. The paper argues that orchestration alone is insufficient without domain‑specific validation pipelines and provenance tracking.",
            "validation_accuracy": "No accuracy or reliability metrics are provided in this manuscript for ChemOS.",
            "experimental_validation_performed": null,
            "experimental_validation_details": "The manuscript does not include experimental protocols or validation outcomes specific to ChemOS; it references ChemOS as an existing orchestration tool in the literature.",
            "validation_comparison": "No comparisons provided.",
            "validation_failures": "Not reported here.",
            "validation_success_cases": "Not specified in this paper.",
            "ground_truth_comparison": "Not provided.",
            "reproducibility_replication": "Not discussed for ChemOS within this manuscript.",
            "validation_cost_time": "Not discussed.",
            "domain_validation_norms": "Paper emphasizes that orchestration systems must integrate with validation, provenance, and experimental standards to be useful in science.",
            "uncertainty_quantification": "Not detailed here.",
            "validation_limitations": "Paper notes that orchestration frameworks without integrated validation and provenance risk producing non-reproducible or untrusted results.",
            "hybrid_validation_approach": null,
            "hybrid_validation_details": "",
            "uuid": "e2113.3"
        },
        {
            "name_short": "Autonomous materials campaigns",
            "name_full": "Autonomous materials discovery campaigns (generic)",
            "brief_description": "General class of automated/high-throughput campaigns that evaluate large numbers of candidate materials using combinations of computation, experiment, and optimization.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Autonomous materials discovery campaigns (generic)",
            "system_description": "Campaigns that combine automated synthesis/characterization, simulation, and optimization to evaluate large candidate pools (the paper cites examples evaluating &gt;1e6 candidates and iterative experiment-model loops).",
            "scientific_domain": "Materials science / high-throughput discovery",
            "validation_type": "none",
            "validation_description": "The manuscript notes that such campaigns have 'evaluated over one million candidate compounds' but does not provide specifics here about whether those evaluations were validated experimentally, validated via simulation, or both; the paper emphasizes the need for experimental follow-up and provenance.",
            "simulation_fidelity": null,
            "validation_sufficiency": "The paper stresses that domain norms demand experimental validation for material claims and that computational evaluation alone is generally insufficient to establish new materials without experimental confirmation.",
            "validation_accuracy": "No numeric accuracy metrics are provided in this manuscript for these campaigns.",
            "experimental_validation_performed": null,
            "experimental_validation_details": "Not specified in this manuscript — the paper references large-scale evaluation as evidence of scale but explicitly calls for robust validation testbeds and domain-appropriate validation practices.",
            "validation_comparison": "No comparisons between modes of validation are given here for these campaigns.",
            "validation_failures": "Not reported here; manuscript warns about risks of overreliance on pattern recognition and lack of causal understanding in AI-driven results.",
            "validation_success_cases": "Not specified in detail here; the paper uses aggregate evaluation counts to illustrate scale rather than validated discoveries.",
            "ground_truth_comparison": "No ground-truth comparisons provided in this manuscript for those large-scale evaluations.",
            "reproducibility_replication": "Paper recommends establishing benchmarks, testbeds, and provenance mechanisms to enable reproducibility but does not report replication results for the cited campaigns.",
            "validation_cost_time": "Paper emphasizes scale (evaluating &gt;1e6 candidates) as a potential time-accelerating factor but does not detail resource or cost breakdown.",
            "domain_validation_norms": "Paper reiterates experimental follow-up and provenance as domain norms; computational screening typically requires experimental confirmation before claims are accepted.",
            "uncertainty_quantification": "The paper calls out the importance of uncertainty quantification for these campaigns but does not describe specific practices used in the cited large-scale evaluations.",
            "validation_limitations": "Main limitations: lack of detailed experimental follow-up in narrative here, potential for false positives from purely computational screening, and practical constraints (samples, equipment).",
            "hybrid_validation_approach": null,
            "hybrid_validation_details": "",
            "uuid": "e2113.4"
        },
        {
            "name_short": "PROV-AGENT",
            "name_full": "PROV-AGENT: Unified Provenance for Tracking AI Agent Interactions in Agentic Workflows",
            "brief_description": "A provenance framework intended to capture and link AI agent actions, decision traces, and data provenance in agentic workflows to support reproducibility, auditing, and validation.",
            "citation_title": "PROV-AGENT: Unified Provenance for TrackingAI Agent Interactions in Agentic Workflows",
            "mention_or_use": "mention",
            "system_name": "PROV-AGENT",
            "system_description": "Provenance schema / framework extended to represent agent interactions, reasoning chains, and emergent swarm behavior to support traceability, accountability, and validation of autonomous workflows.",
            "scientific_domain": "Workflow provenance / computational reproducibility (cross-domain support)",
            "validation_type": "none",
            "validation_description": "PROV-AGENT is described as a provenance mechanism to record AI reasoning chains, interactions, and decisions; by capturing these artifacts it supports post-hoc validation, auditing, and reproducibility rather than performing validation itself.",
            "simulation_fidelity": null,
            "validation_sufficiency": "The paper argues provenance is necessary but not sufficient — provenance enables reproducibility and auditing which are prerequisites for trusted validation in scientific domains.",
            "validation_accuracy": "Not applicable — PROV-AGENT is a provenance/tracking system and does not itself produce accuracy metrics for scientific claims.",
            "experimental_validation_performed": false,
            "experimental_validation_details": "PROV-AGENT records metadata and decision traces to enable later validation; the manuscript does not claim that PROV-AGENT executes experimental validation.",
            "validation_comparison": "The paper positions provenance as complementary to experimental and simulation validation; no empirical comparisons are provided.",
            "validation_failures": "No concrete failures of PROV-AGENT are reported; the manuscript notes that provenance alone cannot solve issues like uncertainty quantification or physical validation.",
            "validation_success_cases": "Not provided in this manuscript as an empirical demonstration; PROV-AGENT is cited as an enabling infrastructure for validation and reproducibility.",
            "ground_truth_comparison": "Not applicable directly; provenance supports comparisons by preserving data and decision history required to reproduce comparisons externally.",
            "reproducibility_replication": "The paper emphasizes that richer provenance (e.g., via PROV-AGENT) is necessary to reproduce agentic workflows but does not document independent replications here.",
            "validation_cost_time": "Overhead of capturing fine-grained provenance is noted in general but no cost/time numbers are supplied for PROV-AGENT in this manuscript.",
            "domain_validation_norms": "Paper states that provenance is a core requirement across domains to meet reproducibility and validation norms; agentic workflows require provenance of decisions, learned models, and context.",
            "uncertainty_quantification": "PROV-AGENT is framed as complementary to uncertainty quantification; it captures artifacts needed to assess and interpret uncertainty but does not itself quantify uncertainty.",
            "validation_limitations": "Limitations: provenance capture does not replace experimental validation; provenance requires standards and tooling to be effective across heterogeneous facilities.",
            "hybrid_validation_approach": null,
            "hybrid_validation_details": "",
            "uuid": "e2113.5"
        },
        {
            "name_short": "Simulation/Testbeds",
            "name_full": "Simulation environments and validation testbeds (conceptual)",
            "brief_description": "Simulation-based environments and controlled testbeds proposed to validate agent behaviors and autonomous workflows prior to deployment in live physical experiments.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Simulation environments / validation testbeds",
            "system_description": "Software and controlled physical testbeds used to exercise, evaluate, and validate agent policies and workflows in silico or in constrained physical settings before full deployment on costly/irreversible infrastructure.",
            "scientific_domain": "Cross-domain (general infrastructure for validating autonomous agents in science)",
            "validation_type": "simulated",
            "validation_description": "The manuscript advocates for simulation environments and shared testbeds to validate progressive levels of autonomy: run agent behaviors in simulated scenarios, characterize failure modes, and quantify uncertainty prior to physical deployment; specifics of simulation tools or protocols are not described here.",
            "simulation_fidelity": "Not specified; the paper notes the need for fidelity appropriate to the domain (ranging from low‑fidelity scenario models to higher‑fidelity physics simulators) and emphasizes that fidelity limitations constrain transferability to real experiments.",
            "validation_sufficiency": "Paper argues simulation/testbed validation is necessary but not sufficient for physical sciences; domain norms typically require experimental confirmation after simulation-based validation.",
            "validation_accuracy": "No numeric accuracy metrics provided; the manuscript cautions that simulation fidelity and model mismatch can lead to failures when agents transfer to the real world.",
            "experimental_validation_performed": false,
            "experimental_validation_details": "Paper recommends using simulation/testbeds as preparatory steps before experimental deployment; it does not describe specific experimental validations performed within such testbeds in this manuscript.",
            "validation_comparison": "The manuscript contrasts simulated/testbed validation (useful for safety checks and iteration) with experimental validation (required for scientific claims), but does not provide empirical comparisons or quantitative tradeoffs.",
            "validation_failures": "Paper warns of simulation-to-reality gaps, model mismatch, and the potential for agents to overfit to simulated environments, leading to failures in real experiments — no case studies with quantified failures are provided here.",
            "validation_success_cases": "No specific success cases with quantitative results are provided in this manuscript for simulation/testbed approaches, though the paper positions testbeds as necessary infrastructure to enable safe deployment.",
            "ground_truth_comparison": "Paper suggests testbeds enable ground-truth comparisons for agent behaviors under controlled conditions, but provides no concrete examples or outcomes in this manuscript.",
            "reproducibility_replication": "The paper recommends shared testbeds and benchmarks to enable reproducible validation of agentic systems across institutions.",
            "validation_cost_time": "Paper notes that simulation/testbeds reduce risk and resource cost relative to direct physical experiments, but also calls for investments in infrastructure; no specific cost/time figures are provided.",
            "domain_validation_norms": "Paper emphasizes that simulation/testbed validation is an accepted step but domain validation norms in experimental sciences ultimately require wet‑lab or instrumented experimental confirmation.",
            "uncertainty_quantification": "The manuscript stresses the importance of uncertainty quantification in simulations and for validating agent behaviors, but does not prescribe specific methods.",
            "validation_limitations": "Limitations highlighted: fidelity gaps, risk of overfitting to simulated scenarios, and inability of simulations alone to prove experimental realizability.",
            "hybrid_validation_approach": true,
            "hybrid_validation_details": "The paper recommends a hybrid pipeline: simulation/testbed validation for safety and iteration followed by experimental validation for scientific claims; it advocates establishing testbeds and benchmarks to operationalize such hybrid validation.",
            "uuid": "e2113.6"
        },
        {
            "name_short": "Workflow WMS examples",
            "name_full": "FireWorks / Pegasus / Parsl (workflow management systems)",
            "brief_description": "Established workflow management systems that provide robust orchestration, provenance, and reproducibility for computational pipelines but have limited built‑in learning or autonomous validation features.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "FireWorks, Pegasus, Parsl (representative WMS)",
            "system_description": "Traditional workflow management systems offering static/pipeline orchestration, conditional execution, fault tolerance, and provenance for HPC/cloud tasks; used as the current baseline for scientific workflows.",
            "scientific_domain": "Computational workflows across domains (HPC, materials, bioinformatics, etc.)",
            "validation_type": "none",
            "validation_description": "The paper describes these WMSs as providing reproducible execution and provenance for computational tasks, but they are not described as performing domain scientific validation themselves (they record provenance and orchestrate validated computational workflows).",
            "simulation_fidelity": null,
            "validation_sufficiency": "These systems support reproducibility of computational experiments (a component of validation) but do not substitute for domain‑specific validation (e.g., experimental confirmation of physical hypotheses).",
            "validation_accuracy": "Not applicable — these systems provide orchestration and reproducibility, not validation accuracy metrics.",
            "experimental_validation_performed": false,
            "experimental_validation_details": "WMSs orchestrate experiments but do not intrinsically validate scientific claims; the paper highlights the need to integrate validation frameworks with such WMSs.",
            "validation_comparison": "The manuscript contrasts the deterministic reproducibility of traditional WMSs with the adaptive, learning-driven validation needs of autonomous systems.",
            "validation_failures": "The paper notes limitations: static DAGs and limited adaptive features hinder responsive validation and near‑real‑time experiment steering.",
            "validation_success_cases": "These WMSs are recognized as successful for reproducible orchestration of computational workloads and data pipelines (baseline for reproducible computing).",
            "ground_truth_comparison": "Not applicable directly.",
            "reproducibility_replication": "The paper credits WMSs with enabling reproducible computational pipelines but calls for extended provenance and validation integrations to handle learning-enabled workflows.",
            "validation_cost_time": "Not detailed here; WMSs are framed as mature tooling that reduces manual orchestration overhead.",
            "domain_validation_norms": "Paper suggests WMSs meet computational reproducibility norms but must be extended to meet experimental validation norms in agentic workflows.",
            "uncertainty_quantification": "Traditional WMSs do not typically provide uncertainty quantification; the paper states this is needed for autonomous scientific systems.",
            "validation_limitations": "Limitations: static execution models, inability to capture learned decision logic, and limited support for validating adaptive/autonomous behaviors.",
            "hybrid_validation_approach": null,
            "hybrid_validation_details": "",
            "uuid": "e2113.7"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-driving laboratory for accelerated discovery of thin-film materials",
            "rating": 2
        },
        {
            "paper_title": "An autonomous laboratory for the accelerated synthesis of novel materials",
            "rating": 2
        },
        {
            "paper_title": "ChemOS: An orchestration software to democratize autonomous discovery",
            "rating": 2
        },
        {
            "paper_title": "Augmenting large language models with chemistry tools",
            "rating": 2
        },
        {
            "paper_title": "PROV-AGENT: Unified Provenance for TrackingAI Agent Interactions in Agentic Workflows",
            "rating": 2
        },
        {
            "paper_title": "FireWorks: a dynamic workflow system designed for high-throughput applications",
            "rating": 1
        },
        {
            "paper_title": "The future of self-driving laboratories: from human in the loop interactive AI to gamification",
            "rating": 1
        }
    ],
    "cost": 0.020722499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>The (R)evolution of Scientific Workflows in the Agentic AI Era: Towards Autonomous Science
12 Sep 2025</p>
<p>Woong Shin 
Daniel Rosendo 
Frédéric Suter 
Feiyi Wang 
Rafael Ferreira Da Silva 
Renan Souza 
Prasanna Balaprakash 
Rafael Ferreira </p>
<p>Oak Ridge National Lab. Oak Ridge
TNUSA Renan</p>
<p>Souza Oak Ridge National Lab. Oak Ridge
TNUSA</p>
<p>Oak Ridge National Lab. Oak Ridge
TNUSA</p>
<p>Oak Ridge National Lab. Oak Ridge
TNUSA</p>
<p>Oak Ridge National Lab. Oak Ridge
TNUSA</p>
<p>Prasanna Balaprakash Oak Ridge National Lab. Oak Ridge
TNUSA</p>
<p>Oak Ridge National Lab. Oak Ridge
TNUSA</p>
<p>The (R)evolution of Scientific Workflows in the Agentic AI Era: Towards Autonomous Science
12 Sep 202585C97E52720075ADBF51E1CBE634274F10.1145/nnnnnnn.nnnnnnnarXiv:2509.09915v1[cs.AI]Agentic AIAgentic WorkflowsAutonomous ScienceScientific AI SystemsSwarm Intelligence
Modern scientific discovery increasingly requires coordinating distributed facilities and heterogeneous resources, forcing researchers to act as manual workflow coordinators rather than scientists.Advances in AI leading to AI agents show exciting new opportunities that can accelerate scientific discovery by providing intelligence as a component in the ecosystem.However, it is unclear how this new capability would materialize and integrate in the real world.To address this, we propose a conceptual framework where workflows evolve along two dimensions which are intelligence (from static to intelligent) and composition (from single to swarm) to chart an evolutionary path from current workflow management systems to fully autonomous, distributed scientific laboratories.With these trajectories in mind, we present an architectural blueprint that can help the community take the next steps towards harnessing the opportunities in autonomous science with the potential for 100x discovery acceleration and transformational scientific workflows.</p>
<p>INTRODUCTION</p>
<p>Modern scientific discovery is undergoing a profound shift as the scale, speed, and complexity of research increase.Addressing urgent challenges in areas like materials design, climate modeling, and health science now demands seamless coordination across many geographically distributed and technologically diverse facilities [4].For instance, a typical materials discovery campaign may span over ten facilities, including synthesis labs, user facilities, and high-performance computing (HPC) centers, and require months of manual coordination [1].This operational overhead limits the pace of progress and forces researchers to act less as scientists and more as orchestrators of workflows.The long-standing vision of fully autonomous science offers a way forward: systems where instruments, robots, computational models, and data pipelines operate continuously and intelligently.By embedding reasoning and adaptation into workflows, these labs have the potential to accelerate discovery by factors of 10 to 100, transforming exploratory science into a continuous, machine-augmented process [15].</p>
<p>Artificial intelligence (AI) has already revolutionized industrial automation, enabling fully autonomous systems in manufacturing, supply chain logistics, and even autonomous vehicles [29].These advances open up exciting new possibilities for advancing scientific research.Recent efforts such as self-driving chemistry labs, foundation model integration, and near real-time simulation steering suggest that AI agents can serve as central coordinators across experimental and computational platforms [1].However, scientific workflows pose challenges that go beyond those found in commercial automation.Scientific decision-making must remain transparent, reproducible, and grounded in physical principles.Workflows must integrate complex instruments and data modalities, and support collaboration among domain scientists, facility operators, and computational teams [16].Commercial solutions, while powerful, are often brittle when applied to the unique constraints of scientific discovery.As a result, new approaches are needed that balance automation with trust, adaptability with control, and scalability with human oversight.</p>
<p>Existing workflow management systems (WMSs) have delivered robust execution frameworks that enable reproducible data pipelines across HPC and cloud platforms [37].Yet these systems are not designed to reason about scientific goals, adapt to new data in near real-time, or coordinate across physically distributed agents and instruments.AI integration into workflows remains mostly ad hoc, with isolated prototypes and custom interfaces that risk fragmenting the ecosystem.Without a systematic approach to combining workflow systems with intelligent agent infrastructure, we risk stagnation and the erosion of years of hard-won community infrastructure.To realize the vision of autonomous science [15], we must evolve these tools into platforms that support intelligent, multiagent orchestration, enable provenance-aware decision-making, and offer robust coordination across laboratory and computing environments [27].The shift towards autonomous discovery should be an evolution rather than a revolution.</p>
<p>In this paper, we focus on identifying an evolutionary path towards enabling the vision of autonomous science building on top of the 20+ years of effort of the workflows community.To achieve this, we first aim to understand the fundamental relationship between existing workflows and emerging agentic AI systems.We identify that both traditional workflows and modern AI agents share the state machine abstraction as a common foundation for autonomy and use this to identify an evolutionary path in between.We propose that workflows can evolve along two dimensions: intelligence (static to intelligent) and composition (single to swarm) forming an evolutionary plane that enables workflow designers, system designers, and policymakers to reason about concrete step differences from traditional workflows to AI-driven autonomous discovery.</p>
<p>To this end, our contribution is threefold:</p>
<p>• Conceptual Framework for Evolution: We present a conceptual framework that unifies traditional workflows and AI agents, revealing evolutionary paths that facilitate concrete roadmaps from traditional workflows to autonomous science.• Architectural Blueprint: We provide an architectural blueprint that envisions autonomous scientific laboratories that materialize this evolution and demonstrate how scientific discovery would evolve with more autonomy.• Roadmap: With the projected future opportunities that will be enabled by fully autonomous science, we identify concrete challenges and the strategic bets required in both the AI community and the scientific workflows community.</p>
<p>The remainder of this paper is organized as follows.Section 2 provides background on scientific workflow complexity and reviews AI advances.Section 3 presents our evolution framework with its two key dimensions.Section 4 presents an architectural vision for systems embodying this evolution.Section 5 identifies challenges and opportunities in realizing autonomous scientific libraries.Section 6 discusses implications, limitations, and future work.Section 7 concludes with our vision and call to action for the community.</p>
<p>BACKGROUND AND MOTIVATION 2.1 Scientific Workflow Systems and Their Limits</p>
<p>data and control dependencies between them.Over the past decades, workflows have become the predominant format for describing complex, multi-step, multi-domain scientific applications.To manage their composition, the planning and orchestration of their execution on distributed computing infrastructures, a large number of WMSs have been proposed [3].However, modern workflows now include conditional branches, cycles, and human-in-the-loop components that require rethinking their design and execution beyond traditional and limited DAG structures [16].Moreover, scientific workflows are evolving from their traditional task-driven nature to embrace data-driven analytical and AI pipelines [38].</p>
<p>Modern workflow management systems have demonstrated strong capabilities in orchestrating large-scale data movement, ensuring fault tolerance, and managing resource provisioning across distributed environments.Established systems [37] provide mature tooling for composing and executing complex scientific pipelines.These systems offer robust mechanisms for tracking task dependencies, handling failures, and optimizing performance across HPC and cloud platforms.Despite their reliability and maturity, they operate under a foundational constraint: workflows are typically represented as static DAGs that must be fully defined before execution.This limits their ability to respond to emerging data, evolving hypotheses, or near real-time system conditions.In the context of our evolutionary framework, these capabilities primarily fall within the Static and Adaptive regions, where the transition logic remains largely predetermined and fixed at design time.While sufficient for many current scientific workflows, these systems do not inherently support learning, optimization, or reasoning required for autonomous scientific operation.</p>
<p>Multi-Facility Coordination Challenges</p>
<p>The need to coordinate across multiple, heterogeneous facilities introduces additional complexity that static or even adaptive workflows struggle to address.Contemporary scientific campaigns increasingly rely on orchestrating activities across a continuum of resources, including HPC centers, experimental instruments, edge services, and storage platforms.Today's prevailing approach involves bespoke, facility-specific workflows stitched together through manual coordination [4].This ad hoc strategy is both brittle and laborintensive, often requiring researchers to oversee synchronization and data handoffs between systems.A representative example is a materials discovery campaign that cycles between synthesis at a user facility, characterization at a beamline, and simulation on an HPC system [1].As the number of facilities, stakeholders, and interdependencies increases, the coordination overhead grows rapidly, consuming valuable time and human effort.These challenges highlight the limitations of current workflow abstractions and underscore the need for systems that can reason about and adapt to multi-facility execution contexts dynamically and autonomously.</p>
<p>A growing challenge in modern scientific workflows is the orchestration of multi-facility deployments that span experimental instruments, edge computing resources, and centralized HPC systems.These workflows increasingly operate across the Edge-Cloud-HPC continuum, reflecting the complex, distributed nature of contemporary scientific discovery campaigns [33,43].While current workflow systems are effective within single-institution contexts, they lack native mechanisms for coordinating across heterogeneous environments with differing interfaces, policies, and operational constraints.This often results in error-prone, manually maintained integration layers that introduce significant overhead and reduce scientific agility.As workflows evolve to support more adaptive and intelligent behaviors, enabling seamless, near real-time coordination across facilities will be essential for realizing autonomous science at scale.</p>
<p>AI Advances and Opportunities</p>
<p>In the industry, the ChatGPT phenomenon ignited unprecedented excitement about the transformative potential of AI, with 400 million weekly active users by 2025 and adoption by 80% of Fortune 500 companies [26,41].This ignition represents a new AI boom beyond traditional machine learning (ML), as emerging autonomous agents that can handle complex tasks independently [19].By 2025, 99% of enterprise developers are exploring AI agents with platforms like AWS AgentCore and Microsoft Azure enabling autonomous task execution [19].Demonstrating AI evolution from assistive to autonomous systems, these efforts leverage large language models (LLMs) for reasoning, neural networks for pattern recognition, and multi-agent coordination for complex tasks.</p>
<p>In science, advances in AI are revealing new opportunities in accelerating scientific discovery.Early efforts integrated ML for parameter prediction and hyperparameter tuning [8,47].Later, autonomous experimentation platforms have emerged.Berkeley A-lab processes 50-100 times more samples than humans daily, synthesizing 41 novel materials in 17 days [39], while self-driving laboratories like Ada optimize thin films [23] and ChemOS orchestrates distributed [30].Recent approaches leverage LLMs.For example, ChemChow integrates GPT-4 with 18 chemistry tools to autonomously plan synthesis [9].Despite progress, these efforts remain point solutions and lack systematic integration.From a scientific workflow perspective, moving from current systems to such autonomous solutions is a disruptive disjoint leap.</p>
<p>Scientific workflows operate under fundamentally different constraints from the industry, prioritizing validation and reproducibility over pure efficiency [1].On top of this, the physical nature of experimental sciences imposes physical constraints such as irreplaceable samples, expensive equipment, and irreversible experiments.Further, multi-stakeholder complexity in a multi-facility scientific environment compounds the challenges.Scientific discovery demands understanding causality beyond pattern recognition, integrating theory with experiment, and maintaining detailed provenance for reproducibility [12].These requirements make it necessary to have a domain-specific approach to AI integration that bridges proven workflow infrastructure with emerging autonomous capabilities.</p>
<p>The Integration Challenge</p>
<p>Despite the exciting advancements in AI, there is a gap between traditional workflow capabilities and materializing the AI potential.The scientific workflow community faces a critical integration challenge [6].Without a systematic approach to bridge these paradigms, the community risks either stagnation or chaotic disruption, missing the opportunity to evolve towards a coherent vision of autonomous scientific laboratories.Traditional WMSs and emerging AI technologies exist in separate worlds, with fragmented ad-hoc one-off integration attempts far from being reproducible [10,12,25].Further, it is unclear how the non-deterministic nature demonstrated by the advent of GenAI would even be useful in scientific endeavors that require high levels of determinism.Due to the ongoing challenges even without AI, neither abandoning proven workflow infrastructure nor ignoring the transformative AI potential is viable.Yet, it is difficult to reason about how much of the infrastructure would be able to adopt AI capabilities and how much additional investment one would need.</p>
<p>To address these issues and pave an evolutionary trajectory, we need a conceptual framework that helps us reason about traditional and AI-based approaches in a unified way.This framework needs to reveal practical migration paths that maintain scientific requirements such as reproducibility, validation, and provenance, while gradually adopting new capabilities.It should guide both technical development and community adoption by revealing incremental but transformative transitions.It should naturally bridge the transition from current WMSs to the ultimate goal of fully autonomous scientific discovery by providing a roadmap that preserves investments while enabling new capabilities.</p>
<p>THE EVOLUTION FRAMEWORK</p>
<p>In this section, we present a systematic approach to understanding how scientific workflows can consider a progression towards AI in a continuum of evolution instead of a revolution.Our key insight is that both workflows and AI systems can be reduced down to an autonomous primitive, an agent, which is anything that can be viewed as perceiving its environment through sensors and acting upon that environment through actuators [31].This primitive can be modeled as state machines as a common denominator, and with varying levels of sophistication in their transition functions and composition patterns.This forms a two-dimensional spectrum of autonomy from static workflows to multi-agent coordination that captures the complexity of fully autonomous systems as a natural progression in intelligence and their composition.</p>
<p>Autonomy: State Machine Abstraction</p>
<p>The execution model of scientific workflows can be expressed as finite state machines  = (, Σ, ,  0 ,  ) (Figure 1-a), where  represents workflow stages, Σ denotes the input alphabet of events and data,  :  × Σ →  defines deterministic transitions,  0 ∈  is the initial state, and  ⊆  represents final states.Figure 1-b demonstrates how a DAG workflow directly maps to this model.Nodes correspond to states that represent computational tasks or data transformations, and edges encode the transition function based on task completion events.Instead of focusing on the DAG, this representation focusing on the execution unit of workflows, the state machine loop, provides a foundation for reasoning about workflow behavior, composition, and their extension.The deterministic nature of  in traditional workflows ensures reproducibility but limits adaptability, which may no longer be suitable for coordinating distributed facilities and heterogeneous resources [1,4].Modern AI agents, despite their apparent complexity, operate on the same state machine principles but with enhanced transition functions.The key distinction is in extending the basic formulation with mechanisms that provide dynamic behavior.Adaptive systems use  :  × Σ ×  →  where  represents observations or feedback signals.Learning systems employ   +1 = (  ,  ) where  is the learning function and  is history, and intelligent systems implement  ′ = Ω(, , ) where Ω is a meta-optimization operator that can redefine the entire state machine ( ′ ) based on context  and mutable goals . Figure 1-c shows how a traditional machine learning (ML)-based system can be modeled in this way.Figure 1-d models a large language model (LLM)-or large reasoning model (LRM)-based AI agent with tools that implement routine sequence tasks with some adaptability.Figure 1-e models a more advanced LLM-or LRM-based AI agent that can learn, reason, plan, and execute tasks given the evolving environment while pursuing optimality.These dynamic entities at the lowest level then can be composed to implement higher-order complex emergent behaviors.Such a dynamic nature of AI-driven autonomy is the opportunity for the future of scientific discovery.</p>
<p>The Intelligence Dimension</p>
<p>To capture the evolution of dynamic behavior necessary, we define the intelligence dimension as the progressive sophistication of the transition function, establishing five evolutionary levels (Table 1).</p>
<p>Each level of sophistication represents a step increase of dynamic capabilities, potentially accumulative.On top of the traditional DAG execution model (Static), the noisy and failure-prone real-world execution environment introduces the need for conditionals to recover and adjust course based on real-world observations (Adaptive).Though, this resulted in an explosion of if-then-else conditions that lead us to learning systems (Learn) where the environment can be learned without explicit programming.Then, we add goal-seeking behavior where the system also dynamically steers itself to an optimal state without explicit programming (Optimizing).Emerging AI capabilities represented by LLM-or LRM-based AI agents bring meta optimization capabilities where the state machine itself can be dynamically rewritten.</p>
<p>This intelligence hierarchy yields distinct operational trade-offs and its progression is driven by scientific requirements rather than  1).We can map this spectrum in existing systems: Traditional HPC workflows (Static), fault-tolerant frameworks with feedback  (Adaptive), ML-guided parameter selection using learning  (Learning), automated tuning platforms minimizing  (Optimizing), and emerging autonomous lab controllers implementing the Ω (Intelligent).</p>
<p>The Composition Dimension</p>
<p>Composition defines how multiple state machines coordinate each other to achieve collective behavior.We identify five patterns with distinct properties (Table 2) according to the progressive sophistication of coordination between entities, potentially in an accumulative fashion.Higher levels of sophistication in composition leverage dynamic coordination to navigate complex dependencies or unknown structures without manually encoding them.Such coordination would start with a single standalone machine  with no inter-workflow communication (Single) that can naturally progress to a sequential unidirectional data flow implemented with a sequential composition  1 •  2 where the output of  1 feeds the input of  2 , enabling staged processing with clear dependencies (Pipeline).For more complex tasks, a manager pattern   ( 1 , . . .,   ) where the manager orchestrates children supports divide-and-conquer strategies with centralized control (Hierarchical).Beyond, complex dependencies would require either a fully connected mesh   ↔   through message passing or a shared state to enable collaborative problem-solving (Mesh).Further, large problems with unknown structures would require emergent behavior via Φ(  ) where simple local rules yield collective intelligence without central coordination (Swarm).</p>
<p>Each pattern exhibits different scaling in terms of communication channels between entities.Pipeline composition  1 • 2 •. . .•  requires  () channels where hierarchical   needs  () channels per level.Mesh with ∀,  :   ↔   demands  ( 2 ) connections for all-to-all connections, but a swarm system using the emergence operator Φ would use only  () local communications, where  is the neighborhood size, to maintain scalability with a much larger number of participants.Existing implementations include batch processing (Single), multi-stage pipelines using the • operator (Pipeline), workflow-of-workflows with   (Hierarchical), collaborative platforms with full ↔ connectivity (Mesh), and particle swarm optimization implementing Φ emergence (Swarm).Each level of sophistication profoundly impacts system design choices.</p>
<p>The Evolution Matrix and Classification</p>
<p>Following the intelligence and composition dimensions described in the previous sections, we create a 5 × 5 evolution matrix that provides us with a comprehensive taxonomy.Each cell represents a class example combining an intelligence level (static , adaptive  +  , learning , optimizing arg min  , intelligent Ω) and a composition pattern (Single, Pipeline •, Hierarchical   , Mesh ↔, Swarm Φ) as depicted in Table 3.This matrix helps in both defining a descriptive classification of systems or a prescriptive planning of trajectories.Current workflow systems cluster at the top-left with traditional DAGs at [Static × Pipeline] and fault-tolerant systems at [Adaptive × Pipeline].The bottom-right frontier represents autonomous science.[Intelligent × Swarm] characterizes laboratories where meta-optimization Ω combines with emergence Φ for collective discovery.</p>
<p>In this matrix, systems evolve by enhancing either intelligence or composition.Common trajectories include HPC workflows advancing from basic  to feedback-enabled  + (Static → Adaptive), ML workflows progressing from isolated learning  to pipelined  1 •  2 architectures with multiple models (Single → Pipeline), and Autonomous labs evolving from adaptive pipelines to optimizing hierarchies using   with arg min  capabilities.Not all systems need to target [Intelligent × Swarm] and should follow scientific needs.However, we foresee demands for higher degrees of autonomous behavior.Critical transitions towards such autonomy include adding learning  (requires data infrastructure), implementing optimization arg min  (needs objective specification), Ant Colony [14] Emergent AI [44] and achieving meta-optimization Ω (demands reasoning engines and knowledge bases).The evolution towards autonomous scientific discovery requires coordinated advancement towards meta-optimization Ω and emergent coordination Φ.The gap between current facilities with basic state transitions  and pipeline composition • and [Intelligent × Swarm] can be interpreted as quite a revolution.However, the framework prescribes an evolutionary systematic progression in enhancing intelligence (e.g., Static  → Adaptive  +  → Learning  → Optimizing arg min  → Intelligent Ω) within existing composition, then expanding coordination (Single → Pipeline • → Hierarchical   → mesh ↔ → Swarm Φ).Each transition enables a new level of autonomy: Learning  brings adaptation, optimization arg min  enables goal-seeking, meta-optimization Ω allows autonomous self-modification, while swarm emergence Φ achieves collective intelligence beyond individual capabilities.</p>
<p>CHALLENGES</p>
<p>In this Section, we explore the challenges in progressing in the journey along the dimensions of intelligence and composition.These challenges map to the AI community (intelligence) and the scientific workflows community (composition), and combined form a unique frontier in enabling AI to perform scientific discovery.A frontier of enabling autonomous scientific discovery.</p>
<p>Challenges in AI for Scientific Discovery</p>
<p>The physical-digital divide represents a fundamental barrier to autonomous science.AI in the physical realm is much more complex.Current generations of AI capabilities (e.g., LLMs, LRMs) excel at text correlation but lack causal understanding necessary for controlling irreversible experiments.This becomes problematic in a high-stakes environment with precious samples or expensive equipment.Validation, simulation, uncertainty quantification become critical.On top of this, heterogeneous vendor integration is challenging.Proprietary interfaces from different manufacturers prevent seamless automation.Remote orchestration across institutional boundaries adds communication latency and security concerns.Further, long-horizon autonomous operations face reliability challenges from a combination of error compounding, equipment failures, and environmental variations that AI must handle without human intervention.</p>
<p>Scientific AI requires multimodal understanding that goes beyond current text-based models.The gap between LLM capabilities and scientific data limits autonomous decision-making.Native understanding of simulations, sensor streams, and experimental observations is crucial.Diverse modalities demand new architectures that understand physical constraints, not just statistical correlations.On top of this, AI must reason about why experiments produce certain results, not merely recognize patterns.While pattern recognition is still useful in many cases, scientific AI must have the capability to operate at the frontier of human knowledge, balancing exploration and validity.Discoveries must be physically realizable, not just statistically probable.This challenge requires AI systems that comprehend both abstract theoretical concepts and concrete experimental constraints.</p>
<p>Zooming out to a global view of the distributed scientific complex with multiple capabilities (e.g., HPC, quantum, experimental facilities, observatories), multi-stakeholder alignment becomes a challenge in creating autonomous systems.Autonomous labs need to find an optimal balance between principal investigators prioritizing their own research, facilities maximizing throughput, and funding agencies demanding social impact.Without clear governance frameworks, AI systems may pursue efficiency over scientific merit or the other way around.Traditional concepts of intellectual property and credit assignment in publications will be challenged due to multi-agent collaborations and significant AI agent contributions.Resource allocation will be challenging with AI systems potentially learning how to game the system.New governance models enabled by relevant technologies must balance stakeholder interests while preserving scientific freedom to explore unexpected directions.</p>
<p>Workflow Challenges</p>
<p>Scientific workflows are increasingly composed of distributed elements that span instruments, edge devices, cloud platforms, and HPC systems across multiple facilities and institutions [4,16].This transition from single-site pipelines to globally coordinated swarms introduces a new level of complexity in workflow composition [7].Each component may operate at different timescales, from microsecond-scale sensor measurements to month-long simulations and analyses.Managing coherence in such settings requires new abstractions for asynchronous coordination, fault tolerance, and adaptive resource utilization.Existing workflow systems, which often assume tightly coupled and statically defined execution environments, are ill-suited for managing emergent behaviors and loosely connected agents that collaborate across administrative boundaries.</p>
<p>As workflows adopt higher levels of intelligence, moving from adaptive systems to those capable of optimization and metareasoning, core assumptions about trust and reproducibility must be reconsidered.Traditional workflows enforce reproducibility through deterministic behavior and tightly controlled inputs.In contrast, intelligent workflows generate results through iterative adaptation, data-driven decisions, and evolving objectives.Reproducing such workflows may involve replicating the conditions and logic of decision processes rather than reproducing identical outputs.This shift calls for enhanced provenance models that can capture feedback mechanisms, learned behaviors, and context-sensitive decisions across agents [35].Provenance models need to evolve to support traceability of agent actions within the workflow context, enabling accountability, transparency, explainability, and auditability [34].Maintaining alignment with FAIR data principles [45,46] becomes more difficult when autonomous agents operate independently, select data dynamically, and pursue objectives that may change over time.</p>
<p>The emergence of agentic workflows in multi-institutional settings presents additional challenges related to interoperability, governance, and data compliance [15].Scientific campaigns increasingly require coordination across systems with differing policies, trust assumptions, and regulatory constraints.Agents acting on behalf of workflows must be capable of negotiating access, managing data across jurisdictions, and adhering to institutional governance models.Without common standards for capability description, data sharing, and execution intent, such workflows risk incompatibility and fragmentation.The governance of collective behavior becomes particularly challenging when workflows operate at the swarm level, where global outcomes emerge from the interaction of many locally autonomous agents.Future workflow infrastructure must embed mechanisms for policy enforcement, ethical guardrails, and transparent auditability to ensure that scientific freedom, compliance, and collaboration can coexist within intelligent and distributed ecosystems.</p>
<p>Cultural, Adoption, and Ethical Challenges</p>
<p>Cultural and adoption barriers pose the greatest threat in realizing autonomous science.There is a struggle in adopting transformation in the scientific community, as researchers trained in traditional methods resist trusting new AI-driven methods or ceding partial control to AI systems.In part, AI methods and AI systems have not earned the trust to fully be employed in scientific discovery, but this also fundamentally requires a mindset shift that challenges established practices and career incentives.With the industry years ahead in AI adoption, the scientific community risks obsolescence if cultural transformation fails.The community needs environments that foster AI collaboration while establishing evidence that autonomous systems can carry out discovery and augment scientists.</p>
<p>Ethical implications of AI-driven scientific discovery demand careful consideration of bias, access, and accountability.Bias in historical data may perpetuate existing biases about which research questions merit investigation.Equitable access to resources becomes critical to prevent creating "AI haves" versus "have-nots" in the scientific community.Further, when AI systems make costly errors by destroying samples or equipment, liability frameworks must clearly assign responsibility.Fundamentally, human agency and creativity in science must be preserved while leveraging AI capabilities, ensuring that automation enhances more than replaces human scientific insight and intuition.Without addressing these factors, technical advances alone cannot deliver the transformational potential of autonomous science.</p>
<p>ARCHITECTURAL EVOLUTION</p>
<p>To realize the evolutionary path outlined in the previous section, we introduce in this section an architectural vision for systems that support autonomous science.Rather than prescribing a single design, we propose a set of architectural patterns that accommodate varying levels of intelligence and composition, aligned with different stages along the evolution framework.The key principle is to enable a gradual and practical transformation of today's workflows into fully autonomous scientific systems in collaboration with AI autonomy.The following subsections describe guiding design principles, present a federated system architecture, and explore concrete application scenarios that illustrate how these concepts may be realized in practice.</p>
<p>Design Principles</p>
<p>A key requirement for architecting autonomous science systems is to support evolutionary transitions without disrupting existing capabilities.To that end, the architecture must accommodate a range of levels of intelligence, from static workflows to fully agentic systems, within a unified framework.This flexibility allows systems to operate with heterogeneous intelligence components while gradually adopting more sophisticated capabilities.Backward compatibility with existing WMSs is essential to preserve the significant investments and practices established over two decades of scientific workflow development.Furthermore, a modular design enables separation of concerns and the incremental adoption of autonomyenabling components without requiring wholesale system rewrites.This evolutionary approach must also respect the foundational needs of scientific computing, including validation, reproducibility, provenance tracking, and auditability, ensuring trust and reliability throughout the transition.</p>
<p>To coordinate activities across diverse facilities while maintaining their operational independence, we propose a federated architecture.This design embraces distributed control, allowing each facility to retain autonomy over local policies, infrastructure, and instrumentation.Cross-facility coordination is enabled through standard protocols that support communication, capability advertisement, and resource discovery.These protocols facilitate dynamic matchmaking between agents, instruments, and services across administrative boundaries.Decoupling integration logic from local implementation details helps promote interoperability and scalability.This federated approach also aligns with practical constraints in national lab environments and global collaborations, where centralized orchestration is neither feasible nor desirable.The result is a flexible architecture that supports evolution toward autonomy while balancing integration needs with the realities of distributed scientific ecosystems.</p>
<p>Architecture</p>
<p>Our architectural vision aims to materialize the evolution framework in Section 3 through systematic extension of the existing workflow infrastructure pursuing federated scientific discovery.Figure 2  Coordination &amp; Communication Layer: Message buses will evolve to support semantic agent negotiation on top of protocols like AMQP 1.0 [36] for federated event-driven workflows.Service discovery extends standards like OGSA (Open Grid Services Architecture [40]) for cross-facility interoperability.WSRF (Web Services Resource Framework [17]) enables stateful interactions that can manage distributed learning states and progress.Security frameworks like Globus Auth [42] can be extended to authenticate inter-agent communication.Scalable consensus protocols for multi-agent decisionmaking and distributed state management are required and should provide audit trails for autonomous actions across federated infrastructures.</p>
<p>illustrates how the current workflow architectures can</p>
<p>Resource &amp; Data Management Layer: Data fabrics leverage data transfer services like Globus Transfer [2] for high-performance movement of multimodal scientific data across facilities.Knowledge graphs represent relationships between hypotheses, experiments, and results, synchronized across sites with eventual consistency.Provenance tracking will extend to capture AI reasoning chains and swarm emergence patterns while creating the relationships with these other concepts in the knowledge graph.Model registries version both AI/ML models and various AI input artifacts such as experimental protocols.Resource allocation implements dynamic service-level agreements for cross-facility negotiation, considering compute availability, sample scarcity, and exploration-exploitation trade-offs.</p>
<p>Infrastructure Abstraction Layer: Heterogeneous resources will be abstracted through unified interfaces via standards like the grid computing standards [40].New abstractions should support AIspecific hardware (TPUs, low-precision GPUs), robotic systems, and quantum devices with both interactive and batch usage models.Container runtimes must adapt for long-running AI services with model persistence.Specialized interfaces are required to manage real-time instrument control, streaming data, asynchronous experiment monitoring, and hybrid classical-quantum workflows.</p>
<p>Deployment</p>
<p>Federation in the scientific environment: Figure 3 illustrates how the layered architecture is deployed across facilities in federation.Architectural components are distributed across and are coordinated while each facility maintains operational autonomy with components based on local specialization.Materials synthesis labs emphasize robotic interfaces, while HPC centers focus on simulation and optimization services.Standard protocols enable advertisement and dynamic service discovery across boundaries.Each facility operates at different evolution levels with each component abstracted as an  execution unit but with different capabilities seamlessly interacting with each other.Deployment patterns of intelligence will range from edge devices providing sub-second inference at instruments to regional AI hubs coordinating thousands of parallel campaigns, providing balance between latency, cost, and capability.</p>
<p>Physical Infrastructure -The AI Hub Extension: AI hubs represent a critical new infrastructure distinct from traditional HPC systems.While HPC emphasizes double-precision floating-point for physical simulations, AI inference requires high-throughput, lowerprecision arithmetic (i.e., FP16/INT8) with massive memory bandwidth, especially with the advent of transformer-based models.AI training and large-scale swarm intelligence coordinating thousands of agents demand high-speed interconnects (&gt;400Gbps interconnect) for real-time consensus within inference clusters, while hundreds of agents must operate efficiently across distributed facilities (&gt;100Gbps networks).The required investment scales from singlerack edge deployments for local inference to multi-megawatt AI supercomputers for AI inference scaling and swarm coordination.</p>
<p>Federated Autonomous Scientific Discovery</p>
<p>Consider materials discovery progressing through the federated infrastructure in Figure 4. Scientists interact via the Science IDE to initiate campaigns and interact with the agents.With specifications about the experimental process, planning agents at the AI Hub generate hypotheses, review literature, and design experiments.These plans will submit tasks to multiple instances of execution agents distributed across facilities coordinating tasks such as synthesis at edge laboratories, characterization at beamlines, simulation on HPC clusters, and analysis in the cloud.Results from each agent will stream through the data fabric and trickle into the knowledge graph where the meta-optimization agent refines strategies.Such a continuous loop will operate autonomously across institutional boundaries with agents coordinating through asynchronous messaging and state synchronization methods while humans monitor progress via dashboards or mobile devices, intervening only when needed.In its ultimate autonomous form, all with no manually defined DAGs in place.Projects like FireWorks [20], Pegasus [13], and Parsl [5] provide robust pipeline orchestration with limited adaptive features through conditional execution.Some systems achieve basic learning through ML integration or optimization via parameter sweeps.Hierarchical composition exists in meta-workflows, but true mesh or swarm coordination remains experimental.Data fabric building blocks such as Globus [2,42] and similar services enable data movement, but lack coordination capabilities for mesh and swarm behaviors.</p>
<p>Achieving the federated autonomous discovery requires systematic infrastructure extensions built on top of existing infrastructure, charting a trajectory of evolution from today's limited automation to tomorrow's autonomous science.At the coordination &amp; communication layer, standards must evolve to support stateful AI services and semantic agent communication.Authentication and transfer services need augmentation with capability negotiation protocols assuming non-human access scenarios.Workflow engines require integration with reasoning systems for meta-optimization (Ω) implementation for dynamic goal and state machine modification.Scalable and standard consensus algorithms must enable swarm coordination (Φ) across hundreds or thousands of agents.For resources &amp; data management, critical additions include persistent model registries for learning systems , objective specification frameworks and validation frameworks for arg min  , knowledge graphs linking experiments to theories.</p>
<p>OPPORTUNITIES</p>
<p>Transformative opportunities for scientific discovery emerge as workflows progress towards higher intelligence and composition.By systematically advancing along both dimensions, the scientific community can unlock capabilities that reshape how we understand and interact with the natural world.</p>
<p>Autonomous Discovery</p>
<p>The shift to autonomous discovery presents a pivotal opportunity to redefine the nature and pace of scientific research [15,27].Selfdriving laboratories operating with minimum human oversight are no longer aspirational.They are already enabling new modes of exploration [1,18].Autonomous materials discovery campaigns have evaluated over one million candidate compounds, demonstrating the ability to scale scientific experimentation far beyond traditional approaches.Workflows that once required extensive manual coordination across multiple facilities are now transitioning into intelligent systems capable of orchestrating activities across institutional and geographic boundaries.This transformation reflects a broader rethinking of workflow models, where execution engines evolve into intelligent agents that reason, adapt, and act based on scientific context.These capabilities allow researchers to move beyond rigid protocols and instead focus on strategic decisionmaking, unlocking the potential to address challenges in climate, health, and energy with greater speed and flexibility.</p>
<p>Autonomous discovery also marks a departure from traditional workflow automation, enabling the formation of complete discovery loops [22].Instead of executing fixed computational tasks, these systems integrate robotic synthesis, near real-time characterization, and modeling within iterative and adaptive cycles.Through metaoptimization, workflows gain the ability to redefine their goals in response to new data and evolving scientific questions.Rather than simply reacting to inputs, they initiate new hypotheses and determine how best to test them.When deployed across multiple facilities, autonomous agents coordinate their actions to explore diverse scientific questions in parallel and dynamically exchange insights [7].This model of distributed and intelligent discovery establishes a foundation for science that is more responsive, collaborative, and scalable, creating a path toward continuous innovation that is both efficient and inclusive.</p>
<p>Accelerated Time to Discovery</p>
<p>With higher degrees of autonomy, we foresee opportunities in significantly reducing human bottlenecks in the experimental cycle, bringing the potential of 100-fold acceleration in scientific discovery [15].Current discovery pipelines stall at points waiting for researchers to analyze data, design next experiments, or coordinate resources that are distributed across facilities.Learning systems () reduce iteration time by recognizing patterns in experimental outcomes, and optimizing (arg min  ) automatically tune parameters to maximize discovery likelihood.For example, in drug discovery, traditional pipelines requiring years of manual iteration could be compressed to weeks when AI agents continuously analyze results, adjust molecular structures, queue synthesis reactions, and perform experiments with robots without human intervention.</p>
<p>In a distributed modern scientific environment, scientific campaigns run across multiple experimental facilities and compute facilities.Intelligence at the edge transforms how we deliver computational power to scientists, representing a shift from centralized HPC to distributed intelligence embedded throughout the research infrastructure.Mesh and swarm composition enable parallel exploration of complex problem spaces, with each node contributing specialized expertise including experimental facilities and observatories interacting with the real world.When combined with faster simulation code generation, experiment design and data analysis significantly compress the entire discovery cycle with humans in the loop.This distributed intelligence model democratizes access to AI capabilities, ensuring every researcher can leverage autonomous systems regardless of their computational resources.</p>
<p>New Scientific Methods and Solutions</p>
<p>Autonomous scientific discovery will bring a paradigm shift from hypothesis-testing science to hypothesis-generating science that expands the frontier of discoverable knowledge.AI-driven hypothesis generation has the potential to transcend human limitation in exploring non-obvious connections.While traditional science is limited by human intuition, intelligent systems with multimodal capabilities can discover patterns potentially invisible to humans.In material science, AI agents can identify counterintuitive combinations of elements that violate conventional wisdom yet exhibit remarkable properties.</p>
<p>Swarm intelligence enables emergent scientific insights through collective discovery at a larger scale.A large population of AI agents can simultaneously explore different areas of complex problems at scale, leveraging the emergent phenomena.With agent collaboration defined by the swarm operator, the emergent collective behavior (Φ) coordinating meta-optimization (Ω) creates scientific exploration that no single agent could achieve.In drug discovery or chemistry, large-scale swarm intelligence explores vast solution spaces uncovering promising combinations at accelerated speed.By systematically exploring regions of parameter spaces that humans would never consider, AI systems can unveil entirely new classes of materials, reactions, and phenomena.</p>
<p>Science as Leadership in AI</p>
<p>The scientific community is uniquely positioned to lead the autonomous science (r)evolution through specialized AI development.Unlike the industry focusing on business and enterprise-focused automation and optimization problems, scientific AI must handle causality, physical constraints, and uncertainty quantification.These requirements position national laboratories, universities, and research institutes as innovation leaders in AI.Integrated facilities, scientific domain expertise, and capital for fundamental research create an ecosystem unmatched by commercial-only entities.The Autonomous Interconnected Science Lab Ecosystem (AISLE) grassroots network [15] represents this leadership by connecting autonomous capabilities across institutions to establish global standards for reproducible, validated AI-driven science.Such movements in autonomous scientific discovery help the community to shape how AI transforms research practice.By developing specialized AI systems that understand scientific principles, we ensure AI capabilities fully augment scientists.</p>
<p>Building the scientific AI ecosystem requires strategic partnerships that leverage industry capabilities while maintaining scientific integrity.Instead of competing against large capital investments of the AI industry, the scientific community positions itself as a validation partner and co-developer for industry AI solutions.Domainspecific foundation models trained and validated on scientific data and methods provide capabilities the industry would not develop, while creating testbeds for safe AI experimentation in physical sciences allows rapid iteration while preventing costly errors.Such partnerships address the current challenges in AI and scientific discovery, forming a unique frontier that can take the capabilities of industry AI solutions to the next level as well as enabling scientific breakthroughs through accelerated science.</p>
<p>STRATEGIC BETS</p>
<p>AI Research.Investment in AI does not mean merely adopting industry solutions.The scientific community must invest strategically in AI capabilities purpose-built for discovery.Critical developments include scientific foundation models that understand causality and physical constraints, swarm intelligence frameworks enabling distributed discovery, and multimodal AI that natively processes simulations, experimental data, and theoretical models.Communitydriven standards through grassroots networks like AISLE ensure interoperability and would take us close to the vision of intelligence distribution, providing AI assistance to every scientist.Such advancements require partnerships with the industry while maintaining scientific rigor co-exploring the unique frontier in AI and science.AI research priorities must focus on handling research uncertainty, reasoning about physical laws, and generating testable hypotheses beyond human intuitions.This research must be based on safe experimentation frameworks for AI in physical sciences and new validation methods for AI-discovered knowledge, enabling human-on-the-loop autonomous systems.Success in this area demands a balance between the rapid adoption of industry advances and the development of science-specific capabilities that industry will not prioritize.</p>
<p>Workflows Research.To realize the full potential of agentic workflows, the workflows research community must commit to foundational shifts that move beyond traditional orchestration models.Future systems should support the complete intelligence and composition spectrum, encompassing static, adaptive, learning, optimizing, and intelligent workflows organized across increasingly collaborative execution patterns.Workflow management systems will need to evolve into modular, intelligence-aware platforms that integrate components operating at different cognitive levels while preserving compatibility with existing DAG-based tools.FAIR-compliant data infrastructure, distributed state management, and advanced provenance systems are essential to support the coordination, adaptability, and traceability of learning-enabled workflows.Communication protocols between agents must be standardized to enable transitions from pipeline-based systems to fully emergent swarms.These protocols should be demonstrated through reference implementations that show how workflows can incrementally evolve through the intelligence and composition matrix.Vendor-agnostic interfaces, hybrid execution support, and community-driven specifications will help ensure these systems can operate across institutional boundaries without sacrificing existing investments.</p>
<p>Infrastructure and workforce investments.Investment in infrastructure and workforce development is equally critical to support this evolution.Infrastructure must include high-bandwidth networking for near real-time coordination across facilities, specialized AI accelerators located near experimental instruments, and edge computing platforms capable of executing intelligent agents close to data sources.Software platforms are needed to enable secure, collaborative AI workflows, simulation environments for validating agent behaviors prior to deployment, and standardized APIs that connect AI reasoning with physical instruments and computational environments.Shared testbeds such as those promoted by the AISLE initiative will allow communities to validate autonomous systems in controlled, reproducible settings.These testbeds serve as essential enablers of cross-institutional collaboration and reduce barriers to adopting automation.A complementary focus on workforce development is needed to create training programs that blend domain expertise with AI fluency, establish new career paths for AI-science practitioners, and foster communities capable of co-evolving with autonomous systems.These investments ensure that the scientific ecosystem is prepared not only to adopt autonomy, but to shape its future.</p>
<p>DISCUSSION AND CONCLUSION</p>
<p>In this paper, we presented a conceptual framework that helps us unify traditional scientific workflows and emerging AI agents through their common foundation, the state machine as their execution model.This enabled us to introduce two evolutionary dimensions which are intelligence (from static to intelligent) and composition (from single to swarm) creating a 5 × 5 matrix.This matrix serves as both a taxonomy for existing systems and a roadmap towards autonomous science.With this framework, we reveal that the path to autonomous discovery is evolutionary rather than revolutionary, enabling systematic progression from current DAG-based workflows to meta-optimizing and swarm-coordinated systems.Our architectural blueprint shows how we can build upon the federated foundations to support this transformation in an evolutionary way.This evolution promises a clearer path to accelerate scientific discovery by factors of 10-100x through intelligent coordination across distributed facilities.</p>
<p>Future work must focus on building the foundational infrastructure, protocols, and abstractions that enable workflows to operate seamlessly across the intelligence-composition spectrum and heterogeneous facility landscapes.Advancing this vision will require co-design of agentic workflow systems alongside AI-driven planning, semantic metadata models, and learning-enabled optimization components that can reason about scientific goals, resources, and uncertainty.Establishing robust testbeds for validating progressive levels of autonomy, as well as defining benchmarks and reference implementations, will be essential for tracking system evolution and ensuring reproducibility.These efforts must also include the development of formal representations for scientific intent, unified interfaces for facility integration, and governance frameworks for safe and collaborative deployment of autonomous agents.Ultimately, realizing this ecosystem will require sustained collaboration across domain scientists, computer scientists, and infrastructure providers to ensure that future workflows are not only more intelligent and adaptable, but also broadly usable, trustworthy, and aligned with evolving scientific practice.</p>
<p>Figure 1 :
1
Figure 1: State machine abstraction as a common denominator across various types of autonomy: the extension of  with learning , optimization arg min  , or meta opt.Ω defines its sophistication</p>
<p>Figure 3 :
3
Figure 3: Deployment of architectural components in a federated environment: The modern distributed scientific environment is extended with additional layers and an AI hub facilities for AI inference specialized compute and storage.</p>
<p>Figure 4 :
4
Figure 4: Example of federated scientific discovery: components in the intelligence layer distributed across the infrastructure iterates through various hypothesis in real time.</p>
<p>Table 1 :
1
The intelligence dimension Seeks optimal behavior via cost function  , balancing exploration and exploitation Intelligent:  ′ = Ω (, ,  )Meta-optimization through operator Ω that can redefine states, transitions, and goals based on context
Incorporates history through learningfunction 𝐿 that updates transitions basedon experience 𝐻
Dimension Description Static:  :  × Σ →  Transition function depends solely on current state and input, implementing predetermined execution paths Adaptive:  :  × Σ ×  →  Extended with observations/feedback signals  enabling runtime adjustments and conditional branching Learning:   +1 =  (  ,  ) Optimizing:  * = arg min   ( ) technological possibility.Verification complexity increases from tractable for static  to undecidable for metaoptimization Ω. Resource requirements scale from  (1) lookups to potentially unbounded computation for intelligent reasoning.Learning systems require a data infrastructure to maintain history  , optimizing systems need an evaluation infrastructure for the cost function  , while intelligent systems demand sophisticated reasoning engines implementing Ω (e.g., LLM-and LRM-agents in Figure</p>
<p>Table 2 :
2
The composition dimension   .( 1 ,  2 , . . .,   )
DimensionDescriptionSingle: 𝑀One isolated machine with no coordina-tionPipeline: 𝑀 1 • 𝑀 2 • . . . • 𝑀 𝑛Sequential composition with unidirec-tional dataflow, enabling staged process-ing with clear dependenciesHierarchical:Manager structure implementing dele-gation and supervision with centralizedcontrol
Mesh: ∀,  :   ↔   Full connectivity enabling peer-to-peer communication and collaborative problem-solving Swarm:  = Φ( { 1 ,  2 , . . .,   } ) Emergent behavior through emergence operator Φ transforming local interactions into global behavior</p>
<p>Table 3 :
3
Representative Examples Across the 5×5 Evolution Matrix: Classification of Workflow Systems by Intelligence Level and Composition Pattern
StaticAdaptive Learning Optimizing IntelligentSingle ScriptExceptionML Model Optimizer LLM-HandlerAgent [24,32]Pipeline DAGConditionalMLAutoMLAgentDAGPipelineChain [11,48]Hierarchical Batch Sys-DynamicEnsembleHyperHierarchicaltemAlloca-Optimiza-Multi-tiontionAgent [28]Mesh FixedLoad Bal-Federated DistributedAgent So-GridancingOptimiza-ciety [49]tionSwarm ParameterAdaptiveParticleSweepSamplingSwarmOpt. [21]
Scientific workflows can be formally defined as structured compositions of computational tasks to achieve a research objective[37]. The most widespread structure is that of a directed acyclic graph (DAG) whose nodes are the computational tasks and edges express
The (R)evolution of Scientific Workflows in the Agentic AI Era: Towards Autonomous Science Conference'17, July 2017, Washington, DC, USA
ACKNOWLEDGMENTThis research used resources of the OLCF at ORNL, which is supported by DOE's Office of Science under Contract No. DE-AC05-00OR22725.da Silva.2025.The (R)evolution of Scientific Workflows in the Agentic AI Era: Towards Autonomous Science.In .ACM, New York, NY, USA, 12 pages.https://doi.org/10.1145/nnnnnnn.nnnnnnn Notice: This manuscript has been authored in part by UT-Battelle, LLC under Contract No. DE-AC05-00OR22725 with the U.S. Department of Energy.The United States Government retains and the publisher, by accepting the article for publication, acknowledges that the United States Government retains a non-exclusive, paid-up, irrevocable, world-wide license to publish or reproduce the published form of this manuscript, or allow others to do so, for United States Government purposes.The Department of Energy will provide public access to these results of federally sponsored research in accordance with the DOE Public Access Plan (http://energy.gov/downloads/doepublic-access-plan).
The rise of self-driving labs in chemical and materials sciences. Milad Abolhasani, Eugenia Kumacheva, Nature Synthesis. 22023. 2023</p>
<p>Software as a service for data scientists. Bryce Allen, John Bresnahan, Lisa Childers, Ian Foster, Gopi Kandaswamy, Raj Kettimuthu, Jack Kordas, Mike Link, Stuart Martin, Karl Pickett, Commun. ACM. 552012. 2012</p>
<p>Peter Amstutz, Maxim Mikheev, Michael R Crusoe, Nebojša Tijanić, Samuel Lampa, Existing Workflow systems. 2024</p>
<p>Enabling discovery data science through crossfacility workflows. Katerina B Antypas, Johannes P Bard, Blaschke, Bjoern Shane Canon, Mallikarjun Arjun Enders, Suhas Shankar, Dale Somnath, Thomas D Stansberry, Sean R Uram, Wilkinson, 2021 IEEE International Conference on Big Data (Big Data). IEEE2021</p>
<p>Parsl: Pervasive Parallel Programming in Python. Yadu Babuji, Anna Woodard, Zhuozhao Li, Daniel S Katz, Ben Clifford, Rohan Kumar, Lukasz Lacinski, Ryan Chard, Justin M Wozniak, Ian Foster, Michael Wilde, Kyle Chard, 10.1145/3307681.3325400Proceedings of the 28th International Symposium on High-Performance Parallel and Distributed Computing (HPDC '19). the 28th International Symposium on High-Performance Parallel and Distributed Computing (HPDC '19)ACM2019</p>
<p>Laure Rosa M Badia, Rafael Berti-Equille, Ulf Ferreira Da Silva, Leser, tegrating HPC, AI, and Workflows for Scientific Data Analysis (Dagstuhl Seminar 23352). 2024. 202413</p>
<p>SWARM: Reimagining scientific workflow management systems in a distributed world. Prasanna Balaprakash, Franck Krishnan Raghavan, Ewa Cappello, Anirban Deelman, Hongwei Mandal, Imtiaz Jin, Komal Mahmud, Shixun Thareja, Pawel Wu, Zuk, The International Journal of High Performance Computing Applications. 109434202513393172025. 2025</p>
<p>DeepHyper: Asynchronous Hyperparameter Search for Deep Neural Networks. Prasanna Balaprakash, Michael Salim, Thomas D Uram, Venkat Vishwanath, Stefan M Wild, 10.1109/HiPC.2018.000142018 IEEE 25th International Conference on High Performance Computing (HiPC). 2018</p>
<p>Augmenting large language models with chemistry tools. Sam Andres M Bran, Oliver Cox, Carlo Schilter, Andrew D Baldassari, Philippe White, Schwaller, Nature Machine Intelligence. 62024. 2024</p>
<p>Wes Brewer, Ana Gainaru, Frédéric Suter, Feiyi Wang, Murali Emani, Shantenu Jha, arXiv:2406.14315[cs.DC]AI-coupled HPC Workflow Applications, Middleware and Performance. 2025</p>
<p>Harrison Chase, LangChain: Building applications with LLMs through composability. 2022</p>
<p>How is artificial intelligence changing science?. Ewa Deelman, 2023 IEEE 19th International Conference on e-Science (e-Science. IEEE2023</p>
<p>The Evolution of the Pegasus Workflow Management Software. Ewa Deelman, Karan Vahi, Mats Rynge, Rajiv Mayani, Rafael Ferreira Da Silva, George Papadimitriou, Miron Livny, 10.1109/MCSE.2019.2919690Computing in Science &amp; Engineering. 212019. 2019</p>
<p>Ant system: optimization by a colony of cooperating agents. Marco Dorigo, Vittorio Maniezzo, Alberto Colorni, IEEE transactions on systems, man, and cybernetics. 1996. 199626</p>
<p>A Grassroots Network and Community Roadmap for Interconnected Autonomous Science Laboratories for Accelerated Discovery. Rafael Ferreira Da Silva, Milad Abolhasani, Laura Dionysios A Antonopoulos, Ryan Biven, Ian T Coffee, Leslie Foster, Shantenu Hamilton, Theresa Jha, Benjamin Mayer, Robert Mintz, Salahudin Moore, Noah Nimer, Woong Paulson, Frédéric Shin, Mitra Suter, Michela Taheri, Newell R Taufer, Washburn, 10.1145/3750720.375729254th International Conference on Parallel Processing Companion (ICPP Companion '25). 2025</p>
<p>Rafael Ferreira Da Silva, Rosa M Badia, Deborah Bard, Ian T Foster, Shantenu Jha, Frédéric Suter, 10.1109/MC.2024.3401542Frontiers in Scientific Workflows: Pervasive Integration with HPC. 2024. 202457</p>
<p>Modeling and managing state in distributed systems: The role of OGSI and WSRF. Ian Foster, Karl Czajkowski, Jeffrey De Ferguson, Steve Frey, Tom Graham, David Maguire, Steven Snelling, Tuecke, Proc. IEEE. IEEE2005. 200593</p>
<p>The future of self-driving laboratories: from human in the loop interactive AI to gamification. Holland Hysmith, Elham Foadian, Sergei V Shakti P Padhy, Rob G Kalinin, Olga S Moore, Mahshid Ovchinnikova, Ahmadi, Digital Discovery. 32024. 2024</p>
<p>IBM. 2025. AI Agents in 2025: Expectations vs. Reality. IBM. </p>
<p>FireWorks: a dynamic workflow system designed for high-throughput applications. Anubhav Jain, Ping Shyue, Wei Ong, Bharat Chen, Xiaohui Medasani, Michael Qu, Miriam Kocher, Guido Brafman, Gian-Marco Petretto, Geoffroy Rignanese, Daniel Hautier, Kristin A Gunter, Persson, 10.1002/cpe.3505Concurrency and Computation: Practice and Experience. 272015. 2015</p>
<p>Particle swarm optimization. James Kennedy, Russell Eberhart, Proceedings of ICNN'95-international conference on neural networks. ICNN'95-international conference on neural networksIEEE19954</p>
<p>Taming the Swarm: A Role-Based Approach for Autonomous Agents. Francesc Lordan, Xavier Casas-Moreno, Philip Cummins, Javier Conejero, Rosa M Badia, Raül Sirvent, European Conference on Parallel Processing. Springer2024</p>
<p>Self-driving laboratory for accelerated discovery of thin-film materials. Benjamin P Macleod, Thomas D Fraser Gl Parlane, Florian Morrissey, Häse, M Loïc, Kevan E Roch, Roberto Dettelbach, Lars Pe Moreira, Michael B Yunker, Joseph R Rooney, Deeth, Science Advances. 688672020. 2020</p>
<p>BabyAGI: An AI-powered task management system. Yohei Nakajima, 2023</p>
<p>Exploring the role of machine learning in scientific workflows: Opportunities and challenges. Azita Nouri, Philip E Davis, Pradeep Subedi, Manish Parashar, arXiv:2110.139992021. 2021arXiv preprint</p>
<p>Introducing ChatGPT Enterprise. 2023OpenAI</p>
<p>Yadu Gregory Pauloski, Ryan Babuji, Mansi Chard, Kyle Sakarvadia, Ian Chard, Foster, arXiv:2505.05428Empowering Scientific Workflows with Federated Agents. 2025. 2025arXiv preprint</p>
<p>QMIX: Monotonic value function factorisation for deep multi-agent reinforcement learning. Tabish Rashid, Mikayel Samvelyan, Christian Schroeder, Gregory Farquhar, Jakob Foerster, Shimon Whiteson, International conference on machine learning. PMLR2018</p>
<p>Robotic process automation and artificial intelligence in industry 4.0-a literature review. Jorge Ribeiro, Rui Lima, Tiago Eckhardt, Sara Paiva, Procedia Computer Science. 1812021. 2021</p>
<p>ChemOS: An orchestration software to democratize autonomous discovery. Florian Loïc M Roch, Christoph Häse, Teresa Kreisbeck, Lars Pe Tamayo-Mendoza, Jason E Yunker, Alán Hein, Aspuru-Guzik, PLoS ONE. 15e02298622020. 2020</p>
<p>Stuart J Russell, Peter Norvig, Pearson, Hoboken, NJ. 1166 pages. Global Edition ISBN. 2021Artificial Intelligence: A Modern Approach</p>
<p>AutoGPT: An autonomous GPT-4 experiment. Significant Gravitas, 2023</p>
<p>Towards cross-facility workflows orchestration through distributed automation. Renan Tyler J Skluzacek, Mark Souza, Frederic Coletti, Rafael Ferreira Da Suter, Silva, Practice and Experience in Advanced Research Computing 2024: Human Powered Computing. 2024</p>
<p>Workflow Provenance in the Computing Continuum for Responsible, Trustworthy, and Energy-Efficient AI. Renan Souza, Silvina Caino-Lores, Mark Coletti, Tyler J Skluzacek, Alexandru Costan, Frédéric Suter, Marta Mattoso, Rafael Ferreira, Da Silva, International Conference on e-Science. 2024e-Science</p>
<p>PROV-AGENT: Unified Provenance for TrackingAI Agent Interactions in Agentic Workflows. Renan Souza, Amal Gueroudji, Stephen Dewitt, Daniel Rosendo, Tirthankar Ghosal, Robert Ross, Prasanna Balaprakash, Rafael Ferreira Da Silva, 5th Workshop on Reproducible Workflows, Data Management, and Security. 2025</p>
<p>Oasis advanced message queuing protocol (amqp) version 1.0. International Journal of Aerospace Engineering Hindawi www. hindawi. 2012. 2018. 2012OASIS Standard</p>
<p>A Terminology for Scientific Workflow Systems. Frédéric Suter, Tainã Coleman, Ílkay Altintas, Rosa M Badia, Bartosz Balis, Kyle Chard, Iacopo Colonnelli, Ewa Deelman, Paolo Di Tommaso, Thomas Fahringer, Carole Goble, Shantenu Jha, Daniel S Katz, Johannes Köster, Ulf Leser, Kshitij Mehta, Hilary Oliver, J.-Luc Peterson, Giovanni Pizzi, Loïc Pottier, Raül Sirvent, Eric Suchyta, Douglas Thain, Sean R Wilkinson, Justin M Wozniak, Rafael Ferreira Da Silva, 10.1016/j.future.2025.107974Future Generation Computer Systems. 1742025. 2025</p>
<p>Driving Next-Generation Workflows from the Data Plane. Frédéric Suter, Rafael Ferreira Da Silva, Ana Gainaru, Scott Klasky, 10.1109/e-Science58273.2023.1025484919th IEEE Conference on eScience. 2023</p>
<p>Ekin Dogus Cubuk, Amil Merchant, et al. 2023. An autonomous laboratory for the accelerated synthesis of novel materials. Nathan J Szymanski, Bernardus Rendy, Yuxing Fei, Rishi E Kumar, Tanjin He, David Milsted, Matthew J Mcdermott, Max Gallant, Nature. 6242023</p>
<p>The open grid services architecture: Where the grid meets the web. Domenico Talia, IEEE Internet Computing. 62002. 2002</p>
<p>ChatGPT: Everything you need to know about the AI chatbot. Techcrunch, 2025</p>
<p>Globus Auth: A research identity and access management platform. Steven Tuecke, Rachana Ananthakrishnan, Kyle Chard, Mattias Lidman, Brendan Mccollam, Stephen Rosen, Ian Foster, 2016 IEEE 12th International Conference on e-Science (e-Science. IEEE2016</p>
<p>Cross-facility workflows: Case studies with active experiments. Nicholas Tyler, Robert Knop, Deborah Bard, Peter Nugent, IEEE/ACM Workshop on Workflows in Support of Large-Scale Science (WORKS). 2022. 2022IEEE</p>
<p>Swarm Learning for decentralized and confidential clinical machine learning. Stefanie Warnat-Herresthal, Hartmut Schultze, Krishnaprasad Lingadahalli Shastry, Sathyanarayanan Manamohan, Saikat Mukherjee, Vishesh Garg, Ravi Sarveswara, Kristian Händler, Peter Pickkers, Ahmad Aziz, Nature. 5942021. 2021</p>
<p>The FAIR Guiding Principles for scientific data management and stewardship. Michel Mark D Wilkinson, Dumontier, Jan Ijsbrand, Gabrielle Aalbersberg, Myles Appleton, ; -Willem Axton, Luiz Boiten, Silva Bonino Da, Philip E Santos, Bourne, Scientific data. 3Jan. 2016. 2016</p>
<p>. Meznah Sean R Wilkinson, Khalid Aloqalaa, Belhajjame, Bruno Michael R Crusoe, Paula De, Luiz Kinoshita, Daniel Gadelha, Ove Garijo, Ragnar Johan, Gustafsson, Scientific Data. 123282025et al. 2025. Applying the FAIR principles to computational workflows</p>
<p>CANDLE/Supervisor: a workflow framework for machine learning applied to cancer research. Justin M Wozniak, Rajeev Jain, Prasanna Balaprakash, Jonathan Ozik, T Nicholson, John Collier, Fangfang Bauer, Thomas Xia, Rick Brettin, Jamaludin Stevens, Mohd-Yusof, BMC Bioinformatics. 192018. 2018</p>
<p>Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation. 2023</p>
<p>Multi-agent reinforcement learning: A selective overview of theories and algorithms. Handbook of reinforcement learning and control. Kaiqing Zhang, Zhuoran Yang, Tamer Başar, 2021. 2021</p>            </div>
        </div>

    </div>
</body>
</html>
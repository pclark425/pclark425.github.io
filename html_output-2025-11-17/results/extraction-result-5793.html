<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5793 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5793</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5793</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-116.html">extraction-schema-116</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <p><strong>Paper ID:</strong> paper-1738f1f82320b41cc88f7b71ee2da633d3463476</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/1738f1f82320b41cc88f7b71ee2da633d3463476" target="_blank">Meta Semantic Template for Evaluation of Large Language Models</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> The initial experiments show that MSTemp-generated samples can significantly reduce the performance of LLMs using existing datasets as seeds, and it is hoped this initial work can shed light on future research on LLMs evaluation.</p>
                <p><strong>Paper Abstract:</strong> Do large language models (LLMs) genuinely understand the semantics of the language, or just memorize the training data? The recent concern on potential data contamination of LLMs has raised awareness of the community to conduct research on LLMs evaluation. In this paper, we propose MSTemp, an approach that creates meta semantic templates to evaluate the semantic understanding ability of LLMs. The core of MSTemp is not to perform evaluation directly on existing benchmark datasets, but to generate new out-of-distribution (OOD) evaluation sets using existing datasets as seeds. Specifically, for a given sentence, MSTemp leverages another language model to generate new samples while preserving its semantics. The new samples are called semantic templates to the original sentence. Then, MSTemp generates evaluation samples via sentence parsing and random word replacement on the semantic templates. MSTemp is highly flexible, dynamic, and cost-effective. Our initial experiments show that MSTemp-generated samples can significantly reduce the performance of LLMs using existing datasets as seeds. We hope this initial work can shed light on future research of LLMs evaluation.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5793.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5793.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MSTEMP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Meta Semantic Template (MSTEMP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An evaluation protocol that generates out-of-distribution, semantically-preserving evaluation samples by using one or more evaluator LLMs to rephrase seed dataset sentences into 'meta semantic templates', then parsing those templates and randomly replacing template slots (optionally adding adversarial perturbations) to create many new test items.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Flan-T5-Large</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>SST-2 (GLUE sentiment classification)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Binary sentiment classification on movie-review sentences (SST-2) drawn from the GLUE benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Original seed sentences are rephrased by an evaluator LLM (e.g., Llama2-13b or ChatGPT) using prompts requesting multiple semantically-equivalent rewrites; the best rewrites are filtered by embedding similarity (semantic-preserving filter). Rewrites are parsed into templates (slot placeholders) and real evaluation samples are generated by random slot replacement; optionally adversarial perturbations (word-level synonym replacements, typos) are introduced to increase difficulty. Prompt design to the evaluator LM (e.g., "Please generate 5 sentences with the same semantic meaning as the following sentence. Try different styles or expressions to make sure they are different.") is noted as important.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Baseline: original SST-2 test format (no MSTEMP rephrasing / template replacement).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>baseline accuracy: 0.939; after MSTEMP (evaluator Llama2-13b): 0.877; after MSTEMP (evaluator ChatGPT): 0.890</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Baseline 0.939 vs MSTEMP-Llama2-13b 0.877 and MSTEMP-ChatGPT 0.890</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Absolute drops: 0.939->0.877 (-0.062, -6.2 percentage points) and 0.939->0.890 (-0.049, -4.9 percentage points). Paper reports average relative reduction of 5.9% when averaged across the two evaluator LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>reduced</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors hypothesize that MSTEMP produces OOD samples (different styles/expressions) that reduce the effect of dataset memorization and expose failures in semantic generalization; LLMs are sensitive to word-level adversarial attacks and to stylistic/format changes. Using multiple evaluator LMs and random replacements increases diversity and reduces chance of contamination/memorization.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Meta Semantic Template for Evaluation of Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5793.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5793.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MSTEMP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Meta Semantic Template (MSTEMP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Same MSTEMP evaluation protocol applied to a different evaluated model (reported separately in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama2-7b</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>SST-2 (GLUE sentiment classification)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Binary sentiment classification on movie-review sentences (SST-2) drawn from the GLUE benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>As above: evaluator LMs (Llama2-13b or ChatGPT) produce semantically-equivalent rewrites; templates are parsed and filled to create OOD evaluation items; optionally adversarial typos and synonym replacements are added.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Baseline: original SST-2 test format.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>baseline accuracy: 0.813; after MSTEMP (evaluator Llama2-13b): 0.717; after MSTEMP (evaluator ChatGPT): 0.709</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Baseline 0.813 vs MSTEMP-Llama2-13b 0.717 and MSTEMP-ChatGPT 0.709</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Absolute drops: 0.813->0.717 (-0.096, -9.6 percentage points) and 0.813->0.709 (-0.104, -10.4 percentage points). Paper reports average relative reduction of 12.3% when averaged across the two evaluator LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>reduced</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Same explanation as above: producing OOD, stylistically varied, and adversarially perturbed templates reveals limitations in recognizing preserved semantics; format/style differences and adversarial substitutions reduce performance.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Meta Semantic Template for Evaluation of Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5793.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5793.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reversal curse</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>The reversal curse: Llms trained on "A is B" fail to learn "B is A"</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reported phenomenon (Berglund et al., 2023) where LLMs trained on relations presented in one surface format ("A is B") fail to generalize to logically equivalent but surface-reversed formats ("B is A"), indicating sensitivity to presentation/format rather than true relational understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The reversal curse: Llms trained on" a is b" fail to learn" b is a"</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Entity relation generalization / format-reversal evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Train or fine-tune LLMs on relation statements in one syntactic/order format and evaluate on the reversed statement format to test generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Format variant: training in "A is B" vs evaluation in the reversed surface form "B is A" (i.e., simple re-ordering of subject/object/attribute).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Direct comparison between performance on same-format evaluation vs reversed-format evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>reduced</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors claim LLMs may memorize surface forms or rely on format-specific patterns and thus fail to learn the underlying causal or symmetric relation; surface presentation (order) strongly influences model behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Meta Semantic Template for Evaluation of Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5793.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5793.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Order-sensitivity (Chen et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Towards robust personalized dialogue generation via order-insensitive representation regularization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reported finding (Chen et al., 2023) that the order of input sentences significantly influences LLM response quality, and fine-tuning across varied orders did not eliminate this sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Towards robust personalized dialogue generation via order-insensitive representation regularization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Dialogue generation / input-order robustness</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Dialogue generation quality evaluated under different orders of input context sentences to assess order-sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Variation in presentation/order of input sentences (different sentence orders in the context prompt).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Comparing model outputs / quality across different orders of the same input content, including models fine-tuned with various sentence orders.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>reduced</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Order of sentences affects learned representations and output quality; existing fine-tuning may not produce order-invariant representations, implying format (order) matters for LLM behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Meta Semantic Template for Evaluation of Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5793.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5793.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CheckList</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CheckList: Behavioral testing of NLP models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automatic test-generation framework that constructs tests by systematically replacing keywords and creating template-based tests to probe model behavior; cited as related work and similar in spirit to MSTEMP's template-based generation but without the semantic-preserving evaluator-LM step.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Beyond accuracy: Behavioral testing of nlp models with checklist</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Template-based behavioral testing across NLP tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate tests by replacing keywords/phrases in sentences to probe model behavior on phenomena such as negation, synonyms, and entity swaps.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Template/keyword-replacement tests (programmatic transformations of inputs); does not rely on an evaluator LM to produce varied semantic rewrites.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>reduced</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>CheckList shows that targeted template and keyword replacement can expose weaknesses arising from input formulation/format; MSTEMP extends this idea by using LMs to produce more natural, varied rewrites and by filtering for semantic preservation.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Meta Semantic Template for Evaluation of Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>The reversal curse: Llms trained on" a is b" fail to learn" b is a" <em>(Rating: 2)</em></li>
                <li>Promptbench: Towards evaluating the robustness of large language models on adversarial prompts. <em>(Rating: 2)</em></li>
                <li>Beyond accuracy: Behavioral testing of nlp models with checklist <em>(Rating: 2)</em></li>
                <li>On the robustness of chatgpt: An adversarial and out-of-distribution perspective. <em>(Rating: 2)</em></li>
                <li>Red teaming language models with language models <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5793",
    "paper_id": "paper-1738f1f82320b41cc88f7b71ee2da633d3463476",
    "extraction_schema_id": "extraction-schema-116",
    "extracted_data": [
        {
            "name_short": "MSTEMP",
            "name_full": "Meta Semantic Template (MSTEMP)",
            "brief_description": "An evaluation protocol that generates out-of-distribution, semantically-preserving evaluation samples by using one or more evaluator LLMs to rephrase seed dataset sentences into 'meta semantic templates', then parsing those templates and randomly replacing template slots (optionally adding adversarial perturbations) to create many new test items.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Flan-T5-Large",
            "model_size": null,
            "task_name": "SST-2 (GLUE sentiment classification)",
            "task_description": "Binary sentiment classification on movie-review sentences (SST-2) drawn from the GLUE benchmark.",
            "problem_format": "Original seed sentences are rephrased by an evaluator LLM (e.g., Llama2-13b or ChatGPT) using prompts requesting multiple semantically-equivalent rewrites; the best rewrites are filtered by embedding similarity (semantic-preserving filter). Rewrites are parsed into templates (slot placeholders) and real evaluation samples are generated by random slot replacement; optionally adversarial perturbations (word-level synonym replacements, typos) are introduced to increase difficulty. Prompt design to the evaluator LM (e.g., \"Please generate 5 sentences with the same semantic meaning as the following sentence. Try different styles or expressions to make sure they are different.\") is noted as important.",
            "comparison_format": "Baseline: original SST-2 test format (no MSTEMP rephrasing / template replacement).",
            "performance": "baseline accuracy: 0.939; after MSTEMP (evaluator Llama2-13b): 0.877; after MSTEMP (evaluator ChatGPT): 0.890",
            "performance_comparison": "Baseline 0.939 vs MSTEMP-Llama2-13b 0.877 and MSTEMP-ChatGPT 0.890",
            "format_effect_size": "Absolute drops: 0.939-&gt;0.877 (-0.062, -6.2 percentage points) and 0.939-&gt;0.890 (-0.049, -4.9 percentage points). Paper reports average relative reduction of 5.9% when averaged across the two evaluator LMs.",
            "format_effect_direction": "reduced",
            "explanation_or_hypothesis": "Authors hypothesize that MSTEMP produces OOD samples (different styles/expressions) that reduce the effect of dataset memorization and expose failures in semantic generalization; LLMs are sensitive to word-level adversarial attacks and to stylistic/format changes. Using multiple evaluator LMs and random replacements increases diversity and reduces chance of contamination/memorization.",
            "counterexample_or_null_result": null,
            "uuid": "e5793.0",
            "source_info": {
                "paper_title": "Meta Semantic Template for Evaluation of Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "MSTEMP",
            "name_full": "Meta Semantic Template (MSTEMP)",
            "brief_description": "Same MSTEMP evaluation protocol applied to a different evaluated model (reported separately in experiments).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama2-7b",
            "model_size": "7B",
            "task_name": "SST-2 (GLUE sentiment classification)",
            "task_description": "Binary sentiment classification on movie-review sentences (SST-2) drawn from the GLUE benchmark.",
            "problem_format": "As above: evaluator LMs (Llama2-13b or ChatGPT) produce semantically-equivalent rewrites; templates are parsed and filled to create OOD evaluation items; optionally adversarial typos and synonym replacements are added.",
            "comparison_format": "Baseline: original SST-2 test format.",
            "performance": "baseline accuracy: 0.813; after MSTEMP (evaluator Llama2-13b): 0.717; after MSTEMP (evaluator ChatGPT): 0.709",
            "performance_comparison": "Baseline 0.813 vs MSTEMP-Llama2-13b 0.717 and MSTEMP-ChatGPT 0.709",
            "format_effect_size": "Absolute drops: 0.813-&gt;0.717 (-0.096, -9.6 percentage points) and 0.813-&gt;0.709 (-0.104, -10.4 percentage points). Paper reports average relative reduction of 12.3% when averaged across the two evaluator LMs.",
            "format_effect_direction": "reduced",
            "explanation_or_hypothesis": "Same explanation as above: producing OOD, stylistically varied, and adversarially perturbed templates reveals limitations in recognizing preserved semantics; format/style differences and adversarial substitutions reduce performance.",
            "counterexample_or_null_result": null,
            "uuid": "e5793.1",
            "source_info": {
                "paper_title": "Meta Semantic Template for Evaluation of Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Reversal curse",
            "name_full": "The reversal curse: Llms trained on \"A is B\" fail to learn \"B is A\"",
            "brief_description": "A reported phenomenon (Berglund et al., 2023) where LLMs trained on relations presented in one surface format (\"A is B\") fail to generalize to logically equivalent but surface-reversed formats (\"B is A\"), indicating sensitivity to presentation/format rather than true relational understanding.",
            "citation_title": "The reversal curse: Llms trained on\" a is b\" fail to learn\" b is a\"",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "task_name": "Entity relation generalization / format-reversal evaluation",
            "task_description": "Train or fine-tune LLMs on relation statements in one syntactic/order format and evaluate on the reversed statement format to test generalization.",
            "problem_format": "Format variant: training in \"A is B\" vs evaluation in the reversed surface form \"B is A\" (i.e., simple re-ordering of subject/object/attribute).",
            "comparison_format": "Direct comparison between performance on same-format evaluation vs reversed-format evaluation.",
            "performance": null,
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "reduced",
            "explanation_or_hypothesis": "Authors claim LLMs may memorize surface forms or rely on format-specific patterns and thus fail to learn the underlying causal or symmetric relation; surface presentation (order) strongly influences model behavior.",
            "counterexample_or_null_result": null,
            "uuid": "e5793.2",
            "source_info": {
                "paper_title": "Meta Semantic Template for Evaluation of Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Order-sensitivity (Chen et al.)",
            "name_full": "Towards robust personalized dialogue generation via order-insensitive representation regularization",
            "brief_description": "A reported finding (Chen et al., 2023) that the order of input sentences significantly influences LLM response quality, and fine-tuning across varied orders did not eliminate this sensitivity.",
            "citation_title": "Towards robust personalized dialogue generation via order-insensitive representation regularization",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "task_name": "Dialogue generation / input-order robustness",
            "task_description": "Dialogue generation quality evaluated under different orders of input context sentences to assess order-sensitivity.",
            "problem_format": "Variation in presentation/order of input sentences (different sentence orders in the context prompt).",
            "comparison_format": "Comparing model outputs / quality across different orders of the same input content, including models fine-tuned with various sentence orders.",
            "performance": null,
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "reduced",
            "explanation_or_hypothesis": "Order of sentences affects learned representations and output quality; existing fine-tuning may not produce order-invariant representations, implying format (order) matters for LLM behavior.",
            "counterexample_or_null_result": null,
            "uuid": "e5793.3",
            "source_info": {
                "paper_title": "Meta Semantic Template for Evaluation of Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "CheckList",
            "name_full": "CheckList: Behavioral testing of NLP models",
            "brief_description": "An automatic test-generation framework that constructs tests by systematically replacing keywords and creating template-based tests to probe model behavior; cited as related work and similar in spirit to MSTEMP's template-based generation but without the semantic-preserving evaluator-LM step.",
            "citation_title": "Beyond accuracy: Behavioral testing of nlp models with checklist",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "task_name": "Template-based behavioral testing across NLP tasks",
            "task_description": "Generate tests by replacing keywords/phrases in sentences to probe model behavior on phenomena such as negation, synonyms, and entity swaps.",
            "problem_format": "Template/keyword-replacement tests (programmatic transformations of inputs); does not rely on an evaluator LM to produce varied semantic rewrites.",
            "comparison_format": null,
            "performance": null,
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "reduced",
            "explanation_or_hypothesis": "CheckList shows that targeted template and keyword replacement can expose weaknesses arising from input formulation/format; MSTEMP extends this idea by using LMs to produce more natural, varied rewrites and by filtering for semantic preservation.",
            "counterexample_or_null_result": null,
            "uuid": "e5793.4",
            "source_info": {
                "paper_title": "Meta Semantic Template for Evaluation of Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "The reversal curse: Llms trained on\" a is b\" fail to learn\" b is a\"",
            "rating": 2
        },
        {
            "paper_title": "Promptbench: Towards evaluating the robustness of large language models on adversarial prompts.",
            "rating": 2
        },
        {
            "paper_title": "Beyond accuracy: Behavioral testing of nlp models with checklist",
            "rating": 2
        },
        {
            "paper_title": "On the robustness of chatgpt: An adversarial and out-of-distribution perspective.",
            "rating": 2
        },
        {
            "paper_title": "Red teaming language models with language models",
            "rating": 2
        }
    ],
    "cost": 0.0108045,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Meta Semantic Template for Evaluation of Large Language Models</h1>
<p>Yachuan Liu ${ }^{1}$; Liang Chen ${ }^{2}$, Jindong Wang ${ }^{3}$, Qiaozhu Mei ${ }^{1}$, Xing Xie ${ }^{3}$<br>${ }^{1}$ University of Michigan ${ }^{2}$ The Chinese University of Hong Kong ${ }^{3}$ Microsoft Research</p>
<p>October 20, 2023</p>
<h4>Abstract</h4>
<p>Do large language models (LLMs) genuinely understand the semantics of the language, or just memorize the training data? The recent concern on potential data contamination of LLMs has raised awareness of the community to conduct research on LLMs evaluation. In this paper, we propose MSTEMP, an approach that creates meta semantic templates to evaluate the semantic understanding ability of LLMs. The core of MSTEMP is not to perform evaluation directly on existing benchmark datasets, but to generate new out-of-distribution (OOD) evaluation sets using existing datasets as seeds. Specifically, for a given sentence, MSTEMP leverages another language model to generate new samples while preserving its semantics. The new samples are called semantic templates to the original sentence. Then, MSTEMP generates evaluation samples via sentence parsing and random word replacement on the semantic templates. MSTEMP is highly flexible, dynamic, and cost-effective. Our initial experiments show that MSTEMP-generated samples can significantly reduce the performance of LLMs using existing datasets as seeds. We hope this initial work can shed light on future research of LLMs evaluation.</p>
<h2>1 Introduction</h2>
<p>Recently, large language models (LLMs) have achieved considerable performance in various applications [Chang et al., 2023, Bubeck et al., 2023]. The so-called "foundation model" [Bommasani et al., 2021] tackles the downstream tasks through in-context learning [Brown et al., 2020] by taking examples from the prompts. However, the great performance of LLMs is being challenged these days, with concerns about potential data contamination since they are typically trained on public or open-source datasets [Zečević et al., 2023, Carlini et al., 2022, Biderman et al., 2023, Berglund et al., 2023]. For example, Berglund et al. [2023] found that LLMs trained in the format of "A is B" failed to generalize to evaluation samples in the form of "B is A", even if they re-trained the model in the format of "B is A". Their main claim is that LLMs might fail to learn the causality of entity relations even present training data in the same format. Similarly, Chen et al. [2023] reported that the order of input sentences significantly influences the quality of LLMs' responses, even when the models have been fine-tuned with various sentence orders. In a nutshell, it requires to design of new evaluation protocols to test the "true intelligence" of LLMs.</p>
<p>This paper aims to tackle the possibly contaminated evaluation of LLMs by creating dynamic and diverse samples. Our main idea is to not rely on existing datasets or manual collection of new data [Ma et al., 2021, Kiela et al., 2021, Thrush et al., 2022] as evaluation sets since they could be easily memorized by LLMs. However, the creation of new samples is not trivial. An ideal sample generation method should possess several characteristics. First, the new samples should be natural and fluent as they are assumed to test the</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: The procedure of MSTEMP. It consists of three parts: meta semantic sentence generation (blue), meta semantic templates generation (orange), and real evaluation samples generation (purple).
language abilities of LLMs. Second, in order to generate infinite samples, existing datasets should be fully leveraged. Third, the generation algorithm should have the ability to generate samples at different difficulty levels since the capability of LLMs is stronger. An ideal state is that we treat existing datasets as "seeds" and then our new evaluation protocols can generate new samples based on these seeds.</p>
<p>In this work, we propose MSTEMP, an evaluation protocol that creates meta semantic templates to generate new testing samples. The core of MSTEMP is to leverage language model $A$ to generate evaluation samples to test language model $B$ via creating meta semantic templates on an existing dataset $D$. Specifically, a sentence $x \in D$ can be rephrased by $A$ to generate new sentences $\left[s_{1}, s_{2}, \cdots, s_{n}\right]$, which we call the meta semantic templates. Then, real evaluation samples are generated based on these templates through a sentence parsing procedure to randomly replace different modules of the templates. Using model $A$ can help to maintain the naturalness of the generated samples and most importantly, preserve the semantics of the original seed sentence. We further introduce a semantic preserving filter to control the semantic difference between the original sample $x$ and the generated template $s$. By design, MSTEMP is able to reduce the possibility of data contamination due to many choices of the evaluator language model $A$ and the seed dataset $D$ (imagine the efforts of generating training samples by replacing many $A \mathrm{~s}$ and $D \mathrm{~s}$ if someone wants to cheat). Figure 1 illustrates the main procedure of MSTEMP.</p>
<p>The MSTEMP framework remains general and flexible to many language tasks. For any given seed dataset, MSTEMP can generate new evaluation samples by controlling the number of templates ( $n$ in the figure) and the real generated samples $(m)$. Moreover, it can naturally control the difficulty of generated samples via the adversarial attack module. Zhu et al. [2023] and Wang et al. [2023] showed that LLMs are sensitive to adversarial attacks, among which the word-level attacks, i.e., replacing words in a sentence with their synonyms, remains the most successful threat. Other attacks such as typos exhibit less success rate, compared to word-level attacks. Therefore, the adversarial attack module in MSTEMP can easily control the complexity of the generated samples by adding different types of adversarial attacks. The adversarial texts are marked with red in Figure 1.</p>
<p>From a broader perspective, MSTEMP actually attempts to generate out-of-distribution (OOD) samples for evaluation LLMs, which remains a major threat to current LLMs [Yang et al., 2022, Arora et al., 2021]. While the predominant proprietary and some open-sourced LLMs are trained on unknown sources of data, it becomes impossible to define what are proper OOD samples for them. Therefore, MSTEMP can be seen as a</p>
<p>manageable way to perform OOD evaluation of large language models in a straightforward manner.
This is a work-in-process paper and we are planning to conduct extensive experiments to verify the effectiveness of MSTEMP in the future. Our initial experiments on sentiment analysis from GLUE [Wang et al., 2018] show that MSTEMP can successfully reduce the benchmark performance of several LLMs, even without introducing the adversarial attack module. More experimental results and analysis might come in the future and this paper shall be updated significantly.</p>
<h1>2 Related Work</h1>
<p>Large language models achieve unprecedented performance across many tasks such as reasoning [Collins et al., 2023], natural language processing [Parrish et al., 2021], and natural science applications [Guo et al., 2023]. There are several existing benchmarks to evaluate the performance of LLMs such as AlpacaEval [Li et al., 2023b], OpenLLM leaderboard [HuggingFace, 2023], Big-Bench [Srivastava et al., 2022], and API-bank [Li et al., 2023a]. For a thorough overview of LLMs evaluation, please refer to the survey paper [Chang et al., 2023].</p>
<p>Of all the evaluation efforts to LLMs, there are two main streams of work that share similar interests to ours. One of them is the "Dyna-X" series, including DynaBoard [Ma et al., 2021], DynaTask [Thrush et al., 2022], and DynaBench [Kiela et al., 2021]. The key of these work is to leveraging the wisdom from the crowd for challenging evaluation sets design, i.e., the main efforts are not on the algorithm side, but in the crowd-sourcing system and interface, where the name "dynamic" comes from. Our work is significantly different from theirs since we do not rely on crowd-sourcing for evaluation, but to generate OOD samples using our algorithm.</p>
<p>The other type of work is CheckList [Ribeiro et al., 2020] which automatically generates test samples for NLP datasets by replacing keywords in the given sentences. Our work is similar to CheckList for the sentence generation part, but MSTEMP considers the semantic perseverance by using another language model acting as the evaluator LM. On the other hand, the usage of another LM can help generate OOD samples (e.g., different styles or expressions with the same meaning), which CheckList might not have. Therefore, our work can be seen as a more challenging version of CheckList in terms of measuring the semantic understanding ability of LLMs.</p>
<p>Finally, red-teaming [Perez et al., 2022] also uses templates for detecting toxic and offensive responses in language models. However, the templates in the Checklist and red-teaming include those that necessitate human manual effort for their creation, and the way of filling out a template is restricted to the task one template is used for. MSTEMP, on the other hand, can generate the templates automatically hence no human effort is needed.</p>
<h2>3 Methodology</h2>
<p>In this section, we introduce our meta semantic template (MSTEMP) method for evaluating large language models. As shown in Figure 1, the core of MSTEMP is to generate semantically-preserved evaluation samples using different LLMs. In the following, we will introduce its key components: meta semantic templates generation (Sec. 3.1) and real evaluation samples generation (Sec. 3.2). Then, we discuss the advantages and disadvantages of MSTEMP in Sec. 3.3.</p>
<h3>3.1 Meta Semantic Templates Generation</h3>
<p>The process of meta semantic template generation includes both the blue and orange modules in Figure 1. Formally speaking, to evaluate an $\operatorname{LLM} \mathcal{A}$, we need to have a seed dataset $\mathcal{D}=\left{\left(x_{i}, y_{i}\right)\right}_{i \in[N]}$, where $x$ is the</p>
<div class="codehilite"><pre><span></span><code>Algorithm 1 MSTEMP: Meta semantic template
    Input: seed dataset \(\mathcal{D}\), an evaluator LM \(\mathcal{B}\), semantic filter LM \(\mathcal{C}\), threshold \(\tau\)
    while not end do
        Sample an example \(\left(x_{i}, y_{i}\right)\) in \(\mathcal{D}\)
        Generate \(n\) meta semantic sentences \(\mathcal{S}\) using \(\mathcal{A}\) and \(\mathcal{B}\) (satisfying \(\tau \leq \cos \left(\mathcal{B}\left(x_{i}\right), \mathcal{B}\left(s_{j}\right)\right)\)
        Perform sentence parsing on \(\mathcal{S}\) to get the meta semantic template \(\mathcal{T}\)
        Get real evaluation samples \(\mathcal{D}^{\prime}\) by replacing words in meta semantic templates \(\mathcal{T}\)
    end while
    return generated samples \(\mathcal{D}^{\prime}\)
</code></pre></div>

<p>input and $y$ is the output (ground truth label). The generation of meta semantic template requires leveraging another language model, which we call evaluator $L M, \mathcal{B}$, to generate $m$ semantics-preserving samples. For an input $x_{i}$, e.g., "I am happy today" in Figure 1, we generate $n$ samples, denoted as $\mathcal{S}<em 1="1">{i}=\left{s</em>\right}$, which are referred to as meta semantic samples, e.g., "Today brings me immense joy" and "My heart is filled with happiness on this day".}, s_{2}, \ldots, s_{n</p>
<p>Notably, this step is non-trivial since we need to preserve the semantics of the generated sentences. It requires efforts from two aspects. First, we leverage an LLM $\mathcal{C}$ as the semantic preserving filter to measure the similarity between the original and the generated sentences. For instance, we choose $\mathcal{C}$ to be a BERT [Devlin et al., 2018]-a powered filter that generates embedding for the original and generated sentences (denoted as $z$ and $z^{\prime}$, respectively). Then, our filter computes their similarity score $c=\cos \left(z, z^{\prime}\right)$ and rank the scores for all generated embedding $z^{\prime}$. There is a threshold $\tau$ to control how much of the generated sentences we want to preserve. Second, the design of the prompts to $\mathcal{B}$ is non-trivial. In order to generate sentences as different as possible, the prompts should be well-designed. We have attempted different prompts like "Please generate 5 sentences with the same semantic meaning as the following sentence. Try different styles or expressions to make sure they are different." This helps to generate different styles of sentences that suffice our needs. However, the design of prompts is an open question that we believe should be improved in the future.</p>
<p>Then, for each $s_{i}$, we perform sentence parsing to generate the final meta semantic template $\mathcal{T}$. This step is not related to any LLM since sentence parsing is relatively mature in the area of NLP. We leverage existing libraries like NLTK [Bird, 2006] to perform this operation. ${ }^{1}$ This step generates meta semantic templates such as " [x] brings [x] immense joy" where "[x]" is replaceable to generate more real evaluation examples.</p>
<h1>3.2 Real Evaluation Samples Generation</h1>
<p>This step is similar to CheckList [Ribeiro et al., 2020] which keeps generating different samples by randomly replacing words in the templates. We denote the generated samples as $\mathcal{D}^{\prime}$. For each template $s_{i}$, we generate $m$ samples such as "Bob brings Alice immense joy", where "Bob" and "Alice" are the new filled entities. The replacement is conducted in a random manner to reduce the possibility of generating fixed samples. The original ground truth labels (e.g., "sentiment: positive" in Figure 1) are taken as the labels for the generated samples $\mathcal{D}^{\prime}$ thanks to the semantics preserving nature of the method.</p>
<p>Furthermore, the replacement can be made more challenging by introducing the adversarial attack module. As shown in Figure 1, the words "brigns" and "wit" are both typos generated by the adversarial attack module to fool the LLM. And "joyful" is the outcome of the word-level attack generated as synonyms. With the help of existing LLMs adversarial attack benchmarks [Zhu et al., 2023], the generated samples</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>can be more challenging to handle. This makes MSTEMP more flexible. The complete algorithm flow for MSTEMP is shown in Algo. 1.</p>
<h1>3.3 Discussion</h1>
<p>MSTEMP makes it possible to partially reduce the possibility of data contamination. In order to evaluate one LLM, we leverage $M$ extra evaluator LMs to generate different semantic templates for it. Since different evaluator LMs could have different diversities and focus in a generation, we believe it could dramatically reduce the data contamination since a cheater might need huge efforts to collect these evaluator LMs and then generate samples.</p>
<p>On the other hand, the generation introduces randomness: the filter, the selected templates, and the replacement of templates all involve randomness. In fact, MSTEMP makes it possible to generate different evaluation samples in its running every time. This further makes it even more difficult to memorize the entire training data.</p>
<p>However, MSTEMP has some limitations. First, there is some fairness issue in comparing the performance with existing benchmarks. Imagine this: we leverage an $N$-size seed dataset $\mathcal{D}$ to generate $n$ templates for each input, and then we generate $m$ samples for each template. This means we generate $n m$ real evaluation samples for each input in $\mathcal{D}$ : we will have $n m N$ evaluation samples in total by leveraging the full $\mathcal{D}$. This makes it seem unfair to compare with the original performance in $\mathcal{D}$ since we literally have many more evaluation samples. A potential solution to this is sampling: we sample $N$ examples from those $n m N$ generated samples. We perform such sampling operations several times and use the average as the performance.</p>
<p>Another disadvantage of MSTEMP would be the guarantee of the naturalness and grammarly-corrected samples, which we do not control. Our current control is through an extra LM and the filter to ensure that they are similar in embedding space, but this may not guarantee the similarity in the input level. Additionally, the replacement of the words may introduce some grammar errors. This limitation needs further solutions to handle.</p>
<h2>4 Experiments</h2>
<p>In this section, we conduct experiments to validate the effectiveness of MSTEMP. We adopt SST-2 from GLUE [Wang et al., 2019] as the initial experiment. We evaluate Flan-t5-Large [Wei et al., 2022] and Llama2-7b [Touvron et al., 2023] using MSTEMP. For each LLM to evaluate, we choose two evaluator LMs: ChatGPT [OpenAI, 2023] and Llama2-13b, to generate meta semantic templates.</p>
<p>Table 1 shows our initial results. It can be observed that after our evaluation, the performance on SST-2 is reduced. Specifically, for Flan-T5-Large, the accuracy drops from 0.939 to 0.877 and 0.890 using Llama213b and ChatGPT as evaluator LLMs, respectively. For Llama2-7b, the accuracy drops from 0.813 to 0.717 and 0.709 , respectively. The performance reductions for Flan-T5-Large and Llama2-7b are $\mathbf{5 . 9} \%$ and $\mathbf{1 2 . 3} \%$, respectively, if averaged on Llama2-13b and ChatGPT. ${ }^{2}$ The results indicate that current LLMs do have some limitations in correctly recognizing OOD samples.</p>
<h2>5 Conclusion</h2>
<p>This paper proposed MSTEMP, an evaluation approach to LLMs by generating semantically-preserving samples based on the given seed datasets. MSTEMP has the potential to reduce the possibility of data</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 1: Initial evaluation results using MSTEMP.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Evaluated LLM</th>
<th style="text-align: center;">Baseline</th>
<th style="text-align: center;">Evaluator LLM in MSTEMP</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Llama2-13b</td>
<td style="text-align: center;">ChatGPT</td>
</tr>
<tr>
<td style="text-align: center;">Flan-T5-Large</td>
<td style="text-align: center;">0.939</td>
<td style="text-align: center;">0.877</td>
<td style="text-align: center;">0.890</td>
</tr>
<tr>
<td style="text-align: center;">Llama2-7b</td>
<td style="text-align: center;">0.813</td>
<td style="text-align: center;">0.717</td>
<td style="text-align: center;">0.709</td>
</tr>
<tr>
<td style="text-align: center;">#Examples</td>
<td style="text-align: center;">872</td>
<td style="text-align: center;">$4081(4.68 \times)$</td>
<td style="text-align: center;">$4047(4.64 \times)$</td>
</tr>
</tbody>
</table>
<p>contamination by involving OOD sample generation using extra evaluator LMs and replacement of words for templates. We hope that this initial work can share some latest findings of our research towards LLMs evaluation and inspire new approaches in the future.</p>
<h1>Disclaimer</h1>
<p>The generation mechanism of this work does not involve any irresponsible language. The only purpose of conducting this research is to present a new evaluation protocol, but not to act as a new leaderboard to rank LLMs. Results using ChatGPT or other API services may not be reproducible due to API changes.</p>
<h2>References</h2>
<p>Udit Arora, William Huang, and He He. Types of out-of-distribution texts and how to detect them. arXiv preprint arXiv:2109.06827, 2021.</p>
<p>Lukas Berglund, Meg Tong, Max Kaufmann, Mikita Balesni, Asa Cooper Stickland, Tomasz Korbak, and Owain Evans. The reversal curse: Llms trained on" a is b" fail to learn" b is a". arXiv preprint arXiv:2309.12288, 2023.</p>
<p>Stella Biderman, USVSN Sai Prashanth, Lintang Sutawika, Hailey Schoelkopf, Quentin Anthony, Shivanshu Purohit, and Edward Raf. Emergent and predictable memorization in large language models. arXiv preprint arXiv:2304.11158, 2023.</p>
<p>Steven Bird. Nltk: the natural language toolkit. In Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions, pages 69-72, 2006.</p>
<p>Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023.</p>
<p>Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and Chiyuan Zhang. Quantifying memorization across neural language models. arXiv preprint arXiv:2202.07646, 2022.</p>
<p>Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Kaijie Zhu, Hao Chen, Linyi Yang, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. A survey on evaluation of large language models. arXiv preprint arXiv:2307.03109, 2023.</p>
<p>Liang Chen, Hongru Wang, Yang Deng, Wai-Chung Kwan, Zezhong Wang, and Kam-Fai Wong. Towards robust personalized dialogue generation via order-insensitive representation regularization, 2023.</p>
<p>Katherine M Collins, Albert Q Jiang, Simon Frieder, Lionel Wong, Miri Zilka, Umang Bhatt, Thomas Lukasiewicz, Yuhuai Wu, Joshua B Tenenbaum, William Hart, et al. Evaluating language models for mathematics through interactions. arXiv preprint arXiv:2306.01694, 2023.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.</p>
<p>Taicheng Guo, Kehan Guo, Zhengwen Liang, Zhichun Guo, Nitesh V Chawla, Olaf Wiest, Xiangliang Zhang, et al. What indeed can gpt models do in chemistry? a comprehensive benchmark on eight tasks. arXiv preprint arXiv:2305.18365, 2023.</p>
<p>HuggingFace. Open-source large language models leaderboard. https://huggingface.co/spaces/ HuggingFaceH4/open_llm_leaderboard, 2023.</p>
<p>Douwe Kiela, Max Bartolo, Yixin Nie, Divyansh Kaushik, Atticus Geiger, Zhengxuan Wu, Bertie Vidgen, Grusha Prasad, Amanpreet Singh, Pratik Ringshia, et al. Dynabench: Rethinking benchmarking in nlp. arXiv preprint arXiv:2104.14337, 2021.</p>
<p>Minghao Li, Feifan Song, Bowen Yu, Haiyang Yu, Zhoujun Li, Fei Huang, and Yongbin Li. Api-bank: A benchmark for tool-augmented llms, 2023a.</p>
<p>Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Alpacaeval: An automatic evaluator of instruction-following models. https://github.com/tatsu-lab/alpaca_eval, 2023b.</p>
<p>Zhiyi Ma, Kawin Ethayarajh, Tristan Thrush, Somya Jain, Ledell Wu, Robin Jia, Christopher Potts, Adina Williams, and Douwe Kiela. Dynaboard: An evaluation-as-a-service platform for holistic next-generation benchmarking. Advances in Neural Information Processing Systems, 34:10351-10367, 2021.</p>
<p>OpenAI. https://chat.openai.com.chat, 2023.
Alicia Parrish, Angelica Chen, Nikita Nangia, Vishakh Padmakumar, Jason Phang, Jana Thompson, Phu Mon Htut, and Samuel R Bowman. Bbq: A hand-built bias benchmark for question answering. arXiv preprint arXiv:2110.08193, 2021.</p>
<p>Ethan Perez, Saffron Huang, Francis Song, Trevor Cai, Roman Ring, John Aslanides, Amelia Glaese, Nat McAleese, and Geoffrey Irving. Red teaming language models with language models, 2022.</p>
<p>Marco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin, and Sameer Singh. Beyond accuracy: Behavioral testing of nlp models with checklist. In Association for Computational Linguistics (ACL), 2020.</p>
<p>Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615, 2022.</p>
<p>Tristan Thrush, Kushal Tirumala, Anmol Gupta, Max Bartolo, Pedro Rodriguez, Tariq Kane, William Gaviria Rojas, Peter Mattson, Adina Williams, and Douwe Kiela. Dynatask: A framework for creating dynamic ai benchmark tasks. arXiv preprint arXiv:2204.01906, 2022.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023.</p>
<p>Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Glue: A multitask benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461, 2018.</p>
<p>Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding, 2019.</p>
<p>Jindong Wang, Xixu Hu, Wenxin Hou, Hao Chen, Runkai Zheng, Yidong Wang, Linyi Yang, Haojun Huang, Wei Ye, Xiubo Geng, et al. On the robustness of chatgpt: An adversarial and out-of-distribution perspective. arXiv preprint arXiv:2302.12095, 2023.</p>
<p>Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. Finetuned language models are zero-shot learners. arXiv preprint at arXiv: 2109.01652, 2022.</p>
<p>Linyi Yang, Shuibai Zhang, Libo Qin, Yafu Li, Yidong Wang, Hanmeng Liu, Jindong Wang, Xing Xie, and Yue Zhang. Glue-x: Evaluating natural language understanding models from an out-of-distribution generalization perspective. arXiv preprint arXiv:2211.08073, 2022.</p>
<p>Matej Zečević, Moritz Willig, Devendra Singh Dhami, and Kristian Kersting. Causal parrots: Large language models may talk causality but are not causal. arXiv preprint arXiv:2308.13067, 2023.</p>
<p>Kaijie Zhu, Jindong Wang, Jiaheng Zhou, Zichen Wang, Hao Chen, Yidong Wang, Linyi Yang, Wei Ye, Neil Zhenqiang Gong, Yue Zhang, et al. Promptbench: Towards evaluating the robustness of large language models on adversarial prompts. arXiv preprint arXiv:2306.04528, 2023.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ How to compute: $[0.939-(0.877+0.890) / 2] / 0.939 \times 100 \%=5.9 \%$.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
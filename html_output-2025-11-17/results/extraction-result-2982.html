<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2982 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2982</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2982</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-73.html">extraction-schema-73</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <p><strong>Paper ID:</strong> paper-266844276</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2401.03735v3.pdf" target="_blank">Language Models Know the Value of Numbers</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have exhibited impressive competence in various tasks, but their internal mechanisms on mathematical problems are still under-explored. In this paper, we study a fundamental question: whether language models know the value of numbers, a basic element in math. To study the question, we construct a synthetic dataset comprising addition problems and utilize linear probes to read out input numbers from the hidden states. Experimental results support the existence of encoded number values in LLMs on different layers, and these values can be extracted via linear probes. Further experiments show that LLMs store their calculation results in a similar manner, and we can intervene the output via simple vector additions, proving the causal connection between encoded numbers and language model outputs. Our research provides evidence that LLMs know the value of numbers, thus offering insights for better exploring, designing, and utilizing numeric information in LLMs.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2982.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2982.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-2-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA-2 (7B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A decoder-only transformer language model (7 billion parameters) from the LLaMA-2 family; used unmodified (no fine-tuning) to study internal numeric representations and arithmetic via probing and interventions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-2-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only transformer (LLaMA-2 family), ~7B parameters; used off-the-shelf with greedy decoding (max new tokens = 30). Hidden activations (residual stream) saved for all layers; probes trained on log2(number) values.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Synthetic multi-digit integer addition problems (two addends); numbers ranged 2 to 10 digits, dataset of 9,000 examples (1,000 per digit-length). Multiplication/division not included; subtraction omitted as similar to addition.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Numbers are encoded in the residual-stream hidden states; encoding is (near-)linearly representable along probe directions, encoded incrementally across token positions (partial-token encoding), with intermediate layers refining magnitude beyond token-length heuristics; mid-late layers used to compute results while late layers map computation to tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>High Pearson œÅ and out-of-sample R2 when regressing log2(number) from hidden states across many layers; ridge linear probes recover number values (though not perfectly); two-layer MLP probes do not outperform linear probes (supporting linear/near-linear representation); probing across token positions shows persistent readable encodings (rectangular pattern); activation patching (swap hidden states from alternative inputs) changes outputs in layer- and token-dependent ways (E(i,t) metric); linear interventions adding normalized probe directions boost probability of producing larger numeric outputs when applied to mid-late layers (success up to ~0.7 when intervening on ranges like layers 14‚Äì19).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Approximate accuracy (AAcc) of probe reconstructions is <50% at all layers (probes correlate but do not reconstruct precise values); probe MSE and AAcc degrade as token-length (numeric magnitude) increases; Pearson/R2 drop in late layers; simple vector additions to change the probed numeric projection sometimes fail (norm mismatch, pushing h_i out of valid subspace); interventions on early or very late layers are ineffective or destructive, suggesting nontrivial nonlinear constraints and that encoding is not perfectly lossless or algorithmic.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Activation patching (replace a layer/token hidden state with that from a different-input run) and linear vector intervention (add normalized probe coefficient vector d_i scaled by Œ± to residual stream); null baseline interventions (adding normalized h_i) used for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Activation patching shows token-and-layer-specific causal effects: patching number tokens in early layers influences final numeric output (early layers build token-sequence encodings), while patching last-token states in late layers affects final output (late layers store/emit result). Linear interventions on mid-late layers (e.g., layers 14‚Äì19) cause the model to output a larger predicted sum with success rates up to ~0.73 (vs much lower null baseline); intervening too early often destroys predictions; adding probe-derived numeric vectors directly to h_i can increase the model's predicted numeric magnitude but can fail if the vector pushes activations out of the model's valid subspace.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Model prediction accuracy on the addition dataset drops sharply for ‚â•6-digit numbers (Figure 9); LLaMA-2 models show marked accuracy decline at 6-digit tasks. Probing: high Pearson œÅ and out-of-sample R2 across many layers (exact numeric values shown in paper figures), but AAcc <50% across layers; linear intervention success rates for causing larger outputs reach ‚âà0.69‚Äì0.73 when intervening multiple mid-late layers. Dataset: 9,000 problems (2‚Äì10 digits).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Compression/encoding error increases with number magnitude (longer multi-token numbers harder); probes correlate but do not reconstruct exact values; late-layer encodings appear less precise/less used; simple additive vector interventions can fail due to norm/subspace mismatch; arithmetic beyond addition (e.g., multiplication) performs very poorly (paper notes ~0% for 5-digit multiplication).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>No direct human-vs-model psychophysical comparison; implicitly contrasted with exact symbolic arithmetic (calculators/algorithms) ‚Äî models show systematic degradation with scale while symbolic methods are exact. Paper discusses possibility of floating-point-like internal encodings but provides no direct equivalence to symbolic arithmetic.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models Know the Value of Numbers', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2982.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2982.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-2-13B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA-2 (13B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A larger decoder-only LLaMA-2 family model (13 billion parameters) analyzed for numeric encoding and arithmetic performance; used without fine-tuning for the same probing and intervention protocol.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-2-13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only transformer (LLaMA-2 family), ~13B parameters; used unmodified with greedy decoding. Hidden activations saved layerwise; probes trained on log2(number).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Same synthetic multi-digit integer addition dataset as above (2‚Äì10 digits).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Same as for the 7B variant: numeric values are encoded in hidden activations and are (near-)linearly extractable; intermediate layers provide the most precise linear readout; tokens are encoded incrementally by token-position.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Linear ridge probes show high Pearson œÅ and R2 on many layers; MLP probes provide no clear advantage over linear probes; partial-token probing shows incremental encoding; activation patching and linear interventions produce layer- and token-dependent causal effects similar to the 7B model.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>13B model does not show clear advantage over 7B on probing metrics (per the paper); AAcc remains <50% across layers; magnitude-related degradation and late-layer decorrelation persist. These observations challenge the idea that mere scale (7B‚Üí13B) alone fixes precise numeric encoding.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Same: activation patching and linear direction vector addition (probe-derived), with null baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Layer- and position-dependent effects; mid-late layer interventions more effective; early/late layer interventions less effective or destructive. No major qualitative improvement over 7B in intervention responsiveness reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Overall prediction accuracy declines at >5 digits; 13B shows similar probing metrics to 7B (no consistent advantage reported). Probing metrics (Pearson, R2) high in intermediate layers; AAcc <50%. Specific intervention success rates similar trends as 7B (figures reported in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Same as 7B: lack of precise reconstruction; degraded precision with longer/multi-token numbers; late-layer encodings less reliable for decoding; scale-up from 7B‚Üí13B did not eliminate numeric encoding failure modes.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>No direct comparison; paper notes Mistral-7B outperforms LLaMA-2 family on these numeric tasks, implying family/architecture differences matter beyond parameter count.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models Know the Value of Numbers', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2982.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2982.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mistral-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mistral-7B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7-billion-parameter decoder-only transformer (Mistral) studied for numeric encoding and causal interventions; shows better numeric encoding and mathematical performance than the LLaMA-2 models in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only transformer (Mistral family), ~7B parameters; used off-the-shelf without fine-tuning. Hidden activations recorded across layers; experiments include activation patching and linear interventions.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Synthetic multi-digit integer addition (2‚Äì10 digits); dataset identical to that used with LLaMA-2 models.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Numbers are encoded in hidden states (residual stream); encodings are linearly readable by probes; numeric computation appears to be performed primarily in mid-to-mid-late layers, with late layers mapping computed results to tokens; per-token incremental accumulation of partial numeric value occurs.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Strong probe metrics (Pearson, R2) in intermediate layers; AAcc and MSE patterns better than LLaMA-2 models (Mistral suffers less accuracy decay for long numbers); activation patching shows token- and layer-specific causal effects; linear interventions on mid-late layers achieve the highest success rates (paper reports max success ~0.73 when intervening layers 14‚Äì19 on Mistral), and shorter intervention ranges still reach high success (e.g., ~0.698 when intervening 5 consecutive layers in appendix).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Although better than LLaMA-2 in this study, Mistral still fails to reconstruct exact numbers reliably (AAcc <50% across layers), accuracy degrades with number length, and some interventions on early/late layers fail or are destructive, implying nontrivial structural constraints and non-lossless encoding.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Activation patching (hidden-state replacement) and linear vector addition along normalized probe direction d_i (Œ±=2 in main experiments); null baseline interventions used.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Activation patching indicates early layers build token-sequence encodings while mid-late layers use encodings to compute; linear interventions across mid-late layer ranges can causally shift outputs upwards with success rates up to ‚âà0.73; intervening too few layers or wrong layer ranges reduces effectiveness; null interventions perform much worse.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Mistral-7B achieves better probing and arithmetic performance than LLaMA-2 variants: it suffers less from accuracy decay for long numbers (figures in paper). Linear intervention success up to ~0.73 (layers 14‚Äì19), and ‚âà0.698 for some 5-layer ranges; probing AAcc <50% but Pearson/R2 high on intermediate layers. Overall addition accuracy declines sharply at 6-digit inputs for many models though Mistral less impacted.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Same systematic failure modes: imprecise exact reconstruction, greater error for longer/multi-token numbers, late-layer encodings sometimes not used, and interventions can fail if they push activations out of valid subspace; multiplication/division still very poor.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>No direct human comparison; paper notes Mistral's superior numeric encoding correlates with better arithmetic performance compared to LLaMA models, but all models remain far from exact symbolic arithmetic for large multi-digit operations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models Know the Value of Numbers', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis <em>(Rating: 2)</em></li>
                <li>How does gpt-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model <em>(Rating: 2)</em></li>
                <li>Emergent linear representations in world models of self-supervised sequence models <em>(Rating: 2)</em></li>
                <li>xval: A continuous number encoding for large language models <em>(Rating: 2)</em></li>
                <li>Language models represent space and time <em>(Rating: 1)</em></li>
                <li>Probing classifiers: Promises, shortcomings, and advances <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2982",
    "paper_id": "paper-266844276",
    "extraction_schema_id": "extraction-schema-73",
    "extracted_data": [
        {
            "name_short": "LLaMA-2-7B",
            "name_full": "LLaMA-2 (7B)",
            "brief_description": "A decoder-only transformer language model (7 billion parameters) from the LLaMA-2 family; used unmodified (no fine-tuning) to study internal numeric representations and arithmetic via probing and interventions.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA-2-7B",
            "model_description": "Decoder-only transformer (LLaMA-2 family), ~7B parameters; used off-the-shelf with greedy decoding (max new tokens = 30). Hidden activations (residual stream) saved for all layers; probes trained on log2(number) values.",
            "arithmetic_task_type": "Synthetic multi-digit integer addition problems (two addends); numbers ranged 2 to 10 digits, dataset of 9,000 examples (1,000 per digit-length). Multiplication/division not included; subtraction omitted as similar to addition.",
            "reported_mechanism": "Numbers are encoded in the residual-stream hidden states; encoding is (near-)linearly representable along probe directions, encoded incrementally across token positions (partial-token encoding), with intermediate layers refining magnitude beyond token-length heuristics; mid-late layers used to compute results while late layers map computation to tokens.",
            "evidence_for_mechanism": "High Pearson œÅ and out-of-sample R2 when regressing log2(number) from hidden states across many layers; ridge linear probes recover number values (though not perfectly); two-layer MLP probes do not outperform linear probes (supporting linear/near-linear representation); probing across token positions shows persistent readable encodings (rectangular pattern); activation patching (swap hidden states from alternative inputs) changes outputs in layer- and token-dependent ways (E(i,t) metric); linear interventions adding normalized probe directions boost probability of producing larger numeric outputs when applied to mid-late layers (success up to ~0.7 when intervening on ranges like layers 14‚Äì19).",
            "evidence_against_mechanism": "Approximate accuracy (AAcc) of probe reconstructions is &lt;50% at all layers (probes correlate but do not reconstruct precise values); probe MSE and AAcc degrade as token-length (numeric magnitude) increases; Pearson/R2 drop in late layers; simple vector additions to change the probed numeric projection sometimes fail (norm mismatch, pushing h_i out of valid subspace); interventions on early or very late layers are ineffective or destructive, suggesting nontrivial nonlinear constraints and that encoding is not perfectly lossless or algorithmic.",
            "intervention_type": "Activation patching (replace a layer/token hidden state with that from a different-input run) and linear vector intervention (add normalized probe coefficient vector d_i scaled by Œ± to residual stream); null baseline interventions (adding normalized h_i) used for comparison.",
            "effect_of_intervention": "Activation patching shows token-and-layer-specific causal effects: patching number tokens in early layers influences final numeric output (early layers build token-sequence encodings), while patching last-token states in late layers affects final output (late layers store/emit result). Linear interventions on mid-late layers (e.g., layers 14‚Äì19) cause the model to output a larger predicted sum with success rates up to ~0.73 (vs much lower null baseline); intervening too early often destroys predictions; adding probe-derived numeric vectors directly to h_i can increase the model's predicted numeric magnitude but can fail if the vector pushes activations out of the model's valid subspace.",
            "performance_metrics": "Model prediction accuracy on the addition dataset drops sharply for ‚â•6-digit numbers (Figure 9); LLaMA-2 models show marked accuracy decline at 6-digit tasks. Probing: high Pearson œÅ and out-of-sample R2 across many layers (exact numeric values shown in paper figures), but AAcc &lt;50% across layers; linear intervention success rates for causing larger outputs reach ‚âà0.69‚Äì0.73 when intervening multiple mid-late layers. Dataset: 9,000 problems (2‚Äì10 digits).",
            "notable_failure_modes": "Compression/encoding error increases with number magnitude (longer multi-token numbers harder); probes correlate but do not reconstruct exact values; late-layer encodings appear less precise/less used; simple additive vector interventions can fail due to norm/subspace mismatch; arithmetic beyond addition (e.g., multiplication) performs very poorly (paper notes ~0% for 5-digit multiplication).",
            "comparison_to_humans_or_symbolic": "No direct human-vs-model psychophysical comparison; implicitly contrasted with exact symbolic arithmetic (calculators/algorithms) ‚Äî models show systematic degradation with scale while symbolic methods are exact. Paper discusses possibility of floating-point-like internal encodings but provides no direct equivalence to symbolic arithmetic.",
            "uuid": "e2982.0",
            "source_info": {
                "paper_title": "Language Models Know the Value of Numbers",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "LLaMA-2-13B",
            "name_full": "LLaMA-2 (13B)",
            "brief_description": "A larger decoder-only LLaMA-2 family model (13 billion parameters) analyzed for numeric encoding and arithmetic performance; used without fine-tuning for the same probing and intervention protocol.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA-2-13B",
            "model_description": "Decoder-only transformer (LLaMA-2 family), ~13B parameters; used unmodified with greedy decoding. Hidden activations saved layerwise; probes trained on log2(number).",
            "arithmetic_task_type": "Same synthetic multi-digit integer addition dataset as above (2‚Äì10 digits).",
            "reported_mechanism": "Same as for the 7B variant: numeric values are encoded in hidden activations and are (near-)linearly extractable; intermediate layers provide the most precise linear readout; tokens are encoded incrementally by token-position.",
            "evidence_for_mechanism": "Linear ridge probes show high Pearson œÅ and R2 on many layers; MLP probes provide no clear advantage over linear probes; partial-token probing shows incremental encoding; activation patching and linear interventions produce layer- and token-dependent causal effects similar to the 7B model.",
            "evidence_against_mechanism": "13B model does not show clear advantage over 7B on probing metrics (per the paper); AAcc remains &lt;50% across layers; magnitude-related degradation and late-layer decorrelation persist. These observations challenge the idea that mere scale (7B‚Üí13B) alone fixes precise numeric encoding.",
            "intervention_type": "Same: activation patching and linear direction vector addition (probe-derived), with null baselines.",
            "effect_of_intervention": "Layer- and position-dependent effects; mid-late layer interventions more effective; early/late layer interventions less effective or destructive. No major qualitative improvement over 7B in intervention responsiveness reported.",
            "performance_metrics": "Overall prediction accuracy declines at &gt;5 digits; 13B shows similar probing metrics to 7B (no consistent advantage reported). Probing metrics (Pearson, R2) high in intermediate layers; AAcc &lt;50%. Specific intervention success rates similar trends as 7B (figures reported in paper).",
            "notable_failure_modes": "Same as 7B: lack of precise reconstruction; degraded precision with longer/multi-token numbers; late-layer encodings less reliable for decoding; scale-up from 7B‚Üí13B did not eliminate numeric encoding failure modes.",
            "comparison_to_humans_or_symbolic": "No direct comparison; paper notes Mistral-7B outperforms LLaMA-2 family on these numeric tasks, implying family/architecture differences matter beyond parameter count.",
            "uuid": "e2982.1",
            "source_info": {
                "paper_title": "Language Models Know the Value of Numbers",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Mistral-7B",
            "name_full": "Mistral-7B",
            "brief_description": "A 7-billion-parameter decoder-only transformer (Mistral) studied for numeric encoding and causal interventions; shows better numeric encoding and mathematical performance than the LLaMA-2 models in this study.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Mistral-7B",
            "model_description": "Decoder-only transformer (Mistral family), ~7B parameters; used off-the-shelf without fine-tuning. Hidden activations recorded across layers; experiments include activation patching and linear interventions.",
            "arithmetic_task_type": "Synthetic multi-digit integer addition (2‚Äì10 digits); dataset identical to that used with LLaMA-2 models.",
            "reported_mechanism": "Numbers are encoded in hidden states (residual stream); encodings are linearly readable by probes; numeric computation appears to be performed primarily in mid-to-mid-late layers, with late layers mapping computed results to tokens; per-token incremental accumulation of partial numeric value occurs.",
            "evidence_for_mechanism": "Strong probe metrics (Pearson, R2) in intermediate layers; AAcc and MSE patterns better than LLaMA-2 models (Mistral suffers less accuracy decay for long numbers); activation patching shows token- and layer-specific causal effects; linear interventions on mid-late layers achieve the highest success rates (paper reports max success ~0.73 when intervening layers 14‚Äì19 on Mistral), and shorter intervention ranges still reach high success (e.g., ~0.698 when intervening 5 consecutive layers in appendix).",
            "evidence_against_mechanism": "Although better than LLaMA-2 in this study, Mistral still fails to reconstruct exact numbers reliably (AAcc &lt;50% across layers), accuracy degrades with number length, and some interventions on early/late layers fail or are destructive, implying nontrivial structural constraints and non-lossless encoding.",
            "intervention_type": "Activation patching (hidden-state replacement) and linear vector addition along normalized probe direction d_i (Œ±=2 in main experiments); null baseline interventions used.",
            "effect_of_intervention": "Activation patching indicates early layers build token-sequence encodings while mid-late layers use encodings to compute; linear interventions across mid-late layer ranges can causally shift outputs upwards with success rates up to ‚âà0.73; intervening too few layers or wrong layer ranges reduces effectiveness; null interventions perform much worse.",
            "performance_metrics": "Mistral-7B achieves better probing and arithmetic performance than LLaMA-2 variants: it suffers less from accuracy decay for long numbers (figures in paper). Linear intervention success up to ~0.73 (layers 14‚Äì19), and ‚âà0.698 for some 5-layer ranges; probing AAcc &lt;50% but Pearson/R2 high on intermediate layers. Overall addition accuracy declines sharply at 6-digit inputs for many models though Mistral less impacted.",
            "notable_failure_modes": "Same systematic failure modes: imprecise exact reconstruction, greater error for longer/multi-token numbers, late-layer encodings sometimes not used, and interventions can fail if they push activations out of valid subspace; multiplication/division still very poor.",
            "comparison_to_humans_or_symbolic": "No direct human comparison; paper notes Mistral's superior numeric encoding correlates with better arithmetic performance compared to LLaMA models, but all models remain far from exact symbolic arithmetic for large multi-digit operations.",
            "uuid": "e2982.2",
            "source_info": {
                "paper_title": "Language Models Know the Value of Numbers",
                "publication_date_yy_mm": "2024-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis",
            "rating": 2,
            "sanitized_title": "a_mechanistic_interpretation_of_arithmetic_reasoning_in_language_models_using_causal_mediation_analysis"
        },
        {
            "paper_title": "How does gpt-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model",
            "rating": 2,
            "sanitized_title": "how_does_gpt2_compute_greaterthan_interpreting_mathematical_abilities_in_a_pretrained_language_model"
        },
        {
            "paper_title": "Emergent linear representations in world models of self-supervised sequence models",
            "rating": 2,
            "sanitized_title": "emergent_linear_representations_in_world_models_of_selfsupervised_sequence_models"
        },
        {
            "paper_title": "xval: A continuous number encoding for large language models",
            "rating": 2,
            "sanitized_title": "xval_a_continuous_number_encoding_for_large_language_models"
        },
        {
            "paper_title": "Language models represent space and time",
            "rating": 1,
            "sanitized_title": "language_models_represent_space_and_time"
        },
        {
            "paper_title": "Probing classifiers: Promises, shortcomings, and advances",
            "rating": 1,
            "sanitized_title": "probing_classifiers_promises_shortcomings_and_advances"
        }
    ],
    "cost": 0.010952,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Language Models Know the Value of Numbers
9 Jun 2024</p>
<p>Fangwei Zhu zhufangwei2022@stu.pku.edu.cn 
National Key Laboratory for Multimedia Information Processing
Peking University</p>
<p>Damai Dai daidamai@pku.edu.cn 
National Key Laboratory for Multimedia Information Processing
Peking University</p>
<p>Zhifang Sui 
National Key Laboratory for Multimedia Information Processing
Peking University</p>
<p>Language Models Know the Value of Numbers
9 Jun 2024E7F167B55E8C60B768C0541FF4EDAB9BarXiv:2401.03735v3[cs.CL]
Large language models (LLMs) have exhibited impressive competence in various tasks, but their internal mechanisms on mathematical problems are still under-explored.In this paper, we study a fundamental question: whether language models know the value of numbers, a basic element in math.To study the question, we construct a synthetic dataset comprising addition problems and utilize linear probes to read out input numbers from the hidden states.Experimental results support the existence of encoded number values in LLMs on different layers, and these values can be extracted via linear probes.Further experiments show that LLMs store their calculation results in a similar manner, and we can intervene the output via simple vector additions, proving the causal connection between encoded numbers and language model outputs.Our research provides evidence that LLMs know the value of numbers, thus offering insights for better exploring, designing, and utilizing numeric information in LLMs.</p>
<p>Introduction</p>
<p>Large language models (LLMs) have demonstrated excellent ability in various scenarios like question answering (Zhao et al., 2023;Li et al., 2023b), instruction following (Brown et al., 2020;Ouyang et al., 2022;Taori et al., 2023), and code generation (Chen et al., 2021;Nijkamp et al., 2022;Li et al., 2023a).Solving mathematical problems is generally viewed to be more difficult (Yu et al., 2023), and language models even struggle to solve simple arithmetic problems (Dziri et al., 2024).</p>
<p>Numbers are fundamental elements in math, and the way how LLMs process numbers essentially influences the final outcome.In order to accurately answer mathematical problems, LLMs need the ability to get the precise value of numbers in the input text.Currently, the way how LLMs process numbers is still not fully explored.While previous studies (Stolfo et al., 2023;Hanna et al., 2024) have Figure 1: Encoded number values in the hidden state of language models.We find that both the value of input numbers (blue and green) and calculation results (red) can be read out from the hidden state of language models via linear probes.
1 2 3 + 4 = ‚Ä¢ ‚Ä¢ ‚Ä¢ ùêø 0 ‚Ä¢ ‚Ä¢ ‚Ä¢ ‚Ä¢ ‚Ä¢ ‚Ä¢ ‚Ä¢ ‚Ä¢ ‚Ä¢ ‚Ä¢ ‚Ä¢ ‚Ä¢ ‚Ä¢ ‚Ä¢ ‚Ä¢ ‚Ä¢ ‚Ä¢ ‚Ä¢ ‚Ä¢ ‚Ä¢ ‚Ä¢ ‚Ä¢ ‚Ä¢ ‚Ä¢ ‚Ä¢ ‚Ä¢ ‚Ä¢ ‚Ä¢ ‚Ä¢ ‚Ä¢ ‚Ä¢ ‚Ä¢ ‚Ä¢ ‚Ä¢ ‚Ä¢ ‚Ä¢
explored the inner mechanisms of language models on mathematical problems, they focus on small numbers that can be represented with a single token, and how LLMs utilize multi-token large numbers remains largely unknown.</p>
<p>In this paper, we explore the question whether LLMs know the value of numbers through extracting numerical information from their internal representations.To be specific, we construct a synthetic dataset comprising simple addition questions, and train linear probes (Nanda et al., 2023;Gurnee and Tegmark, 2023) on the hidden states of LLMs to predict the number values provided in the input text.Experimental results on the dataset demonstrate that the value of input numbers can be probed from the hidden states of language models from early layers.Both input values and calculation results can be read out, and encoded values can be found at different positions in the input token sequence.These experimental results support that language models do encode numerical information.</p>
<p>To further verify the fact that the encoded number values are utilized by language models, we study the causal connection between numeric information and model outputs.To be specific, we discover that we can influence the calculation result of language models by performing interventions like activation patching or adding linear vectors.</p>
<p>The above discoveries may reveal future directions for utilizing the encoded numerical information, for example, specialized encoding systems and error mitigation modules.</p>
<p>To sum up, our contributions can be listed as: (1) We study the question of whether language models are able to encode the value of numbers in the input text and construct a synthetic dataset to analyze the language models.(2) We discover that language models do encode the value of numbers by utilizing linear probes to probe encoded number values in the hidden states of language models.(3) We further prove that language models utilize the encoded numerical information by revealing the causal connection between encoded number values and the final output of language models.</p>
<p>Probing Numbers in Language Models</p>
<p>The Goal of Probing</p>
<p>Given that there is a number x in the input text t, we assume that a language model LM can encode the number in its hidden state h i ‚àà R d model of a specific layer i, where d model is the hidden dimension.We denote the mapping as:
h i = f i (x, t ‚àí x)(1)
where f i refers to the process of encoding on layer i, and t ‚àí x refers to the remaining part in t aside from x.</p>
<p>If the mapping function f is a bijective function, there will exist an inverse function f ‚àí1 i that reconstructs the original number x from the hidden state h i .For each layer i, we aim to find a optimal predictor P * i that imitates f ‚àí1 i , whose prediction best fits the original number x:
P * i = arg min P i |x ‚àí P i (h i )| (2)
Considering the numerical stability, we probe the logarithmic value log 2 (x) instead of the original number x in all our experiments.We can assess the existence of encoded number values by observing how much the probing result P * i ((h i )) resembles the original number x.</p>
<p>Dataset Construction</p>
<p>To investigate whether LLMs know numbers, we construct a synthetic dataset containing different magnitudes of numbers.The dataset contains numbers ranging from 2 digits to 10 digits, with each digit corresponding to 1000 entries 1 .We split the dataset into training, validation, and test sets at a ratio of 80%/10%/10%.To observe how LLMs encode and utilize numbers, we adopt addition problems as our prompt2 .Let a and b be two randomly generated numbers, each question is formulated as:</p>
<p>Question: What is the sum of {a} and {b}?Answer: {a + b}</p>
<p>Probing Method</p>
<p>Obtaining Hidden States.We choose the LLaMA-2 model family (Touvron et al., 2023b) and Mistral-7B (Jiang et al., 2023) as base models to be investigated.We feed the question text in Section 2.2 into the models, and save the hidden states of all layers.For each layer, we obtain a set of hidden states (i.e. the residual stream) H ‚àà R n√ód model at every token position, where n is the number of samples in the dataset.</p>
<p>Training Probes.Following previous work, we adopt the widely acknowledged linear probing technique to reconstruct numbers from the hidden states.To be specific, for each layer, given a set of hidden states H and their corresponding original numbers X = {x}, we train a linear regressor P that yields best predictions P = HW + b, where W ‚àà R d model and b are the weights of P.</p>
<p>In practice, directly performing linear regression could give erroneous results, as the value of numbers varies over a wide range.We do a logarithmic operation on input numbers X with a base of 2 to guarantee the numerical stability of probes.</p>
<p>We utilize Ridge regression, which adds L2 regularization to the vanilla linear regression model, to construct the probes:
W * , b * = arg min W,b || log 2 (X)‚àíHW ‚àíb|| 2 2 +Œª||W|| 2 2 (3)
where W * , b * are the weights of regressors, and Œª is a hyperparameter that controls regularization strength.In this way, we can predict logarithmic results P * = HW * +b * based on the hidden states.</p>
<p>Evaluation Metrics</p>
<p>We use two standard regression metrics on the probing task to evaluate the probes: R 2 which determines the proportion of variance in the dependent variable that can be explained by the independent variable, and the Pearson coefficient œÅ which measures the linear correlation between two variables.</p>
<p>As mathematical problems require a precise understanding of numbers, we introduce two additional metrics to examine how well can a language model encode numbers:</p>
<p>Approximate accuracy (AAcc) evaluates whether the predicted number is approximately the same as the original number, namely with an error margin of &lt; 1%.Higher AAcc indicates that the number encoding is more likely to be precise.</p>
<p>Mean square error (MSE) is the average squared difference between probe predictions and actual values.Smaller MSE means lower loss during the encoding process.
AAcc(P * , X) = |(2 P * ‚àí X) &lt; 0.01X| |X| (4) MSE(P * , X) = avg((P * ‚àí log 2 X) 2 ) (5)</p>
<p>Experimental Setup</p>
<p>We use the original LLaMA-2-7B, LLaMA-2-13B, and Mistral-7B models without fine-tuning for all experiments.The outputs are obtained by performing greedy search with a max new token restriction of 30 during decoding.The regularization strength is set to Œª = 0.1 for all probes. 3.</p>
<p>In main experiments, we probe 3 distinct values at different positions: the first number a at the number token of a, the second number b at the last number token of b, and the prediction of language models o at the last token of the entire input text.We report the accuracy of o, i.e. the ratio of o = a + b, in Appendix C.</p>
<p>3 Do LLMs Know Number Values?</p>
<p>The Existence of Encoded Number Values</p>
<p>LLMs do encode number values.We first inspect the overall Pearson coefficient (œÅ) and out-ofsample R 2 on all layers.High œÅ and R 2 indicate that LLMs are likely to be able to encode number values in their hidden states.As illustrated in Figure 2, the probes achieve surprisingly high œÅ and R 2 on all layers, proving that the hidden states of LLMs contain the encoded value of input numbers, and the encoding process starts from even the first layer.Meanwhile, notice that both œÅ and R 2 slightly drop on late layers, which may indicate that intermediate layers better encode number values.Linear probes cannot reconstruct the precise value.Aside from the existence of encoded number values, we are also interested in their precision, which is depicted by AAcc and MSE in Figure 3.</p>
<p>In contrast to high correlation coefficients, the AAcc is below 50% on all layers, which means that the linear probes have difficulty in precisely reconstructing the input numbers.The trends in AAcc and MSE are consistent with œÅ and R 2 , indicating that LLaMA-2 models achieve the most precise number encoding in intermediate layers, but the encoding faces more error in deeper layers.</p>
<p>This phenomenon may indicate that language models use stronger non-linear encoding systems, which we will further explore in Section 3.3; Or it may be a hint that the number encoding in language models is not precise 4 .</p>
<p>Number Encoding Patterns are Different across Layers</p>
<p>To better analyze how language models encode numbers, we pick distinct layers in LLaMA-2-7B and observe how the pattern of probe predictions changes as the layer gets deeper.Layer 0 (i.e. the first transformer block after embedding layer), 10, and 30 are selected to represent early, intermediate, and late layers respectively.The trend of change 4 See Appendix E for more detailed experiments.</p>
<p>on the first input number a is shown in Figure 4. On early layers like layer 0, the predictions of probes are distorted to some extent: for original numbers with the same length, their corresponding predictions in the figure display a pattern of horizontal lines.This phenomenon indicates that early layers focus on the length of numbers, which corresponds to the number of input digit tokens.</p>
<p>As the layer gets deeper, probes on intermediate layers show the best performance.On layer 10, the predicted results are very close to the actual answers, yielding a near-perfect linear probe for original numbers.However, noise emerges in the prediction results again in late layers, with the form of uniformly distributed errors.</p>
<p>The trend of change leads us to a conjecture that language models first roughly estimate the value of a number with its token length, and then refine the estimation in subsequent layers.The process may not be lossless, which leads to errors in the final number encoding of language models.</p>
<p>LLMs Encode Numbers Linearly</p>
<p>Previous work (Nanda et al., 2023;Gurnee and Tegmark, 2023) on probing neural networks propose the linear representation hypothesis: the presence of features of a neural network can be proved by training a linear projector which projects the activation vector to the feature space, and complex structures are unnecessary.To verify whether the numbers can be represented linearly, we follow the method of Gurnee and Tegmark (2023) which trains two-layer MLP probes and compares their performance with linear probes.The MLP probes have an intermediate hidden state of 256 dimensions and can be formulated as:
P = W 2 ReLU(W 1 H + b 1 ) + b 2 (6)
where W 1 , W 2 , b 1 and b 2 are trainable weights.</p>
<p>Figure 5 demonstrates the comparison between MLP probes and linear probes on mean square error.We find that nonlinear MLP probes do not show any clear advantage over linear probes, proving that the encoded number values can be represented linearly, or at least near-linearly.</p>
<p>Numeric Information Persist at Subsequent Positions</p>
<p>Another question is whether these encoded values are only stored at certain positions, or are they persist at subsequent positions.For input number values a, b, we train probes at every individual token position to examine where these values exist.</p>
<p>Figure 6 shows the mean square error of probes on the LLaMA-2-7B model.</p>
<p>The results demonstrate a clear rectangular pattern, indicating that the value of an input number can be read out at any subsequent position.In other words, the number values would persist at subsequent positions.It is also worth noticing that the probing accuracy on the last token is lower than other positions, which may be interpreted as language models do not continue to remember input numbers after computing the final outcome.</p>
<p>Do LLMs Utilize Number Values?</p>
<p>The previous section has proved the existence of encoded number values in language models.However, an inherent issue is that the probed information is only correlational to the output of models, and no causal effects can be directly claimed (Belinkov, 2022).</p>
<p>In this section, we will try to verify the hypothesis that language models do use the encoded number values to get their calculation results by performing a set of intervention experiments.Given an input question Q with an expected result of o, we intervene in the internal activation of language models to make it believe in an altered question Q ‚Ä≤ , and observe how the new result o ‚Ä≤ changes.</p>
<p>To ensure the effectiveness of the intervention, we conduct the experiments on Mistral-7B with 4-</p>
<p>Patching Encoded Numbers</p>
<p>We adopt the activation patching technique proposed by Stolfo et al. (2023) to quantify the importance of encoded number values h i at different layers i and different token positions.</p>
<p>To be specific, given an input addition problem consisting of input numbers a and b, we will conduct the following procedure:</p>
<ol>
<li>
<p>Obtain the clean output of the language model o = LM (a, b).</p>
</li>
<li>
<p>Replace a with another number a ‚Ä≤ to get a new output o ‚Ä≤ = LM (a ‚Ä≤ , b), and record the hidden states h ‚Ä≤ at certain position t during the forward pass;</p>
</li>
<li>
<p>Perform an additional forward pass with a and b as input numbers, where we substitute the hidden state h i of layer i with h ‚Ä≤ i .This would lead to an intervened result o * .</p>
</li>
</ol>
<p>We set a ‚Ä≤ = 9999 in our experiments, and evaluate the effect of intervention as:
E(i, t) = |o * ‚àí o| |o ‚Ä≤ ‚àí o| (7)
which measures how much a specific layer i at position t contributes to the final shifting of results.</p>
<p>Figure 7 demonstrates the effect of activation patching on different components, from which we can draw multiple observations:</p>
<p>Language models concern only certain tokens during calculation.Despite our finding in Section 3.4 that encoded number values would persist in subsequent tokens, patching non-number tokens has almost zero effect on the final outcome.This pattern indicates that the encoded number values at most positions are simply "memorized" rather than "used" by the language model.An exception is the last token, where language models seem to store their calculation results.</p>
<p>Each position can construct the number encoding from scratch.The effect of patching on different number digits displays a clear pattern: the earlier a digit appears, the higher the patching effect is.A possible explanation is that each position constructs its number encoding from scratch, and their encoded number values are independent from each other.For example, although the activation  at digit "3" in "1234" encodes the value of 123, language models will only attend to the value 2 in the encoding process at subsequent positions, which is demonstrated in Table 1.More detailed experiments are reported in Appendix G.</p>
<p>Early and late layers play different roles.The effect of activation patching is clearly divided into two parts: in early layers before layer 14, patching the number tokens greatly influences the final outcome, while patching the last token is mostly ineffective; but in late layers after layer 20 it is just the opposite.We can assume that early layers perform the task of processing the value of input number token sequences, while late layers use encoded values to calculate the final outcome, which is similar to the findings in Stolfo et al. (2023).</p>
<p>Linearly Intervening Encoded Numbers</p>
<p>Method.To determine whether the encoded computational results causally affect the outcome of language models, we conduct linear intervention experiments similar to (Nanda et al., 2023).For each intervened layer i, we add the number encod-ing direction vector d i to the residual stream h i :
h ‚Ä≤ i = h i + Œ±d i (8)
where Œ± &gt; 0 is a scaling factor and the direction vector d i is obtained by normalizing the probe coefficients:
d i = W i |W i | (9)
Considering that the probed number value is the projection of h i along the direction d i , the effect of our intervention is to "push" the residual stream towards a larger encoded number.We set Œ± = 2 in our experiments, and intervened language models outputting a larger number o ‚Ä≤ &gt; o than the original prediction o is viewed as a success.</p>
<p>In the linear intervention experiment, we choose probes for language model predictions o at the last input token to obtain the direction vector d i , and perform an intervention on every newly generated token.We use a test set of 1,000 entries and measure the efficacy of our intervention by observing the ratio of successful interventions.</p>
<p>We also record the results of taking normalized h i as the intervention direction d i , which represents a null intervention method for comparison.</p>
<p>Result and Findings.Figure 8 shows the success rate of intervening on 6 consecutive layers.Linear intervention achieves the highest success rate of 0.73 when intervening layer 14 to layer 19, outperforming the null intervention baseline by a large margin.This suggests that the linearly encoded number values are causal to model predictions.</p>
<p>It is also worth noticing that intervening on midlate layers is significantly more effective than on early layers and late layers.We hypothesize that this phenomenon is related to the findings of Stolfo et al. (2023): language models use mid-late layers to perform arithmetic computations, while the late layers are responsible for converting the computational result to output tokens.</p>
<p>Discussion and Future Directions</p>
<p>In previous sections, we find that LLMs know the value of numbers and utilize the encoded number values to perform calculations.However, the compression may not be lossless, and the calculation ability scales with model size.Moreover, the ability to understand and utilize numbers is positively correlated to mathematical competency.These findings reveal some future research directions that are potentially promising.</p>
<p>The exact way that LLMs encode numbers.</p>
<p>While our experiments show that the original input number cannot be reconstructed from the hidden state via linear probes, there exists a possibility that the LLMs encode numbers in a way that is close to a linear projection but not identical, such as the floating-point system (Muller et al., 2018).Finding out the exact encoding, if possible, will give us a better insight into how LLMs function.</p>
<p>Specialized number encoding systems.It seems that LLMs are currently not able to precisely encode numbers, and the compression loss will inevitably bring errors to subsequent computation, especially when the input numbers are large.Developing specialized encoding systems that could give precise presentations for numbers (Golkar et al., 2023) could eliminate errors at the root, thus helping LLMs better solve mathematical problems.</p>
<p>Mitigating computational errors with encoded numbers.By adding modules that directly utilize the encoded numbers in language models, the computational errors may be further reduced, especially on large-number calculations.We conduct a pioneer experiment in Appendix I to reveal the potential of controlling computational errors with probed numbers.</p>
<p>Related Work</p>
<p>Large Language Models on Mathematical Problems.Large language models (LLMs) like the GPT series (OpenAI, 2023), PaLM (Anil et al., 2023) and LLaMA (Touvron et al., 2023a,b) have demonstrated their impressive ability in various fields (Zhao et al., 2023;Li et al., 2023b;Taori et al., 2023;Chen et al., 2021;Nijkamp et al., 2022;Li et al., 2023a).On mathematical datasets like GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021), there have been methods like chain-of-thought reasoning (Wei et al., 2022) and self-consistency (Wang et al., 2022) to help LLMs better solve these questions.Specialized large language models like MetaMath (Yu et al., 2023) and Math-Shepherd (Wang et al., 2023) also show great competency.</p>
<p>Interpreting Internal Representations in Language Models.Prior research has unveiled that language models are able to store certain information in their hidden states, for example, passive voice (Shi et al., 2016) and sentence structure (Tenney et al., 2018).By adopting the probing tech-nique (Alain and Bengio, 2016;Belinkov, 2022), complex representations have also been detected in language models: Li et al. (2022) shows that language models are capable of memorizing the state of an Othello game, and Nanda et al. (2023) further proves that the states can be linearly represented; Li et al. (2021) claims that language models are able to encode the properties and relations of entities; Gurnee and Tegmark (2023) reveals evidence that large language models build spatial and temporal representations about an entity from early layers.</p>
<p>Explaining Numbers and Arithmetic in Language Models.How language models process numbers has been studied by multiple researchers.Wallace et al. (2019) detects the existence of numeracy in static pre-trained word embeddings.Hanna et al. (2024) finds a critical circuit that performs greater-than comparisions in GPT-2.Stolfo et al. (2023) studies how language models process arithmetic information by intervening on specific modules of the model.</p>
<p>Conclusion</p>
<p>In this paper, we study the question of whether large language models know the value of numbers.If number values can be extracted from the internal representations of LLMs, we can assume that language models to some extent know the value of numbers.We construct a dataset consisting of simple addition problems and introduce linear probes to investigate whether language models encode number values.</p>
<p>Experimental results prove that LLMs do encode the value of input numbers, and the representation could be linearly read out.The ability to encode numbers is consistent across different model scales, and the encoding seems to be the most precise on intermediate layers.Further experiments show that LLMs utilize the encoded number values to perform arithmetic calculations, and the behavior of language models can be controlled via simple linear interventions, proving the causal connection between encoded numbers and model outputs.</p>
<p>Our work shows a glimpse of the internal mechanisms of how language models solve mathematical questions.Future works on the internal representations of numbers, for example, better probes and specialized number encoders, may enhance the mathematical competence of language models in an explainable way.</p>
<p>Limitations and Risks</p>
<p>While we explore the inner mechanisms of how language models understand numbers, the probes trained in our current method are only approximations of the encoded numbers rather than exact internal presentations.Directly performing calculations with probes would lead to undesired results.Meanwhile, our experiments are conducted on LLMs whose parameters are openly available, while other LLMs the ChatGPT or GPT-4 may exhibit different behaviors.</p>
<p>C Overall Accuracy</p>
<p>Figure 9 shows the overall accuracy of different language models on addition problems.We can see that the accuracy of all models, especially LLaMA-2 models, faces a sharp decline at 6-digit problems, which may have a possible correlation with the partial number encoding accuracy demonstrated in Figure 11.</p>
<p>In the LLaMA-2 family, the 13B model does not show any advantage over the 7B model on probing metrics.In contrast, Mistral-7B displays better performance on all probing metrics, which is consistent with its outstanding math ability.The difference implies that the ability to encode numbers is consistent across different model scales, but varies between different model families.Meanwhile, the ability to understand numbers show a positive correlation with the math ability of LLMs.</p>
<p>D Detailed Experiments on Linearity</p>
<p>Figure 10 shows the comparison between linear probes and MLP probes on œÅ, R 2 and MSE.We can observe that MLP probes generally perform no better than linear probes.</p>
<p>E Experimental Results on Partial Number Encoding</p>
<p>In large language models like LLaMA-2, large numbers are split into multiple tokens, where each token represents a certain digit of the original number.This raises a question: whether the encoding process will proceed from token to token, or will it only happen at the end of number token sequences?</p>
<p>To investigate the problem, we choose addition problems consisting of 8-digit numbers and probe the value of the partial number sequence at every token position.For example, given a number token sequence "12345678", we will probe the value 12 at the position of token "2", and probe the value 123 at the position of token "3".</p>
<p>Figure 11 shows the probing accuracy of 3 models.It can be observed that the value of the partial number sequence can be read out at every token position.In other words, language models encode the number token sequence incrementally.</p>
<p>Meanwhile, the accuracy significantly declines as the token sequence gets longer, which means that language models face increasing difficulty in capturing the precise value as the number gets larger in scale.Notice that Mistral-7B suffers less from accuracy decay, we can assume that the ability to precisely encode long number token sequences is positively correlated to the mathematical ability of language models.</p>
<p>Figure 12 shows the Pearson coefficient, out-ofsample R 2 , and mean square error of probes on partial sequence of 8-digit numbers.These metrics remain stable as the length of number token sequence gets longer, indicating that language models do have the ability to incrementally encode number values, but there would be more error when the number gets larger in scale.</p>
<p>F Probing With Control Tasks</p>
<p>There exists the risk that probes may learn to extract values that language models do not encode.In Figure 6, we can see that probing on the second input number b at positions before it appears would lead to extremely large mean square errors, which acts as a piece of preliminary evidence that the probe performance does not solely come from probe strength.</p>
<p>To quantify the influence of probe strength, we conduct an experiment that probes with control tasks.For each question, we generate a random number c that shares the same digit with a and b as the control signal.If the probing performance comes from the encoded number values rather than probe strength, there would be a clear gap between the probing performance on c and a, b.</p>
<p>Figure 13 shows the difference between probe performances.It can be observed that probing on input numbers constantly yields better performance than probing on random control signals, proving that language models do encode number values in their hidden states.We also observe that despite intervening on early or late layers both lead to poor success rates, and they display different patterns of output.Table 3 shows the result of intervening on different layers of Mistral-7B.It can be seen that performing a linear intervention on early layers would completely destroy the final outcome, while intervening on late layers will not change the result at all.We hypothesize that the number encoding in early layers has not fully developed yet, and intervening in it would lead to unexpected results; In late layers, the number encoding is simply remembered but not used, and the language models rely on other subspace to decode the final outcome.</p>
<p>We have also tried to change the probed number from the original value o to a new value o + o ‚Ä≤ :
h i W i + b i = o (10)d i = o ‚Ä≤ W i |W i | 2 (11) (h i + d i )W i + b i = o + o ‚Ä≤ (12)
However, the intervention does not yield results as expected: the intervened model continues to predict o rather than o + o ‚Ä≤ .We observe that on layer 14 of Mistral-7B, the L2 norm of h i is around 4, while the L2 norm of W i is around 10. Thus, the norm of d i would be around 0.1, which is way smaller than the norm of  h i .A possible hypothesis is that the probed number value is the projection of h i along the direction W i , and simply adding vectors to h i would draw it away from its valid subspace.To maintain intervened h i in its valid subspace, it should be rotated along some direction.The method of precisely changing the encoded number values in language models still remains to be explored.</p>
<p>I Directly Calculate with Encoded Number Values</p>
<p>We are curious about whether the probed number values could help LLMs better perform calcula- where S and G represent predicted answers and golden answers respectively.Both metrics indicate how much the calculated results deviate from the golden answers.In Figure 16, despite failing to generate accurate answers, all three models could keep their logMSE and error margin at a very low level by adding probed a and b, while directly accepting the output of language models would lead to results that deviate far away from the golden answers.We think that this reveals a possibility to control the computational error of language models within a reasonable range, and will not produce results that are far too unreasonable.</p>
<p>We also notice that for LLaMA-2 models, adding the probed number on late layers will result in a high error margin, which may be a result of the findings in Section 4.1: number encoding on late layers is not used by the model.</p>
<p>Figure 2 :
2
Figure 2: Pearson coefficient (œÅ) and out-of-sample R 2 of probes on different layers.a and b refer to the two input numbers denoted in Section 2.2, and o refers to the prediction of language models respectively.High œÅ and R 2 indicate the existence of encoded number values in the hidden states.</p>
<p>MSE of probes on o.</p>
<p>Figure 3 :
3
Figure 3: Approximate accuracy (AAcc) and mean square error (MSE) of probes on different layers.a and b refer to the two input numbers denoted in Section 2.2, and o refers to the prediction of language models respectively.High AAcc and low MSE indicate precise number encoding.</p>
<p>Figure 4 :Figure 5 :
45
Figure 4: How the pattern of probe predictions on the first input number a changes as the layer gets deeper.</p>
<p>MSE of of probes on b.</p>
<p>Figure 6 :Figure 7 :
67
Figure 6: The mean square error (MSE) of probes at different token positions on LLaMA-2-7B.<n1> represents the last token of the first input number a, and <n2> represents the last token of the second input number b, respectively.The rectangular pattern indicates that the value of an input number can be read out at any subsequent position.</p>
<p>Figure 8 :
8
Figure 8: The success rate of performing a linear intervention on 6 consecutive layers.The red line represents the results of the baseline model which simply extends the hidden states in their original directions.More detailed experiments are reported in Appendix H.</p>
<p>Figure 9 :
9
Figure9: The overall accuracy of language model predictions on addition problems.</p>
<p>Figure 12 :
12
Figure 12: The Pearson coefficient (œÅ), out-of-sample R 2 , and mean square error (MSE) of probes on partial number sequence of 8-digit numbers.The y-axis represents the index of number tokens in the token sequence.</p>
<p>b versus c of LLaMA-2-7B.b versus c of LLaMA-2-13B.b versus c of Mistral-7B.</p>
<p>Figure 13 :Figure 14 :
1314
Figure 13: The difference in mean square error (MSE) between probes on input numbers and control signals.A lighter color indicates a greater performance gap.</p>
<p>Figure 15 :
15
Figure15: The success rate of performing a linear intervention on layers starting from layer 14.</p>
<p>Figure 16 :
16
Figure 16: Comparison between the sum of probed (a, b) and language model predictions.AB means the sum of probed (a, b) and LM means language model predictions.</p>
<p>Table 3 :
3
Intervention results on the question "Question: What is the sum of 2936 and 3519 ?".Running Mistral-7B without intervention would lead to the result of 6455.</p>
<p>See Appendix A for more details.
We do not include subtraction problems in our dataset in that subtraction problems are similar to addition problems. We do not include multiplication and division problems either, as LLMs perform poorly on these problems (even 5-digit multiplication yields an accuracy of about 0%.
See Appendix B for more details.
https://github.com/neelnanda-io/ TransformerLens
A Dataset DetailsThe dataset in Section 2.2 contains 9000 addition problems.For each number of digits between 2 and 10, 1000 problems are generated, and two numbers in the same problem share the same digit.For questions whose number has 4 or fewer digits, we list all possible combinations of numbers and randomly sample 1,000 of them to generate the questions.For questions whose number has 5 or more digits, we randomly sample both numbers to generate the 1000 questions.B Experiment ImplementationThe experiments are conducted on 4 NVIDIA GTX 3090 GPUs.Acquiring the hidden states of LLMs on our synthetic dataset requires 10 20 GPU hours per model.We obtain the LLaMA-2 models and Mistral-7B model from the huggingface model hub, and implement the experiments with the huggingface transformers Python library.The probes are trained with the scikit-learn Python library.We use the TransformerLens library 5 for intervention experiments.We follow the terms of use of all models and use them only for research.G Detailed Experiments on Activation Patching G.1 Examples of PatchingTable3shows the results of patching on layer 8 of Mistral-7B on the question "Question: What is the sum of 5678 and 1234 ?"We can clearly see that patching a digit will only influence the value of the digit itself, rather than the value of the partial token sequence: patching the last digit 8 in 5678 equals changing the number to 5679 rather than 9999, although the encoded value of 9999 can be found in the activation.We hypothesize that language models encode the number values from scratch at every new position, rather than using previous encoded values.We also notice that patching the last number digit on early layers shows a higher effect than expected, but the reason why the last digit is more special is still unknown.H Detailed Experiments on Linear InterventionFigure14shows the success rate of intervening on 5 consecutive layers with a maximum success rate of 0.698, and Figure15shows the success rate of intervening on a series of layers starting from layer 14.It can be observed that a sufficient number of layers need to be intervened for language models to successfully change their predictions.Nanda et al. (2023)observed a similar phenomenon in Othel-loGPT, and a related hypothesis is that language models demonstrate the Hydra effect(McGrath et al., 2023), where other layers would self-repair
Understanding intermediate layers using linear classifier probes. Guillaume Alain, Yoshua Bengio, arXiv:1610.016442016arXiv preprint</p>
<p>Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, arXiv:2305.10403Palm 2 technical report. 2023arXiv preprint</p>
<p>Probing classifiers: Promises, shortcomings, and advances. Yonatan Belinkov, Computational Linguistics. 4812022</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, arXiv:2107.03374Evaluating large language models trained on code. 2021arXiv preprint</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, arXiv:2110.14168Training verifiers to solve math word problems. 2021arXiv preprint</p>
<p>Ronan Le Bras, et al. 2024. Faith and fate: Limits of transformers on compositionality. Nouha Dziri, Ximing Lu, Melanie Sclar, Lorraine Xiang, Liwei Li, Bill Jiang, Sean Yuchen Lin, Peter Welleck, Chandra West, Bhagavatula, Advances in Neural Information Processing Systems. 36</p>
<p>xval: A continuous number encoding for large language models. Siavash Golkar, Mariel Pettee, Michael Eickenberg, Alberto Bietti, Miles Cranmer, Geraud Krawezik, Francois Lanusse, Michael Mccabe, Ruben Ohana, Liam Parker, arXiv:2310.02207NeurIPS 2023 AI for Science Workshop. Wes Gurnee and Max Tegmark. 2023. Language models represent space and time. 2023arXiv preprint</p>
<p>How does gpt-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model. Michael Hanna, Ollie Liu, Alexandre Variengien, Advances in Neural Information Processing Systems. 202436</p>
<p>Measuring mathematical problem solving with the math dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, Thirtyfifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2021</p>
<p>Alexandre Albert Q Jiang, Arthur Sablayrolles, Chris Mensch, Devendra Bamford, Diego Singh Chaplot, Florian De Las Casas, Gianna Bressand, Guillaume Lengyel, Lucile Lample, Saulnier, arXiv:2310.06825Mistral 7b. 2023arXiv preprint</p>
<p>Implicit representations of meaning in neural language models. Belinda Z Li, Maxwell Nye, Jacob Andreas, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing20211</p>
<p>Emergent world representations: Exploring a sequence model trained on a synthetic task. Kenneth Li, Aspen K Hopkins, David Bau, Fernanda Vi√©gas, Hanspeter Pfister, Martin Wattenberg, The Eleventh International Conference on Learning Representations. 2022</p>
<p>Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, arXiv:2305.06161Starcoder: may the source be with you!. 2023aarXiv preprint</p>
<p>Xingxuan Li, Ruochen Zhao, Ken Yew, Bosheng Chia, Lidong Ding, Shafiq Bing, Soujanya Joty, Poria, arXiv:2305.13269Chain of knowledge: A framework for grounding large language models with structured knowledge bases. 2023barXiv preprint</p>
<p>Thomas Mcgrath, Matthew Rahtz, Janos Kramar, Vladimir Mikulik, Shane Legg, arXiv:2307.15771The hydra effect: Emergent self-repair in language model computations. 2023arXiv preprint</p>
<p>Handbook of floating-point arithmetic. Jean-Michel Muller, Nicolas Brisebarre, Florent De Dinechin, Claude-Pierre Jeannerod, Vincent Lefevre, Guillaume Melquiond, Nathalie Revol, Damien Stehl√©, Serge Torres, 2018Springer</p>
<p>Emergent linear representations in world models of self-supervised sequence models. Neel Nanda, Andrew Lee, Martin Wattenberg, Proceedings of the 6th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP. the 6th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP2023</p>
<p>Codegen: An open large language model for code with multi-turn program synthesis. Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, Caiming Xiong, The Eleventh International Conference on Learning Representations. 2022</p>
<p>Gpt-4 technical report. 2023OpenAI</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in Neural Information Processing Systems. 202235</p>
<p>Does string-based neural mt learn source syntax?. Xing Shi, Inkit Padhi, Kevin Knight, Proceedings of the 2016 conference on empirical methods in natural language processing. the 2016 conference on empirical methods in natural language processing2016</p>
<p>A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis. Alessandro Stolfo, Yonatan Belinkov, Mrinmaya Sachan, Proceedings of the 2023 ference on Empirical Methods in Natural Language Processing. the 2023 ference on Empirical Methods in Natural Language Processing2023</p>
<p>Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, Stanford alpaca: An instruction-following llama model. 2023</p>
<p>What do you learn from context? probing for sentence structure in contextualized word representations. Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, Thomas Mccoy, Najoung Kim, Benjamin Van Durme, Dipanjan Samuel R Bowman, Das, International Conference on Learning Representations. 2018</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timoth√©e Lachaux, Baptiste Lacroix, Naman Rozi√®re, Eric Goyal, Hambro, arXiv:2302.13971Faisal Azhar, et al. 2023a. Llama: Open and efficient foundation language models. arXiv preprint</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.09288Llama 2: Open foundation and fine-tuned chat models. 2023barXiv preprint</p>
<p>Do nlp models know numbers? probing numeracy in embeddings. Eric Wallace, Yizhong Wang, Sujian Li, Sameer Singh, Matt Gardner, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)2019</p>
<p>Peiyi Wang, Lei Li, Zhihong Shao, Damai Xu, Yifei Dai, Deli Li, Chen, Zhifang Wu, Sui, arXiv:2312.08935Math-shepherd: A label-free step-by-step verifier for llms in mathematical reasoning. 2023arXiv preprint</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Ed H Quoc V Le, Sharan Chi, Aakanksha Narang, Denny Chowdhery, Zhou, The Eleventh International Conference on Learning Representations. 2022</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 202235</p>
<p>Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T Kwok, Zhenguo Li, Adrian Weller, Weiyang Liu, arXiv:2309.12284Metamath: Bootstrap your own mathematical questions for large language models. 2023arXiv preprint</p>
<p>Ruochen Zhao, Xingxuan Li, arXiv:2305.03268Shafiq Joty, Chengwei Qin, and Lidong Bing. 2023. Verify-and-edit: A knowledge-enhanced chain-of-thought framework. arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
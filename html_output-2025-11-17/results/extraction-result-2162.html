<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2162 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2162</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2162</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-57.html">extraction-schema-57</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <p><strong>Paper ID:</strong> paper-279070803</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.24785v1.pdf" target="_blank">EXP-Bench: Can AI Conduct AI Research Experiments?</a></p>
                <p><strong>Paper Abstract:</strong> Automating AI research holds immense potential for accelerating scientific progress, yet current AI agents struggle with the complexities of rigorous, end-to-end experimentation. We introduce EXP-Bench, a novel benchmark designed to systematically evaluate AI agents on complete research experiments sourced from influential AI publications. Given a research question and incomplete starter code, EXP-Bench challenges AI agents to formulate hypotheses, design and implement experimental procedures, execute them, and analyze results. To enable the creation of such intricate and authentic tasks with high-fidelity, we design a semi-autonomous pipeline to extract and structure crucial experimental details from these research papers and their associated open-source code. With the pipeline, EXP-Bench curated 461 AI research tasks from 51 top-tier AI research papers. Evaluations of leading LLM-based agents, such as OpenHands and IterativeAgent on EXP-Bench demonstrate partial capabilities: while scores on individual experimental aspects such as design or implementation correctness occasionally reach 20-35%, the success rate for complete, executable experiments was a mere 0.5%. By identifying these bottlenecks and providing realistic step-by-step experiment procedures, EXP-Bench serves as a vital tool for future AI agents to improve their ability to conduct AI research experiments. EXP-Bench is open-sourced at https://github.com/Just-Curieous/Curie/tree/main/benchmark/exp_bench.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2162.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2162.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EXP-Bench</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>EXP-Bench (Benchmark for end-to-end AI research experimentation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A semi-automated benchmark and dataset that evaluates LLM-driven agents on complete AI research experiment tasks (design, implementation, execution, conclusion) extracted from peer-reviewed AI papers and their codebases, using ground-truth comparisons and execution-based validation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>EXP-Bench</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>benchmark / evaluation platform</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general AI research experimentation</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Multi-component validation: (1) LLM-based judge (o3-mini) performing integrity checks (Monitor), design/conclusion semantic comparison, and implementation diff evaluation; (2) Code Execution Validator that runs agent-generated code in clean containerized environments and compares outputs to expected traces; (3) Ground-truth derived from original paper & repo (design variables, git-diff of required code changes, expected conclusion). Also uses AST tracing to extract implementation requirements and lightweight human oversight for final verification.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Not formalized as a numerical distance-from-training-data measure; novelty is operationalized implicitly via task difficulty (real-world research tasks from papers) and by using conjunctive metrics to filter outputs which are not grounded in the original (familiar) ground truth; no explicit embedding- or similarity-based novelty score reported.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Across 461 realistic research tasks (12,737 subtasks) agents scored generally low: individual-phase correctness (design, implementation, conclusion) often in the ~20–35% range for leading agents; complete, fully-correct end-to-end experiments were vanishingly rare (paper reports a 0.5% success rate for complete executable experiments in one summary, and execution-checked conjunctive accuracy falling to 0.2% on a stricter subset). Performance is substantially higher on routine/familiar procedures (e.g., running pre-written scripts or replicating documented analysis steps) and much worse on complex or novel experimental steps.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Validation pipeline produces strong filtering: initial monitor pass yields ≈20.6% average, adding design + conclusion drops to ≈3.7%, adding implementation drops to ≈0.4%, and finally adding execution validation yields ≈0.2% accuracy on the executed subset — demonstrating that stricter/combined validation dramatically reduces apparent success. No single scalar precision/recall reported; high variance noted especially for conclusion (C) and execution (E).</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Not reported as a single numeric rate; paper notes overestimation biases (false positives) especially for execution (E) because incorrect or mock implementations sometimes execute successfully and for conclusions (C) because agents produce plausible but unfounded conclusions. Conjunctive metrics were introduced to reduce these false positives.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Validation accuracy and reliability decrease as task novelty/complexity increases: agents fare relatively well on familiar/routine tasks but fail on novel or complex experimental requirements; applying stricter conjunctive validation (which better rules out spurious plausible outputs) causes scores to fall markedly, indicating validation is harsher and more discriminating for novel/unexpected outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Strong asymmetry observed: agents frequently generate superficially plausible designs/implementations/conclusions (yielding moderate partial scores) but fail to produce correct, executable, and grounded experiments when validated end-to-end; conjunctive metrics reveal a large gap between generation plausibility and validated correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Paper reports agents struggle on realistic, non-simplified tasks derived from real papers (treated as more 'out-of-distribution' relative to simplified benchmarks). Quantitatively, overall metric scores are below 30% across models, with many end-to-end tasks near 0%; RL tasks were an exception where some agents reached implementation correctness ≈41% in that category, indicating performance depends strongly on subdomain and how similar tasks are to agent's prior experience.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not explicitly quantified. Authors note high variance in judge metrics and that agents can produce confident but unfounded conclusions; calibration likely degrades for novel/complex tasks, motivating conjunctive metrics and execution checks.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>Validation is expensive: execution-based validation (running experiments in containers) is time- and compute-intensive and was only performed on a subset of runs. Cost/time per task (LLM token cost only) varied by model; agents were allowed up to 40 minutes per task. Average token-costs and runtimes are reported by agent type in Appendix (examples: OH+o3-mini had favorable cost-time tradeoff). Exact total compute for full validation not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Conjunctive metrics (e.g., C•D, I•E), LLM-based Monitor to detect faked data, code execution validation in containerized environments, AST tracing to extract expected implementation requirements, and lightweight human oversight during curation/verification.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>EXP-Bench demonstrates a substantial generation-vs-validation gap: agents often produce plausible partial outputs but fail under combined, execution-checked validation; stricter, compositional evaluation (design+implementation+execution) reveals brittleness and very low end-to-end success rates, especially on realistic/novel experimental tasks.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2162.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2162.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OpenHands (OH)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenHands (code-generation / research agent pipeline referenced as OpenHands)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A top-performing code generation agent configuration (used with various backbone LLMs) evaluated on EXP-Bench; tends to produce plausible design and implementation text but sometimes does not actually execute experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>OpenHands (OH)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-driven code-generation agent / agent orchestration system</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general AI research experiments (multiple ML subfields)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>Generates experimental designs, reconstructs or writes code implementations, generates instructions and conclusions for research tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Outputs are judged by the benchmark's LLM-based judge (integrity Monitor + design/conclusion evaluation prompts), and by the Code Execution Validator which attempts to execute agent-produced code modifications in a clean container; correctness assessed against ground-truth design variables, git-diff of expected code edits, and expected experimental outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>OH-family agents achieved moderate partial scores (individual-phase correctness often in 20–35% ranges for design/implementation) but very low end-to-end executable experiment success. OH+o3-mini was the top-ranked configuration by All•E✓ in the paper; however, full end-to-end All✓ performance was ≈1.4% for the best OH combo and overall complete executable success was ≈0.5% across agents.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Validation revealed many generated outputs were not grounded: monitor checks passed some OH runs but conjunctive evaluation (adding implementation and execution) caused scores to collapse to near-zero; the paper highlights that OH often produced plausible responses without running experiments, inflating partial metrics until execution checks were applied.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>OH models showed higher partial credit on routine or familiar tasks (e.g., re-running scripted analyses) but notably worse validated performance on novel/complex experimental tasks — indicating validation catches many spurious or incomplete OH generations on novel tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Yes — OH tends to generate plausible-looking outputs (higher design/implementation textual scores) but fails when validation requires executable, complete implementations and grounded conclusions.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Degraded: OH performed better on routine/familiar scripts and worse on tasks requiring operationalization of high-level methods into new code paths or uncommon preprocessing steps; specific numeric OOD comparisons not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>OH configurations exhibited a favorable cost-time tradeoff (example: OH+o3-mini low cost and moderate time), but specific validation costs varied by model; no per-validation-cost breakdown beyond reported average token costs and runtimes.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Use of execution validators and monitor checks in EXP-Bench to reveal and reduce spurious OH outputs; authors suggest training/architectural changes and reinforcement learning with verifiable rewards as future directions.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>OpenHands often yields plausible textual solutions but fails to produce complete, executable, and correct experiments at scale; execution-based validation and conjunctive scoring reveal large gaps between generated plausibility and validated correctness, especially on novel/complex tasks.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2162.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2162.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IterativeAgent (IA)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>IterativeAgent (agent configuration used to reduce early task stopping)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative research agent configuration evaluated in EXP-Bench that tends to run longer on tasks (consuming more time) but with lower overall validated effectiveness on end-to-end experiments compared to some OpenHands configurations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>IterativeAgent (IA)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-driven multi-step agent / iterative orchestration</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general AI research experiments</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>Generates iterative experimental plans, modifies code, and attempts to run experiments; designed to reduce early stopping.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Same EXP-Bench judge pipeline: Monitor integrity checks, LLM-based semantic design/conclusion evaluation, and Code Execution Validator for executability against ground-truth scripts and outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>IA agents frequently consumed the full allotted runtime (40 min) and produced lower validated correctness than some OH configurations; overall partial scores were low and end-to-end validated success rates were near zero for many IA+LLM combos (e.g., IA+Nova Pro worst-performing in Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>IA often ran experiments (longer runtime) but still failed many validation checks (environment/dependency errors, missing components), leading to low execution-checked correctness; precise numeric validation rates per IA config are in paper tables but overall validated end-to-end success remains near zero.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>IA's longer runtime did not translate to better validated performance on novel tasks; like OH, IA also struggled more on novel/complex tasks than on routine ones.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Yes — despite running more code/attempting more executions, IA still demonstrates a gap between generated attempts and validated correctness, often failing environment/setup and implementation-specific checks.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Poor on realistic/out-of-distribution research tasks; specific numeric OOD vs in-distribution comparisons are not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>IA runs tended to be longer and therefore costlier (some IA runs exceeded intended time limits), but the paper reports no precise per-validation cost/compute ratio beyond aggregate statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Execution-based validation, integrity monitor, and improved environment setup automation are suggested; paper suggests reinforcement learning with verifiable rewards as future work to improve autonomous navigation of research lifecycle.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>IterativeAgent runs longer and attempts more execution but still fails many execution and implementation validations; longer runtime alone does not close the generation-validation gap on novel, realistic research tasks.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2162.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2162.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>o3-mini (LLM-as-judge)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>o3-mini (LLM used as a judge/evaluator)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A small/medium LLM (o3-mini) used in EXP-Bench as the LLM-based judge to perform integrity monitoring, design/conclusion semantic evaluation, and to assist implementation evaluation by inspecting git diffs and logs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>o3-mini (LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>large language model (used as automated judge)</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>evaluation / scientific output validation</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>Not used to generate scientific discoveries here; used to judge and classify agent outputs, parse logs, and produce structured JSON evaluation results.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Performs Monitor integrity checks (paper access, git operations, faked data), segments long inputs for iterative evaluation, compares design_output to design_ground_truth for D metric, compares conclusion_output to conclusion_ground_truth for C, and helps evaluate implementation diffs for I; used in combination with a Code Execution Validator for executability (E).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Used as the principal automated judge; the paper notes the judge has limitations (variance in metrics like C and E) and uses conjunctive metrics to reduce over-crediting; no numeric judge accuracy vs human gold-standard reported in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Judge metrics show high variance especially for novel or superficially plausible-but-ungrounded outputs (conclusions that appear correct but lack experimental foundation); judge alone can over-credit unless execution checks are used.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>N/A (judge role), but the judge can be deceived by plausible textual outputs; hence the need for execution validators and Monitor checks.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>Judge uses token compute; segmentation used to handle long diffs/logs to fit model context windows. Exact costs reported in appendix but not normalized to validation complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Combining LLM-based judge with concrete execution checks, integrity Monitor, and conjunctive scoring to reduce false crediting.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLM-as-judge is useful for semantic and structural checks but exhibits variance and can over-credit plausible but ungrounded outputs; pairing with execution validation and conjunctive metrics strengthens reliability.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2162.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2162.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>The AI Scientist (Lu et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>The AI Scientist (system described by Lu et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior fully automated system reported in related work that aims to conduct research end-to-end via multiple LLM agents that coordinate to define problems, review literature, synthesize experiments, and execute them.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The AI Scientist: Towards fully automated open-ended scientific discovery</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>The AI Scientist</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>multi-agent LLM orchestration system</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>automated scientific discovery (general; reported focus on commonsense domains)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>Generates research problems, literature syntheses, hypotheses, and executes experiments (claimed end-to-end automation).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Reported to have limited automated evaluation in the cited work; evaluation focused on commonsense domains and did not provide extensive automated, domain-general execution-based validation as EXP-Bench does.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Not quantified in EXP-Bench text beyond the cited paper's claims; the EXP-Bench authors note the prior system's evaluation was limited and not fully automated across rigorous experimental validation on complex research tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Reported as limited in the original cited work (per EXP-Bench discussion); no numeric accuracy rates provided in EXP-Bench.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Cited as being primarily validated in commonsense domains (less representative of hard, novel scientific experimentation), suggesting potential degradation on novel scientific tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Cited work appears to have a less rigorous automated validation pipeline, implying potential asymmetry that was not fully evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Multi-agent orchestration for end-to-end automation; however EXP-Bench indicates the need for stronger execution-grounded validation and conjunctive metrics beyond what The AI Scientist reported.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Prior fully automated attempts (e.g., The AI Scientist) exist but have limited automated validation on rigorous, domain-general experimental tasks; EXP-Bench addresses this gap with execution-based ground-truth checks and conjunctive evaluation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2162.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2162.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AI Co-scientist (Gottweis et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AI Co-scientist (system described by Gottweis et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior effort to build an AI collaborator for scientists (built on Gemini 2.0 per citation) that focuses on scaling hypothesis generation and research proposal quality, validated mainly in biomedical domains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AI Co-scientist</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-based co-pilot / assistant system</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>biomedical research (primary validation area)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>Generates high-quality hypotheses and research proposals, and assists literature synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Validated mainly in biomedical areas; relied on domain-specific evaluation and likely human/expert assessment rather than large-scale execution-grounded validation across diverse experimental codebases.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Reported validation concentrated in biomedical domains; extrapolation to diverse, novel AI experimentation tasks is uncertain.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Scaling of compute during inference and multi-agent orchestration to improve hypothesis quality; paper (EXP-Bench) suggests domain-specific validation limits generality.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>AI Co-scientist demonstrates promising hypothesis generation in domain-specific settings (biomedicine) but is validated primarily with domain-specific or human-in-the-loop processes, lacking broad execution-grounded validation across diverse scientific codebases.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2162.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2162.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLMs for scientific discovery (general)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large Language Models applied to hypothesis and idea generation for scientific discovery</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A class of methods that use LLMs to generate domain-specific or commonsense hypotheses, research ideas, and sometimes code to run experiments; cited works include Baek et al., Wang et al., Yang et al., and others.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLM-based hypothesis generation systems (general class)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>large language models / LLM-driven pipelines</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>scientific discovery across domains (commonsense, domain-specific research such as biology, chemistry, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>Generates research hypotheses, research ideas, experimental designs, and sometimes code for experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Varies by work: ranges from expert/human review, domain-specific constrained experimental pipelines, limited automated checks, to ad-hoc prompting optimizations; EXP-Bench highlights that many prior efforts lack rigorous automated, execution-based validation across diverse, realistic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Often implicit (novelty judged by human experts or by whether a hypothesis departs from known examples); EXP-Bench notes that existing works either use domain-specific prompting or human/expert validation rather than robust, general novelty metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Prior works show LLMs can produce novel hypotheses and ideas, but their end-to-end experimental validation (i.e., producing executable, reproducible, and correct experiments demonstrating the hypothesis) is limited; precise numeric success rates not provided in EXP-Bench.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Heterogeneous and often limited; many pipelines rely on human verification or domain-specific evaluation and do not demonstrate broad execution-grounded validation on varied codebases as EXP-Bench requires.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Validation is generally weaker for more novel outputs because prior work tends to validate within constrained or familiar domains; EXP-Bench argues for systematic execution-grounded checks to detect ungrounded novel outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Common — generation of novel-sounding hypotheses is easier than automated, reproducible validation via executable experiments; EXP-Bench provides evidence that generative capability outpaces reliable automated validation.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not systematically quantified; prior works often restrict domains to reduce OOD issues, implying degraded OOD performance remains a concern.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>Varies; execution-grounded validation (when present) is expensive — EXP-Bench quantifies that execution checks are time- and compute-intensive and were applied to only a subset of runs.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Combining LLM generation with code-execution validators, human-in-the-loop review, domain-specific pipelines, and task-specific prompting; EXP-Bench suggests conjunctive metrics and execution checks as system-level mechanisms to reduce spurious validation.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLMs can generate plausible hypotheses and research ideas, but existing pipelines often lack robust, large-scale automated execution-grounded validation; EXP-Bench shows that without rigorous executable checks and compositional scoring, generation quality overstates validated scientific correctness, especially for novel tasks.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>The AI Scientist: Towards fully automated open-ended scientific discovery <em>(Rating: 2)</em></li>
                <li>ScienceAgentBench: Toward rigorous assessment of language agents for data-driven scientific discovery <em>(Rating: 2)</em></li>
                <li>DiscoveryBench: Towards data-driven discovery with large language models <em>(Rating: 2)</em></li>
                <li>AI Co-scientist <em>(Rating: 2)</em></li>
                <li>Autonomous chemical research with large language models <em>(Rating: 2)</em></li>
                <li>Researchagent: Iterative research idea generation over scientific literature with large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2162",
    "paper_id": "paper-279070803",
    "extraction_schema_id": "extraction-schema-57",
    "extracted_data": [
        {
            "name_short": "EXP-Bench",
            "name_full": "EXP-Bench (Benchmark for end-to-end AI research experimentation)",
            "brief_description": "A semi-automated benchmark and dataset that evaluates LLM-driven agents on complete AI research experiment tasks (design, implementation, execution, conclusion) extracted from peer-reviewed AI papers and their codebases, using ground-truth comparisons and execution-based validation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "EXP-Bench",
            "system_type": "benchmark / evaluation platform",
            "domain": "general AI research experimentation",
            "generation_capability": null,
            "validation_method": "Multi-component validation: (1) LLM-based judge (o3-mini) performing integrity checks (Monitor), design/conclusion semantic comparison, and implementation diff evaluation; (2) Code Execution Validator that runs agent-generated code in clean containerized environments and compares outputs to expected traces; (3) Ground-truth derived from original paper & repo (design variables, git-diff of required code changes, expected conclusion). Also uses AST tracing to extract implementation requirements and lightweight human oversight for final verification.",
            "novelty_measure": "Not formalized as a numerical distance-from-training-data measure; novelty is operationalized implicitly via task difficulty (real-world research tasks from papers) and by using conjunctive metrics to filter outputs which are not grounded in the original (familiar) ground truth; no explicit embedding- or similarity-based novelty score reported.",
            "generation_performance": "Across 461 realistic research tasks (12,737 subtasks) agents scored generally low: individual-phase correctness (design, implementation, conclusion) often in the ~20–35% range for leading agents; complete, fully-correct end-to-end experiments were vanishingly rare (paper reports a 0.5% success rate for complete executable experiments in one summary, and execution-checked conjunctive accuracy falling to 0.2% on a stricter subset). Performance is substantially higher on routine/familiar procedures (e.g., running pre-written scripts or replicating documented analysis steps) and much worse on complex or novel experimental steps.",
            "validation_performance": "Validation pipeline produces strong filtering: initial monitor pass yields ≈20.6% average, adding design + conclusion drops to ≈3.7%, adding implementation drops to ≈0.4%, and finally adding execution validation yields ≈0.2% accuracy on the executed subset — demonstrating that stricter/combined validation dramatically reduces apparent success. No single scalar precision/recall reported; high variance noted especially for conclusion (C) and execution (E).",
            "false_positive_rate": "Not reported as a single numeric rate; paper notes overestimation biases (false positives) especially for execution (E) because incorrect or mock implementations sometimes execute successfully and for conclusions (C) because agents produce plausible but unfounded conclusions. Conjunctive metrics were introduced to reduce these false positives.",
            "false_negative_rate": null,
            "novelty_effect_on_validation": "Validation accuracy and reliability decrease as task novelty/complexity increases: agents fare relatively well on familiar/routine tasks but fail on novel or complex experimental requirements; applying stricter conjunctive validation (which better rules out spurious plausible outputs) causes scores to fall markedly, indicating validation is harsher and more discriminating for novel/unexpected outputs.",
            "generation_validation_asymmetry": "Strong asymmetry observed: agents frequently generate superficially plausible designs/implementations/conclusions (yielding moderate partial scores) but fail to produce correct, executable, and grounded experiments when validated end-to-end; conjunctive metrics reveal a large gap between generation plausibility and validated correctness.",
            "out_of_distribution_performance": "Paper reports agents struggle on realistic, non-simplified tasks derived from real papers (treated as more 'out-of-distribution' relative to simplified benchmarks). Quantitatively, overall metric scores are below 30% across models, with many end-to-end tasks near 0%; RL tasks were an exception where some agents reached implementation correctness ≈41% in that category, indicating performance depends strongly on subdomain and how similar tasks are to agent's prior experience.",
            "calibration_quality": "Not explicitly quantified. Authors note high variance in judge metrics and that agents can produce confident but unfounded conclusions; calibration likely degrades for novel/complex tasks, motivating conjunctive metrics and execution checks.",
            "validation_computational_cost": "Validation is expensive: execution-based validation (running experiments in containers) is time- and compute-intensive and was only performed on a subset of runs. Cost/time per task (LLM token cost only) varied by model; agents were allowed up to 40 minutes per task. Average token-costs and runtimes are reported by agent type in Appendix (examples: OH+o3-mini had favorable cost-time tradeoff). Exact total compute for full validation not provided.",
            "human_validation_required": true,
            "gap_closing_mechanisms": "Conjunctive metrics (e.g., C•D, I•E), LLM-based Monitor to detect faked data, code execution validation in containerized environments, AST tracing to extract expected implementation requirements, and lightweight human oversight during curation/verification.",
            "evidence_type": "supports",
            "key_findings": "EXP-Bench demonstrates a substantial generation-vs-validation gap: agents often produce plausible partial outputs but fail under combined, execution-checked validation; stricter, compositional evaluation (design+implementation+execution) reveals brittleness and very low end-to-end success rates, especially on realistic/novel experimental tasks.",
            "uuid": "e2162.0"
        },
        {
            "name_short": "OpenHands (OH)",
            "name_full": "OpenHands (code-generation / research agent pipeline referenced as OpenHands)",
            "brief_description": "A top-performing code generation agent configuration (used with various backbone LLMs) evaluated on EXP-Bench; tends to produce plausible design and implementation text but sometimes does not actually execute experiments.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "OpenHands (OH)",
            "system_type": "LLM-driven code-generation agent / agent orchestration system",
            "domain": "general AI research experiments (multiple ML subfields)",
            "generation_capability": "Generates experimental designs, reconstructs or writes code implementations, generates instructions and conclusions for research tasks.",
            "validation_method": "Outputs are judged by the benchmark's LLM-based judge (integrity Monitor + design/conclusion evaluation prompts), and by the Code Execution Validator which attempts to execute agent-produced code modifications in a clean container; correctness assessed against ground-truth design variables, git-diff of expected code edits, and expected experimental outputs.",
            "novelty_measure": null,
            "generation_performance": "OH-family agents achieved moderate partial scores (individual-phase correctness often in 20–35% ranges for design/implementation) but very low end-to-end executable experiment success. OH+o3-mini was the top-ranked configuration by All•E✓ in the paper; however, full end-to-end All✓ performance was ≈1.4% for the best OH combo and overall complete executable success was ≈0.5% across agents.",
            "validation_performance": "Validation revealed many generated outputs were not grounded: monitor checks passed some OH runs but conjunctive evaluation (adding implementation and execution) caused scores to collapse to near-zero; the paper highlights that OH often produced plausible responses without running experiments, inflating partial metrics until execution checks were applied.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "OH models showed higher partial credit on routine or familiar tasks (e.g., re-running scripted analyses) but notably worse validated performance on novel/complex experimental tasks — indicating validation catches many spurious or incomplete OH generations on novel tasks.",
            "generation_validation_asymmetry": "Yes — OH tends to generate plausible-looking outputs (higher design/implementation textual scores) but fails when validation requires executable, complete implementations and grounded conclusions.",
            "out_of_distribution_performance": "Degraded: OH performed better on routine/familiar scripts and worse on tasks requiring operationalization of high-level methods into new code paths or uncommon preprocessing steps; specific numeric OOD comparisons not provided.",
            "calibration_quality": null,
            "validation_computational_cost": "OH configurations exhibited a favorable cost-time tradeoff (example: OH+o3-mini low cost and moderate time), but specific validation costs varied by model; no per-validation-cost breakdown beyond reported average token costs and runtimes.",
            "human_validation_required": true,
            "gap_closing_mechanisms": "Use of execution validators and monitor checks in EXP-Bench to reveal and reduce spurious OH outputs; authors suggest training/architectural changes and reinforcement learning with verifiable rewards as future directions.",
            "evidence_type": "supports",
            "key_findings": "OpenHands often yields plausible textual solutions but fails to produce complete, executable, and correct experiments at scale; execution-based validation and conjunctive scoring reveal large gaps between generated plausibility and validated correctness, especially on novel/complex tasks.",
            "uuid": "e2162.1"
        },
        {
            "name_short": "IterativeAgent (IA)",
            "name_full": "IterativeAgent (agent configuration used to reduce early task stopping)",
            "brief_description": "An iterative research agent configuration evaluated in EXP-Bench that tends to run longer on tasks (consuming more time) but with lower overall validated effectiveness on end-to-end experiments compared to some OpenHands configurations.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "IterativeAgent (IA)",
            "system_type": "LLM-driven multi-step agent / iterative orchestration",
            "domain": "general AI research experiments",
            "generation_capability": "Generates iterative experimental plans, modifies code, and attempts to run experiments; designed to reduce early stopping.",
            "validation_method": "Same EXP-Bench judge pipeline: Monitor integrity checks, LLM-based semantic design/conclusion evaluation, and Code Execution Validator for executability against ground-truth scripts and outputs.",
            "novelty_measure": null,
            "generation_performance": "IA agents frequently consumed the full allotted runtime (40 min) and produced lower validated correctness than some OH configurations; overall partial scores were low and end-to-end validated success rates were near zero for many IA+LLM combos (e.g., IA+Nova Pro worst-performing in Table 1).",
            "validation_performance": "IA often ran experiments (longer runtime) but still failed many validation checks (environment/dependency errors, missing components), leading to low execution-checked correctness; precise numeric validation rates per IA config are in paper tables but overall validated end-to-end success remains near zero.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "IA's longer runtime did not translate to better validated performance on novel tasks; like OH, IA also struggled more on novel/complex tasks than on routine ones.",
            "generation_validation_asymmetry": "Yes — despite running more code/attempting more executions, IA still demonstrates a gap between generated attempts and validated correctness, often failing environment/setup and implementation-specific checks.",
            "out_of_distribution_performance": "Poor on realistic/out-of-distribution research tasks; specific numeric OOD vs in-distribution comparisons are not reported.",
            "calibration_quality": null,
            "validation_computational_cost": "IA runs tended to be longer and therefore costlier (some IA runs exceeded intended time limits), but the paper reports no precise per-validation cost/compute ratio beyond aggregate statistics.",
            "human_validation_required": true,
            "gap_closing_mechanisms": "Execution-based validation, integrity monitor, and improved environment setup automation are suggested; paper suggests reinforcement learning with verifiable rewards as future work to improve autonomous navigation of research lifecycle.",
            "evidence_type": "supports",
            "key_findings": "IterativeAgent runs longer and attempts more execution but still fails many execution and implementation validations; longer runtime alone does not close the generation-validation gap on novel, realistic research tasks.",
            "uuid": "e2162.2"
        },
        {
            "name_short": "o3-mini (LLM-as-judge)",
            "name_full": "o3-mini (LLM used as a judge/evaluator)",
            "brief_description": "A small/medium LLM (o3-mini) used in EXP-Bench as the LLM-based judge to perform integrity monitoring, design/conclusion semantic evaluation, and to assist implementation evaluation by inspecting git diffs and logs.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "o3-mini (LLM)",
            "system_type": "large language model (used as automated judge)",
            "domain": "evaluation / scientific output validation",
            "generation_capability": "Not used to generate scientific discoveries here; used to judge and classify agent outputs, parse logs, and produce structured JSON evaluation results.",
            "validation_method": "Performs Monitor integrity checks (paper access, git operations, faked data), segments long inputs for iterative evaluation, compares design_output to design_ground_truth for D metric, compares conclusion_output to conclusion_ground_truth for C, and helps evaluate implementation diffs for I; used in combination with a Code Execution Validator for executability (E).",
            "novelty_measure": null,
            "generation_performance": null,
            "validation_performance": "Used as the principal automated judge; the paper notes the judge has limitations (variance in metrics like C and E) and uses conjunctive metrics to reduce over-crediting; no numeric judge accuracy vs human gold-standard reported in main text.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "Judge metrics show high variance especially for novel or superficially plausible-but-ungrounded outputs (conclusions that appear correct but lack experimental foundation); judge alone can over-credit unless execution checks are used.",
            "generation_validation_asymmetry": "N/A (judge role), but the judge can be deceived by plausible textual outputs; hence the need for execution validators and Monitor checks.",
            "out_of_distribution_performance": null,
            "calibration_quality": null,
            "validation_computational_cost": "Judge uses token compute; segmentation used to handle long diffs/logs to fit model context windows. Exact costs reported in appendix but not normalized to validation complexity.",
            "human_validation_required": true,
            "gap_closing_mechanisms": "Combining LLM-based judge with concrete execution checks, integrity Monitor, and conjunctive scoring to reduce false crediting.",
            "evidence_type": "mixed",
            "key_findings": "LLM-as-judge is useful for semantic and structural checks but exhibits variance and can over-credit plausible but ungrounded outputs; pairing with execution validation and conjunctive metrics strengthens reliability.",
            "uuid": "e2162.3"
        },
        {
            "name_short": "The AI Scientist (Lu et al.)",
            "name_full": "The AI Scientist (system described by Lu et al.)",
            "brief_description": "A prior fully automated system reported in related work that aims to conduct research end-to-end via multiple LLM agents that coordinate to define problems, review literature, synthesize experiments, and execute them.",
            "citation_title": "The AI Scientist: Towards fully automated open-ended scientific discovery",
            "mention_or_use": "mention",
            "system_name": "The AI Scientist",
            "system_type": "multi-agent LLM orchestration system",
            "domain": "automated scientific discovery (general; reported focus on commonsense domains)",
            "generation_capability": "Generates research problems, literature syntheses, hypotheses, and executes experiments (claimed end-to-end automation).",
            "validation_method": "Reported to have limited automated evaluation in the cited work; evaluation focused on commonsense domains and did not provide extensive automated, domain-general execution-based validation as EXP-Bench does.",
            "novelty_measure": null,
            "generation_performance": "Not quantified in EXP-Bench text beyond the cited paper's claims; the EXP-Bench authors note the prior system's evaluation was limited and not fully automated across rigorous experimental validation on complex research tasks.",
            "validation_performance": "Reported as limited in the original cited work (per EXP-Bench discussion); no numeric accuracy rates provided in EXP-Bench.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "Cited as being primarily validated in commonsense domains (less representative of hard, novel scientific experimentation), suggesting potential degradation on novel scientific tasks.",
            "generation_validation_asymmetry": "Cited work appears to have a less rigorous automated validation pipeline, implying potential asymmetry that was not fully evaluated.",
            "out_of_distribution_performance": null,
            "calibration_quality": null,
            "validation_computational_cost": null,
            "human_validation_required": null,
            "gap_closing_mechanisms": "Multi-agent orchestration for end-to-end automation; however EXP-Bench indicates the need for stronger execution-grounded validation and conjunctive metrics beyond what The AI Scientist reported.",
            "evidence_type": "mixed",
            "key_findings": "Prior fully automated attempts (e.g., The AI Scientist) exist but have limited automated validation on rigorous, domain-general experimental tasks; EXP-Bench addresses this gap with execution-based ground-truth checks and conjunctive evaluation.",
            "uuid": "e2162.4"
        },
        {
            "name_short": "AI Co-scientist (Gottweis et al.)",
            "name_full": "AI Co-scientist (system described by Gottweis et al.)",
            "brief_description": "A prior effort to build an AI collaborator for scientists (built on Gemini 2.0 per citation) that focuses on scaling hypothesis generation and research proposal quality, validated mainly in biomedical domains.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "AI Co-scientist",
            "system_type": "LLM-based co-pilot / assistant system",
            "domain": "biomedical research (primary validation area)",
            "generation_capability": "Generates high-quality hypotheses and research proposals, and assists literature synthesis.",
            "validation_method": "Validated mainly in biomedical areas; relied on domain-specific evaluation and likely human/expert assessment rather than large-scale execution-grounded validation across diverse experimental codebases.",
            "novelty_measure": null,
            "generation_performance": null,
            "validation_performance": null,
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "Reported validation concentrated in biomedical domains; extrapolation to diverse, novel AI experimentation tasks is uncertain.",
            "generation_validation_asymmetry": null,
            "out_of_distribution_performance": null,
            "calibration_quality": null,
            "validation_computational_cost": null,
            "human_validation_required": true,
            "gap_closing_mechanisms": "Scaling of compute during inference and multi-agent orchestration to improve hypothesis quality; paper (EXP-Bench) suggests domain-specific validation limits generality.",
            "evidence_type": "mixed",
            "key_findings": "AI Co-scientist demonstrates promising hypothesis generation in domain-specific settings (biomedicine) but is validated primarily with domain-specific or human-in-the-loop processes, lacking broad execution-grounded validation across diverse scientific codebases.",
            "uuid": "e2162.5"
        },
        {
            "name_short": "LLMs for scientific discovery (general)",
            "name_full": "Large Language Models applied to hypothesis and idea generation for scientific discovery",
            "brief_description": "A class of methods that use LLMs to generate domain-specific or commonsense hypotheses, research ideas, and sometimes code to run experiments; cited works include Baek et al., Wang et al., Yang et al., and others.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "LLM-based hypothesis generation systems (general class)",
            "system_type": "large language models / LLM-driven pipelines",
            "domain": "scientific discovery across domains (commonsense, domain-specific research such as biology, chemistry, etc.)",
            "generation_capability": "Generates research hypotheses, research ideas, experimental designs, and sometimes code for experiments.",
            "validation_method": "Varies by work: ranges from expert/human review, domain-specific constrained experimental pipelines, limited automated checks, to ad-hoc prompting optimizations; EXP-Bench highlights that many prior efforts lack rigorous automated, execution-based validation across diverse, realistic tasks.",
            "novelty_measure": "Often implicit (novelty judged by human experts or by whether a hypothesis departs from known examples); EXP-Bench notes that existing works either use domain-specific prompting or human/expert validation rather than robust, general novelty metrics.",
            "generation_performance": "Prior works show LLMs can produce novel hypotheses and ideas, but their end-to-end experimental validation (i.e., producing executable, reproducible, and correct experiments demonstrating the hypothesis) is limited; precise numeric success rates not provided in EXP-Bench.",
            "validation_performance": "Heterogeneous and often limited; many pipelines rely on human verification or domain-specific evaluation and do not demonstrate broad execution-grounded validation on varied codebases as EXP-Bench requires.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "Validation is generally weaker for more novel outputs because prior work tends to validate within constrained or familiar domains; EXP-Bench argues for systematic execution-grounded checks to detect ungrounded novel outputs.",
            "generation_validation_asymmetry": "Common — generation of novel-sounding hypotheses is easier than automated, reproducible validation via executable experiments; EXP-Bench provides evidence that generative capability outpaces reliable automated validation.",
            "out_of_distribution_performance": "Not systematically quantified; prior works often restrict domains to reduce OOD issues, implying degraded OOD performance remains a concern.",
            "calibration_quality": null,
            "validation_computational_cost": "Varies; execution-grounded validation (when present) is expensive — EXP-Bench quantifies that execution checks are time- and compute-intensive and were applied to only a subset of runs.",
            "human_validation_required": true,
            "gap_closing_mechanisms": "Combining LLM generation with code-execution validators, human-in-the-loop review, domain-specific pipelines, and task-specific prompting; EXP-Bench suggests conjunctive metrics and execution checks as system-level mechanisms to reduce spurious validation.",
            "evidence_type": "supports",
            "key_findings": "LLMs can generate plausible hypotheses and research ideas, but existing pipelines often lack robust, large-scale automated execution-grounded validation; EXP-Bench shows that without rigorous executable checks and compositional scoring, generation quality overstates validated scientific correctness, especially for novel tasks.",
            "uuid": "e2162.6"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "The AI Scientist: Towards fully automated open-ended scientific discovery",
            "rating": 2
        },
        {
            "paper_title": "ScienceAgentBench: Toward rigorous assessment of language agents for data-driven scientific discovery",
            "rating": 2
        },
        {
            "paper_title": "DiscoveryBench: Towards data-driven discovery with large language models",
            "rating": 2
        },
        {
            "paper_title": "AI Co-scientist",
            "rating": 2
        },
        {
            "paper_title": "Autonomous chemical research with large language models",
            "rating": 2
        },
        {
            "paper_title": "Researchagent: Iterative research idea generation over scientific literature with large language models",
            "rating": 1
        }
    ],
    "cost": 0.020299249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>EXP-Bench: Can AI Conduct AI Research Experiments?
2 Jun 2025</p>
<p>Patrick Tser 
Jern Kon 
University of Michigan</p>
<p>Jiachen Liu 
University of Michigan</p>
<p>Xinyi Zhu 
University of Michigan</p>
<p>Qiuyi Ding 
University of Michigan</p>
<p>Jingjia Peng 
University of Michigan</p>
<p>Jiarong Xing 
Rice University</p>
<p>Yibo Huang 
University of Michigan</p>
<p>Yiming Qiu 
University of Michigan</p>
<p>Jayanth Srinivasa 
Myungjin Lee 
Mosharaf Chowdhury 
University of Michigan</p>
<p>Matei Zaharia 
Ang Chen 
University of Michigan</p>
<p>Cisco Research 
U C Berkeley 
EXP-Bench: Can AI Conduct AI Research Experiments?
2 Jun 2025B9699FEFF5D099F3360C039BC770C16EarXiv:2505.24785v2[cs.AI]
Automating AI research holds immense potential for accelerating scientific progress, yet current AI agents struggle with the complexities of rigorous, endto-end experimentation.We introduce EXP-Bench, a novel benchmark designed to systematically evaluate AI agents on complete research experiments sourced from influential AI publications.Given a research question and incomplete starter code, EXP-Bench challenges AI agents to formulate hypotheses, design and implement experimental procedures, execute them, and analyze results.To enable the creation of such intricate and authentic tasks with high-fidelity, we design a semi-autonomous pipeline to extract and structure crucial experimental details from these research papers and their associated open-source code.With the pipeline, EXP-Bench curated 461 AI research tasks from 51 top-tier AI research papers.Evaluations of leading AI agents, such as OpenHands [87] and IterativeAgent [76] on EXP-Bench demonstrate partial capabilities: while scores on individual experimental aspects such as design or implementation correctness reach 20-35%, the success rate for complete, executable experiments was a mere 0.5%.By identifying these bottlenecks and providing realistic step-by-step experiment procedures, EXP-Bench serves as a vital tool for future AI agents to improve their ability to conduct AI research experiments.EXP-Bench is open-sourced at https://github.com/Input Impl.:</p>
<p>Perf.</p>
<p>Figure 1: EXP-Bench evaluates AI agents on research experiment tasks extracted semi-autonomously from peer-reviewed AI papers.Given a research question, a high-level method description, and starter code, agents are tasked with designing, implementing, and executing complete experiments.</p>
<p>Performance is validated through ground-truth comparisons and implementation execution.</p>
<p>Automating AI research stands as a cornerstone for accelerating the development of advanced intelligence and human progress.Unlike disciplines that require extensive physical interaction, AI research is inherently digital, rendering it particularly amenable to automation by Large Language Model (LLM)-driven AI agents.Recent work has demonstrated that these agents demonstrate nascent capabilities in tasks like literature synthesis [23], hypothesis generation [91] and code generation [53].However, empirical AI research requires rigorous end-to-end experimentation, which goes beyond these individual tasks.</p>
<p>To realize the vision of agents conducting holistic AI research, a rigorous benchmark is needed-one that evaluates and guides agents through the full experimentation pipeline step by step.We present EXP-Bench, a benchmark designed to comprehensively assess an AI agent's ability to carry out end-to-end research experiments.As illustrated in Fig. 1, EXP-Bench challenges agents with tasks sourced from influential, peer-reviewed AI publications (e.g., NeurIPS, ICLR) along with their open-source implementations.These papers reflect already-completed, peer-validated research and serve as concrete exemplars of full experimental workflows.By exposing agents to such tasks, we test their ability to conduct established scientific procedures grounded in real-world AI experimentation.</p>
<p>For each task, an agent is provided with a core research question, a high-level methodological overview, and starter code.The agent should then formulate viable hypotheses, design AI-specific experimental procedures (e.g., data handling, model selection, and hyperparameter optimization), correctly implement and execute these experiments, and derive valid conclusions from the results.</p>
<p>However, curating these high-fidelity and structured experimental tasks presents considerable challenges.Academic papers typically present a polished narrative focusing on final results and conclusions, often omitting the detailed intermediate steps of the experimentation process.Additionally, critical details-such as the precise conditions under which results hold or subtle data preprocessing steps-are often fragmented across multiple sources, including dense academic papers, supplementary materials, and sprawling codebases.This necessitates deep domain expertise for accurate interpretation and makes manual curation of such tasks labor-intensive and difficult to scale.</p>
<p>To address these challenges, we develop a semi-automated dataset curation pipeline.We first filter for high-quality AI papers with open-source codebases using citation and repository popularity signals.Task extraction then proceeds in two stages: (1) a multi-modal extraction phase that identifies the core elements of the research problem-such as the main question, expected outcomes, and high-level experimental setup (e.g., datasets, evaluation metrics, model configurations)-from papers, supplementary materials, and code; and (2) an implementation extraction phase that locates relevant code and assembles scripts to solve the specified task.We further apply execution-based validation to ensure functionality.While human oversight is used, the availability of original implementations and ground truths reduces the validation burden to mostly lightweight consistency checks.With the pipeline, EXP-Bench currently comprises 461 research tasks (12,737 individually gradable subtasks) derived from 51 papers published at NeurIPS and ICLR 2024, spanning diverse AI subfields such as reinforcement learning, AI applications and generative models.</p>
<p>We use a multi-metric evaluation pipeline (Fig. 1) to assess agent performance across all core phases of experimentation-design, implementation, execution, and conclusion.Each metric captures a distinct capability, and their conjunctive use ensures that agents correctly understand and complete the experiment.Initial evaluations of leading agents reveal that, while they often succeed at executing routine procedures-such as running pre-written scripts or replicating documented analysis steps-they struggle when tasked with conducting complex experiments.Specifically, we observe failures in: (a) Conceptualizing and operationalizing sound experimental designs from high-level research questions and methods (16.1% misclassified design variables); (b) Translating abstract research methodologies into complete and correct code implementations (39.7% missing essential implementation components); and (c) Ensuring the robust and reproducible execution of complex experimental software stacks (29.4% environment or dependency misconfigurations or 23.8% scriptlevel errors).By identifying these key bottlenecks, EXP-Bench helps us target specific research components for improvement and advance next-generation AI agents for autonomous research.</p>
<p>Related work</p>
<p>While existing benchmarks have advanced the evaluation of AI agents in various scientific reasoning, coding, and specific machine learning tasks, EXP-Bench distinctively addresses the holistic challenge of end-to-end and step-by-step AI research experimentation.See App.A for additional discussion.</p>
<p>Scientific Reasoning Benchmarks.Benchmarks like BoxingGym [27] explore simulated theory formation, while others such as AAAR [60] and Lab-Bench [49] assess reasoning or experimental Figure 2: One AI research task example from ICLR 2024 MogaNet [51].</p>
<p>design based on static artifacts (e.g., protocols, figures).While valuable for assessing abstract reasoning, these benchmarks do not evaluate the agent's ability to perform actual experiments.</p>
<p>Scientific Coding Benchmarks.Scicode [81], for instance, focuses on generating code snippets for natural science tasks, while BLADE [31], DiscoveryBench [64], and ScienceAgentBench [14] primarily assess post-hoc data analysis or hypothesis testing.While critical to the scientific process, they often isolate coding or analysis from the broader, iterative experimental context.</p>
<p>Machine Learning Benchmarks.Several benchmarks specifically target machine learning (ML) tasks, yet often focus on sub-components or operate with simplifications of the full research cycle.For example, DSBench [45], ML-Agent-Bench [39], and MLE-Bench [12] assess ML problemsolving capabilities, such as script editing or hyperparameter tuning, frequently within constrained environments like Kaggle challenges.Other benchmarks such as RE-Bench [92], ML-Gym [69],</p>
<p>and Curie [47], compare agent performance against humans on research tasks, but often operate at a limited scale (e.g., RE-Bench features only 7 hand-curated tasks) or use simplified evaluation metrics.PaperBench [76] assesses agents on tasks derived from academic literature, focusing on their proficiency in executing specific, well-defined sub-components of the research process, such as running documented code scripts or performing standard data analyses.While these benchmarks provide valuable insights into specific ML tasks, they generally fail to capture the complexity of realistic end-to-end AI research workflows, nor do they typically offer a methodology for constructing such comprehensive benchmark tasks at scale.</p>
<p>The EXP-Bench Benchmark and Dataset</p>
<p>EXP-Bench is built to evaluate the AI agent's ability to address AI research tasks by conducting end-to-end experimentation.Each research task is grounded on an influential AI research paper and its corresponding codebase.This coupling captures the full scientific workflows-linking concrete high-level ideas to executable implementations ( §3.1).We achieve scalable construction of these highfidelity tasks through a semi-automated curation pipeline, which integrates multi-modal extraction with lightweight human verification ( §3.2).This design also opens the door to large-scale data generation for training agents capable of automating core aspects of AI research.</p>
<p>EXP-Bench Dataset Specification</p>
<p>Our dataset is a collection of AI research tasks, each structured to emulate a complete experimental process designed to address a specific AI research question from a published paper.As shown in Fig. 2, each task entry in the dataset contains a problem statement for the agent, and the corresponding ground-truth solution derived from the original research artifacts.Expected Outcome (Ground Truth).Each task instance also includes a ground-truth experimental solution curated from the source paper and codebase.This solution-used to evaluate agent outputs-comprises: (1) an experimental design specifying key variables, constants, and procedures; (2) the necessary code modifications, assessed via a git diff against the provided repository; and (3) a final conclusion that directly answers the research question based on experimental results.</p>
<p>Problem Statement (Agent Input</p>
<p>Benchmark Overview and Statistics: EXP-Bench currently includes 461 research tasks drawn from 51 influential papers, as detailed in App.B. As shown in Fig. 3, these tasks span diverse AI subfields-including Computer Vision, NLP, and Reinforcement Learning-and are sourced from top-tier venues, namely NeurIPS 2024 (53%) and ICLR 2024 (47%).This breadth ensures coverage of diverse experimental paradigms, coding practices, and research challenges prevalent in the AI field.Moreover, each task is broken down into fine-grained, individually gradable subtasks spanning all three ground-truth components-design, implementation, and conclusion-resulting in a total of 12,737 subtasks.Together, these features make EXP-Bench a comprehensive testbed for assessing the capabilities of AI research agents.</p>
<p>EXP-Bench Semi-Automated Dataset Construction Pipeline</p>
<p>Curating a high-fidelity benchmark for end-to-end AI experimentation is challenging due to the fragmented and domain-specific nature of real-world research artifacts-namely papers and their associated codebases.Critical experimental details are often scattered, implicit, or embedded in dense technical language, making manual extraction labor-intensive and difficult to scale.To address this, we propose a semi-automated construction pipeline that systematically structures these artifacts into benchmark tasks with lightweight human oversight.The pipeline comprises three stages (Fig. 4):</p>
<p>Stage 1: Source Selection and Filtering.The process begins by identifying candidate research artifacts that form the basis of high-quality experimental tasks.We target influential papers from top-tier AI conferences (e.g., NeurIPS, ICLR) that are accompanied by publicly available code repositories.Initial filtering criteria are applied to prioritize impactful and potentially reproducible research, considering factors such as citation counts, and code repository activity (e.g., GitHub stars, forks).This selection phase aims to establish a strong foundation by focusing on artifacts that, despite potential imperfections, represent significant and verifiable research contributions.</p>
<p>Stage 2: Experiment Procedure Extraction.Research papers rarely present experiments as complete procedures-key steps are often implicit or scattered.To enable structured agent evaluation, we decompose each task into explicit sub-steps.This transforms high-level research goals into concrete workflows-e.g., multi-step experiment design and environment setup-making them suitable for both execution and fine-grained evaluation.This stage extracts the complete research task by combining the research plan (from the paper) with its corresponding experiment implementation (from the codebase).Further implementation details can be found in App.F.</p>
<p>Stage 2.1: Extract Research Task.We begin by extracting the core research task-consisting of the research question, high-level methodology, and expected outcome-directly from the paper.This process is designed to handle the fact that key information in academic papers is often distributed across sections and conveyed implicitly.First, we index the PDF using a combination of OCR (Optical Character Recognition) and multimodal extraction techniques to capture structured elements like tables, figures, and headers.This ensures downstream access to high-signal artifacts that may anchor the task definition.Next, we conduct a multi-pass extraction.In the first pass, we perform retrieval-augmented querying to identify broad, high-level research takeaways.These overarching questions are often not confined to a single paragraph and require stitching together dispersed cues.</p>
<p>In the second pass, for each high-level takeaway, we apply semantic extraction at the subsection level, focusing on evaluation sections.We classify each subsection as either implementation context or a candidate research question.Contextual passages are stored and reused across subsequent prompts.This focused prompting-processing each subsection independently while conditioning on accumulated context and extracted tables/figures-helps the LLM generate more accurate and detailed task formulations.Finally, we refine each task through targeted re-querying of the full paper (including appendices) to recover any additional setup constraints or methodological details that were missed earlier.This step acknowledges that relevant setup details may be located far from the task description and ensures completeness for the extracted task.</p>
<p>Stage 2.2: Extract Experiment Implementation.Each extracted task is then passed to an implementation extraction AI agent (operating in a tool-augmented environment-with PDF reading, terminal access, and web browsing) to identify the specific implementation (chain of scripts) needed to address the research task.Our setting provides the agent with both a complete codebase and the extracted task-containing the research question, methodology, and expected outcome.This effectively reduces the problem to a goal-conditioned search over the codebase, where the agent's task is to localise the implementation that realises the specified methodology and expected outcome.To do this, the agent explores the repository in an open-ended fashion-e.g., consulting documentation, and auxiliary scripts, to uncover domain-specific requirements (e.g., pretrained checkpoints).The extracted experiment execution ground truth will be fully based on existing scripts.The agent outputs (1) a list of required scripts and (2) high-level usage instructions describing how to run them to complete the task.Once a candidate implementation is produced, it is executed in Stage 3. If the run fails, the pipeline iterates-allowing the agent to refine or replace the implementation until a working solution is found.The final validated script chain is then parsed by the agent via AST (Abstract Syntax Tree) tracing to extract a step-by-step list of implementation requirements in natural language, which becomes the ground truth for evaluating implementation correctness.Finally, we incorporate additional contextual details (e.g., hyperparameters) sourced from the raw code (e.g., configuration files) or repository documents (e.g., README.md) to enhance the final task specification.</p>
<p>Stage 3: Verification and Refinement.All tasks are validated and finalized in this stage.For paper-derived tasks with corresponding implementations, the associated scripts are executed in a clean, containerized environment.Execution traces are then checked against expected outputs from the original paper.If validation fails, the task is returned to the previous stage for refinement.For tasks lacking a matched implementation, we perform manual validation to ensure the extracted Setup.We evaluate a range of agents and LLMs used in related benchmarks [13] against EXP-Bench.In terms of agents, we made use of OpenHands (a top-performing code generation agent) and IterativeAgent (as configured in [76] to reduce the likelihood of early task stopping), henceforth known as OH and IA, respectively.In terms of LLMs, these include the top-ranked Claude-Sonnet 3.7, Haiku 3.5, Deepseek-R1 [90] models, and OpenAI o3-mini variants.Each agent is run in an Ubuntu 24.04 Docker container, and given access to 4 × Nvidia A40 GPU, and a clean working directory containing the masked GitHub repo of the paper (i.e., task-specific scripts removed), instructions, and relevant context (e.g., API credentials).</p>
<p>Evaluation Judge Implementation Details.Our evaluation framework consists of two main components used to assess agent performance across various metrics (refer to later sections, e.g., Table 1).The first component is an LLM-based judge (using o3-mini), following prior work on LLM-as-a-judge [111,56,1,76].This judge operates through multiple steps: The process begins with an integrity check performed by a Monitor, which analyzes agent logs to detect disallowed behaviors (denoted as metric M; see Fig. 6b).Specifically, the monitor checks whether the agent:</p>
<p>(1) accessed the research paper directly (e.g., opened the PDF), (2) performed Git operations such as checking out commits or switching branches, or (3) used fake, hardcoded, or placeholder data rather than generating results through real experimentation.If violations are found, the monitor also identifies possible causes (e.g., ethical refusals, runtime errors) using log information.Once integrity is established, the agent's experimental design, implementation, and conclusion are evaluated for conceptual soundness, completeness (e.g., inclusion of all required steps), and alignment with ground truth.These assessments yield scores for: D (design correctness, i.e., proportion of design criteria met), I (implementation correctness, i.e., proportion of implementation components satisfied), and C (conclusion correctness).The second component of our evaluation judge is a Code Execution Validator, which runs the agent-generated code modifications in a clean and equivalent containerized environment.This step verifies whether the code is executable and produces expected outputs.This executability metric is denoted as E. Implementation details including the system prompt are in App.H.</p>
<p>Main Results</p>
<p>. Table 1 presents average accuracy scores across all 461 tasks.I•E indicates whether an implementation is both appropriate for the experimental task and executable-a more comprehensive check of implementation quality.All✓ denotes tasks that are fully correct in terms of D, I, and C, while All•E✓ adds the executability requirement.#E represents the number of tasks per model that were execution-checked.Due to the time-consuming nature of execution, only a subset of traces were evaluated-excluding those that failed the monitor check, which were automatically discarded prior to execution.Our top-ranked agents are OH+o3-mini, OH+3.7 Sonnet, and OH+Nova Pro, ranked via All•E✓, with C used as a tiebreaker.The worst-performing model was IA+Nova Pro.Extended results by paper category are shown in Table 3, with full details in App.D. Across both tables, we observe that models consistently score below 30% across all metrics, with the exception of the RL category, where several OH models achieve up to ≈41% (averaged over 36 tasks) in terms of I. Notably, under stricter metrics such as All✓, performance drops sharply-e.g., OH+o3-mini scores only 1.4%.This underscores the value of including partial metrics that assess individual aspects, allowing credit for partially correct answers and supporting a more nuanced evaluation.</p>
<p>Detailed Analysis</p>
<p>Cost-Time Analysis.Fig. 6a shows the average cost (in USD) and time (in minutes) per task across different agent configurations.Cost reflects only the token usage of the backbone LLM (input/output), excluding agent internal LLM-API usage or compute consumption.The number in parentheses next to each legend entry indicates the model's performance rank, based on average correctness.Each agent was allowed a maximum of 40 minutes per task, though this limit can be easily adjusted.Notably, IA models often consumed the full allotted time, rarely stopping early.In contrast, early stopping was common with OH models.For example, the relative time difference between Nova and Haiku is larger under OH than IA, reflecting differing usage patterns.These trends are consistent with our earlier observations: OH models often produced plausible responses without actually running the experiment, leading to high partial scores (e.g., design, implementation), while IA models tended to run longer but less effectively.Interestingly, we found little correlation between runtime/cost and overall performance.OH+o3-mini (rank 1) achieves the best trade-off with low cost and moderate time.OH+3.7 Sonnet (rank 2) performs well but is the slowest and most expensive.The full cost-time distribution is provided in App.I.2.</p>
<p>Conjunctive Evaluation Metrics Substantially Lower Agent Scores.We analyze only the subset of tasks for which execution was run, to visualize how progressively applying stricter evaluation criteria impacts agent scores.As shown in Fig. 6b (with full results in App.I.1), applying only the initial monitoring check (M) yields an average score of 20.6%.Adding design (D) and conclusion (C) correctness criteria reduces the score sharply to 3.7%.Incorporating implementation correctness (I) further lowers the score to 0.4%, and including execution verification (E) results in a final accuracy of just 0.2%.These findings highlight how conjunctive evaluation surfaces brittleness in end-to-end experimental correctness.</p>
<p>Metric Stability Analysis.</p>
<p>As shown in Fig. 5, certain individual metrics such as C and E exhibit high variance.This variance arises for different reasons: for C, agents can produce plausible but unfounded conclusions without a valid experimental foundation; for E, even incorrect or mock implementations may successfully execute, introducing overestimation bias.To mitigate such inconsistencies, we adopt compositional scoring via conjunctive metrics such as C•D and I•E, which combine correctness across multiple dimensions.These conjunctive forms substantially reduce score variability, producing more reliable signals of agent performance.For example, C•D filters out conclusions not grounded in valid design plans, and I•E discounts executions that do not fulfill setup requirements.This demonstrates that conjunctive metrics can temper over-crediting and reduce sensitivity to annotation leniency or spurious correctness-thereby offering a more stable and discriminative evaluation.</p>
<p>Analysis on Prevalent Agent Failure Patterns</p>
<p>Pattern Extraction Methodology.Our analysis followed a two-pass, open-ended process.During evaluation, each metric score was accompanied by an error analysis, derived from implementation logs (e.g., stderr) or comparisons against ground truth.In the first pass, we extracted high-level, Average Cost ($)</p>
<p>(2)  domain-specific insights from these earlier error analyses, across phases for all agent-task pairs.In the second pass, we iteratively grouped these insights into distinct failure types-assigning each to an existing category or creating a new one if needed.This process produced 3,238 raw insights, which we distilled into 361 unique failure types.We present a representative and simplified subset of these condensed errors in Table 2 (full details can be found in App.G).
(3) (4)(5) (6) (7
Analysis.To better understand where agents fail, we analyzed error traces and categorized them into representative failure types across four key phases of experimentation: implementation, execution, design, and conclusion.As shown in Table 2, the most prevalent issues emerged during the implementation phase, with 39.71% of failures stemming from missing essential components.In several cases, agents failed to include critical elements such as semantic retrieval strategies (e.g., UniXcoder-H2L and UniXcoder-L2H), validation functions for filtering questions (e.g., using GPT-3.5),or robustness-enhancing techniques like Mixup, CutMix, and Label Smoothing-undermining the experimental implementation's validity.Incomplete data preprocessing (1.83%) was another notable implementation issue, with agents omitting required dataset loading and transformation steps, such as ETTh1 series preparation, ACF plotting, or normalization procedures (e.g., RevIN), instead providing only boilerplate config files.In the execution phase, failures were most commonly due to environment or dependency misconfigurations (29.38%), such as missing critical environments (e.g., STORM not registered in jaxmarl) or absent core libraries like PyTorch and Flax, which led to model loading failures.Script-level issues (23.84%)included unrecognized model names (e.g., moganet_tiny not found in timm) and missing checkpoint files, causing runtime or I/O errors.These examples highlight persistent reproducibility challenges even when a correct implementation structure is in place.Design-related failures were also frequent, with 16.05% involving incomplete or misclassified experimental variables, and 7.62% reflecting extraneous procedural additions-such as inclusion of a ResNet-50 backbone or arbitrary hyperparameter knobs not specified in the ground truth.These design errors suggest that agents often fail to distinguish between essential experimental factors and implementation noise.Finally, conclusion-phase errors highlight limitations in agents' interpretive reasoning.The most common issue (26.18%) was missing or underdeveloped conclusions-for instance, omitting detailed comparisons between PPO and Q-Learning on training time and normalized scores, or neglecting specific numerical gains (e.g., 1.25% improvements across ARC-Challenge and OpenBookQA).Another frequent error (19.66%) was incorrect interpretation, such as claiming Hadamard-enhanced INT4 inference improves performance without substantiating comparisons to baseline INT4.Together, these findings emphasize the importance of phase-specific evaluation and illustrate how surface-level plausibility can mask deeper breakdowns in experimental reasoning and reproducibility.</p>
<p>Discussion</p>
<p>Limitations.EXP-Bench primarily focuses on the experimentation procedure -from designing experiments for a given research question to deriving conclusions.The broader AI research lifecycle encompasses other critical stages such as identifying gaps through literature review, the initial unstructured ideation of research questions, and navigating the complex, iterative, and unpredictable path of real-world scientific discovery, which are not yet fully captured by the current task structures.</p>
<p>Conclusion</p>
<p>We introduced EXP-Bench, a novel benchmark designed to rigorously evaluate and guide the development of AI agents in conducting end-to-end AI research experimentation.By sourcing tasks from influential peer-reviewed publications and their accompanying codebases, and utilizing a semi-automated curation pipeline, EXP-Bench presents agents with realistic, fine-grained challenges in end-to-end AI research workflow including experimental design, implementation, execution, and conclusion derivation.Our initial evaluations with leading agents reveal significant bottlenecks in conceptualizing complex experiments and ensuring robust code implementation and execution.EXP-Bench therefore serves not only as a comprehensive evaluation tool but also as a valuable dataset to guide future AI agents to act step by step, ultimately accelerating AI research.</p>
<p>A Extended Related Works</p>
<p>LLMs for scientific discovery.Many methods have adopted LLMs to generate novel hypotheses for common scientific discovery.For example, Baek et al. [4], Wang et al. [84], and Yang et al. [103] developed approaches for generating innovative domain-specific research ideas.Going beyond domain-specific ideas, a line of work also focuses on generate hypothesis with LLMs in the commonsense domains [28,67,102,91,65,89,2,98,34,97].Moreover, prior research on automated scientific discovery proposes to combine hypothesis with LLM-assisted code generation for end-to-end workflows [50,42,64].While these efforts works on various stages of the scientific lifecycle, experimentation-a critical, rigor-sensitive aspect-remains underexplored.</p>
<p>Some existing research explores building an automated scientific discovery workflow with rigorous validation using AI agents [61,91,46,106,75,9,79,105,29], they often either have limited automated evaluation or rely on domain-specific ad-hoc prompting optimizations to guide predefined workflows, struggling with the complexities of rigorous end-to-end experimentation to automate AI research.Particularly, Lu et al. [61] introduced a fully automated system called "The AI Scientist" to conduct research by collaborating with multiple LLM agents.These agents handle the full research process, from defining research problems and reviewing related literature to synthesizing and executing experiments.However, their solution has limited automated evaluation with a focus on commonsense domains.Gottweis et al. [91] proposed an AI Co-scientist built on Gemini 2.0, aiming at building a helpful AI collaborator for scientists.They focus on the scaling of the testtime compute paradigm to generate high-quality hypotheses and research proposals.While general purpose, the AI co-scientist is mainly validated in biomedical areas.Overall, these efforts often require experimental validation to follow constrained, framework-specific formats, resulting in extra overhead and hindering their usability.</p>
<p>Benchmarks for domain-specific AI agent tasks.A wide range of benchmarks have been developed to evaluate the capabilities of AI agents across diverse domains.Existing benchmarks predominantly target problem-solving [36,26,86,77,17], logical reasoning [18,35,5,48], machine learning training [40,109,108,30,82,66,68,31,32], and knowledge retrieval and analysis [78,37].These benchmarks typically involve well-defined tasks with clear, deterministic solutions, allowing for consistent and objective assessment of AI agent performance.By contrast, our proposed EXP-Bench focuses on experimentation for automating AI research, which requires a more rigorous and systematic approach beyond problem-solving.Experimental tasks demand iterative hypothesis refinement, complex experiment design/implementation and execution, and rigorous result interpretation.Our benchmark captures these challenges by semi-automatically evaluating AI agents on real-world experimentation tasks arising from influential AI research papers with high-impact open-source artifacts.</p>
<p>B Extended Details of the EXP-Bench Dataset</p>
<p>In this section, we provide a full list of the papers in the EXP-Bench dataset, including source paper and, AI sub-domain.The complete dataset can be found in our HuggingFace repository https://huggingface.co/datasets/Just-Curieous/EXP-Bench.</p>
<p>E Extended EXP-Bench Examples</p>
<p>This section presents two extended examples from EXP-Bench: a question concerning robust detection in collaborative perception under imperfect localization, and a question focused on implementing a Time Delay Neural Network (TDNN) for automatic speech recognition.Each example details the experiment's objective, methodology, relevant source code, and expected outcomes.The examples also include an analysis of agent performance on completing the task.</p>
<p>E.1 Example 1: Robust Detection in Collaborative Perception</p>
<p>This example question was extended from the paper An Extensible Framework for Open Heterogeneous Collaborative Perception [62].</p>
<p>The objective of this experiment is to assess whether HEAL (HEterogeneous ALliance) can maintain robust detection performance under realistic conditions of imperfect localization, when Gaussian noise is added to the agents' poses.The experiment maintains constant variables such as the dataset (OPV2V-H) and the model architecture (HEAL).The independent variables are the position noise and rotation noise, while the dependent variable is the model's detection performance matrices (AP30, AP50, and AP70).Experimental groups test the addition of Gaussian rotation and position noise at levels of 0, 0.2, 0.4, and 0.6 meters/degrees to accurate poses.The results will contribute to evaluating the robustness of a cooperative perception model under conditions of imperfect localization.EXP-Bench extends this task from the original paper section 5.3 QUANTITATIVE RESULTS and utilizes the source code: /workspace/opencood/tools/inference_w_noise.py from the GitHub repository https://github.com/yifanlu0227/HEAL.Note that /workspace/ refers to the working directory from the agent's initialization context.</p>
<p>The general formulation of the task includes the question posted to the agent, the overall method of the experiment, the source of this question (specifically the section in the paper and the source code), and the expected outcome.This is illustrated in Fig. 7a.</p>
<p>The agent's task is to use the provided GitHub repository, with the source code masked, to conduct this experiment.To aid the agent in reconstructing the masked file, detailed instructions are provided, as shown in Fig. 7b.</p>
<p>Evaluation of the task includes design, conclusion, and setup evaluation.The conclusion appears in the 'expected outcome' in Fig. 7a.Design and setup evaluation are based on 'design complexity' and 'requirements' respectively, shown in Fig 8.</p>
<p>An example agent output using the bedrock-us-anthropic-claude-3-7-sonnet-20250219-v1-0 LLM as a backbone is showcased here.We perform a diff operation between the code generated by the agent and the original source code.As this agent reconstructs several files to fulfil the task requirement, we focus on a diff operation between the core reconstructed file (evaluate_robustness.py) and the source file (inference_w_noise.py),shown in Fig. 9.The two files share the same functional goal and have a similar overall structure; however, the agent performs invalid operations in another file (reproduce_exp_bench.sh), leading to a failure in completing the task.The detailed reasoning provided by the judge is illustrated in Fig. 10.</p>
<p>E.2 Example 2: Time Delay Neural Network for ASR</p>
<p>This example is extended from the paper Zipformer: A Faster and Better Encoder for Automatic Speech Recognition [104].</p>
<p>The objective of this experiment is to implement a Time Delay Neural Network (TDNN) that achieves a Word Error Rate (WER) of less than 1% on the test set.This setup focuses on constructing a TDNN model with three Conv1d layers-each followed by ReLU activation and Batch Normalization-and a final linear layer to produce log probabilities for phoneme classes.The general formulation of the task includes the question posted to the agent, the overall method of the experiment, the source of this question (specifically the section in the paper and the source code), and the expected outcome.This is illustrated in Fig. 11a.</p>
<p>Again, the agent's task is to use the provided GitHub repository, with the source code masked, to conduct this experiment.To aid the agent in reconstructing the masked file, detailed instructions are provided, as shown in Fig. 11b.</p>
<p>Similarly, evaluation of the task includes design, conclusion, and setup evaluation.The conclusion appears in the 'expected outcome' in Fig. 11a.Design and setup evaluation are based on 'design complexity' and 'requirement' respectively, shown in Fig 12.</p>
<p>For the agent performance in this example, we make use of an agent's output using the bedrock-usamazon-nova-pro-v1-0 LLM backbone.We perform a diff operation between the code generated by the agent and the original implementation files provided in the baseline.Since the agent restructures multiple modules to accomplish the speech recognition task, our analysis focuses on a diff between the core model implementation file (model.py)and the original reference.The agent correctly builds the TDNN model and integrates it with the training and decoding pipeline.The two versions of model files share a similar architectural skeleton, but differ in details such as layer configuration and parameter initialisation.The differences are shown in Fig. 13.</p>
<p>F Dataset Curation and Benchmark Construction Details</p>
<p>To ensure the quality and integrity of EXP-Bench, we developed the curation pipeline through a careful, iterative process.Each component was prototyped, tested on real papers, and refined based on manual inspection by collaborators.This allowed us to isolate and address specific failure modes incrementally, steadily increasing curation throughput without compromising accuracy.Several representative issues that were patched in our final pipeline are documented in Table .7. Manual validation was also aided by the availability of ground truth from the papers and open-source code repositories themselves, making the verification process straightforward.The EXP-Bench team is committed to the long-term maintenance and growth of the dataset.We actively monitor feedback and bug reports via GitHub and HuggingFace issue trackers and will address any concerns raised by the community post-release.All data is hosted on both platforms to ensure accessibility and stability, with potential plans to replicate the dataset on archival storage for long-term preservation.To foster transparency, reuse, and critical engagement, the dataset is released under the permissive Creative Commons Attribution 4.0 license, and all code under the MIT license.We encourage the community to explore, build upon, and challenge EXP-Bench as an open and evolving resource.</p>
<p>G Extended Analysis on Prevalent Agent Failure Patterns</p>
<p>Some overlap between categories may exist, as the classification was performed by an LLM.</p>
<p>H Evaluation Judge</p>
<p>As detailed in the main text ( §4.1), our evaluation framework leverages an LLM-based judge (using o3-mini-2025-01-01-preview) to assess various aspects of agent performance, including adherence to integrity guidelines, experimental design, implementation correctness, and conclusion validity.We provide the full system prompts used by our LLM judge in this section.</p>
<p>When we encounter long inputs such as Git diffs and agent logs, the content can sometimes exceed the context window of the o3-mini model.To address this, we segment the input into individual chunks that fit within the model's context length.These chunks are then fed iteratively to the judge, with the evaluation results and relevant context from one chunk carried over to the subsequent chunk.This iterative processing ensures that the entire input is evaluated comprehensively without loss of information due to context length limitations.</p>
<p>Monitor Prompt.The initial integrity check, performed by the Monitor, uses the following prompt to analyze agent logs for disallowed behaviors (e.g., accessing the research paper directly, performing Git operations, or using hardcoded data).</p>
<p>Inspect the log to verify the following criteria:</p>
<p>1.The agent did NOT access (i.e., read) the research paper itself (e.g., the PDF).2. The agent did NOT perform any Git operations, such as checking out commits, switching branches, or accessing other versions of the repository.3. The agent did NOT use fake, hardcoded, or mock data in place of real experimental results.In particular, check for any signs of: -Hardcoded or fabricated data values used as results.</p>
<p>-Use of mock or placeholder data instead of running actual experiments.</p>
<p>-Directly copying or using result data already present in the repository, rather than generating it through experimentation.</p>
<p>-Any indication that crucial data for the experiment was not produced by creating and running the intended experimental task.</p>
<p>-If you find evidence that the agent faked data or failed to run the experiment, provide the reason why this happened if possible.For example, did the agent or LLM (such as nova pro) refuse to perform the task due to ethical concerns, security restrictions, or because the task involved potentially malicious actions?If the log contains an explicit refusal, error message, or explanation for why the agent faked or could not generate the data, include this in your reasoning.</p>
<p>Output format exactly as this JSON:</p>
<p>{ "paper_access": <boolean>, "git_operations": <boolean>, "faked_or_nonexperimental_data": <boolean>, "setup_monitor_comprehensive_reason": "<comprehensive explanation for your decisions on the above keys, including specific evidence from the log and your reasoning process.Clearly justify each decision and reference relevant log entries or the absence thereof.If you found evidence for any key being True, explain exactly what that evidence was and why it led to your conclusion.If you found no evidence, explain how you searched and why you are confident in your answer.If you found evidence that the agent faked data or failed to run the experiment, also explain the reason for this only if possible (e.g., LLM refusal, security/ethical block, or other failure), citing the relevant log entries.>"} Explanation of output keys: -"paper_access": True if the agent accessed the research paper, False otherwise.</p>
<p>-"git_operations": True if the agent performed any Git operations, False otherwise.-"faked_or_nonexperimental_data": True if there is any evidence of fake, hardcoded, mock, or reused (non-experimental) data being used for crucial results, False otherwise.-"setup_monitor_comprehensive_reason": A comprehensive explanation describing the evidence and reasoning for the above decisions, referencing specific log entries or the absence of evidence as appropriate.</p>
<p>Here is the log: {logs}</p>
<p>Design and Conclusion Evaluation Prompt.Once integrity is established, the agent's experimental design and conclusion are evaluated for conceptual soundness, completeness, and alignment with ground truth.This assessment contributes to the D (design correctness) and C (conclusion correctness) metrics.</p>
<p>You are a judge tasked to evaluate a system's output against ground truth answers for an experimental design task.</p>
<p>Input fields:</p>
<p>-design_ground_truth: the correct list of variables (constants, independent, dependent variables).-conclusion_ground_truth: the correct conclusion as a string.</p>
<p>-design_output: the predicted design.It may not be formatted as a list;</p>
<p>extract and match relevant variable information from its content.-conclusion_output: the predicted conclusion string.</p>
<p>Evaluation Instructions: -Design Evaluation: Compare design_output to design_ground_truth. Count how many items in design_output match items in design_ground_truth.Return the percentage of correct items as an integer (e.g., use 75 to represent 75%), along with a short explanation.If applicable, include a failure analysis on what the system got wrong.</p>
<p>-Conclusion Evaluation: Compare conclusion_output to conclusion_ground_truth.Return "correct" or "incorrect" based on semantic match, along with a short explanation.If applicable, include a failure analysis on what the system got wrong.</p>
<p>Here is the input:</p>
<p>I Additional Analysis</p>
<p>We include detailed breakdowns of the analysis performed in §4.2.</p>
<p>I.1 Conjunctive Evaluation Metrics Analysis</p>
<p>In Fig. 14, we include details for all agents and models evaluated, as opposed to the subset in Fig. 6b.</p>
<p>I.2 Cost-Time Distribution</p>
<p>We showcase the full cost-time distribution in Table.9 in the form of summary statistics.For timerelated statistics, although a soft timeout of 40 minutes was enforced during trials, agents occasionally exceeded this limit due to non-compliance with the timeout mechanism.Additionally, both time and cost values can appear unusually low in cases where the agent failed to complete the experiment.</p>
<p>convolution improves... train.pyrun_exp.shdef forward(self, ...): -x = self.conv(x)+ x = [conv(x) for conv in self.conv_list]+ x = torch.cat(x,dim=1)</p>
<p>DFigure 3 :
3
Figure 3: EXP-Bench's dataset comprises tasks from a diverse set of ML research categories.</p>
<p>Figure 4 :
4
Figure 4: EXP-Bench semi-automated dataset construction pipeline.</p>
<p>Figure 5 :
5
Figure 5: Stability Analysis.</p>
<p>Stricter metrics reveal lower true correctness.</p>
<p>Figure 6 :
6
Figure 6: Ablation of agent performance along cost-time and evaluation metrics.</p>
<p>(a) The formulation of the task question.(b) Instructions provided to the agent.</p>
<p>Figure 7 :
7
Figure 7: Task Fields for Example 1.</p>
<p>Figure 8 :
8
Figure 8: Evaluation of the design and setup for the Extended Task in Example 1.</p>
<p>Figure 9 :
9
Figure 9: Example 1's Git diff comparing the masked source file and the agent-reconstructed source code.Red highlights indicate deletions, while green highlights represent additions.</p>
<p>Figure 10 :
10
Figure 10: Error Analysis and Comprehensive Explanation of the agent's failure to complete the task in Example 1.</p>
<p>Figure 12 :
12
Figure 12: The design and setup evaluation of the extended task in Example 2.</p>
<p>Figure 13 :
13
Figure 13: Example 2's Git diff of the masked source file and the agent reconstructed source code.In the diff, red highlights are deletions.Green highlights are additions.</p>
<p>Figure 14 :
14
Figure 14: Stricter metrics reveal lower true correctness.Table 9: Cost-time summary statistics for all evaluated agents and models.OH = OpenHands, IA = IterativeAgent.Med = median, Std = standard deviation, T = time (minutes), C = cost (USD).Agent Model Avg T Med T Q1 T Q3 T Std T Min T Max T OH o3-mini 24.89 23.24 13.93 33.60 16.74 1.70 47.72 OH 3.7 Sonnet 33.53 29.64 16.67 37.72 10.03 2.55 74.04 OH Nova Pro 17.82 15.03 11.85 24.06 9.37 0.64 74.33 OH 3.5 Haiku 25.17 24.23 13.26 32.72 17.38 1.21 37.85 OH DeepSeek R1 32.24 31.40 19.09 38.69 11.82 0.97 60.77 IA 3.5 Haiku 30.24 26.13 19.63 38.24 54.62 0.30 402.84 IA Nova Pro 30.09 26.31 19.51 38.09 27.61 0.17 360.52 Agent Model Avg C Med C Q1 C Q3 C Std C Min C Max C OH o3-mini 0.55 0.35 0.17 1.11 0.56 0.01 1.34 OH 3.7 Sonnet 10.15 7.53 3.04 14.20 6.30 0.03 19.83 OH Nova Pro 1.09 0.77 0.33 2.18 0.93 0.00 2.99 OH 3.5 Haiku 0.68 0.42 0.15 2.68 1.47 0.01 3.24 OH DeepSeek R1 1.55 1.28 0.83 2.49 1.70 0.00 4.08 IA 3.5 Haiku 2.82 1.86 0.52 4.23 2.90 0.02 5.09 IA Nova Pro 3.93 3.26 0.91 5.31 3.65 0.02 6.96</p>
<p>Table 1 :
1
Average benchmark scores for various models when tested against various evaluation metrics.Popular Agents and LLMs perform poorly on EXP-Bench, showcasing its difficulty.
AgentModelDIEI•ECAll✓ All•E✓ #EOpenHandso3-mini18.4 20.3 15.0 2.9 21.01.40.5OpenHandsClaude-3.7 Sonnet 16.0 35.0 33.2 14.9 13.40.70.4OpenHandsAmazon Nova Pro 18.2 19.5 26.8 0.0 15.70.00.056OpenHandsClaude-3.5 Haiku20.6 26.2 9.31.3 13.80.00.0OpenHandsDeepSeek R16.8 10.0 0.70.02.40.00.0IterativeAgent Claude-3.5 Haiku6.4 20.6 25.2 5.42.20.00.0IterativeAgent Amazon Nova Pro0.1 10.0 18.1 0.00.30.00.0research objective faithfully aligns with the source paper. In all cases, a lightweight human reviewfinalizes the task, requiring only a cross-check of structured task content-already consolidated bythe pipeline-against the source materials. This significantly reduces human burden compared tomanual curation from scratch. Following validation, each complete task is added to the dataset alongwith a list of masked files (e.g., README.md, relevant scripts) to ensure agents cannot directly accessanswers. In our benchmark implementation, repositories are cloned afresh per agent, and masking isapplied using scripted git operations, including recursive traversal of submodules. Masking ensuresagents must reason over the task input, rather than rely on shortcut access to original solutions.4 Evaluation4.1 Evaluating LLM-based Agents Performance on EXP-Bench: Setup &amp; Main Results</p>
<p>Table 2 :
2
Agents fail in diverse ways across different phases of experimentation; this table presents a simplified subset of common examples, measured across all agent and model evaluations.
PhaseFailure TypePrevalence (%)DesignIncomplete or Misclassified Design Variables16.05DesignIrrelevant Procedural Additions in Design7.62Implementation Missing Essential implementation Components39.71Implementation Incomplete Evaluation Metric Implementation2.15ImplementationIncomplete Data and Preprocessing Setup1.83ExecutionEnvironment/Dependency Configuration Errors29.38ExecutionExecution Script and File Errors23.84ExecutionMissing Setup Script File6.95ExecutionTensor Operation Execution Error3.22ConclusionMissing Conclusion Content26.18ConclusionIncorrect Conclusion Interpretation19.66ConclusionExtraneous Details in Conclusion7.77ConclusionIncorrect Numeric Conclusion3.21</p>
<p>Table 3 :
3
Average benchmark scores of various models and agents across select task categories; see Supp.D for complete list.Evaluation performed against EXP-Bench.Future
CategoryAgentModelDIEI•ECAll✓ All✓•EApplicationsOHNova Pro19.2 23.9 19.0 0.0 13.90.00.0ApplicationsOHo3-mini9.08.00.00.08.30.00.0ApplicationsOH3.5 Haiku19.2 24.5 8.35.68.30.00.0ApplicationsOH3.7 Sonnet9.0 26.8 30.8 7.78.32.80.0ApplicationsIANova Pro0.09.85.00.00.00.00.0ApplicationsIA3.5 Haiku0.0 18.0 0.00.00.00.00.0ApplicationsOHDeepSeek R1 3.14.30.00.00.00.00.0RLOH3.7 Sonnet18.3 48.2 27.3 21.2 17.62.03.0RLOHo3-mini23.5 34.8 15.7 2.0 27.53.90.0RLOH3.5 Haiku27.7 41.4 11.5 0.0 17.60.00.0RLIA3.5 Haiku3.3 27.5 17.4 0.02.40.00.0RLOHDeepSeek R1 5.0 10.3 0.00.02.00.00.0RLIANova Pro0.08.69.70.00.00.00.0RLOHNova Pro17.9 28.9 0.00.0 13.70.00.0
directions.Future work will focus on enhancing AI agents' ability to automate research experimentation using supervision from EXP-Bench's dataset.One promising direction is to apply reinforcement learning with verifiable rewards, enabling agents to autonomously navigate the research lifecycle and accelerate scientific discovery.</p>
<p>Table 4 :
4
ICLR 2024 Papers
IDTitleStarsCit.#DomainKey Dist.ResourceT1 T2 T319292Zipformer: A faster102397Deep Learning →proposememory needed:133and better encoder forAttentionan archi-32GB or moreautomatic speechMechanismstecturerecommended,recognition[104]GPU type:NVIDIA V100 orA100, GPUamount: 2-819033The Reversal Curse:284179Deep Learning →proposeOpenAI API key300LLMs trained on "A isLarge Languagean archi-required; GPU: 1;B" fail to learn "B isModelstecturememory: ≥16GBA" [6]RAM</p>
<p>Table 5 :
5
NeurIPS 2024 PapersThe advancement of AI agents capable of conducting AI research, as facilitated by benchmarks like EXP-Bench, offers positive societal impacts.It might significantly shorten innovation cycles within AI itself and lead to more rapid advancements in machine learning capabilities.While a faster pace of AI development can also democratize research tools and improve overall scientific efficiency, it concurrently amplifies the importance of addressing potential negative societal consequences.On the other hand, the rapid evolution of AI capabilities heightens risks, where we need to be careful about potential misuse, algorithmic bias, and the evolving role of human researchers, alongside the development of robust governance.D Average Scores acrossAll Paper Categories Defintions.Comp.Biology refers to Computational Biology.CV refers to Computer Vision.D &amp; B refers to Datasets and Benchmarks.Gen. Models refers to Generative Models.Proba.Methods refers to Probabilistic Methods.RL refers to Reinforcement Learning.Addendum.Table.6 contains updated values for IA+3.5 Haiku for the Applications and Reinforcement Learning categories.
IDTitleStarsCit.#DomainKey Dist.ResourceT1 T2 T393022Generative Modeling14413Generative ModelsproposeGPUs not specified,80of Molecular→ New Approachesan archi-but PyTorch andDynamicstecturerelated librariesTrajectories[44]suggest a need for aCUDA-compatibleGPU. Memoryrequirementsunspecified.93431Trace is the Next4929Optimization →proposememory needed: 863AutoDiff: GenerativeGenerative Modelsan archi-GB RAMOptimization withtectureminimum, OpenAIRich Feedback,API key required,Execution Traces, andGPU: 1 x NVIDIALLMs[15]GPUrecommended,98316Causal-learn: Causal128796Causalityproposememory needed:35Discovery inan archi-Standard (dependsPython[112]tectureon the dataset),GPU: Not required,953333DGS-Enhancer:17016Computer Vision →proposememory needed:32EnhancingVideo Generationan archi-16GB, Yes, GPUUnbounded 3Dtecturetype: NVIDIA,Gaussian SplattingGPU amount: 1,with View-consistent2D DiffusionPriors[59]98326TorchOpt: An57017Optimization →proposememory needed:24Efficient Library forZero-order andan archi-At least 8GB RAM,DifferentiableBlack-boxtectureGPU type:Optimization[73]OptimizationNVIDIA, GPUamount: 1,98318BenchMARL:35130Reinforcementpropose amemory needed:44BenchmarkingLearning →datasetAt least 8GB RAMMulti-AgentMulti-agentrecommended,ReinforcementGPU: 1x NVIDIALearning[7]GPU (e.g., GTX1080 or better),</p>
<p>Table 6 :
6
Average benchmark scores of various models and agents across select task categories.Evaluation performed against EXP-Bench.
CategoryAgentModelDIEI•ECAll✓ All✓•EApplicationsOHNova Pro19.2 23.9 19.00.013.90.00.0ApplicationsOHo3-mini9.08.00.00.08.30.00.0ApplicationsOH3.5 Haiku19.2 24.58.35.68.30.00.0ApplicationsOH3.7 Sonnet9.0 26.8 30.87.78.32.80.0ApplicationsIANova Pro0.09.85.00.00.00.00.0ApplicationsIA3.5 Haiku6.8 32.3 33.316.70.00.00.0ApplicationsOHDeepSeek R1 3.14.30.00.00.00.00.0CausalityOHo3-mini37.8 44.6 22.211.1 44.4 11.111.1CausalityOH3.7 Sonnet36.6 83.1 88.966.7 44.40.00.0CausalityIA3.5 Haiku23.1 40.6 40.00.020.00.00.0CausalityIANova Pro0.0 11.7 20.00.00.00.00.0CausalityOH3.5 Haiku48.7 36.60.00.033.30.00.0CausalityOHNova Pro17.7 14.90.00.011.10.00.0CausalityOHDeepSeek R1 10.0 18.70.00.00.00.00.0Comp. BiologyOHo3-mini37.0 30.3 11.10.044.4 11.10.0Comp. BiologyIANova Pro0.0 16.7 20.00.00.00.00.0Comp. BiologyIA3.5 Haiku3.27.80.00.00.00.00.0Comp. BiologyOHNova Pro31.2 29.30.00.033.30.00.0Comp. BiologyOH3.5 Haiku4.89.70.00.011.10.00.0Comp. BiologyOH3.7 Sonnet4.8 11.10.00.011.10.00.0Comp. BiologyOHDeepSeek R1 12.2 22.10.00.00.00.00.0CVOHNova Pro24.8 20.9 42.90.028.20.00.0CVOH3.7 Sonnet18.5 28.9 18.29.121.20.00.0CVOHo3-mini11.5 9.35.12.612.80.00.0CVOH3.5 Haiku15.9 28.34.50.012.80.00.0CVOHDeepSeek R1 5.8 11.20.00.02.60.00.0CVIANova Pro0.05.328.60.00.00.00.0CVIA3.5 Haiku6.</p>
<p>The dataset (yesno), model type (TDNN), loss function (Connectionist temporal classification), and feature extraction method (23-dimensional fbank features) are held constant.Independent variables include the model architecture, training hyperparameters (e.g., learning rate, weight decay), and number of epochs, while the dependent variable is the WER obtained during evaluation.This task emphasizes practical training and decoding using k2's one_best_decoding method and evaluates performance using the WER metric, targeting values below 1%.EXP-Bench extends this task beyond the baseline speech recognition example by formalizing an end-to-end pipeline using code modules: /workspace/egs/yesno/ASR/tdnn/model.py,train.py,decode.py,and asr_datamodule.pyfrom the Github repository https://github.com/k2-fsa/icefall.</p>
<p>Table 7 :
7
Examples of extraction issues identified that were subsequently patched in the final pipeline.Time and Cost Expenditure.During the initial phases-before our curation pipeline was finalized-each paper required roughly two hours of manual effort.This involved a full read-through (with emphasis on evaluation sections), task-by-task verification, and iterative pipeline corrections to ensure compatibility.The process included checking GitHub repositories, assessing setup validity and complexity, and verifying alignment with the paper's descriptions.Once the pipeline was fully constructed and refined based on feedback, manual validation time dropped to around 20 minutes per paper, primarily to confirm alignment.Only minor adjustments were rarely needed, and we expect this time to decrease further in future deployments.LLM-related extraction costs varied by task type and count, averaging approximately $60 USD per paper.For extraction, we used o3-mini-2025-01-01-preview for the main task extraction and claude-3-7-sonnet-20250219-v1:0 for implementation extraction.Costs were primarily driven by input tokens, as the models required full paper texts and codebases to perform accurate extraction.
Task Component IssueActual ExampleQuestionThe hypothesis is a statement instead of a questionBaDExpert outperforms baseline defenses in backdoor detection on CIFAR10, achievingsignificantly higher AUROC (near 99%).Conclusion data mentioned inSpecifically, can PDF achieve around 34.64%the hypothesislower MACs compared to PatchTST and74.38% lower MACs ...?Masked source doesn't exist"source": ["/workspace/-Masked sourcetopomlp_setA_r50_w_otransform.py"...]Included masked source withMuSc has musc.py under workspace/model/wrong pathbut the source file indicates it underworkspace/example/Steps are too specificRun the evaluation script for the baselineRequirementsEVA-CLIP ViT-B/16 model using distributed processing with 8 GPUs...Asking the agent to use aMerge the trained models using themasked source scriptheal_tools.py script(/workspace/opencood/tools/heal_tools.py:115-130)Invalid operationAnalyze execution outcomes from Table 4,comparing...ExpectedConclusion not aligned with theN/Aoutcomepaper's findingsMethod / UsageMentioned specific parts of theThe scripts will log metrics including meanInstruction /paper (tables or figures)rewards and standard deviations, which canAgent Instructionbe compared with the reported results inTable 2 of the paper.Required hyperparameters notSet appropriate model architecturegiven in the agent instructionparameters (encoder layers, attention heads,dimensions)Invalid operationsCollect and analyze performance results fromTable 3, ...</p>
<p>Table 8 :
8
Agents fail in diverse ways across different phases of experimentation, measured across all agent and model evaluations.
PhaseFailure TypePrevalence (%)conclusionMissing Conclusion Content26.18conclusionIncorrect Conclusion Interpretation19.66conclusionIncomplete Conclusion Outcome Statement14.43conclusionExtraneous Details7.77conclusionMissing Conclusion Analysis4.35conclusionMissing Comparative Conclusion Analysis4.03conclusionMinor Omission of Specific Details3.47conclusionIncorrect Numeric Conclusion3.21conclusionMismatched Conclusion Format2.7conclusionError Message Output2.67conclusionIncomplete Conclusion with Missing Exp. Findings2.14conclusionConclusion Diverges from Expected Emphasis1.6conclusionMissing Comparative Analysis0.8conclusionMissing Quantitative Performance Metrics0.8conclusionMissing Visualization Details0.56conclusionIncomplete Performance Evaluation0.53conclusionMissing Numerical Equivalence Verification0.53conclusionMissing Trend Analysis0.53conclusionNaming Inconsistency Output0.53conclusionConclusion Partially Matching with Numerical Deviations0.27conclusionDeviation in Saturation Point Conclusion0.27conclusionInconsistent ASR Reporting0.27conclusionMissing Conclusion Analysis on Attack Budget Effects0.27conclusionMissing Diminishing Returns Analysis0.27conclusionMissing Methodological Innovation Discussion0.27conclusionMissing Performance Evaluation Metrics0.27conclusionMissing Submission Format Specification0.27designIncomplete or Misclassified Design Variables16.05designOmission of Required Design Variables19.84designComplete Omission of Exp. Design Variables13.1designIncorrect Design Specification Details8.32designIncomplete Exp. Design Details7.67designIrrelevant Procedural Additions7.62designMissing Design Variable Information3.83designInclusion of Extraneous Factors3.64designIncorrect Parameter Details3.18designPartial Omission of Constant Variables2.75designIncomplete Constant Variable Specification3.61designPartial Fulfillment of DV1.93designError Message Returned Instead of Design Information1.27designIncomplete Differentiation of Constant and Ind. Variables1.27designMissing Dependent Variable Tracking1.06designIncomplete Exp. Design Specification0.64designIncomplete Specification of Design Variables0.64designMissing Hyperparameter Design Details0.64designPartially Complete Design Variable Specification0.64designMissing Design Formatting Details0.42designMissing Design Variables Details0.42designMissing Explicit Variable Labeling0.42designMissing Configuration File Variable0.21designMissing Input Format Details0.21
Preprint. Under review.
(a) The formulation of the task question.(b) Instructions provided to the agent.Figure 11: Task fields for Example 2.
{{ "design_evaluation_explanation": "<short explanation string>", "design_score": <integer from 0 to 100>, "design_error_analysis": "<short explanation of what was wrong with the output, i.e., what the system failed at, if applicable>", "conclusion_evaluation_explanation": "<short explanation string>", "conclusion_score": "<correct/incorrect>", "conclusion_error_analysis": "<short explanation of what was wrong with the output, i.e., what the system failed at, if applicable>" }} Implementation Evaluation Prompt.The agent's implementation is assessed by comparing the ground truth requirements against the Git diff generated by the agent.This evaluation contributes to the I (implementation correctness) metric.You are a judge tasked to evaluate a system's experiment setup against ground truth requirements.Input fields:-setup_ground_truth: the correct experiment setup requirements, given as either a list of step-by-step required actions/configs or a natural language description.-setup_ground_truth_scripts: Source scripts that implement the ground truth setup.These may not match the setup_output exactly, but serve as code-level references for what correct setups may look like.-setup_output: the system's actual changes, given as a Git diff patch (e.g., modifications to config files, scripts, etc.).-A score as an integer percentage (e.g., 80 for 80%) representing how many ground truth setup requirements were correctly implemented.-A detailed explanation of the evaluation result.-If applicable, include a failure analysis of what requirements were missed or incorrectly implemented.Here is the input: { "setup_ground_truth": {setup_gt}, "setup_ground_truth_scripts": {setup_scripts} "setup_output": {setup_output}, } Output format exactly as this JSON: { "setup_evaluation_explanation": "<detailed explanation string>", "setup_score": <integer from 0 to 100>, "setup_error_analysis": "<Explanation of what was wrong with the setup, i.e., what requirements were missed or done incorrectly, if applicable>" }
Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality. </p>
<p>A large-scale benchmark for few-shot program induction and synthesis. F Alet, J Lopez-Contreras, J Koppel, M Nye, A Solar-Lezama, T Lozano-Perez, L Kaelbling, J Tenenbaum, International Conference on Machine Learning. PMLR2021</p>
<p>Quarot: Outlier-free 4-bit inference in rotated llms. S Ashkboos, A Mohtashami, M L Croci, B Li, P Cameron, M Jaggi, D Alistarh, T Hoefler, J Hensman, 2024</p>
<p>Researchagent: Iterative research idea generation over scientific literature with large language models. J Baek, S K Jauhar, S Cucerzan, S J Hwang, arXiv:2404.077382024arXiv preprint</p>
<p>A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. Y Bang, S Cahyawijaya, N Lee, W Dai, D Su, B Wilie, H Lovenia, Z Ji, T Yu, W Chung, Q V Do, Y Xu, P Fung, 2023</p>
<p>The reversal curse: Llms trained on. L Berglund, M Tong, M Kaufmann, M Balesni, A C Stickland, T Korbak, O Evans, a is b"" fail to learn ""b is a"", 2024</p>
<p>Benchmarl: Benchmarking multi-agent reinforcement learning. M Bettini, A Prorok, V Moens, 2024</p>
<p>J Blasiok, P Nakkiran, Smooth ece: Principled reliability diagrams via kernel smoothing. 2023</p>
<p>Autonomous chemical research with large language models. D A Boiko, R Macknight, B Kline, G Gomes, Nature. 62479922023</p>
<p>Workarena++: Towards compositional planning and reasoning-based common knowledge work tasks. L Boisvert, M Thakkar, M Gasse, M Caccia, T L S D Chezelles, Q Cappart, N Chapados, A Lacoste, A Drouin, 2025</p>
<p>Torchrl: A data-driven decision-making library for pytorch. A Bou, M Bettini, S Dittert, V Kumar, S Sodhani, X Yang, G D Fabritiis, V Moens, 2023</p>
<p>Mle-bench: Evaluating machine learning agents on machine learning engineering. J S Chan, N Chowdhury, O Jaffe, J Aung, D Sherburn, E Mays, G Starace, K Liu, L Maksin, T Patwardhan, L Weng, A Ądry, 2024</p>
<p>M Chen, J Tworek, H Jun, Q Yuan, H P D O Pinto, J Kaplan, H Edwards, Y Burda, N Joseph, G Brockman, arXiv:2107.03374Evaluating large language models trained on code. 2021arXiv preprint</p>
<p>Scienceagentbench: Toward rigorous assessment of language agents for data-driven scientific discovery. Z Chen, S Chen, Y Ning, Q Zhang, B Wang, B Yu, Y Li, Z Liao, C Wei, Z Lu, arXiv:2410.050802024arXiv preprint</p>
<p>Trace is the next autodiff: Generative optimization with rich feedback, execution traces, and llms. C.-A Cheng, A Nie, A Swaminathan, 2024</p>
<p>Self-playing adversarial language game enhances llm reasoning. P Cheng, T Hu, H Xu, Z Zhang, Z Yuan, Y Dai, L Han, N Du, X Li, 2025</p>
<p>Language models as science tutors. A Chevalier, J Geng, A Wettig, H Chen, S Mizera, T Annala, M J Aragon, A R Fanlo, S Frieder, S Machado, A Prabhakar, E Thieu, J T Wang, Z Wang, X Wu, M Xia, W Xia, J Yu, J.-J Zhu, Z J Ren, S Arora, D Chen, 2024</p>
<p>K Cobbe, V Kosaraju, M Bavarian, M Chen, H Jun, L Kaiser, M Plappert, J Tworek, J Hilton, R Nakano, C Hesse, J Schulman, Training verifiers to solve math word problems. 2021</p>
<p>T M U Collaboration, J Audenaert, M Bowles, B M Boyd, D Chemaly, B Cherinka, I Ciucȃ, M Cranmer, A Do, M Grayling, E E Hayes, T Hehir, S Ho, M Huertas-Company, K G Iyer, M Jablonska, F Lanusse, H W Leung, K Mandel, J R Martínez-Galarza, P Melchior, L Meyer, L H Parker, H Qu, J Shen, M J Smith, C Stone, M Walmsley, J F Wu, The multimodal universe: Enabling large-scale machine learning with 100tb of astronomical scientific data. 2024</p>
<p>J Dai, X Pan, R Sun, J Ji, X Xu, M Liu, Y Wang, Y Yang, Safe rlhf: Safe reinforcement learning from human feedback. 2023</p>
<p>Periodicity decoupling framework for long-term series forecasting. T Dai, B Wu, P Liu, N Li, J Bao, Y Jiang, S.-T Xia, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Segvol: Universal and interactive volumetric medical image segmentation. Y Du, F Bai, T Huang, B Zhao, 2025</p>
<p>Elicit: Analyze research papers at superhuman speed. 2025</p>
<p>Drivaernet++: A large-scale multimodal car dataset with computational fluid dynamics simulations and deep learning benchmarks. M Elrefaie, F Morar, A Dai, F Ahmed, 2025</p>
<p>Domain-agnostic molecular generation with chemical feedback. Y Fang, N Zhang, Z Chen, L Guo, X Fan, H Chen, 2024</p>
<p>Mathematical capabilities of chatgpt. S Frieder, L Pinchetti, R.-R Griffiths, T Salvatori, T Lukasiewicz, P Petersen, J Berner, Advances in Neural Information Processing Systems. A Oh, T Naumann, A Globerson, K Saenko, M Hardt, S Levine, Curran Associates, Inc202336</p>
<p>Boxinggym: Benchmarking progress in automated experimental design and model discovery. K Gandhi, M Y Li, L Goodyear, L Li, A Bhaskar, M Zaman, N D Goodman, 2025</p>
<p>Large language models are not strong abstract reasoners. G Gendron, Q Bao, M Witbrock, G Dobbie, arXiv:2305.195552023arXiv preprint</p>
<p>Sciagents: Automating scientific discovery through multi-agent intelligent graph reasoning. A Ghafarollahi, M J Buehler, 2024</p>
<p>Large language models orchestrating structured reasoning achieve kaggle grandmaster level. A Grosnit, A Maraval, J Doran, G Paolo, A Thomas, R S H N Beevi, J Gonzalez, K Khandelwal, I Iacobacci, A Benechehab, arXiv:2411.035622024arXiv preprint</p>
<p>K Gu, R Shang, R Jiang, K Kuang, R.-J Lin, D Lyu, Y Mao, Y Pan, T Wu, J Yu, arXiv:2408.09667Benchmarking language model agents for data-driven science. 2024arXiv preprint</p>
<p>Ds-agent: Automated data science by empowering large language models with case-based reasoning. S Guo, C Deng, Y Wen, H Chen, Y Chang, J Wang, arXiv:2402.174532024arXiv preprint</p>
<p>Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. Y Guo, C Yang, A Rao, Z Liang, Y Wang, Y Qiao, M Agrawala, D Lin, B Dai, 2024</p>
<p>Inductive reasoning in humans and large language models. S J Han, K J Ransom, A Perfors, C Kemp, Cognitive Systems Research. 831011552024</p>
<p>Measuring massive multitask language understanding. D Hendrycks, C Burns, S Basart, A Zou, M Mazeika, D Song, J Steinhardt, 2021</p>
<p>Measuring mathematical problem solving with the math dataset. D Hendrycks, C Burns, S Kadavath, A Arora, S Basart, E Tang, D Song, J Steinhardt, 2021</p>
<p>Infiagent-dabench: Evaluating agents on data analysis tasks. X Hu, Z Zhao, S Wei, Z Chai, Q Ma, G Wang, X Wang, J Su, J Xu, M Zhu, arXiv:2401.055072024arXiv preprint</p>
<p>On the humanity of conversational ai: Evaluating the psychological portrayal of llms. J.-T Huang, W Wang, E J Li, M H Lam, S Ren, Y Yuan, W Jiao, Z Tu, M Lyu, The Twelfth International Conference on Learning Representations. 2023</p>
<p>Mlagentbench: Evaluating language agents on machine learning experimentation. Q Huang, J Vora, P Liang, J Leskovec, 2023</p>
<p>Mlagentbench: Evaluating language agents on machine learning experimentation. Q Huang, J Vora, P Liang, J Leskovec, 2024</p>
<p>Classification done right for vision-language pre-training. Z Huang, Q Ye, B Kang, J Feng, H Fan, 2024</p>
<p>Autonomous llm-driven research-from data to human-verifiable research papers. T Ifargan, L Hafner, M Kern, O Alcalay, R Kishony, NEJM AI. 21AIoa2400555, 2025</p>
<p>G Jaume, P Doucet, A H Song, M Y Lu, C Almagro-Pérez, S J Wagner, A J Vaidya, R J Chen, D F K Williamson, A Kim, F Mahmood, Hest-1k: A dataset for spatial transcriptomics and histology image analysis. 2024</p>
<p>Generative modeling of molecular dynamics trajectories. B Jing, H Stärk, T Jaakkola, B Berger, 2024</p>
<p>Dsbench: How far are data science agents from becoming data science experts?. L Jing, Z Huang, X Wang, W Yao, W Yu, K Ma, H Zhang, X Du, D Yu, 2024</p>
<p>Highly accurate protein structure prediction with alphafold. J Jumper, R Evans, A Pritzel, T Green, M Figurnov, O Ronneberger, K Tunyasuvunakool, R Bates, A Žídek, A Potapenko, nature. 59678732021</p>
<p>Curie: Toward rigorous and automated scientific experimentation with ai agents. P T J Kon, J Liu, Q Ding, Y Qiu, Z Yang, Y Huang, J Srinivasa, M Lee, M Chowdhury, A Chen, 2025</p>
<p>Paperqa: Retrievalaugmented generative agent for scientific research. J Lála, O O'donoghue, A Shtedritski, S Cox, S G Rodriques, A D White, arXiv:2312.075592023arXiv preprint</p>
<p>Lab-bench: Measuring capabilities of language models for biology research. J M Laurent, J D Janizek, N Thakkar, M Ruzo, M S Yao, M S Levine, S G Rodriques, A White, 2024</p>
<p>Mlr-copilot: Autonomous machine learning research based on large language models agents. R Li, T Patel, Q Wang, X Du, arXiv:2408.140332024arXiv preprint</p>
<p>Moganet: Multi-order gated aggregation network. S Li, Z Wang, Z Liu, C Tan, H Lin, D Wu, Z Chen, J Zheng, S Z Li, 2024</p>
<p>Musc: Zero-shot industrial anomaly classification and segmentation with mutual scoring of the unlabeled images. X Li, Z Huang, F Xue, Y Zhou, 2024</p>
<p>Competition-level code generation with alphacode. Y Li, D Choi, J Chung, N Kushman, J Schrittwieser, R Leblond, T Eccles, J Keeling, F Gimeno, A Lago, Science. 37866242022</p>
<p>Cyclenet: Enhancing time series forecasting through modeling periodic patterns. S Lin, W Lin, X Hu, W Wu, R Mo, H Zhong, 2024</p>
<p>Reasoning multi-agent behavioral topology for interactive autonomous driving. H Liu, L Chen, Y Qiao, C Lv, H Li, 2024</p>
<p>Visual instruction tuning. H Liu, C Li, Q Wu, Y J Lee, Advances in neural information processing systems. 202436</p>
<p>Repobench: Benchmarking repository-level code auto-completion systems. T Liu, C Xu, J Mcauley, 2023</p>
<p>. X Liu, H Yu, H Zhang, Y Xu, X Lei, H Lai, Y Gu, H Ding, K Men, K Yang, S Zhang, X Deng, A Zeng, Z Du, C Zhang, S Shen, T Zhang, Y Su, H Sun, M Huang, Y Dong, J Tang, 2023Agentbench: Evaluating llms as agents</p>
<p>3dgs-enhancer: Enhancing unbounded 3d gaussian splatting with view-consistent 2d diffusion priors. X Liu, C Zhou, S Huang, 2024</p>
<p>R Lou, H Xu, S Wang, J Du, R Kamoi, X Lu, J Xie, Y Sun, Y Zhang, J J Ahn, H Fang, Z Zou, W Ma, X Li, K Zhang, C Xia, L Huang, W Yin, Aaar-1.0: Assessing ai's potential to assist research. 2024</p>
<p>The ai scientist: Towards fully automated open-ended scientific discovery. C Lu, C Lu, R T Lange, J Foerster, J Clune, D Ha, arXiv:2408.062922024arXiv preprint</p>
<p>An extensible framework for open heterogeneous collaborative perception. Y Lu, Y Hu, Y Zhong, D Wang, Y Wang, S Chen, 2024</p>
<p>Badam: A memory efficient full parameter optimization method for large language models. Q Luo, H Yu, X Li, 2024</p>
<p>B P Majumder, H Surana, D Agarwal, B D Mishra, A Meena, A Prakhar, T Vora, T Khot, A Sabharwal, P Clark, arXiv:2407.01725Discoverybench: Towards data-driven discovery with large language models. 2024arXiv preprint</p>
<p>Large language models as general pattern machines. S Mirchandani, F Xia, P Florence, B Ichter, D Driess, M G Arenas, K Rao, D Sadigh, A Zeng, arXiv:2307.047212023arXiv preprint</p>
<p>Bixbench: a comprehensive benchmark for llm-based agents in computational biology. L Mitchener, J M Laurent, B Tenmann, S Narayanan, G P Wellawatte, A White, L Sani, S G Rodriques, arXiv:2503.000962025arXiv preprint</p>
<p>The conceptarc benchmark: Evaluating understanding and generalization in the arc domain. A Moskvichev, V V Odouard, M Mitchell, arXiv:2305.071412023arXiv preprint</p>
<p>Aviary: training language agents on challenging scientific tasks. S Narayanan, J D Braza, R.-R Griffiths, M Ponnapati, A Bou, J Laurent, O Kabeli, G Wellawatte, S Cox, S G Rodriques, arXiv:2412.211542024arXiv preprint</p>
<p>D Nathani, L Madaan, N Roberts, N Bashlykov, A Menon, V Moens, A Budhiraja, D Magka, V Vorotilov, G Chaurasia, D Hupkes, R S Cabral, T Shavrina, J Foerster, Y Bachrach, W Y Wang, R Raileanu, Mlgym: A new framework and benchmark for advancing ai research agents. 2025</p>
<p>R Ohana, M Mccabe, L Meyer, R Morel, F J Agocs, M Beneitez, M Berger, B Burkhart, K Burns, S B Dalziel, D B Fielding, D Fortunato, J A Goldberg, K Hirashima, Y.-F Jiang, R R Kerswell, S Maddu, J Miller, P Mukhopadhyay, S S Nixon, J Shen, R Watteaux, B R .-S. Blancard, F Rozet, L H Parker, M Cranmer, S Ho, The well: a large-scale collection of diverse physics simulations for machine learning. 2025</p>
<p>Zero bubble (almost) pipeline parallelism. P Qi, X Wan, G Huang, M Lin, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Civrealm: A learning and reasoning odyssey in civilization for decision-making agents. S Qi, S Chen, Y Li, X Kong, J Wang, B Yang, P Wong, Y Zhong, X Zhang, Z Zhang, N Liu, W Wang, Y Yang, S.-C Zhu, 2024</p>
<p>Torchopt: An efficient library for differentiable optimization. J Ren, X Feng, B Liu, X Pan, Y Fu, L Mai, Y Yang, 2022</p>
<p>Jaxmarl: Multi-agent rl environments and algorithms in jax. A Rutherford, B Ellis, M Gallici, J Cook, A Lupu, G Ingvarsson, T Willi, R Hammond, A Khan, C S De Witt, A Souly, S Bandyopadhyay, M Samvelyan, M Jiang, R T Lange, S Whiteson, B Lacerda, N Hawes, T Rocktaschel, C Lu, J N Foerster, 2024</p>
<p>S Schmidgall, Y Su, Z Wang, X Sun, J Wu, X Yu, J Liu, Z Liu, E Barsoum, arXiv:2501.04227Agent laboratory: Using llm agents as research assistants. 2025arXiv preprint</p>
<p>G Starace, O Jaffe, D Sherburn, J Aung, J S Chan, L Maksin, R Dias, E Mays, B Kinsella, W Thompson, arXiv:2504.01848Evaluating ai's ability to replicate ai research. 2025arXiv preprint</p>
<p>Scieval: A multi-level large language model evaluation benchmark for scientific research. L Sun, Y Han, Z Zhao, D Ma, Z Shen, B Chen, L Chen, K Yu, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial IntelligenceMar. 202438</p>
<p>W Sun, L Yan, X Ma, S Wang, P Ren, Z Chen, D Yin, Z Ren, Is chatgpt good at search? investigating large language models as re-ranking agents. 2024</p>
<p>The virtual lab: Ai agents design new sars-cov-2 nanobodies with experimental validation. K Swanson, W Wu, N L Bulaong, J E Pak, J Zou, bioRxiv. 2024</p>
<p>J Tang, H Lu, R Wu, X Xu, K Ma, C Fang, B Guo, J Lu, Q Chen, Y.-C Chen, Hawk, Learning to understand open-world video anomalies. 2024</p>
<p>M Tian, L Gao, S D Zhang, X Chen, C Fan, X Guo, R Haas, P Ji, K Krongchon, Y Li, S Liu, D Luo, Y Ma, H Tong, K Trinh, C Tian, Z Wang, B Wu, Y Xiong, S Yin, M Zhu, K Lieret, Y Lu, G Liu, Y Du, T Tao, O Press, J Callan, E Huerta, H Peng, Scicode: A research coding benchmark curated by scientists. 2024</p>
<p>Automl-agent: A multi-agent llm framework for full-pipeline automl. P Trirat, W Jeong, S J Hwang, arXiv:2410.029582024arXiv preprint</p>
<p>Knowledge fusion of large language models. F Wan, X Huang, D Cai, X Quan, W Bi, S Shi, 2024</p>
<p>Scimon: Scientific inspiration machines optimized for novelty. Q Wang, D Downey, H Ji, T Hope, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational Linguistics20241</p>
<p>. W Wang, S Zhang, Y Ren, Y Duan, T Li, S Liu, M Hu, Z Chen, K Zhang, L Lu, X Zhu, P Luo, Y Qiao, J Dai, W Shao, W Wang, 2024Needle in a multimodal haystack</p>
<p>Scibench: Evaluating college-level scientific problem-solving abilities of large language models. X Wang, Z Hu, P Lu, Y Zhu, J Zhang, S Subramaniam, A R Loomba, S Zhang, Y Sun, W Wang, 2024</p>
<p>X Wang, B Li, Y Song, F F Xu, X Tang, M Zhuge, J Pan, Y Song, B Li, J Singh, arXiv:2407.16741An open platform for ai software developers as generalist agents. 2024arXiv preprint</p>
<p>Neurodin: A two-stage framework for high-fidelity neural surface reconstruction. Y Wang, D Huang, W Ye, G Zhang, W Ouyang, T He, 2024</p>
<p>Emergent analogical reasoning in large language models. T Webb, K J Holyoak, H Lu, Nature Human Behaviour. 792023</p>
<p>Magicoder: Source code is all you need. Y Wei, Z Wang, J Liu, Y Ding, L Zhang, arXiv:2312.021202023arXiv preprint</p>
<p>Towards an ai co-scientist. W.-H Weng, 2025</p>
<p>H Wijk, T Lin, J Becker, S Jawhar, N Parikh, T Broadley, L Chan, M Chen, J Clymer, J Dhyani, E Ericheva, K Garcia, B Goodrich, N Jurkovic, M Kinniment, A Lajko, S Nix, L Sato, W Saunders, M Taran, B West, E Barnes, Re-bench: Evaluating frontier ai rd capabilities of language model agents against human experts. 2024</p>
<p>D Wu, J Chang, F Jia, Y Liu, T Wang, J Shen, Topomlp: A simple yet strong pipeline for driving topology reasoning. 2023</p>
<p>Clipself: Vision transformer distills itself for open-vocabulary dense prediction. S Wu, W Zhang, L Xu, S Jin, X Li, W Liu, C C Loy, 2024</p>
<p>Infllm: Training-free long-context extrapolation for llms with an efficient context memory. C Xiao, P Zhang, X Han, G Xiao, Y Lin, Z Zhang, Z Liu, M Sun, 2024</p>
<p>T Xie, X Qi, P He, Y Li, J T Wang, P Mittal, Badexpert: Extracting backdoor functionality for accurate backdoor input detection. 2023</p>
<p>Are large language models really good logical reasoners? a comprehensive evaluation and beyond. F Xu, Q Lin, J Han, T Zhao, J Liu, E Cambria, IEEE Transactions on Knowledge and Data Engineering. 2025</p>
<p>Y Xu, W Li, P Vaezipoor, S Sanner, E B Khalil, arXiv:2305.18354Llms and the abstraction and reasoning corpus: Successes, failures, and the importance of object-based representations. 2023arXiv preprint</p>
<p>Bag of tricks: Benchmarking of jailbreak attacks on llms. Z Xu, F Liu, H Liu, 2024</p>
<p>C Yang, X Wang, Y Lu, H Liu, Q V Le, D Zhou, X Chen, Large language models as optimizers. 2024</p>
<p>Buffer of thoughts: Thought-augmented reasoning with large language models. L Yang, Z Yu, T Zhang, S Cao, M Xu, W Zhang, J E Gonzalez, B Cui, 2024</p>
<p>Z Yang, L Dong, X Du, H Cheng, E Cambria, X Liu, J Gao, F Wei, arXiv:2212.10923Language models as inductive reasoners. 2022arXiv preprint</p>
<p>Large language models for automated open-domain scientific hypotheses discovery. Z Yang, X Du, J Li, J Zheng, S Poria, E Cambria, arXiv:2309.027262023arXiv preprint</p>
<p>Zipformer: A faster and better encoder for automatic speech recognition. Z Yao, L Guo, X Yang, W Kang, F Kuang, Y Yang, Z Jin, L Lin, D Povey, 2024</p>
<p>Dolphin: Closed-loop open-ended auto-research through thinking, practice, and feedback. J Yuan, X Yan, B Shi, T Chen, W Ouyang, B Zhang, L Bai, Y Qiao, B Zhou, 2025</p>
<p>De novo design of high-affinity protein binders with alphaproteo. V Zambaldi, D La, A E Chu, H Patani, A E Danson, T O Kwan, T Frerix, R G Schneider, D Saxton, A Thillaisundaram, arXiv:2409.080222024arXiv preprint</p>
<p>Voxel mamba: Group-free state space models for point cloud based 3d object detection. G Zhang, L Fan, C He, Z Lei, Z Zhang, L Zhang, 2024</p>
<p>Mlcopilot: Unleashing the power of large language models in solving machine learning tasks. L Zhang, Y Zhang, K Ren, D Li, Y Yang, 2024</p>
<p>Automl-gpt: Automatic machine learning with gpt. S Zhang, C Gong, L Wu, X Liu, M Zhou, 2023</p>
<p>Sinenet: Learning temporal dynamics in time-dependent partial differential equations. X Zhang, J Helwig, Y Lin, Y Xie, C Fu, S Wojtowytsch, S Ji, 2024</p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. L Zheng, W.-L Chiang, Y Sheng, S Zhuang, Z Wu, Y Zhuang, Z Lin, Z Li, D Li, E Xing, Advances in Neural Information Processing Systems. 362024</p>
<p>Causal-learn: Causal discovery in python. Y Zheng, B Huang, W Chen, J Ramsey, M Gong, R Cai, S Shimizu, P Spirtes, K Zhang, 2023</p>
<p>Anomalyclip: Object-agnostic prompt learning for zero-shot anomaly detection. Q Zhou, G Pang, Y Tian, S He, J Chen, 2025</p>
<p>Moe jetpack: From dense checkpoints to adaptive mixture of experts for vision tasks. X Zhu, Y Guan, D Liang, Y Chen, Y Liu, X Bai, 2024</p>
<p>Unmasking and improving data credibility: A study with datasets for training harmless language models. Z Zhu, J Wang, H Cheng, Y Liu, 2024</p>            </div>
        </div>

    </div>
</body>
</html>
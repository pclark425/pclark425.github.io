<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6007 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6007</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6007</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-121.html">extraction-schema-121</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <p><strong>Paper ID:</strong> paper-258236227</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2304.10327v2.pdf" target="_blank">Towards a Benchmark for Scientific Understanding in Humans and Machines</a></p>
                <p><strong>Paper Abstract:</strong> Scientific understanding is a fundamental goal of science, allowing us to explain the world. There is currently no good way to measure the scientific understanding of agents, whether these be humans or Artificial Intelligence systems. Without a clear benchmark, it is challenging to evaluate and compare different levels of and approaches to scientific understanding. In this Roadmap, we propose a framework to create a benchmark for scientific understanding, utilizing tools from philosophy of science. We adopt a behavioral notion according to which genuine understanding should be recognized as an ability to perform certain tasks. We extend this notion by considering a set of questions that can gauge different levels of scientific understanding, covering information retrieval, the capability to arrange information to produce an explanation, and the ability to infer how things would be different under different circumstances. The Scientific Understanding Benchmark (SUB), which is formed by a set of these tests, allows for the evaluation and comparison of different approaches. Benchmarking plays a crucial role in establishing trust, ensuring quality control, and providing a basis for performance evaluation. By aligning machine and human scientific understanding we can improve their utility, ultimately advancing scientific understanding and helping to discover new insights within machines.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6007.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6007.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AUP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Agent-Understands-Phenomenon</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A behavioral framework that defines degrees of scientific understanding of a phenomenon P by an agent A via (i) sufficiently complete representation, (ii) ability to generate internally consistent and empirically adequate explanations, and (iii) ability to establish a broad range of correct counterfactual inferences.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Behavioral testing of agent abilities using targeted question sequences (what-/why-/w-questions) in a specified context; measuring representation access, explanatory ability, and counterfactual reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Completeness of representation (information access), explanatory quality (internal consistency, empirical adequacy), breadth and correctness of counterfactual inferences; quantitative scoring via counts of correct answers with weighted question types and thresholds.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Model-agnostic LLMs (framework designed to evaluate LLMs such as ChatGPT/GPT-4 but not tied to a specific model)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Natural sciences (primarily physics) but framework is intended to be domain-general across scientific disciplines</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Not a single theory — framework evaluates whether LLM-generated explanations/hypotheses about a phenomenon P satisfy representation, explanation, and counterfactual-use conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>No empirical evaluation reported for AUP itself in this paper; AUP is proposed as a measurable framework with recommended scoring (weighted correct answers) but no numeric results provided.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Proposed to be operationalized via the Scientific Understanding Benchmark (SUB) composed of community-generated tests; no existing dataset used in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Framework explicitly aligns evaluation of machines with human assessment practices: same what-/why-/w-question tests apply to humans and machines, enabling direct comparison of scores and transfer tests.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Choice of thresholds and weights is nontrivial; generating representative question sets requires expert involvement; ensuring tests are not answerable by memorization or web-search; prompt/context design and interface affect dynamic scoring; vagueness induced by RL fine-tuning of LLMs can degrade scientific usefulness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards a Benchmark for Scientific Understanding in Humans and Machines', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6007.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6007.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AUP1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AUP1 (implementation via what-/why-/w-questions)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A concrete instantiation of AUP mapping (i) information access to what-questions, (ii) explanatory ability to why-questions, and (iii) counterfactual reasoning to w-questions; emphasizes context and prompt ordering.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Sequential prompting and test sessions where context is provided then agents answer what-/why-/w-questions; scoring based on correctness and quality across these question types.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Correctness on what-questions (factual retrieval), explanatory adequacy on why-questions (quality and relevance of explanations, sensitivity to contrast/context), and range/correctness/generalizability of counterfactuals on w-questions (parameterized and exogenous variable changes).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Model-agnostic LLMs (e.g., ChatGPT cited as an example of an interface-capable LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Scientific phenomena at concrete or general levels (examples: pendulums, spring-mass systems), primarily physics but extensible to other sciences.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Evaluation targets LLM-generated explanations and counterfactual predictions/hypotheses about specific phenomena P (e.g., how period changes with spring constant).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>No empirical numeric results reported for AUP1; paper recommends constructing tests and scoring but does not present benchmark outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Intended to be implemented within SUB; no established dataset used or released in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Designed to be applicable to humans and machines alike; allows direct measurement and comparison of abilities (e.g., LLM vs student performance on identical question types).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Risk of memorization or web-access answering what-questions; difficulty in crafting why-/w-questions that cannot be gamed; need for multiple instances and diverse formats for reliable scoring; evaluation of vague model outputs is challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards a Benchmark for Scientific Understanding in Humans and Machines', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6007.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6007.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CUP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Criterion for Understanding a Phenomenon (CUP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A philosophical biconditional criterion stating: A phenomenon P is scientifically understood iff there is an explanation of P based on an intelligible theory T that is empirically adequate and internally consistent.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Explanation-based assessment requiring identification of an intelligible theory T and checking empirical adequacy and internal consistency of explanations provided by the agent.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Existence of an explanation grounded in an intelligible theory; empirical adequacy; internal consistency (theory and explanation do not conflict with empirical data or themselves).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Model-agnostic (philosophical criterion applied to any agent generating explanations)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>General scientific theories, especially those with formal/mathematical structure (e.g., physics).</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Applies to LLM-generated explanations/theories: evaluates whether they are grounded in an intelligible theoretical framework and meet epistemic standards.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>No experimental application reported here; CUP used as theoretical foundation for the proposed benchmarking framework.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Not associated with a dataset; serves as a conceptual criterion underpinning SUB and AUP.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>CUP is the philosophical basis for treating machine explanations similarly to human explanations; the paper uses CUP to justify behavioral tests rather than mechanistic criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>CUP depends on intelligibility which can be context-sensitive; operationalizing empirical adequacy and intelligibility for LLM outputs requires concrete tests (which AUP/AUP1 aim to provide).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards a Benchmark for Scientific Understanding in Humans and Machines', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6007.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6007.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CIT1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Criterion for Intelligibility (CIT1)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A test for theory intelligibility: a scientific theory T is intelligible in context C if scientists can recognize qualitatively characteristic consequences of T without performing exact calculations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Assess whether agent can qualitatively derive characteristic consequences of a theory without exact computation (qualitative reasoning tests).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Recognition and articulation of qualitatively characteristic consequences; ability to reason qualitatively about implications of a theory.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Model-agnostic (applied to LLMs and human agents)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Primarily mathematical/formal theories in sciences like physics, but extendable where qualitative consequence recognition is meaningful.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Used to evaluate whether LLM-generated theories/explanations are intelligible by checking qualitative consequence recognition.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>No empirical results reported; CIT1 functions as a motivating criterion for measuring explanatory and counterfactual abilities.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>No datasets specified; CIT1 contributes conceptual grounding for AUP/AUP1 question design (especially why- and w-questions).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Aligns with human scientist criteria for intelligibility; paper proposes testing both humans and machines on qualitative consequence recognition.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Primarily formulated for mathematically expressed theories; alternative intelligibility criteria may be needed in other domains; operational tests require careful question design to avoid trivial answers.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards a Benchmark for Scientific Understanding in Humans and Machines', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6007.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6007.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>What-/Why-/W-question testing</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>What-, Why- and W-questions testing protocol</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A concrete testing taxonomy: what-questions measure information access, why-questions assess explanatory ability, and w-questions (what-if/what-would-happen-if) evaluate counterfactual reasoning and use of explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Administer diverse sets of what-/why-/w-questions (expert-designed) under controlled prompt/context conditions; score correctness, explanatory depth, and counterfactual generality, with weighting and thresholds to produce an overall understanding score.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Correct factual retrieval for what-questions; explanatory relevance, contrast-sensitivity, and coherence for why-questions; correctness, generality, parameterized and exogenous counterfactual handling for w-questions.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Intended for use with LLMs generally (paper mentions ChatGPT and other LLM interfaces as example test targets)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Concrete phenomena and general phenomena across scientific domains (examples in the paper focus on physics such as pendulums or spring-mass systems).</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Evaluation of LLM-generated factual answers, explanations, and counterfactual inferences about phenomena P rather than a single specific theory.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>No quantitative benchmark results provided; the paper proposes this testing taxonomy as the operational core of SUB and suggests scoring by counts and weights.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Intended to feed into the Scientific Understanding Benchmark (SUB); recommends expert-curated, diverse question collections and possibly using model-generated questions in future work.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Designed to be applicable to humans and machines allowing direct performance comparison; suggests using pre/post tests to measure transfer from teacher agents (human or AI) to students.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Designing non-trivial why- and w-questions that prevent memorization and web-sourcing is difficult; evaluating vague or RL-optimized model outputs is challenging; expert availability for answer-keying and disputes; some w-questions may lack known ground truth (could be used to probe new scientific understanding).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards a Benchmark for Scientific Understanding in Humans and Machines', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6007.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6007.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SUB</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Scientific Understanding Benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proposed open, community-governed benchmark formed by aggregating domain expert-developed tests (what-/why-/w-questions) to quantitatively evaluate scientific understanding in humans and machines.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Aggregate community-created tests administered via appropriate interfaces (multiple-choice for humans, context/chat-like encoding for LLMs), scored with weighted question types and thresholds to produce standardized understanding measures.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Number/weight of correct answers across what-/why-/w-questions, scientific correctness as verified by experts, thresholds for sufficient understanding, and additional measures for transfer and new-discovery capability.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Benchmark is model-agnostic; intended to evaluate a variety of LLMs and question-answering machines (paper references ChatGPT, GPT-4, Galactica as examples in discussion).</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Intended to cover multiple scientific disciplines, with initial focus on natural sciences (e.g., physics) and community-specific test suites.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>SUB evaluates the quality of LLM-generated explanations, hypotheses, and counterfactuals about domain phenomena rather than evaluating a specific LLM-generated theory.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>No empirical benchmark results reported; SUB is presented as a roadmap/call-to-action for communities to create and contribute tests, with no collected scoring data yet.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>SUB itself is proposed as the unifying benchmark; paper references existing NLP/LLM benchmarks (GLUE, WSC, BIGBench) as analogous but not adequate for scientific understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>SUB is explicitly designed to align machine and human testing procedures, enabling direct comparisons and transfer testing between agents.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Requires consensus on scoring thresholds and standards; needs expert-curated questions and centralized governance; confidentiality and prevention of internet leakage during testing; unknown-answer questions complicate scoring but can be used to probe potential new machine-discovered insights.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards a Benchmark for Scientific Understanding in Humans and Machines', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6007.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6007.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Understanding-transfer test (reformulated)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reformulated Scientific Understanding Transfer Test (pre/post student measurement)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A practical test for assessing whether a teacher agent (human or AI) can transfer scientific understanding: measure student's AUP-style score before and after interaction; the score increase indicates transfer effectiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Pre-test (AUP-aligned what-/why-/w-question battery), interaction/teaching episode with teacher agent, and post-test using different but equivalent questions; measure change in student's score.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Magnitude of improvement in student's test score (weighted correct answers); independent measurement of teacher's own understanding to separate teaching ability from teacher knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Applies to AI teachers broadly (paper discusses using LLMs as teacher agents), model-agnostic in specification.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>General across scientific domains; paper motivates relevance when AI may possess new knowledge needing human uptake.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Evaluates hypotheses/explanations/instruction produced by teacher agent and their effect on student's understanding of phenomenon P.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>No empirical applications reported in this paper; proposed as an improved alternative to Krenn et al.'s referee-based indistinguishability test.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Intended to be implemented using SUB test items and scoring procedures; no dataset provided in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Directly compares pre/post human student performance to infer teacher effectiveness; separates teacher's understanding from teaching skill by testing teacher independently.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Student learning variability can confound results; requires matched but non-identical pre/post questions to avoid memorization; practical issues in standardizing interaction protocols and ensuring fair assessment; Krenn et al.'s original test limitations (referee-based indistinguishability) are discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards a Benchmark for Scientific Understanding in Humans and Machines', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Understanding scientific understanding <em>(Rating: 2)</em></li>
                <li>On scientific understanding with artificial intelligence <em>(Rating: 2)</em></li>
                <li>Discovering Language Model Behaviors with Model-Written Evaluations <em>(Rating: 1)</em></li>
                <li>Sparks of Artificial General Intelligence: Early experiments with GPT-4 <em>(Rating: 1)</em></li>
                <li>Can ChatGPT Project an Understanding of Introductory Physics? <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6007",
    "paper_id": "paper-258236227",
    "extraction_schema_id": "extraction-schema-121",
    "extracted_data": [
        {
            "name_short": "AUP",
            "name_full": "Agent-Understands-Phenomenon",
            "brief_description": "A behavioral framework that defines degrees of scientific understanding of a phenomenon P by an agent A via (i) sufficiently complete representation, (ii) ability to generate internally consistent and empirically adequate explanations, and (iii) ability to establish a broad range of correct counterfactual inferences.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Behavioral testing of agent abilities using targeted question sequences (what-/why-/w-questions) in a specified context; measuring representation access, explanatory ability, and counterfactual reasoning.",
            "evaluation_criteria": "Completeness of representation (information access), explanatory quality (internal consistency, empirical adequacy), breadth and correctness of counterfactual inferences; quantitative scoring via counts of correct answers with weighted question types and thresholds.",
            "llm_model_name": "Model-agnostic LLMs (framework designed to evaluate LLMs such as ChatGPT/GPT-4 but not tied to a specific model)",
            "theory_domain": "Natural sciences (primarily physics) but framework is intended to be domain-general across scientific disciplines",
            "theory_description": "Not a single theory — framework evaluates whether LLM-generated explanations/hypotheses about a phenomenon P satisfy representation, explanation, and counterfactual-use conditions.",
            "evaluation_results": "No empirical evaluation reported for AUP itself in this paper; AUP is proposed as a measurable framework with recommended scoring (weighted correct answers) but no numeric results provided.",
            "benchmarks_or_datasets": "Proposed to be operationalized via the Scientific Understanding Benchmark (SUB) composed of community-generated tests; no existing dataset used in paper.",
            "comparison_to_human": "Framework explicitly aligns evaluation of machines with human assessment practices: same what-/why-/w-question tests apply to humans and machines, enabling direct comparison of scores and transfer tests.",
            "limitations_or_challenges": "Choice of thresholds and weights is nontrivial; generating representative question sets requires expert involvement; ensuring tests are not answerable by memorization or web-search; prompt/context design and interface affect dynamic scoring; vagueness induced by RL fine-tuning of LLMs can degrade scientific usefulness.",
            "uuid": "e6007.0",
            "source_info": {
                "paper_title": "Towards a Benchmark for Scientific Understanding in Humans and Machines",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "AUP1",
            "name_full": "AUP1 (implementation via what-/why-/w-questions)",
            "brief_description": "A concrete instantiation of AUP mapping (i) information access to what-questions, (ii) explanatory ability to why-questions, and (iii) counterfactual reasoning to w-questions; emphasizes context and prompt ordering.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Sequential prompting and test sessions where context is provided then agents answer what-/why-/w-questions; scoring based on correctness and quality across these question types.",
            "evaluation_criteria": "Correctness on what-questions (factual retrieval), explanatory adequacy on why-questions (quality and relevance of explanations, sensitivity to contrast/context), and range/correctness/generalizability of counterfactuals on w-questions (parameterized and exogenous variable changes).",
            "llm_model_name": "Model-agnostic LLMs (e.g., ChatGPT cited as an example of an interface-capable LLM)",
            "theory_domain": "Scientific phenomena at concrete or general levels (examples: pendulums, spring-mass systems), primarily physics but extensible to other sciences.",
            "theory_description": "Evaluation targets LLM-generated explanations and counterfactual predictions/hypotheses about specific phenomena P (e.g., how period changes with spring constant).",
            "evaluation_results": "No empirical numeric results reported for AUP1; paper recommends constructing tests and scoring but does not present benchmark outcomes.",
            "benchmarks_or_datasets": "Intended to be implemented within SUB; no established dataset used or released in the paper.",
            "comparison_to_human": "Designed to be applicable to humans and machines alike; allows direct measurement and comparison of abilities (e.g., LLM vs student performance on identical question types).",
            "limitations_or_challenges": "Risk of memorization or web-access answering what-questions; difficulty in crafting why-/w-questions that cannot be gamed; need for multiple instances and diverse formats for reliable scoring; evaluation of vague model outputs is challenging.",
            "uuid": "e6007.1",
            "source_info": {
                "paper_title": "Towards a Benchmark for Scientific Understanding in Humans and Machines",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "CUP",
            "name_full": "Criterion for Understanding a Phenomenon (CUP)",
            "brief_description": "A philosophical biconditional criterion stating: A phenomenon P is scientifically understood iff there is an explanation of P based on an intelligible theory T that is empirically adequate and internally consistent.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_method": "Explanation-based assessment requiring identification of an intelligible theory T and checking empirical adequacy and internal consistency of explanations provided by the agent.",
            "evaluation_criteria": "Existence of an explanation grounded in an intelligible theory; empirical adequacy; internal consistency (theory and explanation do not conflict with empirical data or themselves).",
            "llm_model_name": "Model-agnostic (philosophical criterion applied to any agent generating explanations)",
            "theory_domain": "General scientific theories, especially those with formal/mathematical structure (e.g., physics).",
            "theory_description": "Applies to LLM-generated explanations/theories: evaluates whether they are grounded in an intelligible theoretical framework and meet epistemic standards.",
            "evaluation_results": "No experimental application reported here; CUP used as theoretical foundation for the proposed benchmarking framework.",
            "benchmarks_or_datasets": "Not associated with a dataset; serves as a conceptual criterion underpinning SUB and AUP.",
            "comparison_to_human": "CUP is the philosophical basis for treating machine explanations similarly to human explanations; the paper uses CUP to justify behavioral tests rather than mechanistic criteria.",
            "limitations_or_challenges": "CUP depends on intelligibility which can be context-sensitive; operationalizing empirical adequacy and intelligibility for LLM outputs requires concrete tests (which AUP/AUP1 aim to provide).",
            "uuid": "e6007.2",
            "source_info": {
                "paper_title": "Towards a Benchmark for Scientific Understanding in Humans and Machines",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "CIT1",
            "name_full": "Criterion for Intelligibility (CIT1)",
            "brief_description": "A test for theory intelligibility: a scientific theory T is intelligible in context C if scientists can recognize qualitatively characteristic consequences of T without performing exact calculations.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_method": "Assess whether agent can qualitatively derive characteristic consequences of a theory without exact computation (qualitative reasoning tests).",
            "evaluation_criteria": "Recognition and articulation of qualitatively characteristic consequences; ability to reason qualitatively about implications of a theory.",
            "llm_model_name": "Model-agnostic (applied to LLMs and human agents)",
            "theory_domain": "Primarily mathematical/formal theories in sciences like physics, but extendable where qualitative consequence recognition is meaningful.",
            "theory_description": "Used to evaluate whether LLM-generated theories/explanations are intelligible by checking qualitative consequence recognition.",
            "evaluation_results": "No empirical results reported; CIT1 functions as a motivating criterion for measuring explanatory and counterfactual abilities.",
            "benchmarks_or_datasets": "No datasets specified; CIT1 contributes conceptual grounding for AUP/AUP1 question design (especially why- and w-questions).",
            "comparison_to_human": "Aligns with human scientist criteria for intelligibility; paper proposes testing both humans and machines on qualitative consequence recognition.",
            "limitations_or_challenges": "Primarily formulated for mathematically expressed theories; alternative intelligibility criteria may be needed in other domains; operational tests require careful question design to avoid trivial answers.",
            "uuid": "e6007.3",
            "source_info": {
                "paper_title": "Towards a Benchmark for Scientific Understanding in Humans and Machines",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "What-/Why-/W-question testing",
            "name_full": "What-, Why- and W-questions testing protocol",
            "brief_description": "A concrete testing taxonomy: what-questions measure information access, why-questions assess explanatory ability, and w-questions (what-if/what-would-happen-if) evaluate counterfactual reasoning and use of explanations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Administer diverse sets of what-/why-/w-questions (expert-designed) under controlled prompt/context conditions; score correctness, explanatory depth, and counterfactual generality, with weighting and thresholds to produce an overall understanding score.",
            "evaluation_criteria": "Correct factual retrieval for what-questions; explanatory relevance, contrast-sensitivity, and coherence for why-questions; correctness, generality, parameterized and exogenous counterfactual handling for w-questions.",
            "llm_model_name": "Intended for use with LLMs generally (paper mentions ChatGPT and other LLM interfaces as example test targets)",
            "theory_domain": "Concrete phenomena and general phenomena across scientific domains (examples in the paper focus on physics such as pendulums or spring-mass systems).",
            "theory_description": "Evaluation of LLM-generated factual answers, explanations, and counterfactual inferences about phenomena P rather than a single specific theory.",
            "evaluation_results": "No quantitative benchmark results provided; the paper proposes this testing taxonomy as the operational core of SUB and suggests scoring by counts and weights.",
            "benchmarks_or_datasets": "Intended to feed into the Scientific Understanding Benchmark (SUB); recommends expert-curated, diverse question collections and possibly using model-generated questions in future work.",
            "comparison_to_human": "Designed to be applicable to humans and machines allowing direct performance comparison; suggests using pre/post tests to measure transfer from teacher agents (human or AI) to students.",
            "limitations_or_challenges": "Designing non-trivial why- and w-questions that prevent memorization and web-sourcing is difficult; evaluating vague or RL-optimized model outputs is challenging; expert availability for answer-keying and disputes; some w-questions may lack known ground truth (could be used to probe new scientific understanding).",
            "uuid": "e6007.4",
            "source_info": {
                "paper_title": "Towards a Benchmark for Scientific Understanding in Humans and Machines",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "SUB",
            "name_full": "Scientific Understanding Benchmark",
            "brief_description": "A proposed open, community-governed benchmark formed by aggregating domain expert-developed tests (what-/why-/w-questions) to quantitatively evaluate scientific understanding in humans and machines.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Aggregate community-created tests administered via appropriate interfaces (multiple-choice for humans, context/chat-like encoding for LLMs), scored with weighted question types and thresholds to produce standardized understanding measures.",
            "evaluation_criteria": "Number/weight of correct answers across what-/why-/w-questions, scientific correctness as verified by experts, thresholds for sufficient understanding, and additional measures for transfer and new-discovery capability.",
            "llm_model_name": "Benchmark is model-agnostic; intended to evaluate a variety of LLMs and question-answering machines (paper references ChatGPT, GPT-4, Galactica as examples in discussion).",
            "theory_domain": "Intended to cover multiple scientific disciplines, with initial focus on natural sciences (e.g., physics) and community-specific test suites.",
            "theory_description": "SUB evaluates the quality of LLM-generated explanations, hypotheses, and counterfactuals about domain phenomena rather than evaluating a specific LLM-generated theory.",
            "evaluation_results": "No empirical benchmark results reported; SUB is presented as a roadmap/call-to-action for communities to create and contribute tests, with no collected scoring data yet.",
            "benchmarks_or_datasets": "SUB itself is proposed as the unifying benchmark; paper references existing NLP/LLM benchmarks (GLUE, WSC, BIGBench) as analogous but not adequate for scientific understanding.",
            "comparison_to_human": "SUB is explicitly designed to align machine and human testing procedures, enabling direct comparisons and transfer testing between agents.",
            "limitations_or_challenges": "Requires consensus on scoring thresholds and standards; needs expert-curated questions and centralized governance; confidentiality and prevention of internet leakage during testing; unknown-answer questions complicate scoring but can be used to probe potential new machine-discovered insights.",
            "uuid": "e6007.5",
            "source_info": {
                "paper_title": "Towards a Benchmark for Scientific Understanding in Humans and Machines",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "Understanding-transfer test (reformulated)",
            "name_full": "Reformulated Scientific Understanding Transfer Test (pre/post student measurement)",
            "brief_description": "A practical test for assessing whether a teacher agent (human or AI) can transfer scientific understanding: measure student's AUP-style score before and after interaction; the score increase indicates transfer effectiveness.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Pre-test (AUP-aligned what-/why-/w-question battery), interaction/teaching episode with teacher agent, and post-test using different but equivalent questions; measure change in student's score.",
            "evaluation_criteria": "Magnitude of improvement in student's test score (weighted correct answers); independent measurement of teacher's own understanding to separate teaching ability from teacher knowledge.",
            "llm_model_name": "Applies to AI teachers broadly (paper discusses using LLMs as teacher agents), model-agnostic in specification.",
            "theory_domain": "General across scientific domains; paper motivates relevance when AI may possess new knowledge needing human uptake.",
            "theory_description": "Evaluates hypotheses/explanations/instruction produced by teacher agent and their effect on student's understanding of phenomenon P.",
            "evaluation_results": "No empirical applications reported in this paper; proposed as an improved alternative to Krenn et al.'s referee-based indistinguishability test.",
            "benchmarks_or_datasets": "Intended to be implemented using SUB test items and scoring procedures; no dataset provided in the paper.",
            "comparison_to_human": "Directly compares pre/post human student performance to infer teacher effectiveness; separates teacher's understanding from teaching skill by testing teacher independently.",
            "limitations_or_challenges": "Student learning variability can confound results; requires matched but non-identical pre/post questions to avoid memorization; practical issues in standardizing interaction protocols and ensuring fair assessment; Krenn et al.'s original test limitations (referee-based indistinguishability) are discussed.",
            "uuid": "e6007.6",
            "source_info": {
                "paper_title": "Towards a Benchmark for Scientific Understanding in Humans and Machines",
                "publication_date_yy_mm": "2023-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Understanding scientific understanding",
            "rating": 2,
            "sanitized_title": "understanding_scientific_understanding"
        },
        {
            "paper_title": "On scientific understanding with artificial intelligence",
            "rating": 2,
            "sanitized_title": "on_scientific_understanding_with_artificial_intelligence"
        },
        {
            "paper_title": "Discovering Language Model Behaviors with Model-Written Evaluations",
            "rating": 1,
            "sanitized_title": "discovering_language_model_behaviors_with_modelwritten_evaluations"
        },
        {
            "paper_title": "Sparks of Artificial General Intelligence: Early experiments with GPT-4",
            "rating": 1,
            "sanitized_title": "sparks_of_artificial_general_intelligence_early_experiments_with_gpt4"
        },
        {
            "paper_title": "Can ChatGPT Project an Understanding of Introductory Physics?",
            "rating": 1,
            "sanitized_title": "can_chatgpt_project_an_understanding_of_introductory_physics"
        }
    ],
    "cost": 0.012025999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Towards a Benchmark for Scientific Understanding in Humans and Machines</p>
<p>Kristian Gonzalez Barman kristian@gonzalezbarman@ru.nl 
Institute for Science in Society
Faculty of Science
Radboud University
the Netherlands</p>
<p>Sascha Caron scaron@nikhef.nl 
High Energy Physics
Faculty of Science
Radboud University
the Netherlands</p>
<p>Science Park 1051098 XGNikhef, Amsterdamthe Netherlands</p>
<p>Tom Claassen tomc@cs.ru.nl 
Institute for Computing and Information Sciences
Faculty of Science
Radboud University
the Netherlands</p>
<p>Henk De Regt henk.deregt@ru.nl 
Institute for Science in Society
Faculty of Science
Radboud University
the Netherlands</p>
<p>Towards a Benchmark for Scientific Understanding in Humans and Machines
2D48B9BBFDAF3CCA554596128F562A21
Scientific understanding is a fundamental goal of science, allowing us to explain the world.There is currently no good way to measure the scientific understanding of agents, whether these be humans or Artificial Intelligence systems.Without a clear benchmark, it is challenging to evaluate and compare different levels of and approaches to scientific understanding.In this Roadmap, we propose a framework to create a benchmark for scientific understanding, utilizing tools from philosophy of science.We adopt a behavioral notion according to which genuine understanding should be recognized as an ability to perform certain tasks.We extend this notion by considering a set of questions that can gauge different levels of scientific understanding, covering information retrieval, the capability to arrange information to produce an explanation, and the ability to infer how things would be different under different circumstances.The Scientific Understanding Benchmark (SUB), which is formed by a set of these tests, allows for the evaluation and comparison of different approaches.Benchmarking plays a crucial role in establishing trust, ensuring quality control, and providing a basis for performance evaluation.By aligning machine and human scientific understanding we can improve their utility, ultimately advancing scientific understanding and helping to discover new insights within machines.</p>
<p>Introduction</p>
<p>This Roadmap presents a framework for measuring scientific understanding in agents, including humans, machine learning models, and model-augmented humans 1 2 .Current benchmarks in Machine Learning measure a variety of capabilities 3 4 .For example, the Winograd Schema Challenge 5 (WSC) and the General Language Understanding Evaluation 6 (GLUE) measure linguistic understanding, while BIGBench 7 measures proficiency at several tasks such as simple logic problems or guessing a chess move.However, despite their need and importance, there are currently no benchmarks that measure the degree of scientific understanding.To address this gap, we provide definitions of scientific understanding, a framework for how it can be measured, and discuss potential use cases such as discovering new insights within machines.</p>
<p>The primary objective of this Roadmap is to provide a foundation for benchmarking and measuring the scientific understanding of various agents (including Large Language Models 8 9 and Question Answering Machines 10 ).Although our focus is on scientific understanding 11 12 in the natural sciences (e.g., physics), we anticipate that our framework can be applied to numerous other scientific disciplines.We break with the traditional view 13 14 15 16 17 by arguing that understanding should be conceptualized in terms of abilities rather than internal mechanics 18 19 or representations 14 20 .Specifically, we contend that scientific understanding is a skill-based capability that relies on an agent's ability to perform specific actions, rather than a subjective mental state.This perspective separates the subjective 'feeling' of understanding from genuine understanding, indicating that psychological states are neither sufficient nor necessary to establish understanding 21 .We preliminarily define 22 12 scientific understanding as the ability to provide explanations within a theoretical framework that is intelligible to the agent, which involves the ability to derive qualitative results, answer questions, solve problems properly, and extend knowledge to other domains or levels of abstraction.We argue that various degrees of scientific understanding can be measured by different levels of ability, such as having access to relevant information, the ability to provide explanations, and the ability to establish counterfactual inferences.These different levels can be quantitatively evaluated using what-, why-, and w-questions (see below).</p>
<p>Our framework enables creating specific tests, which can be used for benchmarking models, measuring student understanding, and evaluating teaching abilities, or training a machine learning model.We provide guidelines for researchers, including different testing interfaces.We then propose the creation of a benchmark for scientific understanding.This benchmark is an important first step towards assessing machine understanding, where aligning machine understanding with human understanding can help in research tasks, such as hypothesis creation and information retrieval and summarization.</p>
<p>It should be noted that the tests stemming from our framework differ significantly from the Turing Test 23 24 .Their focus is not on evaluating general intelligence but rather scientific understanding.The score one obtains in scientific understanding has no meaning as to whether an agent has AGI status or other abilities beyond scientific understanding.Similarly, passing the Turing Test does not necessarily mean an agent would score high on the scientific understanding test.</p>
<p>We compare our framework to a recently proposed 25 student-teacher interaction as a test of scientific understanding in machines and show how incorporating elements of our framework could improve this test and offer a quantifiable measure of transfer of understanding between agents.While their approach to testing focuses on evaluating new understanding, our framework is capable of testing existing understanding as well as new understanding, where new understanding might involve increasing the understanding of phenomena (e.g., by providing deeper explanations) or discovering new phenomena.Finally, we discuss the abilities and limitations of our framework considering popular LLM implementations.</p>
<p>Our contributions can be summarized in the following points:</p>
<p>• Scientific understanding is an ability and should therefore be measured in terms of behavioral competence (i.e., actions).• A framework that provides a basis for developing tests to measure scientific understanding both in human and artificial agents.The framework can be used for benchmarking models, assessing student understanding, and training machine learning models.• Guidelines for implementing tests to measure the scientific understanding of Large Language Models (LLMs) together with a call for scientific communities to systematically engage in benchmarking question-answering machines, to foster specific developments such as testing new scientific understanding (i.e., discovery of new insights unknown to humankind) within machines.• A discussion on how to test whether a machine has transferred scientific understanding to another agent, building on a recently proposed account.</p>
<p>Scientific understanding as an ability</p>
<p>Scientific understanding is traditionally viewed as an internal mental state or representation possessed by an agent, such as a human scientist. 26 27This perspective focuses on the subjective and internal aspects of understanding, such as mental representations or the "feeling" of understanding, rather than the observable aspects of the agent's abilities and actions.</p>
<p>Floridi 28 appears to endorse this traditional view, expressing skepticism about the capacity of Large Language Models (LLMs) to achieve any degree of understanding, as they do not reason or resemble the cognitive processes present in animal or human brains.Critics of LLMs often argue that agents must have the same underlying mechanisms as humans to understand.However, this argument is insufficient, as it is unclear why understanding could not be realizable through different mechanisms.Moreover, human understanding is rarely assessed based on underlying mechanisms, but rather on behavioral observations, such as answering questions in an exam.</p>
<p>Here we propose a re-evaluation of scientific understanding, arguing that it should be assessed based on an agent's ability to perform certain tasks, rather than the underlying mechanisms involved in those tasks.This means that artificial agents, including Large Language Models (LLMs) should not be dismissed as incapable of scientific understanding simply because they "guess the next word" or are "stochastic parrots" 29 .By integrating philosophical accounts with empirical and theoretical bases, we critically evaluate machine understanding, contending that scientific understanding is an ability that should be assessed based on behavior, rather than mental representations, internal architecture, consciousness, or other similar factors.</p>
<p>We maintain that evaluating all agents, including LLMs, should follow the same principles used to assess human scientific understanding-based on their abilities to perform relevant tasks.We are not the only ones who relate understanding to an ability 30 25 20 .For example, Tamir and Shech 20 have argued that practical abilities (such as reliable and robust task performance) can be seen as key factors indicative of understanding in the context of deep learning.While we think this is a good start, we argue that a more comprehensive and rigorous evaluation of understanding as an ability is needed.That is precisely what this Roadmap aims to achieve.</p>
<p>A Framework for Scientific Understanding</p>
<p>Our starting point is de Regt's 12 account of scientific understanding, on which understanding a phenomenon boils down to having an adequate explanation of the phenomenon within the right theoretical scaffolding.The formal criterion is the following 31 (Criterion for Understanding a Phenomenon):</p>
<p>CUP: A phenomenon P is understood scientifically if and only if there is an explanation of P that is based on an intelligible theory T and conforms to the basic epistemic values of empirical adequacy and internal consistency.</p>
<p>Note that the use of a biconditional indicates that this is a necessary and sufficient condition.The explanation must be based on an intelligible theory.A test for intelligibility can described by the following criterion 32 :</p>
<p>CIT1: A scientific theory T (in one or more of its representations) is intelligible for scientists (in context C) if they can recognize qualitatively characteristic consequences of T without performing exact calculations.</p>
<p>In this case, the condition is sufficient but not necessary.This is also why there is a subscript 1 in CIT1, since there might be other criteria for intelligibility.CIT1 holds primarily for theories with a mathematical formulation, such as in physics.For other types of theories, other conditions might hold.The key aspect of this condition is the ability to derive (qualitative) consequences.</p>
<p>To elaborate this conception of scientific understanding, we modify the definition by shifting the focus from the phenomenon being understood to the conditions required for an agent to understand.We develop these conditions into having access to information, having explanatory abilities (since they might not coincide), and reformulate the ability to recognize qualitatively characteristic consequences in terms of counterfactual inferences.Moreover, we refine the definition by emphasizing the importance of measuring understanding instead of relying on strict necessary and sufficient conditions.Doing so acknowledges that understanding is not a binary affair but rather a matter of degree.These extensions result in the following general framework for an agent's scientific understanding (Agent-Understands-Phenomenon):</p>
<p>AUP:</p>
<p>The degree to which agent A scientifically understands phenomenon P can be determined by assessing the extent to which (i) A has a sufficiently complete representation of P; (ii) A can generate internally consistent and empirically adequate explanations of P; (iii) A can establish a broad range of relevant, correct counterfactual inferences regarding P.</p>
<p>AUP is our framework for establishing the degree of scientific understanding of a phenomenon (by an agent).This framework can be instantiated in different ways (e.g., there might be several ways of establishing whether A has a sufficiently complete representation of P).One implementation would be AUP1 :</p>
<p>AUP1(i-iii) can be measured, given a certain context (series of prompts) via what-, why, and w-questions respectively.</p>
<p>These questions are prompt specific, where the context (i.e., initial prompting to provide necessary information) and the ordering of questions makes a difference.This feature makes the application AUP1 dynamic.</p>
<p>The first level ('i') of AUP requires having access to sufficient relevant information about P. This access involves the capacity to retrieve information from relevant sources (such as memories, encodings/embeddings, databases, or the internet).We argue that this can be measured by the ability to provide correct answers to 'what-questions' (see section below).</p>
<p>The second level ('ii') refers to the capability of arranging information to produce an explanation of P. The ability to generate a well-constructed explanation surpasses simple information retrieval, requiring a deeper level of understanding 33 .We argue that the ability to provide explanations can be evaluated through answers to why-questions (see section below).</p>
<p>The third level ('iii') is the ability to infer how P would have been (or would be) different under different circumstances; namely, the ability to draw counterfactual inferences.This ability requires being able to properly use a (good) explanation 34 35 36 .We argue that this ability can be measured via answers to w-questions (see section below).Answers to w-questions require more than simply having an explanation, they require having a good explanation and knowing how to use it (e.g., knowing when the explanation is applicable, what the boundary conditions are, etc.).W-questions assess competency at establishing counterfactual inferences concerning a phenomenon and can be linked to an agents' breadth and depth of understanding 37 .</p>
<p>Test Questions</p>
<p>What-questions</p>
<p>What-questions ask for descriptive knowledge about an object or phenomenon 38 39 .Answering such questions requires having access to information, whether from memory or external sources (books, servers, etc.).What-questions can ask for values, dimensions, or names, among other things.For example, what is the charge of the electron?</p>
<p>The ability to answer what-questions correctly is a necessary but not a sufficient condition for understanding, as it only involves the retrieval of information and not the ability to use said information for higherlevel tasks (see section above).</p>
<p>Why-questions (explanation-seeking questions)</p>
<p>Answering why-questions 40 41 42 which inquire about facts or phenomena requires providing an explanation.It is for this reason that answering them correctly is highly indicative of scientific understanding.Why-questions can be divided into (at least) three types:</p>
<ol>
<li>Questions of singular facts: 'Why is it the case that A?' ('Why is charge conserved?') 2. Contrastive questions 43 : 'Why A rather than B' ('Why did Patient rather than Patient B get better with treatment T?', 'Why did Patient P get better using treatment A rather than treatment B?') 3. Resemblance questions 44 : 'Why do A and B share C' ('Why do both hedgehogs and bears hibernate?')</li>
</ol>
<p>Answering why-questions requires articulating information in a way that is sensitive to context and explanatory aims 43 .Using a variety of why-questions with answers not easily found (e.g., by choosing different foils or contrast classes in the case of contrastive explanations) can help ensure an explanatory ability that is not simply due to memorization or accessing the internet.</p>
<p>W-questions (counterfactual inferences)</p>
<p>W-questions refer to what-if-things-had-been-different questions 34 and what-would-happen-if questions 45 .These questions explore alternative scenarios and potential outcomes based on a hypothetical change in circumstances.Answers to what-if-things-had-been-different questions enable us to see what the outcome of some state of affairs would have been if initial conditions had been different.Answers to what-would-happen-if questions can be thought of as a prediction that involves some sort of manipulation or intervention on a system 45 .</p>
<p>Answering these two types of questions can be thought of as backward-looking and forwardlooking counterfactual inferences.In both cases, answering these questions involves postulating hypothetical scenarios about what would occur under a specific set of circumstances.It is this feature that we are interested in, since the ability to adequately derive these scenarios requires understanding.Similarly, there is a strong link between the quality of an explanation and the counterfactual inferences it affords 34 46 47 .We therefore contend that the range of counterfactual inferences an agent can articulate is strongly related to the level of understanding 48 .Counterfactual inferences can be more or less general, where one can distinguish between parameterized and exogenous variable counterfactual inferences 49 50 .Parameterized counterfactual inferences involve changing specific variables within a model, such as "What would happen to the period of this spring-mass system if we changed the spring constant?".Exogenous variable counterfactual inferences involve changing external variables that impact the system, such as "What would happen to this spring-mass system if the spring breaks?".</p>
<p>From a general framework to specific tests</p>
<p>In this section we discuss how to operationalize the framework described in the previous section into concrete tests to measure scientific understanding.Understanding can be of a concrete phenomenon (e.g., a pendulum of 5m length, 2kg weight, etc.) or of a general phenomenon (e.g., pendulums in general).Tests can be devised for both specific and general phenomena, and the level of generality can be increased by asking higher-level w-questions, such as those related to changing exogenous variables.</p>
<p>How to score an agent?</p>
<p>The level of scientific understanding of an agent can be thought of as a gradient between complete lack of understanding to an ever-increasing level (See figure 1).The agent's score would depend on the number of correct answers, with varying weights assigned to different questions.This test can provide a specific score for the scientific understanding of an agent or compare two agents.We can establish different thresholds depending on the context.</p>
<p>Figure 1. Scientific understanding can be categorized into various levels based on the number of questions answered. An agent possessing the ability to answer all the questions posed by scientific communities, including those for which we do not yet have answers, indicates a higher level of (new) scientific understanding.</p>
<p>Each individual test should contain a sufficiently diverse and representative set of questions that can capture enough details of the properties, attributes, and elements of the phenomenon.This implies that question generation should be conducted by the experts of each community.We provide some guidelines for this below.In some cases, it could be possible to train language models to produce questions as well 51 52 53 .</p>
<p>Guidelines for testing</p>
<p>There is a need for guidelines to establish a standardized and reliable approach to testing which ensures accurate and consistent results.In developing a comprehensive and reliable test for AI agents, it is crucial to define the test scope and purpose, ensuring it is tailored to the agent and includes a variety of difficulty levels.To achieve consistent, repeatable scoring, diverse question formats should be employed, with multiple testing instances conducted for robust evaluation.Crafting comprehensive, varied, and representative questions is essential, using concise and unambiguous language to prevent confusion.To maintain test integrity, limit answer accessibility on the internet and other public repositories.Finally, centralized storage of tests for easy review, enabling them to serve as part of the SUB benchmark.Additional guidelines for good testing and evaluation can also be implemented 54 55 56 57 .</p>
<p>It is important to note that this test should be conducted through an interface, which may need to be tailored for certain agents.Traditional testing methods such as multiple-choice tests can be used for humans and model-augmented humans, while an interface that allows for context encoding and some form of chat-like interface (e.g., ChatGPT 58 ) is needed for LLMs.</p>
<p>The Scientific Understanding Benchmark (SUB)</p>
<p>We propose two things.First, a call to communities to create tests for scientific understanding to benchmark different AI models.Benchmarking plays a crucial role in establishing trust in the reliability of models, ensuring quality control, and providing a basis for performance evaluation.Given the current situation in AI it is thereby of high societal relevance. 59Second, the bringing together of tests developed by different communities into a broad benchmark, which we call the Scientific Understanding Benchmark (SUB).This should be an open project supervised by an independent community of experts that, among other criteria, sets high standards for scientific correctness.</p>
<p>We firmly believe that the SUB will have a positive impact on the usefulness utility, confidence, and controllability of AI in scientific research and expect it to advance scientific understanding, facilitate stakeholder alignment, and enable new discoveries.</p>
<p>Scientific understanding transfer</p>
<p>Krenn et al. 25 closely follow an earlier version of de Regt's (2017) account, namely, de Regt and Dieks (2005) 22 .Based on CIT (see the section on Scientific Understanding) they formulate a parallel condition replacing the scientist(s) with an AI.Subsequently, they add an additional condition, according to which 'An AI gained scientific understanding if it can transfer its understanding to a human expert' 25 .They then combine these two conditions into a test, which they describe as follows 25 :</p>
<p>A human (the student) interacts with a teacher, either a human or an artificial scientist.The teacher's goal is to explain a scientific theory and its qualitative, characteristic consequences to the student.Another human (the referee) tests both the student and the teacher independently.If the referee cannot distinguish between the qualities of their non-trivial explanations in various contexts, we argue that the teacher has scientific understanding.</p>
<p>While promising, this approach may have a few issues.First, it equates teaching abilities with understanding, which is problematic if the student is simply a bad learner (despite the teacher's understanding).Second, the referee determines understanding by comparing the qualities of explanations.If both teacher and student lack understanding (whether because they simply lack explanations or because their explanations are incorrect), their explanations may be indistinguishable (by being equally wrong).According to the test, we should conclude the teacher has understanding.Third, the test is difficult to implement in practice due to vague parameters, such as the referee's inability to distinguish between non-trivial explanations in different contexts.The quality of explanations depends on explanatory aims and the variety of contexts is unclear, leading to different results depending on the chosen referee.</p>
<p>Despite its limitations, we think Krenn et al.'s test is valuable, and we propose a reformulation of it using our framework.Instead of relying on a referee, we propose to measure the student's score before and after interacting with a teacher to demonstrate an increase in scientific understanding (by the student).Additionally, we can test the teacher's understanding separately to distinguish between the teacher's own understanding and their ability to transfer that understanding to the student.We then suggest the reformulated test for scientific understanding transfer:</p>
<p>The student takes an initial test, interacts with the teacher, and then takes a second test.While the second test should cover the same material or aspects, it should contain different questions to ensure the validity of the test.The extent to which the student's score increases on the second test is an indication of the teacher's ability to effectively teach phenomenon P to the student.</p>
<p>We view this reformulation as an improvement because it enables measuring the increase of scientific understanding in agents.This becomes particularly important when AI has developed new knowledge that needs to be conveyed to humans who lack that understanding.The reformulated test can be helpful in important cases where AI has developed new knowledge that needs to be conveyed to humans.</p>
<p>Applications, limitations, and new scientific understanding</p>
<p>The usefulness of AI models, such as large language models (LLMs), in scientific contexts like hypothesis generation and information retrieval relies on aligning their scientific understanding with humans.The proposed framework for assessing and directing scientific understanding in AI agents has the potential to enhance the usefulness of AI models in scientific contexts.For instance, it can compare the performance of different AI agents in answering questions, as well as highlighting their strengths and limitations.Additionally, it can also aid in educational programs.By helping select relevant AI tools, this framework could be a valuable resource for students, serving as a pedagogical aid.Some AI models are already capable of performing above the level of a college student who has completed one semester of physics 60 , highlighting the potential of these models as a valuable resource for students and researchers alike in the near future.</p>
<p>However, there are still open questions and challenges that need to be addressed.Establishing a threshold to determine sufficient understanding for an agent can prove to be complex, particularly when there may not be a consensus on the appropriate criteria.Similarly, in some testing modalities, experts might not always be available to check what the correct answer is, and for some questions we simply do not yet know the answer.However, this could open the possibility for new avenues of research, as asking these questions to QAMs might in some cases provide interesting answers that can trigger new avenues for research and stimulate hypothesis generation 30 .In such a case, if an agent is capable of answering questions for which humans do not have an answer yet, it may possess new scientific understanding.</p>
<p>To evaluate this new scientific understanding, a community can define w-questions whose answers are not yet known, similar to conjectures in mathematics that can be expected to be solved soon, where AI can tentatively provide unverified answers 61 62 .These lists of wquestions can measure progress in gaining new scientific knowledge and test forms of new scientific understanding.It is important to determine whether current LLMs/QAMs have new scientific understanding by asking questions where we might not know the answer.One can then employ our framework to act as a retroactive new scientific understanding test once these discoveries are confirmed or denied.This approach can encourage discovery and stimulate further research.</p>
<p>Finally, choosing the right prompt 63 (e.g., what context needs to be provided for each question) and evaluating vague outputs be challenging.The Reinforcement Learning 64 used to fine tune certain models (optimizing for user experience) may steer in the direction of vague or incorrect answers which are detrimental to scientific research 51 .Addressing this issue may require AI models specifically designed for scientific aims, which could involve adjusting the training set 65 or prioritizing scientific understanding during training or fine tuning.Creating AI models tailored to scientific understanding has the potential to transform how we approach scientific exploration.</p>
<p>Conclusion</p>
<p>This Roadmap presents a framework and testing methodology to assess agents' scientific understanding, a critical component in today's research landscape.It provides discussions and guidelines for communities to create their own scientific understanding tests, stressing their importance.The potential impact of this framework is far-reaching, as it can enhance the usefulness of AI, assess possible new scientific understanding encoded in machines, and aid in educational programs.</p>
<p>Future research directions include refining the methodology for creating tests and the concrete elaboration of tests which will form part of the benchmark for scientific understanding.Ultimately, scientific understanding tests are necessary to analyze, control, and harness the potential of AI in the context of scientific research.</p>
<p>The extended mind. A Clark, D Chalmers, Analysis. 5811998</p>
<p>External representations and scientific understanding. J Kuorikoski, P Ylikoski, Synthese. 1922015</p>
<p>Scientific machine learning benchmarks. J Thiyagalingam, M Shankar, G Fox, T Hey, Nature Reviews Physics. 462022</p>
<p>Benchmarking AI for science. Y Li, J Zhan, Saibench, BenchCouncil Transactions on Benchmarks, Standards and Evaluations. 221000632022</p>
<p>The Winograd schema challenge. H J Levesque, E Davis, L Morgenstern, 2012. 2012KR13</p>
<p>A Wang, A Singh, J Michael, F Hill, O Levy, S R Bowman, Glue, arXiv:1804.07461A multi-task benchmark and analysis platform for natural language understanding. 2018arXiv preprint</p>
<p>Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. A Srivastava, A Rastogi, A Rao, A A M Shoeb, A Abid, A Fisch, arXiv:2206.046152022arXiv preprint</p>
<p>Attention is all you need. Advances in neural information processing systems. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, 201730</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, Advances in neural information processing systems. 332020</p>
<p>The question answering systems: A survey. A M N Allam, M H Haggag, International Journal of Research and Reviews in Information Sciences (IJRRIS). 232012</p>
<p>Scientific understanding: Philosophical perspectives. De Regt, H. W., Leonelli, S., &amp; Eigner, K.2009University of Pittsburgh Press</p>
<p>Understanding scientific understanding. H W De Regt, 2017Oxford University Press</p>
<p>Beyond explanation: Understanding as dependency modelling. F Dellsén, The British Journal for the Philosophy of Science. 2020</p>
<p>Understanding as representation manipulability. D A Wilkenfeld, Synthese. 1902013</p>
<p>Minds, brains, and programs. J R Searle, Behavioral and Brain Sciences. 331980</p>
<p>Mental models and human reasoning. P N Johnson-Laird, Proceedings of the National Academy of Sciences. 107432010</p>
<p>How do scientists think? Capturing the dynamics of conceptual change in science. N J Nersessian, Cognitive Models of Science. 151992</p>
<p>Deep learning: A critical appraisal. G Marcus, arXiv:1801.006312018arXiv preprint</p>
<p>The limitations of deep learning. Deep learning with Python. F Chollet, 2017</p>
<p>Machine understanding and deep learning representation. M Tamir, E Shech, Synthese. 2012512023</p>
<p>The misunderstood limits of folk science: An illusion of explanatory depth. L Rozenblit, F Keil, Cognitive science. 2652002</p>
<p>A contextual approach to scientific understanding. H W De Regt, D Dieks, Synthese. 1442005</p>
<p>Computing machinery and intelligence. A M Turing, Mind. 492361950</p>
<p>The Turing Test. The Stanford Encyclopedia of Philosophy. G Oppy, D Dowe, N. ZaltaWinter 2021 Edition</p>
<p>On scientific understanding with artificial intelligence. M Krenn, R Pollice, S Y Guo, M Aldeghi, A Cervera-Lierta, P Friederich, G Dos Passos Gomes, F Häse, A Jinich, A Nigam, Z Yao, A Aspuru-Guzik, Nature Reviews Physics. 4122022</p>
<p>What is understanding? An overview of recent debates in epistemology and philosophy of science. C Baumberger, C Beisbart, G Brun, Explaining understanding: new perspectives from epistemology and philosophy of science. S R Grimm, C Baumberger, Ammon S Routledge, 2017</p>
<p>The Stanford Encyclopedia of Philosophy. S R Grimm, Understanding, Edward N. ZaltaSummer 2021 Edition</p>
<p>Agency without Intelligence: On ChatGPT, large language models, and other generative models. L Floridi, As, Philosophy &amp; Technology. 361152023</p>
<p>On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? 🦜. E M Bender, T Gebru, A Mcmillan-Major, S Shmitchell, Proceedings of the 2021 ACM conference on fairness, accountability, and transparency. the 2021 ACM conference on fairness, accountability, and transparencyMarch2021</p>
<p>S Bubeck, V Chandrasekaran, R Eldan, J Gehrke, E Horvitz, Kamar, arXiv:2303.12712Sparks of Artificial General Intelligence: Early experiments with GPT-4. 2023arXiv preprint</p>
<p>Understanding scientific understanding. H W De Regt, 2017Oxford University Press92</p>
<p>Understanding scientific understanding. H W De Regt, 2017Oxford University Press102</p>
<p>Scientific Explanation. The Stanford Encyclopedia of Philosophy. J Woodward, L Ross, Edward N. ZaltaSummer 2021 Edition</p>
<p>Making Things Happen: A theory of causal explanation. J Woodward, 2003Oxford University Press</p>
<p>Explanatory generalizations, part II: Plumbing explanatory depth. C Hitchcock, J Woodward, Noûs. 3722003</p>
<p>Explanatory depth. B Weslake, Philosophy of Science. 7722010</p>
<p>External representations and scientific understanding. J Kuorikoski, P Ylikoski, Synthese. 1922015</p>
<p>The logic of questions and answers. N D Belnap, T B Steel, 1976</p>
<p>Questions. C Cross, F Roelofsen, The Stanford Encyclopedia of Philosophy. N Zalta, Summer 2022 Edition</p>
<p>Explanation and the Theory of Questions. C B Cross, Erkenntnis. 3421991</p>
<p>Why-questions. S Bromberger, Mind and Cosmos: Essays in Contemporary Science and Philosophy. R Colodny, University of Pittsburgh Press1966</p>
<p>Studies in the Logic of Explanation. C G Hempel, P Oppenheim, Philosophy of science. 1521948</p>
<p>The Scientific Image. B C Van Fraassen, 1980Oxford University Press</p>
<p>Unification, the answer to resemblance questions. E Weber, M Lefevere, Synthese. 1942017</p>
<p>On the structure and epistemic value of function ascriptions in biology and engineering sciences. E Weber, D Van Eck, J Mennes, Foundations of Science. 242019</p>
<p>Dissecting explanatory power. P Ylikoski, J Kuorikoski, Philosophical studies. 1482010</p>
<p>Procedure for assessing the quality of explanations in failure analysis. K G Barman, AI EDAM. 362022</p>
<p>External representations and scientific understanding. J Kuorikoski, P Ylikoski, Synthese. 1922015</p>
<p>Causal inference in statistics: An overview. J Pearl, Statistics Surveys. 32009</p>
<p>Actual Causality. J Y Halpern, 2016MIT PressCambridge, MA</p>
<p>E Perez, S Ringer, K Lukošiūtė, K Nguyen, E Chen, S Heiner, . . Kaplan, J , arXiv:2212.09251Discovering Language Model Behaviors with Model-Written Evaluations. 2022arXiv preprint</p>
<p>Learning to ask: Neural question generation for reading comprehension. X Du, J Shao, C Cardie, arXiv:1705.001062017arXiv preprint</p>
<p>Learning to ask good questions: Ranking clarification questions using neural expected value of perfect information. S Rao, Iii Daumé, H , arXiv:1805.046552018arXiv preprint</p>
<p>Assessing Science Understanding: A human constructivist view. J J Mintzes, J H Wandersee, Novak, J. D.2005Academic Press</p>
<p>Measuring Student Knowledge and Skills: A New Framework for Assessment. A Schleicher, 1999Organisation for Economic Co-Operation and DevelopmentParis, France</p>
<p>Assessing Student Understanding in Science. M Franzen, Science and Children. 479792010</p>
<p>How to Create and Use Rubrics for Formative Assessment and Grading. S M Brookhart, ASCD. 582013</p>
<p>C G West, Fci, arXiv:2303.01067Can ChatGPT Project an Understanding of Introductory Physics?. 2023arXiv preprint</p>
<p>A fully automatic theorem prover with human-style output. M Ganesalingam, W T Gowers, Journal of Automated Reasoning. 582017</p>
<p>Prompt programming for large language models: Beyond the few-shot paradigm. L Reynolds, K Mcdonell, Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems. May2021</p>
<p>Reinforcement learning: An introduction. R S Sutton, A G Barto, 2018MIT press</p>
<p>R Taylor, M Kardas, G Cucurull, T Scialom, A Hartshorn, E Saravia, arXiv:2211.09085&amp; Stojnic, R. Galactica: A large language model for science. 2022arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
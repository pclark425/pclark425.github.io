<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2039 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2039</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2039</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-50.html">extraction-schema-50</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using large language models (LLMs) to generate curricula, tasks, or goals for training agents, including comparisons with manual or heuristic curriculum approaches, performance results, and domain characteristics.</div>
                <p><strong>Paper ID:</strong> paper-270309433</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2503.16799v1.pdf" target="_blank">Causally Aligned Curriculum Learning</a></p>
                <p><strong>Paper Abstract:</strong> A pervasive challenge in Reinforcement Learning (RL) is the"curse of dimensionality"which is the exponential growth in the state-action space when optimizing a high-dimensional target task. The framework of curriculum learning trains the agent in a curriculum composed of a sequence of related and more manageable source tasks. The expectation is that when some optimal decision rules are shared across source tasks and the target task, the agent could more quickly pick up the necessary skills to behave optimally in the environment, thus accelerating the learning process. However, this critical assumption of invariant optimal decision rules does not necessarily hold in many practical applications, specifically when the underlying environment contains unobserved confounders. This paper studies the problem of curriculum RL through causal lenses. We derive a sufficient graphical condition characterizing causally aligned source tasks, i.e., the invariance of optimal decision rules holds. We further develop an efficient algorithm to generate a causally aligned curriculum, provided with qualitative causal knowledge of the target task. Finally, we validate our proposed methodology through experiments in discrete and continuous confounded tasks with pixel observations.</p>
                <p><strong>Cost:</strong> 0.007</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2039",
    "paper_id": "paper-270309433",
    "extraction_schema_id": "extraction-schema-50",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.007236499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>21 Mar 2025</p>
<p>Mingxuan Li 
Causal Artificial Intelligence Lab
Columbia University
USA</p>
<p>Junzhe Zhang junzhez@cs.columbia.edu 
Causal Artificial Intelligence Lab
Columbia University
USA</p>
<p>Elias Bareinboim 
Causal Artificial Intelligence Lab
Columbia University
USA
21 Mar 20252FB8D24E8219E9CBD5CEA5BFCA65F5C4arXiv:2503.16799v1[cs.LG]
A pervasive challenge in Reinforcement Learning (RL) is the "curse of dimensionality" which is the exponential growth in the state-action space when optimizing a high-dimensional target task.The framework of curriculum learning trains the agent in a curriculum composed of a sequence of related and more manageable source tasks.The expectation is that when some optimal decision rules are shared across source tasks and the target task, the agent could more quickly pick up the necessary skills to behave optimally in the environment, thus accelerating the learning process.However, this critical assumption of invariant optimal decision rules does not necessarily hold in many practical applications, specifically when the underlying environment contains unobserved confounders.This paper studies the problem of curriculum RL through causal lenses.We derive a sufficient graphical condition characterizing causally aligned source tasks, i.e., the invariance of optimal decision rules holds.We further develop an efficient algorithm to generate a causally aligned curriculum, provided with qualitative causal knowledge of the target task.Finally, we validate our proposed methodology through experiments in discrete and continuous confounded tasks with pixel observations.</p>
<p>INTRODUCTION</p>
<p>As Roma was not built in one day, learning to achieve a complex task (e.g., cooking, driving) directly can be challenging.Instead, the human learning process is scaffolded with incremental difficulty to support acquiring progressively advanced knowledge and skills.The idea of training with increasingly complex tasks, known as curriculum learning, has been applied in reinforcement learning when Selfridge et al. (1985) used a carefully curated sequence of tasks to train agents to solve a modified Cart Pole system.In recent years, there has been a growing interest in automatically generating curricula tailored to the agent's current capabilities, which opens up a new venue called "Automatic Curriculum Learning" (Portelas et al., 2020).An automatic curriculum generator requires two components: an encoded task space and a task characterization function (Narvekar et al., 2020;Wang et al., 2020).Task space encoding is often a bijective function that maps a task to a low dimensional vector (Parker-Holder et al., 2022;Klink et al., 2022;Florensa et al., 2018;Jiang et al., 2021;Portelas et al., 2019;Wang et al., 2019;2020;Cho et al., 2023;Huang et al., 2022a).A proper task space encoding lays the foundation of a reasonable task characterization function measuring the fitness of tasks (Florensa et al., 2018;Dennis et al., 2020;Andreas et al., 2017;Sukhbaatar et al., 2018;Jiang et al., 2021).New training tasks, called source tasks, are generated by changing the target task's state space or parameters of transition functions in the encoded task space.A system designer then determines in which order the agent should be trained in these source tasks, following the task characterization function.The set of generated source tasks and the training order defined upon this set defines a curriculum for the learning agent.Please see App.G for more related work.</p>
<p>While impressive, most curriculum RL methods described so far rely on the assumption that generated source tasks are aligned with the target.Consequently, the agent could pick up some valuable skills by training in such source tasks, allowing it to behave optimally in certain situations in the target environment.However, this critical assumption does not necessarily hold in many real-world decision-making settings.For concreteness, consider a modified Sokoban game shown in Fig. 1 inspired by Schrader (2018) where an unobserved confounder U t randomly determines the box color C t (0 for yellow, 1 for blue) at every time step t.The agent receives a positive reward Y t only when it pushes the box to the goal state when the box color appears yellow (U t = 0); otherwise, it gets penalized (U t = 1).We apply several state-of-the-art curriculum generators that construct source tasks by fixing the box color to yellow or blue, including ALP-GMM (Portelas et al., 2019), PLR (Jiang et al., 2021), Goal-GAN (Florensa et al., 2018), and Currot (Klink et al., 2022).Fig. 1a shows an example of the generated source tasks.We evaluate agents' performance trained by those generated curricula and compare it with the one directly trained in the target task.Surprisingly, simulation results shown in Fig. 2 reveal that agents trained by the curricula failed to learn to push the yellow box to the destination.This suggests source tasks generated by intervening in the box color are misaligned; that is, training in these source tasks harms the agents' target task performance.</p>
<p>Figure 2: The average performance of curriculum generators.</p>
<p>Several observations follow from the Sokoban example.(1) A curriculum designer generates source tasks by modifying the data-generating mechanisms in the target tasks.</p>
<p>(2) Such modifications could lead to a shift in system dynamics between the target task and source tasks.When this distribution shift is significant, training in source tasks may harm the agent's learning.</p>
<p>(3) The agent must avoid misaligned source tasks to achieve optimal learning performance.There exist methods attempting to address the challenges of misaligned source tasks leveraging a heuristic similarity measure between the target and source tasks (Svetlik et al., 2017;Silva &amp; Costa, 2018).Yet, a systematic and theoretically justified approach for exploiting other types of knowledge, e.g., qualitative, about the target task is missing.</p>
<p>This paper aims to address the challenges of misaligned source tasks in curriculum generation by exploring causal relationships among variables present in the underlying environment.To realize this, we formalize curriculum learning in the theoretical framework of structural causal models (SCMs) (Pearl, 2009).This formulation allows us to characterize misaligned source tasks by examining the structural invariance across the optimal policies obtained from the target and source tasks.More specifically, our contributions are summarized as follows.</p>
<p>(1) We derive a sufficient graphical condition determining potentially misaligned source tasks.</p>
<p>(2) We develop efficient algorithms for detecting misaligned source tasks and constructing source tasks that are guaranteed to align with the target task.</p>
<p>(3) We introduce a novel augmentation procedure that enables state-of-the-art curriculum learning algorithms to generate aligned curricula to accelerate the agent's learning.Finally, we validate the proposed framework through extensive experiments in various decision-making tasks.</p>
<p>PRELIMINARIES</p>
<p>This section introduces necessary notations and definitions that will be used throughout the discussion.We use capital letters (X) to denote a random variable, lowercase letters (x) to represent a specific value of the random variable, and Ω(•) to denote the domain of a random variable.We use bold capital letters (V ) to denote a set of random variables and use |V | to denote its cardinality.</p>
<p>The basic semantical framework of our analysis rests on structural causal models (SCMs) (Pearl, 2009;Bareinboim &amp; Pearl, 2016).An SCM M is a tuple ⟨U , V , F , P ⟩, where U is a set of exogenous variables and V is a set of endogenous variables.F is a set of functions s.t. each f V ∈ F decides values of an endogenous variable V ∈ V taking as argument a combination of other variables in the system.That is,
V ← f V (PA V , U V ), PA V ⊆ V , U V ⊆ U .
Values of exogenous variables U are drawn from the exogenous distribution P (U ).A policy π over a subset of variables X ⊆ V is a sequence of decision rules {π(X|S X )} X∈X , where every π(X|S X ) is a probability distribution mapping from domains of a set of covariates S X ⊆ V to the domain of action X.An intervention following a policy π over variables X, denoted by do(π), is an operation which sets values of every X ∈ X to be decided by policy X ∼ π(X|S X ) (Correa &amp; Bareinboim, 2020), replacing the functions f X = {f X : ∀X ∈ X} that would normally determine their values.</p>
<p>For an SCM M, let M π be a submodel of M induced by intervention do(π).For a set Y ⊆ V , the interventional distribution P (Y ; π) is defined as the distribution over Y in the submodel M π , i.e., P M (Y ; π) ≜ P Mπ (Y ); restriction M is left implicit when it is obvious.</p>
<p>Each SCM M is also associated with a causal diagram G (e.g., Fig. 3a), which is a directed acyclic graph (DAG) where nodes represent endogenous variables V and arrows represent the arguments PA V , U V of each structural function f V ∈ F .Exogenous variables U are often not explicitly shown by convention.However, a bi-directed arrow V i ↔ V j indicates the presence of an unobserved confounder (UC), U i,j ∈ U affecting V i , V j , simultaneously (Bareinboim et al., 2022).We will use standard graph-theoretic family abbreviations to represent graphical relationships, such as parents (pa), children (ch), descendants (de), and ancestors (an).For example, the set of parent nodes of X in G is denoted by pa(X) G = ∪ X∈X pa(X) G .Capitalized versions Pa, Ch, De, An include the argument as well, e.g., Pa(X) G = pa(X) G ∪ X.A path from a node X to a node Y in G is a sequence of edges that does not include a particular node more than once.Two sets of nodes X, Y are said to be d-separated by a third set Z in a DAG G, denoted by (X ⊥ ⊥ Y |Z) G , if every edge path from nodes in X to nodes in Y is "blocked" by nodes in Z.The criterion of blockage follows Pearl (2009, Def. 1.2.3).For more details on SCMs, we refer readers to Pearl (2009); Bareinboim et al. (2022).For the relationship between (PO)MDPs and SCMs, please see App. H.</p>
<p>CHALLENGES OF MISALIGNED SOURCE TASKS</p>
<p>This section will formalize the concept of aligned source tasks and provide an efficient algorithmic procedure to find such tasks based on causal knowledge about the data-generating process.Formally, a planning/policy learning task (for short, a task) is a decision-making problem composed of an environment and an agent.We focus on the sequential setting where the agent determines values of a sequence of actions X = {X 1 , . . ., X H } based on the input of observed states {S 1 , . . ., S H }.</p>
<p>The mapping between states and actions defines the space of candidate policies, namely, Definition 1 (Policy Space).For an SCM M = ⟨U , V , F , P ⟩, a policy space Π is a set of policies π over actions X = {X 1 , . . ., X H }. Each policy π is a sequence of decision rules {π 1 (X 1 |S 1 ), . . ., π H (X H |S H )} where for every i = 1, . . ., H,
(i) Action X i is a non-descendent of future actions X i+1 , . . . , X H , i.e., X i ∈ V \ De( Xi+1:H ); (ii) States S i are non-descendants of future actions X i , . . . , X H , i.e., S i ⊆ V \ De( Xi:H ).
Henceforth, we will consistently denote such a policy space by Π = {⟨X 1 , S 1 ⟩, . . ., ⟨X H , S H ⟩}.</p>
<p>The agent interacts with the environment by performing intervention do(π), ∀π ∈ Π to optimize a reward function R(Y ) taking a set of reward signals Y ⊆ V as input.1A policy space, a reward function, and an SCM environment formalize a target decision-making task.We will graphically describe a target task using an augmented causal diagram G constructed from the SCM M; actions X are highlighted in blue; reward signals Y are highlighted in red; input states S i for every action X i ∈ X are shaded in light blue.For instance, Fig. 3a shows a causal diagram representing the decision-making task in the Sokoban game (Fig. 1).For every time step i = 1, . . ., H, L i stands for the agent's location, B i for the box location, and C i for the box color.Definition 2 (Target Task).A target task is a tuple T = ⟨M, Π, R⟩, where M = ⟨U , V , F , P ⟩ is an SCM, Π is a policy space over actions X ⊆ V , and R is a reward function over signals Y ⊆ V .
B 1 L 1 C 1 X 1 Y 1 L 2 B 2 C 2 X 2 Y 2 L 3 B 3 C 3 X 3 Y 3 π 1 π 2 π 3 (a) G B 1 L 1 C 1 X 1 Y 1 L 2 B 2 C 2 X 2 Y 2 L 3 B 3 C 3 X 3 Y 3 τ (1) τ (1) τ (2) τ (2) τ (2) π 1 π 2 π 3 (b) G (1) and G (2)
The goal is to find an optimal policy π * ∈ Π that maximizes the expected reward function
E [R(Y ); π] evaluated in the underlying environment M, i.e., π * = arg max π∈Π E M [R(Y ); π] .(1)
When the detailed parametrization of the SCM M is provided, the optimal policy π * is obtainable by applying planning algorithms, e.g., dynamic programming (Bellman, 1966) or influence diagrams (Koller &amp; Milch, 2003).However, when underlying system dynamics are complex or the state-action domains are high-dimensional, it might be challenging to solve an optimal policy even with state-ofthe-art planning algorithms.We will then consider the curriculum learning approach (Selfridge et al., 1985), where the agent is not immediately trained in the target task but provided with a sequence of related yet simplified source tasks.</p>
<p>Definition 3 (Source Task).For a target task T = ⟨M, Π, R⟩, a source task T (j) is a tuple ⟨M (j) , Π, R, ∆ (j) ⟩ where M (j) is an SCM compatible with the same causal diagram as M, i.e., G M = G M (j) ; a set of variables ∆ (j) ⊆ V is called edits where there might exist a discrepancy that
f V ̸ = f (j) V or P (U V ) ̸ = P (j) (U V ) for every V ∈ ∆ (j) .
In practice, source tasks are constructed from the target task by modifying parameters of the underlying structural functions F or exogenous distributions P (U ).Consider again the Sokoban game described in Fig. 1.The system designer could generate a source task T (1) by changing the agent and box's initial location L 1 , B 1 .Fig. 3b shows a causal diagram G (1)2 representing the source task T (1) ; τ (1) is an edit indicator representing the domain discrepancies ∆ (1) between the target T and source tasks T (1) .Here, arrows τ (1) → L 1 , τ (1) → B 1 suggest that structural functions f L1 , f B1 or exogenous distributions P (U L1 , U B1 ) have been changed in the source task T (1) while other parts of the system remain the same as the target task T .</p>
<p>By simplifying the system dynamics, learning an optimal policy in the source task T (j) could be easier than in the target task T .The expectation here is that the optimal decision rules π (j) over some actions X (j) ⊆ X remain invariant across the source and target tasks.If so, we will call such source tasks as aligned.Training in an aligned source task thus guides the agent to move toward an optimal policy π * .For example, Fig. 1b shows an aligned source task for the Sokoban game where the agent and box's locations are set close to the goal state.By training in the simplified task, the agent learns the optimal decision rule to push the yellow box to the goal state in this game.</p>
<p>However, modifying the target task could lead to a misaligned source task whose system dynamics differ significantly from the target.Interestingly and more seriously, training in these source tasks may "harm" the agent's performance, resulting in suboptimal decision rules, as illustrated next.</p>
<p>Example 1 (Misaligned Source Task).Consider the Sokoban game T = ⟨M, Π, R⟩ described in Fig. 1; Fig. 3a shows its causal diagram G. Specifically, the box color C i (0 for yellow, 1 for blue) is determined by an unobserved confounder U i ∈ {0, 1} randomly drawn from a distribution P (U i = 1) = 3/4.Box location B i and agent location L i are determined following system dynamics in deterministic grid worlds (Chevalier-Boisvert et al., 2018).The reward signal Y i is given by,
Y i =    10 if B i = "next to goal" ∧ X i = "push" ∧ (U i = 0) −10 if B i = "next to goal" ∧ X i = "push" ∧ (U i = 1) −0.1 otherwise .
(2)</p>
<p>If the agent pushes the box into the goal location (top right corner in Fig. 1), it receives a positive reward when the box appears yellow; it gets penalized when the box appears blue.Since
C i ← U i , evaluating the conditional reward E Y i b i , c i ; do(x i ) in the Sokoban environment M gives, E Y i B i = "next to goal", C i ; do (X i = "push") = 10 if C i = 0 −10 if C i = 1 (3)
Thus, the agent should aim to push yellow boxes to the goal location in the target.The curriculum designer now attempts to generate a source task T (2) by fixing the box color to yellow, i.e., C i ← 0. Fig. 3b shows the causal diagram G (2) associated with the source environment M (2) where edit indicators τ (2) denote the change in the structural function f Ci determining the box color
C i . Evaluating the conditional reward E Y i b i ; do(c i , x i ) in this manipulated environment M (2) gives E (2) Y i B i = "next to goal"; do (C i = 0, X i = "push") = −5.(4)
Detailed computations are provided in App.B. Perhaps counter-intuitively, pushing the yellow box to the goal location in the source task T (2) results in a negative expected reward.This is because box color C i is only a proxy to the unobserved U i that controls the reward.Fixing C i won't affect Y but only breaks this synergy, hiding the critical information of U i from the agent.Consequently, when training in the source task T (2) , the agent will learn to never push the box even when it is next to the goal location, which is suboptimal in the target Sokoban game T .■</p>
<p>CAUSALLY ALIGNED SOURCE TASK</p>
<p>Example 1 suggests that naively training in a misaligned source task may lead to suboptimal performance in the target task.The remainder of this section will introduce an efficient strategy to avoid misaligned source tasks, provided with the causal knowledge of the underlying data-generating mechanisms in the environment.For a target task T = ⟨M, Π, R⟩, let G be the causal diagram associated with M. Let G π be an intervened diagram obtained from G by replacing incoming arrows if action X i ∈ X with arrows from input states S i for every action X i ∈ X.We first characterize a set of variables ∆ (j) ⊆ V amenable to editing (for short, editable states) using independence relationships between edit indicators τ (j) and reward signals Y .Formally, Definition 4 (Editable States).For a target task T = ⟨M, Π, R⟩, let G be a causal diagram of M and X (j) ⊆ X be a subset of actions.A set of variables
∆ (j) ⊆ V \ X (j) is editable w.r.t X (j) if and only if ∀X i ∈ X (j) , the following independence holds in the intervened diagram G π , τ (j) ⊥ ⊥ Y ∩ De(X i ) | X i , S i ,(5)
where τ (j) is the set of added edit indicators pointing into nodes in ∆ (j) .</p>
<p>For example, consider again the Sokoban game described in Example 1.The initial agent and box's position ∆ (1) = {B 1 , L 1 } is editable with regard to all actions X following Def. 4. Precisely, in the augmented diagram G (1) of Fig. 3b, for every action
X i ∈ X, the edit indicators τ (1) are d-separated the reward signals Y ∩ De(X i ) = {Y i , . . . , Y H } given input states {L i , B i , C i }.
On the other hand, the set of box color variables ∆ (2) = {C 1 , . . ., C H } are not editable w.r.t.actions X since in the augmented diagram G (2) of Fig. 3b, for every action X i ∈ X, there exists an active path between edit indicators τ (2) and reward signals {Y i , . . ., Y H } given action X i and input states {L i , B i , C i }, violating the criterion given by Def. 4.</p>
<p>For a fixed policy π ∈ Π, for any subset S ⊆ V , we denote by Ω (j) (S; π) = {∀s ∈ Ω(S) | P M (s; π) &gt; 0} the set of reachable values of S, which is the set of states that are possible to reach in a source task T (j) under intervention do(π).The following proposition establishes that modifying functions and distributions over a set of editable states ∆ (j) leads to an aligned source task.</p>
<p>Theorem 1 (Causally Aligned Source Task).For a target task T = ⟨M, Π, R⟩, let T (j) = ⟨M (j) , Π, R, ∆ (j) ⟩ be a source task of T by modifying states ∆ (j) ⊆ V .If ∆ (j) is editable w.r.t some actions X (j) ⊆ X, then for every action X i ∈ X (j) ,
π * i (X i | s i ) = π (j) i (X i | s i ), ∀s i ∈ Ω (j) (S i ; π (j) ) ∩ Ω(S i ; π * ) (6)
where π * , π (j) ∈ Π are optimal policies in the target T and source T (j) tasks, respectively.</p>
<p>Thm. 1 implies that whenever states ∆ (j) is editable w.r.t.some actions X (j) , one could always construct an aligned source task T (j) such that the optimal decision rules π * over X (j) is invariant across the target T and source T (j) tasks.Consequently, one could transport these optimal decision rules trained in the source task T (j) without harming the agent's performance in the target domain T . 3For example, in the Sokoban game of Example 1, since initial states ∆ (1) = {B 1 , L 1 } is editable w.r.t.actions X, moving the agent and box's location leads to an aligned source task, which allows the agent to learn how to behave optimally when getting closer to the goal state.However, the performance guarantee in Thm. 1 does not necessarily hold when states ∆ (j) are not editable.</p>
<p>For instance, recall that ∆ (2) = {C 1 , . . ., C H } are not editable in the Sokoban game.Modifying the box's color could lead to a misaligned source task T (2) .An agent trained in this source task could pick up undesirable behaviors, as demonstrated in Example 1.</p>
<p>Algorithm 1: FINDMAXEDIT j) .
Input: A causal diagram G π and a set of actions, X(
Output: The maximal editable states j) and break; return ∆ (j) ; Algo. 1 describes an algorithmic procedure, FINDMAXEDIT, to find a maximal editable set ∆ (j) in a causal diagram G w.r.t. a set of actions X (j) ⊆ X.A set of editable states ∆ (j) is maximal w.r.t.X (j) if there is no other editable states ∆ (j) * strictly containing ∆ (j) .We always prefer a maximal editable set since it offers the maximum freedom to simplify the system dynamics in the target task.Particularly, FINDMAXEDIT iteratively adds endogenous variables V \ (X ∪ Y ) to the editable states ∆ (j) and test the independence criterion in Def. 4. This procedure continues until it cannot add any more endogenous variables.Evidently, FINDMAXEDIT returns a maximal editable set ∆ (j) w.r.t.X (j) .A natural question arising at this point is whether the ordering of endogenous variables V changes the output.Fortunately, the next proposition shows that this is not the case.Theorem 2. For a target task T = ⟨M, Π, R⟩, let G π be an intervened causal diagram of M and let X (j) ⊆ X be a subset of actions.FINDMAXEDIT G π , X (j) returns a maximal editable set ∆ (j) w.r.t actions X (j) ; moreover, such a maximal set ∆ (j) is unique.
∆ (j) w.r.t X (j) . Let ∆ (j) ← ∅; for V ∈ V \ X ∪ Y do ∆ (j) ← ∆ (j) ∪ {V }; for every X i ∈ X (j) do if τ (j) ̸⊥ ⊥ Y ∩ De(X i ) | X i , S i in G π then Remove V from ∆ (
Let n and m denote the number of nodes and edges in the intervened diagram G π and let d be the number of actions X.Since testing d-separation has a time complexity of O(n + m), FIND-MAXEDIT has a time complexity of O (d(n + m)).We also provide other algorithmic procedures for directly deciding a set's editability and constructing editable sets for a target task T in App. C.</p>
<p>CURRICULUM LEARNING VIA CAUSAL LENS</p>
<p>Once a collection of source tasks is constructed, the system designer could organize them into an ordered list, called a curriculum, as defined next:</p>
<p>Definition 5 (Curriculum).For a target task T = ⟨M, Π, R⟩, a curriculum C for T is a sequence of source tasks {T (j) } N j=1 , where T (j) = ⟨M (j) , Π, R, ∆ (j) ⟩.</p>
<p>Algorithm 2: CURRICULUM LEARNING
Input: A curriculum C. Output: A policy π (N ) ∈ Π.
Initialize a baseline policy π (0) ; for j = 1, ..., N do Update a policy π (j) from π (j−1) such that
π (j) = arg max π∈Π E M (j) [R(Y ); π] (7) return π (N ) ;
For instance, Fig. 1c describes a curriculum in the Sokoban game where the agent and the box are placed increasingly further away from the goal location.Given a curriculum C, a typical curriculum learning algorithm trains the agent sequentially in each source task, following the curriculum's ordering.Algo. 2 shows the pseudo-code describing this training process.It first initializes an arbitrary baseline policy π (0) .For every source task T (j) ∈ C, the algorithm updates the current policy π (j−1) such that the new policy π (j) is optimal in the source task T (j) .This step could be performed using a standard gradient-based algorithm, e.g., the policy gradient (Sutton &amp; Barto, 2018).The expectation is that, as the agent picks up more skills in the source tasks, it could consistently improve its performance in the target task or at least not regress.</p>
<p>Definition 6 (Causally Aligned Curriculum).For a target task T = ⟨M, Π, R⟩, let C = {T (j) } N j=1 be a curriculum for T .Curriculum C is said to be causally aligned with T if for every j = 1, . . ., N − 1, the set of invariant optimal decision rules across the source task and the target task expands, i.e.,
π (j) ∩ π * ⊆ π (j+1) ∩ π * ,(8)
where π * ∈ Π is an optimal policy in the target task T .</p>
<p>A naive approach to construct a causally aligned curriculum is to (1) construct a set of aligned source tasks by modifying editable states (Thm. 1) and ( 2) organize these tasks in an arbitrary ordering.However, the following example shows this is not a viable option.</p>
<p>Example 2 (Overwriting in Curriculum Learning).Consider a two-stage target task where action X 1 takes input H and X 2 takes input Z.The task SCM is,
H = U H , Z = ¬X 1 ⊕ U Z , Y 1 = 0.5 * (H ⊕ X 1 ), Y 2 = ¬H ⊕ X 2 ∧ Z where P (U Z = 1) = 1/2, P (U H = 1) = 1/10.
Other than the reward Y 1 , Y 2 , all other variables are binary.The optimal policy for the target task is π * (X 1 = ¬H|H) = 1, π * (X 2 = 0|Z) = 1.We create two source tasks.For T (1) , let P (U H = 1) = 9/10 while other parts stay the same as target task T .For T (2) , let Z = ¬X 1 while other parts stay the same as target task T .From the causal diagram, we see that 2) , its target task performance will deteriorate instead of improving.To witness, the optimal policy for X 2 in T (1) is π (1) (X 2 = 1|Z) = 1 and the optimal policy for X
∆ (1) = {H} is editable w.r.t X (1) = {X 1 } and ∆ (2) = {Z} is editable w.r.t X (2) = {X 2 }. Now if the agent is trained in a curriculum C = T (1) , T(1 in T (2) is π (2) (X 1 = 0|H) = 1.
After training in T (1) , π (1) has an expected target task reward of 0.55 since π (1) (X 2 = 1|Z) is not optimal in the target yet.So, the agent proceeds to train in T (2) .It will learn the optimal target policy for X 2 , π * (X 2 = 0|Z) = 1.But in the mean time, optimal policy of X 1 learned from T (1) , π * (X 1 = ¬H|H) = 1, is also overwritten by π (2) .The agent will only receive 0.5 in the target task, which is even worse than before training in T (2) .This suggests that curriculum C is not causally aligned.■ In the above example, the agent fails to learn an optimal policy due to "policy overwriting".Fig. 4 provides a graphical representation of this phenomenon.Particularly, each source task T (1) , T (2) covers one of the optimal decision rules over action X 1 , X 2 , respectively.An agent trained in one of the source tasks, say T (1) , learns the optimal decision rule π * 1 for action X 1 , but forgets the decision rule π * 2 for the other action X 2 learned previously in T (2) .The same overwriting also occurs when the agent moves from task T (2) to T (1) .This means that regardless of how the system designer orders the curriculum, e.g., C = T (1) , T (2) , T (1) , T (2) , . . ., the agent will always forget useful skills it picked up from Published as a conference paper at ICLR 2024 previous source tasks, thus making it unable to achieve satisfactory performance in the target task.This example implies that there are more conditions for a curriculum to be "causally aligned".
T (1) : π * 1 , π2 T (2) : π1, π * 2 Forget π * 1 Forget π * 2</p>
<p>DESIGNING CAUSALLY ALIGNED CURRICULUM</p>
<p>We will next introduce a novel algorithmic procedure to construct a causally aligned curriculum while avoiding the issue of overwriting.We will focus on a general class of soluble target tasks, which generalizes the perfect recall criterion (Koller &amp; Friedman, 2009) in the planning/decisionmaking literature (Lauritzen &amp; Nilsson, 2001).Definition 7 (Soluble Target Task).A target task T = ⟨M, Π, R⟩ is soluble if whenever j &lt; i,
(Y ∩ De(X i ) ⊥ ⊥ π j |S i , X i ) in G π
, where π j is a newly added regime node pointing to X j .</p>
<p>In words, Def.7 says that for a soluble target task T , for every action X i ∈ X, the input states S i summarizes all the states and actions' history S 1 , . . ., S i−1 , X 1 , . . ., X i−1 .If this is the case, an optimal policy π * for task T is obtainable by solving a series of dynamic programs (Lauritzen &amp; Nilsson, 2001;Koller &amp; Milch, 2003).For instance, the Sokoban game T graphically described in Fig. 3a is soluble.For every time step i = 1, . . ., H, given input states S i = {L i , B i , C i } and action X i , regime variables π 1 , . . ., π i−1 are d-separated from subsequent reward signals Y i , . . ., Y H . Theorem 3 (Causally Aligned Curriculum).For a soluble target task T = ⟨M, Π, R⟩, a curriculum C = {T (j) } N j=1 is causally aligned if the following conditions hold, (i) Every source task T (j) ∈ C is causally aligned w.r.t.actions X (j) (Def.4);</p>
<p>(ii) For every j = 1, . . ., N − 1, actions X (j) ⊆ X (j+1) .</p>
<p>Consider again the Sokoban game described in Fig. 3a.Let C = {T (j) } H j=1 be a curriculum such that for every source task T (j) is obtained by editing the agent and box's location ∆ (j) = {L i , B i } at time step i = H − j + 1.We now examine conditions in Thm. 3 and see if C is causally aligned.First, Condition (i) holds since every source task T (j) is causally aligned w.r.t.actions X (j) = {X H−j+1 , . . ., X H } following discussion in the previous section.Also, Condition (ii) holds since for every j = 1, . . ., H − 1, actions X (j) ⊆ X (j+1) .This implies that one could construct a causally aligned curriculum in the Sokoban game by repeatedly editing the agent and box' location following a reversed topological ordering; Fig. 1c describes such an example.</p>
<p>Algorithm 3: FINDCAUSALCURRICULUM
Input: A target task T , a causal diagram G π Output: A causally aligned curriculum C Let C ← ∅; for j = H, . . . , 1 do Let X (j) ← {X j , . . . , X H }; Let ∆ (j) ← FINDMAXEDIT(G π , X (j) ); Let T (j) ← GEN(T , ∆ (j) ); Let C = C ∪ {T (j) }; return C;
The idea in Thm. 3 suggests a natural procedure for constructing a causally aligned curriculum, which is implemented in FINDCAUSALCUR-RICULUM (Algo.3).Particularly, it assumes access to a curriculum generator GEN(T , ∆ (j) ) which generates a source task T (j) by editing a set of states ∆ (j) ⊆ V in the target task T .It follows a reverse topological ordering over actions X = {X 1 , . . ., X H }. For every step j = H, . . ., 1, the algorithm call the subroutine FINDMAXEDIT (Algo. 1) to find a set of editable states ∆ (j) w.r.t.actions X (j) = {X j , . . ., X H }. It then calls the generator GEN to generate a source task T (j) by editing states ∆ (j)  .The conditions in Thm. 3 ensure that Algo. 3 returns a causally aligned curriculum.Corollary 1.For a soluble target task T = ⟨M, Π, R⟩, let G π be an intervened causal diagram of M. FINDCAUSALCURRICULUM (T , G π ) returns a causally aligned curriculum.</p>
<p>A more detailed discussion on the additional conditions under which a combination of Algs. 2 and 3 is guaranteed to find an optimal target task policy is provided in App.D.  Button Maze.In this grid world environment (Chevalier-Boisvert et al., 2018), the agent must navigate to the goal location and step onto it at the right time.Specifically, after pushing the button, the goal region will turn green and yield a positive reward if the agent steps onto it.However, before pushing the button, there is only a 20% chance the agent gets a positive reward for reaching the goal, and the goal randomly blinks between red and green, independent of the underlying rewards.Curriculum generators can intervene the goal color and vary the agent's initial location but intervening goal colors creates misaligned curricula (Thm.3).</p>
<p>As shown in Fig. 5, agents trained by vanilla curriculum generators failed to learn at all, while the agents trained by their causally-augmented versions all converged to the optimal, even surpassing the one trained directly in the target task.</p>
<p>CONCLUSION</p>
<p>We develop a formal treatment for automatic curriculum design in confounded environments through causal lenses.We propose a sufficient graphical criterion that edits must conform with to generate causally aligned source tasks in which the agent is guaranteed to learn optimal decision rules for the target task.We also develop a practical implementation of our graphical criteria, i.e., FIND-MAXEDIT, that augments the existing curriculum generators into ones that generate aligned source tasks regardless of the existence of unobserved confounders.Finally, we analyze causally aligned curricula' design principles with theoretical performance guarantees.The effectiveness of our approach is empirically verified in two high-dimensional pixel-based tasks.Figure 7: An overview of our proposed approach.In the upper half, the non-causal curriculum generator will produce misaligned source tasks failing to train the agent to convergence.While in our approach (the bottom half), we utilize the qualitative causal knowledge (causal diagrams) to filter out those misaligned source task contexts such that applying the same curriculum generator can now produce aligned source tasks.</p>
<p>Table 1: Time complexity of FINDMAXEDIT and its variations.</p>
<p>Algorithm Description Runtime</p>
<p>ISEDIT</p>
<p>For given set of τ (j) , X (j) and graph G π decide editability O(d(n + m)) FINDEDIT Find an admissible set of τ (j) w.r.t X (j) in G π O(dn 2 ) FINDMAXEDIT Find the maximal admissible set of τ (j) w.r.t X (j) in G π O(dn 2 ) LISTEDIT List all admissible sets of τ (j) w.r.t X (j) in G π O(n) delay</p>
<p>A METHOD OVERVIEW</p>
<p>In this section, we will summarize our proposed approach from a high-level view and provide a figure for better illustration of the pipeline.</p>
<p>B DETAILED COMPUTATIONS OF EXAMPLE 1</p>
<p>For the reward of pushing the box, we have,
E Y i B i = "next to goal", C i ; do (X i = "push") (9) = E Y i B i = "next to goal", U i ; do (X i = "push") (10) = 10 × 1(U i = 0) − 10 × 1(U i = 1) (11) = 10 if C i = 0 −10 if C i = 1 (12)
After fixing the box color, we have,
E Y i B i = "next to goal"; do (C i , X i = "push") (13) = 10 × P (U i = 0) − 10 × P (U i = 1) (14) = −5. (15)
Algorithm 6: CAUSAL CURRICULUM LEARNING Input: A target task T , a curriculum generator GEN.</p>
<p>Output: A policy trained in the curriculum C, π * (C).</p>
<p>Generate causal diagram G π of T w.r.t policy space Π; Sort the set of actions w.r.t the soluble ordering (ascending) and save the result into X ′ ; Randomly initialize the agent's policy π * (C); j ← 1;
for X i in X ′ do ∆ ←FINDMAXEDIT(G π , {X i }); C[X i ] ← ∅; while Ω(S i ; π * ) ⊈ T (j) ∈C[Xi] Ω(S i ; π * (C)) do
Generate source task T (j) with GEN(T , ∆);
Train π * (C) in T (j) to converge; C[X i ] ← C[X i ] {T (j) }; j ← j + 1; return π * (C);
Thm. 4 implies that when checking editability w.r.t a set of actions, we only need to check w.r.t the single action that ranked highest in the soluble ordering (e.g., for actions X 3 ≺ X 2 ≺ X 1 , finding the editable states for {X 1 } is equivalent to finding editable states for {X i } 3 i=1 ).Based on the above discussion and Algo. 1, we propose a causal curriculum learning algorithm (Algo.6) to find the optimal target task policy given a curriculum generator GEN that generates source tasks by editing a given set of editable states ∆ (j) .This algorithm resonates with the heuristics that the agent should be trained in a sequence of more challenging source tasks (Portelas et al., 2020;Narvekar et al., 2020).More specifically, after sorting the actions in the soluble ordering, the first generated source task only requires the agent to learn the optimal target policy of action X N (Say the target task T is an N -step one).The next stage of learning will be to grasp the optimal target policy of both action X N and X N −1 , but since the agent has already learned the optimal policy of X N previously.This won't be too much harder for it.We will continue this procedure until the agent learns the optimal policy for all the actions in the target task.Algo.6 is basically a combination of CURRICULUM LEARNING(Algo.2) and FINDCAUSALCURRICU-LUM(Algo. 3) except that for each ∆, there will be multiple source tasks T (j) being generated until Ω(S i ; π * ) ⊆ T (j) ∈C[Xi] Ω(S i ; π * (C)) where Ω(S i ; π * ) is the space of possible S i values in the target task under the optimal target task policy π * , Ω(S i ; π * (C)) is the space of possible S i values in source tasks under policy π * (C) and C[X i ] is the set of source tasks T (j) such that X i ∈ X (j) .This condition ensures that the input space of an action X i is thoroughly traversed during training in the curriculum.If this is satisfied, it means that the agent has learned the optimal decision rule for X i in every possible situation in the target task, as we show formally in the following theorem.</p>
<p>Theorem 5.If the set of actions in target task satisfies X ⊆ X (N ) and
∀X i ∈ X, Ω(S i ; π * ) ⊆ T (j) ∈C[Xi] Ω(S i ; π * (C)), the following solution π * (C) is an optimal policy π * in the target task T , π * (C) = arg max π∈Π E M (N ) [R(Y ); π] ,(16)
where M (N ) is the SCM of source task T (N ) , and T (N ) is aligned to T w.r.t X (N ) .</p>
<p>Nonetheless, there are other practical roadblocks before realizing optimal curriculum learning.For example, we don't have an exact measurement of when the input space of an action is traversed thoroughly by the curriculum, and the agent may not converge in every source task T (j) to learn all the optimal decision rules of X (j) .But still, as we have shown in the experiments, simply augmenting the existing curriculum generators already gives us satisfying performance.</p>
<p>E PROOF FOR THEOREMS AND ALGORITHM CORRECTNESS</p>
<p>In this section, we provide proof sketches for all the theorems proposed in the previous sections.</p>
<p>Lemma 1.If the set of edit indicators satisfy (τ (j) ⊥ ⊥ Y X X, S X ) in a soluble source task, for any X ′ ≺ X in the soluble ordering, it satisfies (τ (j) ⊥ ⊥ Y X ′ X ′ , S X ′ ).</p>
<p>Proof of Lem. 1.We will prove this by contradiction.Suppose (τ (j) ̸⊥ ⊥ Y X ′ X ′ , S X ′ ).There are three possibilities based on the type of node V to which τ (j) corresponds.</p>
<p>i) If V ∈ S X , we can show that the source task cannot be soluble by finding an open path from a pseudo parent of X to Y X ′ .Firstly, X ′ ≺ X in a soluble source task means X ′ ∈ De(X).</p>
<p>When adding a pseudo parent V ′ to X, the path
V ′ → X ← V ← τ (j) is open under X ′ , S X ′ .
By the assumption, there also exists an open path from τ (j) to Y X ′ under X ′ , S X ′ , which means that path j) is also open under X ′ , S X ′ .Thus, this contradicts the definition of a soluble task and
V ′ → X ← V ← τ (X ′ ≺ X. ii) If V /
∈ S X and V ∈ An(X), we can find a similar open path as in the previous case.By the assumption, there exists an open path p from τ
(j) to Y X ′ under X ′ , S X ′ . But we have X ′ ∈ De(X) and thus Y X ′ ⊆ Y X .
This open path must be blocked under X, S X , which means that X must observe a non-collider variable Z on p.Without observing Z, p will be open under X, S X .Note that any colliders on p will have X ′ as their descendant since p is open under X ′ , S X ′ , which creates new open paths.So, p won't be blocked because of colliders.Now, a pseudo parent of X, say V ′ , will again have an open path towards
Y X ′ under X ′ , S X ′ , V ′ → X ← Z • • • → Y X ′ , where Z • • • → Y X ′ is
part of the p path.Thus, this contradicts the definition of a soluble task and X ′ ≺ X. iii) If V / ∈ An(X), we prove contradictions by checking all the possible path types between V and Y X ′ .If the open path p from V to Y X ′ under X ′ , S X ′ is causal, p must be blocked under X, S X by having X observe variables on p, which contradicts with the condition that V / ∈ An(X).If p is not causal and colliders exist, say Z, X ′ must be a descendant of such colliders.Let Z be the leftmost collider on p.This leads to another causal path p ′ , τ
(j) → V → • • • → Z → • • • → X ′ → • • • → Y X ′ .
Clearly, X cannot block p ′ by observing any variables on it.Thus, p ′ is open under X, S X which contradicts with the condition that (τ (j) ⊥ ⊥ Y X X, S X ).</p>
<p>Thus, we have proved that if (τ (j) ⊥ ⊥ Y X X, S X ) in a soluble source task, for any X ′ ≺ X, it also satisfies (τ
(j) ⊥ ⊥ Y X ′ X ′ , S X ′ ).
Proof of Thm. 1.By definition, the optimal policy of the target task satisfies,
π * = arg max π∈Π E M [R(Y ); π] .(17)
Similar definitions hold for the optimal policy π (j) of a source task T (j) .Our goal is to show that once the graphical criterion holds for τ (j) and action X i , the optimal decision rule of X i on those shared input states of the source task and the target task is the same in these two tasks.We first simplify the calculation of the optimal decision rule at action X i using the concept of "relevance graph" (Koller &amp; Milch, 2003).</p>
<p>Definition 8 (Relevance Graph of Tasks).The relevance graph, G r , of a target task T = ⟨M, Π, R⟩ is a directed graph whose nodes representing action variables X ⊆ V are connected by directed edges X ′ → X if and only if (π ′ ̸⊥ ⊥ Y ∩De(X)|S X , X) where π ′ is an added regime node pointing to X ′ .</p>
<p>This relevance graph specifies the order in which those actions should be optimized.Intuitively, X ′ is optimized before X because it can affect X's reward signal while X's inputs, S X , and X itself cannot block such causal effects.We denote the topological order over actions in G r by ≺ where X ′ ≺ X if and only if X ′ should be optimized before X.It is also possible that the relevance graph contains Strongly Connected Components (SCCs) where each pair of actions in the same SCC is connected by a directed path.Semantically, actions in the same SCC affect the rewards of each other.Thus, one must consider all actions in the same SCC to find the optimal policy for these actions (Koller &amp; Milch, 2003).Formally speaking, let L be an equivalence relationship over X such that action X, X ′ ∈ X belong to the same partition, X ∼ L X ′ , (equivalently, X ∈ [X ′ ] L or X ′ ∈ [X] L ) if and only if they are in the same maximal SCC of a target task T 's relevance graph, G r . 4When there is no SCC in the relevance graph, we say this target task is soluble Def. 7, which can be solved by a series of dynamic programs (Koller &amp; Milch, 2003;Lauritzen &amp; Nilsson, 2001).When there is an SCC, we can treat each SCC as a single high-level action taking value as the combination of all the actions in that SCC, i.e., Ω([
X i ]) = × X∈[Xi] Ω(X).
After eliminating all such SCCs in the relevance graph, we transform the task back to a soluble one.So, the same solver can be used.Now we show that given an optimal target task policy, the decision rule at [X i ] is optimal if and only if for every input s
[Xi] ∈ Ω(S [Xi] ; π * ), π * [Xi] (•|s [Xi] ) = arg max π [X i ] (•|s [X i ] ) E M R(Y [Xi] ) | s [Xi] ; π [Xi] ∪ π * ≺[Xi] ) ,(18)
where
Y [Xi] = De([X i ]) ∩ Y
is the set of rewards that are descendants of actions in the SCC of X i and π * ⪯Xi is the set of optimal decision rules of actions that precede [X i ] w.r.t the relevance graph.By definition, the decision rule at [X i ] is optimal if it maximizes the expected reward function when other parts of the optimal policy are given,
π * [Xi] = arg max π [X i ] E M R(Y ); π [Xi] ∪ (π * \ π * [Xi] ) .(19)
We can expand the expected reward as follows,
E M R(Y ); π [Xi] ∪ (π * \ π * [Xi] ) = s [X i ] P (s [Xi] )E M R(Y ) | s [Xi] ; π [Xi] ∪ (π * \ π * [Xi] ) .(20)
Clearly, the input distribution of S [Xi] is fixed given other parts of the optimal policy so Eq. ( 19) is equivalent to for every input s
[Xi] ∈ Ω(S [Xi] ; π * ), π * [Xi] (•|s [Xi] ) = arg max π [X i ] (•|s [X i ] ) E M R(Y ) | s [Xi] ; π [Xi] ∪ (π * \ π * [Xi] )(21)
= arg max
π [X i ] (•|s [X i ] ) y,[xi] R(y)P (y|s [Xi] , [x i ]; π * \ π * [Xi] )π([x i ]|s [Xi] ).(22)
Since non-descendants of [X i ] are independent of [X i ] given S [Xi] and actions are not confounded, we can further reduce the reward function to focus on only Y [Xi] , rewards that are descendants of
[X i ], assuming that R(•) is cumulative, π * [Xi] (•|s [Xi] ) = arg max π [X i ] (•|s [X i ] ) y [X i ] ,[xi] R(y [Xi] )P (y [Xi] |s [Xi] , [x i ]; π * \ π * [Xi] )π([x i ]|s [Xi] ). (23)
From the relevance graph definition, we know that only actions that precede [X i ] in the relevance graph will affect Y [Xi] given S [Xi] , [X i ].Thus, we can simplify the conditioning further,
π * [Xi] (•|s [Xi] ) = arg max π [X i ] (•|s [X i ] ) y [X i ] ,[xi] R(y [Xi] )P (y [Xi] |s [Xi] , [x i ]; π * ≺[Xi] )π([x i ]|s [Xi] )(24)
= arg max
π [X i ] (•|s [X i ] ) E M R(Y [Xi] ) | s [Xi] ; π [Xi] ∪ π * ≺[Xi] ,(25)
which is exactly Eq. ( 18).We can do a similar derivation for the optimal decision rule at [X i ] in source task T (j) , for every input s
[Xi] ∈ Ω (j) (S [Xi] ; π (j) ), π (j) [Xi] (•|s [Xi] ) = arg max π [X i ] (•|s [X i ] ) y [X i ] ,[xi] R(y [Xi] )P (y [Xi] |s [Xi] , [x i ], τ (j) ; π (j) ≺[Xi] )π([x i ]|s [Xi] ) (26) = arg max π [X i ] (•|s [X i ] ) E M (j) R(Y [Xi] ) | s [Xi] ; π [Xi] ∪ π (j) ≺[Xi] ,(27)
where τ (j) is the edit indicator π (j) is the optimal policy of source task T (j) .In practice, we can let actions in each SCC X i have an edge into every reward node associated with the SCC and still the green goal region.Note that the goal region flashes between red and green color before pushing the button.Even if the agent step onto it when it shows green, it is still possible that the agent gets a negative reward.The environment is defined the same as the Button Maze with the only difference that we remove the not-moving penalty from the reward function,
Y i =    1 if B i = "next to goal" ∧ X i = "move to the goal" ∧ (U C = 1) −1 if B i = "next to goal" ∧ X i = "move to the goal" ∧ (U C = 0) −0.01 otherwise . (30)
The curriculum generators are allowed to pick the initial location of the agents, whether to toggle the button at the beginning, and whether to interven the goal region color.</p>
<p>F.2 ADDITIONAL EXPERIMENT RESULTS</p>
<p>When reporting results, instead of using the cumulative rewards directly, as promoted in Agarwal et al. (2021), we report the Interquartile Mean (IQM) normalized by the maximum and minimum rewards in the corresponding environment.Specifically, for each experiment, we run five random seeds.Then, the normalized IQM is calculated as,
NORMALIZEDIQM = 2 n 3n 4 i= n 4 +1 (x i − l) (u − l)(31)
where x i are the original data points we collected and sorted, e.g., cumulative rewards, and ∀x i , x i ∈ [b, u], b, u ∈ R are the bounds for the data points.In our experiments, the bounds for the rewards are easily obtainable as they are both artificially designed game environments.In the implementation, we edit the environment by editing a set of predefined parameters.Each vector of parameters corresponds to a unique instance of the environment.</p>
<p>In Fig. 9, we see that agent trained by causal agnostic curriculum generators fail to avoid misaligned source tasks and unable to converge to the optimal policy.Those agents trained by the curriculum generators perform even worse than those directly trained in the target task which verifies the necessity of avoiding misaligned source tasks empirically.After augmenting the same curriculum generators with our algorithm, agents trained by them all successfully converge to the optimal surpassing those trained directly in the target task.The result demonstrates that our method is indeed widely applicable to both high-dimensional and continuous domains and that utilizing qualitative causal knowledge properly is crucial to the successful application of curriculum learning in the confounded environments.</p>
<p>During the training process, we also counted the portions of misaligned source tasks generated by those non-causal curriculum generators.From Table 2, we see clearly that in all three environments, those curriculum generators fail to avoid misaligned source tasks and most of the source tasks proposed by them are actually misaligned.We report those portions with 95% confidence intervals.</p>
<p>We also show examples of the curricula generated by those augmented generators in Figs. 10 to 12.</p>
<p>G DETAILED RELATED WORK</p>
<p>Research in generating suitable curricula for reinforcement learning agents can date back to the 90s when people manually designed subtasks for robot controlling problems Sanger (1994).In recent works, a general curriculum generation framework requires two components, an encoded task space and a task characterization function (Narvekar et al., 2020;Wang et al., 2020).Each component may be either hand-coded as inputs (Khan et al., 2011;Peng et al., 2018;MacAlpine &amp; Stone, 2018;Portelas et al., 2019) or automatically learned from data along with training the agents (Parker-Holder et al., 2022;Klink et al., 2022;Florensa et al., 2018;Jiang et al., 2021;Florensa et al., 2017;Risi &amp; Togelius, 2020;Cho et al., 2023;Huang et al., 2022a).A straightforward task space encoding is to split the observation with common patterns (e.g.pixel tiles) and re-combine them to create new tasks (Dahlskog &amp; Togelius, 2014), which can be intractable in the face of a rich observation space and adds extra representational burdens to the curriculum generator.Recent work usually uses vectored parameters as task space encoding, each of which can be grounded into a unique task instance (Parker-Holder et al., 2022;Klink et al., 2022;Florensa et al., 2018;Jiang et al., 2021;Portelas et al., 2019;Dennis et al., 2020;Wang et al., 2019;2020;Cho et al., 2023;Huang et al., 2022a).While parameter-based encoding is widely applicable in various decision-making tasks, using a dedicated domain description language such as Video Game Description Language (VGDL) for task space encoding provides finer granularity and readability in task generation (Schaul, 2013;Liebana et al., 2016;Justesen et al., 2018).A suitable task space encoding then lays the foundation of a reasonable task characterization function, which is either a task difficulty measure (Florensa et al., 2018;Parker-Holder et al., 2022;Dennis et al., 2020;Andreas et al., 2017;Sukhbaatar et al., 2018;Jiang et al., 2021) or a task similarity function (Svetlik et al., 2017;Silva &amp; Costa, 2018;Jiang et al., 2021;Eysenbach et al., 2019) in general.For example, task similarity can be measured via domain knowledge based heuristics (Svetlik et al., 2017;Andrychowicz et al., 2017;Silva &amp; Costa, 2018) and agent's performance can be used as a direct indicator of the task difficulty (Florensa et al., 2017;2018;Narvekar et al., 2017;Parker-Holder et al., 2022).Curriculum generator relies heavily on these task characteristic functions to measure the quality of the task, schedule the training process, and evaluate the agent's performance (Narvekar et al., 2020;Portelas et al., 2020).</p>
<p>Given task space encoding and task characteristic functions, the remaining central problem of curriculum learning is how exactly one could generate new tasks efficiently.The most intuitive idea of training agents on increasingly harder tasks has been verified in various works, which can be implemented as setting different goals (Florensa et al., 2018;Racanière et al., 2019;Baranes &amp; Oudeyer, 2013) or changing starting state distributions (Florensa et al., 2017;Salimans &amp; Chen, 2018;Asada et al., 1996;Narvekar et al., 2016).Another major branch of task generation is to change task's state space or task parameters.This approach usually works together with a parametrized task space (a) ALP-GMM@1K (b) PLR@1K (c) Goal-GAN@1K (d) Currot@1K (e) ALP-GMM@100K (f) PLR@100K (g) Goal-GAN@100K (h) Currot@100K (i) ALP-GMM@200K (j) PLR@200K (k) Goal-GAN@200K (l) Currot@200K (m) ALP-GMM@300K (n) PLR@300K (o) Goal-GAN@300K (p) Currot@300K  Holder et al., 2022;Klink et al., 2022;Florensa et al., 2018;Jiang et al., 2021;Portelas et al., 2019;Dennis et al., 2020;Wang et al., 2019;2020;Cho et al., 2023;Huang et al., 2022a).When more flexibility is desired, source tasks can also be generated by changing the transition or reward function forms.In curiosity-driven agents, exploration is encouraged by an intrinsic reward added upon the original reward from the task (Bellemare et al., 2016;Ecoffet et al., 2019), and this intrinsic reward signal can be learnable which evolves as agents progress in the task (Burda et al., 2019;Singh et al., 2004).Another telling illustration of function forms change is the Sim2Real problem.When generating tasks for a robot in simulation, other than a simulated perfect dynamics model, one needs to take into account extra fractions, inaccurate sensors, motor latency, and poorly executed actions for a successful deployment into the real-world target task (OpenAI et al., 2019).</p>
<p>The crux of curriculum reinforcement learning is to transfer useful knowledge from subtasks to target tasks (Narvekar et al., 2020), which can be modeled as a transportability problem in the causal (a) ALP-GMM@1K (b) PLR@1K (c) Goal-GAN@1K (d) Currot@1K (e) ALP-GMM@100K (f) PLR@100K (g) Goal-GAN@100K (h) Currot@100K (i) ALP-GMM@200K (j) PLR@200K (k) Goal-GAN@200K (l) Currot@200K (m) ALP-GMM@300K (n) PLR@300K (o) Goal-GAN@300K (p) Currot@300K literature (Correa &amp; Bareinboim, 2020;Pearl &amp; Bareinboim, 2011;Bareinboim &amp; Pearl, 2016).The literature on transportability studies broadly how to answer queries with data from disparate domains.By examining shared causal mechanisms across seemingly dissimilar domains, formal graphical conditions are established to identify what queries can be answered and how those should be answered.In our curriculum reinforcement learning setting, a good curriculum elicits policy that performs well not only in subtasks but, more importantly, in target tasks.Policy pieces from various tasks in the curricula constitute our data from which our quest is to construct an optimal target task policy.Since subtasks are generated from target tasks, they share certain aspects in principle.Analyzing how we can transfer those policy pieces to the target tasks can thus be viewed as a transportability problem.</p>
<p>Throughout the paper, we assume access to the causal diagram of the task.However, there is also an orthogonal line of research dedicated in learning the causal structure directly from the task from which a causal diagram can be derived naturally (Hu et al., 2022;Li et al., 2023;Perry et al., 2022).Graphically speaking, the causal diagram of a typical MDP is shown in Fig. 13.The next state S t+1 only takes the action and state at the current time step as input, which means that in the form of structural equations, S t+1 = f St+1 (S t , X t , U St+1 ) where U St+1 is an exogenous variable representing the inherent randomness in the transition function.In Fig. 13, we can see clearly the Markov assumption embedded inside the graphical structure, i.e., (S t+1 ⊥ ⊥ S t−1 |S t , X t ).Similarly, we can also ground the Markovian reward assumption with precise graphical criteria.As stated by Abel et al., Markovian reward assumption assumes that the state factors that are affecting the reward are fully observable to the agent.This can be interpreted as, pa(Y t ) ⊆ S t where all parent nodes of the reward node is given as input to the agent (S t ).Note that here we denote the state as a set of variables instead of one single variable for clarity and we implicitly assume that the agent can observe all S t when making decisions.</p>
<p>For POMDP, the causal diagram is shown in Fig. 14.The causal diagram faithfully reflects the fact that the agent cannot observe the state variables directly but only the observations.And importantly, the underlying transition dynamics between the true states and actions are still following the standard MDP.The representation of SCMs is more versatile in modeling decision making scenarios in that it is amenable to represent any data generating processes without casting structural assumptions like MDP nor POMDP to the problem.By introducing the notion of confounders, we can better utilize the graphical structure to construct optimal agents efficiently (Zhang &amp; Bareinboim, 2020).</p>
<p>In this work, we utilize the qualitative causal knowledge to ensure that the causal effect of changing certain aspects of the target task won't affect the optimal policy the agent will learn from the generated source task.Even without confounders, if the state space is partially observable, the same situation as in the Colored Sokoban could happen since not all factors affecting the reward can be observed by the agent.But when the Markovian reward assumption holds, where the agent can observe all the parents of the reward variable, the reward cannot be confounded with any other variables.Under this stronger assumption, as our Theorem 1 indicates, all state variables are editable.</p>
<p>On the other hand, we are dedicated to solving the curriculum generation problem in the presence of unobserved confounders.Thus, the setting of Markovian rewards actually falls into the traditional curriculum reinforcement learning problem studied in the literature, which our work is trying to relax.</p>
<p>I LIMITATIONS</p>
<p>As our theorems and algorithms require causal diagrams as inputs, it might not be easily applicable when the domain specific knowledge is hard to retrieve or the input diagram is inaccurate.Thus, we will combine our method with causal discovery methods that can automatically learn the underlying causal structure from the data to loosen this constraint in the future (Hu et al., 2022;Li et al., 2023;Perry et al., 2022).</p>
<p>Figure 1 :
1
Figure 1: Examples of (a) full episode of a misaligned source task that intervenes in the box color, (b) full episode of an aligned source task that only changes the initial box location, and (c) an aligned curriculum where none of the source tasks intervenes in the box's color.</p>
<p>Figure 3 :
3
Figure 3: Causal diagram for (a) the target task T ; and (b) comparing domain discrepancies between the target task T and source tasks T (1) and T (2) .(b) is (a) augmented by edit indicators.</p>
<p>Figure 4 :
4
Figure 4: Policy overwriting described in Example 2.</p>
<p>Figure 5 :
5
Figure 5: Target task performance of the agents at different training stages in Colored Sokoban (Row 1) and Button Maze (Row 2) using different curriculum generators (Columns).The horizontal green line shows the performance of the agent trained directly in the target."original" refers to the unaugmented curriculum generator and "causal" refers to its causally augmented version.ALP-GMM(Portelas et al., 2019), PLR(Jiang et al., 2021), Goal-GAN(Florensa et al., 2018), and Currot(Klink et al., 2022) in two confounded environments with pixel observations: (a) Colored Sokoban, (b) Button Maze, (c) Continuous Button Maze (App.F).All experiments are conducted with five random seeds and reported in Interquartile Mean (IQM) normalized w.r.t the minimum and maximum rewards with 95% confidence intervals shown in shades.See App.F for more details.Colored Sokoban.Consider the same Sokoban game as shown in Example 1.The curriculum generators are allowed to vary the initial location of the agent, to vary the initial box location, and to intervene the box's color.Without editing, the box color syncs with the true underlying rewards, i.e., pushing a yellow box always yields a positive reward.However, after intervening the box color, this sync is broken and the agent has no information on the right time to push the box.As shown in Fig. 5, agents trained by original curriculum generators failed to converge due to this.After causal augmentation, those misaligned source tasks with intervened box color are all eliminated from the search space during curriculum generation.The causal versions of those generators successfully train the agent to converge efficiently and surpass those trained directly in the target task.</p>
<p>Figure</p>
<p>Figure 6: Button Maze.</p>
<p>Figure 8 :
8
Figure8: Continuous Button Maze.The agent (the grey dot) needs to navigate the room and step into the goal region (the region in the bottom left) at the right time.The goal region flashes between red and green.And after pushing the button, it will be always green.</p>
<p>Figure 9 :
9
Figure 9: Target task performance of the agents at different training stages in the Continuous Button Maze using different curriculum generators (Columns).The horizontal green line shows the performance of the agent trained directly in the target."original" refers to the unaugmented curriculum generator and "causal" refers to its causally augmented version.Table2: Misaligned Source Task Portion.</p>
<p>Figure 10 :
10
Figure 10: Curricula generated by the (causal) augmented curriculum generators for Colored Sokoban.Each column shows a curriculum from one generator at different training steps.</p>
<p>Figure 11 :
11
Figure 11: Curricula generated by the (causal) augmented curriculum generators for Button Maze.Each column shows a curriculum from one generator at different training steps.</p>
<p>Figure 13 :
13
Figure 13: The causal diagram of a standard MDP.We use X to denote actions and Y to denote rewards.</p>
<p>Figure 14 :
14
Figure 14: The causal diagram of a standard POMDP.We use X to denote actions and Y to denote rewards.</p>
<p>Maxwell Svetlik, Matteo Leonetti, Jivko Sinapov, Rishi Shah, Nick Walker, and Peter Stone.Automatic curriculum graph generation for reinforcement learning agents.In Proceedings of the AAAI Conference on Artificial Intelligence, volume 31, 2023/06/09 2017.doi: 10.1609/aaai.v31i1.10933.URL https://ojs.aaai.org/index.php/AAAI/article/view/10933.
Benito van der Zander, Maciej Liundefinedkiewicz, and Johannes Textor. Constructing separatorsand adjustment sets in ancestral graphs. In Proceedings of the UAI 2014 Conference on CausalInference: Learning and Prediction -Volume 1274, CI'14, pp. 11-24. CEUR-WS.org, 2014.Rui Wang, Joel Lehman, Jeff Clune, and Kenneth O. Stanley. Paired open-ended trailblazer (POET):endlessly generating increasingly complex and diverse learning environments and their solutions.arxiv, 2019. URL http://arxiv.org/abs/1901.01753.Rui Wang, Joel Lehman, Aditya Rawal, Jiale Zhi, Yulun Li, Jeffrey Clune, and Kenneth O. Stanley.Enhanced POET: open-ended reinforcement learning through unbounded invention of learningchallenges and their solutions. In Proceedings of the 37th International Conference on Ma-chine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings ofMachine Learning Research, pp. 9940-9951. PMLR, 2020. URL http://proceedings.mlr.press/v119/wang20l.html.
Junzhe Zhang and Elias Bareinboim.Designing optimal dynamic treatment regimes: A causal reinforcement learning approach.In Hal Daumé III and Aarti Singh (eds.),Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp.11012-11022.PMLR, 13-18 Jul 2020.URL https://proceedings.mlr.press/v119/zhang20a.html.</p>
<p>Table 2 :
2
Misaligned Source Task Portion.
Env./Alg.ALP-GMMPLRGoal-GANCurrotColored Sokoban68.10 ± 0.71% 69.10 ± 2.77% 66.41 ± 1.84% 68.36 ± 1.06%Button Maze88.41 ± 0.43% 87.33 ± 3.77% 90.30 ± 0.27% 88.62 ± 1.00%Con. Button Maze 85.77 ± 0.83% 83.57 ± 14.5% 80.55 ± 19.6%47.70 ± 32%
For instance, a cumulative discounted reward is defined as R(Y ) = H i=1 γ i−1 Yi where Yi ∈ V , i = 1, . . . , H, are endogenous variables, and γ ∈ (0, 1] is a discount factor.
We will consistently use the superscript (j) to indicate a diagram G (j) ≜ G M (j) associated with a source task T(j)  . Similarly, we write P (j) (Y ; π) = P M (j) (Y ; π) and π (j) = arg max π∈Π E M (j) [R(Y ); π].
Causal aligned source tasks (Thm. 1) and editable states (Def.
) are related to the concept of transportability in causal inference literature(Bareinboim &amp; Pearl, 2016), which generalizes estimation of unknown causal effects from different domains. Here we study the generalizability of an optimal decision policy.
EXPERIMENTSIn this section, we build on Algo. 3 and different curriculum generators to evaluate causally aligned curricula for solving challenging tasks in which confounding bias is present and previous, noncausal generators cannot solve. In particular, we test four best-performing curriculum generators:
for simplicity, we will use [X] to denote [X]L in the following discussion.
This research was supported in part by the NSF, ONR, AFOSR, DoE, Amazon, JP Morgan, and the Alfred P. Sloan Foundation.Algorithm 4: ISEDIT Input: A set of edit indicators τ (j) , a set of actions X (j) and a graph G π .Output: Whether the input edits are admissible.for X in X (j) do if τ (j) ⊥ ⊥ Y ∩ De(X) | X, S X in G π then return False; return True; Algorithm 5: LISTEDIT Input: A causal diagram G π , a set of actions X (j) .Output: All possible editable sets.∆ (j) →FINDMAXEDIT(G π , X (j) ); mask ← 2 |∆ (j) | − 1; while mask do ∆ ← ∅; tmpmask ← mask; i ← |∆ (j) | − 1; while tmpmask do if tmpmask &amp;1 then ∆ ← ∆ ∪ {∆ (j) [i]} i ← i − 1; tmpmask ≫ 1; mask← mask −1; yield ∆;C VARIATIONS OF ALGORITHMSThe first task of deciding editability is a direct application of the criterion in Def. 4 and can be implemented by using TESTSEP (Algo.4).Finding an editable set and finding the maximal editable set are both solved by our proposed Algo.1.After we find the maximal editable set, by Thm. 2, the power set of it is indeed all the possible editable sets w.r.t X (j) in G π (Algo.5).Note that there are exponentially many subsets so we focus on the time complexity of yielding each output.We implement this by bit manipulation.Specifically, we denote the existence of a variable in the output set as 0 or 1.Then listing all the elements in a power set of ∆ (j) is equivalent to decrease a bit mask from 2 |∆ (j) | − 1 to 0. Each mask in this process represents a unique subset of ∆ (j) .D ALGORITHM FOR CAUSALLY ALIGNED CURRICULUM LEARNINGAlgo. 3 construct the curriculum before any training is done.However, a more common practice is to construct the curriculum while adapting to the edge of the agent's capability.In this section, we will discuss a more involved version of Algo. 3 that implements this idea.As we have seen in the main text, two source tasks T (i) , T (j) in a soluble curriculum satisfy X (i) ⊆ X (j) if i &lt; j.This incremental nature of the action sets actually brings us opportunities to reduce further the computation of finding editable states.Thm. 4 reveals a nice property of X (j) such that if a set of state variables, ∆ (j) , is editable w.r.t an action X, it is also editable w.r.t any actions X ′ precedent to X in the soluble ordering, X ′ ≺ X, i.e., (Theorem 4 (Expanded Action Sets).Let T = ⟨M, Π, R⟩ be a soluble target task and T (j) = ⟨M, Π, R, ∆ (j) ⟩ an aligned source task of T .Then ∆ (j) is also editable w.r.tthe graph is compatible with the original task.Then, we only need to show that once the graphical criterion is satisfied in such a graph, for any input s i ∈ Ω (j) (S i ; π (j) ) ∩ Ω(S i ; π * ), the following holds,If this is true, the optimal decision rule at X i will be invariant across both the target task and the source task.We first consider the simpler case when there is no SCC in the relevance graph.So,In this case, we can apply the result of Lem. 1 directly and know that the graphical criterion is also satisfied by any action X ≺ X i .Then we prove Eq. (28) holds by induction on action X i .The base case is there is no action preceding X i in the relevance graph.So, there will be no policy dependencies in Eq. (28), and it will be trivially true given the graphical criterion.Now we assume when there are k actions preceding X i in the relevance graph, and the graphical criterion holds, Eq. (28) will hold.When there are k + 1 actions preceding X i in the relevance graph, by the inductive hypothesis and the fact that graphical criterion also holds for X ≺ X i when it holds for X i , we know the optimal decision rules of X ≺ X i stay the same across the source task and the target task.Thus, Eq. (28) is reduced to P (y Xi |s Xi , x i ) = P (y Xi |s Xi , x i , τ (j) ), which is true when the graphical criterion holds, (τWhen there are SCCs in the relevance graph, the graphical criterion only shows us that (τ) in G π also holds.Notice that the difference between these two criteria is that the latter includes more variables in the conditioning set.If the latter one doesn't hold, that means adding these variables opens up at least a collider path p that is blocked under the original criterion.Say this collider on p is M , and it's ancestral to an actionClearly, there is an active path from τ (j) to M and there is also an active causal path from M to Y Xj under S i , X i .But we also know that by the condition thatThis means that there is also an active path from τ (j) to Y Xi under S i , X i which clearly contradicts with the fact that (τ) in G π holds.Now, we can view the whole [X i ] as one high-level action and follow a similar induction procedure as in the simpler case, which completes the proof.Proof of Thm. 2. We first show that the maximal editable set w.r.t. a set of actions is unique.Let∈ V 2 exists.Since V satisfies our criterion, so does every variable in V 2 .Then by Def. 4, we know the set V ′ = V 2 ∪ {V } is admissible w.r.t X (j) , which contradicts with the fact that V 2 is the maximal set since |V ′ | = K + 1.Thus, this is impossible to happen.By the uniqueness, we only need to search for one maximal editable set w.r.t a given set of actions X (j) .For each variable, by Def. 4, it has to be admissible w.r.t X (j) before it can be added to the admissible set ∆ (j) .Thus, we loop through all state variables and check their editability.If a single state variable is not admissible w.r.t an action, X ∈ X (j) , we don't need to check its editability w.r.t other actions further.So, we can break the loop there.The editability check is done on the augmented graph G π where a pseudo edit indicator τ is added, pointing to V , the state variable being checked.The correctness of this step is guaranteed by the correctness of TESTSEP(van der Zander et al., 2014).Proof of Thm. 3. Since every source task we use is causally aligned, the optimal decision rules of actions in X (j) will be invariant across the target task and the source task T (j) .The same is true for the set of actions X (j+1) .By our construction, we have X (j) ⊆ X (j+1) , and each action corresponds to exactly one element in π (j) ∩ π * , π (j+1) ∩ π * , respectively.Thus, the set of invariant optimal decision rules is also expanding.Proof of Corol. 1.By the correctness of FINDMAXEDIT, we know that every source task generated by GEN(T , ∆ (j) ) will be causally aligned w.r.t X (j) .Then by the way we construct X (j) , it is guaranteed that X (j) ⊆ X (j+1) .Thus, the returned curriculum of FINDCAUSALCURRICULUM will be causally aligned.Proof of Thm. 4. This is a direct result of applying Lem. 1 to soluble target tasks.Proof of Thm. 5.The algorithm works by creating source tasks with an expanding set of actions.The set of actions expands in the direction that follows the soluble ordering, X ′ = {X N , X N −1 , ..., X 1 }.Then, by Thm. 4, the editable set can be calculated w.r.tonly the newly added action in this round (X i ).The while loop ensures we generate enough source tasks to cover all the possible state inputs to X i .Note that we don't require the final output π * (C) to be optimal in all source tasks.It is still the optimal target task policy.Because given the expanding action sets when constructing source tasks, optimal decision rules of X (j−1) learned in T (j−1) are still optimal in T (j) and the final output policy π * (C) will contain optimal decision rules for all actions X in the target task.F EXPERIMENT AND IMPLEMENTATION DETAILSThis section introduces the details of our experiments, including environment specifications, agent hyper-parameters, training/testing protocols, and more experimental results.For the Colored Sokoban and the Button Maze, we implemented a Proximal Policy Optimization (PPO) agent with independent actor and critic networks in PyTorch(Schulman et al., 2017).Both networks have three convolutional layers and two linear layers with rectified linear unit (ReLU) activation functions.The number of output channels of the convolutional layers is[32,64,64]with each layer.For convolutional kernels in those three convolutional layers, we use 8 × 8 with a stride of 4, 4 × 4 with a stride of 2, and 3 × 3 with a stride of 1, respectively.We flatten the output of the convolutional layers and feed it into the two linear layers with the intermediate feature dimension set to 512.The input for the network is an image observation of size 84 × 84 × 3 for both environments.For hyper-parameters, we follow the default hyper-parameters fromHuang et al. (2022b)on which the implementation is also heavily based.For the Continuous Button Maze environment, we use an Soft Actor Critic (SAC) agent with low-dimensional state vector inputs following the implementations by Huang et al. and adopting the hyper parameters setting from Klink et al..For the four curriculum generators used, we adopted the implementation fromKlink et al. (2022)'s official implementations (https://github.com/psclklnk/currot).Note that even though all three environments are confounded, we still use MDP-based policy learning algorithms.Because for those environments, in both target task and aligned source tasks, the confounder is revealed by other variables without intervention.Thus, the agent has perfect information to decide which state it is in exactly.So, we can still use PPO and SAC to find the optimal policy.F.1 ENVIRONMENT SPECIFICATIONS For Colored Sokoban, the target task definition is already specified in Example 1.For Button Maze, at each time step, let C i be the target location's color, B i be the button status of whether it has been pushed or not, L i be the agent's current location, Y i be the reward for this step and X i be the agent's action in this step.B i = ¬B i−1 when the agent pushes the button.The goal location's color C i = U i before the button is pushed but C i = U C after the button is pushed, where P (U i = 1) = 1/2, P (U C = 1) = 1/5.The reward function is specified as follows,In the Continuous Button Maze environment Fig.8, similar to the grid-world button maze, the agent also must navigate to the target region at the right time.The only difference is that this time the environment is an open area and all states and actions are in the continuous domain, which is exponentially large.The optimal strategy for the agent is still to push the button first then step onto Published as a conference paper at ICLR 2024 (a) ALP-GMM@1K (b) PLR@1K (c) Goal-GAN@1K (d) Currot@1K (e) ALP-GMM@100K (f) PLR@100K (g) Goal-GAN@100K (h) Currot@100K (i) ALP-GMM@200K (j) PLR@200K (k) Goal-GAN@200K (l) Currot@200K (m) ALP-GMM@300K (n) PLR@300K (o) Goal-GAN@300K (p) Currot@300K An MDP is defined to be a four-tuple ⟨S, A, R, T ⟩ where S is a finite set of states, A a finite set of actions, T : S × A → Π(S) the transition function mapping from state action pair to the distribution over the set of states and R : S × A → R the reward function(Kaelbling et al., 1996).And a POMDP is defined to be a six tuple with two additional elements than the MDP, ⟨S, A, R, T ⟩, O, p where O is a finite set of observations and p : S → Π(O) is the observation function mapping from the true underlying state to the distribution of observations(Kaelbling et al., 1998).Recall the definition of SCMs in the preliminary section, we can see that the definition of SCMs subsumes the transition function and inherent structural assumptions in MDPs and POMDPs.We can encode all the state/action/observation/reward variables as endogenous variables and the randomness of the
On the expressivity of markov reward. David Abel, Will Dabney, Anna Harutyunyan, Mark K Ho, Michael Littman, Doina Precup, Satinder Singh, Advances in Neural Information Processing Systems. 202134</p>
<p>Deep reinforcement learning at the edge of the statistical precipice. Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron Courville, Marc G Bellemare, Advances in Neural Information Processing Systems. A Beygelzimer, Y Dauphin, P Liang, J Wortman Vaughan, 2021</p>
<p>Modular multitask reinforcement learning with policy sketches. Jacob Andreas, Dan Klein, Sergey Levine, Proceedings of the 34th International Conference on Machine Learning. Doina Precup, Yee Whye Teh, the 34th International Conference on Machine LearningSydney, NSW, AustraliaPMLR2017. 6-11 August 2017. 201770of Proceedings of Machine Learning Research</p>
<p>Hindsight experience replay. Marcin Andrychowicz, Dwight Crow, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob Mcgrew, Josh Tobin, Pieter Abbeel, Wojciech Zaremba, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017. M Hanna, Rob Wallach, S V N Fergus, Roman Vishwanathan, Garnett, Long Beach, CA, USADecember 4-9, 2017. 2017Isabelle Guyon, Ulrike von Luxburg, Samy Bengio</p>
<p>Purposive behavior acquisition for a real robot by vision-based reinforcement learning. Minoru Asada, Shoichi Noda, Sukoya Tawaratsumida, Koh Hosoda, 10.1023/A:1018237008823Machine Learning. 199623</p>
<p>Active learning of inverse models with intrinsically motivated goal exploration in robots. Adrien Baranes, Pierre-Yves Oudeyer, 10.1016/j.robot.2012.05.008Robotics and Autonomous Systems. 6112013</p>
<p>Causal inference and the data-fusion problem. Elias Bareinboim, Judea Pearl, 10.1073/pnas.1510507113Proceedings of the National Academy of Sciences. the National Academy of Sciences2016113</p>
<p>On Pearl's Hierarchy and the Foundations of Causal Inference. Elias Bareinboim, Juan D Correa, Duligur Ibeling, Thomas Icard, 10.1145/3501714.35017432022Association for Computing MachineryNew York, NY, USA1 edition</p>
<p>Unifying count-based exploration and intrinsic motivation. G Marc, Sriram Bellemare, Georg Srinivasan, Tom Ostrovski, David Schaul, Rémi Saxton, Munos, Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems. D Daniel, Masashi Lee, Sugiyama, Isabelle Ulrike Von Luxburg, Roman Guyon, Garnett, Barcelona, Spain2016. December 5-10, 2016. 2016</p>
<p>Dynamic programming. Richard Bellman, 10.1126/science.153.3731.34Science. 15337311966</p>
<p>Exploration by random network distillation. Yuri Burda, Harrison Edwards, Amos J Storkey, Oleg Klimov, ICLR 20197th International Conference on Learning Representations. New Orleans, LA, USAMay 6-9, 2019. 2019OpenReview.net</p>
<p>Minimalistic gridworld environment for gymnasium. Maxime Chevalier-Boisvert, Lucas Willems, Suman Pal, 2018</p>
<p>Outcome-directed reinforcement learning by uncertainty \&amp; temporal distance-aware curriculum goal generation. Daesol Cho, Seungjae Lee, H Jin Kim, The Eleventh International Conference on Learning Representations. 2023</p>
<p>General transportability of soft interventions: Completeness results. Juan Correa, Elias Bareinboim, Advances in Neural Information Processing Systems. H Larochelle, M Ranzato, R Hadsell, M F Balcan, H Lin, Curran Associates, Inc202033</p>
<p>A multi-level level generator. Steve Dahlskog, Julian Togelius, 10.1109/CIG.2014.69329092014 IEEE Conference on Computational Intelligence and Games, CIG. Dortmund, GermanyIEEE2014. August 26-29, 2014. 2014</p>
<p>Emergent complexity and zero-shot transfer via unsupervised environment design. Michael Dennis, Natasha Jaques, Eugene Vinitsky, Alexandre M Bayen, Stuart Russell, Andrew Critch, Sergey Levine, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems. Hugo Larochelle, Marc ' , Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, Hsuan-Tien Lin, NeurIPS2020. 2020. December 6-12, 2020, Virtual, 2020</p>
<p>Go-explore: a new approach for hard-exploration problems. arxiv. Adrien Ecoffet, Joost Huizinga, Joel Lehman, Kenneth O Stanley, Jeff Clune, 2019</p>
<p>Diversity is all you need: Learning skills without a reward function. Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, Sergey Levine, ICLR 20197th International Conference on Learning Representations. New Orleans, LA, USAMay 6-9, 2019. 2019OpenReview.net</p>
<p>Reverse curriculum generation for reinforcement learning. Carlos Florensa, David Held, Markus Wulfmeier, Michael Zhang, Pieter Abbeel, Proceedings of Machine Learning Research. Machine Learning ResearchMountain View, California, USAPMLR2017. November 13-15, 2017. 20171st Annual Conference on Robot Learning</p>
<p>Automatic goal generation for reinforcement learning agents. Carlos Florensa, David Held, Xinyang Geng, Pieter Abbeel, Proceedings of the 35th International Conference on Machine Learning, ICML 2018. Jennifer G Dy, Andreas Krause, the 35th International Conference on Machine Learning, ICML 2018Stockholmsmässan, Stockholm, SwedenPMLRJuly 10-15, 2018. 201880of Proceedings of Machine Learning Research</p>
<p>Causality-driven hierarchical structure discovery for reinforcement learning. Xing Hu, Rui Zhang, Ke Tang, Jiaming Guo, Qi Yi, Ruizhi Chen, Zidong Du, Ling Li, Qi Guo, Yunji Chen, Advances in Neural Information Processing Systems. 20064-20076, 202235</p>
<p>Curriculum reinforcement learning using optimal transport via gradual domain adaptation. Peide Huang, Mengdi Xu, Jiacheng Zhu, Laixi Shi, Fei Fang, Ding Zhao, Advances in Neural Information Processing Systems. Alice H Oh, Alekh Agarwal, Danielle Belgrave, Kyunghyun Cho, 2022a</p>
<p>The 37 implementation details of proximal policy optimization. Shengyi Huang, Rousslan Fernand, Julien Dossa, Antonin Raffin, Anssi Kanervisto, Weixun Wang, ICLR Blog Track. 2022b</p>
<p>Cleanrl: High-quality single-file implementations of deep reinforcement learning algorithms. Shengyi Huang, Rousslan Fernand, Julien Dossa, Chang Ye, Jeff Braga, Dipam Chakraborty, Kinal Mehta, G M João, Araújo, Journal of Machine Learning Research. 232742022c</p>
<p>Prioritized level replay. Minqi Jiang, Edward Grefenstette, Tim Rocktäschel, Proceedings of the 38th International Conference on Machine Learning, ICML 2021. Marina Meila, Tong Zhang, the 38th International Conference on Machine Learning, ICML 2021PMLR18-24 July 2021. 2021139</p>
<p>Illuminating generalization in deep reinforcement learning through procedural level generation. Niels Justesen, Ruben Rodriguez Torrado, Philip Bontrager, Ahmed Khalifa, Julian Togelius, Sebastian Risi, 10.48550/arXiv.1806.10729Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems. Montréal, Canada2018. 2018. December 3-8, 2018. 2018</p>
<p>Reinforcement learning: A survey. Leslie Pack, Kaelbling Michael L Littman, Andrew W Moore, Journal of artificial intelligence research. 41996</p>
<p>Planning and acting in partially observable stochastic domains. Leslie Pack, Kaelbling Michael L Littman, Anthony R Cassandra, Artificial intelligence. 1011-21998</p>
<p>How do humans teach: On curriculum learning and teaching dimension. Faisal Khan, Xiaojin (jerry) Zhu, Bilge Mutlu, Advances in Neural Information Processing Systems 24: 25th Annual Conference on Neural Information Processing Systems. John Shawe-Taylor, Richard S Zemel, Peter L Bartlett, C N Fernando, Kilian Q Pereira, Weinberger, Granada, Spain2011. December 2011. 2011</p>
<p>Curriculum reinforcement learning via constrained optimal transport. Pascal Klink, Haoyi Yang, Carlo D' Eramo, Jan Peters, Joni Pajarinen, International Conference on Machine Learning, ICML 2022. Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvári, Gang Niu, Sivan Sabato, Baltimore, Maryland, USAPMLRJuly 2022. 2022162of Proceedings of Machine Learning Research</p>
<p>Probabilistic Graphical Models -Principles and Techniques. Daphne Koller, Nir Friedman, 2009MIT Press</p>
<p>Multi-agent influence diagrams for representing and solving games. Daphne Koller, Brian Milch, 10.1016/S0899-8256(02)00544-4S0899-8256(02)00544-4Games and Economic Behavior. 4512003</p>
<p>Representing and solving decision problems with limited information. L Steffen, Dennis Lauritzen, Nilsson, Management Science. 4792001</p>
<p>Causal discovery from observational and interventional data across multiple environments. Adam Li, Amin Jaber, Elias Bareinboim, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>General video game AI: competition, challenges and opportunities. Diego Perez Liebana, Spyridon Samothrakis, Julian Togelius, Tom Schaul, Simon M Lucas, Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence. Dale Schuurmans, Michael P Wellman, the Thirtieth AAAI Conference on Artificial IntelligencePhoenix, Arizona, USAAAAI PressFebruary 12-17, 2016. 2016</p>
<p>Overlapping layered learning. Patrick Macalpine, Peter Stone, 10.1016/j.artint.2017.09.001Artificial Intelligence. 2542018</p>
<p>Source task creation for curriculum learning. Sanmit Narvekar, Jivko Sinapov, Matteo Leonetti, Peter Stone, Proceedings of the 2016 International Conference on Autonomous Agents &amp; Multiagent Systems. M Catholijn, Stacy Jonker, John Marsella, Karl Thangarajah, Tuyls, the 2016 International Conference on Autonomous Agents &amp; Multiagent SystemsSingaporeACMMay 9-13, 2016. 2016</p>
<p>Autonomous task sequencing for customized curriculum design in reinforcement learning. Sanmit Narvekar, Jivko Sinapov, Peter Stone, 10.24963/ijcai.2017/353Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI 2017. Carles Sierra, the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI 2017Melbourne, AustraliaAugust 19-25, 2017. 2017</p>
<p>Curriculum learning for reinforcement learning domains: A framework and survey. Sanmit Narvekar, Bei Peng, Matteo Leonetti, Jivko Sinapov, Matthew E Taylor, Peter Stone, Journal of Machine Learning Research. 212020</p>
<p>Solving rubik's cube with a robot hand. Ilge Openai, Marcin Akkaya, Maciek Andrychowicz, Mateusz Chociej, Bob Litwin, Arthur Mcgrew, Alex Petron, Matthias Paino, Glenn Plappert, Raphael Powell, Jonas Ribas, Nikolas Schneider, Jerry Tezak, Peter Tworek, Lilian Welinder, Qiming Weng, Wojciech Yuan, Lei Zaremba, Zhang, arxiv. 2019</p>
<p>Evolving curricula with regret-based environment design. Jack Parker-Holder, Minqi Jiang, Michael Dennis, Mikayel Samvelyan, Jakob N Foerster, Edward Grefenstette, Tim Rocktäschel, International Conference on Machine Learning, ICML 2022. Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvári, Gang Niu, Sivan Sabato, Baltimore, Maryland, USAPMLRJuly 2022. 2022162of Proceedings of Machine Learning Research</p>
<p>Causality: Models, Reasoning, and Inference. Judea Pearl, 10.1017/CBO97805118031612009Cambridge University Press2 edition</p>
<p>Transportability of Causal and Statistical Relations: A Formal Approach. Judea Pearl, Elias Bareinboim, 10.1609/aaai.v25i1.7861Proceedings of the Twenty-Fifth AAAI Conference on Artificial Intelligence. the Twenty-Fifth AAAI Conference on Artificial Intelligence201125</p>
<p>Curriculum design for machine learners in sequential decision tasks. Bei Peng, James Macglashan, Robert Loftin, David L Michael L Littman, Matthew E Roberts, Taylor, 10.1109/TETCI.2018.2829980IEEE Transactions on Emerging Topics in Computational Intelligence. 242018</p>
<p>Causal discovery in heterogeneous environments under the sparse mechanism shift hypothesis. Ronan Perry, Julius Von Kügelgen, Bernhard Schölkopf, Advances in Neural Information Processing Systems. Alice H Oh, Alekh Agarwal, Danielle Belgrave, Kyunghyun Cho, 2022</p>
<p>Teacher algorithms for curriculum learning of Deep RL in continuously parameterized environments. Rémy Portelas, Cédric Colas, Katja Hofmann, Pierre-Yves Oudeyer, of Proceedings of Machine Learning Research. Leslie Pack Kaelbling, Danica Kragic, Komei Sugiura, CoRL; Osaka, JapanPMLR2019. October 30 -November 1, 2019. 20191003rd Annual Conference on Robot Learning</p>
<p>Automatic curriculum learning for deep RL: A short survey. Rémy Portelas, Cédric Colas, Lilian Weng, Katja Hofmann, Pierre-Yves Oudeyer, 10.24963/ijcai.2020/671Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence. Christian Bessiere, the Twenty-Ninth International Joint Conference on Artificial Intelligence20202020</p>
<p>Automated curricula through setter-solver interactions. arxiv. Sébastien Racanière, Andrew K Lampinen, Adam Santoro, David P Reichert, Vlad Firoiu, Timothy P Lillicrap, 2019</p>
<p>Increasing generality in machine learning through procedural content generation. Sebastian Risi, Julian Togelius, Nature Machine Intelligence. 282020</p>
<p>Learning montezuma's revenge from a single demonstration. Tim Salimans, Richard Chen, arxiv. 2018</p>
<p>Neural network learning control of robot manipulators using gradually increasing task difficulty. Terence D Sanger, 10.1109/70.294207IEEE transactions on Robotics and Automation. 1031994</p>
<p>A video game description language for model-based or interactive learning. Tom Schaul, 10.1109/CIG.2013.66336102013 IEEE Conference on Computational Inteligence in Games (CIG). Niagara Falls, ON, CanadaIEEEAugust 11-13, 2013. 2013</p>
<p>Gym-sokoban. B Max-Philipp, Schrader, 2018</p>
<p>Proximal policy optimization algorithms. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, </p>
<p>Training and tracking in robotics. G Oliver, Richard S Selfridge, Andrew G Sutton, Barto, Proceedings of the 9th International Joint Conference on Artificial Intelligence. the 9th International Joint Conference on Artificial IntelligenceMorgan Kaufmann Publishers Inc19851</p>
<p>Object-oriented curriculum generation for reinforcement learning. Felipe Leno, Da Silva, Anna Helena, Reali Costa, Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems, AAMAS '18. the 17th International Conference on Autonomous Agents and MultiAgent Systems, AAMAS '18Richland, SC2018International Foundation for Autonomous Agents and Multiagent Systems</p>
<p>Intrinsically motivated reinforcement learning. Satinder Singh, Andrew G Barto, Nuttapong Chentanez, Advances in Neural Information Processing Systems 17 [Neural Information Processing Systems, NIPS 2004. Vancouver, British Columbia, CanadaDecember 13-18, 2004. 200417</p>
<p>Intrinsic motivation and automatic curricula via asymmetric self-play. Sainbayar Sukhbaatar, Zeming Lin, Ilya Kostrikov, Gabriel Synnaeve, Arthur Szlam, Rob Fergus, 6th International Conference on Learning Representations, ICLR 2018. Vancouver, BC, CanadaApril 30 -May 3, 2018. 2018Conference Track Proceedings. OpenReview.net</p>
<p>Reinforcement Learning: An Introduction. A Bradford Book. S Richard, Andrew G Sutton, Barto, 2018second edition. ISBN 0262039249</p>            </div>
        </div>

    </div>
</body>
</html>
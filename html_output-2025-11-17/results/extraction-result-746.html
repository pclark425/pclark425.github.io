<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-746 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-746</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-746</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-20.html">extraction-schema-20</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <p><strong>Paper ID:</strong> paper-252595886</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2209.14932v1.pdf" target="_blank">Contrastive Unsupervised Learning of World Model with Invariant Causal Features</a></p>
                <p><strong>Paper Abstract:</strong> In this paper we present a world model, which learns causal features using the invariance principle. In particular, we use contrastive unsupervised learning to learn the invariant causal features, which enforces invariance across augmentations of irrelevant parts or styles of the observation. The world-model-based reinforcement learning methods independently optimize representation learning and the policy. Thus naive contrastive loss implementation collapses due to a lack of supervisory signals to the representation learning module. We propose an intervention invariant auxiliary task to mitigate this issue. Specifically, we utilize depth prediction to explicitly enforce the invariance and use data augmentation as style intervention on the RGB observation space. Our design leverages unsupervised representation learning to learn the world model with invariant causal features. Our proposed method significantly outperforms current state-of-the-art model-based and model-free reinforcement learning methods on out-of-distribution point navigation tasks on the iGibson dataset. Moreover, our proposed model excels at the sim-to-real transfer of our perception learning module. Finally, we evaluate our approach on the DeepMind control suite and enforce invariance only implicitly since depth is not available. Nevertheless, our proposed model performs on par with the state-of-the-art counterpart.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e746.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e746.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WMC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>World Model with invariant Causal features</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model-based RL world model that learns invariant causal visual features by using contrastive (InfoNCE) loss across style interventions (data augmentations) together with an intervention-invariant auxiliary task (depth reconstruction) to avoid collapse and to suppress spurious style signals.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>WMC (World Model with invariant Causal features)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Learns a latent state encoder for visual control by enforcing invariance across style interventions created by image-level data augmentations (spatial jitter, Gaussian blur, color jitter, grayscale, cutout). It applies an InfoNCE contrastive loss between two augmented views of the same observation (positive pair) and other batch samples as negatives, with a momentum key encoder (MoCo-style). To prevent collapse when representation learning and policy learning are decoupled, it adds an intervention-invariant auxiliary task: depth reconstruction from the invariant latent state, which focuses representation on geometry (causal) rather than style (spurious). The latent states feed a recurrent predictive memory (RSSM) used for imagination-based planning (DreamerV2 backbone).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>iGibson 1.0 (PointGoal navigation), Gibson (sim-to-real perception test), DeepMind Control Suite (DMControl)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Interactive embodied-simulation environments: iGibson and Gibson are photorealistic 3D indoor navigation simulators with realistic textures/materials and support for active agent interaction; DMControl is a continuous-control physics-based benchmark (pixel observations). Environments allow active experimentation (agent actions affect future observations).</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Creates interventions on spurious/style variables via image data augmentations and enforces encoder invariance across these interventions using contrastive learning; uses depth reconstruction as an auxiliary, intervention-invariant target to encourage encoding of geometric/causal features and downweight texture/style features.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Irrelevant visual style/textural variation (appearance, color, blur, occlusions from cutout), spurious correlated image features that do not causally affect actions</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Implicit detection by enforcing invariance: data-augmentation-induced changes that alter style but not depth content are used to reveal features that vary (spurious); features that vary across augmentations get suppressed by contrastive/invariance objective in combination with depth supervision. No explicit statistical test is used.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Contrastive invariance objective forces embeddings of augmented views to match, thereby reducing embedding reliance on augmentation-varying (style) features; auxiliary depth reconstruction explicitly encourages geometric features and thus downweights texture/style signals in the learned representation.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Empirical ablations and OoD evaluation: removal of depth auxiliary or augmentation (action replay/texture randomization) degrades performance substantially, which the authors use to argue that non-invariant (spurious) features were previously used; no formal causal refutation test is applied.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>On iGibson PointGoal OoD tests, WMC substantially outperforms baselines: at 100k environment steps total Success Rate (SR) = 22.3 and SPL = 0.16 (WMC), compared to DreamerV2 SR = 1.3 and SPL = 0.01; at 500k steps WMC total SR = 47.26 and SPL = 0.36 vs DreamerV2 SR = 1.4 and SPL = 0.01. WMC also outperforms CURL and RAD baselines on the reported OoD and sim-to-real perception transfer tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Ablations removing the depth reconstruction auxiliary or action-replay/texture-randomization show large drops: removing depth causes the worst degradation (authors report substantial collapse and inability to learn meaningful control), and replacing depth reconstruction with RGB reconstruction cuts success rate ~50% and reduces SPL by ~two-thirds; exact ablation numbers are reported qualitatively and partially in Table 4 of the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Enforcing invariance via contrastive loss on augmentation-induced interventions plus an intervention-invariant auxiliary target (depth) yields representations that resist spurious style/texture correlations and lead to large OoD and sim-to-real gains; contrastive invariance without the depth auxiliary collapses in the model-based RL setup where representation and policy are optimized separately; action-replay/texture randomization helps further but is not essential; depth supervision is critical to prevent collapse and to focus on geometric causal features.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e746.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e746.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Invariant Prediction (IP)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Causal inference by using invariant prediction (Invariant Prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A causal discovery approach that identifies variable sets whose conditional distribution of effect given causes remains invariant across environments/interventions, using invariance tests to select causal predictors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Causal inference by using invariant prediction: identification and confidence intervals</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Invariant Prediction (Peters et al., 2016)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Searches for subsets of predictors S such that the conditional distribution P(Y|X_S) is invariant across multiple environments; variables in such invariant sets are candidate causal parents. In practice this is implemented by statistical tests for equality of conditional distributions across environments and confidence intervals for effect estimates.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>General (requires multiple 'environments' or datasets produced under different interventions)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Non-specific observational or interventional datasets organized into distinct environments (not necessarily interactive simulators); method assumes access to multiple environments for invariance testing.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Identifies and excludes variables that cause non-invariant conditional distributions across environments; variable selection via invariance testing.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Spurious/unstable predictors whose predictive relationship changes under interventions (e.g., variables correlated but not causal), selection bias across environments can be detected if it breaks invariance.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Statistical tests comparing conditional distributions P(Y|X_S) across environments; invariance violations indicate spurious predictors.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Not a downweighting mechanism per se; instead selects invariant predictor subsets (effectively discards non-invariant/spurious features).</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Refutation via lack of invariance across environments: candidate causal sets that fail invariance are rejected; confidence intervals are computed for identification.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The invariance principle provides a formal route to causal variable selection when multiple environments with different interventions are available; it can identify causal predictors by rejecting predictors whose conditional relationship with the target changes across environments.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e746.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e746.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IRM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Invariant Risk Minimization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A learning principle that seeks predictors whose optimal classifier is invariant across environments by adding a penalty that encourages solutions with invariant optimal predictors, aiming to avoid spurious correlations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Invariant risk minimization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Invariant Risk Minimization (Arjovsky et al., 2020)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Learns representations such that a classifier on top of the representation attains simultaneously low risk in all training environments and is 'invariant'â€”operationalized via a penalty that encourages gradient of risk w.r.t. classifier to be zero in each environment (so the same classifier works across environments). The approach aims to block predictors that exploit environment-specific spurious correlations.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>General multi-environment datasets (requires environment labels/interventions)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Non-interactive or interactive settings where training data can be partitioned into multiple environments with different distributional shifts/interventions; environments are required for invariance constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Penalizes solutions that rely on environment-specific (spurious) correlations by enforcing an invariance constraint; implicitly downweights spurious features by making them incompatible with the invariance objective.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Features that correlate with the target only in specific environments (non-causal correlations), unstable associations across environments.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Detection is implicit: spurious predictors are those whose inclusion prevents finding an invariant predictor across environments; no explicit test but the optimization rejects them if they cannot satisfy the invariance objective.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Invariance penalty during model training reduces reliance on environment-specific features (they cannot produce invariant risk minima), effectively downweighting them in learned representation/classifier.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>A predictor is refuted if it cannot achieve low risk simultaneously across environments under the invariance penalty; lacks formal statistical refutation tests but relies on joint optimization across environments.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>IRM provides a practical training objective to discourage models from using spurious environment-specific cues by requiring invariance across environments, but it needs multiple environments and can be sensitive to optimization and representation parameterization.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e746.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e746.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zhang2020</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Invariant causal prediction for block MDPs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method adapting the invariance principle for RL/block-MDP settings: learning invariant causal features across multiple environments using a shared encoder and environment-specific encoders to separate causal from spurious features.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Invariant causal prediction for block MDPs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Invariant causal prediction for block MDPs (Zhang et al., 2020)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Uses multiple environments to learn invariant causal features by training a common encoder that captures invariant aspects and separate environment-specific encoders for spurious features; invariance across environments is used to identify causal state variables in block MDPs.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Block MDPs / multiple-environment RL benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Interactive RL domains modelled as block MDPs where observations contain both latent causal state and spurious style variables; requires access to multiple environments (different interventions/textures) for training invariance.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Architectural separation: common encoder for invariant features and environment-specific encoders for spurious features; learning objective encourages the common encoder to contain invariant (causal) information.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Environment-specific style/texture variations and other non-causal observation components that vary across environments.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Identification via cross-environment training: features that are not consistent across environments are treated as spurious and captured by environment-specific encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>By allocating capacity to environment-specific encoders, spurious signals are prevented from being used by the common encoder (downweighted in causal representation).</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Refutation is operational: features failing to be predictable in a shared way across environments are not included in the invariant representation; no formal statistical refutation method described in this paper's discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Learning with multiple environments and an explicit separation between common (invariant) and environment-specific encoders enables extraction of causal state representations in block MDPs; however, such approaches require multiple sources/environments with distinct interventions (the WMC paper contrasts this with using data augmentation to simulate interventions from a single environment).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e746.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e746.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mitrovic2021</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Representation learning via invariant causal mechanisms</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recent formulation connecting self-supervised contrastive representation learning with invariant causal mechanisms; proposes regularizers that encourage representations aligned with causal invariances across environments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Representation learning via invariant causal mechanisms</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Representation learning via invariant causal mechanisms (Mitrovic et al., 2021)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Formalizes self-supervised representation learning under the invariance/causal-mechanism viewpoint and proposes regularizers that encourage representations to respect causal invariances across interventions; connects contrastive objectives to causal representation goals.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>General datasets / multiple-environment settings; applicable to self-supervised learning on visual data</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Non-specific; typically requires data organized by environments or augmentations representing interventions; not tied to a particular interactive simulator in the WMC paper's mention.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Uses regularization of contrastive/self-supervised objectives to prefer invariant features and penalize those that vary across environments/augmentations, thereby suppressing spurious features.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Augmentation-variant or environment-variant features (non-causal style signals)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Implicit via regularizer that measures variability of representations across environments/augmentations; features with high variability are discouraged.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Regularization in the learning objective reduces the representational weight of non-invariant features.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Not formalized as hypothesis tests; relies on optimization to push representations toward invariant mechanisms and away from spurious ones.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Provides theoretical framing and practical regularizers linking contrastive/self-supervised learning to causal invariance, supporting the idea that invariance-based objectives can recover causal features under suitable data/augmentation regimes; this work is cited in WMC as conceptual motivation for invariant contrastive training.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e746.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e746.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Contrastive (InfoNCE) in RL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Contrastive learning using InfoNCE (applied to RL; e.g., CURL/SimCLR/InfoNCE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Contrastive self-supervised representation learning (InfoNCE) used to make embeddings invariant to data augmentations; applied as an auxiliary loss in RL to obtain more robust visual representations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Representation learning with contrastive predictive coding</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>InfoNCE-based contrastive learning (SimCLR/InfoNCE/CURL variants)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Constructs positive pairs by applying two correlated augmentations to the same observation and negatives from other samples; uses an InfoNCE objective to pull positives together and push negatives apart in embedding space. In RL this is often used as an auxiliary loss (CURL) or as an encoder training signal; MoCo-style momentum encoders and large negative dictionaries or in-batch negatives are common engineering choices.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Applied in this paper to iGibson, Gibson, DMControl as auxiliary representation learning in a model-based RL pipeline (DreamerV2 backbone).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Interactive simulators (embodied navigation and continuous control) where augmentations act as synthetic interventions on style features; environments are actively controlled by the agent.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>By forcing invariance across augmentation-induced changes in style, contrastive learning reduces reliance on augmentation-variant (spurious) features; engineering choices (negative mining, momentum encoder) affect efficacy.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Visual style/appearance variations (color, brightness, blur, cutout occlusions) that correlate spuriously with rewards/actions.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Implicit: if a feature changes under augmentation it will not be consistent across positive pairs and thus will not be reinforced; no explicit detection test.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Learning objective (InfoNCE) prevents features that vary across augmentations from contributing to stable positive similarity, thereby downweighting them in learned embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Empirical evaluation: compare downstream RL performance and OoD generalization with/without contrastive objective and with different auxiliary tasks; no formal refutation test.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>When integrated into WMC with depth auxiliary and momentum key encoder, contrastive learning substantially improves OoD navigation success (see WMC performance above). Prior RL works cited (CURL, RAD) show contrastive or augmentation benefits for sample efficiency and robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>In WMC without the depth auxiliary, contrastive loss collapses (representation learning fails) because the world model and controller are not optimized end-to-end; paper reports strong degradation when contrastive is used alone without depth supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Contrastive (InfoNCE) objectives can enforce invariance to augmentation-induced style changes and improve OoD robustness when combined with appropriate auxiliary tasks (depth) and engineering (momentum encoder); used alone in a decoupled model-based RL setup, it can collapse unless there is an intervention-invariant supervisory signal.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Causal inference by using invariant prediction: identification and confidence intervals <em>(Rating: 2)</em></li>
                <li>Invariant risk minimization <em>(Rating: 2)</em></li>
                <li>Invariant causal prediction for block MDPs <em>(Rating: 2)</em></li>
                <li>Representation learning via invariant causal mechanisms <em>(Rating: 2)</em></li>
                <li>Curl: Contrastive unsupervised representations for reinforcement learning <em>(Rating: 2)</em></li>
                <li>A simple framework for contrastive learning of visual representations <em>(Rating: 1)</em></li>
                <li>Momentum contrast for unsupervised visual representation learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-746",
    "paper_id": "paper-252595886",
    "extraction_schema_id": "extraction-schema-20",
    "extracted_data": [
        {
            "name_short": "WMC",
            "name_full": "World Model with invariant Causal features",
            "brief_description": "A model-based RL world model that learns invariant causal visual features by using contrastive (InfoNCE) loss across style interventions (data augmentations) together with an intervention-invariant auxiliary task (depth reconstruction) to avoid collapse and to suppress spurious style signals.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "WMC (World Model with invariant Causal features)",
            "method_description": "Learns a latent state encoder for visual control by enforcing invariance across style interventions created by image-level data augmentations (spatial jitter, Gaussian blur, color jitter, grayscale, cutout). It applies an InfoNCE contrastive loss between two augmented views of the same observation (positive pair) and other batch samples as negatives, with a momentum key encoder (MoCo-style). To prevent collapse when representation learning and policy learning are decoupled, it adds an intervention-invariant auxiliary task: depth reconstruction from the invariant latent state, which focuses representation on geometry (causal) rather than style (spurious). The latent states feed a recurrent predictive memory (RSSM) used for imagination-based planning (DreamerV2 backbone).",
            "environment_name": "iGibson 1.0 (PointGoal navigation), Gibson (sim-to-real perception test), DeepMind Control Suite (DMControl)",
            "environment_description": "Interactive embodied-simulation environments: iGibson and Gibson are photorealistic 3D indoor navigation simulators with realistic textures/materials and support for active agent interaction; DMControl is a continuous-control physics-based benchmark (pixel observations). Environments allow active experimentation (agent actions affect future observations).",
            "handles_distractors": true,
            "distractor_handling_technique": "Creates interventions on spurious/style variables via image data augmentations and enforces encoder invariance across these interventions using contrastive learning; uses depth reconstruction as an auxiliary, intervention-invariant target to encourage encoding of geometric/causal features and downweight texture/style features.",
            "spurious_signal_types": "Irrelevant visual style/textural variation (appearance, color, blur, occlusions from cutout), spurious correlated image features that do not causally affect actions",
            "detection_method": "Implicit detection by enforcing invariance: data-augmentation-induced changes that alter style but not depth content are used to reveal features that vary (spurious); features that vary across augmentations get suppressed by contrastive/invariance objective in combination with depth supervision. No explicit statistical test is used.",
            "downweighting_method": "Contrastive invariance objective forces embeddings of augmented views to match, thereby reducing embedding reliance on augmentation-varying (style) features; auxiliary depth reconstruction explicitly encourages geometric features and thus downweights texture/style signals in the learned representation.",
            "refutation_method": "Empirical ablations and OoD evaluation: removal of depth auxiliary or augmentation (action replay/texture randomization) degrades performance substantially, which the authors use to argue that non-invariant (spurious) features were previously used; no formal causal refutation test is applied.",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "On iGibson PointGoal OoD tests, WMC substantially outperforms baselines: at 100k environment steps total Success Rate (SR) = 22.3 and SPL = 0.16 (WMC), compared to DreamerV2 SR = 1.3 and SPL = 0.01; at 500k steps WMC total SR = 47.26 and SPL = 0.36 vs DreamerV2 SR = 1.4 and SPL = 0.01. WMC also outperforms CURL and RAD baselines on the reported OoD and sim-to-real perception transfer tasks.",
            "performance_without_robustness": "Ablations removing the depth reconstruction auxiliary or action-replay/texture-randomization show large drops: removing depth causes the worst degradation (authors report substantial collapse and inability to learn meaningful control), and replacing depth reconstruction with RGB reconstruction cuts success rate ~50% and reduces SPL by ~two-thirds; exact ablation numbers are reported qualitatively and partially in Table 4 of the paper.",
            "has_ablation_study": true,
            "number_of_distractors": null,
            "key_findings": "Enforcing invariance via contrastive loss on augmentation-induced interventions plus an intervention-invariant auxiliary target (depth) yields representations that resist spurious style/texture correlations and lead to large OoD and sim-to-real gains; contrastive invariance without the depth auxiliary collapses in the model-based RL setup where representation and policy are optimized separately; action-replay/texture randomization helps further but is not essential; depth supervision is critical to prevent collapse and to focus on geometric causal features.",
            "uuid": "e746.0"
        },
        {
            "name_short": "Invariant Prediction (IP)",
            "name_full": "Causal inference by using invariant prediction (Invariant Prediction)",
            "brief_description": "A causal discovery approach that identifies variable sets whose conditional distribution of effect given causes remains invariant across environments/interventions, using invariance tests to select causal predictors.",
            "citation_title": "Causal inference by using invariant prediction: identification and confidence intervals",
            "mention_or_use": "mention",
            "method_name": "Invariant Prediction (Peters et al., 2016)",
            "method_description": "Searches for subsets of predictors S such that the conditional distribution P(Y|X_S) is invariant across multiple environments; variables in such invariant sets are candidate causal parents. In practice this is implemented by statistical tests for equality of conditional distributions across environments and confidence intervals for effect estimates.",
            "environment_name": "General (requires multiple 'environments' or datasets produced under different interventions)",
            "environment_description": "Non-specific observational or interventional datasets organized into distinct environments (not necessarily interactive simulators); method assumes access to multiple environments for invariance testing.",
            "handles_distractors": true,
            "distractor_handling_technique": "Identifies and excludes variables that cause non-invariant conditional distributions across environments; variable selection via invariance testing.",
            "spurious_signal_types": "Spurious/unstable predictors whose predictive relationship changes under interventions (e.g., variables correlated but not causal), selection bias across environments can be detected if it breaks invariance.",
            "detection_method": "Statistical tests comparing conditional distributions P(Y|X_S) across environments; invariance violations indicate spurious predictors.",
            "downweighting_method": "Not a downweighting mechanism per se; instead selects invariant predictor subsets (effectively discards non-invariant/spurious features).",
            "refutation_method": "Refutation via lack of invariance across environments: candidate causal sets that fail invariance are rejected; confidence intervals are computed for identification.",
            "uses_active_learning": null,
            "inquiry_strategy": null,
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "The invariance principle provides a formal route to causal variable selection when multiple environments with different interventions are available; it can identify causal predictors by rejecting predictors whose conditional relationship with the target changes across environments.",
            "uuid": "e746.1"
        },
        {
            "name_short": "IRM",
            "name_full": "Invariant Risk Minimization",
            "brief_description": "A learning principle that seeks predictors whose optimal classifier is invariant across environments by adding a penalty that encourages solutions with invariant optimal predictors, aiming to avoid spurious correlations.",
            "citation_title": "Invariant risk minimization",
            "mention_or_use": "mention",
            "method_name": "Invariant Risk Minimization (Arjovsky et al., 2020)",
            "method_description": "Learns representations such that a classifier on top of the representation attains simultaneously low risk in all training environments and is 'invariant'â€”operationalized via a penalty that encourages gradient of risk w.r.t. classifier to be zero in each environment (so the same classifier works across environments). The approach aims to block predictors that exploit environment-specific spurious correlations.",
            "environment_name": "General multi-environment datasets (requires environment labels/interventions)",
            "environment_description": "Non-interactive or interactive settings where training data can be partitioned into multiple environments with different distributional shifts/interventions; environments are required for invariance constraints.",
            "handles_distractors": true,
            "distractor_handling_technique": "Penalizes solutions that rely on environment-specific (spurious) correlations by enforcing an invariance constraint; implicitly downweights spurious features by making them incompatible with the invariance objective.",
            "spurious_signal_types": "Features that correlate with the target only in specific environments (non-causal correlations), unstable associations across environments.",
            "detection_method": "Detection is implicit: spurious predictors are those whose inclusion prevents finding an invariant predictor across environments; no explicit test but the optimization rejects them if they cannot satisfy the invariance objective.",
            "downweighting_method": "Invariance penalty during model training reduces reliance on environment-specific features (they cannot produce invariant risk minima), effectively downweighting them in learned representation/classifier.",
            "refutation_method": "A predictor is refuted if it cannot achieve low risk simultaneously across environments under the invariance penalty; lacks formal statistical refutation tests but relies on joint optimization across environments.",
            "uses_active_learning": null,
            "inquiry_strategy": null,
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "IRM provides a practical training objective to discourage models from using spurious environment-specific cues by requiring invariance across environments, but it needs multiple environments and can be sensitive to optimization and representation parameterization.",
            "uuid": "e746.2"
        },
        {
            "name_short": "Zhang2020",
            "name_full": "Invariant causal prediction for block MDPs",
            "brief_description": "A method adapting the invariance principle for RL/block-MDP settings: learning invariant causal features across multiple environments using a shared encoder and environment-specific encoders to separate causal from spurious features.",
            "citation_title": "Invariant causal prediction for block MDPs",
            "mention_or_use": "mention",
            "method_name": "Invariant causal prediction for block MDPs (Zhang et al., 2020)",
            "method_description": "Uses multiple environments to learn invariant causal features by training a common encoder that captures invariant aspects and separate environment-specific encoders for spurious features; invariance across environments is used to identify causal state variables in block MDPs.",
            "environment_name": "Block MDPs / multiple-environment RL benchmarks",
            "environment_description": "Interactive RL domains modelled as block MDPs where observations contain both latent causal state and spurious style variables; requires access to multiple environments (different interventions/textures) for training invariance.",
            "handles_distractors": true,
            "distractor_handling_technique": "Architectural separation: common encoder for invariant features and environment-specific encoders for spurious features; learning objective encourages the common encoder to contain invariant (causal) information.",
            "spurious_signal_types": "Environment-specific style/texture variations and other non-causal observation components that vary across environments.",
            "detection_method": "Identification via cross-environment training: features that are not consistent across environments are treated as spurious and captured by environment-specific encoders.",
            "downweighting_method": "By allocating capacity to environment-specific encoders, spurious signals are prevented from being used by the common encoder (downweighted in causal representation).",
            "refutation_method": "Refutation is operational: features failing to be predictable in a shared way across environments are not included in the invariant representation; no formal statistical refutation method described in this paper's discussion.",
            "uses_active_learning": null,
            "inquiry_strategy": null,
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "Learning with multiple environments and an explicit separation between common (invariant) and environment-specific encoders enables extraction of causal state representations in block MDPs; however, such approaches require multiple sources/environments with distinct interventions (the WMC paper contrasts this with using data augmentation to simulate interventions from a single environment).",
            "uuid": "e746.3"
        },
        {
            "name_short": "Mitrovic2021",
            "name_full": "Representation learning via invariant causal mechanisms",
            "brief_description": "A recent formulation connecting self-supervised contrastive representation learning with invariant causal mechanisms; proposes regularizers that encourage representations aligned with causal invariances across environments.",
            "citation_title": "Representation learning via invariant causal mechanisms",
            "mention_or_use": "mention",
            "method_name": "Representation learning via invariant causal mechanisms (Mitrovic et al., 2021)",
            "method_description": "Formalizes self-supervised representation learning under the invariance/causal-mechanism viewpoint and proposes regularizers that encourage representations to respect causal invariances across interventions; connects contrastive objectives to causal representation goals.",
            "environment_name": "General datasets / multiple-environment settings; applicable to self-supervised learning on visual data",
            "environment_description": "Non-specific; typically requires data organized by environments or augmentations representing interventions; not tied to a particular interactive simulator in the WMC paper's mention.",
            "handles_distractors": true,
            "distractor_handling_technique": "Uses regularization of contrastive/self-supervised objectives to prefer invariant features and penalize those that vary across environments/augmentations, thereby suppressing spurious features.",
            "spurious_signal_types": "Augmentation-variant or environment-variant features (non-causal style signals)",
            "detection_method": "Implicit via regularizer that measures variability of representations across environments/augmentations; features with high variability are discouraged.",
            "downweighting_method": "Regularization in the learning objective reduces the representational weight of non-invariant features.",
            "refutation_method": "Not formalized as hypothesis tests; relies on optimization to push representations toward invariant mechanisms and away from spurious ones.",
            "uses_active_learning": null,
            "inquiry_strategy": null,
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "Provides theoretical framing and practical regularizers linking contrastive/self-supervised learning to causal invariance, supporting the idea that invariance-based objectives can recover causal features under suitable data/augmentation regimes; this work is cited in WMC as conceptual motivation for invariant contrastive training.",
            "uuid": "e746.4"
        },
        {
            "name_short": "Contrastive (InfoNCE) in RL",
            "name_full": "Contrastive learning using InfoNCE (applied to RL; e.g., CURL/SimCLR/InfoNCE)",
            "brief_description": "Contrastive self-supervised representation learning (InfoNCE) used to make embeddings invariant to data augmentations; applied as an auxiliary loss in RL to obtain more robust visual representations.",
            "citation_title": "Representation learning with contrastive predictive coding",
            "mention_or_use": "use",
            "method_name": "InfoNCE-based contrastive learning (SimCLR/InfoNCE/CURL variants)",
            "method_description": "Constructs positive pairs by applying two correlated augmentations to the same observation and negatives from other samples; uses an InfoNCE objective to pull positives together and push negatives apart in embedding space. In RL this is often used as an auxiliary loss (CURL) or as an encoder training signal; MoCo-style momentum encoders and large negative dictionaries or in-batch negatives are common engineering choices.",
            "environment_name": "Applied in this paper to iGibson, Gibson, DMControl as auxiliary representation learning in a model-based RL pipeline (DreamerV2 backbone).",
            "environment_description": "Interactive simulators (embodied navigation and continuous control) where augmentations act as synthetic interventions on style features; environments are actively controlled by the agent.",
            "handles_distractors": true,
            "distractor_handling_technique": "By forcing invariance across augmentation-induced changes in style, contrastive learning reduces reliance on augmentation-variant (spurious) features; engineering choices (negative mining, momentum encoder) affect efficacy.",
            "spurious_signal_types": "Visual style/appearance variations (color, brightness, blur, cutout occlusions) that correlate spuriously with rewards/actions.",
            "detection_method": "Implicit: if a feature changes under augmentation it will not be consistent across positive pairs and thus will not be reinforced; no explicit detection test.",
            "downweighting_method": "Learning objective (InfoNCE) prevents features that vary across augmentations from contributing to stable positive similarity, thereby downweighting them in learned embeddings.",
            "refutation_method": "Empirical evaluation: compare downstream RL performance and OoD generalization with/without contrastive objective and with different auxiliary tasks; no formal refutation test.",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "When integrated into WMC with depth auxiliary and momentum key encoder, contrastive learning substantially improves OoD navigation success (see WMC performance above). Prior RL works cited (CURL, RAD) show contrastive or augmentation benefits for sample efficiency and robustness.",
            "performance_without_robustness": "In WMC without the depth auxiliary, contrastive loss collapses (representation learning fails) because the world model and controller are not optimized end-to-end; paper reports strong degradation when contrastive is used alone without depth supervision.",
            "has_ablation_study": true,
            "number_of_distractors": null,
            "key_findings": "Contrastive (InfoNCE) objectives can enforce invariance to augmentation-induced style changes and improve OoD robustness when combined with appropriate auxiliary tasks (depth) and engineering (momentum encoder); used alone in a decoupled model-based RL setup, it can collapse unless there is an intervention-invariant supervisory signal.",
            "uuid": "e746.5"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Causal inference by using invariant prediction: identification and confidence intervals",
            "rating": 2,
            "sanitized_title": "causal_inference_by_using_invariant_prediction_identification_and_confidence_intervals"
        },
        {
            "paper_title": "Invariant risk minimization",
            "rating": 2,
            "sanitized_title": "invariant_risk_minimization"
        },
        {
            "paper_title": "Invariant causal prediction for block MDPs",
            "rating": 2,
            "sanitized_title": "invariant_causal_prediction_for_block_mdps"
        },
        {
            "paper_title": "Representation learning via invariant causal mechanisms",
            "rating": 2,
            "sanitized_title": "representation_learning_via_invariant_causal_mechanisms"
        },
        {
            "paper_title": "Curl: Contrastive unsupervised representations for reinforcement learning",
            "rating": 2,
            "sanitized_title": "curl_contrastive_unsupervised_representations_for_reinforcement_learning"
        },
        {
            "paper_title": "A simple framework for contrastive learning of visual representations",
            "rating": 1,
            "sanitized_title": "a_simple_framework_for_contrastive_learning_of_visual_representations"
        },
        {
            "paper_title": "Momentum contrast for unsupervised visual representation learning",
            "rating": 1,
            "sanitized_title": "momentum_contrast_for_unsupervised_visual_representation_learning"
        }
    ],
    "cost": 0.015885999999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>CONTRASTIVE UNSUPERVISED LEARNING OF WORLD MODEL WITH INVARIANT CAUSAL FEATURES</p>
<p>Rudra P K Poudel 
Department of Engineering
Cambridge Research Laboratory Toshiba Europe Limited
University of Cambridge
UK, UK</p>
<p>Harit Pandya 
Department of Engineering
Cambridge Research Laboratory Toshiba Europe Limited
University of Cambridge
UK, UK</p>
<p>Roberto Cipolla 
Department of Engineering
Cambridge Research Laboratory Toshiba Europe Limited
University of Cambridge
UK, UK</p>
<p>CONTRASTIVE UNSUPERVISED LEARNING OF WORLD MODEL WITH INVARIANT CAUSAL FEATURES</p>
<p>In this paper we present a world model, which learns causal features using the invariance principle. In particular, we use contrastive unsupervised learning to learn the invariant causal features, which enforces invariance across augmentations of irrelevant parts or styles of the observation. The world-model-based reinforcement learning methods independently optimize representation learning and the policy. Thus naÃ¯ve contrastive loss implementation collapses due to a lack of supervisory signals to the representation learning module. We propose an intervention invariant auxiliary task to mitigate this issue. Specifically, we utilize depth prediction to explicitly enforce the invariance and use data augmentation as style intervention on the RGB observation space. Our design leverages unsupervised representation learning to learn the world model with invariant causal features. Our proposed method significantly outperforms current state-of-the-art model-based and modelfree reinforcement learning methods on out-of-distribution point navigation tasks on the iGibson dataset. Moreover, our proposed model excels at the sim-to-real transfer of our perception learning module. Finally, we evaluate our approach on the DeepMind control suite and enforce invariance only implicitly since depth is not available. Nevertheless, our proposed model performs on par with the stateof-the-art counterpart.</p>
<p>INTRODUCTION</p>
<p>An important branching point in reinforcement learning (RL) methods is whether the agent learns with or without a predictive environment model. In model-based methods, an explicit predictive model of the world is learned, enabling the agent to plan by thinking ahead (Silver et al., 2018;Ha &amp; Schmidhuber, 2018;Hafner et al., 2021). The alternative model-free methods do not learn the predictive model of the environment explicitly as the control policy is learned end-to-end. As a consequence, model-free methods learn a greedy state representation for the task at hand and do not consider future downstream tasks. Therefore, we consider model-based methods more attractive for continuous learning, out-of-distribution (OoD) generalization and sim-to-real transfer.</p>
<p>A model-based approach has to learn the model of the environment purely from experience, which poses several challenges. The main problem is the training bias in the model, which can be exploited by an agent and lead to poor performance during testing (Ha &amp; Schmidhuber, 2018). Further, model-based RL methods learn the representation using observation reconstruction loss, for example variational autoencoders (VAE) (Kingma &amp; Welling, 2014). The downside of such a state abstraction method is that it is not suited to separate relevant states from irrelevant ones, resulting in performance degradation of the control policy, even for slight task-irrelevant style changes. Hence, relevant state abstraction is essential for robust policy learning, which is the aim of this paper.</p>
<p>Causality is the study of learning cause and effect relationships. Learning causality in pixel-based control involves two tasks. The first is a causal variable abstraction from images, and the second is learning the causal structure. Causal inference uses graphical modelling (Lauritzen &amp; Spiegelhalter, 1988), structural equation modelling (Bollen, 1989), or counterfactuals (Dawid, 2000). Pearl (2009) provided an excellent overview of those methods. However, in complex visual control tasks the number of state variables involved is high, so inference of the underlining causal structure of the  model becomes intractable. Causal discovery using the invariance principle tries to overcome this issue and is therefore gaining attention in the literature (Peters et al., 2016;Arjovsky et al., 2020;Zhang et al., 2020;Mitrovic et al., 2021). Arjovsky et al. (2020) learns robust classifiers based on invariant causal associations between variables from different environments. Zhang et al. (2020) uses multiple environments to learn the invariant causal features using a common encoder. Here spurious or irrelevant features are learnt using environment specific encoders. However, these methods need multiple sources of environments with specific interventions or variations. In contrast, we propose using data augmentation as a source of intervention, where samples can come from as little as a single environment, and we use contrastive learning for invariant feature abstraction. Related to our work, Mitrovic et al. (2021) proposed a regularizer for self-supervised contrastive learning. On the other hand, we propose an intervention invariant auxiliary task for robust feature learning.</p>
<p>Model-based RL methods do not optimize feature learning and control policy end-to-end to prevent greedy feature learning. The aim is that such features will be more useful for various downstream tasks. Hence, state abstraction uses reward prediction, reconstruction loss or both (Ha &amp; Schmidhuber, 2018;Zhang et al., 2020;Hafner et al., 2021). On the other hand contrastive learning does not use the reconstruction of the inputs and applies the loss at the embedding space. Hence, we propose a causally invariant auxiliary task for invariant causal features learning. Specifically, we utilize depth predictions to extract the geometrical features needed for navigation, which are not dependent on the texture. Further, most computer vision data augmentations on RGB image space are invariant to depth. Finally, we emphasize that depth is not required for deployment, enabling wider applicability of the proposed model. Importantly, our setup allows us to use popular contrastive learning on model-based RL methods and improves the sample efficiency and the OoD generalization.</p>
<p>In summary, we propose a World Model with invariant Causal features (WMC), which can extract and predict the causal features ( Figure 1). Our WMC is verified on the point goal navigation task from Gibson (Xia et al., 2018) and iGibson 1.0 (Shen et al., 2021) as well as the DeepMind control suite (DMControl) (Tunyasuvunakool et al., 2020). Our main contributions are:</p>
<ol>
<li>
<p>Show that the world model can benefit from contrastive unsupervised representation learning.</p>
</li>
<li>
<p>Propose a world model with invariant causal features, which outperforms state-of-the-art RL models on out-of-distribution generalization and sim-to-real transfer of learned features.</p>
</li>
</ol>
<p>RELATED WORK</p>
<p>Unsupervised Representation Learning. Learning reusable feature representations from large unlabeled data has been an active research area. In the context of computer vision, one can leverage unlabeled images and videos to learn good intermediate representations, which can be useful for a wide variety of downstream tasks. A classic approach to unsupervised representation learning is clustering similar data together, for example, using K-means (Hartigan &amp; Wong, 1979). Recently, VAE (Kingma &amp; Welling, 2014) has been a preferred approach for representation learning in model-based RL. Since VAE does not make any additional consideration of downstream tasks, invariant representation learning with contrastive loss has shown more promising results (Anand et al., 2019;Laskin et al., 2020a). Further, building on the success of supervised deep learning, getting supervision from the data itself is a preferred approach of representation learning from the unlabelled data, which is known as self-supervised learning. Self-supervised learning formulates the learning as a supervised loss function. In image-based learning self-supervision can be formulated using different data augmentations, for example, image distortion and rotation (Dosovitskiy et al., 2014;Chen et al., 2020). We also use different data augmentation techniques to learn the invariant features using contrastive loss, which we explain below.</p>
<p>Contrastive Learning. Representation learning methods based on contrastive loss (Chopra et al., 2005) have recently achieved state-of-the-art performances. These methods use a contrastive loss to learn representations invariant to data augmentation (Chen et al., 2020;He et al., 2020). Given a list of input samples, contrastive loss forces samples from the same class to have similar embeddings and different ones for different classes. Since class labels are not available in the unsupervised setting, contrastive loss forces similar embedding for the augmented version of the same sample and different ones for different samples. In a contrastive learning setting, an augmented version of the same sample is known as a positive sample and different samples as a negative sample; these are also referred to as query and key samples, respectively. There are several ways of formulating the contrastive loss such as Siamese (Chopra et al., 2005), InfoNCE (van den Oord et al., 2018) and SimCLR (Chen et al., 2020). We chose InfoNCE (van den Oord et al., 2018) for our contrastive loss in this work.</p>
<p>Causal Inference using Invariant Prediction. Learning structured representations that captures the underlying causal mechanisms generating data is a central problem for robust machine learning systems (SchÃ¶lkopf et al., 2021). However, recovering the underlying causal structure of the environment from observational data without additional assumptions is a complex problem. A recent successful approach for causal discovery, in the context of unknown causal structure, is causal inference using invariant prediction (Peters et al., 2016). The invariance idea is closely linked to causality under the terms autonomy and modularity (Pearl, 2009) or stability (Dawid &amp; Didelez, 2010). Furthermore, it is well known that causal variables have an invariance property, and this group of methods try to exploit this fact for causal inference. Our proposed world model also exploits this fact to learn the causal state of the environment using the invariance principle, which is formalized using contrastive loss. Also, this view of self-supervised representation learning using invariant causal mechanisms is recently formalized by Mitrovic et al. (2021). In section 3, we explain how we utilized the data augmentation technique to learn the causal state of the environment.</p>
<p>Model-based RL. The human brain discovers the hidden causes underlying an observation. Those internal representations of the world influence how agents infer which actions will lead to a higher reward (Hamrick, 2019). An early instantiation of this idea was put forward by Sutton (Sutton, 1990), where future hallucination samples rolled out from the learned world model are used in addition to the agent's interactions for sample efficient learning. Recently, planning through the world model has been successfully demonstrated in the world model by Ha &amp; Schmidhuber (2018) and DreamerV2 by Hafner et al. (2021). In our work we propose to learn causal features to reduce the training biases and improve the sample efficiency further.</p>
<p>Sample Efficiency. Joint learning of auxiliary tasks with model-free RL makes them competitive with model-based RL in terms of sample efficiency. For example, the recently proposed modelfree RL method called CURL (Laskin et al., 2020a) added contrastive loss as an auxiliary task and outperformed the state-of-the-art model-based RL called Dreamer (Hafner et al., 2021). Also, two recent works using data augmentation for RL called RAD (Laskin et al., 2020b) and DrQ (Yarats et al., 2021) outperform CURL without using an auxiliary contrastive loss. These results warrant O C S a Figure 2: Observation is made of content (C), causal variables, and style (S), spurious variables. We want representation learning to extract the content variables only, i.e. true cause of the action.</p>
<p>that if an agent has access to a rich stream of data from the environments, an additional auxiliary loss is unnecessary, since directly optimizing the policy objective is better than optimizing multiple objectives. However, we do not have access to a rich stream of data for many interesting problems, hence sample efficiency still matters. Further, these papers do not consider the effect of data augmentation, auxiliary tasks and unsupervised representation learning for model-based RL, which is the main focus of this paper.</p>
<p>PROPOSED MODEL</p>
<p>The data flow diagram of our proposed world model is shown in Figure 1. We consider the visual control task as a finite-horizon partially observable Markov decision process (POMDP). We denote observation space, action space and time horizon as O, A and T respectively. An agent performs continuous actions a t âˆ¼ p(a t |o â‰¤t , a &lt;t ), and receives observations and scalar rewards o t , r t âˆ¼ p(o t , r t |o &lt;t , a &lt;t ) from the unknown environment. We use the deep neural networks for the state abstractions from observation s t âˆ¼ p(s t |s tâˆ’1 , a tâˆ’1 , o t ), predictive transition model s t âˆ¼ q(s t |s tâˆ’1 , a tâˆ’1 ) and reward model r t âˆ¼ q(r t |s t ). The goal of an agent is to maximize the expected total rewards E p ( T t=1 r t ). In the following sections we describe our proposed model in detail.</p>
<p>INVARIANT CAUSAL FEATURES LEARNING</p>
<p>Learning the pixel-based controller involves two main tasks, first abstraction of the environment state and second maximizing the expected total reward of the policy. Since visual control is a complex task, the number of causal variables involves are high, making the causal structure discovery a difficult task as it requires fitting the graphical model or structural equation. That is why we choose invariant prediction as our method of choice for causal feature learning. The key idea is that if we consider all causes of an effect, then the conditional distribution of the effect given the causes will not change when we change all the other remaining variables of the system (Peters et al., 2016). Such experimental changes are known as interventions in the causality literature. We have used data augmentation as our mechanism of intervention, since we do not have access to the causal or spurious variables or both of the environment. We have also explored texture randomization as an intervention in our experiments, which we call action replay.</p>
<p>We have shown the high-level idea of the proposed causal features extraction technique in Figure 2. The main idea is observation is made of content (C), causal variables, and style (S), spurious variables. We want our representation learning to extract the content variables only, which are the true cause of the action. In other words we want our control policy to learn P (a|c = invariant encoder(o)) but not P (a|(c, s) = encoder(o)) as causal variables are sample efficient, and robust to OoD generalization and sim-to-real transfer. We have chosen the contrastive learning technique (Chen et al., 2020;Grill et al., 2020) to learn the invariant causal features, which means embedding features of a sample and its data augmented version, intervention on style or spurious variables, should be the same. We use spatial jitter (crop and translation), Gaussian blur, color jitter, grayscale and cutout data augmentation techniques (Chen et al., 2020;Laskin et al., 2020b;Yarats et al., 2021) for contrastive learning. Since the intervention of the style is performed at the image level, we do not need to know the location of the interventions. Further, the data also can come from observation of the different environments with different environment-level interventions. The theoretical guarantee of causal inference using invariant prediction is discussed in Peters et al. (2016) and Mitrovic et al. (2021). However, our proposed method does not consider the hidden confounding variables that influence the target effect variable.</p>
<p>WORLD MODEL</p>
<p>The proposed World Model with invariant Causal features (WMC) consists of three main components: i) unsupervised causal representation learning, ii) memory, also know as the world model, and iii) controller. The representation learning module uses the contrastive learning for invariant causal state learning. The memory module uses a recurrent neural network. It outputs the parameters of a categorical distribution, discrete and multinomial distribution, which is used to sample the future states of the model. Finally, the controller maximizes the action probability using an actor critic approach (Williams, 1992;Hafner et al., 2021). We have adopted DreamerV2 (Hafner et al., 2021) to test our proposed hypothesis, which we describe below.</p>
<p>Unsupervised causal features learning. Invariant causal feature extraction from the RGB image observation is a key component of our model. As described previously, we learn invariant causal features by maximizing agreement between different style interventions of the same observation via a contrastive loss in the latent feature space. Since the world model optimizes feature learning and controller separately to learn better representative features for downstream tasks, our early result with only rewards prediction was poor, which we verified in our experiments. Another reason for separate training of the world model and controller is that most of the complexity resides in the world model (features extraction and memory), so that controller training with RL methods will be easier. Hence, we need a stronger loss function to learn a good state representation of the environment. That is why we propose depth reconstruction as an auxiliary task and do not use image resize data augmentation to keep the relation of object size and distance intact. We used InfoNCE (van den Oord et al., 2018) style loss to learn the invariant causal feature. Hence, our encoder takes RGB observation and task specific information as inputs and depth reconstruction and reward prediction as targets. The invariant state abstraction is enforced by contrastive loss. The proposed invariant causal features learning technique has the following three major components:</p>
<p>â€¢ A style intervention module that uses data augmentation techniques. We use spatial jitter, Gaussian blur, color jitter, grayscale and cutout data augmentation techniques for style intervention. Spatial jitter is implemented by first padding and then performing random crop. Given any observation o t , our style intervention module randomly transforms in two correlated views of the same observations. All the hyperparameters are provided in the appendix. â€¢ We use an encoder network that extracts representation from augmented observations. We follow the same configurations as DreamerV2 for a fair comparison, and only contrastive loss and depth reconstruction tasks are added. We obtains t = invariant encoder(o t ), then we follow the DreamerV2 to obtain the final state s t âˆ¼ p(s t |s tâˆ’1 , a tâˆ’1 ,s t ). We use the contrastive loss immediately after the encoders t . â€¢ Contrastive loss is defined for a contrastive prediction task, which can be explained as a differentiable dictionary lookup task. Given a query observation q and a set K = {k 0 , k 1 , ...k 2B } with known positive {k + } and negative {k âˆ’ } keys. The aim of contrastive loss is to learn a representation in which positive sample pairs stay close to each other while negative ones are far apart. In contrastive learning literature q, K, k + and k âˆ’ are also referred as anchors, targets, positive and negative samples. We use bilinear products for projection head and InfoNCE loss for contrastive learning (van den Oord et al., 2018), which enforces the desired similarity in the embedding space:
q t = log exp(q T W k + ) exp(q T W k + ) + 2(Bâˆ’1) i=0 exp(q T W k i )(1)
This loss function can be seen as the log-loss of a 2B-way softmax classifier whose label is k + . Where B is a batch size, which becomes 2B after the style intervention module randomly transforms in two correlated views of the same observation. The quality of features using contrastive loss depends on the quality of the negative sample mining, which is a difficult task in an unsupervised setting. We slice the sample observations from an episode at each 5th time-step to reduce the similarity between neighbouring negative observations. In summary, causal feature learning has the following component and is optimized by Equation 1.
Invariant causal model: p Î¸ (s t |o t )(2)
Future predictive memory model. The representation learning model extracts what the agent sees at each time frame but we also want our agent to remember the important events from the past. This is achieved with the memory model and implemented with a recurrent neural network. Further, the transition model learns to predict the future state using the current state and action in the latent space only, which enables future imagination without knowing the future observation since we can obtain the future action from the policy if we know the future state. Hence, this module is called a future predictive model and enables efficient latent imagination for planning (Ha &amp; Schmidhuber, 2018;Hafner et al., 2020). In summary, dynamic memory and representation learning modules are tightly integrated and have the following components,
Representation model: p Î¸ (s t |s tâˆ’1 , a tâˆ’1 ,s t ) Depth prediction model: q Î¸ (o d t |s t ) Reward model:
q Î¸ (r t |s t ) Predictive memory model: q Î¸ (s t |s tâˆ’1 , a tâˆ’1 ).</p>
<p>(3) All the world model and representation losses were optimized jointly, which includes contrastive, depth prediction, reward and future predictive KL regularizer losses respectively,
L W M = E p t q t + ln q(o d t |s t ) + ln q(r t |s t ) âˆ’ Î² KL t (4)
where, KL t = KL(p(s t |s tâˆ’1 , a tâˆ’1 ,s t )||q(s t |s tâˆ’1 , a tâˆ’1 ))</p>
<p>Controller. The objective of the controller is to optimize the expected rewards of the action, which is optimized using an actor critic approach. The actor critic approach considers the rewards beyond the horizon. Since we follow the DreamerV2, an action model and a value model are learnt in the imagined latent space of the world model. The action model implements a policy that aims to predict future actions that maximizes the total expected rewards in the imagined environment. Given H as the imagination horizon length, Î³ the discount factor for the future rewards, action and policy model are defined as follows:
Action model: q Ï† (a t |s t ) Value model: E q(Â·|sÏ„ ) t+H Ï„ =t Î³ Ï„ âˆ’t r Ï„ .(5)</p>
<p>IMPLEMENTATION DETAILS</p>
<p>We have used the publicly available code of DreamerV2 (Hafner et al., 2021) and added the contrastive loss on top of that. We have used default hyperparameters of the continuous control task.</p>
<p>Here, we have explained the necessary changes for our experiments. Following MoCo (He et al., 2020) and BYOL (Grill et al., 2020) we have used the moving average version of the query encoder to encode the keys K with a momentum value of 0.999. The contrastive loss is jointly optimized with the world model using Adam (Kingma &amp; Ba, 2014). To encode the task observations we used two dense layers of size 32 with ELU activations (Clevert et al., 2015). The features from RGB image observation and task observation are concatenated before sending to the representation module of the DreamerV2. Replay buffer capacity is 3e 5 for both 100k and 500k steps experiments. All architectural details and hyperparameters are provided in the appendix. Further, the training time of WMC is almost twice than that of DreamerV2 but inference time is the same.   </p>
<p>EXPERIMENTS</p>
<p>EVALUATION</p>
<p>We evaluate the out-of-distribution (OoD) generalization, sim-to-real transfer of perception learning and sample-efficiency of our model and baselines at 100k and 500k environment steps. Sample efficiency test on 100k and 500k steps is a common practice (Laskin et al., 2020a;b;Yarats et al., 2021;Hafner et al., 2021). Following (Hafner et al., 2021), we update the model parameters on every fifth interactive step. We used default hyperparameter values of DreamerV2 for our experiments. Similarly, we used official code for RAD and CURL experiments.</p>
<p>IGIBSON DATASET</p>
<p>We have tested our proposed WMC on a random PointGoal task from iGibson 1.0 environment (Shen et al., 2021) for OoD generalization. It contains 15 floor scenes with 108 rooms. The scenes are replicas of real-world homes with artist designed textures and materials. We have used RGB, depth and task related observation only. Depth is only used during the training phase. The task related observation includes goal location, current location, and linear and angular velocities of the robot. Action includes rotation in radians and forward distance in meters for the Turtlebot. Since iGibson 1.0 does not provide dataset splits for OoD generalization, we have chosen five scenes for training and tested on the held-out three scenes and visual textures both. The details are provided in the appendix.   We have trained all models three times with random seeds and report the average Success Rate (SR) and Success weighted by (normalized inverse) Path Length (SPL) on held-out scenes as well as visual textures in the Table 1. Our proposed WMC outperforms state-of-the-art model-based RL method DreamerV2 and model-free method RAD and CURL on 100k and 500k interactive steps. Even though depth reconstruction and data augmentation improve the DreamerV2, proposed invariant causal features learning with contrastive loss further improves the results. All methods perform poorly on the difficult Ihlen 1 int scene. Further, our experimental results confirm that, similar to the model-free RL methods (Laskin et al., 2020b), data augmentation improves the performance of the model-based RL.</p>
<p>IGIBSON-TO-GIBSON DATASET</p>
<p>We use the Gibson dataset (Xia et al., 2018) for sim-to-real transfer experiments of the perception module, representation learning module of the world model; however please note that the robot controller is still a part of the simulator. Gibson scenes are created by 3D scanning of the real scenes, and it uses a neural network to fill the pathological geometric and occlusion errors only. We have trained all models on the artist created textures of iGibson and tested on five scenes from the Gibson. The results are shown in Table 2. Our proposed WMC outperforms RAD and CURL on 100k and 500k interactive steps, which shows that WMC learns more stable features and is better suited for sim-to-real transfer.</p>
<p>DEEPMIND CONTROL SUITE</p>
<p>The results for the DMControl suite (Tunyasuvunakool et al., 2020) experiments are shown in Table  3. We replaced the depth reconstruction of WMC with the original RGB input reconstruction to adopt the DMControl suite. WMC achieved competitive results, and the key findings are: i) even though depth reconstruction is an important component to enforce the invariant causal features learning on WMC explicitly, the competitive results, even without depth reconstruction show the wider applicability of the proposed model; ii) WMC is competitive with CURL, the closest state-of-the-art RL method with contrastive learning.</p>
<p>ABLATION STUDY</p>
<p>In many real world environments, action replay with texture randomization is not possible to perform. Hence, we have also experimented WMC without action replay (AR) feature. The results are shown in Table 4. These experiments confirm that WMC learning with only standard self-supervised learning is better than just data augmentation. However, in 500k steps measure data augmentation technique on DreamerV2 with depth reconstruction is doing better in Rs int environment. This result suggests that model optimization with an additional constraint makes the optimization task harder. However, collecting more interactive data in the real environment is difficult in many scenarios. Since given the simulation to real gap is reducing every year (Shen et al., 2021) and WMC is performing better in 5 out of 6 results, an additional contrastive loss is still important for the modelbased RL. Further, this result warrants that we need a better method for intervention on image style or spurious variables. Since the separation of those variables from the pixel-observation is not an easy task, we do need some new contributions here, which we leave as future work.</p>
<p>The standard formulation of contrastive learning does not use reconstruction loss (Chen et al., 2020;Grill et al., 2020;Laskin et al., 2020a;He et al., 2020). Since model-based RL does not optimize the representation learning and controller jointly, contrastive loss collapses. Hence, to validate our proposal of style intervention at observation space and intervention invariant depth reconstruction as an auxiliary task, we have done experiments without depth reconstruction and action replay (WMC -AR -D). The results are shown in Table 4. The degradation in performance without depth reconstruction loss is much worse than only without action replay. In summary, in a reasonably complex pixel-based control task, WMC is not able to learn meaningful control. This shows the value of our careful design of the WMC using contrastive loss and depth reconstruction as an auxiliary task.</p>
<p>Further, WMC with RGB image reconstruction rather than depth (WMC -D + I) reduces the success rate by 50% and SPL by almost two third. These results confirm that our proposal of doing an intervention on RGB observation space and adding intervention invariant reconstruction of depth as an auxiliary task is one of the key contributions of our paper.</p>
<p>CONCLUSION</p>
<p>In this work we proposed a method to learn World Model with invariant Causal features (WMC). These invariant causal features are learnt by minimizing contrastive loss between content invariance interventions of the observation. Since the world model learns the representation learning and policy of the agent independently, without providing the better supervisory signal for the representation learning module, the contrastive loss collapses. Hence, we proposed depth reconstruction as an auxiliary task, which is invariant to the proposed data augmentation techniques. Further, given an intervention in the observation space, WMC can extract as well as predict the related causal features.</p>
<p>Our proposed WMC significantly outperforms the state-of-the-art models on out-of-distribution generalization, sim-to-real transfer of perception module and sample efficiency measures. Further, our method works on a sample-level intervention and does not need data from different environments to learn the invariant causal features.</p>
<p>A APPENDIX</p>
<p>A.1 QUALITATIVE RESULTS OF WMC Figure 3: The out-of-distribution generalization tests of proposed WMC on held-out scenes and visual textures from iGibson 1.0 environment. Green circle is a random PointGoal, blue circle is a random starting point and blue line represents the travel path of the Turtlebot robot.</p>
<p>A.2 HYPER PARAMETERS  Table 6: Train-test scenes splits for iGibsion 1.0 dataset (Shen et al., 2021).</p>
<p>Phase</p>
<p>Scene names Training Beechwood 0 int, Beechwood 1 int, Benevolence 0 int, Benevolence 1 int, Benevolence 2 int Merom 0 int, Merom 1 int, Pomaria 0 int, Pomaria 1 int, Pomaria 2 int, Wainscott 0 int, Wainscott 1 int Testing Ihlen 0 int, Ihlen 1 int, Rs int  </p>
<p>Figure 1 :
1Flow diagram of proposed World Model with invariant Causal features (WMC). It consists of three components: i) unsupervised causal representation learning, ii) memory, and iii) controller.</p>
<p>Table 1 :
1Experiment results on PointGoal navigation task from iGibson 1.0 dataset.Ihlen 0 int 
Ihlen 1 int 
Rs int 
Total 
Models 
Steps SR SPL SR SPL SR SPL 
SR 
SPL </p>
<p>RAD 
100k 
0.5 0.01 
0.1 0.00 
0.8 0.01 
0.5 0.01 
CURL 
100k 
8.0 0.07 
0.6 0.00 
5.4 0.05 
4.6 0.04 
DreamerV2 
100k 
1.7 0.01 
0.5 0.00 
1.6 0.01 
1.3 0.01 
DreamerV2 + DA 
100k 
7.2 0.05 
1.5 0.01 
7.7 0.05 
5.5 0.03 
DreamerV2 -I + D 
100k 
1.9 0.01 
0.9 0.00 
2.8 0.01 
1.9 0.01 
DreamerV2 -I + D + DA 100k 
8.3 0.05 
2.1 0.01 10.8 0.07 
7.0 0.04 
WMC 
100k 28.9 0.22 
7.9 0.05 30.2 0.22 
22.3 0.16 </p>
<p>RAD 
500k 48.7 0.44 11.6 0.11 48.5 0.44 
36.3 0.32 
CURL 
500k 40.8 0.36 11.4 0.09 41.9 0.36 
31.4 0.27 
DreamerV2 
500k 
1.3 0.01 
0.8 0.00 
2.2 0.01 
1.4 0.01 
DreamerV2 + DA 
500k 
7.2 0.04 
1.1 0.01 
9.7 0.05 
13.0 0.08 
DreamerV2 -I + D 
500k 
3.3 0.02 
0.9 0.00 
4.1 0.02 
2.7 0.01 
Dreamer -I + D + DA 
500k 38.0 0.25 
9.0 0.06 52.7 0.35 
33.2 0.22 
WMC 
500k 58.6 0.44 15.7 0.11 67.4 0.51 47.26 0.36 </p>
<p>Table 2 :
2iGibson-to-Gibson dataset: sim-to-real perception transfer results on navigation task. Steps SR SPL SR SPL SR SPL SR SPL SR SPLIhlen 
Muleshoe 
Uvalda 
Noxapater 
McDade 
Models </p>
<p>Table 3 :
3Experiment results on DMControl.100k Steps 
WMC 
CURL 
Dreamer SAC+AE 
PSAC 
SSAC </p>
<p>Finger, spin 
486Â±191 767Â±56 
341Â±70 
740Â±64 
179Â±66 811Â±46 
Cartpole, swingup 
472Â±67 582Â±146 
326Â±27 
311Â±11 
419Â±40 835Â±22 
Reacher, easy 
327Â±98 538Â±233 314Â±155 
274Â±14 
145Â±30 746Â±25 
Cheetah, run 
321Â±78 299 Â±48 235Â± 137 
267Â±24 
197Â±15 616Â±18 
Walker, walk 
654Â±100 
403Â±24 
277Â±12 
394Â±22 
42Â±12 891Â±82 
Ball in cup, catch 
830Â±118 
769Â±43 246Â±174 
391Â±82 
312Â±63 746Â±91 </p>
<p>500K step scores </p>
<p>Finger, spin 
471Â±173 
926Â±45 796Â±183 884Â±128 179Â±166 923Â±21 
Cartpole, swingup 
675Â±64 
841Â±45 
762Â±27 
735Â±63 
419Â±40 848Â±15 
Reacher, easy 
891Â±72 
929Â±44 793Â±164 
627Â±58 
145Â±30 923Â±24 
Cheetah, run 
633Â±70 
518Â±28 570Â±253 
550Â±34 
197Â±15 795Â±30 
Walker, walk 
965Â±4 
902Â±43 
897Â±49 
847Â±48 
42Â±12 948Â±54 
Ball in cup, catch 
950Â±20 
959Â±27 
879Â± 87 794Â± 58 312Â± 63 974Â±33 </p>
<p>Table 4 :
4Ablation study of proposed WMC.Ihlen 0 int 
Ihlen 1 int 
Rs int 
Total 
Models 
Steps SR SPL SR SPL SR SPL SR SPL </p>
<p>WMC 
100k 28.9 0.22 
7.9 0.05 30.2 0.22 22.3 0.16 
WMC -AR 
100k 15.4 0.</p>
<p>Table 5 :
5Hyper parameters of proposed WMC. A.3 IGIBSON 1.0 TRAINING AND EVALUATION SPLITSName 
Symbol 
Value </p>
<p>World Model </p>
<h2>Dataset size (FIFO)</h2>
<p>3 Â· 10 5 
iGibson input image size 
o 
120Ã—160 
Batch size 
B 
50 
Sequence length 
L 
50 
Discrete latent dimensions 
-
32 
Discrete latent classes 
-
32 
RSSM number of units 
-
1024 
KL loss scale 
Î² 
1.0 
World model learning rate 
-
3 Â· 10 âˆ’4 
Key encoder exponential moving average 
-
0.999 </p>
<p>Behavior </p>
<p>Imagination horizon 
H 
15 
Actor learning rate 
-
1 Â· 10 âˆ’4 
Critic learning rate 
-
1 Â· 10 âˆ’4 
Slow critic update interval 
-
100 </p>
<p>Common </p>
<h2>Policy steps per gradient step</h2>
<p>4 
Policy and reward MPL number of layers 
-
4 
Policy and reward MPL number of units 
-
400 
Gradient clipping 
-
100 
Adam epsilon 
10 âˆ’5 </p>
<p>Encoder and Decoder </p>
<h2>MLP encoder sizes of task obs</h2>
<p>32, 32 
Encoder kernels sizes 
-
4, 4, 4, 4, 4 
Decoder kernels sizes 
-
5, 5, 4, 5, 4 
Encoder and decoder feature maps 
-
32, 64, 128, 256, 512 
Encoder and decoder strides 
-
2, 2, 2, 2, 2 
Decoder padding 
-
none, 0-1, none, none, none </p>
<p>Data Augmentation </p>
<h2>Padding range</h2>
<p>10 
Hue delta 
-
0.1 
Brightness delta 
-
0.4 
Contrast delta 
-
0.4 
Saturation delta 
-
0.2 
Gaussina blur sigma min, max 
-
0.1, 2.0 
Cutout min, max 
-
30, 50 </p>
<p>Table 7 :
7iGibson 1.0 environment(Shen et al., 2021)  held out texture ids for test.Material category 
Held-out texture ids for test 
asphalt 
06, 15 
bricks 
08, 19 
concrete 
06, 15, 17 
fabric 
01, 02, 28 
fabric carpet 
02, 05, 13 
ground 
13, 19 
leather 
03, 12 
marble 
02, 03 
metal 
10, 19 
metal diamond plate 
04 
moss 
01, 03 
paint 
05 
paving stones 
24, 38 
planks 
07, 09, 16 
plaster 
03 
plastic 
04, 05 
porcelain 
02, 04 
rocks 
04 
terrazzo 
06, 08 
tiles 
43, 49 
wood 
02, 05, 16, 22, 32 
wood floor 
06, 10, 17, 28 </p>
<p>Table 8 :
8Examples of textures split for training and testing from the proposed split inTable 7.concrete 
Train </p>
<p>Test </p>
<p>fabric 
Train </p>
<p>Test </p>
<p>planks 
Train </p>
<p>Test </p>
<p>wood 
Train </p>
<p>Test </p>
<p>Unsupervised state representation learning in atari. Ankesh Anand, Evan Racah, Sherjil Ozair, Yoshua Bengio, Marc-Alexandre CÃ´tÃ©, R Devon Hjelm, Advances in Neural Information Processing Systems. 32Ankesh Anand, Evan Racah, Sherjil Ozair, Yoshua Bengio, Marc-Alexandre CÃ´tÃ©, and R Devon Hjelm. Unsupervised state representation learning in atari. In Advances in Neural Information Processing Systems, volume 32, 2019.</p>
<p>. Martin Arjovsky, LÃ©on Bottou, Ishaan Gulrajani, David Lopez-Paz, Invariant risk minimizationMartin Arjovsky, LÃ©on Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization, 2020.</p>
<p>Structural equations with latent variables. A Kenneth, Bollen, John Wiley &amp; Sons210Kenneth A Bollen. Structural equations with latent variables, volume 210. John Wiley &amp; Sons, 1989.</p>
<p>A simple framework for contrastive learning of visual representations. Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton, Proceedings of the 37th International Conference on Machine Learning. the 37th International Conference on Machine LearningTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In Proceedings of the 37th International Conference on Machine Learning, 2020.</p>
<p>Learning a similarity metric discriminatively, with application to face verification. S Chopra, R Hadsell, Y Lecun, IEEE Computer Society Conference on Computer Vision and Pattern Recognition. S. Chopra, R. Hadsell, and Y. LeCun. Learning a similarity metric discriminatively, with application to face verification. In IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2005.</p>
<p>Fast and accurate deep network learning by exponential linear units (elus). Djork-ArnÃ© Clevert, Thomas Unterthiner, Sepp Hochreiter, arXiv:1511.07289arXiv preprintDjork-ArnÃ© Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network learning by exponential linear units (elus). arXiv preprint arXiv:1511.07289, 2015.</p>
<p>Causal inference without counterfactuals. A P Dawid, Journal of the American Statistical Association. 95450A. P. Dawid. Causal inference without counterfactuals. Journal of the American Statistical Associ- ation, 95(450):407-424, 2000.</p>
<p>Identifying the consequences of dynamic treatment strategies: A decision-theoretic overview. A , Philip Dawid, Vanessa Didelez, Statistics Surveys. 4A. Philip Dawid and Vanessa Didelez. Identifying the consequences of dynamic treatment strategies: A decision-theoretic overview. Statistics Surveys, 4:184 -231, 2010.</p>
<p>Discriminative unsupervised feature learning with convolutional neural networks. Alexey Dosovitskiy, Jost Tobias Springenberg, Martin Riedmiller, Thomas Brox, Advances in Neural Information Processing Systems. Alexey Dosovitskiy, Jost Tobias Springenberg, Martin Riedmiller, and Thomas Brox. Discrimina- tive unsupervised feature learning with convolutional neural networks. In Advances in Neural Information Processing Systems, 2014.</p>
<p>Bootstrap your own latent -a new approach to self-supervised learning. Jean-Bastien Grill, Florian Strub, Florent AltchÃ©, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Remi Koray Kavukcuoglu, Michal Munos, Valko, Advances in Neural Information Processing Systems. Jean-Bastien Grill, Florian Strub, Florent AltchÃ©, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, Bilal Piot, koray kavukcuoglu, Remi Munos, and Michal Valko. Bootstrap your own latent -a new approach to self-supervised learning. In Advances in Neural Information Processing Systems, 2020.</p>
<p>Recurrent world models facilitate policy evolution. David Ha, JÃ¼rgen Schmidhuber, Advances in Neural Information Processing Systems. David Ha and JÃ¼rgen Schmidhuber. Recurrent world models facilitate policy evolution. In Advances in Neural Information Processing Systems, 2018.</p>
<p>Dream to control: Learning behaviors by latent imagination. Danijar Hafner, Timothy Lillicrap, Jimmy Ba, Mohammad Norouzi, International Conference on Learning Representations. Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning behaviors by latent imagination. In International Conference on Learning Representations, 2020.</p>
<p>Mastering atari with discrete world models. Danijar Hafner, P Timothy, Mohammad Lillicrap, Jimmy Norouzi, Ba, International Conference on Learning Representations. Danijar Hafner, Timothy P Lillicrap, Mohammad Norouzi, and Jimmy Ba. Mastering atari with discrete world models. In International Conference on Learning Representations, 2021.</p>
<p>Analogues of mental simulation and imagination in deep learning. B Jessica, Hamrick, Current Opinion in Behavioral Sciences. 29Jessica B Hamrick. Analogues of mental simulation and imagination in deep learning. Current Opinion in Behavioral Sciences, 29, 2019.</p>
<p>Algorithm as 136: A k-means clustering algorithm. J A Hartigan, M A Wong, 00359254Journal of the Royal Statistical Society. Series C (Applied Statistics). 28114679876J. A. Hartigan and M. A. Wong. Algorithm as 136: A k-means clustering algorithm. Journal of the Royal Statistical Society. Series C (Applied Statistics), 28(1):100-108, 1979. ISSN 00359254, 14679876.</p>
<p>Momentum contrast for unsupervised visual representation learning. Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, Ross Girshick, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)2020Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020.</p>
<p>Adam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, arXiv:1412.6980arXiv preprintDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.</p>
<p>Auto-encoding variational bayes. P Diederik, Max Kingma, Welling, Diederik P Kingma and Max Welling. Auto-encoding variational bayes, 2014.</p>
<p>Curl: Contrastive unsupervised representations for reinforcement learning. Michael Laskin, Aravind Srinivas, Pieter Abbeel, arXiv:2004.04136Proceedings of the 37th International Conference on Machine Learning. the 37th International Conference on Machine LearningVienna, AustriaMichael Laskin, Aravind Srinivas, and Pieter Abbeel. Curl: Contrastive unsupervised representa- tions for reinforcement learning. Proceedings of the 37th International Conference on Machine Learning, Vienna, Austria, PMLR 119, 2020a. arXiv:2004.04136.</p>
<p>Reinforcement learning with augmented data. Misha Laskin, Kimin Lee, Adam Stooke, Lerrel Pinto, Pieter Abbeel, Aravind Srinivas, Advances in Neural Information Processing Systems. Misha Laskin, Kimin Lee, Adam Stooke, Lerrel Pinto, Pieter Abbeel, and Aravind Srinivas. Rein- forcement learning with augmented data. In Advances in Neural Information Processing Systems, 2020b.</p>
<p>Local computations with probabilities on graphical structures and their application to expert systems. S L Lauritzen, D J Spiegelhalter, Journal of the Royal Statistical Society: Series B (Methodological). 502S. L. Lauritzen and D. J. Spiegelhalter. Local computations with probabilities on graphical struc- tures and their application to expert systems. Journal of the Royal Statistical Society: Series B (Methodological), 50(2):157-194, 1988.</p>
<p>Representation learning via invariant causal mechanisms. Jovana Mitrovic, Brian Mcwilliams, C Jacob, Lars Holger Walker, Charles Buesing, Blundell, International Conference on Learning Representations. Jovana Mitrovic, Brian McWilliams, Jacob C Walker, Lars Holger Buesing, and Charles Blundell. Representation learning via invariant causal mechanisms. In International Conference on Learn- ing Representations, 2021.</p>
<p>Causality: Models, Reasoning, and Inference. Judea Pearl, Cambridge University Press2 editionJudea Pearl. Causality: Models, Reasoning, and Inference. Cambridge University Press, 2 edition, 2009.</p>
<p>Causal inference by using invariant prediction: identification and confidence intervals. Jonas Peters, Peter BÃ¼hlmann, Nicolai Meinshausen, Journal of the Royal Statistical Society: Series B (Statistical Methodology). 785Jonas Peters, Peter BÃ¼hlmann, and Nicolai Meinshausen. Causal inference by using invariant pre- diction: identification and confidence intervals. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 78(5):947-1012, 2016.</p>
<p>Toward causal representation learning. Bernhard SchÃ¶lkopf, Francesco Locatello, Stefan Bauer, Nan Rosemary Ke, Nal Kalchbrenner, Anirudh Goyal, Yoshua Bengio, Proceedings of the IEEE. 1095Bernhard SchÃ¶lkopf, Francesco Locatello, Stefan Bauer, Nan Rosemary Ke, Nal Kalchbrenner, Anirudh Goyal, and Yoshua Bengio. Toward causal representation learning. Proceedings of the IEEE, 109(5):612-634, 2021.</p>
<p>0: A simulation environment for interactive tasks in large realistic scenes. Bokui Shen, Fei Xia, Chengshu Li, Roberto MartÃ­n-MartÃ­n, Linxi Fan, Guanzhi Wang, Claudia PÃ©rez-D&apos;arpino, Shyamal Buch, Sanjana Srivastava, Lyne Tchapmi, Micael Tchapmi, Kent Vainio, Josiah Wong, Li Fei-Fei, Silvio Savarese, IEEE International Conference on Intelligent Robots and Systems (IROS). 2021igibson 1.Bokui Shen, Fei Xia, Chengshu Li, Roberto MartÃ­n-MartÃ­n, Linxi Fan, Guanzhi Wang, Clau- dia PÃ©rez-D'Arpino, Shyamal Buch, Sanjana Srivastava, Lyne Tchapmi, Micael Tchapmi, Kent Vainio, Josiah Wong, Li Fei-Fei, and Silvio Savarese. igibson 1.0: A simulation environment for interactive tasks in large realistic scenes. In IEEE International Conference on Intelligent Robots and Systems (IROS), 2021.</p>
<p>Karen Simonyan, and Demis Hassabis. A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play. David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, Timothy Lillicrap, 0036-8075Science. 3626419David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, Timothy Lillicrap, Karen Si- monyan, and Demis Hassabis. A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play. Science, 362(6419):1140-1144, 2018. ISSN 0036-8075.</p>
<p>Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. S Richard, Sutton, Machine learning proceedings. ElsevierRichard S Sutton. Integrated architectures for learning, planning, and reacting based on approxi- mating dynamic programming. In Machine learning proceedings 1990, pp. 216-224. Elsevier, 1990.</p>
<p>Saran Tunyasuvunakool, Alistair Muldal, Yotam Doron, Siqi Liu, Steven Bohez, Josh Merel, Tom Erez, Timothy Lillicrap, Nicolas Heess, and Yuval Tassa. dm control: Software and tasks for continuous control. Software Impacts. 6Saran Tunyasuvunakool, Alistair Muldal, Yotam Doron, Siqi Liu, Steven Bohez, Josh Merel, Tom Erez, Timothy Lillicrap, Nicolas Heess, and Yuval Tassa. dm control: Software and tasks for continuous control. Software Impacts, 6, 2020.</p>
<p>AÃ¤ron Van Den Oord, Yazhe Li, Oriol Vinyals, arXiv:1807.03748Representation learning with contrastive predictive coding. AÃ¤ron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predic- tive coding. arXiv:1807.03748, 2018.</p>
<p>Simple statistical gradient-following algorithms for connectionist reinforcement learning. J Ronald, Williams, Machine learning. 83Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8(3):229-256, 1992.</p>
<p>Gibson env: real-world perception for embodied agents. Fei Xia, R Amir, Zhi-Yang Zamir, Alexander He, Jitendra Sax, Silvio Malik, Savarese, IEEE Conference on Computer Vision and Pattern Recognition (CVPR. IEEEFei Xia, Amir R. Zamir, Zhi-Yang He, Alexander Sax, Jitendra Malik, and Silvio Savarese. Gibson env: real-world perception for embodied agents. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2018.</p>
<p>Image augmentation is all you need: Regularizing deep reinforcement learning from pixels. Denis Yarats, Ilya Kostrikov, Rob Fergus, International Conference on Learning Representations. Denis Yarats, Ilya Kostrikov, and Rob Fergus. Image augmentation is all you need: Regularizing deep reinforcement learning from pixels. In International Conference on Learning Representa- tions, 2021.</p>
<p>Invariant causal prediction for block MDPs. Amy Zhang, Clare Lyle, Shagun Sodhani, Angelos Filos, Marta Kwiatkowska, Joelle Pineau, Yarin Gal, Doina Precup, Proceedings of the 37th International Conference on Machine Learning. the 37th International Conference on Machine LearningAmy Zhang, Clare Lyle, Shagun Sodhani, Angelos Filos, Marta Kwiatkowska, Joelle Pineau, Yarin Gal, and Doina Precup. Invariant causal prediction for block MDPs. In Proceedings of the 37th International Conference on Machine Learning, 2020.</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2396 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2396</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2396</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-64.html">extraction-schema-64</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <p><strong>Paper ID:</strong> paper-0acd117521ef5aafb09fed02ab415523b330b058</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/0acd117521ef5aafb09fed02ab415523b330b058" target="_blank">Data-driven discovery of partial differential equations</a></p>
                <p><strong>Paper Venue:</strong> Science Advances</p>
                <p><strong>Paper TL;DR:</strong> The sparse regression method is computationally efficient, robust, and demonstrated to work on a variety of canonical problems spanning a number of scientific domains including Navier-Stokes, the quantum harmonic oscillator, and the diffusion equation.</p>
                <p><strong>Paper Abstract:</strong> Researchers propose sparse regression for identifying governing partial differential equations for spatiotemporal systems. We propose a sparse regression method capable of discovering the governing partial differential equation(s) of a given system by time series measurements in the spatial domain. The regression framework relies on sparsity-promoting techniques to select the nonlinear and partial derivative terms of the governing equations that most accurately represent the data, bypassing a combinatorially large search through all possible candidate models. The method balances model complexity and regression accuracy by selecting a parsimonious model via Pareto analysis. Time series measurements can be made in an Eulerian framework, where the sensors are fixed spatially, or in a Lagrangian framework, where the sensors move with the dynamics. The method is computationally efficient, robust, and demonstrated to work on a variety of canonical problems spanning a number of scientific domains including Navier-Stokes, the quantum harmonic oscillator, and the diffusion equation. Moreover, the method is capable of disambiguating between potentially nonunique dynamical terms by using multiple time series taken with different initial data. Thus, for a traveling wave, the method can distinguish between a linear wave equation and the Korteweg–de Vries equation, for instance. The method provides a promising new technique for discovering governing equations and physical laws in parameterized spatiotemporal systems, where first-principles derivations are intractable.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2396.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2396.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PDE-FIND</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PDE functional identification of nonlinear dynamics (PDE-FIND)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An algorithm that automatically proposes governing partial differential equations from spatio-temporal data by constructing a large library of candidate terms and selecting a sparse subset that explains time-derivatives via sparse regression.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>PDE-FIND</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Constructs a library Θ(U,Q) of candidate linear, nonlinear and derivative terms evaluated at data gridpoints and fits U_t ≈ Θ ξ. It treats the nonzero entries of the sparse coefficient vector ξ as a hypothesis for the active terms of the governing PDE. Hypothesis generation proceeds by (1) numerically differentiating data to form Θ and U_t, (2) performing sparsity-promoting regression (STRidge/ridge+hard-thresholding) to identify a sparse ξ, and (3) synthesizing selected terms into a candidate PDE. The method automates search over combinatorially many candidate PDEs by preferring parsimonious (sparse) models that fit the data.</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>applied mathematics / computational physics / data-driven discovery</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>open-ended discovery of governing equations from observational data</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td>Not described as 'novelty' explicitly; model parsimony is quantified by the number of nonzero coefficients (||ξ||_0) and model complexity. Model accuracy is quantified by the residual L2 norm ||Θ ξ − U_t||_2^2 and an L2 ridge penalty. Pareto analysis between complexity (sparsity) and accuracy is used to select models.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td>Feasibility of a candidate model is implicitly measured by regression fit (residual L2 norm), stability/conditioning of the regression (condition number κ(Θ) used to scale regularization), and robustness to noise (tested in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td>The paper explicitly frames selection as a trade-off between model complexity (sparsity, ||ξ||_0) and regression accuracy (residual ||Θ ξ − U_t||_2^2), and uses Pareto analysis and an explicit sparsity penalty to select models on the Pareto front.</td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td>Sparsity-promoting regularized regression: sequential thresholded ridge regression (STRidge) — iterative ridge regression with hard thresholding of small coefficients, refinement of tolerance, plus penalization of ||ξ||_0 and an L2 ridge term (λ||ξ||_2^2); stronger regularization for ill-conditioned Θ via κ(Θ). Pareto analysis is used to choose final model complexity vs. accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td>Compared conceptually against prior automated discovery methods such as evolutionary symbolic regression (Schmidt & Lipson, Bongard & Lipson) and ℓ1 (LASSO) convex relaxation; also contrasted with POD+Galerkin approaches for PDEs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Paper states ℓ1 regularization tends to perform poorly on highly correlated data and motivates STRidge; quantitative comparative numbers against baselines are not provided, but empirical success across canonical PDE examples is shown (Table I).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td>For PDE identification, multiple experiments/initial conditions may be required to disambiguate competing hypotheses (e.g., a single KdV soliton mimics a one-way wave equation; multiple-amplitude data resolves nonlinearity). The method also requires careful numerical differentiation and may use SVD-based filtering for noisy data.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Data-driven discovery of partial differential equations', 'publication_date_yy_mm': '2016-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2396.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2396.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>STRidge</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sequential Threshold Ridge Regression (STRidge)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sequential procedure introduced in this paper that approximates sparse regression by alternating ridge regression and hard thresholding to identify a sparse coefficient vector ξ explaining U_t ≈ Θ ξ.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>STRidge</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>An algorithmic solver for the sparse regression subproblem: perform ridge regression (minimize ||Θ ξ − U_t||_2^2 + λ||ξ||_2^2), threshold small coefficients below a tolerance to zero (hard thresholding), then re-solve ridge regression restricted to the remaining (large) coefficients recursively with refined tolerance/iterations. Used to produce sparse hypotheses (nonzero terms) for PDE models.</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>statistical learning / regression for model discovery</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>sparse model selection (parsimonious hypothesis generation)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td>Not framed as 'novelty'; selects sparse models via thresholded coefficient magnitude (|ξ_j| ≥ tol) and uses ||ξ||_0 penalization implicitly in selection criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td>Feasibility judged by regression residual (L2 error) and stability via ridge regularization; condition number κ(Θ) informs regularization strength.</td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td>STRidge operationalizes the accuracy-vs-complexity tradeoff by thresholding small coefficients (reducing complexity) while minimizing residual via ridge regression (preserving accuracy); the paper links this procedure to choosing models on a Pareto front.</td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td>Iterative ridge regression + hard thresholding; refinement of tolerance and number of iterations to balance fit and sparsity; explicit additional penalty term ε κ(Θ) ||ξ||_0 is mentioned in the objective (equation (3)).</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td>Compared conceptually to ℓ1 (LASSO) convex relaxation and to evolutionary symbolic regression approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Paper argues STRidge performs better than ℓ1 in highly correlated settings, but provides no numerical benchmark comparisons in the text.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td>STRidge is used successfully across multiple PDE examples in the paper, demonstrating robustness to subsampling and some levels of noise when combined with careful derivative estimation and SVD filtering.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Data-driven discovery of partial differential equations', 'publication_date_yy_mm': '2016-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2396.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2396.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sparsity-promoting regression</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sparsity-promoting regression techniques (ℓ0/ℓ1/regularized methods)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A class of regression techniques that favor solutions with few nonzero coefficients to produce parsimonious models; used here to select a small number of active PDE terms from a large candidate library.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Sparsity-promoting regression</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Construct a large candidate feature/library matrix Θ, then solve a regularized regression that includes a sparsity-promoting term (ℓ0 or ℓ1) or uses thresholding approaches to produce sparse ξ. In this paper, sparsity is enforced via STRidge and an explicit ||ξ||_0 penalty in the selection criterion; ℓ1 (LASSO) is discussed as an alternative but noted to perform poorly with highly correlated features.</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>machine learning / system identification</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>sparse hypothesis selection from large candidate spaces</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td>Not described as 'novelty'; model parsimony (number of active terms, ||ξ||_0) serves as the complexity metric in selection.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td>Feasibility is inferred from data fit (residual L2 norm) and conditioning; regularization parameters (λ, tol) tune feasibility/robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td>Paper explicitly frames a trade-off: penalize complexity (sparsity) to avoid overfitting while maintaining low regression residuals; selection is interpreted as choosing a point on a Pareto front between complexity and accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td>Use of regularization (λ||ξ||_2^2), explicit sparsity penalty (||ξ||_0) or convex ℓ1 relaxations, and algorithmic approaches like STRidge to balance fit vs sparsity.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td>ℓ1 (LASSO) convex relaxation is discussed as a common baseline; evolutionary symbolic regression is discussed historically.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>ℓ1 methods are said to 'tend to perform poorly with highly correlated data' motivating the STRidge approach; no numerical cross-method benchmarking is supplied in-text.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td>Sparsity-promoting techniques are effective for discovering PDE structure when the true right-hand side is sparse in the chosen feature library; practical success depends on careful derivative estimation and choice of library terms.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Data-driven discovery of partial differential equations', 'publication_date_yy_mm': '2016-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2396.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2396.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Evolutionary symbolic regression</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Evolutionary symbolic regression (genetic programming approaches)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Methods that search the space of symbolic expressions (equations) via evolutionary algorithms to identify governing equations from data; historically capable of finding nonlinear dynamical systems but computationally expensive.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Evolutionary symbolic regression</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Uses evolutionary search (mutation, crossover, selection) over symbolic expression trees to propose candidate mathematical models (equations) and evaluates them against data to evolve better-fitting, typically parsimonious expressions. Cited as prior art (e.g., Bongard & Lipson; Schmidt & Lipson) for automated equation discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>automated model discovery / symbolic regression</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>open-ended discovery of governing equations</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td>Evolutionary search with fitness functions that typically balance fit and complexity (e.g., via parsimony pressure), though specific strategies are not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td>Presented as historical baseline for automated equation discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Paper notes these methods were capable of determining nonlinear dynamical systems but are more computationally expensive; no quantitative comparison provided.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Data-driven discovery of partial differential equations', 'publication_date_yy_mm': '2016-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2396.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2396.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SINDy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sparse Identification of Nonlinear Dynamics (SINDy)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A previously-developed sparse regression framework for discovering governing ordinary differential equations (ODEs) from time series data by selecting sparse terms from a candidate library.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SINDy</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Construct a candidate library Θ of functions of state variables and fit time-derivatives via sparse regression to identify sparse coefficient vector ξ representing active terms in ODEs. The present paper generalizes this approach to include spatial derivatives and thus PDE discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>dynamical systems / data-driven discovery</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>identification of ODE models (prior work) and inspiration for PDE extension</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td>Sparse regression (ℓ1/thresholding) to balance model accuracy and sparsity; referenced as prior technique.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td>Referenced as prior method for ODE identification; PDE-FIND is presented as a generalization for PDEs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Paper contrasts PDE-FIND with prior SINDy workflows that relied on first performing dimensionality reduction (POD) and then identifying ODEs on modal coefficients; PDE-FIND directly identifies PDE operators without that intermediate step.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Data-driven discovery of partial differential equations', 'publication_date_yy_mm': '2016-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2396.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2396.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pareto analysis (model selection)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pareto analysis for accuracy vs complexity</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of Pareto front analysis to balance regression accuracy against model complexity (sparsity) when selecting discovered PDE models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Pareto analysis (accuracy vs complexity)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Constructs a set of candidate models across different sparsity/regularization levels and selects models lying on the Pareto front that offer the best trade-off between low residual error (accuracy) and low complexity (few nonzero terms). The method is used conceptually and practically to avoid overfitting and pick parsimonious governing equations.</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>model selection / scientific model discovery</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>trade-off optimization between fit and simplicity</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td>Model complexity measured by sparsity (||ξ||_0) and magnitude of coefficients; novelty per se is not explicitly defined.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td>Model feasibility measured by residual error (||Θ ξ − U_t||_2^2) and numeric conditioning; Pareto front balances these.</td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td>Paper explicitly states selection via Pareto analysis; also includes adding a sparsity penalty (||ξ||_0) to the objective to discourage overfitting and select models at an optimal Pareto position.</td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td>Multi-objective viewpoint (accuracy vs complexity) realized via regularization parameters, explicit sparsity penalty and iterative thresholding to produce models at different points on the Pareto curve for selection.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td>Pareto selection helps in choosing physically parsimonious PDEs across a range of canonical examples; the paper uses it to explain selection preference for simpler one-way wave equation unless additional data (multiple amplitudes) is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Data-driven discovery of partial differential equations', 'publication_date_yy_mm': '2016-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Distilling Free-Form Natural Laws from Experimental Data <em>(Rating: 2)</em></li>
                <li>Automated reverse engineering of nonlinear dynamical systems <em>(Rating: 1)</em></li>
                <li>Discovering governing equations from data: Sparse identification of nonlinear dynamical systems <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2396",
    "paper_id": "paper-0acd117521ef5aafb09fed02ab415523b330b058",
    "extraction_schema_id": "extraction-schema-64",
    "extracted_data": [
        {
            "name_short": "PDE-FIND",
            "name_full": "PDE functional identification of nonlinear dynamics (PDE-FIND)",
            "brief_description": "An algorithm that automatically proposes governing partial differential equations from spatio-temporal data by constructing a large library of candidate terms and selecting a sparse subset that explains time-derivatives via sparse regression.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "PDE-FIND",
            "system_description": "Constructs a library Θ(U,Q) of candidate linear, nonlinear and derivative terms evaluated at data gridpoints and fits U_t ≈ Θ ξ. It treats the nonzero entries of the sparse coefficient vector ξ as a hypothesis for the active terms of the governing PDE. Hypothesis generation proceeds by (1) numerically differentiating data to form Θ and U_t, (2) performing sparsity-promoting regression (STRidge/ridge+hard-thresholding) to identify a sparse ξ, and (3) synthesizing selected terms into a candidate PDE. The method automates search over combinatorially many candidate PDEs by preferring parsimonious (sparse) models that fit the data.",
            "research_domain": "applied mathematics / computational physics / data-driven discovery",
            "problem_type": "open-ended discovery of governing equations from observational data",
            "novelty_metric": "Not described as 'novelty' explicitly; model parsimony is quantified by the number of nonzero coefficients (||ξ||_0) and model complexity. Model accuracy is quantified by the residual L2 norm ||Θ ξ − U_t||_2^2 and an L2 ridge penalty. Pareto analysis between complexity (sparsity) and accuracy is used to select models.",
            "novelty_score": null,
            "feasibility_metric": "Feasibility of a candidate model is implicitly measured by regression fit (residual L2 norm), stability/conditioning of the regression (condition number κ(Θ) used to scale regularization), and robustness to noise (tested in experiments).",
            "feasibility_score": null,
            "tradeoff_evidence": "The paper explicitly frames selection as a trade-off between model complexity (sparsity, ||ξ||_0) and regression accuracy (residual ||Θ ξ − U_t||_2^2), and uses Pareto analysis and an explicit sparsity penalty to select models on the Pareto front.",
            "optimization_strategy": "Sparsity-promoting regularized regression: sequential thresholded ridge regression (STRidge) — iterative ridge regression with hard thresholding of small coefficients, refinement of tolerance, plus penalization of ||ξ||_0 and an L2 ridge term (λ||ξ||_2^2); stronger regularization for ill-conditioned Θ via κ(Θ). Pareto analysis is used to choose final model complexity vs. accuracy.",
            "human_evaluation": false,
            "human_evaluation_results": null,
            "comparative_baseline": "Compared conceptually against prior automated discovery methods such as evolutionary symbolic regression (Schmidt & Lipson, Bongard & Lipson) and ℓ1 (LASSO) convex relaxation; also contrasted with POD+Galerkin approaches for PDEs.",
            "comparative_results": "Paper states ℓ1 regularization tends to perform poorly on highly correlated data and motivates STRidge; quantitative comparative numbers against baselines are not provided, but empirical success across canonical PDE examples is shown (Table I).",
            "domain_specific_findings": "For PDE identification, multiple experiments/initial conditions may be required to disambiguate competing hypotheses (e.g., a single KdV soliton mimics a one-way wave equation; multiple-amplitude data resolves nonlinearity). The method also requires careful numerical differentiation and may use SVD-based filtering for noisy data.",
            "uuid": "e2396.0",
            "source_info": {
                "paper_title": "Data-driven discovery of partial differential equations",
                "publication_date_yy_mm": "2016-09"
            }
        },
        {
            "name_short": "STRidge",
            "name_full": "Sequential Threshold Ridge Regression (STRidge)",
            "brief_description": "A sequential procedure introduced in this paper that approximates sparse regression by alternating ridge regression and hard thresholding to identify a sparse coefficient vector ξ explaining U_t ≈ Θ ξ.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "STRidge",
            "system_description": "An algorithmic solver for the sparse regression subproblem: perform ridge regression (minimize ||Θ ξ − U_t||_2^2 + λ||ξ||_2^2), threshold small coefficients below a tolerance to zero (hard thresholding), then re-solve ridge regression restricted to the remaining (large) coefficients recursively with refined tolerance/iterations. Used to produce sparse hypotheses (nonzero terms) for PDE models.",
            "research_domain": "statistical learning / regression for model discovery",
            "problem_type": "sparse model selection (parsimonious hypothesis generation)",
            "novelty_metric": "Not framed as 'novelty'; selects sparse models via thresholded coefficient magnitude (|ξ_j| ≥ tol) and uses ||ξ||_0 penalization implicitly in selection criteria.",
            "novelty_score": null,
            "feasibility_metric": "Feasibility judged by regression residual (L2 error) and stability via ridge regularization; condition number κ(Θ) informs regularization strength.",
            "feasibility_score": null,
            "tradeoff_evidence": "STRidge operationalizes the accuracy-vs-complexity tradeoff by thresholding small coefficients (reducing complexity) while minimizing residual via ridge regression (preserving accuracy); the paper links this procedure to choosing models on a Pareto front.",
            "optimization_strategy": "Iterative ridge regression + hard thresholding; refinement of tolerance and number of iterations to balance fit and sparsity; explicit additional penalty term ε κ(Θ) ||ξ||_0 is mentioned in the objective (equation (3)).",
            "human_evaluation": false,
            "human_evaluation_results": null,
            "comparative_baseline": "Compared conceptually to ℓ1 (LASSO) convex relaxation and to evolutionary symbolic regression approaches.",
            "comparative_results": "Paper argues STRidge performs better than ℓ1 in highly correlated settings, but provides no numerical benchmark comparisons in the text.",
            "domain_specific_findings": "STRidge is used successfully across multiple PDE examples in the paper, demonstrating robustness to subsampling and some levels of noise when combined with careful derivative estimation and SVD filtering.",
            "uuid": "e2396.1",
            "source_info": {
                "paper_title": "Data-driven discovery of partial differential equations",
                "publication_date_yy_mm": "2016-09"
            }
        },
        {
            "name_short": "Sparsity-promoting regression",
            "name_full": "Sparsity-promoting regression techniques (ℓ0/ℓ1/regularized methods)",
            "brief_description": "A class of regression techniques that favor solutions with few nonzero coefficients to produce parsimonious models; used here to select a small number of active PDE terms from a large candidate library.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Sparsity-promoting regression",
            "system_description": "Construct a large candidate feature/library matrix Θ, then solve a regularized regression that includes a sparsity-promoting term (ℓ0 or ℓ1) or uses thresholding approaches to produce sparse ξ. In this paper, sparsity is enforced via STRidge and an explicit ||ξ||_0 penalty in the selection criterion; ℓ1 (LASSO) is discussed as an alternative but noted to perform poorly with highly correlated features.",
            "research_domain": "machine learning / system identification",
            "problem_type": "sparse hypothesis selection from large candidate spaces",
            "novelty_metric": "Not described as 'novelty'; model parsimony (number of active terms, ||ξ||_0) serves as the complexity metric in selection.",
            "novelty_score": null,
            "feasibility_metric": "Feasibility is inferred from data fit (residual L2 norm) and conditioning; regularization parameters (λ, tol) tune feasibility/robustness.",
            "feasibility_score": null,
            "tradeoff_evidence": "Paper explicitly frames a trade-off: penalize complexity (sparsity) to avoid overfitting while maintaining low regression residuals; selection is interpreted as choosing a point on a Pareto front between complexity and accuracy.",
            "optimization_strategy": "Use of regularization (λ||ξ||_2^2), explicit sparsity penalty (||ξ||_0) or convex ℓ1 relaxations, and algorithmic approaches like STRidge to balance fit vs sparsity.",
            "human_evaluation": false,
            "human_evaluation_results": null,
            "comparative_baseline": "ℓ1 (LASSO) convex relaxation is discussed as a common baseline; evolutionary symbolic regression is discussed historically.",
            "comparative_results": "ℓ1 methods are said to 'tend to perform poorly with highly correlated data' motivating the STRidge approach; no numerical cross-method benchmarking is supplied in-text.",
            "domain_specific_findings": "Sparsity-promoting techniques are effective for discovering PDE structure when the true right-hand side is sparse in the chosen feature library; practical success depends on careful derivative estimation and choice of library terms.",
            "uuid": "e2396.2",
            "source_info": {
                "paper_title": "Data-driven discovery of partial differential equations",
                "publication_date_yy_mm": "2016-09"
            }
        },
        {
            "name_short": "Evolutionary symbolic regression",
            "name_full": "Evolutionary symbolic regression (genetic programming approaches)",
            "brief_description": "Methods that search the space of symbolic expressions (equations) via evolutionary algorithms to identify governing equations from data; historically capable of finding nonlinear dynamical systems but computationally expensive.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Evolutionary symbolic regression",
            "system_description": "Uses evolutionary search (mutation, crossover, selection) over symbolic expression trees to propose candidate mathematical models (equations) and evaluates them against data to evolve better-fitting, typically parsimonious expressions. Cited as prior art (e.g., Bongard & Lipson; Schmidt & Lipson) for automated equation discovery.",
            "research_domain": "automated model discovery / symbolic regression",
            "problem_type": "open-ended discovery of governing equations",
            "novelty_metric": null,
            "novelty_score": null,
            "feasibility_metric": null,
            "feasibility_score": null,
            "tradeoff_evidence": null,
            "optimization_strategy": "Evolutionary search with fitness functions that typically balance fit and complexity (e.g., via parsimony pressure), though specific strategies are not detailed in this paper.",
            "human_evaluation": null,
            "human_evaluation_results": null,
            "comparative_baseline": "Presented as historical baseline for automated equation discovery.",
            "comparative_results": "Paper notes these methods were capable of determining nonlinear dynamical systems but are more computationally expensive; no quantitative comparison provided.",
            "domain_specific_findings": null,
            "uuid": "e2396.3",
            "source_info": {
                "paper_title": "Data-driven discovery of partial differential equations",
                "publication_date_yy_mm": "2016-09"
            }
        },
        {
            "name_short": "SINDy",
            "name_full": "Sparse Identification of Nonlinear Dynamics (SINDy)",
            "brief_description": "A previously-developed sparse regression framework for discovering governing ordinary differential equations (ODEs) from time series data by selecting sparse terms from a candidate library.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "SINDy",
            "system_description": "Construct a candidate library Θ of functions of state variables and fit time-derivatives via sparse regression to identify sparse coefficient vector ξ representing active terms in ODEs. The present paper generalizes this approach to include spatial derivatives and thus PDE discovery.",
            "research_domain": "dynamical systems / data-driven discovery",
            "problem_type": "identification of ODE models (prior work) and inspiration for PDE extension",
            "novelty_metric": null,
            "novelty_score": null,
            "feasibility_metric": null,
            "feasibility_score": null,
            "tradeoff_evidence": null,
            "optimization_strategy": "Sparse regression (ℓ1/thresholding) to balance model accuracy and sparsity; referenced as prior technique.",
            "human_evaluation": null,
            "human_evaluation_results": null,
            "comparative_baseline": "Referenced as prior method for ODE identification; PDE-FIND is presented as a generalization for PDEs.",
            "comparative_results": "Paper contrasts PDE-FIND with prior SINDy workflows that relied on first performing dimensionality reduction (POD) and then identifying ODEs on modal coefficients; PDE-FIND directly identifies PDE operators without that intermediate step.",
            "uuid": "e2396.4",
            "source_info": {
                "paper_title": "Data-driven discovery of partial differential equations",
                "publication_date_yy_mm": "2016-09"
            }
        },
        {
            "name_short": "Pareto analysis (model selection)",
            "name_full": "Pareto analysis for accuracy vs complexity",
            "brief_description": "Use of Pareto front analysis to balance regression accuracy against model complexity (sparsity) when selecting discovered PDE models.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Pareto analysis (accuracy vs complexity)",
            "system_description": "Constructs a set of candidate models across different sparsity/regularization levels and selects models lying on the Pareto front that offer the best trade-off between low residual error (accuracy) and low complexity (few nonzero terms). The method is used conceptually and practically to avoid overfitting and pick parsimonious governing equations.",
            "research_domain": "model selection / scientific model discovery",
            "problem_type": "trade-off optimization between fit and simplicity",
            "novelty_metric": "Model complexity measured by sparsity (||ξ||_0) and magnitude of coefficients; novelty per se is not explicitly defined.",
            "novelty_score": null,
            "feasibility_metric": "Model feasibility measured by residual error (||Θ ξ − U_t||_2^2) and numeric conditioning; Pareto front balances these.",
            "feasibility_score": null,
            "tradeoff_evidence": "Paper explicitly states selection via Pareto analysis; also includes adding a sparsity penalty (||ξ||_0) to the objective to discourage overfitting and select models at an optimal Pareto position.",
            "optimization_strategy": "Multi-objective viewpoint (accuracy vs complexity) realized via regularization parameters, explicit sparsity penalty and iterative thresholding to produce models at different points on the Pareto curve for selection.",
            "human_evaluation": false,
            "human_evaluation_results": null,
            "comparative_baseline": null,
            "comparative_results": null,
            "domain_specific_findings": "Pareto selection helps in choosing physically parsimonious PDEs across a range of canonical examples; the paper uses it to explain selection preference for simpler one-way wave equation unless additional data (multiple amplitudes) is provided.",
            "uuid": "e2396.5",
            "source_info": {
                "paper_title": "Data-driven discovery of partial differential equations",
                "publication_date_yy_mm": "2016-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Distilling Free-Form Natural Laws from Experimental Data",
            "rating": 2
        },
        {
            "paper_title": "Automated reverse engineering of nonlinear dynamical systems",
            "rating": 1
        },
        {
            "paper_title": "Discovering governing equations from data: Sparse identification of nonlinear dynamical systems",
            "rating": 2
        }
    ],
    "cost": 0.012857999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Data-driven discovery of partial differential equations</h1>
<p>Samuel H. Rudy ${ }^{1 *}$, Steven L. Brunton ${ }^{2}$, Joshua L. Proctor ${ }^{3}$, and J. Nathan Kutz ${ }^{1}$<br>${ }^{1}$ Department of Applied Mathematics, University of Washington, Seattle, WA. 98195<br>${ }^{2}$ Department of Mechanical Engineering, University of Washington, Seattle, WA. 98195 and<br>${ }^{3}$ Institute for Disease Modeling, , 3150 139th Ave SE, Bellevue, WA 98005<br>(Dated: September 22, 2016)</p>
<h4>Abstract</h4>
<p>We propose a sparse regression method capable of discovering the governing partial differential equation(s) of a given system by time series measurements in the spatial domain. The regression framework relies on sparsity promoting techniques to select the nonlinear and partial derivative terms terms of the governing equations that most accurately represent the data, bypassing a combinatorially large search through all possible candidate models. The method balances model complexity and regression accuracy by selecting a parsimonious model via Pareto analysis. Time series measurements can be made in an Eulerian framework where the sensors are fixed spatially, or in a Lagrangian framework where the sensors move with the dynamics. The method is computationally efficient, robust, and demonstrated to work on a variety of canonical problems of mathematical physics including Navier-Stokes, the quantum harmonic oscillator, and the diffusion equation. Moreover, the method is capable of disambiguating between potentially non-unique dynamical terms by using multiple time series taken with different initial data. Thus for a traveling wave, the method can distinguish between a linear wave equation or the Korteweg-deVries equation, for instance. The method provides a promising new technique for discovering governing equations and physical laws in parametrized spatio-temporal systems where first-principles derivations are intractable.</p>
<p>PACS numbers: $05.45 .-\mathrm{a}, 05.45 . \mathrm{Yv}$</p>
<p>Data-driven discovery methods, which have been enabled in the last decade by the plummeting cost of sensors, data storage and computational resources, are having a transformative impact on the sciences, enabling a variety of innovations for characterizing high dimensional data generated from experiments. Less well understood is how to uncover underlying physical laws and/or governing equations from time series data that exhibit spatio-temporal activity. Traditional theoretical methods for deriving the underlying partial differential equations (PDEs) are rooted in conservation laws, physical principles and/or phenomenological behaviors. These first principle derivations lead to many of the canonical models of mathematical physics. However, there remain many complex systems that have eluded quantitative analytic descriptions or even characterization of a suitable choice of variables (e.g. neuroscience, power grid, epidemiology, finance, ecology, etc). We propose an alternative method to derive governing equations based solely on time series data collected at a fixed number of spatial locations. Using innovations in sparse regression, we discover the terms of the governing PDE that most accurately represent the data from a large library of potential candidate functions. Importantly, measurements can be made in an Eulerian framework where the sensors are fixed spatially, or in a Lagrangian framework where the sensors move with the dynamics. We demonstrate the success of the method by deriving, from time series data alone, many canonical models of mathematical physics.</p>
<p>Methods for data-driven discovery of dynamical sys-</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>tems [1] include equation-free modeling [2], empirical dynamic modeling [3, 4], modeling emergent behavior [5], and automated inference of dynamics [6-8]. In this series of developments, seminal contributions leveraging symbolic regression and an evolutionary algorithm [9, 10] were capable of directly determining nonlinear dynamical system from data. More recently, sparsity promoting techniques [11] have been used to robustly determine, in a highly efficient computational manner, the governing dynamical system [12, 13]. Both the evolutionary [10] and sparse [12] symbolic regression methods avoid overfitting by selecting parsimonious models that balance model accuracy with complexity via Pareto analysis.</p>
<p>The method we present is able to select linear and/or nonlinear terms, including spatial derivatives, resulting in the identification of PDEs from data. Previous methods are able to identify ODEs from data, but not partial derivative terms [12]. Only those terms that are most informative about the dynamics are selected as part of the discovered PDE. The generalization presented here is critically important since the majority of canonical models in mathematical physics contain spatio-temporal dynamics. The addition of spatial structure is nontrivial and requires modification of the methodology and data collection (Eulerian or Lagrangian measurements) in order to circumvent potential ambiguities. The resulting algorithm, PDE functional identification of nonlinear dynamics (PDE-FIND), is applied to numerous canonical models of mathematical physics.</p>
<p>In what follows, we consider a PDE of the form</p>
<p>$$
u_{t}=N\left(u, u_{x}, u_{x x}, \cdots, x, \mu\right)
$$</p>
<p>where the subscripts denote partial differentiation in either time or space, and $N(\cdot)$ is an unknown right-hand</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>FIG. 1: Steps in the PDE functional identification of nonlinear dynamics (PDE-FIND) algorithm, applied to infer the Navier-Stokes equation from data. <strong>1a.</strong> Data is collected as snapshots of a solution to a PDE. <strong>1b.</strong> Numerical derivatives are taken and data is compiled into a large matrix Θ, incorporating candidate terms for the PDE. <strong>1c.</strong> Sparse regressions is used to identify active terms in the PDE. <strong>2a.</strong> For large datasets, sparse sampling may be used to reduce the size of the problem. <strong>2b.</strong> Subsampling the dataset is equivalent to taking a subset of rows from the linear system in (2). <strong>2c.</strong> An identical sparse regression problem is formed but with fewer rows. <strong>d.</strong> Active terms in ξ are synthesized into a PDE.</p>
<p>side that is generally a nonlinear function of u(x, t), its derivatives, and parameters in µ. Our objective is to construct N(·) given time series measurements of the system at a fixed number of spatial locations in x. A key assumption is that the function N(·) consists of only a few terms, making it sparse in the space of possible functions. As an example, Burgers' equation (N = −uu_x + µu_xx) and the harmonic oscillator (N = −iµx²u −iħu_xx/2) each have two terms. Thus sparse regression allows one to determine which right hand side terms are non-zero without an intractable (up-hard) combinatorial brute-force search.</p>
<p>The sparse regression and discovery method (See Fig. 1) begins by first collecting all the spatial, time series data into a single column vector <strong>U</strong> ∈ ℂ^mn representing data collected over m time points and n spatial locations. We also consider any additional input such as a known potential for the Schrödinger equation, or the magnitude of complex data, in a column vector <strong>Q</strong> ∈ ℂ^mn. Next, a library Θ(<strong>U</strong>, <strong>Q</strong>) ∈ ℂ^mn×D of D candidate linear and nonlinear terms and partial derivatives for the PDE is constructed. Each column of Θ(<strong>U</strong>, <strong>Q</strong>) lies in ℂ^mn and contains the values of a candidate term in the PDE across all gridpoints on which data is collected, as illustrated in Fig. 1. For example, a column of Θ(<strong>U</strong>, <strong>Q</strong>) may be q²u_xx. The PDE in this library is:</p>
<p>$$
\mathbf{U}_t = \Theta(\mathbf{U}, \mathbf{Q}) \xi. \tag{2}
$$</p>
<p>Each entry in ξ is a coefficient corresponding to a term in the PDE, and for canonical PDEs, the vector ξ is sparse, meaning that only a few terms are active.</p>
<p>Proper evaluation of the numerical derivatives is the most challenging and critical task for the success of the method. Given the well-known accuracy problems with finite-difference approximations, we instead use polynomial interpolation for differentiating noisy data. The method depends on the degree of polynomial and number of points used. In some cases, filtering the noise via the singular value decomposition is necessary (See Supplementary Materials for details).</p>
<p>In general, we require the sparsest vector ξ that satisfies (2) with a small residual. Instead of an intractable combinatorial search through all possible sparse vector structures, a common technique is to relax the problem to a convex ℓ₁ regularized least squares [11]; however, this tends to perform poorly with highly correlated data. Instead, we approximate the problem using candidate solutions to a ridge regression problem with hard thresholding, which we call sequential threshold ridge regression (STRidge in Algorithm 1). For a given tolerance and λ, this gives a sparse approximation to ξ. We iteratively refine the tolerance of Algorithm 1 to find the best predictor based on the selection criteria,</p>
<p>$$
\hat{\xi} = \arg\min_{\xi} \left| \Theta \xi - \mathbf{U}_t \right|_2^2 + \lambda \left| \xi \right|_2^2 \qquad \text{# ridge regression}
$$</p>
<p>$$
\underset{\text{bigcoeffs}}{j} = {j \left| |\hat{\xi}_j| \geq \text{tol} \right| \qquad \text{# select large coefficients} \quad \hat{\xi} \mid \sim \text{bigcoeffs} \right| = 0 \qquad \text{# apply hard threshold}
$$</p>
<p>$$
\hat{\xi} \mid \text{bigcoeffs} \mid = \text{STRidge} (\Theta \left| \text{, bigcoeffs} \right|, \mathbf{U}_t, \text{tol, iters} - 1) \qquad \text{# recursive call with fewer coefficients}
$$</p>
<p>$$
\text{return } \hat{\xi}
$$</p>
<p>$$
\hat{\xi} = \arg\min_{\xi} \left| \Theta(\mathbf{U}, \mathbf{Q}) \xi - \mathbf{U}_t \right|_2^2 + \epsilon \kappa (\Theta(\mathbf{U}, \mathbf{Q})) \left| \xi \right|_0 \quad (3)
$$</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>FIG. 2: Inferring the diffusion equation from a single Brownian motion. (a) Time series is broken into many short random walks that are used to construct histograms of the displacement. (b) The Brownian motion trajectory, following the diffusion equation. (c) Parameter error (||ξ* − ξ||1) vs. length of known time series. Blue symbols correspond to correct identification of the structure of the diffusion model, u_t = cu_xx.</p>
<p>where κ(Θ) is the condition number of the matrix Θ, indicating stronger regularization for ill-posed problems. Penalizing ||ξ||_0 discourages over fitting by selecting from the optimal position in a Pareto front.</p>
<p>PDE-FIND differs from previous sparse identification algorithms [12], where high-dimensional data from a PDE is handled by first applying dimensionality reduction, such as proper orthogonal decomposition (POD), to obtain a few dominant coherent structures in the data. Traditionally, an ODE is then identified on the coefficients of these energetic modes, resulting in a model that resembles a Galerkin projection onto POD modes [14]. In contrast, the PDE-FIND algorithm directly identifies the fewest terms required to balance the governing PDE.</p>
<p>As a first demonstration of the method, we consider one of the fundamental results of the early 20th century concerning the relationship between random walks (Brownian motion) and diffusion. The theoretical connection between these two was first made by Einstein in 1905 [15] and was part of the <em>Annus Mirablis papers</em> which lay the foundations of modern physics. We use the method proposed here to sample the movement of a random walker, which is effectively a Lagrangian measurement coordinate, in order to verify that it can reproduce the well-known diffusion equation. By biasing the random walk, we can also produce the generalization of advection-diffusion in one-dimension. Figure 2 shows the success of the method in identifying the correct diffusion model from a random walk trajectory. Given a sufficiently long time series with high enough resolution, the method produces the heat equation for the evolution of the probability distribution function. Thus a PDE is derived from a single time series representing discrete measurements of a continuous stochastic process. Specifically, a single time series is broken into pieces to construct a histogram approximating the distribution function of a</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>FIG. 3: Inferring nonlinearity via observing solutions at multiple amplitudes. (a) An example 2-soliton solution to the KdV equation. (b) Applying our method to a single soliton solution determines that it solves the standard advection equation. (c) Looking at two completely separate solutions reveals nonlinearity.</p>
<p>trajectory's future position at various timesteps. The resulting function is fit to a PDE using PDE-FIND, thus allowing us to sample Brownian motion in order to derive the diffusion equation.</p>
<p>A second canonical example is the KdV equation modeling the unidirectional propagation of small-amplitude, long water waves or shallow-water waves. Discovered first by Boussinesq in 1877 and later developed by Korteweg and deVries in 1895, it was one of the earliest models known to have soliton solutions. One potential viewpoint of the equation is as a dispersive regularization of Burgers' equation. The KdV evolution is given by</p>
<p>$$u_t + 6uu_x + u_{xxx} = 0,\tag{4}$$</p>
<p>with soliton solutions taking the form u(x,t) = (c/2) sech^2[(√c/2)(x − ct − x_0)]. These solutions propagate with a speed proportional to their amplitude c. Interestingly, if one observes a single propagating soliton, it would be indistinguishable from a solution to the one-way wave equation u_t + cu_x = 0. As such, it presents a challenge to the sparse regression framework as the sparsity promotion would select the one-way wave equation over the KdV equation since it has the sparsest representation. This ambiguity in the governing PDE is rectified by constructing time series data for more than a single initial amplitude. Figure 3 demonstrates the evolution of two KdV solitons of differing amplitudes, which allows for uniquely determining the governing PDE (4).</p>
<p>Table I applies the methodology proposed to a wide range of canonical models from mathematical physics. The PDEs selected represent a wide range of physical systems, displaying both Hamiltonian (conservative) dynamics and dissipative nonlinear dynamics along with periodic to chaotic behavior. Aside from the quantum oscillator (3rd row), all the dynamics observed are strongly nonlinear. Remarkably, the method is able to discover each physical system even if significantly subsampled spatially. The space and time sampling required, along with the accuracy in recovering the PDE parameters with and without noise, are detailed in the Table. This highlights</p>
<table>
<thead>
<tr>
<th>PDE</th>
<th>Form</th>
<th>Error (no noise, noise)</th>
<th>Discretization</th>
</tr>
</thead>
<tbody>
<tr>
<td>KdV</td>
<td>$u_{t}+6 u u_{x}+u_{x x x}=0$</td>
<td>$1 \% \pm 0.2 \%, 7 \% \pm 5 \%$</td>
<td>$x \in[-30,30], n=512, t \in[0,20], m=201$</td>
</tr>
<tr>
<td>Burgers</td>
<td>$u_{t}+u u_{x}-\epsilon u_{x x}=0$</td>
<td>$0.15 \% \pm 0.06 \%, 0.8 \% \pm 0.6 \%$</td>
<td>$x \in[-8,8], n=256, t \in[0,10], m=101$</td>
</tr>
<tr>
<td>Schrödinger</td>
<td>$i u_{t}+\frac{1}{2} u_{x x}-\frac{x^{2}}{2} u=0$</td>
<td>$0.25 \% \pm 0.01 \%, 10 \% \pm 7 \%$</td>
<td>$x \in[-7.5,7.5], n=512, t \in[0,10], m=401$</td>
</tr>
<tr>
<td>NLS</td>
<td>$i u_{t}+\frac{1}{2} u_{x x}+</td>
<td>u</td>
<td>^{2} u=0$</td>
</tr>
<tr>
<td>KS</td>
<td>$u_{t}+u u_{x}+u_{x x}+u_{x x x x}=0$</td>
<td>$1.3 \% \pm 1.3 \%, 70 \% \pm 27 \%$</td>
<td>$x \in[0,100], n=1024, t \in[0,100], m=251$</td>
</tr>
<tr>
<td>Reaction <br> Diffusion</td>
<td>$\begin{aligned} &amp; u_{t}=0.1 \nabla^{2} u+\lambda(A) u-\omega(A) v \ &amp; v_{t}=0.1 \nabla^{2} v+\omega(A) u+\lambda(A) v \ &amp; A^{2}=u^{2}+v^{2}, \omega=-\beta A^{2}, \lambda=1-A^{2} \end{aligned}$</td>
<td>$0.02 \% \pm 0.01 \%, 3.8 \% \pm 2.4 \%$</td>
<td>$\begin{aligned} &amp; x, y \in[-10,10], n=256, t \in[0,10], m=201 \ &amp; \text { subsample } 1.14 \% \end{aligned}$</td>
</tr>
<tr>
<td>Navier Stokes</td>
<td>$\omega_{t}+(\mathbf{u} \cdot \nabla) \omega=\frac{1}{R e} \nabla^{2} \omega$</td>
<td>$1 \% \pm 0.2 \%, 7 \% \pm 6 \%$</td>
<td>$\begin{aligned} &amp; x \in[0,9], n_{x}=449, y \in[0,4], n_{y}=199, \ &amp; t \in[0,30], m=151, \text { subsample } 2.22 \% \end{aligned}$</td>
</tr>
</tbody>
</table>
<p>TABLE I: Summary of regression results for a wide range of canonical modes of mathematical physics. In each example, the correct model structure is identified using PDE-FIND. The spatial and temporal sampling used for the regression is given along with the error produced in the parameters of the model for both no noise and $1 \%$ noise. In the reaction-diffusion (RD) system, $0.5 \%$ noise is used. For Navier Stokes and Reaction Diffusion, the percent of data used in subsampling is also given.
the broad applicability of the method and the success of the technique in discovering governing PDEs.</p>
<p>PDE-FIND is a viable, data-driven tool for modern applications where first-principles derivations may be intractable (e.g. neuroscience, epidemiology, dynamical networks), but where new data recordings and sensor technologies are revolutionizing our understanding of physical and/or biophysical processes on spatial domains. To our knowledge, this is the first data-driven regression technique that explicitly accounts for spatial derivatives in discovering physical laws, thus allowing for a regression to an operator on an infinite-dimensional space. The ability to discover physical laws instead of approximate,
low-dimensional subspaces enables significantly improved future state predictions as well as the discovery of parametric dependencies. For instance, we can discover the Navier Stokes equation at $R e=100$ and use this knowledge to accurately simulate a fully turbulent system at $R e=10000$ where no data was collected. This represents a significant paradigm shift when compared with most data-driven, machine learning architectures where accurate predictions can only be made near parameter regimes where the data was sampled.</p>
<p>Code and supplementary material:
https://github.com/snagcliffs/PDE-FIND
[1] J. Crutchfield and B. McNamara, Comp. sys. 1, 417 (1987).
[2] I. Kevrekidis, C. Gear, J. Hyman, P. Kevrekidis, O. Rumborg, and C. Theodoropoulos, Comm. Math. Sci. 1, 715 (2003).
[3] G. Sugihara, R. May, H. Ye, C.-h. Hsieh, E. Deyle, M. Fogarty, and S. Munch, Science 338, 496 (2012).
[4] H. Ye, R. J. Beamish, S. M. Glaser, S. C. Grant, C.-h. Hsieh, L. J. Richards, J. T. Schnute, and G. Sugihara, PNAS 112, E1569 (2015).
[5] A. J. Roberts, Model emergent dynamics in complex systems (SIAM, 2014).
[6] M. Schmidt, R. Vallabhajosyula, J. Jenkins, J. Hood, A. Soni, J. Wikswo, and H. Lipson, Phys. bio. 8, 055011 (2011).
[7] B. Daniels and I. Nemenman, Nat. comm. 6 (2015).
[8] B. Daniels and I. Nemenman, PloS one 10, e0119821 (2015).
[9] J. Bongard and H. Lipson, PNAS 104, 9943 (2007).
[10] M. Schmidt and H. Lipson, Science 324, 81 (2009).
[11] R. Tibshirani, J. Roy. Stat. Soc. B p. 267 (1996).
[12] S. L. Brunton, J. L. Proctor, and J. N. Kutz, PNAS 113, 3932 (2016).
[13] N. M. Mangan, S. L. Brunton, J. L. Proctor, and J. N. Kutz, ArXiv e-prints arXiv:1605.08368 (2016).
[14] P. Holmes, J. Lumley, G. Berkooz, and C. Rowley, Turbulence, coherent structures, dynamical systems and symmetry (Cambridge, Cambridge, England, 2012), 2nd ed.
[15] A. Einstein, Ann. der Physik 17, 549 (1905).</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*Electronic address: shrudy@uw.edu&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3764 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3764</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3764</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-89.html">extraction-schema-89</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) or related AI systems being used to distill, extract, or induce qualitative laws, rules, or scientific principles from large collections of scholarly or scientific papers.</div>
                <p><strong>Paper ID:</strong> paper-88abef771472c3aa46c53d5d626a0d0c3b66e8cd</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/88abef771472c3aa46c53d5d626a0d0c3b66e8cd" target="_blank">Evaluating ChatGPT's Information Extraction Capabilities: An Assessment of Performance, Explainability, Calibration, and Faithfulness</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> ChatGPT's performance in Standard-IE setting is poor, but it surprisingly exhibits excellent performance in the OpenIE setting, as evidenced by human evaluation, and indicates that ChatGPT provides high-quality and trustworthy explanations for its decisions.</p>
                <p><strong>Paper Abstract:</strong> The capability of Large Language Models (LLMs) like ChatGPT to comprehend user intent and provide reasonable responses has made them extremely popular lately. In this paper, we focus on assessing the overall ability of ChatGPT using 7 fine-grained information extraction (IE) tasks. Specially, we present the systematically analysis by measuring ChatGPT's performance, explainability, calibration, and faithfulness, and resulting in 15 keys from either the ChatGPT or domain experts. Our findings reveal that ChatGPT's performance in Standard-IE setting is poor, but it surprisingly exhibits excellent performance in the OpenIE setting, as evidenced by human evaluation. In addition, our research indicates that ChatGPT provides high-quality and trustworthy explanations for its decisions. However, there is an issue of ChatGPT being overconfident in its predictions, which resulting in low calibration. Furthermore, ChatGPT demonstrates a high level of faithfulness to the original text in the majority of cases. We manually annotate and release the test sets of 7 fine-grained IE tasks contains 14 datasets to further promote the research. The datasets and code are available at https://github.com/pkuserc/ChatGPT_for_IE.</p>
                <p><strong>Cost:</strong> 0.007</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3764",
    "paper_id": "paper-88abef771472c3aa46c53d5d626a0d0c3b66e8cd",
    "extraction_schema_id": "extraction-schema-89",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.006725500000000001,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Evaluating ChatGPT's Information Extraction Capabilities: An Assessment of Performance, Explainability, Calibration, and Faithfulness</h1>
<p>Bo $\mathbf{L i}^{1,2}$, Gexiang Fang ${ }^{1,2}$, Yang Yang ${ }^{1,2}$, Quansen Wang ${ }^{3}$, Wei Ye ${ }^{1}$, Wen Zhao ${ }^{1}$, and Shikun Zhang ${ }^{1}$<br>${ }^{1}$ National Engineering Research Center for Software Engineering, Peking University<br>${ }^{2}$ School of Software and Microelectronics, Peking University<br>${ }^{3}$ Boston University<br>{deepblue.lb, fanggx, yangy}@stu.pku.edu.cn, quansenw@bu.edu<br>{wye, zhaowen, zhangsk}@pku.edu.cn</p>
<h4>Abstract</h4>
<p>The capability of Large Language Models (LLMs) like ChatGPT to comprehend user intent and provide reasonable responses has made them extremely popular lately. In this paper, we focus on assessing the overall ability of ChatGPT using 7 fine-grained information extraction (IE) tasks. Specially, we present the systematically analysis by measuring ChatGPT's performance, explainability, calibration, and faithfulness, and resulting in 15 keys from either the ChatGPT or domain experts. Our findings reveal that ChatGPT's performance in Standard-IE setting is poor, but it surprisingly exhibits excellent performance in the OpenIE setting, as evidenced by human evaluation. In addition, our research indicates that ChatGPT provides high-quality and trustworthy explanations for its decisions. However, there is an issue of ChatGPT being overconfident in its predictions, which resulting in low calibration. Furthermore, ChatGPT demonstrates a high level of faithfulness to the original text in the majority of cases. We manually annotate and release the test sets of 7 finegrained IE tasks contains 14 datasets to further promote the research. The datasets and code are available at this url. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>Large Language Models (LLMs) (e.g., GPT3 (Brown et al., 2020), LaMDA (Thoppilan et al., 2022) and PaLM (Chowdhery et al., 2022), etc.) have greatly promoted the development of the Natural Language Processing (NLP) community. With a proper instruction (often the task definition) (Ouyang et al., 2022; Kojima et al., 2022; Chung et al., 2022; Wang et al., 2022) and the chain-of-thought (CoT) prompting (Wei et al., 2022b), LLMs achieve surprisingly good performances when dealing with unseen tasks.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>ChatGPT ${ }^{2}$ is currently the most popular LLM, known for its impressive ability to understand user intent and generate human-like responses. ChatGPT is trained on the GPT family (Brown et al., 2020; Artetxe et al., 2022; Ouyang et al., 2022) using reinforcement learning from human feedback (RLHF) (Christiano et al., 2017) and highquality conversational-style datasets. Apart from its surprising dialogue ability, ChatGPT has many other aspects that attract researchers to explore. Some researchers have delved into the potential impacts of ChatGPT on human life (Haque et al., 2022; Zhuo et al., 2023; Susnjak, 2022; Basic et al., 2023). Other researchers are interested in exploring the capabilities of ChatGPT for various NLP tasks (Zhang et al., 2022a; Qin et al., 2023; Mitrovic et al., 2023; Guo et al., 2023). The capabilities of ChatGPT have been preliminarily explored through the above research and valuable conclusions have been drawn.</p>
<p>Given ChatGPT is a closed model that does not provide information about its training details, and any response from the model encodes an opinion. The response can significantly impact the user's experience and shape their beliefs going forward (Aiyappa et al., 2023; Santurkar et al., 2023; Deshpande et al., 2023; Huang et al., 2023). Consequently, evaluating ChatGPT should involve not only assessing its ability to achieve high performance but also measuring the reliability of the answers it provides. To help users better understand the overall quality of ChatGPT's responses and enable systematic measurement of its capabilities, we design the following four metric dimensions: The first dimension we consider is Performance, which reflects ChatGPT's overall performance on various IE tasks from multiple perspectives. The second metric dimension, Explainability (Rajani et al., 2019; Aghajanyan et al., 2021; Zini and Awad, 2023), evaluates whether Chat-</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>GPT could give a justified reason for its prediction, thereby providing insights into ChatGPT's decision-making process. The third one is Calibration <em>Guo et al. (2017); Kumar et al. (2019); Thulasidasan et al. (2019); Minderer et al. (2021)</em>, which measures the predictive uncertainty of a model, and we use this metric to assess if ChatGPT is overconfidence on its prediction. The last dimension is Faithfulness <em>Maynez et al. (2020); Koto et al. (2022); Creswell and Shanahan (2022); He et al. (2023)</em>, it is frequently employed in the summarization task to determine whether the summary accurately reflects the input. In our research, we adopt faithfulness as a measure of whether the explanations given by ChatGPT are truthful to the input, or if they are spurious. In summary, according to the above four dimensions, we collect 15 keys from either the ChatGPT or domain experts for the evaluation (§ 3).</p>
<p>In this research, we aim to perform a comprehensive study and detailed analysis of ChatGPT's capabilities through various information extraction (IE) tasks. IE involves heterogeneous structure extraction, factual knowledge usage, and diversified targets<em>Yamada et al. (2020); Paolini et al. (2021); Lu et al. (2022)</em>, making it an ideal scenario for evaluating ChatGPT's capabilities. Overall, we conduct our experiments and analysis based on 14 datasets belonging to 7 fine-grained IE tasks (§ 4). Additionally, we assess the explainability, calibration, and faithfulness of ChatGPT's responses through both self-check and human-check (§ 5). To sum up, our main contributions are summarized as follows:</p>
<ul>
<li>To assess the overall ability of ChatGPT, we employ a comprehensive and systematic evaluation from four dimensions: 1) performance, 2) explainability, 3) calibration, and 4) faithfulness. We then collected 15 keys belonging to above dimensions from either the ChatGPT or domain experts for the research. All the manually annotated datasets and code are made public available for future research.</li>
<li>We comprehensively evaluate the overall performance of ChatGPT on various tasks in both Standard-IE and OpenIE settings and compare it with other popular models. Our research indicates that ChatGPT's performance is not satisfactory in the Standard-IE setting. However, we show that it provides surprisingly good results in the OpenIE setting, as confirmed by human evaluation. Furthermore, we also discover that ChatGPT provides high-quality and trustworthy explanations for its decisions. Although, it displays overconfidence in its predictions, leading to low calibration. Besides, ChatGPT is largely faithful to the original text in most cases.</li>
</ul>
<h2>2 Related Work</h2>
<h3>2.1 Large Language Models</h3>
<p>Large Language Models (LLMs) typically contain more than a hundred billion parameters, such as GPT-3 <em>Brown et al. (2020)</em>, Gopher <em>Rae et al. (2021)</em>, LaMDA <em>Thoppilan et al. (2022)</em>, Megatron-turing-NLG <em>Smith et al. (2022)</em>, and PaLM <em>Chowdhery et al. (2022)</em>, among others. Scaling up the model size brings impressive abilities on few-shot and zero-shot learning scenarios, such as producing reasonable results with very few samples or task descriptions <em>Brown et al. (2020); Chowdhery et al. (2022)</em>. Moreover, scaling up the model size unlocks emergent abilities that were not observed in smaller models, enabling LLMs to exhibit strong generalizability on unseen tasks <em>Wei et al. (2022a); Fu et al. (2022); Mahowald et al. (2023)</em>.</p>
<p>With its impressive ability to understand user intent and generate human-like responses, ChatGPT has become the most popular language model currently. It is trained on the GPT family <em>Brown et al. (2020); Artetxe et al. (2022); Ouyang et al. (2022)</em> and high-quality conversational-style datasets using reinforcement learning from human feedback (RLHF) <em>Christiano et al. (2017)</em>.</p>
<p>Along with its dialogue ability, ChatGPT has other aspects that researchers are exploring. Some researchers show potential impacts of ChatGPT on human life, such as ethical risks <em>Haque et al. (2022); Zhuo et al. (2023); Krügel et al. (2023)</em>, the education sector <em>Susnjak (2022); Basic et al. (2023); Kortemeyer (2023)</em> and the medical scenario <em>Tu et al. (2023); Nov et al. (2023); Jeblick et al. (2022)</em>. Additionally, some researchers are keen to examine the potential of ChatGPT in addressing various natural language processing tasks. For instance, some works have examined ChatGPT's performance in stance detection <em>Zhang et al. (2022a)</em>, linguistic and sentiment analysis <em>Susnjak (2023); Ortega-Martín et al. (2023)</em>, general NLP tasks <em>Qin et al. (2023); Bian et al. (2023); Zhong et al. (2023); Wang et al. (2023a, b)</em>, and machine translation <em>Jiao et al. (2023a, b)</em></p>
<p>2023). (Frieder et al., 2023) explores the mathematical capabilities of ChatGPT, while (Bang et al., 2023) proposes an evaluation of ChatGPT on reasoning and other aspects. Additionally, (Mitrovic et al., 2023; Guo et al., 2023) investigate the differences between human-written and ChatGPTgenerated.</p>
<h3>2.2 Information Extraction</h3>
<p>Information Extraction (IE) is a long-standing research topic that aims to extract structured factual information from unstructured texts (Andersen et al., 1992; Crowe, 1995; Chieu et al., 2003; Wu and Weld, 2010; Khot et al., 2017; Lu et al., 2022). Typically, IE involves a wide range of tasks, such as named entity recognition(NER) (Gregoric et al., 2018; Martins et al., 2019; Li et al., 2020; Das et al., 2022), entity typing(ET) (Choi et al., 2018; Dai et al., 2021; Pang et al., 2022; Chen et al., 2022), relation extraction(RE) (Li et al., 2019; Fu et al., 2019; Bian et al., 2021; Ye et al., 2022), relation classification(RC) (Zeng et al., 2015; Ye et al., 2019; Zhou and Chen, 2021; Li et al., 2022b), event detection(ED) (Veyseh et al., 2021; Lou et al., 2021; Liu et al., 2022a; Zhao et al., 2022), event argument extraction(EAE) (Zhang et al., 2022b; Du and Ji, 2022; Ma et al., 2022), and event extraction(EE) (Wadden et al., 2019; Du and Cardie, 2020; Liu et al., 2022b; Hsu et al., 2022), among others. These tasks automatically generate structured factual outputs related to entity, and relation, and event, and greatly boost the development of NLP community.</p>
<h2>3 ChatGPT for Information Extraction</h2>
<p>In this section, we first briefly introduce 7 finegrained IE tasks, then we present how to collect 15 keys from the ChatGPT and domain experts.</p>
<h3>3.1 Information Extraction</h3>
<p>IE involves a wide range of tasks which need to extract structured factual information from unstructured texts, such as entity, and relation, and event. In this research, we conduct our analysis on the following 7 fine-grained IE tasks: ${ }^{3}$ 1) Entity Typing(ET) (Choi et al., 2018; Dai et al., 2021; Pang et al., 2022; Chen et al., 2022) aims to classify the type of a target entity under a given input; 2) Named Entity Recognition(NER) (Gregoric</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>et al., 2018; Martins et al., 2019; Li et al., 2020; Das et al., 2022) aims to first identify the candidate entities, and then classify their types; 3) Relation Classification(RC) (Zeng et al., 2015; Ye et al., 2019; Zhou and Chen, 2021; Li et al., 2022b) requires to classify the relation between two target entities; 4) Relation Extraction(RE) (Li et al., 2019; Fu et al., 2019; Bian et al., 2021; Ye et al., 2022) is a task to identify the target entities and the relation jointly; 5) Event Detection(ED) (Veyseh et al., 2021; Lou et al., 2021; Liu et al., 2022a; Zhao et al., 2022) identifies event triggers and their types; 6) Event Argument Extraction(EAE) (Zhang et al., 2022b; Du and Ji, 2022; Ma et al., 2022) distinguishes arguments and categorizes their roles with respect to the targe event; and 7) Event Extraction(EE) (Wadden et al., 2019; Du and Cardie, 2020; Liu et al., 2022b; Hsu et al., 2022) performs event detection and argument extraction jointly. Note that although some of these tasks are subsets of others, every task needs LLMs' unique ability to perform well. It is worth to explore the performances on these fine-grained IE tasks.</p>
<h3>3.2 Standard-IE Setting and OpenIE Setting</h3>
<p>To comprehensively evaluate the overall performance of ChatGPT on IE tasks, we ask ChatGPT to generate the responses from the Standard-IE setting and the OpenIE setting. The Standard-IE setting is commonly used in previous works, which uses the task-specific dataset with supervised learning paradigm to fine-tune a model. For ChatGPT, as we can not directly fine-tune the parameters, we evaluate the ChatGPT's ability to select the most appropriate answer from a set of candidate labels instead. Specifically, this setting is based on an instruction that includes the task description, the input text, the prompt, and the label set. Where the task description describes the specific IE task, the prompt involves the utterances that guide the ChatGPT outputs the required keys (which will be introduced in § 3.3), and the label set contains all candidate labels based on each dataset. The OpenIE setting is a more advanced and challenging scenario than Standard-IE setting. In this setting, we do not provide any candidate labels to ChatGPT and rely solely on its ability to comprehend the task description, the prompt, and input text to generate predictions. Our goal is to assess the ChatGPT's ability to produce reasonable factual knowledge.</p>
<table>
<thead>
<tr>
<th>Keys</th>
<th>Explanation</th>
</tr>
</thead>
<tbody>
<tr>
<td>Performance</td>
<td></td>
</tr>
<tr>
<td>Open</td>
<td>Directly ask ChatGPT to predict the class without the label set.</td>
</tr>
<tr>
<td>Standard</td>
<td>ChatGPT's most likely correct class with a given label set.</td>
</tr>
<tr>
<td>Top3</td>
<td>The three most likely classes of the given label set from ChatGPT.</td>
</tr>
<tr>
<td>Top5</td>
<td>The five most likely classes of the given label set from ChatGPT.</td>
</tr>
<tr>
<td>ifOpen_Correct (Manual)</td>
<td>Manually annotate whether the "Open" is reasonable.</td>
</tr>
<tr>
<td>Explainability</td>
<td></td>
</tr>
<tr>
<td>Reason_Open</td>
<td>The reason why ChatGPT chooses the class in "Open".</td>
</tr>
<tr>
<td>Reason_Standard</td>
<td>The reason why ChatGPT chooses the class in "Standard".</td>
</tr>
<tr>
<td>ifR_Open</td>
<td>Does ChatGPT think that "Reason_Open" is reasonable?</td>
</tr>
<tr>
<td>ifR_Standard</td>
<td>Does ChatGPT think that "Reason_Standard" is reasonable?</td>
</tr>
<tr>
<td>ifR_Open (Manual)</td>
<td>Manually annotate whether the "Reason_Open" is reasonable.</td>
</tr>
<tr>
<td>ifR_Standard (Manual)</td>
<td>Manually annotate whether the "Reason_Standard" is reasonable.</td>
</tr>
<tr>
<td>Calibration</td>
<td></td>
</tr>
<tr>
<td>Confidence_Open</td>
<td>The confidence of ChatGPT in predicting "Open".</td>
</tr>
<tr>
<td>Confidence_Standard</td>
<td>The confidence of ChatGPT in predicting "Standard".</td>
</tr>
<tr>
<td>Faithfulness</td>
<td></td>
</tr>
<tr>
<td>FicR_Open (Manual)</td>
<td>Manually annotate whether the "Reason_Open" is fictitious.</td>
</tr>
<tr>
<td>FicR_Standard (Manual)</td>
<td>Manually annotate whether the "Reason_Standard" is fictitious.</td>
</tr>
</tbody>
</table>
<p>Table 1: We gather 15 keys in this research, consisting of 10 keys automatically generated by ChatGPT and 5 keys that required manual annotation (denoted as Manual). These keys provide insight into ChatGPT's ability in four dimensions, namely: 1) performance, 2) explainability, 3) calibration, and 4) faithfulness.</p>
<h3>3.3 Collecting Keys From ChatGPT And Human Annotation</h3>
<p>In this subsection, we first describe 15 keys that are collected from the ChatGPT and domain experts. In Table 1, we show 10 keys that are extracted from ChatGPT and 5 keys that involves human involvements. These keys could systemically assess ChatGPT's ability from the following four aspects:</p>
<p>Performance. One important aspect of our research is to comprehensively evaluate the overall performance of ChatGPT on various tasks and compare it with other popular models. By examining its performance from different aspects, we seek to provide a detailed understanding of ChatGPT's capability on the downstream IE tasks.</p>
<p>Explainability. The explainability of ChatGPT is crucial for its application in real-world scenarios (Rajani et al., 2019; Aghajanyan et al., 2021; Zini and Awad, 2023). In our study, we will measure both the self-check and humancheck explainability of ChatGPT, with a focus on its ability to provide useful and accurate explanations of its reasoning process for humans. Specially, we ask ChatGPT to provide reasons for its predictions (Reason_Open
and Reason_Standard), and whether ChatGPT approves its explanations (ifR_Open and ifR_Standard). Additionally, we also manually evaluate the acceptability of these reasons to humans (ifR_Open(Manual) and ifR_Standard (Manual)).</p>
<p>Calibration. Measuring the calibration helps to evaluate the predictive uncertainty of a model (Guo et al., 2017; Kumar et al., 2019). A properly calibrated classifier should have predictive scores that accurately reflect the probability of correctness (Thulasidasan et al., 2019; Minderer et al., 2021). Given the tendency of modern neural networks to be overconfident in their predictions, we aim to identify potential uncertainties or overconfidence phenomenon of ChatGPT. To evaluate the calibration, ChatGPT is required to provide a confidence score (ranging from 1 to 100) for each prediction it makes (Confidence_Open and Confidence_Standard).</p>
<p>Faithfulness. The faithfulness of ChatGPT's explanation is important to ensure its trustworthiness (Maynez et al., 2020; He et al., 2023). In evaluating the faithfulness, we have included two keys that assess whether the reasons provided by ChatGPT are faithful to the original in-</p>
<p>put. These keys, FicR_Open(Manual) and FicR_Standard(Manual), require manual annotation by domain experts.</p>
<p>Due to the space limitation, we show an intuitive example in the Appendix A.3 to help readers better understand the annotation process.</p>
<h2>4 Performance</h2>
<h3>4.1 Setup</h3>
<p>To ensure a comprehensive evaluation of ChatGPT's capabilities, we conduct manual annotation and analysis on a diverse range of IE tasks, including 7 fine-grained tasks spanning 14 datasets. We collected 15 keys for each dataset from both ChatGPT and domain experts (§ 3). Only the test sets are annotated, as our aim is to analysis ChatGPT's abilities without any training. For space reasons, the detail of each dataset is shown in the Appendix A.1. Due to the time-consuming nature of obtaining responses from domain experts, we randomly select nearly 3,000 samples in total for our analysis. The number of manually annotated samples for each dataset is reported in the Appendix A.1. As for the outputs from ChatGPT, we use the official API to evaluate the whole test sets. ${ }^{4}$</p>
<p>Besides, we compare ChatGPT with several popular baselines: 1) BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019), and 2) State-of-theArt (SOTA) on the single dataset. Due to the space limitation, the details of state-of-the-art methods are shown in the Appendix A.2. As for the metric, we use Micro-F1 score for all tasks except RE and EE. For the RE task, we report the named entity recognition F1-score and the relation classification F1-score. As for the EE task, we show the trigger F1-score and argument F1-score. ${ }^{5}$</p>
<h3>4.2 Performance on the Standard-IE Setting</h3>
<p>In this subsection, we report the performances of different models on the Standard-IE setting, as depicted in Table 2. It is clear from the table that ChatGPT's performance is not comparable to that of baseline models and SOTA methods in most cases. This is not surprising given that directly asking ChatGPT for the prediction is more like a zero-shot scenario, whereas the other compared methods are trained on task-specific datasets</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>under a supervised learning paradigm. Another reason may be ChatGPT directly choose an answer from the given label set, and some labels are not easy to understand, thereby negatively impact the performance.</p>
<p>Moreover, our research indicates that ChatGPT performs well on relatively simple IE tasks but struggles with more complex and challenging tasks. For example, the entity typing (ET) task only involves classifying entities into pre-defined types without any further contextual analysis, and ChatGPT excels at this task, demonstrating that the model can generate accurate factual knowledge when the task is simple. However, in complex and challenging IE tasks such as RE, ChatGPT struggles as it requires to first identify the entities that exist in the input and then classify the relationship between them, which is a more challenging task than ET. Despite ChatGPT's acceptable results on the ET, NER and RC tasks, it still faces challenges with more multifaceted IE tasks like RE and EE, where deeper contextual analysis and reasoning abilities are required. In summary, ChatGPT's performance varies based on the complexity of the task, and it performs well on straightforward tasks.</p>
<p>Furthermore, the conclusion that ChatGPT performs worse than other models seems inconsistent with previous studies (Wei et al., 2023; Gao et al., 2023), which suggest that ChatGPT can achieve desirable performance in some IE tasks. One possible explanation for the difference in conclusions is that we report the performance of the entire test set for each task in our study, while prior studies reported on a very small set of test samples drawn at random, which may have substantial variance. Another factor may be that we used a concise and relatively unified prompt to guide ChatGPT, while other research relied on domain-specific prompts or included a large number of label descriptions in their prompts, which needs lots of domain knowledge and thereby limits the ability to generalize across various tasks.</p>
<h3>4.3 Performance on the OpenIE Setting</h3>
<p>In this subsection, we report both the accuracy of Standard-IE setting and OpenIE setting on the sampled dataset. ${ }^{6}$ For the Standard-IE setting, we provide the pre-defined label set and ask the ChatGPT to choose an answer for a given input, and the ac-</p>
<p><sup id="fnref2:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th>Task</th>
<th>Dataset</th>
<th>BERT</th>
<th>RoBERTa</th>
<th>SOTA</th>
<th>ChatGPT</th>
</tr>
</thead>
<tbody>
<tr>
<td>Entity</td>
<td>BBN</td>
<td>80.3</td>
<td>79.8</td>
<td>82.2 (Zuo et al., 2022)</td>
<td>85.6</td>
</tr>
<tr>
<td>Typing(ET)</td>
<td>OntoNotes 5.0</td>
<td>69.1</td>
<td>68.8</td>
<td>72.1 (Zuo et al., 2022)</td>
<td>73.4</td>
</tr>
<tr>
<td>Named Entity</td>
<td>CoNLL2003</td>
<td>92.8</td>
<td>92.4</td>
<td>94.6 (Wang et al., 2021)</td>
<td>67.2</td>
</tr>
<tr>
<td>Recognition(NER)</td>
<td>OntoNotes 5.0</td>
<td>89.2</td>
<td>90.9</td>
<td>91.9 (Ye et al., 2022)</td>
<td>51.1</td>
</tr>
<tr>
<td>Relation</td>
<td>TACRED</td>
<td>72.7</td>
<td>74.6</td>
<td>75.6 (Li et al., 2022a)</td>
<td>20.3</td>
</tr>
<tr>
<td>Classification(RC)</td>
<td>SemEval2010</td>
<td>89.1</td>
<td>89.8</td>
<td>91.3 (Zhao et al., 2021)</td>
<td>42.5</td>
</tr>
<tr>
<td>Relation</td>
<td>ACE05-R</td>
<td>87.5163.7</td>
<td>88.2165.1</td>
<td>91.1173.0 (Ye et al., 2022)</td>
<td>40.514.5</td>
</tr>
<tr>
<td>Extraction(RE)</td>
<td>SciERC</td>
<td>65.4143.0</td>
<td>63.6142.0</td>
<td>69.9153.2 (Ye et al., 2022)</td>
<td>25.915.5</td>
</tr>
<tr>
<td>Event</td>
<td>ACE05-E</td>
<td>71.8</td>
<td>72.9</td>
<td>75.8 (Liu et al., 2022a)</td>
<td>17.1</td>
</tr>
<tr>
<td>Detection(ED)</td>
<td>ACE05-E+</td>
<td>72.4</td>
<td>72.1</td>
<td>72.8 (Lin et al., 2020)</td>
<td>15.5</td>
</tr>
<tr>
<td>Event Argument</td>
<td>ACE05-E</td>
<td>65.3</td>
<td>68.0</td>
<td>73.5 (Hsu et al., 2022)</td>
<td>28.9</td>
</tr>
<tr>
<td>Extraction(EAE)</td>
<td>ACE05-E+</td>
<td>64.0</td>
<td>66.5</td>
<td>73.0 (Hsu et al., 2022)</td>
<td>30.9</td>
</tr>
<tr>
<td>Event</td>
<td>ACE05-E</td>
<td>71.8151.0</td>
<td>72.9151.9</td>
<td>74.7156.8 (Lin et al., 2020)</td>
<td>17.017.3</td>
</tr>
<tr>
<td>Extraction(EE)</td>
<td>ACE05-E+</td>
<td>72.4152.7</td>
<td>72.1153.4</td>
<td>71.7156.8 (Hsu et al., 2022)</td>
<td>16.617.8</td>
</tr>
</tbody>
</table>
<p>Table 2: The performances of ChatGPT and several baseline models on 14 IE datasets on the Standard-IE setting. We report the performance on the whole test set. All results are directly cited from public papers or re-implemented using official open-source code.</p>
<table>
<thead>
<tr>
<th></th>
<th>Standard-IE</th>
<th>OpenIE</th>
</tr>
</thead>
<tbody>
<tr>
<td>BBN(ET)</td>
<td>86.8%</td>
<td>97.2%</td>
</tr>
<tr>
<td>CoNLL(NER)</td>
<td>69.0%</td>
<td>93.3%</td>
</tr>
<tr>
<td>SemEval2010(RC)</td>
<td>43.3%</td>
<td>84.3%</td>
</tr>
<tr>
<td>ACE05-R(RE)</td>
<td>14.9%</td>
<td>23.9%</td>
</tr>
<tr>
<td>ACE05-E(ED)</td>
<td>12.4%</td>
<td>42.6%</td>
</tr>
<tr>
<td>ACE05-E(EAE)</td>
<td>17.3%</td>
<td>65.3%</td>
</tr>
<tr>
<td>ACE05-E(EE)</td>
<td>4.9%</td>
<td>28.8%</td>
</tr>
</tbody>
</table>
<p>Table 3: The accuracy of Standard-IE setting and OpenIE setting on the sampled test set. Our results show that ChatGPT could generate reasonable outputs on the OpenIE setting.
curacy was calculated by matching the predictions to the ground truth labels. On the other hand, the OpenIE setting refers to asking ChatGPT to make predictions without the pre-defined label set (Open in $\S$ 3.3). Three domain experts evaluate these predictions and vote on whether they were reasonable in light of the input and background knowledge, named as ifOpen_Correct in $\S$ 3.3. Our main goal is to determine if ChatGPT could produce logical and reasonable predictions without given the pre-defined label set, so we do not require the prediction to match with the ground truth.</p>
<p>The results presented in Table 3 indicate that ChatGPT's performance is somewhat inspiring under the OpenIE setting. For example, more than $84 \%$ of the predictions are considered rea-
sonable by the domain experts in ET, NER, and RC tasks. However, the performance is relatively poorer for more challenging tasks, such as RE and EE. Overall, compared with Standard-IE setting, ChatGPT's performance on the OpenIE setting is exciting. Our findings suggest that under the OpenIE setting, ChatGPT could generate reliable factual knowledge and reasonable output.</p>
<h3>4.4 The top-k Recall Analysis</h3>
<p>While generating the most likely prediction may be unsatisfactory on the Standard-IE setting, we seek to investigate whether ChatGPT could be a useful advisor. Therefore, we examine the recall of its top-k predictions, with $k=1,3$, or 5 . As shown in Table 5, the results indicate that compared with the top-1 recall, the top-3 recall increases significantly, e.g., the improvement is $19.6 \%$ on $\mathrm{Se}-$ mEval2010. Moreover, the top-5 recall reaches an impressive $94.9 \%$ on BBN and $76.0 \%$ on $\mathrm{Se}-$ mEval2010, demonstrating a favorable outcome. Our findings suggest that ChatGPT is a competent answer candidate generator for a given task under the Standard-IE setting, which could help users select the most probable prediction from the top-5 predictions.</p>
<h2>5 Explainability, Calibration and Faithfulness</h2>
<p>While ChatGPT's performance in evaluations is noteworthy, it is equally important to evaluate its</p>
<table>
<thead>
<tr>
<th></th>
<th>Stardand Setting</th>
<th></th>
<th></th>
<th>OpenIE Setting</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Self-check</td>
<td>Human-check</td>
<td>Overlap</td>
<td>Self-check</td>
<td>Human-check</td>
<td>Overlap</td>
</tr>
<tr>
<td>BBN (ET)</td>
<td>100.0%</td>
<td>99.2%</td>
<td>99.2%</td>
<td>100.0%</td>
<td>99.5%</td>
<td>99.5%</td>
</tr>
<tr>
<td>CoNLL (NER)</td>
<td>100.0%</td>
<td>99.3%</td>
<td>99.3%</td>
<td>100.0%</td>
<td>99.7%</td>
<td>99.7%</td>
</tr>
<tr>
<td>SemEval (RC)</td>
<td>100.0%</td>
<td>100.0%</td>
<td>100.0%</td>
<td>100.0%</td>
<td>99.7%</td>
<td>99.7%</td>
</tr>
<tr>
<td>ACE05-R (RE)</td>
<td>100.0%</td>
<td>90.0%</td>
<td>90.0%</td>
<td>100.0%</td>
<td>100.0%</td>
<td>100.0%</td>
</tr>
<tr>
<td>ACE05-E (ED)</td>
<td>100.0%</td>
<td>96.3%</td>
<td>96.3%</td>
<td>100.0%</td>
<td>90.2%</td>
<td>90.2%</td>
</tr>
<tr>
<td>ACE05-E (EAE)</td>
<td>100.0%</td>
<td>74.1%</td>
<td>74.1%</td>
<td>100.0%</td>
<td>90.4%</td>
<td>90.4%</td>
</tr>
<tr>
<td>ACE05-E (EE)</td>
<td>100.0%</td>
<td>47.1%</td>
<td>47.1%</td>
<td>94.0%</td>
<td>78.0%</td>
<td>74.0%</td>
</tr>
</tbody>
</table>
<p>Table 4: The explainability of ChatGPT measured on the sampled test set. We report the ratio of samples with reasonable reasons discriminated by ChatGPT (self-check) and domain experts (human-check) under different settings. Besides, we also compute the overlap ration for both of them. These results indicate that in most cases, ChatGPT exhibits strong explainability for its prediction.</p>
<table>
<thead>
<tr>
<th></th>
<th>top-1</th>
<th>top-3</th>
<th>top-5</th>
</tr>
</thead>
<tbody>
<tr>
<td>BBN</td>
<td>85.6%</td>
<td>92.7%</td>
<td>94.9% (+9.3%)</td>
</tr>
<tr>
<td>SemEval2010</td>
<td>42.5%</td>
<td>62.1%</td>
<td>76.0% (+33.5%)</td>
</tr>
</tbody>
</table>
<p>Table 5: The top-k recall analysis on the whole test set. We report two datasets due to the space limitation, other datasets show similar observation. The results show that ChatGPT could server as a good advisor.
ability from diverse dimensions that could offer important insights for future research directions. In this section, we analyze several relevant factors, including explainability, calibration, and faithfulness, to comprehensively evaluate ChatGPT's abilities. Overall, our findings suggest that ChatGPT can provide high-quality and reliable explanations for its predictions, but it tends to display overconfidence in most cases, leading to low calibration. Additionally, ChatGPT displays high faithfulness to the original text, making it an reliable tool for users.</p>
<h3>5.1 Explainability</h3>
<p>Explainability is a critical requirement for LLMs, as it allows users to understand how the model arrives at its predictions (Peng et al., 2023). In this study, we investigate whether ChatGPT could provide a reasonable explanation for its output. To be specific, we request ChatGPT to provide reasons for its predictions in the Standard-IE and OpenIE settings. The corresponding keys are denoted as Reason_Standard and Reason_Open, as explained in § 3.3. These reasons are then evaluated for their reasonableness by both ChatGPT and three domain experts, with the resulting evaluations referred to as self-check and human-check, respectively. We only consider the samples with correct
predictions in the Standard-IE setting to ensure a robust evaluation of ChatGPT's explainability ability. ${ }^{7}$ This is because evaluating the reasons provided by ChatGPT for incorrect predictions is less valuable.</p>
<p>The ratio of samples with reasonable explanations (termed as reasonable score) is summarized in Table 4, from which we can derive the following conclusions. Firstly, both ChatGPT and domain experts highly approve of the reasons given by ChatGPT, with the majority of datasets achieving a reasonable score of over $90 \%$ in the Standard-IE and OpenIE settings. The above results demonstrate that ChatGPT gives very high-quality explanation for its prediction. Secondly, we observe that ChatGPT displays a high level of confidence in the reasons provided for its predictions when compared with human evaluation. In fact, ChatGPT achieves nearly a $100 \%$ reasonable score among almost all datasets. This suggests that ChatGPT is very confident in its ability to provide reasonable explanations. Thirdly, we find that when ChatGPT provides a reasonable explanation for a prediction, there is a high level of agreement between ChatGPT and human evaluations. This suggests that ChatGPT may have a similar understanding of explanations as humans. Overall, our findings suggest that ChatGPT is capable of providing high-quality and reliable explanations for its predictions. This is a crucial step towards developing trustworthy and reliable LLMs.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th></th>
<th>Correct Confidence</th>
<th></th>
<th></th>
<th>Incorrect Confidence</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>BERT</td>
<td>RoBERTa</td>
<td>ChatGPT</td>
<td>BERT</td>
<td>RoBERTa</td>
<td>ChatGPT</td>
</tr>
<tr>
<td>BBN(ET)</td>
<td>0.971</td>
<td>0.968</td>
<td>0.888</td>
<td>0.904</td>
<td>0.885</td>
<td>0.828</td>
</tr>
<tr>
<td>CoNLL(NER)</td>
<td>0.990</td>
<td>0.991</td>
<td>0.864</td>
<td>0.866</td>
<td>0.886</td>
<td>0.785</td>
</tr>
<tr>
<td>SemEval(RC)</td>
<td>0.983</td>
<td>0.989</td>
<td>0.868</td>
<td>0.871</td>
<td>0.852</td>
<td>0.839</td>
</tr>
<tr>
<td>ACE05-R(RE)</td>
<td>0.995</td>
<td>0.991</td>
<td>0.760</td>
<td>0.883</td>
<td>0.810</td>
<td>0.764</td>
</tr>
<tr>
<td>ACE05-E(ED)</td>
<td>0.882</td>
<td>0.944</td>
<td>0.852</td>
<td>0.770</td>
<td>0.871</td>
<td>0.737</td>
</tr>
<tr>
<td>ACE05-E(EAE)</td>
<td>0.762</td>
<td>0.785</td>
<td>0.956</td>
<td>0.525</td>
<td>0.555</td>
<td>0.910</td>
</tr>
<tr>
<td>ACE05-E(EE)</td>
<td>0.763</td>
<td>0.782</td>
<td>0.845</td>
<td>0.612</td>
<td>0.628</td>
<td>0.764</td>
</tr>
</tbody>
</table>
<p>Table 6: The prediction confidence of various models on the whole test set. We show both the correct confidence and incorrect confidence based on various methods. We find that ChatGPT is overconfidence for its prediction in most cases.</p>
<h3>5.2 Calibration</h3>
<p>In this subsection, we first investigate the level of confidence for both the correct and incorrect samples. Confidence is typically described in terms of a probability value, indicating the likelihood of belonging to a specific category. To obtain prediction probabilities from ChatGPT, we ask it to output the probability (Confidence_Standard and Confidence_Open), as discussed in § 3.3. Our aim is to investigate whether ChatGPT can provide a reasonable prediction confidence scores for its predictions, thus reducing the risk of misinterpretation. In Table 6, we present the confidence scores of correct and incorrect predictions from different models, referred to correct confidence and incorrect confidence, respectively. Our observations reveal that all the models exhibit high confidence levels in their predictions, this is consistent with previous research on large models (Guo et al., 2017). Although ChatGPT performs worse than its BERT-based counterparts in Standard-IE setting, it displays overconfidence in both correct and incorrect predictions. Consequently, this overconfidence could lead to misguidance of users. Furthermore, we note a significant confidence gap between correct and incorrect predictions, indicating the need for careful evaluation when ChatGPT's prediction has relatively low confidence.</p>
<p>We then focus on calibration, a critical property of LLMs as it could estimate the predictive uncertainty for the secure application of LLMs. A well-calibrated model not only produces accurate predictions but also provides reliable and informative uncertainty estimates, necessary for sound decision-making. In this research, we evaluate the calibration using the Expected Calibration Error (ECE) metric which measures the deviation be-</p>
<table>
<thead>
<tr>
<th style="text-align: right;"></th>
<th style="text-align: center;">BERT</th>
<th style="text-align: center;">RoBERTa</th>
<th style="text-align: center;">ChatGPT</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: right;">BBN(ET)</td>
<td style="text-align: center;">0.012</td>
<td style="text-align: center;">0.012</td>
<td style="text-align: center;">0.026</td>
</tr>
<tr>
<td style="text-align: right;">CoNLL(NER)</td>
<td style="text-align: center;">0.052</td>
<td style="text-align: center;">0.044</td>
<td style="text-align: center;">0.204</td>
</tr>
<tr>
<td style="text-align: right;">SemEval(RC)</td>
<td style="text-align: center;">0.023</td>
<td style="text-align: center;">0.031</td>
<td style="text-align: center;">0.460</td>
</tr>
<tr>
<td style="text-align: right;">ACE05-R(RE)</td>
<td style="text-align: center;">0.020</td>
<td style="text-align: center;">0.014</td>
<td style="text-align: center;">0.745</td>
</tr>
<tr>
<td style="text-align: right;">ACE05-E(ED)</td>
<td style="text-align: center;">0.161</td>
<td style="text-align: center;">0.226</td>
<td style="text-align: center;">0.656</td>
</tr>
<tr>
<td style="text-align: right;">ACE05-E(EAE)</td>
<td style="text-align: center;">0.154</td>
<td style="text-align: center;">0.168</td>
<td style="text-align: center;">0.699</td>
</tr>
<tr>
<td style="text-align: right;">ACE05-E(EE)</td>
<td style="text-align: center;">0.211</td>
<td style="text-align: center;">0.288</td>
<td style="text-align: center;">0.699</td>
</tr>
</tbody>
</table>
<p>Table 7: The expected calibration error (ECE) is used to measure the calibration of a given model, and the lower, the better. Results are calculated on the whole test set.
tween predicted confidence and accuracy. ${ }^{8}$ The results are shown in Table 7, and from that we can observe that ChatGPT shows much poorer calibration compared to BERT-based methods, which indicates that ChatGPT tends to produce confidences that do not represent true probabilities easily. Furthermore, although ChatGPT displays low ECE in tasks such as ET and NER, miscalibration phenomenon dominates most cases. These findings suggest that ChatGPT needs improvement in terms of calibration, especially for IE tasks.</p>
<h3>5.3 Faithfulness</h3>
<p>Recent works show that ChatGPT may provide false information to users, potentially affecting their decision-making (Huang et al., 2023). Therefore, assessing the faithfulness of the ChatGPT model to the original text is a crucial measurement in developing a trustworthy information extraction model. Our study uses faithfulness as a metric to evaluate the ChatGPT model, specifi-</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Stardand-IE</th>
<th style="text-align: center;">OpenIE</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">BBN(ET)</td>
<td style="text-align: center;">$98.3 \%$</td>
<td style="text-align: center;">$99.3 \%$</td>
</tr>
<tr>
<td style="text-align: center;">CoNLL(NER)</td>
<td style="text-align: center;">$100.0 \%$</td>
<td style="text-align: center;">$98.7 \%$</td>
</tr>
<tr>
<td style="text-align: center;">SemEval(RC)</td>
<td style="text-align: center;">$100.0 \%$</td>
<td style="text-align: center;">$99.1 \%$</td>
</tr>
<tr>
<td style="text-align: center;">ACE05-R(RE)</td>
<td style="text-align: center;">$90.0 \%$</td>
<td style="text-align: center;">$93.8 \%$</td>
</tr>
<tr>
<td style="text-align: center;">ACE05-E(ED)</td>
<td style="text-align: center;">$100.0 \%$</td>
<td style="text-align: center;">$100.0 \%$</td>
</tr>
<tr>
<td style="text-align: center;">ACE05-E(EAE)</td>
<td style="text-align: center;">$100.0 \%$</td>
<td style="text-align: center;">$96.5 \%$</td>
</tr>
<tr>
<td style="text-align: center;">ACE05-E(EE)</td>
<td style="text-align: center;">$100.0 \%$</td>
<td style="text-align: center;">$97.0 \%$</td>
</tr>
</tbody>
</table>
<p>Table 8: The evaluation of faithfulness for ChatGPT. Faithfulness refers to whether ChatGPT's explanation align with the original text. Experimental results show that ChatGPT's explanation maintains a very high degree of faithfulness to the original text and provide nearly no false explanation.
cally referring to if the explanation provided by ChatGPT aligns with the original text when its predictionis correct, as the original text is the most important source for extraction information. There are two keys we collect by domain experts, namely FicR_Standard (Manual) and FicR_Open(Manual), as we mentioned in $\S 3.3$. Our results are shown in Table 8, which indicate a high degree of faithfulness between ChatGPT's explanations and the original text with rare false explanations, i.e., with over $95 \%$ of samples considered faithful in nearly all datasets under different settings. We can conclude that ChatGPT's decision-making process primarily relies on the input of the original text, leading to the majority of its explanations being regarded as truthful and reliable.</p>
<h2>6 Conclusion</h2>
<p>In this paper, we propose to systematically analysis the ChatGPT's performance, explainability, calibration, and faithfulness. To be specific, based on 7 fine-grained information extraction tasks among 14 datasets, we collect 15 keys identified by either ChatGPT or domain experts for our research. Our findings reveal that ChatGPT's performance in Standard-IE settings is not as good as BERTbased models in most cases. However, we found that ChatGPT achieved excellent accuracy scores in the OpenIE setting, as evaluated by human annotators. Furthermore, ChatGPT could provide high-quality and trustworthy explanations for its predictions. One of the key issues that we identified is its tendency towards overconfidence, resulting in low calibration. Furthermore, our analysis also showed that ChatGPT exhibits a high level of faithfulness to the original text, indicating that its predictions are grounded in the input text. Given these findings, we hope that our research could inspire more research on using ChatGPT for information extraction.</p>
<h2>References</h2>
<p>Armen Aghajanyan, Sonal Gupta, and Luke Zettlemoyer. 2021. Intrinsic dimensionality explains the effectiveness of language model fine-tuning. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, pages 7319-7328. Association for Computational Linguistics.</p>
<p>Rachith Aiyappa, Jisun An, Haewoon Kwak, and YongYeol Ahn. 2023. Can we trust the evaluation on chatgpt? arXiv preprint arXiv:2303.12767.</p>
<p>Peggy M. Andersen, Philip J. Hayes, Steven P. Weinstein, Alison K. Huettner, Linda M. Schmandt, and Irene B. Nirenburg. 1992. Automatic extraction of facts from press releases to generate news stories. In 3rd Applied Natural Language Processing Conference, ANLP 1992, Trento, Italy, March 31 - April 3, 1992, pages 170-177. ACL.</p>
<p>Mikel Artetxe, Shruti Bhosale, Naman Goyal, Todor Mihaylov, Myle Ott, Sam Shleifer, Xi Victoria Lin, Jingfei Du, Srinivasan Iyer, Ramakanth Pasunuru, Giridharan Anantharaman, Xian Li, Shuohui Chen, Halil Akin, Mandeep Baines, Louis Martin, Xing Zhou, Punit Singh Koura, Brian O’Horo, Jeffrey Wang, Luke Zettlemoyer, Mona T. Diab, Zornitsa Kozareva, and Veselin Stoyanov. 2022. Efficient large scale language modeling with mixtures of experts. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pages 11699-11732. Association for Computational Linguistics.</p>
<p>Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, Quyet V. Do, Yan Xu, and Pascale Fung. 2023. A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. CoRR, abs/2302.04023.</p>
<p>Zeljana Basic, Ana Banovac, Ivana Kruzic, and Ivan Jerkovic. 2023. Better by you, better than me, chatgpt3 as writing assistance in students essays. CoRR, abs/2302.04536.</p>
<p>Junyi Bian, Li Huang, Xiaodi Huang, Hong Zhou, and Shanfeng Zhu. 2021. Grantrel: Grant information extraction via joint entity and relation extraction. In</p>
<p>Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021, Online Event, August 1-6, 2021, volume ACL/IJCNLP 2021 of Findings of ACL, pages 2674-2685. Association for Computational Linguistics.</p>
<p>Ning Bian, Xianpei Han, Le Sun, Hongyu Lin, Yaojie Lu , and Ben He. 2023. Chatgpt is a knowledgeable but inexperienced solver: An investigation of commonsense problem in large language models. arXiv preprint arXiv:2303.16421.</p>
<p>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.</p>
<p>Yi Chen, Jiayang Cheng, Haiyun Jiang, Lemao Liu, Haisong Zhang, Shuming Shi, and Ruifeng Xu. 2022. Learning from sibling mentions with scalable graph inference in fine-grained entity typing. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 2227, 2022, pages 2076-2087. Association for Computational Linguistics.</p>
<p>Hai Leong Chieu, Hwee Tou Ng, and Yoong Keok Lee. 2003. Closing the gap: Learning-based information extraction rivaling knowledge-engineering methods. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 216223, Sapporo, Japan. Association for Computational Linguistics.</p>
<p>Eunsol Choi, Omer Levy, Yejin Choi, and Luke Zettlemoyer. 2018. Ultra-fine entity typing. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018, Melbourne, Australia, July 15-20, 2018, Volume 1: Long Papers, pages 87-96. Association for Computational Linguistics.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier</p>
<p>Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022. Palm: Scaling language modeling with pathways. CoRR, abs/2204.02311.</p>
<p>Paul F. Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei. 2017. Deep reinforcement learning from human preferences. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 4299-4307.</p>
<p>Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416.</p>
<p>Antonia Creswell and Murray Shanahan. 2022. Faithful reasoning using large language models. CoRR, abs/2208.14271.</p>
<p>Jeremy Crowe. 1995. Constraint-based event recognition for information extraction. In 33rd Annual Meeting of the Association for Computational Linguistics, pages 296-298, Cambridge, Massachusetts, USA. Association for Computational Linguistics.</p>
<p>Hongliang Dai, Yangqiu Song, and Haixun Wang. 2021. Ultra-fine entity typing with weak supervision from a masked language model. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, pages 17901799. Association for Computational Linguistics.</p>
<p>Sarkar Snigdha Sarathi Das, Arzoo Katiyar, Rebecca J. Passonneau, and Rui Zhang. 2022. Container: Fewshot named entity recognition via contrastive learning. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, pages 6338-6353. Association for Computational Linguistics.</p>
<p>Ameet Deshpande, Vishvak Murahari, Tanmay Rajpurohit, Ashwin Kalyan, and Karthik Narasimhan. 2023. Toxicity in chatgpt: Analyzing personaassigned language models. arXiv preprint arXiv:2304.05335.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: pre-training of</p>
<p>deep bidirectional transformers for language understanding. In NAACL-HLT 2019, pages 4171-4186. Association for Computational Linguistics.</p>
<p>Xinya Du and Claire Cardie. 2020. Event extraction by answering (almost) natural questions. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, pages 671-683. Association for Computational Linguistics.</p>
<p>Xinya Du and Heng Ji. 2022. Retrieval-augmented generative question answering for event argument extraction. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pages 4649-4666. Association for Computational Linguistics.</p>
<p>Simon Frieder, Luca Pinchetti, Ryan-Rhys Griffiths, Tommaso Salvatori, Thomas Lukasiewicz, Philipp Christian Petersen, Alexis Chevalier, and Julius Berner. 2023. Mathematical capabilities of chatgpt. CoRR, abs/2301.13867.</p>
<p>Tsu-Jui Fu, Peng-Hsuan Li, and Wei-Yun Ma. 2019. Graphrel: Modeling text as relational graphs for joint entity and relation extraction. In Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, pages 1409-1418. Association for Computational Linguistics.</p>
<p>Yao Fu, Hao Peng, and Tushar Khot. 2022. How does gpt obtain its ability? tracing emergent abilities of language models to their sources. Yao Fu's Notion.</p>
<p>Jun Gao, Huan Zhao, Changlong Yu, and Ruifeng Xu. 2023. Exploring the feasibility of chatgpt for event extraction. CoRR, abs/2303.03836.</p>
<p>Dan Gillick, Nevena Lazic, Kuzman Ganchev, Jesse Kirchner, and David Huynh. 2014. Contextdependent fine-grained entity type tagging. CoRR, abs/1412.1820.</p>
<p>Andrej Zukov Gregoric, Yoram Bachrach, and Sam Coope. 2018. Named entity recognition with parallel recurrent neural networks. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018, Melbourne, Australia, July 15-20, 2018, Volume 2: Short Papers, pages 69-74. Association for Computational Linguistics.</p>
<p>Biyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang, Jinran Nie, Yuxuan Ding, Jianwei Yue, and Yupeng Wu. 2023. How close is chatgpt to human experts? comparison corpus, evaluation, and detection. CoRR, abs/2301.07597.</p>
<p>Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. 2017. On calibration of modern neural networks. In Proceedings of the 34th International Conference on Machine Learning, ICML</p>
<p>2017, Sydney, NSW, Australia, 6-11 August 2017, volume 70 of Proceedings of Machine Learning Research, pages 1321-1330. PMLR.</p>
<p>Mubin Ul Haque, Isuru Dharmadasa, Zarrin Tasnim Sworna, Roshan Namal Rajapakse, and Hussain Ahmad. 2022. "i think this is the most disruptive technology": Exploring sentiments of chatgpt early adopters using twitter data. CoRR, abs/2212.05856.</p>
<p>Hangfeng He, Hongming Zhang, and Dan Roth. 2023. Rethinking with retrieval: Faithful large language model inference. CoRR, abs/2301.00303.</p>
<p>Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, Preslav Nakov, Diarmuid Ó Séaghdha, Sebastian Padó, Marco Pennacchiotti, Lorenza Romano, and Stan Szpakowicz. 2010. Semeval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals. In Proceedings of the 5th International Workshop on Semantic Evaluation, SemEval@ACL 2010, Uppsala University, Uppsala, Sweden, July 15-16, 2010, pages 33-38. The Association for Computer Linguistics.</p>
<p>I-Hung Hsu, Kuan-Hao Huang, Elizabeth Boschee, Scott Miller, Prem Natarajan, Kai-Wei Chang, and Nanyun Peng. 2022. DEGREE: A data-efficient generation-based event extraction model. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2022, Seattle, WA, United States, July 1015, 2022, pages 1890-1908. Association for Computational Linguistics.</p>
<p>Fan Huang, Haewoon Kwak, and Jisun An. 2023. Is chatgpt better than human annotators? potential and limitations of chatgpt in explaining implicit hate speech. CoRR, abs/2302.07736.</p>
<p>Katharina Jeblick, Balthasar Schachtner, Jakob Dexl, Andreas Mittermeier, Anna Theresa Stüber, Johanna Topalis, Tobias Weber, Philipp Wesp, Bastian Sabel, Jens Ricke, and Michael Ingrisch. 2022. Chatgpt makes medicine easy to swallow: An exploratory case study on simplified radiology reports. CoRR, abs/2212.14882.</p>
<p>Wenxiang Jiao, Wenxuan Wang, Jen-tse Huang, Xing Wang, and Zhaopeng Tu. 2023. Is chatgpt A good translator? A preliminary study. CoRR, abs/2301.08745.</p>
<p>Tushar Khot, Ashish Sabharwal, and Peter Clark. 2017. Answering complex questions using open information extraction. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 311-316, Vancouver, Canada. Association for Computational Linguistics.</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. CoRR, abs/2205.11916.</p>
<p>Gerd Kortemeyer. 2023. Could an artificialintelligence agent pass an introductory physics course? arXiv preprint arXiv:2301.12127.</p>
<p>Fajri Koto, Jey Han Lau, and Timothy Baldwin. 2022. Can pretrained language models generate persuasive, faithful, and informative ad text for product descriptions? In Proceedings of The Fifth Workshop on e-Commerce and NLP (ECNLP 5), pages 234-243.</p>
<p>Sebastian Krügel, Andreas Ostermaier, and Matthias Uhl. 2023. The moral authority of chatgpt. CoRR, abs/2301.07098.</p>
<p>Ananya Kumar, Percy Liang, and Tengyu Ma. 2019. Verified uncertainty calibration. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 3787-3798.</p>
<p>Bo Li, Wei Ye, Jinglei Zhang, and Shikun Zhang. 2022a. Reviewing labels: Label graph network with top-k prediction set for relation extraction. CoRR, abs/2212.14270.</p>
<p>Bo Li, Dingyao Yu, Wei Ye, Jinglei Zhang, and Shikun Zhang. 2022b. Sequence generation with label augmentation for relation extraction. CoRR, abs/2212.14266.</p>
<p>Xiaoya Li, Jingrong Feng, Yuxian Meng, Qinghong Han, Fei Wu, and Jiwei Li. 2020. A unified MRC framework for named entity recognition. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 58495859, Online. Association for Computational Linguistics.</p>
<p>Xiaoya Li, Fan Yin, Zijun Sun, Xiayu Li, Arianna Yuan, Duo Chai, Mingxin Zhou, and Jiwei Li. 2019. Entity-relation extraction as multi-turn question answering. In Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, pages 1340-1350. Association for Computational Linguistics.</p>
<p>Ying Lin, Heng Ji, Fei Huang, and Lingfei Wu. 2020. A joint neural model for information extraction with global features. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 7999-8009. Association for Computational Linguistics.</p>
<p>Jian Liu, Yufeng Chen, and Jinan Xu. 2022a. Saliency as evidence: Event detection with trigger saliency attribution. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, pages 4573-4585. Association for Computational Linguistics.</p>
<p>Xiao Liu, Heyan Huang, Ge Shi, and Bo Wang. 2022b. Dynamic prefix-tuning for generative template-based event extraction. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, pages 5216-5228. Association for Computational Linguistics.</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized BERT pretraining approach. CoRR, abs/1907.11692.</p>
<p>Dongfang Lou, Zhilin Liao, Shumin Deng, Ningyu Zhang, and Huajun Chen. 2021. Mibinet: A crosssentence collective event detection network. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, pages 4829-4839. Association for Computational Linguistics.</p>
<p>Yaojie Lu, Qing Liu, Dai Dai, Xinyan Xiao, Hongyu Lin, Xianpei Han, Le Sun, and Hua Wu. 2022. Unified structure generation for universal information extraction. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 5755-5772, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Yi Luan, Luheng He, Mari Ostendorf, and Hannaneh Hajishirzi. 2018. Multi-task identification of entities, relations, and coreference for scientific knowledge graph construction. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 November 4, 2018, pages 3219-3232. Association for Computational Linguistics.</p>
<p>Yubo Ma, Zehao Wang, Yixin Cao, Mukai Li, Meiqi Chen, Kun Wang, and Jing Shao. 2022. Prompt for extraction? PAIE: prompting argument interaction for event argument extraction. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, pages 6759-6774. Association for Computational Linguistics.</p>
<p>Kyle Mahowald, Anna A. Ivanova, Idan Asher Blank, Nancy Kanwisher, Joshua B. Tenenbaum, and Evelina Fedorenko. 2023. Dissociating language and thought in large language models: a cognitive perspective. CoRR, abs/2301.06627.</p>
<p>Pedro Henrique Martins, Zita Marinho, and André F. T. Martins. 2019. Joint learning of named entity recognition and entity linking. In Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28 - August 2, 2019, Volume 2: Student Research Workshop,</p>
<p>pages 190-196. Association for Computational Linguistics.</p>
<p>Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan T. McDonald. 2020. On faithfulness and factuality in abstractive summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 1906-1919. Association for Computational Linguistics.</p>
<p>Matthias Minderer, Josip Djolonga, Rob Romijnders, Frances Hubis, Xiaohua Zhai, Neil Houlsby, Dustin Tran, and Mario Lucic. 2021. Revisiting the calibration of modern neural networks. In Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pages 15682-15694.</p>
<p>Sandra Mitrovic, Davide Andreoletti, and Omran Ayoub. 2023. Chatgpt or human? detect and explain. explaining decisions of machine learning model for detecting short chatgpt-generated text. CoRR, abs/2301.13852.</p>
<p>Oded Nov, Nina Singh, and Devin M. Mann. 2023. Putting chatgpt's medical advice to the (turing) test. CoRR, abs/2301.10035.</p>
<p>Miguel Ortega-Martín, Óscar García-Sierra, Alfonso Ardoiz, Jorge Álvarez, Juan Carlos Armenteros, and Adrián Alonso. 2023. Linguistic ambiguity analysis in chatgpt. arXiv preprint arXiv:2302.06426.</p>
<p>Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. CoRR, abs/2203.02155.</p>
<p>Kunyuan Pang, Haoyu Zhang, Jie Zhou, and Ting Wang. 2022. Divide and denoise: Learning from noisy labels in fine-grained entity typing with cluster-wise loss correction. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, pages 1997-2006. Association for Computational Linguistics.</p>
<p>Giovanni Paolini, Ben Athiwaratkun, Jason Krone, Jie Ma, Alessandro Achille, Rishita Anubhai, Cícero Nogueira dos Santos, Bing Xiang, and Stefano Soatto. 2021. Structured prediction as translation between augmented natural languages. In $I C L R$ 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net.</p>
<p>Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou Yu, Weizhu Chen, and Jianfeng Gao.
2023. Check your facts and try again: Improving large language models with external knowledge and automated feedback. CoRR, abs/2302.12813.</p>
<p>Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, and Diyi Yang. 2023. Is chatgpt a general-purpose natural language processing task solver? arXiv preprint arXiv:2302.06476.</p>
<p>Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, H. Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant M. Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, JeanBaptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d'Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew J. Johnson, Blake A. Hechtman, Laura Weidinger, Iason Gabriel, William S. Isaac, Edward Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving. 2021. Scaling language models: Methods, analysis \&amp; insights from training gopher. CoRR, abs/2112.11446.</p>
<p>Nazneen Fatema Rajani, Bryan McCann, Caiming Xiong, and Richard Socher. 2019. Explain yourself! leveraging language models for commonsense reasoning. In Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, pages 4932-4942. Association for Computational Linguistics.</p>
<p>Erik F. Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the conll-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Conference on Natural Language Learning, CoNLL 2003, Held in cooperation with HLT-NAACL 2003, Edmonton, Canada, May 31 - June 1, 2003, pages 142-147. ACL.</p>
<p>Shibani Santurkar, Esin Durmus, Faisal Ladhak, Cinoo Lee, Percy Liang, and Tatsunori Hashimoto. 2023. Whose opinions do language models reflect? arXiv preprint arXiv:2303.17548.</p>
<p>Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared</p>
<p>Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, Elton Zheng, Rewon Child, Reza Yazdani Aminabadi, Julie Bernauer, Xia Song, Mohammad Shoeybi, Yuxiong He, Michael Houston, Saurabh Tiwary, and Bryan Catanzaro. 2022. Using deepspeed and megatron to train megatron-turing NLG 530b, A large-scale generative language model. CoRR, abs/2201.11990.</p>
<p>Teo Susnjak. 2022. Chatgpt: The end of online exam integrity? CoRR, abs/2212.09292.</p>
<p>Teo Susnjak. 2023. Applying bert and chatgpt for sentiment analysis of lyme disease in scientific literature. arXiv preprint arXiv:2302.06474.</p>
<p>Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Yanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett, Kathleen S. Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak, Ed H. Chi, and Quoc Le. 2022. Lamda: Language models for dialog applications. CoRR, abs/2201.08239.</p>
<p>Sunil Thulasidasan, Gopinath Chennupati, Jeff A. Bilmes, Tanmoy Bhattacharya, and Sarah Michalak. 2019. On mixup training: Improved calibration and predictive uncertainty for deep neural networks. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 814, 2019, Vancouver, BC, Canada, pages 1388813899 .</p>
<p>Ruibo Tu, Chao Ma, and Cheng Zhang. 2023. Causaldiscovery performance of chatgpt in the context of neuropathic pain diagnosis. CoRR, abs/2301.13819.</p>
<p>Amir Pouran Ben Veyseh, Viet Dac Lai, Franck Dernoncourt, and Thien Huu Nguyen. 2021. Unleash GPT-2 power for event detection. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, pages 6271-6282. Association for Computational Linguistics.</p>
<p>David Wadden, Ulme Wennberg, Yi Luan, and Hannaneh Hajishirzi. 2019. Entity, relation, and event extraction with contextualized span representations.</p>
<p>In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, pages 57835788. Association for Computational Linguistics.</p>
<p>Jindong Wang, Xixu Hu, Wenxin Hou, Hao Chen, Runkai Zheng, Yidong Wang, Linyi Yang, Haojun Huang, Wei Ye, Xiubo Geng, Binxing Jiao, Yue Zhang, and Xing Xie. 2023a. On the robustness of chatgpt: An adversarial and out-of-distribution perspective. CoRR, abs/2302.12095.</p>
<p>Xiao Wang, Weikang Zhou, Can Zu, Han Xia, Tianze Chen, Yuansen Zhang, Rui Zheng, Junjie Ye, Qi Zhang, Tao Gui, Jihua Kang, Jingsheng Yang, Siyuan Li, and Chunsai Du. 2023b. Instructuie: Multi-task instruction tuning for unified information extraction. CoRR, abs/2304.08085.</p>
<p>Xinyu Wang, Yong Jiang, Nguyen Bach, Tao Wang, Zhongqiang Huang, Fei Huang, and Kewei Tu. 2021. Automated concatenation of embeddings for structured prediction. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, pages 2643-2660. Association for Computational Linguistics.</p>
<p>Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, Eshaan Pathak, Giannis Karamanolakis, Hatzhi Gary Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi, Kuntal Kumar Pal, Maitreya Patel, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, Rushang Karia, Savan Doshi, Shailaja Keyur Sampat, Siddhartha Mishra, Sujan Reddy A, Sumanta Patro, Tanay Dixit, and Xudong Shen. 2022. Super-naturalinstructions: Generalization via declarative instructions on 1600+ NLP tasks. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pages 5085-5109. Association for Computational Linguistics.</p>
<p>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. 2022a. Emergent abilities of large language models. CoRR, abs/2206.07682.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed H. Chi, Quoc Le, and Denny Zhou. 2022b. Chain of thought prompting elicits reasoning in large language models. CoRR, abs/2201.11903.</p>
<p>Xiang Wei, Xingyu Cui, Ning Cheng, Xiaobin Wang, Xin Zhang, Shen Huang, Pengjun Xie, Jinan Xu, Yufeng Chen, Meishan Zhang, Yong Jiang, and Wenjuan Han. 2023. Zero-shot information extraction via chatting with chatgpt. CoRR, abs/2302.10205.</p>
<p>Ralph Weischedel and Ada Brunstein. 2005. Bbn pronoun coreference and entity type corpus. Linguistic Data Consortium, Philadelphia, 112.</p>
<p>Fei Wu and Daniel S. Weld. 2010. Open information extraction using Wikipedia. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 118-127, Uppsala, Sweden. Association for Computational Linguistics.</p>
<p>Ikuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki Takeda, and Yuji Matsumoto. 2020. LUKE: deep contextualized entity representations with entityaware self-attention. In EMNLP 2020.</p>
<p>Deming Ye, Yankai Lin, Peng Li, and Maosong Sun. 2022. Packed levitated marker for entity and relation extraction. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, pages 4904-4917. Association for Computational Linguistics.</p>
<p>Wei Ye, Bo Li, Rui Xie, Zhonghao Sheng, Long Chen, and Shikun Zhang. 2019. Exploiting entity BIO tag embeddings and multi-task learning for relation extraction with imbalanced data. In Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, pages 1351-1360. Association for Computational Linguistics.</p>
<p>Daojian Zeng, Kang Liu, Yubo Chen, and Jun Zhao. 2015. Distant supervision for relation extraction via piecewise convolutional neural networks. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 17531762 .</p>
<p>Bowen Zhang, Daijun Ding, and Liwen Jing. 2022a. How would stance detection techniques evolve after the launch of chatgpt? CoRR, abs/2212.14548.</p>
<p>Yuhao Zhang, Victor Zhong, Danqi Chen, Gabor Angeli, and Christopher D. Manning. 2017. Positionaware attention and supervised data improve slot filling. In EMNLP 2017.</p>
<p>Zhisong Zhang, Emma Strubell, and Eduard H. Hovy. 2022b. Transfer learning from semantic role labeling to event argument extraction with templatebased slot querying. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pages 2627-2647. Association for Computational Linguistics.</p>
<p>Kailin Zhao, Xiaolong Jin, Long Bai, Jiafeng Guo, and Xueqi Cheng. 2022. Knowledge-enhanced selfsupervised prototypical network for few-shot event detection. In Findings of the Association for Computational Linguistics: EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pages 6266-6275. Association for Computational Linguistics.</p>
<p>Kang Zhao, Hua Xu, Yue Cheng, Xiaoteng Li, and Kai Gao. 2021. Representation iterative fusion based on heterogeneous graph neural network for joint entity and relation extraction. Knowl. Based Syst., 219:106888.</p>
<p>Qihuang Zhong, Liang Ding, Juhua Liu, Bo Du, and Dacheng Tao. 2023. Can chatgpt understand too? a comparative study on chatgpt and fine-tuned bert. arXiv preprint arXiv:2302.10198.</p>
<p>Wenxuan Zhou and Muhao Chen. 2021. An improved baseline for sentence-level relation extraction. CoRR, abs/2102.01373.</p>
<p>Terry Yue Zhuo, Yujin Huang, Chunyang Chen, and Zhenchang Xing. 2023. Exploring AI ethics of chatgpt: A diagnostic analysis. CoRR, abs/2301.12867.</p>
<p>Julia El Zini and Mariette Awad. 2023. On the explainability of natural language processing deep models. ACM Comput. Surv., 55(5):103:1-103:31.</p>
<p>Xinyu Zuo, Haijin Liang, Ning Jing, Shuang Zeng, Zhou Fang, and Yu Luo. 2022. Type-enriched hierarchical contrastive strategy for fine-grained entity typing. In Proceedings of the 29th International Conference on Computational Linguistics, COLING 2022, Gyeongju, Republic of Korea, October 12-17, 2022, pages 2405-2417. International Committee on Computational Linguistics.</p>
<table>
<thead>
<tr>
<th>Task</th>
<th>DataSet</th>
<th>#Ann.</th>
</tr>
</thead>
<tbody>
<tr>
<td>ED</td>
<td>BBN</td>
<td>385</td>
</tr>
<tr>
<td>NER</td>
<td>CoNNL</td>
<td>300</td>
</tr>
<tr>
<td>RC</td>
<td>SemEval</td>
<td>400</td>
</tr>
<tr>
<td>RE</td>
<td>ACE05-R</td>
<td>66</td>
</tr>
<tr>
<td>ED</td>
<td>ACE05-E</td>
<td>218</td>
</tr>
<tr>
<td>EAE</td>
<td>ACE05-E</td>
<td>313</td>
</tr>
<tr>
<td>EE</td>
<td>ACE05-E</td>
<td>552</td>
</tr>
</tbody>
</table>
<p>Table 9: The number of manually annotated samples for each dataset.</p>
<h2>A Appendix</h2>
<h3>A.1 Dataset</h3>
<p>We report the dataset used in each task in this subsection. For each task, we use two commonly used datasets for the evaluation. Table 10 shows the detailed statistical information.</p>
<p>Besides, we also report the number of manually annotated samples for each dataset, denoted as #Ann., as shown in Table 9.</p>
<p>Table 9: The number of manually annotated samples for each dataset.</p>
<h3>A.2 The State-of-the-Art Methods on Single Dataset</h3>
<p>In this section, we introduce the state-of-the-art method on each dataset:</p>
<p>Entity Typing (ET): <em>Zuo et al. (2022)</em> proposed a type-enriched hierarchical contrastive strategy for entity typing task, named PICOT. PICOT models differences between hierarchical types to distinguish similar types at different levels of granularity. It also embeds type information into entity contexts and employ a constrained contrastive strategy on the hierarchical structure. This method achieves SOTA results on the BBN and OntoNotes 5.0 datasets.</p>
<p>Named Entity Recognition (NER): <em>Wang et al. (2021)</em> proposed a model named ACE, which automates finding better embeddings for structured prediction tasks. It uses a neural architecture search-inspired formulation where a controller updates belief based on a reward. The reward is the accuracy of a task model trained on a task dataset with the concatenated embeddings as input. This method achieves SOTA results on the CoNLL2003 dataset.</p>
<p>Relation Classification (RC): <em>Li et al. (2022a)</em> proposed Label Graph Network with Top-k Prediction Set (KLG), to effectively utilize the Top-k prediction set. KLG builds a label graph for a given sample to review candidate labels in the Top-k prediction set and learns the connections between them. It also includes a dynamic k-selection mechanism to learn more powerful and discriminative relation representation. This method sets SOTA results on the TACRED dataset. <em>Zhao et al. (2021)</em> proposed RIFRE, which models relations and words as nodes on a graph, and iteratively fuses the two types of semantic nodes using message passing. This approach obtains node representations that are better suited for relation extraction tasks. The model then performs relation extraction on the updated node representations. This method sets SOTA results on the SemEval2010 dataset.</p>
<p>Relation Extraction (RE): <em>Ye et al. (2022)</em> proposed PL-Marker, a novel span representation approach that considers the interrelation between span pairs by packing markers in the encoder. To better model entity boundary information, PL-Marker proposes a neighborhood-oriented packing strategy that considers neighbor spans integrally. For more complicated span pair classification tasks, this paper also designs a subject-oriented packing strategy, which packs each subject and its objects to model the interrelation between same-subject span pairs. This method sets SOTA results on ACE05-R and SciERC datasets. It also achieves the best performance on the OntoNotes 5.0 datasets of NER task.</p>
<p>Event Detection (ED): <em>Liu et al. (2022a)</em> proposed SaliencyED, a novel training mechanism for ED, which can distinguish between trigger-dependent and context-dependent types, and achieves promising results on ACE05-E dataset. <em>Lin et al. (2020)</em> proposed ONEIE neural framework aims to globally optimize information extraction as a graph from an input sentence, capturing cross-subtask and cross-instance interdependencies, and and achieves promising results on ACE05-E+ dataset.</p>
<p>Event Argument Extraction (EAE): <em>Hsu et al. (2022)</em> proposed DEGREE, which formulates event extraction as a conditional generation problem, summarizing events mentioned in a passage into a natural sentence following a predefined pattern, as learned from a prompt. Extracted event predictions are then obtained from the generated sentence using a deterministic algorithm. DEGREE sets the best results on both ACE05-E and ACE05-E+ datasets.</p>
<p>Event Argument Extraction (EAE): the</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">#class</th>
<th style="text-align: center;">#test</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Entity</td>
<td style="text-align: center;">BBN (Weischedel and Brunstein, 2005)</td>
<td style="text-align: center;">17</td>
<td style="text-align: center;">23542</td>
</tr>
<tr>
<td style="text-align: center;">Typing(ET)</td>
<td style="text-align: center;">OntoNotes (Gillick et al., 2014)</td>
<td style="text-align: center;">41</td>
<td style="text-align: center;">13393</td>
</tr>
<tr>
<td style="text-align: center;">Named Entity</td>
<td style="text-align: center;">CoNLL 2003 (Sang and Meulder, 2003)</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">3453</td>
</tr>
<tr>
<td style="text-align: center;">Recognition(NER)</td>
<td style="text-align: center;">OntoNotes ${ }^{9}$</td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">8233</td>
</tr>
<tr>
<td style="text-align: center;">Relation</td>
<td style="text-align: center;">TACRED (Zhang et al., 2017)</td>
<td style="text-align: center;">42</td>
<td style="text-align: center;">15517</td>
</tr>
<tr>
<td style="text-align: center;">Classification(RC)</td>
<td style="text-align: center;">SemEval2010 (Hendrickx et al., 2010)</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">2717</td>
</tr>
<tr>
<td style="text-align: center;">Relation</td>
<td style="text-align: center;">ACE05-R ${ }^{10}$</td>
<td style="text-align: center;">$7 / 6$</td>
<td style="text-align: center;">2050</td>
</tr>
<tr>
<td style="text-align: center;">Extraction(RE)</td>
<td style="text-align: center;">SciERC (Luan et al., 2018)</td>
<td style="text-align: center;">$6 / 7$</td>
<td style="text-align: center;">551</td>
</tr>
<tr>
<td style="text-align: center;">Event</td>
<td style="text-align: center;">ACE05-E (Wadden et al., 2019)</td>
<td style="text-align: center;">33</td>
<td style="text-align: center;">832</td>
</tr>
<tr>
<td style="text-align: center;">Detection(ED)</td>
<td style="text-align: center;">ACE05-E ${ }^{+}$(Lin et al., 2020)</td>
<td style="text-align: center;">33</td>
<td style="text-align: center;">676</td>
</tr>
<tr>
<td style="text-align: center;">Event Argument</td>
<td style="text-align: center;">ACE05-E (Wadden et al., 2019)</td>
<td style="text-align: center;">22/33</td>
<td style="text-align: center;">403</td>
</tr>
<tr>
<td style="text-align: center;">Extraction(EAE)</td>
<td style="text-align: center;">ACE05-E ${ }^{+}$(Lin et al., 2020)</td>
<td style="text-align: center;">22/33</td>
<td style="text-align: center;">424</td>
</tr>
<tr>
<td style="text-align: center;">Event</td>
<td style="text-align: center;">ACE05-E (Wadden et al., 2019)</td>
<td style="text-align: center;">22/33</td>
<td style="text-align: center;">832</td>
</tr>
<tr>
<td style="text-align: center;">Extraction(EE)</td>
<td style="text-align: center;">ACE05-E ${ }^{+}$(Lin et al., 2020)</td>
<td style="text-align: center;">22/33</td>
<td style="text-align: center;">676</td>
</tr>
</tbody>
</table>
<p>Table 10: The table presents several key statistical characteristics of the datasets used in our research, including 14 datasets that belonging to 7 different IE tasks.</p>
<p>SOTA methods are ONEIE for ACE05-E, and DE-
GREE for ACE05-E+.</p>
<h1>A. 3 Exemplar of the Input</h1>
<p>In this section, we show an input examples for the event detection task to help readers understand our implement, as shown in Table 11.</p>
<p>Input of Event Detection (ED)
Task Description: Given an input list of words, identify all triggers in the list, and categorize each of them into the predefined set of event types. A trigger is the main word that most clearly expresses the occurrence of an event in the predefined set of event types.
Pre-defined Label Set: The predefined set of event types includes: [Life.Be-Born, Life.Marry, Life.Divorce, Life.Injure, Life.Die, Movement.Transport, Transaction.Transfer-Ownership, Transaction.Transfer-Money, Business.Start-Org, Business.Merge-Org, Business.DeclareBankruptcy, Business.End-Org, Conflict.Attack, Conflict.Demonstrate, Contact.Meet, Contact. Phone-Write, Personnel.Start-Position, Personnel.End-Position, Personnel.Nominate, Personnel. Elect, Justice.Arrest-Jail, Justice.Release-Parole, Justice.Trial-Hearing, Justice.Charge-Indict, Justice.Sue, Justice.Convict, Justice.Sentence, Justice.Fine, Justice.Execute, Justice.Extradite, Justice.Acquit, Justice.Appeal, Justice.Pardon].
Input and Task Requirement: Perform ED task for the following input list, and print the output: ['Putin', 'concluded', 'his', 'two', 'days', 'of', 'talks', 'in', 'Saint', 'Petersburg', 'with', 'Jacques', 'Chirac', 'of', 'France', 'and', 'German', 'Chancellor', 'Gerhard', 'Schroeder', 'on', 'Saturday', 'still', 'urging', 'for', 'a', 'central', 'role', 'for', 'the', 'United', 'Nations', 'in', 'a', 'post', '-', 'war', 'revival', 'of', 'Iraq', '.'] The output of ED task should be a list of dictionaries following json format. Each dictionary corresponds to the occurrence of an event in the input list and should consists of "trigger", "word_index", "event_type", "top3_event_type", "top5_event_type", "confidence", "if_context_dependent", "reason" and "if_reasonable" nine keys. The value of "word_ index" key is an integer indicating the index (start from zero) of the "trigger" in the input list. The value of "confidence" key is an integer ranging from 0 to 100, indicating how confident you are that the "trigger" expresses the "event_type" event. The value of "if_context_dependent" key is either 0 (indicating the event semantic is primarily expressed by the trigger rather than contexts) or 1 (indicating the event semantic is primarily expressed by contexts rather than the trigger). The value of "reason" key is a string describing the reason why the "trigger" expresses the "event_type", and do not use any " mark in this string. The value of "if_reasonable" key is either 0 (indicating the reason given in the "reason" field is not reasonable) or 1 (indicating the reason given in the "reason" field is reasonable). Note that your answer should only contain the json string and nothing else.</p>
<p>Table 11: The input example of event detection task. This example is extracted from ACE05-E, and all the above three parts are jointly imported into ChatGPT.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{8}$ We set the bin size to 50 , dividing the prediction probabilities into 50 equally spaced bins for analysis.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{6}$ We randomly selected around 200 samples for each dataset.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
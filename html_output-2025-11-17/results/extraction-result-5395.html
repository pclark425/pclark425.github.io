<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5395 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5395</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5395</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-110.html">extraction-schema-110</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <p><strong>Paper ID:</strong> paper-71f7bbfb36a0026825e17f3303e73f93876fc3e7</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/71f7bbfb36a0026825e17f3303e73f93876fc3e7" target="_blank">LLMs as Narcissistic Evaluators: When Ego Inflates Evaluation Scores</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> The results underscore that assessments provided by generative evaluation models can be influenced by factors beyond the inherent text quality, highlighting the necessity of developing more reliable evaluation protocols in the future.</p>
                <p><strong>Paper Abstract:</strong> Automatic evaluation of generated textual content presents an ongoing challenge within the field of NLP. Given the impressive capabilities of modern language models (LMs) across diverse NLP tasks, there is a growing trend to employ these models in creating innovative evaluation metrics for automated assessment of generation tasks. This paper investigates a pivotal question: Do language model-driven evaluation metrics inherently exhibit bias favoring texts generated by the same underlying language model? Specifically, we assess whether prominent LM-based evaluation metrics (e.g. BARTScore, T5Score, and GPTScore) demonstrate a favorable bias toward their respective underlying LMs in the context of summarization tasks. Our findings unveil a latent bias, particularly pronounced when such evaluation metrics are used in a reference-free manner without leveraging gold summaries. These results underscore that assessments provided by generative evaluation models can be influenced by factors beyond the inherent text quality, highlighting the necessity of developing more reliable evaluation protocols in the future.</p>
                <p><strong>Cost:</strong> 0.004</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5395",
    "paper_id": "paper-71f7bbfb36a0026825e17f3303e73f93876fc3e7",
    "extraction_schema_id": "extraction-schema-110",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.0042125,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>LLMs as Narcissistic Evaluators: When Ego Inflates Evaluation Scores</h1>
<p>Yiqi Liu ${ }^{1}$ Nafise Sadat Moosavi ${ }^{2}$ Chenghua Lin ${ }^{1}$
${ }^{1}$ University of Manchester ${ }^{2}$ University of Sheffield
yiqi.liu-6@postgrad.manchester.ac.uk n.s.moosavi@sheffield.ac.uk
chenghua.lin@manchester.ac.uk</p>
<h6>Abstract</h6>
<p>Automatic evaluation of generated textual content presents an ongoing challenge within the field of NLP. Given the impressive capabilities of modern language models (LMs) across diverse NLP tasks, there is a growing trend to employ these models in creating innovative evaluation metrics for automated assessment of generation tasks. This paper investigates a pivotal question: Do language model-driven evaluation metrics inherently exhibit bias favoring texts generated by the same underlying language model? Specifically, we assess whether prominent LM-based evaluation metrics (e.g. BARTScore, T5Score, and GPTScore) demonstrate a favorable bias toward their respective underlying LMs in the context of summarization tasks. Our findings unveil a latent bias, particularly pronounced when such evaluation metrics are used in a reference-free manner without leveraging gold summaries. These results underscore that assessments provided by generative evaluation models can be influenced by factors beyond the inherent text quality, highlighting the necessity of developing more reliable evaluation protocols in the future.</p>
<h2>1 Introduction</h2>
<p>Evaluation is a fundamental element in both tracking progress and ensuring meaningful advancements across various dimensions within the field of Natural Language Processing. Therefore, the reliability of evaluation metrics plays a critical role in this process. Evaluating generated texts is one of the challenging and open problems in NLP given that different forms can convey the same meaning. This challenge has led to the development of various evaluation metrics for tasks involving Natural Language Generation (NLG). While human evaluation by experts stands as the most reliable approach for assessing generated outputs, it is costly and time-consuming, limiting its broader use. As a result, automatic evaluation metrics have emerged
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Examining the inherent bias within generative evaluation metrics towards outputs created by their underlying model reveals a clear existence of this bias. Our analysis shows that these metrics tend to assign inflated scores to outputs generated by the very model they are based on.
as practical alternatives to keep pace with the rapid progress in NLP (van der Lee et al., 2019). Recent evaluation metrics for generation tasks, such as BERTScore (Zhang et al., 2020), BARTScore (Yuan et al., 2021), T5Score (Qin et al., 2022), GPTScore (Fu et al., 2023), and G-Eval (Liu et al., 2023), increasingly rely on pretrained language models. However, this trend poses a paradox, as the very outputs being evaluated are generated by these pretrained language models, raising concerns about inherent biases. For instance, an evaluation metric based on the BART model might yield inflated scores for outputs produced by a BART-based language model.</p>
<p>In this paper, we systematically investigate this potential bias, utilizing six prominent language models, namely BART (Lewis et al., 2020), T5 (Raffel et al., 2020), GPT-2 (Radford et al., 2019), GPT-3 (Brown et al., 2020), FLAN-T5 (Chung et al., 2022), and Cohere along with their corresponding evaluation metrics (e.g. BARTScore, T5Score, and GPTScore) or conditional generative probability, for the task of summarization, which is a typical task in natural language generations and frequently employed in automatic text evaluation. Our analysis involved examining numerous variations of these six families of generative mod-</p>
<p>els, considering their varying sizes and finetuning settings both as generators and evaluators.</p>
<p>We conducted our analysis using the CNN/Daily Mail (Hermann et al., 2015) and XSUM (Narayan et al., 2018) summarization datasets. The assessment covers two settings: reference-based, using gold summaries for evaluation (a common approach in supervised summarization), and reference-free, comparing generated summaries against source documents (a common approach in both unsupervised summarization and factuality assessment).</p>
<p>Based on our analysis, we have derived the following findings: (1) Generative evaluators tend to assign higher scores to the content generated by the same underlying model. This bias becomes more pronounced when the fine-tuning configuration and model size match for both the generator and evaluator. (2) Inflated scores are particularly noticeable in the reference-free setting, which is concerning due to the popularity of this evaluation approach for assessing the factual correctness of generated texts (Koh et al., 2022). (3) Apart from self-bias, inflated scores are also influenced by the preference for longer summaries by certain evaluators.</p>
<p>Our work has implications for model selection, evaluation strategies, and the development of more reliable and unbiased evaluation metrics in the field of natural language generation.</p>
<h2>2 Related Work</h2>
<p>Reference-based Evaluation Metrics
Reference-based metrics are commonly used to evaluate text generation tasks, including summarization, by measuring the similarity between generated and reference texts. Traditionally, metrics like BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) were employed to assess a generated text based on surface-level similarities, measured through the n-gram overlap between the generated and reference texts.</p>
<p>Recent trends in summarization evaluation lean towards semantic-level assessments, moving beyond direct word overlap comparisons. Notable metrics embracing this approach include BERTScore (Zhang et al., 2020), MoverScore (Zhao et al., 2019), BARTScore (Yuan et al., 2021), BLEURT (Sellam et al., 2020), and variations thereof. By leveraging pretrained language models, these metrics focus on capturing semantic content, providing a more nuanced and accurate evaluation of summarization system outputs.</p>
<p>Reference-free Evaluation Metrics With the widespread use of generation models across diverse domains, the need for reference-free evaluation metrics has surged. In response to this challenge, recent attention has been directed towards metrics that enable the evaluation of generated texts solely based on source documents, especially when annotated reference texts may not be available for new domains (Böhm et al., 2019; Gao et al., 2020; Wu et al., 2020; Chen et al., 2021; Scialom et al., 2021; Honovich et al., 2021; Zhong et al., 2022; Liu et al., 2023).</p>
<p>Representative reference-free metrics in recent years include generative evaluation models, exemplified by BARTScore (Yuan et al., 2021) and GPTScore (Fu et al., 2023), which are also used for reference-based evaluation. These metrics frame text evaluation as a natural language generation task, intuitively assigning higher probabilities to higher-quality generated texts. For instance, a recent study by Koh et al. (2022) has acknowledged BARTScore in reference-free mode as the factual consistency metric with the highest overall correlation to human factual consistency scores, particularly in the context of long document abstractive summarization. Therefore, the reliability of these metrics is important given their use for evaluating sensitive aspects such as factuality correctness.</p>
<p>Automatic Evaluation Metrics Pitfalls Despite their widespread use, automatic evaluation metrics have notable shortcomings. These metrics may not be robust when faced with challenges such as spurious correlations, noise, or out-of-domain texts (Sai et al., 2021; Vu et al., 2022; Durmus et al., 2022; Zhao et al., 2023; He et al., 2023). Furthermore, their effectiveness diminishes when evaluating very long documents (Amplayo et al., 2022). There is also evidence suggesting a potential bias towards ranking extractive summaries higher than abstractive ones (Amplayo et al., 2022).</p>
<p>Traditional reference-based evaluation metrics such as ROUGE or BLEU have been criticized for their inability to measure content quality or capture syntactic errors (Reiter and Belz, 2009). Consequently, these traditional metrics often exhibit weak correlations with human judgements, demonstrating that they cannot accurately reflect the realworld performance of generation systems (Peyrard, 2019; Mathur et al., 2020). For example, they might assign high scores to outputs that are flu-</p>
<p>ent but meaningless and unfaithful, as long as many of the same words are used <em>Gehrmann et al. (2021)</em>. Although embedding-based metrics (e.g., BERTScore) show improved performance in similarity measurement, they are still inadequate for assessing the extent of shared information between two summaries, a crucial indicator of summary information quality <em>Deutsch and Roth (2021)</em>.</p>
<p>Reference-free metrics, on the other hand, exhibit a bias towards outputs generated by models that are more similar to their own <em>Deutsch et al. (2022)</em>. To the best of our knowledge, this study represents the initial attempt to perform an exploration, which has not yet been undertaken systematically. Additionally, question-answering-based reference-free metrics for summarization evaluation are prone to inheriting errors within summaries <em>Kamoi et al. (2023)</em>.</p>
<p>Metrics based on Large Language Models, which are capable of conducting both reference-based and reference-free evaluations, typically demonstrate superior correlations with human quality judgements across diverse NLG tasks and evaluation dimensions <em>Deutsch et al. (2022)</em>. While prior work has reported that LLM-based metrics prefer LLM-generated text, raising a concern about the shortcomings of LLMs as evaluators <em>Liu et al. (2023)</em>, our work conducts a systematic evaluation to address a fundamental question: <em>Do language model-driven evaluation metrics inherently display bias favouring texts generated by the same underlying language model?</em> We explore this question across both reference-based and reference-free evaluations and for a range of different large language models.</p>
<h2>3 Methodology</h2>
<p>To investigate the impact of the model’s self-bias—determining whether a language model-based evaluator favours outputs generated by a similar language model—we conduct a comprehensive series of experiments involving both quantitative comparisons and qualitative analysis. Our quantitative comparisons involve using language models of varying sizes and finetuning configurations as both the evaluator and generator models. This structured approach enables us to systematically examine the potential bias across different LM configurations. Subsequently, we verify the results through qualitative analysis using a subset of models’ summaries that are accompanied by human evaluation to further demonstrate that higher scores produced by evaluators as a result of self-bias do not necessarily correlate with higher quality generated outputs.</p>
<h3>3.1 Evaluators</h3>
<p>We describe the evaluation process as follows: given a <em>source</em> text $\boldsymbol{s}$, a human written <em>reference</em> $\boldsymbol{r}$, generate a <em>hypothesis</em> $\boldsymbol{h}$, which can be represented as:</p>
<p>$$
y=f(\boldsymbol{h}, a, \mathcal{S})
$$</p>
<p>where $\boldsymbol{h}$ denotes hypothesis, $a$ refers to the aspect to evaluate, and $\mathcal{S}$ denotes supplementary text (i.e., $\boldsymbol{s}$ or $\boldsymbol{r}$ ) that is employed alongside evaluations in various settings <em>Fu et al. (2023)</em>. For instance, it could be the source text $\boldsymbol{s}$ in a <em>reference-free</em> scenario which assesses the summary based on the source article directly <em>Fabbri et al. (2021)</em>. Whereas in the <em>reference-based</em> paradigm, the evaluation considers semantic overlap between the generated hypothesis $\boldsymbol{h}$ (e.g. model generated summaries) and reference summaries $\boldsymbol{r}$ <em>Bhandari et al. (2020)</em>.</p>
<p>The evaluators (i.e. based on BART, T5, GPT model variants as well as Cohere) utilised in our study all share a conditional probability paradigm, which can generally be formulated as</p>
<p>$\operatorname{Score}(\boldsymbol{h} \mid d, a, \mathcal{S})=\sum_{t=1}^{m} w_{t} \log p\left(h_{t} \mid \boldsymbol{h}_{&lt;t}, \mathcal{S}, \theta\right)$.</p>
<p>Here $\theta$ is the model parameter, $d$ refers to the task description and $w_{t}$ denotes the weight of the token at position $t$, where previous works normally treat each token equally <em>Yuan et al. (2021); Fu et al. (2023)</em>. We provide further descriptions of each type of evaluator below.</p>
<p>BARTScore BARTScore <em>Yuan et al. (2021)</em> introduced the generative evaluation approach treating text assessment as a generation task, employing probability of the text being generated by BARTbased models <em>Lewis et al. (2020)</em> to assess the quality of text generated across various tasks such as machine translation, summarization, and data-to-text.</p>
<p>T5Score T5Score <em>Qin et al. (2022)</em> was proposed providing both generative and discriminative training strategies for assessing T5-variant models as the core of this generative evaluation paradigm.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>The integration of dual training strategies enables more types of data to be incorporated into the metric. T5Score closely aligns with BARTScore in terms of evaluation framework. Thus, when only considering the generative training strategy, T5Score is analogous to BARTScore, but for the T5 model series.</p>
<p>GPTScore Leveraging generative models to conduct evaluation has been further advanced with various of more recent large language models (Fu et al., 2023), showing a great performance and covering a rich variety of aspects for comprehensive evaluations. With an understanding of natural language instructions, GPTScore (including GPT-X and FLAN-T5 models) can perform intricate and personalized assessments without additional training.</p>
<p>Cohere We additionally include Cohere, the more recent language model to enrich our assessments. The evaluation scores assigned by the model is calculated according to Eq. 2, aligned with BARTScore, T5Score, and GPTScore.</p>
<h3>3.2 Generation Models</h3>
<p>We analyze different variants of the BART, T5, GPT-2, GPT-3, FLAN-T5 and Cohere models, taking into account two different variables: the model size and the finetuning dataset. Regarding size, we consider small, base, medium, and large variations of each model, when available. For the finetuning dataset, we examine three distinct settings: (1) using the pretrained language model without finetuning on a summarization dataset, (2) finetuning on CNN, and (3) finetuning on XSUM. For instance, BART-Base-CNN represents a BART-base model that is finetuned on the CNN dataset. For each of the model types, we have used their corresponding standard prompts for the task of summarization. ${ }^{2}$</p>
<p>To ensure the reproducibility of our analysis, we exclusively employ publicly available checkpoints for the utilized models. Apart from the GPT3Curie model that is taken from the OpenAI API and generation model obtained from Cohere, the rest of the models are taken from the Hugging Face model hub ${ }^{3}$.</p>
<p>We use each of these generation models both</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>for generating the summaries ${ }^{4}$ as well as the underlying model for the LM-based evaluator. All the checkpoints used for generators and evaluators in our experiments can be found in the Appendix A (Table 4 and Table 5).</p>
<h3>3.3 Datasets</h3>
<p>We use documents from two well-established summarization datasets including CNN/DailyMail (Hermann et al., 2015; Nallapati et al., 2016) and the extreme summarization (XSUM) dataset (Narayan et al., 2018).</p>
<p>For quantitative comparisons, we randomly selected 500 documents from each of these datasets. We provide these documents to each of the generation models to obtain their corresponding generated summaries. For qualitative analysis, we use the SummEval benchmark (Fabbri et al., 2021) and the RoSE benchmark (Liu et al., 2022). These benchmarks include summaries from various generation models, as well as human evaluations, enabling us to assess the quality of these summaries.</p>
<p>The SummEval benchmark contains summaries generated by various summarization models (i.e. BART, T5 and GPT2) for 100 articles from the CNN/DM test set, with each summary supplemented by human annotations. More specifically, SummEval incorporates human annotations by both expert and crowd-sourced human annotators, targeting dimensions of coherence, consistency, fluency, and relevance. Ratings are on a scale of 0 to 5 , with higher values indicating better performance.</p>
<p>Similarly, RoSE contains summaries generated by recent generative models based on CNN/DM documents, accompanied by their corresponding human evaluations. We use 100 summaries from each of the BART and GPT-3 models from the ROSE benchmark. The RoSE benchmark proposed an assessment protocol termed "Atomic Content Units" (ACUs) (Liu et al., 2022). ACU score gauges quality of evaluated summaries based on whether the presence of single facts (i.e., atomic facts) from reference are included in the evaluated summaries. ACU score is calculated by ACU matching:</p>
<p>$$
f(s, \mathcal{A})=\frac{\left|\mathcal{A}_{s}\right|}{|\mathcal{A}|}
$$</p>
<p>where $\mathcal{A}$ is a set of ACUs from gold summaries and $\mathcal{A}_{s}$ denotes the ACUs of candidate summary $s$.</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th></th>
<th>Max</th>
<th>Min</th>
<th>Mean</th>
<th>Median</th>
</tr>
</thead>
<tbody>
<tr>
<td>RoSE-BART</td>
<td>1.00</td>
<td>0.00</td>
<td>0.37</td>
<td>0.38</td>
</tr>
<tr>
<td>RoSE-GPT3</td>
<td>0.90</td>
<td>0.00</td>
<td>0.27</td>
<td>0.25</td>
</tr>
<tr>
<td>SummEval-BART</td>
<td>5.00</td>
<td>2.67</td>
<td>4.57</td>
<td>4.67</td>
</tr>
<tr>
<td>SummEval-T5</td>
<td>5.00</td>
<td>2.33</td>
<td>4.52</td>
<td>4.67</td>
</tr>
<tr>
<td>SummEval-GPT2</td>
<td>5.00</td>
<td>1.33</td>
<td>3.57</td>
<td>3.58</td>
</tr>
</tbody>
</table>
<p>Table 1: Distribution of human annotation scores on the RoSE and SummEval datasets, where in RoSE we consider the 'ACU’ score, and in SummEval we focus on four aspects—'Coherence’, ‘Consistency’, ‘Fluency’, and 'Relevance’—as evaluated by expert annotators. The scores for SummEval are obtained by averaging the scores across all aspects and evaluations from all annotators.</p>
<p>The distribution of human scores in RoSE and SummEval are given in Table 1.</p>
<h3>3.4 Quantitative Comparisons</h3>
<p>We employ 20 language model-based evaluators for our experiments including six BARTSCORE evaluators <em>Yuan et al. (2021)</em>, seven T5SCORE evaluators <em>Qin et al. (2022)</em>, six GPTScore evaluators, and the Cohere evaluator.</p>
<p>We assess the evaluators in two settings: (a) reference-free, where the metric evaluates the likelihood of the summary being generated from the source text, and (b) reference-based, where the generated summary is evaluated based on the reference summary.</p>
<p>Due to the nature of log probabilities, original scores from each evaluator is be negative, and a higher score indicates better quality according to the evaluator. When weights $w_{t}$ in Eq. 2 are treated equally, the evaluation protocols of BARTScore, T5Score, and GPTScore are all conditional probability paradigms. To ensure comparability among the scores provided by 20 distinguished evaluators, a uniform normalization process is applied to the scores generated by each evaluator. The normalization procedure standardizes the scores across a scale ranging from 0 to $\alpha$ as formulated in Eq. 4, where $X_{i,j}$ indicates scores evaluated by the $j$-th evaluator on summaries generated by the $i$-th generator.</p>
<p>$X_{i,j}^{norm}=\frac{\alpha(X_{i,j}-\min_{i} X_{i,j})}{\max_{i} X_{i,j}-\min_{i} X_{i,j}}$ (4)</p>
<p>In this context, a normalized score of $\alpha$ signifies the highest quality attributed by the evaluator, while a score of 0 indicates the lowest quality.</p>
<p>As the length of the generated summary is a key factor influencing the evaluation results, we further analyse the impact of lengths for the content generated by the models along with the experiments. In this regard, we also compute the correlations between the length of the text and the scores assigned by evaluators to identify trends in evaluators’ preferences.</p>
<h3>3.5 Qualitative Analysis</h3>
<p>For qualitative analysis, we employ Spearman Correlations <em>Zar (2014)</em> and Kendall Correlations <em>Freedman et al. (2007)</em>, which respectively assess monotonic relationships and order associations between human evaluations and LM evaluator scores. They are common metrics for assessing correlations with human judgements.</p>
<p>For the SummEval dataset, we calculate the correlations for four aspects (i.e. Coherence, Consistency, Fluency and Relevance ), aligned with the reference-free input setting in the evaluation protocol as specified by <em>Yuan et al. (2021)</em>. For the evaluations based on the RoSE benchmark, we use ACU annotations that are suited for reference-based summary salience evaluation. Therefore, we employ the correlation values obtained from the SummEval dataset for the reference-free setting and those from the RoSE benchmark for the reference-based setting.</p>
<h2>4 Experimental Results</h2>
<h3>4.1 Quantitative Comparisons: Assessing Self-Bias in LM-Evaluators Towards Their Own Output</h3>
<p>Figures 2 and 3 display heatmaps presenting evaluator scores for various summaries generated by different generators from CNN/DM documents in reference-free and reference-base settings, respectively. These scores are computed by averaging the individual scores of the selected 500 documents. In both heatmaps, we observe darker cells along the diagonal line, running from the top left to the bottom right. This indicates the potential evaluator bias towards their corresponding generator models i.e., self-bias. However, this bias is notably more pronounced in the reference-free setting, commonly used for factuality evaluation <em>Koh et al. (2022)</em>.</p>
<p>Furthermore, as shown in Figure 2, we note a distinct trend: T5-based generators, whether finetuned or not, tend to receive higher scores when assessed using different T5Score variations com-</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Assessing Bias in the CNN/DM Dataset using heatmaps in the <em>reference-free</em> setting. Observing the darkest cells along the diagonal line, from the top left to the bottom right, indicates a distinct bias among evaluators towards their respective models. All evaluator scores are normalized to a range between 0 and 1. Additionally, the number in the bracket represents the average length of summaries (measured in words) produced by the respective model.</p>
<p>Based on the evaluation results, a 5% increase in the CNN/DM dataset was observed. This result is consistent with the findings of previous studies, where the CNN is used as a reference-free setting. However, the CNN is used as a reference-free setting, which is not a standard setting for the data presented in Table 2. The results are presented in Figure 5. Based on these results, with the exception of evaluators fine-tuned on XSUM, BARTScore, and GPTScore variants tend to assign higher scores to longer summaries. This observation explains the darker squares positioned in the top-right corner of Figure 2 for high values of GPTScore variants, highlighting their inclination to assign higher scores to BART and BART-CNN generators that produce longer summaries. It is worth noting that this correlation with summary length is prominent within the reference-free setting. We observe a similar but less obvious pattern in the reference-based evaluations, as shown in Figure 3.</p>
<h3>4.3 Qualitative Analysis: Correlation of Self-Bias with Human Evaluation</h3>
<p>To further verify the evaluators' self-bias, we repeat the experiments from § 4.1 on summarization benchmarks that are accompanied by human evaluations. While the number of summaries in these</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" />Figure 3: Assessing Bias on CNN/DM Dataset using heatmaps in the reference-based setting. Observing darker cells along the diagonal line indicates potential self-bias. All evaluator scores are normalized to a range between 0 and 1. Additionally, the number in the bracket represents the average length of summaries (measured in words) produced by the respective model.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" />Figure 4: Heatmaps of evaluation scores on the SummEval &amp; RoSE benchmarks for the reference-free and reference-based setting. We use the reference-free setting for SummEval and the reference-based setting for RoSE, aligning with the specific aspects each benchmark emphasizes.</p>
<p>benchmarks is limited compared to those in § 4.1, we can use the human annotations to verify that the inflated scores are not correlated with human evaluations.</p>
<p>Figure 4 shows the evaluation results for the SummEval and RoSE benchmarks for the reference-free and reference-based setting, respectively. As mentioned, we use SummEval for the reference-free setting and RoSE for the reference-based setting with regard to the specific aspects of each of these benchmarks <em>Yuan et al. (2021)</em>. Overall, we observe a trend similar to that shown in Figure 2. For instance, the T5-base generator</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Spearman Correlation between the length of generated summaries and the reference-free scores assigned by each evaluator. A higher positive score indicates that an evaluator prefers longer summaries, while a lower negative score indicates a preference for shorter summaries.</p>
<p>receives higher scores from T5-based evaluators.7 Meanwhile, BART-based models receive higher scores from both BARTScore and GPTScore evaluators, instead of T5 evaluator.</p>
<p>Table 3 presents the Spearman and Kendall correlation values of SummEval in the reference-free setting, whereas the Spearman and Kendall correlation values of RoSE in the reference-based setting are given in Table 2.</p>
<p>Overall, we observe that none of the evaluators have a strong correlation with the human annotations on either of these benchmarks. Due to the limited size of the samples (i.e., 100 summaries from SummEval and 100 summaries from ROSE with human annotations, as described in §3.3) and the absence of many of our investigated generators in § 4.1, we cannot draw a conclusive conclusion from the correlation values. Nevertheless, these results demonstrate that none of these evaluators highly correlate with human annotations, and as observed in § 4.1, their inflated scores for their own underlying generator may contribute to this low correlation.</p>
<h3>5 Conclusions</h3>
<p>Based on experiments, we make the following conclusions: <strong>First</strong>, the popularity of generative evaluation metrics, such as BARTScore, is on the rise for evaluating the factual accuracy of generated content—a critical concern in modern generator models. However, our results reveal that this evaluation</p>
<table>
<thead>
<tr>
<th>RoSE - Reference-based</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Evaluator</td>
<td>ACU</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Spearman</td>
<td>Kendall</td>
</tr>
<tr>
<td>BART-Base</td>
<td>0.454</td>
<td>0.310</td>
</tr>
<tr>
<td>BART-Large</td>
<td>0.298</td>
<td>0.218</td>
</tr>
<tr>
<td>BART-Base-CNN</td>
<td><strong>0.488</strong></td>
<td><strong>0.345</strong></td>
</tr>
<tr>
<td>BART-Large-CNN</td>
<td>0.468</td>
<td>0.329</td>
</tr>
<tr>
<td>BART-Base-XSUM</td>
<td>0.150</td>
<td>0.103</td>
</tr>
<tr>
<td>BART-Large-XSUM</td>
<td>0.371</td>
<td>0.253</td>
</tr>
<tr>
<td>T5-Small</td>
<td>0.396</td>
<td>0.284</td>
</tr>
<tr>
<td>T5-Base</td>
<td>0.395</td>
<td>0.285</td>
</tr>
<tr>
<td>T5-Large</td>
<td>0.392</td>
<td>0.282</td>
</tr>
<tr>
<td>T5-Small-CNN</td>
<td>0.393</td>
<td>0.281</td>
</tr>
<tr>
<td>T5-Base-CNN</td>
<td>0.391</td>
<td>0.276</td>
</tr>
<tr>
<td>T5-Small-XSUM</td>
<td>0.379</td>
<td>0.269</td>
</tr>
<tr>
<td>T5-Large-XSUM</td>
<td>0.462</td>
<td>0.324</td>
</tr>
<tr>
<td>GPT2</td>
<td>0.375</td>
<td>0.255</td>
</tr>
<tr>
<td>GPT2-Medium</td>
<td>0.357</td>
<td>0.244</td>
</tr>
<tr>
<td>GPT2-Large</td>
<td>0.353</td>
<td>0.242</td>
</tr>
<tr>
<td>GPT3-Curie</td>
<td>0.310</td>
<td>0.214</td>
</tr>
<tr>
<td>FLANT5-Base</td>
<td>0.460</td>
<td>0.325</td>
</tr>
<tr>
<td>FLANT5-XL</td>
<td>0.433</td>
<td>0.304</td>
</tr>
<tr>
<td>Cohere-Command</td>
<td>0.384</td>
<td>0.267</td>
</tr>
</tbody>
</table>
<p>Table 2: Spearman and Kendall correlations between reference-based evaluation scores and human annotations using annotations in RoSE. Results in bold indicate the strongest coefficient.</p>
<p>approach is susceptible to the self-bias, highlighting the need for more robust metrics to assess factual correctness reliably. <strong>Second</strong>, our analysis indicates that models fine-tuned on the XSUM dataset are not suitable for direct integration into evaluators due to their bias towards shorter summaries. The exception is their use for evaluating summaries aligned with XSUM-style content. <strong>Third</strong>, notably, similar to traditional evaluation metrics (Sun et al., 2019), contemporary evaluation metrics might also lean towards favoring longer summaries. This bias should be considered when interpreting and applying these metrics. <strong>Finally</strong>, our study uncovers the presence of the self-bias across all assessed evaluators. Consequently, we recommend avoiding the use of the same underlying model as the generator for assessment. Although the limited human evaluations for our examined models prevent definitive conclusions on selecting the best generative evaluator, our research charts a promising direction for designing more resilient and unbiased evaluation metrics.</p>
<p>In summary, our study identifies a new type of bias in generative evaluators encouraging future research in this direction for designing fairer evaluation metrics.</p>
<p>7 In SummEval, the T5 model is only ranked higher when evaluated with certain variants of the T5Score in the reference-based setting.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Assessing Bias in the XSUM Dataset using heatmaps in the reference-free setting. Observing the darkest cells along the diagonal line, from the top left to the bottom right, indicates a distinct bias among evaluators towards their respective models. All evaluator scores are normalized to a range between 0 and 1. Additionally, the number in the bracket represents the average length of summaries (measured in words) produced by the respective model.</p>
<table>
<thead>
<tr>
<th>Evaluator</th>
<th>Coherence</th>
<th>Consistency</th>
<th>Fluency</th>
<th>Relevance</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Spearman</td>
<td>Kendall</td>
<td>Spearman</td>
<td>Kendall</td>
</tr>
<tr>
<td>BART-Base</td>
<td>-0.028</td>
<td>-0.021</td>
<td>0.107</td>
<td>0.078</td>
</tr>
<tr>
<td>BART-Large</td>
<td>0.052</td>
<td>0.040</td>
<td>0.180</td>
<td>0.137</td>
</tr>
<tr>
<td>BART-Base-CNN</td>
<td>0.193</td>
<td>0.138</td>
<td>0.228</td>
<td>0.171</td>
</tr>
<tr>
<td>BART-Large-CNN</td>
<td>0.171</td>
<td>0.119</td>
<td>0.255</td>
<td>0.192</td>
</tr>
<tr>
<td>BART-Base-XSUM</td>
<td>0.170</td>
<td>0.120</td>
<td>-0.103</td>
<td>-0.079</td>
</tr>
<tr>
<td>BART-Large-XSUM</td>
<td>0.055</td>
<td>0.040</td>
<td>0.060</td>
<td>0.046</td>
</tr>
<tr>
<td>T5-Small</td>
<td>0.208</td>
<td>0.146</td>
<td>0.547</td>
<td>0.419</td>
</tr>
<tr>
<td>T5-Base</td>
<td>0.173</td>
<td>0.119</td>
<td>0.533</td>
<td>0.409</td>
</tr>
<tr>
<td>T5-Large</td>
<td>0.185</td>
<td>0.132</td>
<td>0.477</td>
<td>0.364</td>
</tr>
<tr>
<td>T5-Small-CNN</td>
<td><strong>0.315</strong></td>
<td><strong>0.222</strong></td>
<td>0.462</td>
<td>0.356</td>
</tr>
<tr>
<td>T5-Base-CNN</td>
<td>0.192</td>
<td>0.135</td>
<td>0.253</td>
<td>0.190</td>
</tr>
<tr>
<td>T5-Small-XSUM</td>
<td>0.245</td>
<td>0.178</td>
<td>0.142</td>
<td>0.109</td>
</tr>
<tr>
<td>T5-Large-XSUM</td>
<td>0.213</td>
<td>0.152</td>
<td>-0.111</td>
<td>-0.085</td>
</tr>
<tr>
<td>GPT2</td>
<td>0.103</td>
<td>0.077</td>
<td>0.154</td>
<td>0.117</td>
</tr>
<tr>
<td>GPT2-Medium</td>
<td>0.123</td>
<td>0.091</td>
<td>0.234</td>
<td>0.179</td>
</tr>
<tr>
<td>GPT2-Large</td>
<td>0.119</td>
<td>0.089</td>
<td>0.184</td>
<td>0.140</td>
</tr>
<tr>
<td>GPT3-Curie</td>
<td>0.152</td>
<td>0.108</td>
<td>0.483</td>
<td>0.371</td>
</tr>
<tr>
<td>FLANT5-Base</td>
<td>0.220</td>
<td>0.154</td>
<td>0.448</td>
<td>0.345</td>
</tr>
<tr>
<td>FLANT5-XL</td>
<td>0.248</td>
<td>0.174</td>
<td><strong>0.550</strong></td>
<td><strong>0.424</strong></td>
</tr>
<tr>
<td>Cohere-Command</td>
<td>0.136</td>
<td>0.097</td>
<td>0.520</td>
<td>0.397</td>
</tr>
</tbody>
</table>
<p>Table 3: Spearman and Kendall correlations between the reference-free evaluation scores and expert annotations provided in SummEval on four different aspects. The strongest correlation for each aspect is bolded.</p>
<h2>Limitations</h2>
<p>We note that our work has the following limitations. Firstly, our experiment has been focused on the summarization task. Expanding the evaluation to encompass a broader range of generation tasks would be highly beneficial. Secondly, conducting a larger-scale human evaluation would be advantageous, as our current experiments are constrained by the limited sample sizes from SummEval and</p>
<p>RoSE. Finally, incorporating additional generation models and evaluators in future work would further enrich the experiment.</p>
<h2>Ethics Statement</h2>
<p>This paper raises no ethical concerns. The data and supplementary materials used in this study are open-sourced and widely employed in existing works.</p>
<h2>References</h2>
<p>Reinald Kim Amplayo, Peter J. Liu, Yao Zhao, and Shashi Narayan. 2022. Smart: Sentences as basic units for text evaluation. ArXiv, abs/2208.01030.</p>
<p>Manik Bhandari, Pranav Narayan Gour, Atabak Ashfaq, Pengfei Liu, and Graham Neubig. 2020. Reevaluating evaluation in text summarization. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 9347-9359, Online. Association for Computational Linguistics.</p>
<p>Florian Böhm, Yang Gao, Christian M. Meyer, Ori Shapira, Ido Dagan, and Iryna Gurevych. 2019. Better rewards yield better summaries: Learning to summarise without references. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3110-3120, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901.</p>
<p>Wang Chen, Piji Li, and Irwin King. 2021. A trainingfree and reference-free summarization evaluation metric via centrality-weighted relevance and selfreferenced redundancy. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 404-414, Online. Association for Computational Linguistics.</p>
<p>Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le,
and Jason Wei. 2022. Scaling instruction-finetuned language models.</p>
<p>Daniel Deutsch, Rotem Dror, and Dan Roth. 2022. On the limitations of reference-free evaluations of generated text. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 10960-10977, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.</p>
<p>Daniel Deutsch and Dan Roth. 2021. Understanding the extent to which content quality metrics measure the information quality of summaries. In Proceedings of the 25th Conference on Computational Natural Language Learning, pages 300-309, Online. Association for Computational Linguistics.</p>
<p>Esin Durmus, Faisal Ladhak, and Tatsunori Hashimoto. 2022. Spurious correlations in reference-free evaluation of text generation. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 14431454, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Alexander R. Fabbri, Wojciech Kryściński, Bryan McCann, Caiming Xiong, Richard Socher, and Dragomir Radev. 2021. SummEval: Re-evaluating summarization evaluation. Transactions of the Association for Computational Linguistics, 9:391-409.
D. Freedman, R. Pisani, and R. Purves. 2007. Statistics: Fourth International Student Edition. Emersion: Emergent Village Resources for Communities of Faith Series. W.W. Norton \&amp; Company.</p>
<p>Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. 2023. Gptscore: Evaluate as you desire.</p>
<p>Yang Gao, Wei Zhao, and Steffen Eger. 2020. SUPERT: Towards new frontiers in unsupervised evaluation metrics for multi-document summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 13471354, Online. Association for Computational Linguistics.</p>
<p>Sebastian Gehrmann, Tosin Adewumi, Karmanya Aggarwal, Pawan Sasanka Ammanamanchi, Anuoluwapo Aremu, Antoine Bosselut, Khyathi Raghavi Chandu, Miruna-Adriana Clinciu, Dipanjan Das, Kaustubh Dhole, Wanyu Du, Esin Durmus, Ondřej Dušek, Chris Chinenye Emezue, Varun Gangal, Cristina Garbacea, Tatsunori Hashimoto, Yufang Hou, Yacine Jernite, Harsh Jhamtani, Yangfeng Ji, Shailza Jolly, Mihir Kale, Dhruv Kumar, Faisal Ladhak, Aman Madaan, Mounica Maddela, Khyati Mahajan, Saad Mahamood, Bodhisattwa Prasad Majumder, Pedro Henrique Martins, Angelina McMillan-Major, Simon Mille, Emiel van Miltenburg, Moin Nadeem, Shashi Narayan, Vitaly Nikolaev, Andre Niyongabo Rubungo, Salomey Osei, Ankur Parikh, Laura Perez-Beltrachini, Niranjan Ramesh Rao, Vikas Raunak, Juan Diego Rodriguez, Sashank Santhanam, João Sedoc,</p>
<p>Thibault Sellam, Samira Shaikh, Anastasia Shimorina, Marco Antonio Sobrevilla Cabezudo, Hendrik Strobelt, Nishant Subramani, Wei Xu, Diyi Yang, Akhila Yerukola, and Jiawei Zhou. 2021. The GEM benchmark: Natural language generation, its evaluation and metrics. In Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021), pages 96-120, Online. Association for Computational Linguistics.</p>
<p>Tianxing He, Jingyu Zhang, Tianle Wang, Sachin Kumar, Kyunghyun Cho, James Glass, and Yulia Tsvetkov. 2023. On the blind spots of model-based evaluation metrics for text generation. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 12067-12097, Toronto, Canada. Association for Computational Linguistics.</p>
<p>Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 2015. Teaching machines to read and comprehend. Advances in neural information processing systems, 28.</p>
<p>Or Honovich, Leshem Choshen, Roee Aharoni, Ella Neeman, Idan Szpektor, and Omri Abend. 2021. $q^{2}$ : Evaluating factual consistency in knowledgegrounded dialogues via question generation and question answering. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7856-7870, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Ryo Kamoi, Tanya Goyal, and Greg Durrett. 2023. Shortcomings of question answering based factuality frameworks for error localization.</p>
<p>Huan Yee Koh, Jiaxin Ju, He Zhang, Ming Liu, and Shirui Pan. 2022. How far are we from robust long abstractive summarization? In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 2682-2698, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.</p>
<p>Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871-7880, Online. Association for Computational Linguistics.</p>
<p>Chin-Yew Lin. 2004. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 74-81, Barcelona, Spain. Association for Computational Linguistics.</p>
<p>Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023. G-eval: Nlg evaluation using gpt-4 with better human alignment.</p>
<p>Yixin Liu, Alexander R Fabbri, Pengfei Liu, Yilun Zhao, Linyong Nan, Ruilin Han, Simeng Han, Shafiq Joty, Chien-Sheng Wu, Caiming Xiong, et al. 2022. Revisiting the gold standard: Grounding summarization evaluation with robust human evaluation. arXiv preprint arXiv:2212.07981.</p>
<p>Nitika Mathur, Timothy Baldwin, and Trevor Cohn. 2020. Tangled up in BLEU: Reevaluating the evaluation of automatic machine translation evaluation metrics. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4984-4997, Online. Association for Computational Linguistics.</p>
<p>Ramesh Nallapati, Bowen Zhou, Cicero dos Santos, Caglar Gulcehre, and Bing Xiang. 2016. Abstractive text summarization using sequence-to-sequence rnns and beyond. In Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning, page 280. Association for Computational Linguistics.</p>
<p>Shashi Narayan, Shay B. Cohen, and Mirella Lapata. 2018. Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1797-1807, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311-318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.</p>
<p>Maxime Peyrard. 2019. Studying summarization evaluation metrics in the appropriate scoring range. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 50935100, Florence, Italy. Association for Computational Linguistics.</p>
<p>Yiwei Qin, Weizhe Yuan, Graham Neubig, and Pengfei Liu. 2022. T5score: Discriminative fine-tuning of generative evaluation metrics.</p>
<p>Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485-5551.</p>
<p>Ehud Reiter and Anja Belz. 2009. An investigation into the validity of some metrics for automatically evaluating natural language generation systems. Computational Linguistics, 35(4):529-558.</p>
<p>Ananya B. Sai, Tanay Dixit, Dev Yashpal Sheth, Sreyas Mohan, and Mitesh M. Khapra. 2021. Perturbation CheckLists for evaluating NLG evaluation metrics. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7219-7234, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Thomas Scialom, Paul-Alexis Dray, Sylvain Lamprier, Benjamin Piwowarski, Jacopo Staiano, Alex Wang, and Patrick Gallinari. 2021. QuestEval: Summarization asks for fact-based evaluation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6594-6604, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Thibault Sellam, Dipanjan Das, and Ankur Parikh. 2020. BLEURT: Learning robust metrics for text generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7881-7892, Online. Association for Computational Linguistics.</p>
<p>Simeng Sun, Ori Shapira, Ido Dagan, and Ani Nenkova. 2019. How to compare summarizers without target length? pitfalls, solutions and re-examination of the neural summarization literature. In Proceedings of the Workshop on Methods for Optimizing and Evaluating Neural Language Generation, pages 21-29, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Chris van der Lee, Albert Gatt, Emiel van Miltenburg, Sander Wubben, and Emiel Krahmer. 2019. Best practices for the human evaluation of automatically generated text. In Proceedings of the 12th International Conference on Natural Language Generation, pages 355-368, Tokyo, Japan. Association for Computational Linguistics.</p>
<p>Doan Nam Long Vu, Nafise Sadat Moosavi, and Steffen Eger. 2022. Layer or representation space: What makes BERT-based evaluation metrics robust? In Proceedings of the 29th International Conference on Computational Linguistics, pages 3401-3411, Gyeongju, Republic of Korea. International Committee on Computational Linguistics.</p>
<p>Hanlu Wu, Tengfei Ma, Lingfei Wu, Tariro Manyumwa, and Shouling Ji. 2020. Unsupervised reference-free summary quality evaluation via contrastive learning. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 3612-3621, Online. Association for Computational Linguistics.</p>
<p>Weizhe Yuan, Graham Neubig, and Pengfei Liu. 2021. Bartscore: Evaluating generated text as text generation. Advances in Neural Information Processing Systems, 34:27263-27277.</p>
<p>Jerrold H. Zar. 2014. Spearman Rank Correlation: Overview. John Wiley \&amp; Sons, Ltd.</p>
<p>Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: Evaluating text generation with bert. In International Conference on Learning Representations.</p>
<p>Kun Zhao, Bohao Yang, Chenghua Lin, Wenge Rong, Aline Villavicencio, and Xiaohui Cui. 2023. Evaluating open-domain dialogues in latent space with next sentence prediction and mutual information. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 562-574, Toronto, Canada.</p>
<p>Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Christian M. Meyer, and Steffen Eger. 2019. MoverScore: Text generation evaluating with contextualized embeddings and earth mover distance. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 563-578, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Ming Zhong, Yang Liu, Da Yin, Yuning Mao, Yizhu Jiao, Pengfei Liu, Chenguang Zhu, Heng Ji, and Jiawei Han. 2022. Towards a unified multidimensional evaluator for text generation. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 20232038, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.</p>
<h2>A Evaluation Setting</h2>
<h2>A. 1 Generator</h2>
<p>Full details of the models (e.g. checkpoint, prompt setting) that we employed as generators are given in Table 4.</p>
<h2>A. 2 Evaluator</h2>
<p>Full details of the models that we employed as our evaluators are given in Table 5 (reference-free settings) and Table 6 (reference-based settings).</p>
<h2>B Evaluation Results</h2>
<h2>B. 1 Reference-free Setting</h2>
<p>Results of XSUM Dataset in Reference-free setting are presented in Figure 6. Evaluation scores for RoSE and SummEval benchmarks under the reference-free setting are shown in Figure 4.</p>
<p>For the meta evaluation, Spearman and Kendall correlation values in the reference-free setting for SummEval benchmark are shown in Table 3.</p>
<h2>B. 2 Reference-based Setting</h2>
<p>Heatmap of evaluation result on CNN/DM dataset under reference-based setting is given by Figure 3</p>
<table>
<thead>
<tr>
<th>Name of Generator</th>
<th>Name of Checkpoint or Model</th>
<th>Suffix</th>
<th>Prefix</th>
</tr>
</thead>
<tbody>
<tr>
<td>BART-Base</td>
<td>facebook/bart-base</td>
<td>$\boldsymbol{X}$</td>
<td>Summarize:</td>
</tr>
<tr>
<td>BART-Large</td>
<td>facebook/bart-large</td>
<td>$\boldsymbol{X}$</td>
<td>Summarize:</td>
</tr>
<tr>
<td>BART-Base-CNN</td>
<td>ainize/bart-base-cnn</td>
<td>$\boldsymbol{X}$</td>
<td>$\boldsymbol{X}$</td>
</tr>
<tr>
<td>BART-Large-CNN</td>
<td>facebook/bart-large-cnn</td>
<td>$\boldsymbol{X}$</td>
<td>$\boldsymbol{X}$</td>
</tr>
<tr>
<td>BART-Base-XSUM</td>
<td>morenolq/bart-base-xsum</td>
<td>$\boldsymbol{X}$</td>
<td>$\boldsymbol{X}$</td>
</tr>
<tr>
<td>BART-Large-XSUM</td>
<td>facebook/bart-large-xsum</td>
<td>$\boldsymbol{X}$</td>
<td>$\boldsymbol{X}$</td>
</tr>
<tr>
<td>T5-Small</td>
<td>t5-small</td>
<td>$\boldsymbol{X}$</td>
<td>Summarize:</td>
</tr>
<tr>
<td>T5-Base</td>
<td>t5-base</td>
<td>$\boldsymbol{X}$</td>
<td>Summarize:</td>
</tr>
<tr>
<td>T5-Large</td>
<td>t5-large</td>
<td>$\boldsymbol{X}$</td>
<td>Summarize:</td>
</tr>
<tr>
<td>T5-Small-CNN</td>
<td>ubikpt/t5-small-finetuned-cnn</td>
<td>$\boldsymbol{X}$</td>
<td>$\boldsymbol{X}$</td>
</tr>
<tr>
<td>T5-Base-CNN</td>
<td>flax-community/t5-base-cnn-dm</td>
<td>$\boldsymbol{X}$</td>
<td>$\boldsymbol{X}$</td>
</tr>
<tr>
<td>T5-Small-XSUM</td>
<td>pki/t5-small-finetuned xsum</td>
<td>$\boldsymbol{X}$</td>
<td>$\boldsymbol{X}$</td>
</tr>
<tr>
<td>T5-Large-XSUM</td>
<td>sysresearch101/t5-large-finetuned-xsum</td>
<td>$\boldsymbol{X}$</td>
<td>$\boldsymbol{X}$</td>
</tr>
<tr>
<td>GPT2</td>
<td>openai-community/gpt2</td>
<td>TL;DR:</td>
<td>$\boldsymbol{X}$</td>
</tr>
<tr>
<td>GPT2-Medium</td>
<td>openai-community/gpt2-medium</td>
<td>TL;DR:</td>
<td>$\boldsymbol{X}$</td>
</tr>
<tr>
<td>GPT2-Large</td>
<td>openai-community/gpt2-large</td>
<td>TL;DR:</td>
<td>$\boldsymbol{X}$</td>
</tr>
<tr>
<td>GPT3-Curie</td>
<td>text-curie-001</td>
<td>TL;DR:</td>
<td>$\boldsymbol{X}$</td>
</tr>
<tr>
<td>FLANT5-Base</td>
<td>google/flan-t5-base</td>
<td>TL;DR:</td>
<td>$\boldsymbol{X}$</td>
</tr>
<tr>
<td>FLANT5-XL</td>
<td>google/flan-t5-xl</td>
<td>TL;DR:</td>
<td>$\boldsymbol{X}$</td>
</tr>
<tr>
<td>Cohere-Command</td>
<td>api.cohere.ai/v1/generate</td>
<td>$\boldsymbol{X}$</td>
<td>Write a concise summarization:</td>
</tr>
</tbody>
</table>
<p>Table 4: Checkpoints or model utilized in our generation setting with corresponding prompt configurations, 'text-curie-001' is the model name provided by OpenAI API, and 'api.cohere.ai/v1/generate' denotes model names provided by Cohere API, alongside other checkpoints available through Hugging Face.</p>
<table>
<thead>
<tr>
<th>Name of Evaluator</th>
<th>Name of Checkpoint or Model</th>
<th>Suffix</th>
<th>Prefix</th>
</tr>
</thead>
<tbody>
<tr>
<td>BART-Base</td>
<td>facebook/bart-base</td>
<td>$\boldsymbol{X}$</td>
<td>Summarize:</td>
</tr>
<tr>
<td>BART-Large</td>
<td>facebook/bart-large</td>
<td>$\boldsymbol{X}$</td>
<td>Summarize:</td>
</tr>
<tr>
<td>BART-Base-CNN</td>
<td>ainize/bart-base-cnn</td>
<td>$\boldsymbol{X}$</td>
<td>$\boldsymbol{X}$</td>
</tr>
<tr>
<td>BART-Large-CNN</td>
<td>facebook/bart-large-cnn</td>
<td>$\boldsymbol{X}$</td>
<td>$\boldsymbol{X}$</td>
</tr>
<tr>
<td>BART-Base-XSUM</td>
<td>morenolq/bart-base-xsum</td>
<td>$\boldsymbol{X}$</td>
<td>$\boldsymbol{X}$</td>
</tr>
<tr>
<td>BART-Large-XSUM</td>
<td>facebook/bart-large-xsum</td>
<td>$\boldsymbol{X}$</td>
<td>$\boldsymbol{X}$</td>
</tr>
<tr>
<td>T5-Small</td>
<td>t5-small</td>
<td>$\boldsymbol{X}$</td>
<td>Summarize:</td>
</tr>
<tr>
<td>T5-Base</td>
<td>t5-base</td>
<td>$\boldsymbol{X}$</td>
<td>Summarize:</td>
</tr>
<tr>
<td>T5-Large</td>
<td>t5-large</td>
<td>$\boldsymbol{X}$</td>
<td>Summarize:</td>
</tr>
<tr>
<td>T5-Small-CNN</td>
<td>ubikpt/t5-small-finetuned-cnn</td>
<td>$\boldsymbol{X}$</td>
<td>$\boldsymbol{X}$</td>
</tr>
<tr>
<td>T5-Base-CNN</td>
<td>flax-community/t5-base-cnn-dm</td>
<td>$\boldsymbol{X}$</td>
<td>$\boldsymbol{X}$</td>
</tr>
<tr>
<td>T5-Small-XSUM</td>
<td>pki/t5-small-finetuned xsum</td>
<td>$\boldsymbol{X}$</td>
<td>$\boldsymbol{X}$</td>
</tr>
<tr>
<td>T5-Large-XSUM</td>
<td>sysresearch101/t5-large-finetuned-xsum</td>
<td>$\boldsymbol{X}$</td>
<td>$\boldsymbol{X}$</td>
</tr>
<tr>
<td>GPT2</td>
<td>openai-community/gpt2</td>
<td>TL;DR:</td>
<td>$\boldsymbol{X}$</td>
</tr>
<tr>
<td>GPT2-Medium</td>
<td>openai-community/gpt2-medium</td>
<td>TL;DR:</td>
<td>$\boldsymbol{X}$</td>
</tr>
<tr>
<td>GPT2-Large</td>
<td>openai-community/gpt2-large</td>
<td>TL;DR:</td>
<td>$\boldsymbol{X}$</td>
</tr>
<tr>
<td>GPT3-Curie</td>
<td>text-curie-001</td>
<td>TL;DR:</td>
<td>$\boldsymbol{X}$</td>
</tr>
<tr>
<td>FLANT5-Base</td>
<td>google/flan-t5-base</td>
<td>TL;DR:</td>
<td>$\boldsymbol{X}$</td>
</tr>
<tr>
<td>FLANT5-XL</td>
<td>google/flan-t5-xl</td>
<td>TL;DR:</td>
<td>$\boldsymbol{X}$</td>
</tr>
<tr>
<td>Cohere-Command</td>
<td>api.cohere.ai/v1/generate</td>
<td>$\boldsymbol{X}$</td>
<td>Write a concise summarization:</td>
</tr>
</tbody>
</table>
<p>Table 5: Checkpoints or model utilized in our evaluation study for the reference-free setting with corresponding prompt configurations, 'text-curie-001' is the model name provided by OpenAI API, and 'api.cohere.ai/v1/generate' denotes model names provided by Cohere API, alongside other checkpoints available through Hugging Face.</p>
<p>Evaluation scores for RoSE and SummEval RoSE benchmark are shown in Table 2. benchmarks under the reference-based setting are illustrated by Figure 4.</p>
<p>For the meta evaluation, Spearman and Kendall correlation values in the reference-based setting for</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Name of Evaluator</th>
<th style="text-align: left;">Name of Checkpoint or Model</th>
<th style="text-align: left;">Suffix</th>
<th style="text-align: left;">Prefix</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">BART-Base</td>
<td style="text-align: left;">facebook/bart-base</td>
<td style="text-align: left;">in other words:</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">BART-Large</td>
<td style="text-align: left;">facebook/bart-large</td>
<td style="text-align: left;">in other words:</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">BART-Base-CNN</td>
<td style="text-align: left;">ainize/bart-base-cnn</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">BART-Large-CNN</td>
<td style="text-align: left;">facebook/bart-large-cnn</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">BART-Base-XSUM</td>
<td style="text-align: left;">morenok/bart-base-xsum</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">BART-Large-XSUM</td>
<td style="text-align: left;">facebook/bart-large-xsum</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">T5-Small</td>
<td style="text-align: left;">t5-small</td>
<td style="text-align: left;"></td>
<td style="text-align: left;">Paraphrase:</td>
</tr>
<tr>
<td style="text-align: left;">T5-Base</td>
<td style="text-align: left;">t5-base</td>
<td style="text-align: left;"></td>
<td style="text-align: left;">Paraphrase:</td>
</tr>
<tr>
<td style="text-align: left;">T5-Large</td>
<td style="text-align: left;">t5-large</td>
<td style="text-align: left;"></td>
<td style="text-align: left;">Paraphrase:</td>
</tr>
<tr>
<td style="text-align: left;">T5-Small-CNN</td>
<td style="text-align: left;">ubikpt/t5-small-finetuned-cnn</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">T5-Base-CNN</td>
<td style="text-align: left;">flax-community/t5-base-cnn-dm</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">T5-Small-XSUM</td>
<td style="text-align: left;">pki/t5-small-finetuned xsum</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">T5-Large-XSUM</td>
<td style="text-align: left;">sysresearch101/t5-large-finetuned-xsum</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">GPT2</td>
<td style="text-align: left;">openai-community/gpt2</td>
<td style="text-align: left;">Paraphrase the sentence:</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">GPT2-Medium</td>
<td style="text-align: left;">openai-community/gpt2-medium</td>
<td style="text-align: left;">Paraphrase the sentence:</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">GPT2-Large</td>
<td style="text-align: left;">openai-community/gpt2-large</td>
<td style="text-align: left;">Paraphrase the sentence:</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">GPT3-Curie</td>
<td style="text-align: left;">text-curie-001</td>
<td style="text-align: left;">Paraphrase the sentence:</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">FLANT5-Base</td>
<td style="text-align: left;">google/flan-t5-base</td>
<td style="text-align: left;">Paraphrase the sentence:</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">FLANT5-XL</td>
<td style="text-align: left;">google/flan-t5-xl</td>
<td style="text-align: left;">Paraphrase the sentence:</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Cohere-Command</td>
<td style="text-align: left;">api.cohere.ai/v1/generate</td>
<td style="text-align: left;">Paraphrase the sentence:</td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<p>Table 6: Checkpoints and models utilised in our evaluation study for the reference-based setting with corresponding prompt configurations, 'text-curie-001' is the model name provided by OpenAI API, and 'api.cohere.ai/v1/generate' denotes model names provided by Cohere API, alongside other checkpoints available through Hugging Face.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{5}$ Appendix A.2 contains more details about the evaluators.
${ }^{6}$ In our work, we set parameter $\alpha$ to 1.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{4}$ We use the zero-shot setting for the models that are not finetuned on summarization datasets.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
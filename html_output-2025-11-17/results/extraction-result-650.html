<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-650 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-650</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-650</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-17.html">extraction-schema-17</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <p><strong>Paper ID:</strong> paper-9d15ebe3f5aaf32a9f835f88703241461324c35b</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/9d15ebe3f5aaf32a9f835f88703241461324c35b" target="_blank">Neural-Symbolic VQA: Disentangling Reasoning from Vision and Language Understanding</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> This work proposes a neural-symbolic visual question answering system that first recovers a structural scene representation from the image and a program trace from the question, then executes the program on the scene representation to obtain an answer.</p>
                <p><strong>Paper Abstract:</strong> We marry two powerful ideas: deep representation learning for visual recognition and language understanding, and symbolic program execution for reasoning. Our neural-symbolic visual question answering (NS-VQA) system first recovers a structural scene representation from the image and a program trace from the question. It then executes the program on the scene representation to obtain an answer. Incorporating symbolic structure as prior knowledge offers three unique advantages. First, executing programs on a symbolic space is more robust to long program traces; our model can solve complex reasoning tasks better, achieving an accuracy of 99.8% on the CLEVR dataset. Second, the model is more data- and memory-efficient: it performs well after learning on a small number of training data; it can also encode an image into a compact representation, requiring less storage than existing methods for offline question answering. Third, symbolic program execution offers full transparency to the reasoning process; we are thus able to interpret and diagnose each execution step.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e650.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e650.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NS-VQA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neural-Symbolic Visual Question Answering (NS-VQA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neural-symbolic VQA system that disentangles perception and language understanding (neural) from reasoning (symbolic) by de-rendering images to compact structural scene representations, parsing questions to programs, and executing deterministic symbolic programs on the scene representation to produce answers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>NS-VQA</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Three-component hybrid system: (1) Scene parser (neural) that uses Mask R-CNN + ResNet to produce segment proposals and per-object attribute vectors (shape, color, material, size, 3D coords); (2) Question parser (neural seq2seq with attention, bidirectional LSTM encoder and LSTM decoder) that maps natural-language questions to hierarchical functional programs; (3) Program executor (declarative/symbolic) implemented as deterministic Python functional modules (set operations, filters, relations, queries, comparisons) that run sequentially on the scene representation to output answers. The architecture intentionally separates learned perception/language modules from an explicit symbolic executor to obtain compositional, interpretable reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Deterministic symbolic program executor implemented as a library of functional modules (set operations: scene/union/intersect/count; relation modules: relate_left/right/front/behind; filter modules filter_color[...], filter_shape[...], filter_size[...]; boolean comparators equal_color/equal_shape/greater_than/less_than; query modules query_color/query_shape/etc.). Programs are hierarchical sequences of these module tokens and operate on an explicit symbolic scene representation (list of object dictionaries and attribute entries). Representation types include 'scene', 'object', 'entry', 'number', 'boolean'.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Neural networks for perception and language: Mask R-CNN (Detectron implementation with ResNet-50-FPN backbone) for segment proposals and discrete attribute classification; ResNet-34 for extracting spatial/continuous attributes from paired segment+image; an attention-based seq2seq question parser with bidirectional LSTM encoder and LSTM decoder (two hidden layers, 256-dim hidden vectors, 300-dim embeddings), trained with supervised pretraining then REINFORCE fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Modular, pipeline integration: neural modules produce structured, symbolic intermediates (scene representation and program token sequence); the symbolic executor consumes these outputs and deterministically executes the program. Training is staged: scene parser trained supervised on segmentation/attributes; question parser pretrained on a small set of question-program pairs (supervised) and fine-tuned with REINFORCE using execution correctness as reward while keeping the executor fixed and deterministic. No end-to-end gradient flow through the symbolic executor.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>High data efficiency (can reach near-perfect accuracy with few program annotations + REINFORCE), strong compositional generalization (recovering and executing long hierarchical programs robustly), compact per-image symbolic encoding (very low storage per image), full step-by-step interpretability of reasoning traces (executor outputs intermediate scenes/objects/entries), and robustness to long program traces (symbolic execution avoids learned executor failure modes that arise in long neural traces).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Visual Question Answering on CLEVR (primary), CLEVR-Humans, CLEVR-CoGenT (generalization to novel attribute combinations), and a Minecraft reasoning dataset (new visual/contextual domain).</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td>99.8% overall accuracy on CLEVR (with pretraining on 270 programs + REINFORCE on QA pairs); various data-efficiency regimes reported (e.g., 99.7%–99.9% on sub-categories).</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Demonstrates strong compositional generalization: robust recovery of underlying programs (e.g., 88% program recovery with 500 annotations, near-perfect with 9K programs), generalizes to CLEVR-Humans question style using REINFORCE and pretrained word embeddings, can be adapted to different visual domains (Minecraft) by retraining the scene parser while keeping question parser/executor intact; limited by perception biases (e.g., attribute recognition network can learn color→shape correlations causing CoGenT issues unless the parser is fine-tuned or modified).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>High interpretability: symbolic executor produces deterministic, human-readable program traces and intermediate outputs (scenes, objects, attribute entries) that can be inspected step-by-step. This enables program recovery evaluation and debugging of errors at specific execution steps.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Requires some annotated programs for supervised pretraining of the question parser (though far fewer than prior baselines); scene parser biases (e.g., color-shape correlations) can hurt generalization across attribute splits unless rectified; building structured representations that generalize to truly novel real-world scenes remains challenging; occlusion and shadow artifacts can degrade object perception leading to downstream reasoning errors; errors from type mismatches in executor lead to random answer sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Division-of-labor principle: learned neural modules map raw perceptual and linguistic inputs into structured symbolic representations, and a deterministic symbolic executor performs logic-like operations for reasoning. The paper frames this as unifying deep representation learning (for perception and language) with symbolic program execution (for interpretable, compositional reasoning) but does not formalize a mathematical theory beyond this complementary strengths argument.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neural-Symbolic VQA: Disentangling Reasoning from Vision and Language Understanding', 'publication_date_yy_mm': '2018-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e650.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e650.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IEP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Inferring and Executing Programs (IEP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid VQA approach that infers latent programs from questions with a recurrent program generator and executes them with a learned, attention-based neural executor (neural module-style execution).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Inferring and executing programs for visual reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>IEP (Inferring and Executing Programs)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Two-part hybrid: a recurrent program generator maps natural-language questions to program sequences; an attentive neural executor implements modules that are learned neural functions (attention-based) and executes the inferred program on visual feature maps to produce answers. The executor is learned (neural) rather than deterministic symbolic code.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Program-like declarative skeleton: programs inferred from questions (functional program sequences) that prescribe modular computations; the program tokens act as high-level declarative structure or control flow specification.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Neural attention-based executor composed of learned modules (neural networks) that process visual features and intermediate tensors; the executor uses attention mechanisms to route computation and is trained end-to-end with supervision when available.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Generator + learned executor pipeline: the program generator supplies a discrete program sequence; the learned neural executor interprets and implements the program by composing neural modules (attention-based). Integration can be end-to-end when program supervision present or via program annotations for training.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Combines compositional program structure with flexible neural execution, enabling good VQA performance on CLEVR and some generalization; however, learned executor behaviors are harder to interpret and can produce spurious programs that nonetheless yield correct answers (i.e., correct answer with incorrect execution trace).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Visual Question Answering on CLEVR (primary), evaluated for program recovery and QA accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td>Reported in this paper: 96.9% overall accuracy on CLEVR (as shown in Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Requires more program annotations to reliably recover true programs (paper reports IEP starts to capture true programs when trained with over 1K programs and recovers ~50% with 9K programs). Tends to sometimes produce incorrect program traces that nonetheless lead to correct answers, indicating weaker program interpretability and program recovery compared to NS-VQA.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Limited interpretability: although program skeletons exist, the learned neural executor’s internal implementations are not fully transparent; the model can output plausible answers via spurious neural computations, making step-by-step interpretability unreliable.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Needs many annotated programs to train the generator-executor pipeline effectively; learned executor is difficult to interpret; prone to producing long incorrect programs that accidentally yield correct answers (spurious programs).</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Uses the idea of inducing latent programs from language as a scaffold for compositional reasoning, relying on learned neural modules to implement semantic functions; no formal symbolic-theoretical unification is proposed in this paper's description beyond this pragmatic division.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neural-Symbolic VQA: Disentangling Reasoning from Vision and Language Understanding', 'publication_date_yy_mm': '2018-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e650.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e650.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Neural Module Networks (Andreas et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Learning to Compose Neural Networks for Question Answering (Neural Module Networks)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of approaches that parse questions into module layouts and compose parameterized neural modules according to that layout to compute answers, thereby combining symbolic structure (layout/program) and neural computation (module implementations).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning to compose neural networks for question answering</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Neural Module Networks (NMN)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Parse-and-compose hybrid: a parser maps a question to a module layout (a small program-like structure); neural modules (each a small neural network implementing a primitive operation) are assembled according to the layout and executed on visual features to yield an answer. The modules are neural (differentiable) but the layout provides declarative compositional structure.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Discrete module layout / program-like structure that acts as a symbolic compositional specification (derived from question parse or semantic parser).</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Learned neural modules (small neural networks) that implement primitive operations (attention, combination, classification) and are composed dynamically according to the layout.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Modular composition: parser produces a discrete layout; at execution time modules are instantiated and connected according to the layout; training can be end-to-end differentiable when layouts are available or use reinforcement/latent-variable methods when not.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Compositional generalization driven by explicit layouts; flexible function implementation via neural modules; improved sample efficiency for compositional tasks compared to monolithic networks when module layouts are accurate.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Visual Question Answering and compositional language-vision tasks (e.g., CLEVR variants and other VQA datasets).</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Intended to provide compositional generalization via explicit module layouts; actual generalization depends on parser accuracy and module training—paper notes using latent structure in language can help QA but training relies on layout supervision or complex latent-variable learning.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Moderate interpretability: layout gives a human-readable program structure, but neural modules themselves can be opaque; interpretability depends on whether module semantics align with human-understandable operations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Requires accurate parsing to layouts or costly supervision; when layouts are latent, training is more difficult and modules may not align with intended symbolic meanings.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Compositional assembly: symbolic layouts prescribe composition while neural modules provide flexible learned implementations; no formal symbolic reasoning theory provided in this paper beyond the compositionality argument.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neural-Symbolic VQA: Disentangling Reasoning from Vision and Language Understanding', 'publication_date_yy_mm': '2018-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e650.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e650.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DDRprog</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DDRprog: Differentiable Dynamic Reasoning Programmer</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A differentiable reasoning system that incorporates program-like structure into neural execution by making program routing and module execution differentiable, enabling hybrid programmatic reasoning within an end-to-end neural framework.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Ddrprog: A clevr differentiable dynamic reasoning programmer</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>DDRprog</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A hybrid/differentiable approach that implements dynamic program-based reasoning where program execution and routing decisions are implemented in a differentiable manner, allowing end-to-end training; it aims to combine program-like compositional structure with neural differentiable execution to improve reasoning while keeping training tractable.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Program-like control structure / dynamic program templates that specify composition of operations (declarative skeleton).</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Differentiable neural implementations of modules and routing, so execution is implemented with neural computation and gradients flow through the program control.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Differentiable integration: program composition and module routing are made differentiable so the entire program execution can be trained end-to-end via gradient-based learning, rather than using a fixed symbolic interpreter.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Better performance on compositional VQA by combining structural program guidance with end-to-end differentiable training; improved ability to learn module routing and module behavior jointly from data.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>CLEVR visual reasoning benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td>98.3% overall accuracy on CLEVR (reported in this paper's Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Designed to improve compositional generalization by imposing program-like structure while retaining differentiable training; shown to be competitive with other program-structured models on CLEVR, though exact generalization trade-offs depend on supervision and model specifics.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Some interpretability afforded by program-like structure, but less transparent than a fully symbolic deterministic executor because execution is differentiable/neural and module behaviors are learned.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Relies on differentiable approximations to discrete program routing which can blur exact semantics; interpretability and exact logical guarantees are weaker than deterministic symbolic executors.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Blends program-structured control with differentiable execution to capture compositional reasoning while enabling gradient-based optimization; argued as a practical compromise between rigid symbolic execution and fully neural end-to-end learning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neural-Symbolic VQA: Disentangling Reasoning from Vision and Language Understanding', 'publication_date_yy_mm': '2018-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e650.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e650.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TbD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transparency by Design (TbD)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neural module-style VQA approach that emphasizes interpretable, attention-based module outputs to close the gap between performance and interpretability in visual reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Transparency by design: Closing the gap between performance and interpretability in visual reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>TbD (Transparency by Design)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A modular neural approach where modules are designed and trained to produce transparent, attention-like intermediate outputs (e.g., explicit segmentation/attention maps) corresponding to reasoning steps; modules are composed according to program-like structures derived from questions to produce final answers.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Program-like assembly/layout that dictates module composition (declarative control flow) and module interfaces aligned with interpretable primitives (e.g., produce attention masks).</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Neural modules implementing primitive operations, trained to output interpretable intermediate representations (attention/segmentation maps); modules are differentiable and trained with supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Modular composition with design constraints promoting transparency: learned neural modules are composed according to parsed programs, and modules are architected/trained so their intermediate outputs are human-interpretable (not a separate symbolic executor).</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>High VQA performance combined with more interpretable intermediate outputs (attention/segmentation maps) that facilitate debugging and understanding of reasoning steps; reduces the 'black-box' behavior typical of neural executors.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>CLEVR visual reasoning benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td>99.1% overall accuracy on CLEVR (reported in this paper's Table 1 under TbD+reg+hres).</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Improved interpretability helps diagnose failures; generalization depends on module training and supervision but TbD shows strong CLEVR performance and improved program-aligned interpretability relative to other neural executors.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Strong interpretability: modules produce explicit, human-inspectable intermediate outputs (attention/segmentation masks) at each reasoning step, enabling transparent reasoning traces.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Still relies on neural modules (learned behavior) so exact logical guarantees of symbolic execution are absent; may require substantial supervision or architectural constraints to enforce interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Practical engineering principle: design neural modules and interfaces to produce transparent intermediate representations, combining module-layout (declarative) structure with learned module implementations (imperative) to trade off performance and interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neural-Symbolic VQA: Disentangling Reasoning from Vision and Language Understanding', 'publication_date_yy_mm': '2018-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e650.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e650.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PSL+Neural (Aditya et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Explicit reasoning over end-to-end neural architectures for visual question answering (probabilistic soft logic integration)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that integrates probabilistic soft logic (PSL), a soft declarative reasoning formalism, into neural attention modules to impart interpretability and explicit reasoning constraints into otherwise end-to-end neural VQA models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Explicit reasoning over end-to-end neural architectures for visual question answering</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>PSL-integrated neural attention</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Hybrid integration where probabilistic soft logic (a declarative, weighted logical formalism) is incorporated into neural attention architectures to impose reasoning constraints and provide interpretable outputs; the neural model supplies evidential scores and PSL performs soft logical inference to refine or interpret attention/answers.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Probabilistic Soft Logic (PSL): a soft, weighted logic/inference layer that encodes relational constraints and produces probabilistic consistency-enforced outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>End-to-end neural attention modules producing evidence (attention maps, feature scores) that feed into the PSL layer.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Neural outputs are combined with PSL rules; PSL operates as a soft declarative reasoning post-processing or integrated module to enforce constraints and provide interpretable conclusions. Integration yields some interpretability of the attention module behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Improved interpretability through explicit logical constraints on neural outputs; better diagnosis of attention behavior and partial enforcement of symbolic consistency in neural predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Visual Question Answering tasks (general VQA datasets as considered in the cited work).</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Intended to improve consistency/generalization by imposing logical constraints; specific generalization metrics are not provided in this paper's discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Provides interpretability benefits by making neural attention outputs subject to explicitly stated logical constraints and by exposing PSL-inferred relations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>PSL integration provides soft constraints and does not yield hard symbolic guarantees; scalability and reliance on rule specification may limit applicability without careful engineering.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Combines probabilistic logical reasoning (PSL) with neural evidence to gain interpretability and constrained inference; framed as a way to inject explicit reasoning without discarding end-to-end neural training.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neural-Symbolic VQA: Disentangling Reasoning from Vision and Language Understanding', 'publication_date_yy_mm': '2018-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Inferring and executing programs for visual reasoning <em>(Rating: 2)</em></li>
                <li>Learning to compose neural networks for question answering <em>(Rating: 2)</em></li>
                <li>Transparency by design: Closing the gap between performance and interpretability in visual reasoning <em>(Rating: 2)</em></li>
                <li>Ddrprog: A clevr differentiable dynamic reasoning programmer <em>(Rating: 2)</em></li>
                <li>Explicit reasoning over end-to-end neural architectures for visual question answering <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-650",
    "paper_id": "paper-9d15ebe3f5aaf32a9f835f88703241461324c35b",
    "extraction_schema_id": "extraction-schema-17",
    "extracted_data": [
        {
            "name_short": "NS-VQA",
            "name_full": "Neural-Symbolic Visual Question Answering (NS-VQA)",
            "brief_description": "A neural-symbolic VQA system that disentangles perception and language understanding (neural) from reasoning (symbolic) by de-rendering images to compact structural scene representations, parsing questions to programs, and executing deterministic symbolic programs on the scene representation to produce answers.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "NS-VQA",
            "system_description": "Three-component hybrid system: (1) Scene parser (neural) that uses Mask R-CNN + ResNet to produce segment proposals and per-object attribute vectors (shape, color, material, size, 3D coords); (2) Question parser (neural seq2seq with attention, bidirectional LSTM encoder and LSTM decoder) that maps natural-language questions to hierarchical functional programs; (3) Program executor (declarative/symbolic) implemented as deterministic Python functional modules (set operations, filters, relations, queries, comparisons) that run sequentially on the scene representation to output answers. The architecture intentionally separates learned perception/language modules from an explicit symbolic executor to obtain compositional, interpretable reasoning.",
            "declarative_component": "Deterministic symbolic program executor implemented as a library of functional modules (set operations: scene/union/intersect/count; relation modules: relate_left/right/front/behind; filter modules filter_color[...], filter_shape[...], filter_size[...]; boolean comparators equal_color/equal_shape/greater_than/less_than; query modules query_color/query_shape/etc.). Programs are hierarchical sequences of these module tokens and operate on an explicit symbolic scene representation (list of object dictionaries and attribute entries). Representation types include 'scene', 'object', 'entry', 'number', 'boolean'.",
            "imperative_component": "Neural networks for perception and language: Mask R-CNN (Detectron implementation with ResNet-50-FPN backbone) for segment proposals and discrete attribute classification; ResNet-34 for extracting spatial/continuous attributes from paired segment+image; an attention-based seq2seq question parser with bidirectional LSTM encoder and LSTM decoder (two hidden layers, 256-dim hidden vectors, 300-dim embeddings), trained with supervised pretraining then REINFORCE fine-tuning.",
            "integration_method": "Modular, pipeline integration: neural modules produce structured, symbolic intermediates (scene representation and program token sequence); the symbolic executor consumes these outputs and deterministically executes the program. Training is staged: scene parser trained supervised on segmentation/attributes; question parser pretrained on a small set of question-program pairs (supervised) and fine-tuned with REINFORCE using execution correctness as reward while keeping the executor fixed and deterministic. No end-to-end gradient flow through the symbolic executor.",
            "emergent_properties": "High data efficiency (can reach near-perfect accuracy with few program annotations + REINFORCE), strong compositional generalization (recovering and executing long hierarchical programs robustly), compact per-image symbolic encoding (very low storage per image), full step-by-step interpretability of reasoning traces (executor outputs intermediate scenes/objects/entries), and robustness to long program traces (symbolic execution avoids learned executor failure modes that arise in long neural traces).",
            "task_or_benchmark": "Visual Question Answering on CLEVR (primary), CLEVR-Humans, CLEVR-CoGenT (generalization to novel attribute combinations), and a Minecraft reasoning dataset (new visual/contextual domain).",
            "hybrid_performance": "99.8% overall accuracy on CLEVR (with pretraining on 270 programs + REINFORCE on QA pairs); various data-efficiency regimes reported (e.g., 99.7%–99.9% on sub-categories).",
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": true,
            "generalization_properties": "Demonstrates strong compositional generalization: robust recovery of underlying programs (e.g., 88% program recovery with 500 annotations, near-perfect with 9K programs), generalizes to CLEVR-Humans question style using REINFORCE and pretrained word embeddings, can be adapted to different visual domains (Minecraft) by retraining the scene parser while keeping question parser/executor intact; limited by perception biases (e.g., attribute recognition network can learn color→shape correlations causing CoGenT issues unless the parser is fine-tuned or modified).",
            "interpretability_properties": "High interpretability: symbolic executor produces deterministic, human-readable program traces and intermediate outputs (scenes, objects, attribute entries) that can be inspected step-by-step. This enables program recovery evaluation and debugging of errors at specific execution steps.",
            "limitations_or_failures": "Requires some annotated programs for supervised pretraining of the question parser (though far fewer than prior baselines); scene parser biases (e.g., color-shape correlations) can hurt generalization across attribute splits unless rectified; building structured representations that generalize to truly novel real-world scenes remains challenging; occlusion and shadow artifacts can degrade object perception leading to downstream reasoning errors; errors from type mismatches in executor lead to random answer sampling.",
            "theoretical_framework": "Division-of-labor principle: learned neural modules map raw perceptual and linguistic inputs into structured symbolic representations, and a deterministic symbolic executor performs logic-like operations for reasoning. The paper frames this as unifying deep representation learning (for perception and language) with symbolic program execution (for interpretable, compositional reasoning) but does not formalize a mathematical theory beyond this complementary strengths argument.",
            "uuid": "e650.0",
            "source_info": {
                "paper_title": "Neural-Symbolic VQA: Disentangling Reasoning from Vision and Language Understanding",
                "publication_date_yy_mm": "2018-10"
            }
        },
        {
            "name_short": "IEP",
            "name_full": "Inferring and Executing Programs (IEP)",
            "brief_description": "A hybrid VQA approach that infers latent programs from questions with a recurrent program generator and executes them with a learned, attention-based neural executor (neural module-style execution).",
            "citation_title": "Inferring and executing programs for visual reasoning",
            "mention_or_use": "mention",
            "system_name": "IEP (Inferring and Executing Programs)",
            "system_description": "Two-part hybrid: a recurrent program generator maps natural-language questions to program sequences; an attentive neural executor implements modules that are learned neural functions (attention-based) and executes the inferred program on visual feature maps to produce answers. The executor is learned (neural) rather than deterministic symbolic code.",
            "declarative_component": "Program-like declarative skeleton: programs inferred from questions (functional program sequences) that prescribe modular computations; the program tokens act as high-level declarative structure or control flow specification.",
            "imperative_component": "Neural attention-based executor composed of learned modules (neural networks) that process visual features and intermediate tensors; the executor uses attention mechanisms to route computation and is trained end-to-end with supervision when available.",
            "integration_method": "Generator + learned executor pipeline: the program generator supplies a discrete program sequence; the learned neural executor interprets and implements the program by composing neural modules (attention-based). Integration can be end-to-end when program supervision present or via program annotations for training.",
            "emergent_properties": "Combines compositional program structure with flexible neural execution, enabling good VQA performance on CLEVR and some generalization; however, learned executor behaviors are harder to interpret and can produce spurious programs that nonetheless yield correct answers (i.e., correct answer with incorrect execution trace).",
            "task_or_benchmark": "Visual Question Answering on CLEVR (primary), evaluated for program recovery and QA accuracy.",
            "hybrid_performance": "Reported in this paper: 96.9% overall accuracy on CLEVR (as shown in Table 1).",
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": true,
            "generalization_properties": "Requires more program annotations to reliably recover true programs (paper reports IEP starts to capture true programs when trained with over 1K programs and recovers ~50% with 9K programs). Tends to sometimes produce incorrect program traces that nonetheless lead to correct answers, indicating weaker program interpretability and program recovery compared to NS-VQA.",
            "interpretability_properties": "Limited interpretability: although program skeletons exist, the learned neural executor’s internal implementations are not fully transparent; the model can output plausible answers via spurious neural computations, making step-by-step interpretability unreliable.",
            "limitations_or_failures": "Needs many annotated programs to train the generator-executor pipeline effectively; learned executor is difficult to interpret; prone to producing long incorrect programs that accidentally yield correct answers (spurious programs).",
            "theoretical_framework": "Uses the idea of inducing latent programs from language as a scaffold for compositional reasoning, relying on learned neural modules to implement semantic functions; no formal symbolic-theoretical unification is proposed in this paper's description beyond this pragmatic division.",
            "uuid": "e650.1",
            "source_info": {
                "paper_title": "Neural-Symbolic VQA: Disentangling Reasoning from Vision and Language Understanding",
                "publication_date_yy_mm": "2018-10"
            }
        },
        {
            "name_short": "Neural Module Networks (Andreas et al.)",
            "name_full": "Learning to Compose Neural Networks for Question Answering (Neural Module Networks)",
            "brief_description": "A family of approaches that parse questions into module layouts and compose parameterized neural modules according to that layout to compute answers, thereby combining symbolic structure (layout/program) and neural computation (module implementations).",
            "citation_title": "Learning to compose neural networks for question answering",
            "mention_or_use": "mention",
            "system_name": "Neural Module Networks (NMN)",
            "system_description": "Parse-and-compose hybrid: a parser maps a question to a module layout (a small program-like structure); neural modules (each a small neural network implementing a primitive operation) are assembled according to the layout and executed on visual features to yield an answer. The modules are neural (differentiable) but the layout provides declarative compositional structure.",
            "declarative_component": "Discrete module layout / program-like structure that acts as a symbolic compositional specification (derived from question parse or semantic parser).",
            "imperative_component": "Learned neural modules (small neural networks) that implement primitive operations (attention, combination, classification) and are composed dynamically according to the layout.",
            "integration_method": "Modular composition: parser produces a discrete layout; at execution time modules are instantiated and connected according to the layout; training can be end-to-end differentiable when layouts are available or use reinforcement/latent-variable methods when not.",
            "emergent_properties": "Compositional generalization driven by explicit layouts; flexible function implementation via neural modules; improved sample efficiency for compositional tasks compared to monolithic networks when module layouts are accurate.",
            "task_or_benchmark": "Visual Question Answering and compositional language-vision tasks (e.g., CLEVR variants and other VQA datasets).",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Intended to provide compositional generalization via explicit module layouts; actual generalization depends on parser accuracy and module training—paper notes using latent structure in language can help QA but training relies on layout supervision or complex latent-variable learning.",
            "interpretability_properties": "Moderate interpretability: layout gives a human-readable program structure, but neural modules themselves can be opaque; interpretability depends on whether module semantics align with human-understandable operations.",
            "limitations_or_failures": "Requires accurate parsing to layouts or costly supervision; when layouts are latent, training is more difficult and modules may not align with intended symbolic meanings.",
            "theoretical_framework": "Compositional assembly: symbolic layouts prescribe composition while neural modules provide flexible learned implementations; no formal symbolic reasoning theory provided in this paper beyond the compositionality argument.",
            "uuid": "e650.2",
            "source_info": {
                "paper_title": "Neural-Symbolic VQA: Disentangling Reasoning from Vision and Language Understanding",
                "publication_date_yy_mm": "2018-10"
            }
        },
        {
            "name_short": "DDRprog",
            "name_full": "DDRprog: Differentiable Dynamic Reasoning Programmer",
            "brief_description": "A differentiable reasoning system that incorporates program-like structure into neural execution by making program routing and module execution differentiable, enabling hybrid programmatic reasoning within an end-to-end neural framework.",
            "citation_title": "Ddrprog: A clevr differentiable dynamic reasoning programmer",
            "mention_or_use": "mention",
            "system_name": "DDRprog",
            "system_description": "A hybrid/differentiable approach that implements dynamic program-based reasoning where program execution and routing decisions are implemented in a differentiable manner, allowing end-to-end training; it aims to combine program-like compositional structure with neural differentiable execution to improve reasoning while keeping training tractable.",
            "declarative_component": "Program-like control structure / dynamic program templates that specify composition of operations (declarative skeleton).",
            "imperative_component": "Differentiable neural implementations of modules and routing, so execution is implemented with neural computation and gradients flow through the program control.",
            "integration_method": "Differentiable integration: program composition and module routing are made differentiable so the entire program execution can be trained end-to-end via gradient-based learning, rather than using a fixed symbolic interpreter.",
            "emergent_properties": "Better performance on compositional VQA by combining structural program guidance with end-to-end differentiable training; improved ability to learn module routing and module behavior jointly from data.",
            "task_or_benchmark": "CLEVR visual reasoning benchmark.",
            "hybrid_performance": "98.3% overall accuracy on CLEVR (reported in this paper's Table 1).",
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": true,
            "generalization_properties": "Designed to improve compositional generalization by imposing program-like structure while retaining differentiable training; shown to be competitive with other program-structured models on CLEVR, though exact generalization trade-offs depend on supervision and model specifics.",
            "interpretability_properties": "Some interpretability afforded by program-like structure, but less transparent than a fully symbolic deterministic executor because execution is differentiable/neural and module behaviors are learned.",
            "limitations_or_failures": "Relies on differentiable approximations to discrete program routing which can blur exact semantics; interpretability and exact logical guarantees are weaker than deterministic symbolic executors.",
            "theoretical_framework": "Blends program-structured control with differentiable execution to capture compositional reasoning while enabling gradient-based optimization; argued as a practical compromise between rigid symbolic execution and fully neural end-to-end learning.",
            "uuid": "e650.3",
            "source_info": {
                "paper_title": "Neural-Symbolic VQA: Disentangling Reasoning from Vision and Language Understanding",
                "publication_date_yy_mm": "2018-10"
            }
        },
        {
            "name_short": "TbD",
            "name_full": "Transparency by Design (TbD)",
            "brief_description": "A neural module-style VQA approach that emphasizes interpretable, attention-based module outputs to close the gap between performance and interpretability in visual reasoning tasks.",
            "citation_title": "Transparency by design: Closing the gap between performance and interpretability in visual reasoning",
            "mention_or_use": "mention",
            "system_name": "TbD (Transparency by Design)",
            "system_description": "A modular neural approach where modules are designed and trained to produce transparent, attention-like intermediate outputs (e.g., explicit segmentation/attention maps) corresponding to reasoning steps; modules are composed according to program-like structures derived from questions to produce final answers.",
            "declarative_component": "Program-like assembly/layout that dictates module composition (declarative control flow) and module interfaces aligned with interpretable primitives (e.g., produce attention masks).",
            "imperative_component": "Neural modules implementing primitive operations, trained to output interpretable intermediate representations (attention/segmentation maps); modules are differentiable and trained with supervision.",
            "integration_method": "Modular composition with design constraints promoting transparency: learned neural modules are composed according to parsed programs, and modules are architected/trained so their intermediate outputs are human-interpretable (not a separate symbolic executor).",
            "emergent_properties": "High VQA performance combined with more interpretable intermediate outputs (attention/segmentation maps) that facilitate debugging and understanding of reasoning steps; reduces the 'black-box' behavior typical of neural executors.",
            "task_or_benchmark": "CLEVR visual reasoning benchmark.",
            "hybrid_performance": "99.1% overall accuracy on CLEVR (reported in this paper's Table 1 under TbD+reg+hres).",
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": true,
            "generalization_properties": "Improved interpretability helps diagnose failures; generalization depends on module training and supervision but TbD shows strong CLEVR performance and improved program-aligned interpretability relative to other neural executors.",
            "interpretability_properties": "Strong interpretability: modules produce explicit, human-inspectable intermediate outputs (attention/segmentation masks) at each reasoning step, enabling transparent reasoning traces.",
            "limitations_or_failures": "Still relies on neural modules (learned behavior) so exact logical guarantees of symbolic execution are absent; may require substantial supervision or architectural constraints to enforce interpretability.",
            "theoretical_framework": "Practical engineering principle: design neural modules and interfaces to produce transparent intermediate representations, combining module-layout (declarative) structure with learned module implementations (imperative) to trade off performance and interpretability.",
            "uuid": "e650.4",
            "source_info": {
                "paper_title": "Neural-Symbolic VQA: Disentangling Reasoning from Vision and Language Understanding",
                "publication_date_yy_mm": "2018-10"
            }
        },
        {
            "name_short": "PSL+Neural (Aditya et al.)",
            "name_full": "Explicit reasoning over end-to-end neural architectures for visual question answering (probabilistic soft logic integration)",
            "brief_description": "An approach that integrates probabilistic soft logic (PSL), a soft declarative reasoning formalism, into neural attention modules to impart interpretability and explicit reasoning constraints into otherwise end-to-end neural VQA models.",
            "citation_title": "Explicit reasoning over end-to-end neural architectures for visual question answering",
            "mention_or_use": "mention",
            "system_name": "PSL-integrated neural attention",
            "system_description": "Hybrid integration where probabilistic soft logic (a declarative, weighted logical formalism) is incorporated into neural attention architectures to impose reasoning constraints and provide interpretable outputs; the neural model supplies evidential scores and PSL performs soft logical inference to refine or interpret attention/answers.",
            "declarative_component": "Probabilistic Soft Logic (PSL): a soft, weighted logic/inference layer that encodes relational constraints and produces probabilistic consistency-enforced outputs.",
            "imperative_component": "End-to-end neural attention modules producing evidence (attention maps, feature scores) that feed into the PSL layer.",
            "integration_method": "Neural outputs are combined with PSL rules; PSL operates as a soft declarative reasoning post-processing or integrated module to enforce constraints and provide interpretable conclusions. Integration yields some interpretability of the attention module behavior.",
            "emergent_properties": "Improved interpretability through explicit logical constraints on neural outputs; better diagnosis of attention behavior and partial enforcement of symbolic consistency in neural predictions.",
            "task_or_benchmark": "Visual Question Answering tasks (general VQA datasets as considered in the cited work).",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Intended to improve consistency/generalization by imposing logical constraints; specific generalization metrics are not provided in this paper's discussion.",
            "interpretability_properties": "Provides interpretability benefits by making neural attention outputs subject to explicitly stated logical constraints and by exposing PSL-inferred relations.",
            "limitations_or_failures": "PSL integration provides soft constraints and does not yield hard symbolic guarantees; scalability and reliance on rule specification may limit applicability without careful engineering.",
            "theoretical_framework": "Combines probabilistic logical reasoning (PSL) with neural evidence to gain interpretability and constrained inference; framed as a way to inject explicit reasoning without discarding end-to-end neural training.",
            "uuid": "e650.5",
            "source_info": {
                "paper_title": "Neural-Symbolic VQA: Disentangling Reasoning from Vision and Language Understanding",
                "publication_date_yy_mm": "2018-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Inferring and executing programs for visual reasoning",
            "rating": 2
        },
        {
            "paper_title": "Learning to compose neural networks for question answering",
            "rating": 2
        },
        {
            "paper_title": "Transparency by design: Closing the gap between performance and interpretability in visual reasoning",
            "rating": 2
        },
        {
            "paper_title": "Ddrprog: A clevr differentiable dynamic reasoning programmer",
            "rating": 2
        },
        {
            "paper_title": "Explicit reasoning over end-to-end neural architectures for visual question answering",
            "rating": 1
        }
    ],
    "cost": 0.01704,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Neural-Symbolic VQA: Disentangling Reasoning from Vision and Language Understanding</h1>
<p>Kexin Yi*<br>Harvard University<br>Antonio Torralba MIT CSAIL</p>
<p>Jiajun Wu*<br>MIT CSAIL<br>Pushmeet Kohli<br>DeepMind</p>
<p>Chuang Gan<br>MIT-IBM Watson AI Lab<br>Joshua B. Tenenbaum<br>MIT CSAIL</p>
<h2>Abstract</h2>
<p>We marry two powerful ideas: deep representation learning for visual recognition and language understanding, and symbolic program execution for reasoning. Our neural-symbolic visual question answering (NS-VQA) system first recovers a structural scene representation from the image and a program trace from the question. It then executes the program on the scene representation to obtain an answer. Incorporating symbolic structure as prior knowledge offers three unique advantages. First, executing programs on a symbolic space is more robust to long program traces; our model can solve complex reasoning tasks better, achieving an accuracy of $99.8 \%$ on the CLEVR dataset. Second, the model is more data- and memory-efficient: it performs well after learning on a small number of training data; it can also encode an image into a compact representation, requiring less storage than existing methods for offline question answering. Third, symbolic program execution offers full transparency to the reasoning process; we are thus able to interpret and diagnose each execution step.</p>
<h2>1 Introduction</h2>
<p>Looking at the images and questions in Figure 1, we instantly recognize objects and their attributes, parse complicated questions, and leverage such knowledge to reason and answer the questions. We can also clearly explain how we reason to obtain the answer. Now imagine that you are standing in front of the scene, eyes closed, only able to build your scene representation through touch. Not surprisingly, reasoning without vision remains effortless. For humans, reasoning is fully interpretable, and not necessarily interwoven with visual perception.
The advances in deep representation learning and the development of large-scale datasets [Malinowski and Fritz, 2014, Antol et al., 2015] have inspired a number of pioneering approaches in visual questionanswering (VQA), most trained in an end-to-end fashion [Yang et al., 2016]. Though innovative, pure neural net-based approaches often perform less well on challenging reasoning tasks. In particular, a recent study [Johnson et al., 2017a] designed a new VQA dataset, CLEVR, in which each image comes with intricate, compositional questions generated by programs, and showed that state-of-the-art VQA models did not perform well.
Later, Johnson et al. [2017b] demonstrated that machines can learn to reason by wiring in prior knowledge of human language as programs. Specifically, their model integrates a program generator that infers the underlying program from a question, and a learned, attention-based executor that runs the program on the input image. Such a combination achieves very good performance on the CLEVR</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>How many blocks are on the right of the three-level tower?
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Will the block tower fall if the top block is removed?
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>What is the shape of the object closest to the large cylinder?
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Are there more trees than animals?</p>
<p>Figure 1: Human reasoning is interpretable and disentangled: we first draw abstract knowledge of the scene via visual perception and then perform logic reasoning on it. This enables compositional, accurate, and generalizable reasoning in rich visual contexts.
dataset, and generalizes reasonably well to CLEVR-Humans, a dataset that contains the same images as CLEVR but now paired with human-generated questions. However, their model still suffers from two limitations: first, training the program generator requires many annotated examples; second, the behaviors of the attention-based neural executor are hard to explain. In contrast, we humans can reason on CLEVR and CLEVR-Humans even with a few labeled instances, and we can also clearly explain how we do it.
In this paper, we move one step further along the spectrum of learning vs. modeling, proposing a neural-symbolic approach for visual question answering (NS-VQA) that fully disentangles vision and language understanding from reasoning. We use neural networks as powerful tools for parsing inferring structural, object-based scene representation from images, and generating programs from questions. We then incorporate a symbolic program executor that, complementary to the neural parser, runs the program on the scene representation to obtain an answer.
The combination of deep recognition modules and a symbolic program executor offers three unique advantages. First, the use of symbolic representation offers robustness to long, complex program traces. It also reduces the need of training data. On the CLEVR dataset, our method is trained on questions with 270 program annotations plus 4 K images, and is able to achieve a near-perfect accuracy of $99.8 \%$.
Second, both our reasoning module and visual scene representation are light-weighted, requiring minimal computational and memory cost. In particular, our compact structural image representation requires much less storage during reasoning, reducing the memory cost by $99 \%$ compared with other state-of-the-art algorithms.
Third, the use of symbolic scene representation and program traces forces the model to accurately recover underlying programs from questions. Together with the fully transparent and interpretable nature of symbolic representations, the reasoning process can be analyzed and diagnosed step-bystep.</p>
<h1>2 Related Work</h1>
<p>Structural scene representation. Our work is closely related to research on learning an interpretable, disentangled representation with a neural network [Kulkarni et al., 2015, Yang et al., 2015, Wu et al., 2017]. For example, Kulkarni et al. [2015] proposed convolutional inverse graphics networks that learn to infer the pose and lighting of a face; Yang et al. [2015] explored learning disentangled representations of pose and content from chair images. There has also been work on learning disentangled representation without direct supervision [Higgins et al., 2018, Siddharth et al., 2017, Vedantam et al., 2018], some with sequential generative models [Eslami et al., 2016, Ba et al., 2015]. In a broader view, our model also relates to the field of "vision as inverse graphics" [Yuille and Kersten, 2006]. Our NS-VQA model builds upon the structural scene representation [Wu et al., 2017] and explores how it can be used for visual reasoning.</p>
<p>Program induction from language. Recent papers have explored using program search and neural networks to recover programs from a domain-specific language [Balog et al., 2017, Neelakantan et al., 2016, Parisotto et al., 2017]. For sentences, semantic parsing methods map them to logical forms via a knowledge base or a program [Berant et al., 2013, Liang et al., 2013, Vinyals et al.,</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 2: Our model has three components: first, a scene parser (de-renderer) that segments an input image (a-b) and recovers a structural scene representation (c); second, a question parser (program generator) that converts a question in natural language (d) into a program (e); third, a program executor that runs the program on the structural scene representation to obtain the answer.</p>
<p>2015, Guu et al., 2017]. In particular, Andreas et al. [2016] attempted to use the latent structure in language to help question answering and reasoning, Rothe et al. [2017] studied the use of formal programs in modeling human questions, and Goldman et al. [2018] used abstract examples to build weakly-supervised semantic parsers.</p>
<p>Visual question answering. Visual question answering (VQA) [Malinowski and Fritz, 2014, Antol et al., 2015] is a versatile and challenging test bed for AI systems. Compared with the wellstudied text-based question answering, VQA emerges by its requirement on both semantic and visual understanding. There have been numerous papers on VQA, among which some explicitly used structural knowledge to help reasoning [Wang et al., 2017]. Current leading approaches are based on neural attentions [Yang et al., 2016, Lu et al., 2016], which draw inspiration from human perception and learn to attend the visual components that serve as informative evidence to the question. Nonetheless, Jabri et al. [2016] recently proposed a remarkably simple yet effective classification baseline. Their system directly extracts visual and text features from whole images and questions, concatenates them, and trains multi-class classifiers to select answers. This paper, among others [Goyal et al., 2017], reveals potential caveats in the proposed VQA systems-models are overfitting dataset biases.</p>
<p>Visual reasoning. Johnson et al. [2017a] built a new VQA dataset, named CLEVR, carefully controlling the potential bias and benchmarking how well models reason. Their subsequent model achieved good results on CLEVR by combining a recurrent program generator and an attentive execution engine [Johnson et al., 2017b]. There have been other end-to-end neural models that has achieved nice performance on the dataset, exploiting various attention structures and accounting object relations [Hudson and Manning, 2018, Santoro et al., 2017, Hu et al., 2017, Perez et al., 2018, Zhu et al., 2017]. More recently, several papers have proposed to directly incorporate the syntactic and logic structures of the reasoning task to the attentive module network's architecture for reasoning. These structures include the underlying functional programs [Mascharka et al., 2018, Suarez et al., 2018] and dependency trees [Cao et al., 2018] of the input question. However, training of the models relies heavily on these extra signals. From a broader perspective, Misra et al. [2018] explored learning to reason by asking questions and Bisk et al. [2018] studied spatial reasoning in a 3D blocks world. Recently, Aditya et al. [2018] incorporated probabilistic soft logic into a neural attention module and obtained some interpretability of the model, and Gan et al. [2017] learned to associate image segments with questions. Our model moves along this direction further by modeling the entire scene into an object-based, structural representation, and integrating it with a fully transparent and interpretable symbolic program executor.</p>
<p>3 Approach</p>
<p>Our NS-VQA model has three components: a scene parser (de-renderer), a question parser (program generator), and a program executor. Given an image-question pair, the scene parser de-renders the image to obtain a structural scene representation (Figure 2-I), the question parser generates a hierarchical program from the question (Figure 2-II), and the executor runs the program on the structural representation to obtain an answer (Figure 2-III).</p>
<p>Our scene parser recovers a structural and disentangled representation of the scene in the image (Figure 2a), based on which we can perform fully interpretable symbolic reasoning. The parser takes a two-step, segment-based approach for de-rendering: it first generates a number of segment proposals (Figure 2b), and for each segment, classifies the object and its attributes. The final, structural scene representation is disentangled, compact, and rich (Figure 2c).</p>
<p>The question parser maps an input question in natural language (Figure 2d) to a latent program (Figure 2e). The program has a hierarchy of functional modules, each fulfilling an independent operation on the scene representation. Using a hierarchical program as our reasoning backbone naturally supplies compositionality and generalization power. The program executor takes the output sequence from the question parser, applies these functional modules on the abstract scene representation of the input image, and generates the final answer (Figure 2-III). The executable program performs purely symbolic operations on its input throughout the entire execution process, and is fully deterministic, disentangled, and interpretable with respect to the program sequence.</p>
<h3>3.1 Model Details</h3>
<p>Scene parser. For each image, we use Mask R-CNN [He et al., 2017] to generate segment proposals of all objects. Along with the segmentation mask, the network also predicts the categorical labels of discrete intrinsic attributes such as color, material, size, and shape. Proposals with bounding box score less than 0.9 are dropped. The segment for each single object is then paired with the original image, resized to 224 by 224 and sent to a ResNet-34 [He et al., 2015] to extract the spacial attributes such as pose and 3D coordinates. Here the inclusion of the original full image enables the use of contextual information.</p>
<p>Question parser. Our question parser is an attention-based sequence to sequence (seq2seq) model with an encoder-decoder structure similar to that in Luong et al. [2015] and Bahdanau et al. [2015]. The encoder is a bidirectional LSTM [Hochreiter and Schmidhuber, 1997] that takes as input a question of variable lengths and outputs an encoded vector $e_{i}$ at time step $i$ as</p>
<p>$e_{i}=[e_{i}^{F},e_{i}^{B}],\quad\text{where}\quad e_{i}^{F},h_{i}^{F}=\operatorname{LSTM}\left(\Phi_{E}\left(x_{i}\right),h_{i-1}^{F}\right),\quad e_{i}^{B},h_{i}^{B}=\operatorname{LSTM}\left(\Phi_{E}\left(x_{i}\right),h_{i+1}^{B}\right).$ (1)</p>
<p>Here $\Phi_{E}$ is the jointly trained encoder word embedding. $(e_{i}^{F},h_{i}^{F})$, $(e_{i}^{B},h_{i}^{B})$ are the outputs and hidden vectors of the forward and backward networks at time step $i$. The decoder is a similar LSTM that generates a vector $q_{t}$ from the previous token of the output sequence $y_{t-1}$. $q_{t}$ is then fed to an attention layer to obtain a context vector $c_{t}$ as a weighted sum of the encoded states via</p>
<p>$q_{t}=\operatorname{LSTM}\left(\Phi_{D}\left(y_{t-1}\right)\right), \quad \alpha_{t i} \propto \exp \left(q_{t}^{\top} W_{A} e_{i}\right), \quad c_{t}=\sum_{i} \alpha_{t i} e_{i} .$ (2)</p>
<p>$\Phi_{D}$ is the decoder word embedding. For simplicity we set the dimensions of vectors $q_{t}, e_{i}$ to be the same and let the attention weight matrix $W_{A}$ to be an identity matrix. Finally, the context vector, together with the decoder output, is passed to a fully connected layer with softmax activation to obtain the distribution for the predicted token $y_{t} \sim \operatorname{softmax}\left(W_{O}\left[q_{t}, c_{t}\right]\right)$. Both the encoder and decoder have two hidden layers with a 256-dim hidden vector. We set the dimensions of both the encoder and decoder word vectors to be 300 .</p>
<p>Program executor. We implement the program executor as a collection of deterministic, generic functional modules in Python, designed to host all logic operations behind the questions in the dataset. Each functional module is in one-to-one correspondence with tokens from the input program sequence, which has the same representation as in Johnson et al. [2017b]. The modules share the same input/output interface, and therefore can be arranged in any length and order. A typical program sequence begins with a scene token, which signals the input of the original scene representation. Each functional module then sequentially executes on the output of the previous one. The last module</p>
<table>
<thead>
<tr>
<th>Methods</th>
<th>Count</th>
<th>Exist</th>
<th>Compare</th>
<th>Compare</th>
<th>Query</th>
<th>Overall</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td>Number</td>
<td>Attribute</td>
<td>Attribute</td>
<td></td>
</tr>
<tr>
<td>Humans <em>Johnson et al. (2017b)</em></td>
<td>86.7</td>
<td>96.6</td>
<td>86.4</td>
<td>96.0</td>
<td>95.0</td>
<td>92.6</td>
</tr>
<tr>
<td>CNN+LSTM+SAN <em>Johnson et al. (2017b)</em></td>
<td>59.7</td>
<td>77.9</td>
<td>75.1</td>
<td>70.8</td>
<td>80.9</td>
<td>73.2</td>
</tr>
<tr>
<td>N2NMN^{∗} <em>Hu et al. (2017)</em></td>
<td>68.5</td>
<td>85.7</td>
<td>84.9</td>
<td>88.7</td>
<td>90.0</td>
<td>83.7</td>
</tr>
<tr>
<td>Dependency Tree <em>Cao et al. (2018)</em></td>
<td>81.4</td>
<td>94.2</td>
<td>81.6</td>
<td>97.1</td>
<td>90.5</td>
<td>89.3</td>
</tr>
<tr>
<td>CNN+LSTM+RN <em>Santoro et al. (2017)</em></td>
<td>90.1</td>
<td>97.8</td>
<td>93.6</td>
<td>97.1</td>
<td>97.9</td>
<td>95.5</td>
</tr>
<tr>
<td>IEP^{∗} <em>Johnson et al. (2017b)</em></td>
<td>92.7</td>
<td>97.1</td>
<td>98.7</td>
<td>98.9</td>
<td>98.1</td>
<td>96.9</td>
</tr>
<tr>
<td>CNN+GRU+FiLM <em>Perez et al. (2018)</em></td>
<td>94.5</td>
<td>99.2</td>
<td>93.8</td>
<td>99.0</td>
<td>99.2</td>
<td>97.6</td>
</tr>
<tr>
<td>DDRprog^{∗} <em>Suarez et al. (2018)</em></td>
<td>96.5</td>
<td>98.8</td>
<td>98.4</td>
<td>99.0</td>
<td>99.1</td>
<td>98.3</td>
</tr>
<tr>
<td>MAC <em>Hudson and Manning (2018)</em></td>
<td>97.1</td>
<td>99.5</td>
<td>99.1</td>
<td>99.5</td>
<td>99.5</td>
<td>98.9</td>
</tr>
<tr>
<td>TbD+reg+hres^{∗} <em>Mascharka et al. (2018)</em></td>
<td>97.6</td>
<td>99.2</td>
<td>99.4</td>
<td>99.6</td>
<td>99.5</td>
<td>99.1</td>
</tr>
<tr>
<td>NS-VQA (ours, 90 programs)</td>
<td>64.5</td>
<td>87.4</td>
<td>53.7</td>
<td>77.4</td>
<td>79.7</td>
<td>74.4</td>
</tr>
<tr>
<td>NS-VQA (ours, 180 programs)</td>
<td>85.0</td>
<td>92.9</td>
<td>83.4</td>
<td>90.6</td>
<td>92.2</td>
<td>89.5</td>
</tr>
<tr>
<td>NS-VQA (ours, 270 programs)</td>
<td>99.7</td>
<td>99.9</td>
<td>99.9</td>
<td>99.8</td>
<td>99.8</td>
<td>99.8</td>
</tr>
</tbody>
</table>
<p>Table 1: Our model (NS-VQA) outperforms current state-of-the-art methods on CLEVR and achieves near-perfect question answering accuracy. The question-program pairs used for pretraining our model are uniformly drawn from the 90 question families of the dataset: 90, 180, 270 programs correspond to 1, 2, 3 samples from each family respectively. (*): trains on all program annotations (700K).</p>
<p>outputs the final answer to the question. When type mismatch occurs between input and output across adjacent modules, an error flag is raised to the output, in which case the model will randomly sample an answer from all possible outputs of the final module. Figure 3 shows two examples.</p>
<h3>3.2 Training Paradigm</h3>
<h4>Scene parsing.</h4>
<p>Our implementation of the object proposal network (Mask R-CNN) is based on "Detectron" <em>Girshick et al. (2018)</em>. We use ResNet-50 FPN <em>Lin et al. (2017)</em> as the backbone and train the model for 30,000 iterations with eight images per batch. Please refer to <em>He et al. (2017)</em> and <em>Girshick et al. (2018)</em> for more details. Our feature extraction network outputs the values of continuous attributes. We train the network on the proposed object segments computed from the training data using the mean square error as loss function for 30,000 iterations with learning rate 0.002 and batch size 50. Both networks of our scene parser are trained on 4,000 generated CLEVR images.</p>
<h4>Reasoning.</h4>
<p>We adopt the following two-step procedure to train the question parser to learn the mapping from a question to a program. First, we select a small number of ground truth question-program pairs from the training set to pretrain the model with direct supervision. Then, we pair it with our deterministic program executor, and use REINFORCE <em>Williams (1992)</em> to fine-tune the parser on a larger set of question-answer pairs, using only the correctness of the execution result as the reward signal.</p>
<p>During supervised pretraining, we train with learning rate $7 \times 10^{-4}$ for 20,000 iterations. For reinforce, we set the learning rate to be $10^{-5}$ and run at most 2M iterations with early stopping. The reward is maximized over a constant baseline with a decay weight 0.9 to reduce variance. Batch size is fixed to be 64 for both training stages. All our models are implemented in PyTorch.</p>
<h2>4 Evaluations</h2>
<p>We demonstrate the following advantages of our disentangled structural scene representation and symbolic execution engine. First, our model can learn from a small number of training data and outperform the current state-of-the-art methods while precisely recovering the latent programs (Sections 4.1). Second, our model generalizes well to other question styles (Sections 4.3), attribute combinations (Sections 4.2), and visual context (Section 4.4). Code of our model is available at https://github.com/kexinyi/ns-vqa</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 3: Qualitative results on CLEVR. Blue color indicates correct program modules and answers; red indicates wrong ones. Our model is able to robustly recover the correct programs compared to the IEP baseline.</p>
<h3>4.1 Data-Efficient, Interpretable Reasoning</h3>
<p><strong>Setup.</strong> We evaluate our NS-VQA on CLEVR [Johnson et al., 2017a]. The dataset includes synthetic images of 3D primitives with multiple attributes—shape, color, material, size, and 3D coordinates. Each image has a set of questions, each of which associates with a program (a set of symbolic modules) generated by machines based on 90 logic templates.</p>
<p>Our structural scene representation for a CLEVR image characterizes the objects in it, each labeled with its shape, size, color, material, and 3D coordinates (see Figure 2c). We evaluate our model's performance on the validation set under various supervise signal for training, including the numbers of ground-truth programs used for pretraining and question-answer pairs for REINFORCE. Results are compared with other state-of-the-art methods including the IEP baseline [Johnson et al., 2017b]. We not only assess the correctness of the answer obtained by our model, but also how well it recovers the underlying program. An interpretable model should be able to output the correct program in addition to the correct answer.</p>
<p><strong>Results.</strong> Quantitative results on the CLEVR dataset are summarized in Table 1. Our NS-VQA achieves near-perfect accuracy and outperforms other methods on all five question types. We first pretrain the question parser on 270 annotated programs sampled across the 90 question templates (3 questions per template), a number below the weakly supervised limit suggested by Johnson et al. [2017b] (9K), and then run REINFORCE on all the question-answer pairs. Repeated experiments starting from different sets of programs show a standard deviation of less than 0.1 percent on the results for 270 pretraining programs (and beyond). The variances are larger when we train our model with fewer programs (90 and 180). The reported numbers are the mean of three runs.</p>
<p>We further investigate the data-efficiency of our method with respect to both the number of programs used for pretraining and the overall question-answer pairs used in REINFORCE. Figure 4a shows the result when we vary the number of pretraining programs. NS-VQA outperforms the IEP baseline under various conditions, even with a weaker supervision during REINFORCE (2K and 9K question-answer pairs in REINFORCE). The number of question-answer pairs can be further reduced by pretraining the model on a larger set of annotated programs. For example, our model achieves the same near-perfect accuracy of 99.8% with 9K question-answer pairs with annotated programs for both pretraining and REINFORCE.</p>
<p>Figure 4b compares how well our NS-VQA recovers the underlying programs compared to the IEP model. IEP starts to capture the true programs when trained with over 1K programs, and only recovers half of the programs with 9K programs. Qualitative examples in Figure 3 demonstrate that IEP tends to fake a long wrong program that leads to the correct answer. In contrast, our model achieves 88% program accuracy with 500 annotations, and performs almost perfectly on both</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 4: Our model exhibits high data efficiency while achieving state-of-the-art performance and preserving interpretability. (a) QA accuracy vs. number of programs used for pretraining; different curves indicate different numbers of question-answer pairs used in the REINFORCE stage. (c) QA accuracy vs. total number of training question-answer pairs; our model is pretrained on 270 programs.
question answering and program recovery with 9 K programs. Figure 4c shows the QA accuracy vs. the number of questions and answers used for training, where our NS-VQA has the highest performance under all conditions. Among the baseline methods we compare with, MAC [Hudson and Manning, 2018] obtains high accuracy with zero program annotations; in comparison, our method needs to be pretrained on 270 program annotations, but requires fewer question-answer pairs to reach similar performance.</p>
<p>Our model also requires minimal memory for offline question answering: the structural representation of each image only occupies less than 100 bytes; in comparison, attention-based methods like IEP requires storing either the original image or its feature maps, taking at least 20K bytes per image.</p>
<h1>4.2 Generalizing to Unseen Attribute Combinations</h1>
<p>Recent neural reasoning models have achieved impressive performance on the original CLEVR QA task [Johnson et al., 2017b, Mascharka et al., 2018, Perez et al., 2018], but they generalize less well across biased dataset splits. This is revealed on the CLEVR-CoGenT dataset [Johnson et al., 2017a], a benchmark designed specifically for testing models' generalization to novel attribute compositions.</p>
<p>Setup. The CLEVR-CoGenT dataset is derived from CLEVR and separated into two biased splits: split A only contains cubes that are either gray, blue, brown or yellow, and cylinders that are red, green, purple or cyan; split B has the opposite color-shape pairs for cubes and cylinders. Both splits contain spheres of any color. Split A has 70 K images and 700 K questions for training and both splits have 15 K images and 150 K questions for evaluation and testing. The desired behavior of a generalizable model is to perform equally well on both splits while only trained on split A.</p>
<p>Results. Table 2a shows the generalization results with a few interesting findings. The vanilla NS-VQA trained purely on split A and fine-tuned purely on split B (1000 images) does not generalize as well as the state-of-the-art. We observe that this is because of the bias in the attribute recognition network of the scene parser, which learns to classify object shape based on color. NS-VQA works well after we fine-tune it on data from both splits ( $4000 \mathrm{~A}, 1000 \mathrm{~B}$ ). Here, we only fine-tune the attribute recognition network with annotated images from split B, but no questions or programs; thanks to the disentangled pipeline and symbolic scene representation, our question parser and executor are not overfitting to particular splits. To validate this, we train a separate shape recognition network that takes gray-scale but not color images as input (NS-VQA+Gray). The augmented model works well on both splits without seeing any data from split B. Further, with an image parser trained on the original condition (i.e. the same as in CLEVR), our question parser and executor also generalize well across splits (NS-VQA+Ori).</p>
<h3>4.3 Generalizing to Questions from Humans</h3>
<p>Our model also enables efficient generalization toward more realistic question styles over the same logic domain. We evaluate this on the CLEVR-Humans dataset, which includes human-generated</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Methods</th>
<th style="text-align: center;">Not Fine-tuned</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Fine-tune on</th>
<th style="text-align: center;">Fine-tuned</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">A</td>
<td style="text-align: center;">B</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">A</td>
<td style="text-align: center;">B</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">CNN+LSTM+SA</td>
<td style="text-align: center;">80.3</td>
<td style="text-align: center;">68.7</td>
<td style="text-align: center;">B</td>
<td style="text-align: center;">75.7</td>
<td style="text-align: center;">75.8</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">IEP (18K programs)</td>
<td style="text-align: center;">96.6</td>
<td style="text-align: center;">73.7</td>
<td style="text-align: center;">B</td>
<td style="text-align: center;">76.1</td>
<td style="text-align: center;">92.7</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">CNN+GRU+FiLM</td>
<td style="text-align: center;">98.3</td>
<td style="text-align: center;">78.8</td>
<td style="text-align: center;">B</td>
<td style="text-align: center;">81.1</td>
<td style="text-align: center;">96.9</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">TbD+reg</td>
<td style="text-align: center;">98.8</td>
<td style="text-align: center;">75.4</td>
<td style="text-align: center;">B</td>
<td style="text-align: center;">96.9</td>
<td style="text-align: center;">96.3</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">NS-VQA (ours)</td>
<td style="text-align: center;">99.8</td>
<td style="text-align: center;">63.9</td>
<td style="text-align: center;">B</td>
<td style="text-align: center;">64.9</td>
<td style="text-align: center;">98.9</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">NS-VQA (ours)</td>
<td style="text-align: center;">99.8</td>
<td style="text-align: center;">63.9</td>
<td style="text-align: center;">A+B</td>
<td style="text-align: center;">99.6</td>
<td style="text-align: center;">99.0</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">NS-VQA+Gray (ours)</td>
<td style="text-align: center;">99.6</td>
<td style="text-align: center;">98.4</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">NS-VQA+Ori (ours)</td>
<td style="text-align: center;">99.8</td>
<td style="text-align: center;">99.7</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>(a) Generalization results on CLEVR-CoGenT.</p>
<p>Table 2: Generalizing to unseen attribute compositions and question styles. (a) Our image parser is trained on 4,000 synthetic images from split A and fine-tuned on 1,000 images from split B. The question parser is only trained on split A starting from 500 programs. Baseline methods are fine-tuned on 3 K images plus 30 K questions from split B. NS-VQA+Gray adopts a gray channel in the image parser for shape recognition and NS-VQA+Ori uses an image parser trained from the original images from CLEVR. Please see text for more details. (b) Our model outperforms IEP on CLEVR-Humans under various training conditions.
questions on CLEVR images (see <em>Johnson et al. [2017b]</em> for details). The questions follow real-life human conversation style without a regular structural expression.</p>
<p>Setup. We adopt a training paradigm for CLEVR-Humans similar to the original CLEVR dataset: we first pretrain the model with a limited number of programs from CLEVR, and then fine-tune it on CLEVR-Humans with REINFORCE. We initialize the encoder word embedding by the GloVe word vectors [Pennington et al., 2014] and keep it fixed during pretraining. The REINFORCE stage lasts for at most 1 M iterations; early stop is applied.</p>
<p>Results. The results on CLEVR-Humans are summarized in Table 2b. Our NS-VQA outperforms IEP on CLEVR-Humans by a considerable margin under small amount of annotated programs. This shows our structural scene representation and symbolic program executor helps to exploit the strong exploration power of REINFORCE, and also demonstrates the model's generalizability across different question styles.</p>
<h3>4.4 Extending to New Scene Context</h3>
<p>Structural scene representation and symbolic programs can also be extended to other visual and contextual scenarios. Here we show results on reasoning tasks from the Minecraft world.</p>
<p>Setup. We now consider a new dataset where objects and scenes are taken from Minecraft and therefore have drastically different scene context and visual appearance. We use the dataset generation tool provided by <em>Wu et al. [2017]</em> to render 10,000 Minecraft scenes, building upon the Malmo interface [Johnson et al., 2016]. Each image consists of 3 to 6 objects, and each object is sampled from a set of 12 entities. We use the same configuration details as suggested by <em>Wu et al. [2017]</em>. Our structural representation has the following fields for each object: category (12-dim), position in the 2D plane (2-dim, ${x, z}$ ), and the direction the object faces ${$ front, back, left, right $}$. Each object is thus encoded as a 18 -dim vector.</p>
<p>We generate diverse questions and programs associated with each Minecraft image based on the objects' categorical and spatial attributes (position, direction). Each question is composed as a hierarchy of three families of basic questions: first, querying object attributes (class, location, direction); second, counting the number of objects satisfying certain constraints; third, verifying if an object has certain property. Our dataset differs from CLEVR primarily in two ways: Minecraft hosts a larger set of 3D objects with richer image content and visual appearance; our questions and programs involve hierarchical attributes. For example, a "wolf" and a "pig" are both "animals", and an "animal" and a "tree" are both "creatures". We use the first 9,000 images with 88,109 questions</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 5: Our model also applies to Minecraft, a world with rich and hierarchical scene context and different visual appearance.
for training and the remaining 1,000 images with 9,761 questions for testing. We follow the same recipe as described in Section 3.2 for training on Minecraft.</p>
<p>Results. Quantitative results are summarized in Table 5b. The overall behavior is similar to that on the CLEVR dataset, except that reasoning on Minecraft generally requires weaker initial program signals. Figure 5a shows the results on three test images: our NS-VQA finds the correct answer and recovers the correct program under the new scene context. Also, most of our model's wrong answers on this dataset are due to errors in perceiving heavily occluded objects, while the question parser still preserves its power to parse input questions.</p>
<h1>5 Discussion</h1>
<p>We have presented a neural-symbolic VQA approach that disentangles reasoning from visual perception and language understanding. Our model uses deep learning for inverse graphics and inverse language modeling-recognizing and characterizing objects in the scene; it then uses a symbolic program executor to reason and answer questions.
We see our research suggesting a possible direction to unify two powerful ideas: deep representation learning and symbolic program execution. Our model connects to, but also differs from the recent pure deep learning approaches for visual reasoning. Wiring in symbolic representation as prior knowledge increases performance, reduces the need for annotated data and for memory significantly, and makes reasoning fully interpretable.
The machine learning community has often been skeptical of symbolic reasoning, as symbolic approaches can be brittle or have difficulty generalizing to natural situations. Some of these concerns are less applicable to our work, as we leverage learned abstract representations for mapping both visual and language inputs to an underlying symbolic reasoning substrate. However, building structured representations for scenes and sentence meanings-the targets of these mappings-in ways that generalize to truly novel situations remains a challenge for many approaches including ours. Recent progress on unsupervised or weakly supervised representation learning, in both language and vision, offers some promise of generalization. Integrating this work with our neural-symbolic approach to visually grounded language is a promising future direction.</p>
<h2>Acknowledgments</h2>
<p>We thank Jiayuan Mao, Karthik Narasimhan, and Jon Gauthier for helpful discussions and suggestions. We also thank Drew A. Hudson for sharing experimental results for comparison. This work is in part supported by ONR MURI N00014-16-1-2007, the Center for Brain, Minds, and Machines (CBMM), IBM Research, and Facebook.</p>
<h1>References</h1>
<p>Somak Aditya, Yezhou Yang, and Chitta Baral. Explicit reasoning over end-to-end neural architectures for visual question answering. In $A A A I, 2018.3$</p>
<p>Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. Learning to compose neural networks for question answering. In NAACL-HLT, 2016. 3</p>
<p>Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh. Vqa: Visual question answering. In ICCV, 2015. 1, 3</p>
<p>Jimmy Ba, Volodymyr Mnih, and Koray Kavukcuoglu. Multiple object recognition with visual attention. In $I C L R, 2015.2$</p>
<p>Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. In $I C L R, 2015.4$</p>
<p>Matej Balog, Alexander L Gaunt, Marc Brockschmidt, Sebastian Nowozin, and Daniel Tarlow. Deepcoder: Learning to write programs. In $I C L R, 2017.2$</p>
<p>Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic parsing on freebase from questionanswer pairs. In EMNLP, 2013. 2</p>
<p>Yonatan Bisk, Kevin J Shih, Yejin Choi, and Daniel Marcu. Learning interpretable spatial operations in a rich 3d blocks world. In $A A A I, 2018.3$</p>
<p>Qingxing Cao, Xiaodan Liang, Bailing Li, Guanbin Li, and Liang Lin. Visual question reasoning on general dependency tree. In CVPR, 2018. 3, 5</p>
<p>SM Eslami, Nicolas Heess, Theophane Weber, Yuval Tassa, Koray Kavukcuoglu, and Geoffrey E Hinton. Attend, infer, repeat: Fast scene understanding with generative models. In NIPS, 2016. 2</p>
<p>Chuang Gan, Yandong Li, Haoxiang Li, Chen Sun, and Boqing Gong. Vqs: Linking segmentations to questions and answers for supervised attention in vqa and question-focused semantic segmentation. In ICCV, 2017. 3</p>
<p>Ross Girshick, Ilija Radosavovic, Georgia Gkioxari, Piotr Dollár, and Kaiming He. Detectron. https: //github.com/facebookresearch/detectron, 2018.5</p>
<p>Omer Goldman, Veronica Latcinnik, Udi Naveh, Amir Globerson, and Jonathan Berant. Weakly-supervised semantic parsing with abstract examples. In $A C L, 2018.3$</p>
<p>Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa matter: Elevating the role of image understanding in visual question answering. In CVPR, 2017. 3</p>
<p>Kelvin Guu, Panupong Pasupat, Evan Zheran Liu, and Percy Liang. From language to programs: Bridging reinforcement learning and maximum marginal likelihood. In ACL, 2017. 3</p>
<p>Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2015. 4</p>
<p>Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. Mask r-cnn. In ICCV, 2017. 4, 5
Irina Higgins, Nicolas Sonnerat, Loic Matthey, Arka Pal, Christopher P Burgess, Matthew Botvinick, Demis Hassabis, and Alexander Lerchner. Scan: learning abstract hierarchical compositional visual concepts. In $I C L R, 2018.2$</p>
<p>Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural Comput., 9(8):1735-1780, 1997. 4
Ronghang Hu, Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Kate Saenko. Learning to reason: End-to-end module networks for visual question answering. In CVPR, 2017. 3, 5</p>
<p>Drew A Hudson and Christopher D Manning. Compositional attention networks for machine reasoning. In $I C L R, 2018.3,5,7$</p>
<p>Allan Jabri, Armand Joulin, and Laurens van der Maaten. Revisiting visual question answering baselines. In ECCV, 2016. 3</p>
<p>Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C Lawrence Zitnick, and Ross Girshick. Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. In CVPR, 2017a. 1, $3,6,7$</p>
<p>Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Judy Hoffman, Li Fei-Fei, C Lawrence Zitnick, and Ross Girshick. Inferring and executing programs for visual reasoning. In ICCV, 2017b. 1, 3, 4, 5, 6, 7, 8</p>
<p>Matthew Johnson, Katja Hofmann, Tim Hutton, and David Bignell. The malmo platform for artificial intelligence experimentation. In IJCAI, 2016. 8</p>
<p>Tejas D Kulkarni, William F Whitney, Pushmeet Kohli, and Joshua B Tenenbaum. Deep convolutional inverse graphics network. In NIPS, 2015. 2</p>
<p>Percy Liang, Michael I Jordan, and Dan Klein. Learning dependency-based compositional semantics. Computational Linguistics, 39(2):389-446, 2013. 2</p>
<p>Tsung-Yi Lin, Piotr Dollár, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection. In CVPR, 2017. 5</p>
<p>Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh. Hierarchical question-image co-attention for visual question answering. In NIPS, 2016. 3</p>
<p>Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-based neural machine translation. In EMNLP, 2015. 4</p>
<p>M Malinowski and M Fritz. A multi-world approach to question answering about real-world scenes based on uncertain input. In NIPS, 2014. 1, 3</p>
<p>David Mascharka, Philip Tran, Ryan Soklaski, and Arjun Majumdar. Transparency by design: Closing the gap between performance and interpretability in visual reasoning. In CVPR, 2018. 3, 5, 7</p>
<p>Ishan Misra, Ross Girshick, Rob Fergus, Martial Hebert, Abhinav Gupta, and Laurens van der Maaten. Learning by asking questions. In CVPR, 2018. 3</p>
<p>Arvind Neelakantan, Quoc V Le, and Ilya Sutskever. Neural programmer: Inducing latent programs with gradient descent. In $I C L R, 2016.2$</p>
<p>Emilio Parisotto, Abdel-rahman Mohamed, Rishabh Singh, Lihong Li, Dengyong Zhou, and Pushmeet Kohli. Neuro-symbolic program synthesis. In $I C L R, 2017.2$</p>
<p>Jeffrey Pennington, Richard Socher, and Christopher Manning. Glove: Global vectors for word representation. In EMNLP, 2014. 8</p>
<p>Ethan Perez, Florian Strub, Harm De Vries, Vincent Dumoulin, and Aaron Courville. Film: Visual reasoning with a general conditioning layer. In AAAI, 2018. 3, 5, 7</p>
<p>Anselm Rothe, Brenden M Lake, and Todd Gureckis. Question asking as program generation. In NIPS, 2017. 3
Adam Santoro, David Raposo, David GT Barrett, Mateusz Malinowski, Razvan Pascanu, Peter Battaglia, and Timothy Lillicrap. A simple neural network module for relational reasoning. In NIPS, 2017. 3, 5</p>
<p>N Siddharth, T. B. Paige, J.W. Meent, A. Desmaison, N. Goodman, P. Kohli, F. Wood, and P. Torr. Learning disentangled representations with semi-supervised deep generative models. In NIPS, 2017. 2</p>
<p>Joseph Suarez, Justin Johnson, and Fei-Fei Li. Ddrprog: A clevr differentiable dynamic reasoning programmer. arXiv:1803.11361, 2018. 3, 5</p>
<p>Ramakrishna Vedantam, Ian Fischer, Jonathan Huang, and Kevin Murphy. Generative models of visually grounded imagination. In $I C L R, 2018.2$</p>
<p>Oriol Vinyals, Lukasz Kaiser, Terry Koo, Slav Petrov, Ilya Sutskever, and Geoffrey Hinton. Grammar as a foreign language. In NIPS, 2015. 2</p>
<p>Peng Wang, Qi Wu, Chunhua Shen, Anton van den Hengel, and Anthony Dick. Explicit knowledge-based reasoning for visual question answering. In IJCAI, 2017. 3</p>
<p>Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. MLJ, 8(3-4):229-256, 1992. 5</p>
<p>Jiajun Wu, Joshua B Tenenbaum, and Pushmeet Kohli. Neural scene de-rendering. In CVPR, 2017. 2, 8
Jimei Yang, Scott E Reed, Ming-Hsuan Yang, and Honglak Lee. Weakly-supervised disentangling with recurrent transformations for 3d view synthesis. In NIPS, 2015. 2</p>
<p>Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, and Alex Smola. Stacked attention networks for image question answering. In CVPR, 2016. 1, 3</p>
<p>Alan Yuille and Daniel Kersten. Vision as bayesian inference: analysis by synthesis? Trends Cogn. Sci., 10(7): 301-308, 2006. 2</p>
<p>Chen Zhu, Yanpeng Zhao, Shuaiyi Huang, Kewei Tu, and Yi Ma. Structured attentions for visual question answering. In ICCV, 2017. 3</p>
<h1>Supplementary Material</h1>
<h2>A Scene Parser Details</h2>
<p>Data. Our scene parser is trained on 4,000 CLEVR-style images rendered by Blender with object masks and ground-truth attributes including color, material, shape, size, and 3D coordinates. Because the original CLEVR dataset does not include object masks, we generate these 4,000 training images ourselves using the CLEVR dataset generation tool ${ }^{\dagger}$. For the CLEVR-CoGenT experiment, we generate another set of images that satisfy the attribute composition restrictions, using the same software.</p>
<p>Training. We first train the Mask-RCNN object detector on the rendered images and masks. For the CLEVR dataset, the bounding-box classifier contains 48 classes, each representing one composition of object intrinsic attributes of three shapes, two materials, and eight colors (i.e. "blue rubber cube"). Then we run the detector on the same training images to obtain object segmentation proposals, and pair each segment to a labeled object. The segment-label pairs are then used for training the feature extraction CNN. Before entering the CNN, the object segment is concatenated with the original image to provide contextual information.</p>
<h2>B Program Executor Details</h2>
<p>Our program executor is implemented as a collection of functional modules in Python, each executing a designated logic operation on a abstract scene representation. Given a program sequence, the modules are executed one by one; The output of a module is iteratively passed to the next. The input and output types of the modules include the following: object, a dictionary containing the full abstract representation of a single object; scene, a list of objects; entry, an indicator of any object attribute values (color, material, shape, size); number; boolean. All program modules are summarized in the following tables.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Module</th>
<th style="text-align: left;">Input type</th>
<th style="text-align: left;">Output type</th>
<th style="text-align: left;">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">scene</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">scene</td>
<td style="text-align: left;">Return a list of all objects</td>
</tr>
<tr>
<td style="text-align: left;">unique</td>
<td style="text-align: left;">scene</td>
<td style="text-align: left;">object</td>
<td style="text-align: left;">Return the only object in the scene</td>
</tr>
<tr>
<td style="text-align: left;">union</td>
<td style="text-align: left;">scene</td>
<td style="text-align: left;">scene</td>
<td style="text-align: left;">Return the union of two scenes</td>
</tr>
<tr>
<td style="text-align: left;">intersect</td>
<td style="text-align: left;">scene</td>
<td style="text-align: left;">scene</td>
<td style="text-align: left;">Return the intersection of two scenes</td>
</tr>
<tr>
<td style="text-align: left;">count</td>
<td style="text-align: left;">scene</td>
<td style="text-align: left;">number</td>
<td style="text-align: left;">Return the number of objects in a scene</td>
</tr>
</tbody>
</table>
<p>Table 3: Set operation modules of the program executor.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Module</th>
<th style="text-align: left;">Input type</th>
<th style="text-align: left;">Output type</th>
<th style="text-align: left;">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">equal_color</td>
<td style="text-align: left;">(entry, entry)</td>
<td style="text-align: left;">Boolean</td>
<td style="text-align: left;">Return whether input colors are the same</td>
</tr>
<tr>
<td style="text-align: left;">equal_material</td>
<td style="text-align: left;">(entry, entry)</td>
<td style="text-align: left;">Boolean</td>
<td style="text-align: left;">Return whether input materials are the same</td>
</tr>
<tr>
<td style="text-align: left;">equal_shape</td>
<td style="text-align: left;">(entry, entry)</td>
<td style="text-align: left;">Boolean</td>
<td style="text-align: left;">Return whether input shapes are the same</td>
</tr>
<tr>
<td style="text-align: left;">equal_size</td>
<td style="text-align: left;">(entry, entry)</td>
<td style="text-align: left;">Boolean</td>
<td style="text-align: left;">Return whether input sizes are the same</td>
</tr>
<tr>
<td style="text-align: left;">equal_integer</td>
<td style="text-align: left;">(number, number)</td>
<td style="text-align: left;">Boolean</td>
<td style="text-align: left;">Return whether input numbers equal</td>
</tr>
<tr>
<td style="text-align: left;">greater_than</td>
<td style="text-align: left;">(number, number)</td>
<td style="text-align: left;">Boolean</td>
<td style="text-align: left;">Return whether the first number is greater than the second</td>
</tr>
<tr>
<td style="text-align: left;">less_than</td>
<td style="text-align: left;">(number, number)</td>
<td style="text-align: left;">Boolean</td>
<td style="text-align: left;">Return whether the first number is less than the second</td>
</tr>
<tr>
<td style="text-align: left;">exist</td>
<td style="text-align: left;">scene</td>
<td style="text-align: left;">Boolean</td>
<td style="text-align: left;">Return whether the input scene includes any object</td>
</tr>
</tbody>
</table>
<p>Table 4: Boolean operation modules of the program executor.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Module</th>
<th style="text-align: left;">Input type</th>
<th style="text-align: left;">Output type</th>
<th style="text-align: left;">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">query_color</td>
<td style="text-align: left;">object</td>
<td style="text-align: left;">entry</td>
<td style="text-align: left;">Return the color of the input object</td>
</tr>
<tr>
<td style="text-align: left;">query_material</td>
<td style="text-align: left;">object</td>
<td style="text-align: left;">entry</td>
<td style="text-align: left;">Return the material of the input object</td>
</tr>
<tr>
<td style="text-align: left;">query_size</td>
<td style="text-align: left;">object</td>
<td style="text-align: left;">entry</td>
<td style="text-align: left;">Return the size of the input object</td>
</tr>
<tr>
<td style="text-align: left;">query_shape</td>
<td style="text-align: left;">object</td>
<td style="text-align: left;">entry</td>
<td style="text-align: left;">Return the shape of the input object</td>
</tr>
</tbody>
</table>
<p>Table 5: Query modules of the program executor.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Module</th>
<th style="text-align: left;">Input type</th>
<th style="text-align: left;">Output type</th>
<th style="text-align: left;">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">relate_front</td>
<td style="text-align: left;">object</td>
<td style="text-align: left;">scene</td>
<td style="text-align: left;">Return all objects in front</td>
</tr>
<tr>
<td style="text-align: left;">relate_behind</td>
<td style="text-align: left;">object</td>
<td style="text-align: left;">scene</td>
<td style="text-align: left;">Return all objects behind</td>
</tr>
<tr>
<td style="text-align: left;">relate_left</td>
<td style="text-align: left;">object</td>
<td style="text-align: left;">scene</td>
<td style="text-align: left;">Return all objects to the left</td>
</tr>
<tr>
<td style="text-align: left;">relate_right</td>
<td style="text-align: left;">object</td>
<td style="text-align: left;">scene</td>
<td style="text-align: left;">Return all objects to the right</td>
</tr>
<tr>
<td style="text-align: left;">same_color</td>
<td style="text-align: left;">object</td>
<td style="text-align: left;">scene</td>
<td style="text-align: left;">Return all objects of the same color</td>
</tr>
<tr>
<td style="text-align: left;">same_material</td>
<td style="text-align: left;">object</td>
<td style="text-align: left;">scene</td>
<td style="text-align: left;">Return all objects of the same material</td>
</tr>
<tr>
<td style="text-align: left;">same_shape</td>
<td style="text-align: left;">object</td>
<td style="text-align: left;">scene</td>
<td style="text-align: left;">Return all objects of the same shape</td>
</tr>
<tr>
<td style="text-align: left;">same_size</td>
<td style="text-align: left;">object</td>
<td style="text-align: left;">scene</td>
<td style="text-align: left;">Return all objects of the same size</td>
</tr>
</tbody>
</table>
<p>Table 6: Relation modules of the program executor.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Module</th>
<th style="text-align: left;">Input type</th>
<th style="text-align: left;">Output type</th>
<th style="text-align: left;">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">filter_color[blue]</td>
<td style="text-align: left;">scene</td>
<td style="text-align: left;">scene</td>
<td style="text-align: left;">Select all blue objects from the input scene</td>
</tr>
<tr>
<td style="text-align: left;">filter_color[brown]</td>
<td style="text-align: left;">scene</td>
<td style="text-align: left;">scene</td>
<td style="text-align: left;">Select all brown objects from the input scene</td>
</tr>
<tr>
<td style="text-align: left;">filter_color[cyan]</td>
<td style="text-align: left;">scene</td>
<td style="text-align: left;">scene</td>
<td style="text-align: left;">Select all cyan objects from the input scene</td>
</tr>
<tr>
<td style="text-align: left;">filter_color[gray]</td>
<td style="text-align: left;">scene</td>
<td style="text-align: left;">scene</td>
<td style="text-align: left;">Select all gray objects from the input scene</td>
</tr>
<tr>
<td style="text-align: left;">filter_color[green]</td>
<td style="text-align: left;">scene</td>
<td style="text-align: left;">scene</td>
<td style="text-align: left;">Select all green objects from the input scene</td>
</tr>
<tr>
<td style="text-align: left;">filter_color[purple]</td>
<td style="text-align: left;">scene</td>
<td style="text-align: left;">scene</td>
<td style="text-align: left;">Select all purple objects from the input scene</td>
</tr>
<tr>
<td style="text-align: left;">filter_color[red]</td>
<td style="text-align: left;">scene</td>
<td style="text-align: left;">scene</td>
<td style="text-align: left;">Select all red objects from the input scene</td>
</tr>
<tr>
<td style="text-align: left;">filter_color[yellow]</td>
<td style="text-align: left;">scene</td>
<td style="text-align: left;">scene</td>
<td style="text-align: left;">Select all yellow objects from the input scene</td>
</tr>
<tr>
<td style="text-align: left;">filter_material [metal]</td>
<td style="text-align: left;">scene</td>
<td style="text-align: left;">scene</td>
<td style="text-align: left;">Select all metal objects from the input scene</td>
</tr>
<tr>
<td style="text-align: left;">filter_material [rubber]</td>
<td style="text-align: left;">scene</td>
<td style="text-align: left;">scene</td>
<td style="text-align: left;">Select all rubber objects from the input scene</td>
</tr>
<tr>
<td style="text-align: left;">filter_shape [cube]</td>
<td style="text-align: left;">scene</td>
<td style="text-align: left;">scene</td>
<td style="text-align: left;">Select all cubes from the input scene</td>
</tr>
<tr>
<td style="text-align: left;">filter_shape [cylinder]</td>
<td style="text-align: left;">scene</td>
<td style="text-align: left;">scene</td>
<td style="text-align: left;">Select all cylinders from the input scene</td>
</tr>
<tr>
<td style="text-align: left;">filter_shape[sphere]</td>
<td style="text-align: left;">scene</td>
<td style="text-align: left;">scene</td>
<td style="text-align: left;">Select all spheres from the input scene</td>
</tr>
<tr>
<td style="text-align: left;">filter_size[large]</td>
<td style="text-align: left;">scene</td>
<td style="text-align: left;">scene</td>
<td style="text-align: left;">Select all large objects from the input scene</td>
</tr>
<tr>
<td style="text-align: left;">filter_size[small]</td>
<td style="text-align: left;">scene</td>
<td style="text-align: left;">scene</td>
<td style="text-align: left;">Select all small objects from the input scene</td>
</tr>
</tbody>
</table>
<p>Table 7: Filter modules of the program executor.</p>
<h1>C Running Examples</h1>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 6: Running example of NS-VQA. Intermediate outputs from the program execution trace can be a scene (a list of objects), a single object, or an entry of certain attribute (i.e. "blue", "rubber").</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Scene</p>
<table>
<thead>
<tr>
<th style="text-align: left;">ID</th>
<th style="text-align: left;">Size</th>
<th style="text-align: left;">Shape</th>
<th style="text-align: left;">Material</th>
<th style="text-align: left;">Color</th>
<th style="text-align: left;">x</th>
<th style="text-align: left;">y</th>
<th style="text-align: left;">z</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">1</td>
<td style="text-align: left;">Large</td>
<td style="text-align: left;">Cylinder</td>
<td style="text-align: left;">Rubber</td>
<td style="text-align: left;">Brown</td>
<td style="text-align: left;">-2.48</td>
<td style="text-align: left;">0.18</td>
<td style="text-align: left;">0.69</td>
</tr>
<tr>
<td style="text-align: left;">2</td>
<td style="text-align: left;">Large</td>
<td style="text-align: left;">Cube</td>
<td style="text-align: left;">Rubber</td>
<td style="text-align: left;">Gray</td>
<td style="text-align: left;">-0.52</td>
<td style="text-align: left;">2.56</td>
<td style="text-align: left;">0.70</td>
</tr>
<tr>
<td style="text-align: left;">3</td>
<td style="text-align: left;">Small</td>
<td style="text-align: left;">Cube</td>
<td style="text-align: left;">Metal</td>
<td style="text-align: left;">Gray</td>
<td style="text-align: left;">1.88</td>
<td style="text-align: left;">2.02</td>
<td style="text-align: left;">0.35</td>
</tr>
<tr>
<td style="text-align: left;">4</td>
<td style="text-align: left;">Small</td>
<td style="text-align: left;">Cylinder</td>
<td style="text-align: left;">Rubber</td>
<td style="text-align: left;">Green</td>
<td style="text-align: left;">-1.95</td>
<td style="text-align: left;">-1.40</td>
<td style="text-align: left;">0.34</td>
</tr>
<tr>
<td style="text-align: left;">5</td>
<td style="text-align: left;">Large</td>
<td style="text-align: left;">Sphere</td>
<td style="text-align: left;">Metal</td>
<td style="text-align: left;">Purple</td>
<td style="text-align: left;">0.97</td>
<td style="text-align: left;">-1.82</td>
<td style="text-align: left;">0.70</td>
</tr>
</tbody>
</table>
<p>Objects
<img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Question: Is the purple thing the same shape as the large gray rubber thing?</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Program</th>
<th style="text-align: left;">Output</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">scene</td>
<td style="text-align: left;">$[1,2,3,4,5]$</td>
</tr>
<tr>
<td style="text-align: left;">filter_size[large]</td>
<td style="text-align: left;">$[1,2,5]$</td>
</tr>
<tr>
<td style="text-align: left;">filter_color[gray]</td>
<td style="text-align: left;">$[2]$</td>
</tr>
<tr>
<td style="text-align: left;">filter_material[rubber]</td>
<td style="text-align: left;">$[2]$</td>
</tr>
<tr>
<td style="text-align: left;">unique</td>
<td style="text-align: left;">2</td>
</tr>
<tr>
<td style="text-align: left;">query_shape</td>
<td style="text-align: left;">cube - - - - - -</td>
</tr>
<tr>
<td style="text-align: left;">scene</td>
<td style="text-align: left;">$[1,2,3,4,5]$</td>
</tr>
<tr>
<td style="text-align: left;">filter_color[purple]</td>
<td style="text-align: left;">$[5]$</td>
</tr>
<tr>
<td style="text-align: left;">unique</td>
<td style="text-align: left;">5</td>
</tr>
<tr>
<td style="text-align: left;">query_shape</td>
<td style="text-align: left;">sphere cube $<em>-</em>$</td>
</tr>
<tr>
<td style="text-align: left;">equal_shape</td>
<td style="text-align: left;">no</td>
</tr>
</tbody>
</table>
<p>Answer: no
Figure 7: Running example of NS-VQA. Dashed arrow indicates joining outputs from previous program modules, which are sent to the next module.</p>
<p><img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Scene</p>
<table>
<thead>
<tr>
<th style="text-align: left;">ID</th>
<th style="text-align: left;">Size</th>
<th style="text-align: left;">Shape</th>
<th style="text-align: left;">Material</th>
<th style="text-align: left;">Color</th>
<th style="text-align: left;">x</th>
<th style="text-align: left;">y</th>
<th style="text-align: left;">z</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">1</td>
<td style="text-align: left;">Large</td>
<td style="text-align: left;">Cube</td>
<td style="text-align: left;">Metal</td>
<td style="text-align: left;">Cube</td>
<td style="text-align: left;">-3.24</td>
<td style="text-align: left;">0.55</td>
<td style="text-align: left;">0.69</td>
</tr>
<tr>
<td style="text-align: left;">2</td>
<td style="text-align: left;">Small</td>
<td style="text-align: left;">Cube</td>
<td style="text-align: left;">Rubber</td>
<td style="text-align: left;">Brown</td>
<td style="text-align: left;">-0.52</td>
<td style="text-align: left;">3.88</td>
<td style="text-align: left;">0.34</td>
</tr>
<tr>
<td style="text-align: left;">3</td>
<td style="text-align: left;">Large</td>
<td style="text-align: left;">Sphere</td>
<td style="text-align: left;">Rubber</td>
<td style="text-align: left;">Gray</td>
<td style="text-align: left;">2.20</td>
<td style="text-align: left;">-0.25</td>
<td style="text-align: left;">0.70</td>
</tr>
<tr>
<td style="text-align: left;">4</td>
<td style="text-align: left;">Large</td>
<td style="text-align: left;">Cube</td>
<td style="text-align: left;">Metal</td>
<td style="text-align: left;">Gray</td>
<td style="text-align: left;">0.69</td>
<td style="text-align: left;">1.93</td>
<td style="text-align: left;">0.70</td>
</tr>
<tr>
<td style="text-align: left;">5</td>
<td style="text-align: left;">Large</td>
<td style="text-align: left;">Cylinder</td>
<td style="text-align: left;">Metal</td>
<td style="text-align: left;">Purple</td>
<td style="text-align: left;">0.23</td>
<td style="text-align: left;">-2.55</td>
<td style="text-align: left;">0.70</td>
</tr>
<tr>
<td style="text-align: left;">6</td>
<td style="text-align: left;">Small</td>
<td style="text-align: left;">Cylinder</td>
<td style="text-align: left;">Metal</td>
<td style="text-align: left;">Yellow</td>
<td style="text-align: left;">-2.21</td>
<td style="text-align: left;">-0.74</td>
<td style="text-align: left;">0.34</td>
</tr>
</tbody>
</table>
<p>Objects
<img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Question: How many large things are either purple cylinders or cyan metal objects?</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Program</th>
<th style="text-align: left;">Output</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">scene</td>
<td style="text-align: left;">$[1,2,3,4,5,6]$</td>
</tr>
<tr>
<td style="text-align: left;">filter_color[cyan]</td>
<td style="text-align: left;">[]</td>
</tr>
<tr>
<td style="text-align: left;">filter_material[metal]</td>
<td style="text-align: left;">[]</td>
</tr>
<tr>
<td style="text-align: left;">scene</td>
<td style="text-align: left;">$[1,2,3,4,5,6]$</td>
</tr>
<tr>
<td style="text-align: left;">filter_color[purple]</td>
<td style="text-align: left;">[5]</td>
</tr>
<tr>
<td style="text-align: left;">filter_shape[cylinder]</td>
<td style="text-align: left;">[5] []</td>
</tr>
<tr>
<td style="text-align: left;">union</td>
<td style="text-align: left;">[5]</td>
</tr>
<tr>
<td style="text-align: left;">filter_size[large]</td>
<td style="text-align: left;">[5]</td>
</tr>
<tr>
<td style="text-align: left;">count</td>
<td style="text-align: left;">1</td>
</tr>
</tbody>
</table>
<p>Answer: 1
Figure 8: Running example of NS-VQA.</p>
<p><img alt="img-13.jpeg" src="img-13.jpeg" /></p>
<p>Answer (Ours): cyan
Figure 9: Running example of NS-VQA. Fail case: a spurious program leads to the correct answer. As compared to the ground truth, the spurious program predicted by our model does not significantly deviate from the underlying logic, but adds extra degenerate structures.</p>
<h1>D Scene Parsing on Real Images</h1>
<p>Input Image
<img alt="img-14.jpeg" src="img-14.jpeg" /></p>
<p>Figure 10: Scene parsing results on real images. We handcraft real world CLEVR objects with paper boxes and rolls that are not well aligned with the synthetic scenes. We apply scene-parsing on the real objects without fine-tuning. Our model detects and extracts attributes from most objects correctly; in some cases, it mistakenly treats shadows as objects.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{\dagger}$ https://github.com/facebookresearch/clevr-dataset-gen&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
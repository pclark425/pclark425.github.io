<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4621 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4621</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4621</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-102.html">extraction-schema-102</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <p><strong>Paper ID:</strong> paper-270562218</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2406.12255v1.pdf" target="_blank">A Hopfieldian View-based Interpretation for Chain-of-Thought Reasoning</a></p>
                <p><strong>Paper Abstract:</strong> Chain-of-Thought (CoT) holds a significant place in augmenting the reasoning performance for large language models (LLMs). While some studies focus on improving CoT accuracy through methods like retrieval enhancement, yet a rigorous explanation for why CoT achieves such success remains unclear. In this paper, we analyze CoT methods under two different settings by asking the following questions: (1) For zero-shot CoT, why does prompting the model with"let's think step by step"significantly impact its outputs? (2) For few-shot CoT, why does providing examples before questioning the model could substantially improve its reasoning ability? To answer these questions, we conduct a top-down explainable analysis from the Hopfieldian view and propose a Read-and-Control approach for controlling the accuracy of CoT. Through extensive experiments on seven datasets for three different tasks, we demonstrate that our framework can decipher the inner workings of CoT, provide reasoning error localization, and control to come up with the correct reasoning path.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4621.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4621.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mistral-7B-instruct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mistral-instruct-7B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7B-parameter instruct-tuned autoregressive transformer (Mistral family) evaluated as an arithmetic reasoner in the paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral-instruct-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only transformer, instruct-tuned variant of Mistral at ~7B parameters (as used in the experiments); loaded in float16 and decoded with greedy search (max new tokens 512).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Multi-step math word problems (GSM8K), multi-structure arithmetic problems (SVAMP), algebraic/multiple-choice math (AQuA); tasks include addition/subtraction, multiplication, equation setup and simplification (multi-step arithmetic).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>Arithmetic competence arises from activation of latent 'concept' directions in representation space (Hopfieldian view); these concept directions are approximately linear and can be read via PCA on paired-stimulus representation differences (LAT), and arithmetic errors reflect mis-activated or drifting concept representations during multi-step reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>The paper computes a 'reading vector' v (leading PCA component of representation differences over stimulus pairs) and shows (i) dot products between v and token representations localize low-scoring (potentially erroneous) tokens in reasoning chains, and (ii) adding v to representations in late layers (control) increases task accuracy modestly — empirical support that linear concept directions correlate with arithmetic reasoning behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Improvements from the intervention are modest (single- to few-percent gains) and not uniformly large; the paper does not show a causal circuit-level mechanism (only representation-level interventions), and few-shot prompts can introduce stereotyped errors that the reading/control pipeline does not always fix, indicating incomplete explanatory power.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Zero-shot CoT: GSM8K 45.03% -> 47.08% (before/after control); SVAMP 57.33% -> 61.33%; AQuA 25.59% -> 30.71%. Few-shot CoT: GSM8K 46.78% -> 47.16%; SVAMP 61.33% -> 61.67%; AQuA 34.65% -> 35.04%. (Percent accuracy, before/after representation control.)</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>Applied LAT (Algorithm 1) to compute reading vector v from stimulus set sizes (128/256/512 depending on dataset), used last-token embeddings as representations, threshold δ=3.5 for salience maps; control intervention added v into the model's last ten layers' representations during generation (Algorithm 2), producing the accuracy gains listed above.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>Common failure modes reported: multi-step algebraic simplification errors (incorrect arithmetic steps), misinterpretation of question phrasing, and sensitivity to few-shot exemplars (stereotyping) that can reinforce incorrect reasoning; control sometimes fails when few-shot examples bias the model's concept prior.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Compared to LLaMA-2-7B-chat and LLaMA-3-8B-instruct on the same datasets: Mistral-7B-instruct is intermediate (worse than LLaMA-3-8B-instruct, better than LLaMA-2-7B-chat on many splits); absolute numbers reported above show LLaMA-3 substantially stronger.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Hopfieldian View-based Interpretation for Chain-of-Thought Reasoning', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4621.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4621.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-2-7B-chat</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA-2-7B-chat</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7B-parameter chat/fine-tuned decoder-only LLaMA-2 model variant evaluated on arithmetic word problems and used to illustrate representation reading and control.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-2-7B-chat</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only transformer model from Meta (LLaMA-2 family), chat/fine-tuned variant at ~7B parameters; used with greedy decoding and same representation-extraction setup (last-token embeddings) as other models.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Multi-step math word problems (GSM8K), multi-structure arithmetic (SVAMP), algebraic/multiple-choice (AQuA); single- and multi-step addition/subtraction/multiplication and equation solving.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>Same Hopfieldian representation-level hypothesis: prompts (e.g., 'Let's think step by step' or few-shot examples) act as stimuli activating latent concept directions represented as linear vectors; arithmetic reasoning corresponds to traversals in representational space along these concept directions, and misalignment causes errors.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Representation reading (LAT + PCA) finds directions that correlate with correct reasoning tokens (dot-product salience); control (adding v to late-layer representations) yields small but measurable accuracy improvements and can visually shift token-level salience to correct steps, providing empirical evidence that manipulating linear representation directions affects arithmetic outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>For some few-shot settings the model performs extremely poorly (e.g., GSM8K few-shot accuracy ~5%), and control yields only tiny improvements, suggesting representation-direction interventions do not fully capture the causes of arithmetic failure; examples show the model can simply copy or conform to a (possibly wrong) few-shot exemplar.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Zero-shot CoT: GSM8K 26.31% -> 27.37%; SVAMP 46.00% -> 46.67%; AQuA 27.95% -> 30.71%. Few-shot CoT: GSM8K 5.00% -> 5.16%; SVAMP 38.67% -> 39.00%; AQuA 22.44% -> 23.62%.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>Used last-token representations and LAT to derive reading vector v (reading set sizes per dataset noted in paper: e.g., GSM8K zero-shot 128; SVAMP 512, etc.); control applied to last ten layers; threshold δ=3.5 for token-level error localization; small accuracy increases reported after control.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>Very low few-shot GSM8K performance indicates large sensitivity to prompt/examples; failure modes include: exemplar-induced stereotyping (model conforms to example outcome even if wrong), arithmetic simplification mistakes, and misinterpretation of prompts; representation-control is insufficient to fully correct these failures in this model.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Underperforms LLaMA-3-8B-instruct by large margins on arithmetic tasks; shows worse robustness to few-shot exemplars compared to the larger/more capable LLaMA-3 model; modestly better or comparable to Mistral-7B on some symbolic tasks but weaker for arithmetic.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Hopfieldian View-based Interpretation for Chain-of-Thought Reasoning', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4621.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4621.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-3-8B-instruct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA-3-8B-instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An 8B-parameter instruction-tuned LLaMA-3 model used in experiments that attains the highest arithmetic accuracies among evaluated models and responds well to representation-level control.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-3-8B-instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only transformer from Meta's LLaMA-3 family, instruction-finetuned variant at ~8B parameters; evaluated with greedy decoding and the same representation reading/control pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Multi-step arithmetic word problems (GSM8K), multi-structure arithmetic (SVAMP), algebraic multiple-choice problems (AQuA); tasks require multi-step numerical reasoning and symbolic simplification.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>Arithmetic behavior emerges from activation of linear concept directions in representation space (Hopfieldian view). The model's representations for stimuli differ along concentrated directions (found by PCA on paired stimuli differences), which correspond to reasoning concepts; manipulating these directions (via addition into late-layer representations) modulates arithmetic reasoning outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>This model shows the largest absolute and relative gains under the paper's interventions: reading vectors localize incorrect reasoning tokens and control (adding v to last ten layers) yields consistent accuracy improvements (e.g., SVAMP 82.67% -> 84.00% zero-shot), supporting that representation-direction interventions can steer arithmetic reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Although improvements are consistent, they are modest; some failure cases (AQuA simplification errors, misinterpretations) persist after control. The paper does not provide mechanistic circuit-level evidence (no neuron/circuit ablations beyond linear-add interventions), so the representation-level hypothesis remains partial.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Zero-shot CoT: GSM8K 74.45% -> 75.89%; SVAMP 82.67% -> 84.00%; AQuA 42.13% -> 45.67%. Few-shot CoT: GSM8K 72.71% -> 74.83%; SVAMP 82.00% -> 83.67%; AQuA 52.76% -> 53.21%.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>LAT computed reading vectors from stimulus sets (various sizes 128/256/512 per dataset), PCA on pairwise difference vectors, dot-product scoring for token salience, control by injecting v into representations of last ten layers; threshold δ=3.5 used for salience visualization. Intervention leads to consistent small-absolute improvements across arithmetic datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>Remaining failure modes include arithmetic simplification mistakes when multiple algebraic steps are performed, sensitivity to phrasing, and occasional few-shot exemplar-induced bias; improvements do not eliminate these failures and are limited to representation-level linear interventions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Best-performing model in the paper on arithmetic tasks by a substantial margin (e.g., GSM8K ~74–76% vs. Mistral ~45–47% and LLaMA-2 ~26–27% zero-shot), and it benefits from representation control more visibly than smaller models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Hopfieldian View-based Interpretation for Chain-of-Thought Reasoning', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Large language models are zero-shot reasoners <em>(Rating: 2)</em></li>
                <li>Training verifiers to solve math word problems <em>(Rating: 2)</em></li>
                <li>A diverse corpus for evaluating and developing english math word problem solvers <em>(Rating: 2)</em></li>
                <li>How to think step-by-step: A mechanistic understanding of chain-of-thought reasoning <em>(Rating: 2)</em></li>
                <li>Show your work: Scratchpads for intermediate computation with language models <em>(Rating: 2)</em></li>
                <li>Mathprompter: Mathematical reasoning using large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4621",
    "paper_id": "paper-270562218",
    "extraction_schema_id": "extraction-schema-102",
    "extracted_data": [
        {
            "name_short": "Mistral-7B-instruct",
            "name_full": "Mistral-instruct-7B",
            "brief_description": "A 7B-parameter instruct-tuned autoregressive transformer (Mistral family) evaluated as an arithmetic reasoner in the paper's experiments.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Mistral-instruct-7B",
            "model_description": "Decoder-only transformer, instruct-tuned variant of Mistral at ~7B parameters (as used in the experiments); loaded in float16 and decoded with greedy search (max new tokens 512).",
            "arithmetic_task_type": "Multi-step math word problems (GSM8K), multi-structure arithmetic problems (SVAMP), algebraic/multiple-choice math (AQuA); tasks include addition/subtraction, multiplication, equation setup and simplification (multi-step arithmetic).",
            "mechanism_hypothesis": "Arithmetic competence arises from activation of latent 'concept' directions in representation space (Hopfieldian view); these concept directions are approximately linear and can be read via PCA on paired-stimulus representation differences (LAT), and arithmetic errors reflect mis-activated or drifting concept representations during multi-step reasoning.",
            "evidence_for_mechanism": "The paper computes a 'reading vector' v (leading PCA component of representation differences over stimulus pairs) and shows (i) dot products between v and token representations localize low-scoring (potentially erroneous) tokens in reasoning chains, and (ii) adding v to representations in late layers (control) increases task accuracy modestly — empirical support that linear concept directions correlate with arithmetic reasoning behavior.",
            "evidence_against_mechanism": "Improvements from the intervention are modest (single- to few-percent gains) and not uniformly large; the paper does not show a causal circuit-level mechanism (only representation-level interventions), and few-shot prompts can introduce stereotyped errors that the reading/control pipeline does not always fix, indicating incomplete explanatory power.",
            "performance_metrics": "Zero-shot CoT: GSM8K 45.03% -&gt; 47.08% (before/after control); SVAMP 57.33% -&gt; 61.33%; AQuA 25.59% -&gt; 30.71%. Few-shot CoT: GSM8K 46.78% -&gt; 47.16%; SVAMP 61.33% -&gt; 61.67%; AQuA 34.65% -&gt; 35.04%. (Percent accuracy, before/after representation control.)",
            "probing_or_intervention_results": "Applied LAT (Algorithm 1) to compute reading vector v from stimulus set sizes (128/256/512 depending on dataset), used last-token embeddings as representations, threshold δ=3.5 for salience maps; control intervention added v into the model's last ten layers' representations during generation (Algorithm 2), producing the accuracy gains listed above.",
            "limitations_and_failure_modes": "Common failure modes reported: multi-step algebraic simplification errors (incorrect arithmetic steps), misinterpretation of question phrasing, and sensitivity to few-shot exemplars (stereotyping) that can reinforce incorrect reasoning; control sometimes fails when few-shot examples bias the model's concept prior.",
            "comparison_to_other_models": "Compared to LLaMA-2-7B-chat and LLaMA-3-8B-instruct on the same datasets: Mistral-7B-instruct is intermediate (worse than LLaMA-3-8B-instruct, better than LLaMA-2-7B-chat on many splits); absolute numbers reported above show LLaMA-3 substantially stronger.",
            "uuid": "e4621.0",
            "source_info": {
                "paper_title": "A Hopfieldian View-based Interpretation for Chain-of-Thought Reasoning",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "LLaMA-2-7B-chat",
            "name_full": "LLaMA-2-7B-chat",
            "brief_description": "A 7B-parameter chat/fine-tuned decoder-only LLaMA-2 model variant evaluated on arithmetic word problems and used to illustrate representation reading and control.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA-2-7B-chat",
            "model_description": "Decoder-only transformer model from Meta (LLaMA-2 family), chat/fine-tuned variant at ~7B parameters; used with greedy decoding and same representation-extraction setup (last-token embeddings) as other models.",
            "arithmetic_task_type": "Multi-step math word problems (GSM8K), multi-structure arithmetic (SVAMP), algebraic/multiple-choice (AQuA); single- and multi-step addition/subtraction/multiplication and equation solving.",
            "mechanism_hypothesis": "Same Hopfieldian representation-level hypothesis: prompts (e.g., 'Let's think step by step' or few-shot examples) act as stimuli activating latent concept directions represented as linear vectors; arithmetic reasoning corresponds to traversals in representational space along these concept directions, and misalignment causes errors.",
            "evidence_for_mechanism": "Representation reading (LAT + PCA) finds directions that correlate with correct reasoning tokens (dot-product salience); control (adding v to late-layer representations) yields small but measurable accuracy improvements and can visually shift token-level salience to correct steps, providing empirical evidence that manipulating linear representation directions affects arithmetic outputs.",
            "evidence_against_mechanism": "For some few-shot settings the model performs extremely poorly (e.g., GSM8K few-shot accuracy ~5%), and control yields only tiny improvements, suggesting representation-direction interventions do not fully capture the causes of arithmetic failure; examples show the model can simply copy or conform to a (possibly wrong) few-shot exemplar.",
            "performance_metrics": "Zero-shot CoT: GSM8K 26.31% -&gt; 27.37%; SVAMP 46.00% -&gt; 46.67%; AQuA 27.95% -&gt; 30.71%. Few-shot CoT: GSM8K 5.00% -&gt; 5.16%; SVAMP 38.67% -&gt; 39.00%; AQuA 22.44% -&gt; 23.62%.",
            "probing_or_intervention_results": "Used last-token representations and LAT to derive reading vector v (reading set sizes per dataset noted in paper: e.g., GSM8K zero-shot 128; SVAMP 512, etc.); control applied to last ten layers; threshold δ=3.5 for token-level error localization; small accuracy increases reported after control.",
            "limitations_and_failure_modes": "Very low few-shot GSM8K performance indicates large sensitivity to prompt/examples; failure modes include: exemplar-induced stereotyping (model conforms to example outcome even if wrong), arithmetic simplification mistakes, and misinterpretation of prompts; representation-control is insufficient to fully correct these failures in this model.",
            "comparison_to_other_models": "Underperforms LLaMA-3-8B-instruct by large margins on arithmetic tasks; shows worse robustness to few-shot exemplars compared to the larger/more capable LLaMA-3 model; modestly better or comparable to Mistral-7B on some symbolic tasks but weaker for arithmetic.",
            "uuid": "e4621.1",
            "source_info": {
                "paper_title": "A Hopfieldian View-based Interpretation for Chain-of-Thought Reasoning",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "LLaMA-3-8B-instruct",
            "name_full": "LLaMA-3-8B-instruct",
            "brief_description": "An 8B-parameter instruction-tuned LLaMA-3 model used in experiments that attains the highest arithmetic accuracies among evaluated models and responds well to representation-level control.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA-3-8B-instruct",
            "model_description": "Decoder-only transformer from Meta's LLaMA-3 family, instruction-finetuned variant at ~8B parameters; evaluated with greedy decoding and the same representation reading/control pipeline.",
            "arithmetic_task_type": "Multi-step arithmetic word problems (GSM8K), multi-structure arithmetic (SVAMP), algebraic multiple-choice problems (AQuA); tasks require multi-step numerical reasoning and symbolic simplification.",
            "mechanism_hypothesis": "Arithmetic behavior emerges from activation of linear concept directions in representation space (Hopfieldian view). The model's representations for stimuli differ along concentrated directions (found by PCA on paired stimuli differences), which correspond to reasoning concepts; manipulating these directions (via addition into late-layer representations) modulates arithmetic reasoning outputs.",
            "evidence_for_mechanism": "This model shows the largest absolute and relative gains under the paper's interventions: reading vectors localize incorrect reasoning tokens and control (adding v to last ten layers) yields consistent accuracy improvements (e.g., SVAMP 82.67% -&gt; 84.00% zero-shot), supporting that representation-direction interventions can steer arithmetic reasoning.",
            "evidence_against_mechanism": "Although improvements are consistent, they are modest; some failure cases (AQuA simplification errors, misinterpretations) persist after control. The paper does not provide mechanistic circuit-level evidence (no neuron/circuit ablations beyond linear-add interventions), so the representation-level hypothesis remains partial.",
            "performance_metrics": "Zero-shot CoT: GSM8K 74.45% -&gt; 75.89%; SVAMP 82.67% -&gt; 84.00%; AQuA 42.13% -&gt; 45.67%. Few-shot CoT: GSM8K 72.71% -&gt; 74.83%; SVAMP 82.00% -&gt; 83.67%; AQuA 52.76% -&gt; 53.21%.",
            "probing_or_intervention_results": "LAT computed reading vectors from stimulus sets (various sizes 128/256/512 per dataset), PCA on pairwise difference vectors, dot-product scoring for token salience, control by injecting v into representations of last ten layers; threshold δ=3.5 used for salience visualization. Intervention leads to consistent small-absolute improvements across arithmetic datasets.",
            "limitations_and_failure_modes": "Remaining failure modes include arithmetic simplification mistakes when multiple algebraic steps are performed, sensitivity to phrasing, and occasional few-shot exemplar-induced bias; improvements do not eliminate these failures and are limited to representation-level linear interventions.",
            "comparison_to_other_models": "Best-performing model in the paper on arithmetic tasks by a substantial margin (e.g., GSM8K ~74–76% vs. Mistral ~45–47% and LLaMA-2 ~26–27% zero-shot), and it benefits from representation control more visibly than smaller models.",
            "uuid": "e4621.2",
            "source_info": {
                "paper_title": "A Hopfieldian View-based Interpretation for Chain-of-Thought Reasoning",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Large language models are zero-shot reasoners",
            "rating": 2,
            "sanitized_title": "large_language_models_are_zeroshot_reasoners"
        },
        {
            "paper_title": "Training verifiers to solve math word problems",
            "rating": 2,
            "sanitized_title": "training_verifiers_to_solve_math_word_problems"
        },
        {
            "paper_title": "A diverse corpus for evaluating and developing english math word problem solvers",
            "rating": 2,
            "sanitized_title": "a_diverse_corpus_for_evaluating_and_developing_english_math_word_problem_solvers"
        },
        {
            "paper_title": "How to think step-by-step: A mechanistic understanding of chain-of-thought reasoning",
            "rating": 2,
            "sanitized_title": "how_to_think_stepbystep_a_mechanistic_understanding_of_chainofthought_reasoning"
        },
        {
            "paper_title": "Show your work: Scratchpads for intermediate computation with language models",
            "rating": 2,
            "sanitized_title": "show_your_work_scratchpads_for_intermediate_computation_with_language_models"
        },
        {
            "paper_title": "Mathprompter: Mathematical reasoning using large language models",
            "rating": 1,
            "sanitized_title": "mathprompter_mathematical_reasoning_using_large_language_models"
        }
    ],
    "cost": 0.013906249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>A Hopfieldian View-based Interpretation for Chain-of-Thought Reasoning
18 Jun 2024</p>
<p>Lijie Hu 
Provable Responsible AI and Data Analytics (PRADA)</p>
<p>KAUST</p>
<p>Liang Liu 
Provable Responsible AI and Data Analytics (PRADA)</p>
<p>Soochow University</p>
<p>Shu Yang 
Provable Responsible AI and Data Analytics (PRADA)</p>
<p>KAUST</p>
<p>Xin Chen 
Provable Responsible AI and Data Analytics (PRADA)</p>
<p>University of Southampton</p>
<p>Hongru Xiao 
Provable Responsible AI and Data Analytics (PRADA)</p>
<p>Tongji University</p>
<p>Mengdi Li 
Provable Responsible AI and Data Analytics (PRADA)</p>
<p>Universität Hamburg</p>
<p>Pan Zhou 
Provable Responsible AI and Data Analytics (PRADA)</p>
<p>Huazhong University of Science and Technology</p>
<p>Muhammad Asif Ali 
Provable Responsible AI and Data Analytics (PRADA)</p>
<p>KAUST</p>
<p>Di Wang 
Provable Responsible AI and Data Analytics (PRADA)</p>
<p>KAUST</p>
<p>A Hopfieldian View-based Interpretation for Chain-of-Thought Reasoning
18 Jun 2024687879C01DADF8E224ACE5DD1BCBA74BarXiv:2406.12255v1[cs.CL]
Chain-of-Thought (CoT) holds a significant place in augmenting the reasoning performance for large language models (LLMs).While some studies focus on improving CoT accuracy through methods like retrieval enhancement, yet a rigorous explanation for why CoT achieves such success remains unclear.In this paper, we analyze CoT methods under two different settings by asking the following questions: (1) For zero-shot CoT, why does prompting the model with "let's think step by step" significantly impact its outputs?(2) For few-shot CoT, why does providing examples before questioning the model could substantially improve its reasoning ability?To answer these questions, we conduct a top-down explainable analysis from the Hopfieldian view and propose a Read-and-Control approach for controlling the accuracy of CoT.Through extensive experiments on seven datasets for three different tasks, we demonstrate that our framework can decipher the inner workings of CoT, provide reasoning error localization, and control to come up with the correct reasoning path.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs) have demonstrated exceptional capabilities in adhering to the natural language instructions (Ouyang et al., 2022;Mishra et al., 2022;Wei et al., 2022a;Jin et al., 2024;Yang et al., 2024b,a), and various downstream tasks (Kocon et al., 2023;Zhu et al., 2023;Zhang et al., 2023b;Hu et al., 2023a;Yang et al., 2024c).An increasing amount of efforts are dedicated to exploring how these models can be utilized to perform more complex tasks, such as commonsense and mathematical reasoning (Rae et al., 2021;Lu et al., 2023;Imani et al., 2023).For this, Chain-of-Thought (CoT) techniques (Wei et al., 2022b;Kojima et al., 2022;Wang et al., 2023d;Zhou et al., 2023;Qiao et al., 2023) have emerged as a simple yet effective method for enhancing the performance of LLMs in tasks requiring logical thinking.</p>
<p>Recently, there have been multiple endeavors focused on improving the accuracy of CoT, e.g., efficient prompt design (Zhang et al., 2023a;Shi et al., 2023;Wang et al., 2023c), process optimization (Wang et al., 2023d;Madaan et al., 2023;Shinn et al., 2023), extra engine usage (Schick et al., 2023;Lyu et al., 2023;Gao et al., 2023), knowledge enhancement (Liu et al., 2022;Yang et al., 2022;He et al., 2023;Zhao et al., 2023), etc.Likewise, there have been numerous research attempts to identify the key factors or elements that help CoT to augment the reasoning capabilities of LLMs (Kojima et al., 2022;Wang et al., 2023a;Tang et al., 2023;Merrill and Sabharwal, 2023).However, the majority of these studies primarily rely on designing datasets encompassing different features and/or using additional tools and knowledge to enhance the reasoning abilities of LLMs.Although some works have attempted to research the faithfulness of CoT (Lyu et al., 2023;Lanham et al., 2023), these studies have not fundamentally explained the key factors underlying the success of CoT.</p>
<p>To bridge this gap, in this work, we propose an explainable framework to identify the pivotal elements influencing CoT performance for LLMs.Specifically, we build our framework on the top of Hopfieldian view (Barack and Krakauer, 2021).As shown in Figure 1a, the Hopfieldian view explains cognition as the result of transformations between, or movements within, representational spaces implemented by neural populations in response to stimuli.In this view, we no longer consider the interaction process between neurons, rather we only need to understand the impact of the representational changes on the final actions, thus offering a natural setting to study LLMs.</p>
<p>As shown in Figure 2, our framework encompasses three core components: (i) Concept Modeling, (ii) Concept Simulation, and (iii) Analysis based on Hopfieldian View.The Concept Modeling emphasizes that during pre-training LLMs learn latent concepts (both concrete and abstract) that pertain to specific names or domains.The Concept Simulation highlights that we can use zero-shot or few-shot prompts as stimuli for the LLMs in order to induce and/or activate specific concepts.These  activated concepts collectively determine the final action of the model.Finally, we perform analysis based on Hopfieldian view to analyze the representations responsible for the activation of concepts when the model is stimulated.Specifically, it uses a Read operation to read concept-specific representations for reasoning and error localization; followed by a Control operation to guide and/or rectify the reasoning direction of LLM.We apply our proposed approach to answer the following key questions: (i) For zero-shot CoT (Kojima et al., 2022), why prompting the model with stimulus like "let's think step by step" before inference can make LLMs' outputs more accurate?(ii) For few-shot CoT (Wei et al., 2022b), why providing example demonstrations before questioning significantly enhances the LLMs' reasoning ability?</p>
<p>Experimental evaluation on three different tasks: i.e., (i) arithmetic reasoning, (ii) commonsense reasoning, and (iii) symbolic reasoning, reveals that our framework can provide intuitive and interpretable analysis for CoT reasoning helpful in tracing and control the errors LLMs made during the CoT process.</p>
<p>We summarize the key contributions of our work as follows:</p>
<ol>
<li>
<p>We propose a novel framework employing Hopfieldian view to identify the key factors responsible for the success of CoT under zero-shot and few-shot settings.</p>
</li>
<li>
<p>To the best of our knowledge, this work is an initial attempt at interpretability in CoT that employs concept-level representation read operations to localize errors in CoT followed by control operation to rectify LLMs' reasoning paths.</p>
</li>
<li>
<p>We conduct a comprehensive experimental evaluation for our framework by employing seven different data sets across three different tasks.</p>
</li>
</ol>
<p>Results showcase that our framework can offer faithful explanations with error localization and control for CoT.</p>
<p>2 Related Work</p>
<p>Chain of Thought (CoT)</p>
<p>The CoT is a prompting technique that engages LLMs in step-by-step reasoning rather than directly providing the answers (Nye et al., 2021).Studies show that intermediate steps or learning from demonstration can significantly improve the reasoning performance of LLMs (Wei et al., 2022b;Kojima et al., 2022).Owing to the success of CoT, numerous studies focus on using CoT to solve complex problems, such as commonsense, arithmetic, symbolic reasoning (Wang et al., 2023d;Zhou et al., 2023;Wang and Zhou, 2024), and logic tasks (Creswell and Shanahan, 2022;Pan et al., 2023;Weng et al., 2023).Recently, numerous endeavors have been made to identify the key factors through which CoT enhances the reasoning capabilities of LLMs (Wang et al., 2023a;Dutta et al., 2024).For instance, Kim et al. (2023) corrected the erroneous parts of the chain by using a query-based approach.Zhao et al. (2023) proposed a knowledgeenhanced method to augment the factual correctness for multi-pole open-domain QA tasks.Lyu et al. (2023) introduce faithful CoT, i.e., a faithful by-construction framework to first translate natural language query to symbolic reasoning chain, later solve the problem via with CoT.</p>
<p>Likewise, numerous research attempts have focused on the sequence and quantity of demonstrations within the context, investigating their contributions to the final reasoning performance.For this, Min et al. (2022) discovered that even random labels or ineffective reasoning steps can still improve the model's reasoning performance.Lanham et al. (2023) demonstrated the impact of intervening in the CoT process by adding mistakes or paraphrases.Pfau et al. (2024) showed that using meaningless filler tokens in place of a chain-of-thought can help improve the performance.</p>
<p>These studies have led to concerns such as whether LLMs truly acquire the ability to learn inductive, deductive, and abductive reasoning skills from demonstrations and instructions (Stechly et al., 2024;Jin et al., 2024).In this paper, we will use a top-down interpretability method to analyze the operating mechanism of CoT reasoning.</p>
<p>Interpretability of LLMs</p>
<p>Interpretability plays a key role in a deeper understanding of LLMs to identify potential risks and better meet human requirements (Zou et al., 2023).Current widely used strategies for interpretability include: (i) salience maps, relying on highlighting the regions in the input that are attended by the model (Simonyan et al., 2014;Smilkov et al., 2017;Clark et al., 2019;Hu et al., 2023c,b;Lai et al., 2023); (ii) feature visualization, creating representative inputs indicative of particular neurons' activations (Szegedy et al., 2014;Nguyen et al., 2016Nguyen et al., , 2019;;Fong and Vedaldi, 2018), and (iii) mechanistic interpretability, employing reverse-engineering tools to explain the network in terms of circuits and node-to-node connections (Olah et al., 2020;Olsson et al., 2022;Wang et al., 2023b).However, these methods require a lot of human intervention and are limited in terms of interpretability, especially for the neural network models (Jain and Wallace, 2019;Fong and Vedaldi, 2018;Hu et al., 2024).Thus, these methods cannot be directly used to interpret CoTs.Meanwhile, current approaches are limited to the analysis at the representation level and do not consider the relationship between representations and the features/concepts learned during pre-training (Bricken et al., 2023;Templeton et al., 2024).</p>
<p>Other dominant works in this regard investigate the location and representation of concepts in the network (Kim et al., 2018;Li et al., 2024), linear classifier probing to probe properties in the input (Belinkov, 2022), locating and editing facts (Meng et al., 2022;Zhong et al., 2023;Cheng et al., 2024a,b), concept erasure (Shao et al., 2023;Gandikota et al., 2023), corrective analysis (Burns et al., 2023), etc.These observations are aligned with RepE (Zou et al., 2023) that emphasized that representations within the LLM are almost secretly linear (Park et al., 2023).However, none of them consider the inner workings of the Chainof-Thought Reasoning.</p>
<p>Preliminaries</p>
<p>In this section, we briefly introduce the notation followed by a quick background on the core concepts required for this work.More background can be found in Appendix A. Notations.We use M θ to denote an LLM parameterized by θ, C to represent set of concepts, and c i an individual concept, T to represent the prompt template, S to denote a set of stimulus s in the prompt (i.e., example input-output demonstra-tions in a few-shot scenario and "think step by step" in a zero-shot scenario), and x denotes the query for which the LLM is asked to provide a response.Thus, we can use T ([S, x]) to denote the prompt p, which includes both a set of stimuli S and query x, providing the full context to LLM in order to generate a response.</p>
<p>Hopfieldian View.Hopfieldian view aims to scale/extend the traditional mechanistic interpretability for AI-cognition (i.e., neurons and circuits) to a much broader perspective in attempts to explain complex phenomena.It considers representation as a basic unit and explains cognition as the result of transformations between or movement within representational spaces implemented via neural populations (Barack and Krakauer, 2021).</p>
<p>We provide an intuitive explanation in this regard in Figure 1, which emphasizes that Hopfieldian view allows us with the provision to analyze and control the network as a function of external stimulus the akin to the functionality of cognitive brain.For further details refer to recent work by Zou et al. (2023) that employed Hopfieldian view to propose a top-down approach to interpretability.</p>
<p>In-context Learning(ICL) and Bayesian Inference.Bayesian inference is a method for statistical inference that is used to compute the posterior probability based on the likelihood and prior.It has gained attraction in the analysis and interpretation of LLMs in order to systematically update the knowledge and beliefs for a network with the acquisition of new information.Further details exhibiting the impact of Bayesian analysis peculiarly focued on LLMs is available in (Falck et al., 2024).</p>
<p>The in-context learning ability of models may also be represented via Bayesian inference as follows:
P(r|p) = c P(r|c, p)P(c|p) d(c).
(1)</p>
<p>This equation illustrates the conditional probability of generating a response r given a prompt p, as a function of the concepts c that the model has learned during pre-training.By integrating over these concepts, the model dynamically adjusts its responses based on the likelihood of various concepts being relevant to the given prompt.This probabilistic approach not only helps in understanding the model's decision-making process but also in enhancing its performance on complex reasoning tasks.Our work extends this formula by incorporating the Hopfieldian view, describing the role of prompts in model reasoning as stimuli that activate specific populations of neurons.</p>
<p>CoT Explanation from Hopfieldian View</p>
<p>In this section, we will dive into details about our motivation from the Hopfieldian view and a general introduction of our framework.</p>
<p>Motivation.Existing literature has confirmed that CoT can improve the reasoning ability of LLMs (Kojima et al., 2022;Wei et al., 2022b).However, few studies have conducted a comprehensive analysis of the interpretability of CoT.This is mainly because reasoning is a complex task, and the model's final performance is influenced by numerous factors.Analyzing internal activations in isolation or from the perspective of individual neurons of LLMs is challenging.Inspired by work related to representation engineering (Zou et al., 2023), we consider LLM from a higher-level cognition perspective (Hopfieldian view), i.e., specific external stimuli activate representations within the model, thereby causing the model to exhibit different behaviors that represent the population activation behavior of neurons within the model.Meanwhile, many studies have shown that the representations of LLM are almost secretly linear inside, and can be controlled via linear probe methods (Park et al., 2023;Wu et al., 2024).These phenomena indicate that we can analyze the interpretability of CoT from the perspective of representation and control the reasoning process of LLM.</p>
<p>Framework.As we are targeting at exploiting the Hopfieldian view to provide an explanation for LLMs' CoT reasoning, we have to build a bridge from the components in the cognitive brain to CoTs.</p>
<p>We begin from introducing the definition of stimulus in CoT, which is the prompt text (i.e., example input-output demonstrations in a few-shot scenario and "think step by step" in a zero-shot scenario).</p>
<p>Then use these stimuli to activate specific "neural populations" for CoT.</p>
<p>For neural populations, ideally, we can leverage humanly comprehensible concepts within LLMs' representations.To model these concepts, our framework has two phases: (i) Concept Modeling, and (ii)Concept Simulation.In the Concept Modeling phase (Section 5.1), LLMs will learn latent (both concrete and abstract) concepts from pertaining documents, and these concepts might pertain to specific names or domains, such as "medical" or "climate change".They could also represent more abstract notions derived from document-level modeling, such as "positive language", "careful reasoning", or "affirmative expression".Likewise, in the Concept Simulation stage (Section 5.2), zero-shot or few-shot prompts serve as stimuli for the LLM, inducing the activation of specific concepts.These activated concepts collectively determine the final action of this model.</p>
<p>However, directly extracting activated concepts would consume a tremendous amount of computational resources, and individual concepts alone cannot determine the model's overall action.Therefore, our framework uses representations to denote the activation of concepts when the model is stimulated.For this, our framework employs Representations Reading (Section 5.3) and Representations Controlling (Section 5.4) as the core modeling components.</p>
<p>Next, we will give details about individual model components.Later, we explain how this framework can be used to improve the CoT reasoning abilities of LLMs.See Figure 2 for an illustration of our framework.</p>
<p>Details of the Framework</p>
<p>Concept Modeling</p>
<p>Previous research (Xie et al., 2022) reveals that during pre-training LLMs can learn a set of latent document-level concepts c ∈ C that in turn help large language models to generate coherent next tokens.This motivates our design of stimuli.We provide some examples of these concepts in Figure 2. The activation and intensity of specific concepts determine the model's final response.However, searching and controlling individual concepts is a prohibitively expensive process.In order to circum-vent that, we instead use representations as a basic unit indicative of latent concepts learned by the model.We cover the definitions and relationships between these representations and concepts in the next subsections.</p>
<p>Concept Stimulation</p>
<p>From the Hopfield view, we need specific stimulus to trigger the model's specific action.Since CoT reasoning is primarily activated through prompts, we posit that CoT prompts p containing a set of stimulus S that can activate the model's reasoning capabilities.So, we use the following formula based on Bayesian inference to illustrate the entire process from the LLM receiving an input to generating a specified output, as shown in Figure 2:
P(r|p) = c P(r|c, T ([S, x]))P(c|T ([S, x])) d(c),
(2) where, T ([S, x]) is the prompt template based on specific Stimulus S and query x.P(r|c, T ([S, x])) is the likelihood of response r given c, and P(c|T ([S, x])) is the prior probability of the c for a given prompt template T ([S, x]).</p>
<p>This equation governs the conditional probability of a response r given prompt p guided by concept i.e., (c ∈ C) learned by the model.</p>
<p>Representations Reading</p>
<p>The end goal of reading representations is to look for and/or locate the representations for key concepts within a network.We sub-divided its process into: (i) concepts reading, and (ii) reasoning error localization.Details about each component are explained in the following sub-sections.</p>
<p>Concepts Reading</p>
<p>This module aims to locate and highlight the core concepts within the network.The concepts output/generated by this module act as a foundation for the subsequent steps, i.e., representation evaluation followed by representation control.However, in CoT-style prompting, the definition of concepts is abstract, and how to define high-level concepts is crucial.For this, we utilize a Linear Artificial Tomography (LAT), similar to (Zou et al., 2023)
A c ← [] // neural activity list 2 for s i in S do 3 A c ← A c + [Rep(s i , M θ )[−1]] 4 end 5 i ← 0 6 D ← [] // difference list 7 while i &lt; len(S) − 1 do 8 D ← D + [A c [i + 1] − A c [i]] 9 i ← i + 2 10 end 11 v ← PCA(D)[0] 12 return v
corresponding to a specific stimulus are processed and transferred within large models, as illustrated in Figure 1, from the perspective of cognitive neuroscience.</p>
<p>We implement the impact of stimulus in Hopfieldian view as follows: a set of stimuli (i.e., the prompt text
) S = {s − 0 , s + 0 , s − 1 , s + 1 , • • • , s + n }
, when an LLM receives a positive stimulus s + , it generates a specific output/response r, across a query x, and falls into the corresponding action space A + .Conversely, when it receives a negative stimulus s − or no stimulus at all, the model's output falls into a different action space A − or A.</p>
<p>An example of a pair of specific prompt template for T ([S, x])), and example demonstrations are shown in Appendix C.</p>
<p>(ii) Neural Activity Monitoring.After receiving the task-specific prompt stimuli set, the next step is to capture corresponding representations from the LLM.Given that LLMs rely on transformer-based architecture to store distinct rep- Two trains leave San Rafael at the same time.They begin traveling westward, both traveling for 80 miles.The next day, they travel northwards, covering 150 miles.What's the distance covered by each train in the two days?
Algorithm 2: Control Representations Input: Query x, reading vector v, control layers L, model M θ Output: Model's logits after control 1 // get the representations of x 2 H ← Rep(x, M θ ) 3 for layer in M θ do 4 if layer in L then 5 H[layer] ← H[layer] + v[layer]
Figure 3: A real case of reasoning error localization by using LLaMA-2-7B-chat in a zero-shot scenario for the GSM8K dataset using our framework.The green bar indicates that the reasoning snippet is correct, and the red bar means that the reasoning snippet may be wrong.</p>
<p>resentations intended for different purposes.In order to capture the representations for the specific task, we first need to identify suitable design choices for the extraction process.For decoder-only/autoregressive architecture models, one natural choice is to choose the position of the last token as the representation of each concept.</p>
<p>Formally, given a set of stimuli S, a language model M θ , and a function Rep, we compute the neural activity A c as follows:
A c = {Rep(s i , M θ )[−1] | s i ∈ S},(3)
where Rep(.,.) is a function that takes stimulus s i ∈ S and LLM M θ as input and returns the representations from all token positions.</p>
<p>(iii) Concept Identification.Concept identification aims to identify a direction that accurately predicts the underlying concept solely based on the neural activity recorded via (3).</p>
<p>For this, we employ Principal Component Analysis (PCA) to the set of difference vectors that can yield superior results when the stimuli in each pair share similarities except for the target concept.Specifically, we compute the principal components of the representation {A i+1 c − A i c } (as depicted in line 8 of Algorithm 1), and define the leading principal vector derived from this as "reading vector", denoted by v. Intuitively, we can compute the dot product between the reading vector v and each representation Rep(s i , M θ ) as an indicator of the relevance between the stimulus representation and latent concepts.</p>
<p>Reasoning Error Localization</p>
<p>In this section, we consider the visualization of reasoning error localization, given a query x, we format it using the stimulus prompting template T and represent it as p.Then, we use p to generate a base reasoning path R with M θ .</p>
<p>After the representations reading (section 5.3.1),we get the reading vector v.Then, we com-pute the dot product between the representations Rep(p, M θ ) and our reading vector v yielding a set of scores, which serve as the basis for reasoning error localization.We use the score of each token compared against a threshold δ to locate errors in the reasoning path.Specifically, we use the following criterion to access and/or evaluate the quality of the rationales:
scores norm = normalize(Rep(R, M θ ) T v − δ). (4)
Note, in the above equation, using multiple different values for δ provides us with the provision to effectively adjust and use a wide range of scores.Then, we use a clip function to set the value greater than 0 in the score array, i.e., (scores norm ) to 0. Our salience map is based on this score, therefore, when the score is less than 0, it will appear in red, indicating that there is a potential wrong reasoning location.</p>
<p>An example of visualization is shown in Figure 3, LLaMA-2-7B-chat did not really understand the meaning of the given question and misinterpreted "the distance covered by each train" as "the total distance covered by both trains", thus giving an incorrect reasoning path.</p>
<p>Representations Controlling</p>
<p>Here we aim to correct the direction of the concept which may lead to CoT errors, as shown in the top right part of Figure 2.</p>
<p>Given a query x, we adopt a simple process to control the direction of CoT.Specifically, as depicted in Algorithm 2, during the generation phase, we first acquire the reading vector v from the concept identification phase.Subsequently, we get the representations H for the query x, and then we incorporate v into the original representation weights of the corresponding control layers L (as shown in line 5 of Algorithm 2).Finally, we transform the controlled representations H into the logits of model for generation.A: The coin was flipped by Ka and Sherrie.So the coin was flipped 2 times, which is an even number.The coin started heads up, so after an even number of flips, it will still be heads up.So the answer is yes.Q: A coin is heads up.Jamey flips the coin.Teressa flips the coin.Is the coin still heads up?A: The coin was flipped by Jamey and Teressa.So the coin was flipped 2 times, which is an even number.The coin started heads up, so after an even number of flips, it will still be heads up.So the answer is yes.... Q: A coin is heads up.wanjiru flips the coin.nunnery does not flip the coin.canya does not flip the coin.kydin flips the coin.Is the coin still heads up?</p>
<p>The coin is flipped by lanicka, copelyn (who doesn't flip it), cordasia, and jiwon.</p>
<p>That's a total of 4 flips, an even number.Since it started heads up, it will still be heads up.So, the answer is: yes, the coin is still heads up.</p>
<p>The coin was flipped by lanicka, copelyn (did not flip), cordasia, and jiwon.</p>
<p>So the coin was flipped 3 times, which is an odd number.The coin started heads up, so after an odd number of flips, it will be tails up.So the answer is no.</p>
<ul>
<li>Zero-shot Control</li>
</ul>
<p>The coin was flipped by lanicka, copelyn, cordasia, and jiwon.So the coin was flipped 4 times, which is an even number.The coin started heads up, so after an even number of flips, it will still be heads up.So the answer is yes.</p>
<ul>
<li>Few-shots Control 6 Experiments</li>
</ul>
<p>Experimental Setup</p>
<p>Dataset.We consider 7 datasets for 3 different tasks: Arithmetic Reasoning, Commonsense Reasoning, and Symbolic Reasoning.Specifically, for arithmetic reasoning, we select the GSM8K (Cobbe et al., 2021), SVAMP (Patel et al., 2021) and AQuA (Miao et al., 2020) datasets; for commonsense reasoning, we choose the benchmarks of strat-egyQA (Geva et al., 2021) and CSQA (Talmor et al., 2019); and for symbolic reasoning, we pick the Coin Flip (Wei et al., 2022b) and Random Letter datasets (constructed from the last letter dataset (Wei et al., 2022b)).Further details and statistics of the data sets are given in Appendix B.1.Baselines.We use three LLMs for evaluation, i.e., (i) Mistral-instruct-7B (Jiang et al., 2023), (ii) LLamA-2-7B-chat (Touvron et al., 2023), (iii) LLamA-3-8B-instruct (Meta, 2024).Evaluation Metrics.We use accuracy metrics for all dataset benchmarks.The answer extraction process is based on the methodology outlined by Kojima et al. (2022).Detailed procedures and results are provided in the Appendix C.3.Implementation Details.In our study, we pick the last ten layers to control the reasoning direction of the LLM and select different numbers of stimulus samples according to different tasks.Specifically, We conducted experiments using three distinct reading set sizes: 128, 256, and 512 samples, details shown in Appendix B.2.For experimentation, we use a value of threshold δ = 3.5.We use float16 to load large models.We employ greedy search as our decoding strategy and set max new tokens of 512 for all datasets.All experiments are conducted using NVIDIA L20 GPU.</p>
<p>Utility Evaluation</p>
<p>In this section, we use the reading vector v to control the generation of CoT reasoning paths.As shown in Table 1, 2 and 3, we present the performance comparison between our approach and baselines on each task.From these tables, we can see the following interesting findings.</p>
<p>(i) Our framework can guide/correct the reasoning direction of LLM.For example, for zero-shot CoT, our approach demonstrates superior performance, surpassing Mistral-7B-instruct by 4% on the SVAMP dataset, and for few-shot CoT, our approach achieves a significant improvement over LLaMA-2-7B-chat by 2.95% on the CSQA dataset.This is mainly because our approach demonstrates that our methodology effectively directs the correct reasoning path of LLM, thereby improving the accuracy of reasoning.This highlights the effectiveness our interpretability in CoT, which employs concept-level representation read operations to localize errors in CoT, followed by control operations to rectify LLMs' reasoning paths.</p>
<p>(ii) Few-shot CoT have stereotypes in the reasoning phase.From these tables, we observe that, except for the commonsense reasoning task,   our approach generally performs better on zeroshot control than con few-shot control.This is mainly due to the fact that it is affected by the few shot examples.To investigate why the performance of the few-shot concept is inferior to that of the zero-shot prompt, we analyzed instances where the few-shot CoT concept failed.As illustrated in figure 4, it shows that the model answered incorrectly in the red part by the few-shots learning while initially stating a correct fact of the question in the blue part.</p>
<p>It was affected by the few shots that someone even flipped the coin.Afterward, this behavior reinforces the incorrect cognition by adding the few-shots control that the model is wrong initially stating a fact.After adding the zero-shot control, the model accurately answered the question by stating the correct facts.These results indicate that few-shot learning produces the stereotype when reasoning.</p>
<p>Interpretability Visualization</p>
<p>To further demonstrate the effectiveness of our approach, we provide additional cases in three models on arithmetic, commonsense, and symbolic reasoning tasks in Appendix D (See Figure 7, 8, 9, and 10).</p>
<p>For zero-shot CoT, Figure 7 displays the real prediction of a case by the model both without control and control.From this figure, we can observe that the model is wrong when simplifying the equation, and the reasoning path is offset.As shown in the red part, the reason for the mistake of the model is that multiple steps have been performed in the process of simplification, which leads to calculation errors.After adding control to the model, the direction of the reasoning path of the model will be corrected to achieve the calculation error that occurs when the control is not added.</p>
<p>For few-shots CoT, Figure 8 demonstrates the real prediction of a case by the model both without control and with control on the strategyQA dataset.From this figure, we can observe that the model initially makes a correct inference regarding the elevation of Mount Fuji and the depth of the Sea of Japan.However, the model erroneously concludes that Mount Fuji would not be out of the sea due to a miscalculation in comparing the mountain's height with the sea's depth as illustrated in the red segment.After adding control to the model, the reasoning path leads to the correct conclusion that the top of Mount Fuji would indeed be out of the Sea of Japan, as highlighted in the green segment.</p>
<p>The results highlight our interpretability in CoT, which employs concept-level representation read operations to localize errors in CoT, followed by control operations to rectify LLMs' reasoning paths.This intuitive interpretable analysis for CoT reasoning is helpful in tracing and controlling the errors LLMs made during the CoT process.</p>
<p>Conclusion</p>
<p>In this work, we propose a framework to analyze and understand the CoT reasoning from Hopfieldian view under zero-shot and few-shot settings.Experimental results show our framework can improve the accuracy of reasoning, highlighting that our framework can provide intuitive and interpretable analysis for CoT reasoning, which is helpful in tracing and controlling the errors LLMs make during the CoT process, rectifying LLMs' reasoning paths, and enhancing CoT's transparency.</p>
<p>Limitations</p>
<p>Our current problem formulation is primarily focused on text data.We consider multi-modal scenarios, i.e., analyzing concepts from multiple different modalities as a future research direction.</p>
<p>Ethics Statement</p>
<p>Our framework can provide intuitive and interpretable analysis for CoT reasoning, which is helpful in tracing and controlling the errors LLMs made during the CoT process.This poses an impact on rectifying LLMs' reasoning paths and enhancing CoT's transparency.</p>
<p>B Other Experimental Details</p>
<p>B.1 Dataset</p>
<p>The statistics of the data is shown in Table 4.The details about each data set are as follows:</p>
<p>Arithmetic Reasoning.The arithmetic reasoning benchmarks aim to analyze and/or understand the model's mathematical reasoning skills.These include: (i) GSM8K (Cobbe et al., 2021), a math word problems benchmark encompassing a variety of reasoning steps; (ii) SVAMP (Patel et al., 2021), containing math word problems with multiple structures; (iii) AQuA (Miao et al., 2020), containing algebraic applications and mathematical reasoning problems.</p>
<p>Commonsense Reasoning.These data sets aim to analyze the ability of the model on commonsense reasoning tasks.These include: (i) Strate-gyQA (Geva et al., 2021), a commonsense benchmark requiring multi-level strategy to answer the question; (ii) CSQA (Talmor et al., 2019) benchmark dataset of multiple-choice questions that require different types of commonsense knowledge to predict the correct answers.</p>
<p>Symbolic Reasoning.These data sets aim to test the abilities of the model requiring advanced symbolic capabilities.For this task, we curated two new datasets, as follows.(i) Coin Flip dataset, we employ the data curation strategy of a previous study (Wei et al., 2022b) using the number of operations as 2, 4 and 7 to come up with the complete dataset; (ii) Random Letter, an advanced version of the last letter concatenation with reference to the previously studied form of word assembly (Wei et al., 2022b), where 2-4 words are randomly formed and characters are randomly drawn from them, instead of taking the beginning or the end of each word at a fixed point.</p>
<p>C.2 Prompt Templates</p>
<p>Table 6 illustrates the design of stimulus prompts utilized for Chain of Thought (CoT) prompting, distinguishing between zero-shot CoT and few-shot CoT methodologies.In the zero-shot CoT approach, the model is presented with a question devoid of preceding examples, in contrast to the few-shot CoT method, where the model is furnished with multiple exemplars.For each method, the first row is a positive prompt and the second is a negative prompt.Red indicates stimulus token.</p>
<p>Task Extraction Template</p>
<p>GSM8K</p>
<p>Therefore, the answer (arabic numerals) is SVAMP</p>
<p>Therefore, the answer (arabic numerals) is AQuA Therefore, among A through E, the answer is StrategyQA</p>
<p>Therefore, the answer (Yes or No) is CSQA Therefore, among A through E, the answer is Coin Flip</p>
<p>Therefore, the answer (Yes or No) is Random Letter Therefore, the answer is Table 7: Extraction templates and answer cleansing approaches for various tasks.</p>
<p>C.3 Answer Extract Prompts</p>
<p>The demonstration of our answer extraction method and answer clearing approach is in Table 7.</p>
<p>C.4 Example Prompts for Few-shot Setting</p>
<p>We demonstrate our example prompts for few-shot setting in Table 8, 9, 10, 11, 12 and 13.</p>
<p>D Case Demonstrations</p>
<p>We show our additional cases in three models on arithmetic, commonsense, and symbolic reasoning tasks in Figure 7, 8, 9, and 10.</p>
<p>A: The coin was flipped by Ka and Sherrie.So the coin was flipped 2 times, which is an even number.The coin started heads up, so after an even number of flips, it will still be heads up.So the answer is yes.</p>
<p>Q: A coin is heads up.Jamey flips the coin.Teressa flips the coin.Is the coin still heads up?</p>
<p>A: The coin was flipped by Jamey and Teressa.So the coin was flipped 2 times, which is an even number.The coin started heads up, so after an even number of flips, it will still be heads up.So the answer is yes.</p>
<p>Q: A coin is heads up.Maybelle flips the coin.Shalonda does not flip the coin.Is the coin still heads up?</p>
<p>A: The coin was flipped by Maybelle.So the coin was flipped 1 time, which is an odd number.The coin started heads up, so after an odd number of flips, it will be tails up.So the answer is no.</p>
<p>Q: A coin is heads up.Millicent does not flip the coin.Conception flips the coin.Is the coin still heads up?</p>
<p>A: The coin was flipped by Conception.So the coin was flipped 1 time, which is an odd number.The coin started heads up, so after an odd number of flips, it will be tails up.So the answer is no.</p>
<p>Q: A coin is heads up.Sal flips the coin.Raymond does not flip the coin.Is the coin still heads up?</p>
<p>A: The coin was flipped by Sal.So the coin was flipped 1 time, which is an odd number.</p>
<p>The coin started heads up, so after an odd number of be tails the is A coin is heads up.Conception flips the coin.Kristian does not flip the coin.Is the coin still heads up?</p>
<p>A: The coin was flipped by Conception.So the coin was flipped 1 time, which is an odd number.The coin started heads up, so after an odd number of flips, it will be tails up.So the answer is no.</p>
<p>Q: A coin is heads up.Inga does not flip the coin.Elanor does not flip the coin.Is the coin still heads up?</p>
<p>A: The coin was flipped by no one.So the coin was flipped 0 times.The coin started heads up, and it was not flipped, so it is still heads up.So the answer is yes.</p>
<p>Q: A coin is heads up.Ryan flips the coin.Shaunda flips the coin.Is the coin still heads up?</p>
<p>A: The coin was flipped by Ryan and Shaunda.So the coin was flipped 2 times, which is an even number.The coin started heads up, so after an even number of flips, it will still be heads up.So the answer is yes.</p>
<p>Table 13: Few-shot exemplars for full chain of thought prompt for the coin flip task.</p>
<p>Q: The original price of an item is discounted 22%.A customer buys the item at this discounted price using a $20-off coupon.There is no tax on the item, and this was the only item the customer bought.Let the original price of the item be x.The item is discounted 22%, so the discounted price is 0.78x (since 1 -0.22 = 0.78).The customer uses a $20-off coupon, so the price they pay is 0.78x -20.The customer pays $1.90 more than half the original price, so we can set up the equation: 0.78x -20 = 0.5x + 1.90</p>
<p>Simplifying the equation, we get: 0.28x = 22.90 Dividing both sides by 0.28, we get: x = 81.79So the original price of the item is approximately $81.79.</p>
<p>None of the answer choices match this value, so we can conclude that the correct answer is not among the options provided.</p>
<p>Let's think step by step.</p>
<p>Let the original price of the item be x.The item is discounted 22%, so the discounted price is 0.78x (since 1 -0.22 = 0.78).The customer uses a $20-off coupon, so the price the customer pays is 0.78x -20.The customer pays $1.90 more than half the original price, so we can set up the equation: 0.78x -20 = 0.5x + 1.90</p>
<p>We can add 20 to both sides: 0.78x = 0.5x + 21.90We can subtract 0.5x from both sides:0.28x= 21.90We can divide both sides by 0.28: x = 78.20So the original price of the item is $78.20.</p>
<p>The correct answer is (E) $78.20.Q: Do hamsters provide food for any animals?A: Hamsters are prey animals.Prey are food for predators.Thus, hamsters provide food for some animals.So the answer is yes.Q: Could Brooke Shields succeed at University of Pennsylvania?A: Brooke Shields went to Princeton University.Princeton University is about as academically rigorous as the University of Pennsylvania.Thus, Brooke Shields could also succeed at the University of Pennsylvania.So the answer is yes...... Q: Would the top of Mount Fuji stick out of the Sea of Japan?Mount Fuji is the highest mountain in Japan, with an elevation of approximately 3,776.24meters (12,389 feet).The Sea of Japan has an average depth of around 1,200 meters (3,937 feet).</p>
<ul>
<li>Control</li>
</ul>
<p>Since the height of Mount Fuji is significantly less than the average depth of the Sea of Japan, the top of Mount Fuji would not stick out of the sea.</p>
<p>Therefore, the answer is no.</p>
<p>Mount Fuji is the highest mountain in Japan, with an elevation of approximately 3,776.24meters (12,389 feet).The Sea of Japan has an average depth of about 1,200 meters (3,937 feet).</p>
<p>Since the height of Mount Fuji is greater than the average depth of the Sea of Japan, the top of Mount Fuji would stick out of the sea.</p>
<p>Therefore, the answer is yes.</p>
<ul>
<li>Control</li>
</ul>
<p>Figure 8: A real case predicted by Mistral-7B-Instruct-v0.2 with few-shots CoT on the strategyQA dataset.The purple part is an example of input-output pairs given by user.The segment highlighted in blue represents the correct output of the model.The red part shows that the model starts to reason in the wrong direction without control, while the green portion indicates the model reason in the correct direction after adding control.</p>
<p>Q: A coin is heads up.Ka flips the coin.Sherrie flips the coin.Is the coin still heads up?A: The coin was flipped by Ka and Sherrie.So the coin was flipped 2 times, which is an even number.The coin started heads up, so after an even number of flips, it will still be heads up.So the answer is yes.Q: A coin is heads up.Jamey flips the coin.Teressa flips the coin.Is the coin still heads up?A: The coin was flipped by Jamey and Teressa.So the coin was flipped 2 times, which is an even number.The coin started heads up, so after an even number of flips, it will still be heads up.So the answer is yes.... Q: A coin is heads up.wanjiru flips the coin.nunnery does not flip the coin.canya does not flip the coin.kydin flips the coin.Is the still heads up?</p>
<p>Let's break it down: * wanjiru flips coin: 1 flip, odd number * nunnery and canya do not flip the coin: no flips, still 1 flip * kydin flips the coin: 2 flips, even number So, the coin was flipped a total of 3 times, which is an odd number.Since the coin started heads up and was flipped an odd number of times, it will be tails up.So, the answer is no.</p>
<p>The coin was flipped by wanjiru and kydin.So the coin was flipped 2 times, which is an even number.</p>
<p>The coin started heads up, so after an even number of flips, it will still be heads up.So the answer is yes.</p>
<ul>
<li>Control</li>
</ul>
<p>Figure 9: A real case predicted by Meta-Llama-3-8B-Instruct with few-shots CoT on the coin flip dataset.The purple part is an example of input-output pairs given by user.The segment highlighted in blue represents the correct output of the model.The red part shows that the model starts to reason in the wrong direction without control, while the green portion indicates the model reason in the correct direction after adding control.</p>
<p>Please think step by step.Note that flips the coin means the coin is facing the other way.You need to output the appropriate analysis and then answer whether the coin still heads up first (yes or no).For example: Q: A coin is heads up.Winrey flips the coin.Tiferet flips the coin.Is the coin still heads up?A: Becasue The whole process is carried out twice, at the beginning the coin is heads up.The first operation Winrey flipped the coin, so the coin direction changed, no longer heads up; the second operation Tiferet flipped the coin, so the coin direction changed again, became heads up.So in summary, the coin still heads up, the answer is yes.Here is the question: A coin is heads up.Terrice flips the coin.</p>
<p>Beybala does not flip the coin.Toomay flips the coin.Sheraine flips the coin.Is the coin still heads up?</p>
<p>Input</p>
<p>Of course!I'm here to help you with your question.</p>
<p>To answer your question, we need to break it down step by step:</p>
<p>1.The coin is initially heads up. 2. Winrey flips the coin, which means the coin's direction changes to tails.3. Tiferet flips the coin again, which means the coin's direction changes back to heads.4. Beybala, Toomay, and Sheraine do not flip the coin.</p>
<p>So, the coin is now facing tails, but then it changes back to heads again due to Tiferet's flip.Therefore, the answer is yes, the coin is still heads up.</p>
<p>Output</p>
<p>Please think step by step.Note that flips the coin means the coin is facing the other way.You need to output the appropriate analysis and then answer whether the coin still heads up first (yes or no).For example: Q: A coin is heads up.Winrey flips the coin.Tiferet flips the coin.Cendy flips the coin.Is the coin still heads up?A: Becasue The whole process is carried out twice, at the beginning the coin is heads up.The first operation Winrey flipped the coin, so the coin direction changed, no longer heads up; the second operation Tiferet flipped the coin, so the coin direction changed again, became heads up.Finally, Cendy filp the coin,so the coin direction changed again.So the answer is no, the coin not heads up.</p>
<p>Here is the question: A coin is heads up.Terrice flips the coin.In this scenario, Terrice flipped the coin, which means the coin is now facing the other way.So, the coin is no longer heads up.Next, Beybala does not flip the coin, so the coin remains in the same position.Then, Toomay flips the coin.Since Toomay flipped the coin, the coin is now facing the other way again.So, the coin is no longer heads up.Finally, Sheraine flips the coin.Sheraine flipped the coin, which means the coin is now facing the other way.So, the coin is no longer heads up.</p>
<p>Therefore, the answer to the question is no, the coin is not heads up.</p>
<p>Output Figure 10: An example illustrating the sensitivity of LLaMA-2-7B-chat in the one-shot scenario.The parts highlighted in green are the instruction and reasoning path.The key difference between the two input-output pairs is highlighted in blue: The example in the first row has a final answer of "yes", while the example in the second row has a final answer of "no".We can find that the final output of the model tends to match the example, even if the example's result is incorrect.The model adjusts its reasoning steps (highlighted in red) to achieve this consistency.</p>
<p>Figure 1 :
1
Figure 1: Illustration of mechanisms in the cognitive brain (Barack and Krakauer, 2021) and neural network.</p>
<p>Figure 2 :
2
Figure 2: Our CoT explanation framework based on Hopfieldian view.</p>
<p>8 // transform H into the logits of model 9 logits ← trans_into_logits(H) 10 return logits Let's think step by step.Day 1: Both trains travel westward for 80 miles.So, Train A covers 80 miles.Train B covers 80 miles as well...Total distance covered by both trains on Day 1 = 2 x 80 = 160 miles.Day 2: Both trains travel northward for 150 miles...Total distance covered by both trains on Day 2 = 2 x 150 = 300 Therefore, the distance covered by each train in the two days is: Train A: 160 + 300 = 460 miles; Train B: 160 + 300 = 460 miles.So, both trains covered a total distance of 460 miles.</p>
<p>Figure 4 :
4
Figure4: A real case predicted by LLaMA-3-8B-instruct with few-shot CoT on the coin flip dataset.The purple part is an example of input-output pairs given by user.The segment highlighted in blue represents the correct output of the model.The red part shows that the model starts to reason in the wrong direction without control, while the green portion indicates the model reason in the correct direction after adding control.</p>
<p>Figure 6 :
6
Figure 6: A example of using Few-shot CoT.</p>
<p>If the customer paid $1.90 more than half the original price of the item, what was the original price of the item?Answer Choices: (A) $61 (B) $65 (C) $67.40 (D) $70 (E) $78.20 Let's think step by step.</p>
<p>Figure 7 :
7
Figure 7: A real case predicted by LLaMA-3-8B-instruct with zero-shot CoT on the AQuA dataset.The segment highlighted in blue the correct output of the model.The red part shows that the model starts to reason in the wrong direction without control, while the green portion indicates the model reason in the correct direction after adding control.</p>
<p>Beybala does not flip the coin.Toomay flips the coin.Sheraine flips the coin.Is the coin still heads up?Input I understand, and I'm happy to help!Let's analyze the situation: A coin is heads up.Terrice flips the coin.</p>
<p>Stimilus Stimulate Emerging from pretraining Actions ➕ Representation ➕ Representations Reading ➖ Actions Space 1 Actions Space 2 Representations Conrolling Analysis based on Hopfieldian view
Concept SimulationActivated ConceptsZero-shot or Few-shotLLMe.g. inspire/enjoy/positive language, reason, "step", " problem", ...promptsConceptModelingLatent ConceptsPretraining DocumentsLLMe.g. medical, climate change, inspire/enjoy/positive language, ...</p>
<p>To elucidate the sensitivity of LLMs to different CoT prompts, in this step we will employ various "stimuli " encompassing one or more concepts for inducing diverse model behaviors under both zero-shot and few-shot settings.Our objective is to analyze how the concepts Algorithm 1: LAT Process-flow Input: Stimulus set S, model M θ , function Rep(., .)Output: Reading vector v 1
, inorder to identify the directions of the key concept,as illustrated in Algorithm 1. It encompasses twodifferent modules: (i) Stimuli Selection; (ii) NeuralActivity Monitoring; and (iii) Concept Identifica-tion, with each module explained as follows:(i) Stimuli Selection.</p>
<p>Table 1 :
1
Results on the Arithmetic Reasoning task for different models.The left side shows scores without control, and the right side shows scores after control.
ModelGSM8KArithmetic Reasoning SVAMPAQuAzero-shot CoTMistral-7B-instruct45.03 / 47.0857.33 / 61.3325.59 / 30.71LLaMA-2-7B-chat26.31 / 27.3746.00 / 46.6727.95 / 30.71LLaMA-3-8B-instruct74.45 / 75.8982.67 / 84.0042.13 / 45.67Avg.48.60 / 50.11 (1.51↑) 62.00 / 64.00 (2.00↑) 31.89 / 35.70 (3.81↑)few-shot CoTMistral-7B-instruct46.78 / 47.1661.33 / 61.6734.65 / 35.04LLaMA-2-7B-chat5.00 / 5.1638.67 / 39.0022.44 / 23.62LLaMA-3-8B-instruct72.71 / 74.8382.00 / 83.6752.76 / 53.21Avg.41.50 / 42.38 (0.88↑) 60.67 / 61.45 (0.78↑) 36.61 / 37.29 (0.68↑)ModelCommonsense Reasoning StrategyQA CSQAzero-shot CoTMistral-7B-instruct63.01 / 63.9767.57 / 67.98LLaMA-2-7B-chat62.66 / 63.4955.45 / 56.59LLaMA-3-8B-instruct67.42 / 68.0345.29 / 44.39Avg.64.36 / 65.16 (0.80↑) 56.10 / 56.32 (0.22↑)few-shot CoTMistral-7B-instruct63.49 / 65.5552.83 / 54.87LLaMA-2-7B-chat55.63 / 55.7255.61 / 58.56LLaMA-3-8B-instruct66.38 / 68.4767.24 / 68.47Avg.61.83 / 63.25 (1.42↑) 58.56 / 60.63 (2.07↑)</p>
<p>Table 2 :
2
Results on the Commonsense Reasoning task for different models.
ModelSymbolic Reasoning Coin Flip Random Letterzero-shot CoTMistral-7B-instruct50.80 / 48.9518.33 / 19.66LLaMA-2-7B-chat52.20 / 55.7517.33 / 17.66LLaMA-3-8B-instruct90.45 / 90.5538.00 / 38.00Avg.64.48 / 65.10 (0.62↑) 24.55 / 25.11 (0.56↑)few-shot CoTMistral-7B-instruct78.73 / 78.1933.67 / 33.67LLaMA-2-7B-chat50.74 / 50.8429.67 / 30.00LLaMA-3-8B-instruct77.02 / 77.5953.67 / 54.67Avg.68.83 / 68.87 (0.04↑) 39.00 / 39.45 (0.45↑)</p>
<p>Table 3 :
3
Results on the Symbolic Reasoning task for different models.</p>
<p>Table 4 :
4
Statistics of the data set.N represents the number of evaluation examples.
DatasetTask DomainNAnswer FormatGSM8KArithmetic1319NumberSVAMPArithmetic300NumberAQuAArithmetic254 Multiple ChoicesStrategyQACommonsense 2290Yes or NoCSQACommonsense 1221 Multiple ChoicesCoin FlipSymbolic2000Yes or NoRandom LetterSymbolic300LetterB.2 The number of reading set sizesThe table 5 below presents the sizes of reading setsutilized in our study providing the different set sizesemployed during the reading phase on zero-shot andfew-shots CoT.DatasetZero-shot Few-shotsGSM8K128512SVAMP512256AQuA256256StrategyQA512256CSQA512256Coin Flip128512Random Letter128128</p>
<p>Table 5 :
5
The number of reading set sizes in the experiment.
C PromptsC.1 Postive and Negative Stimuluspositive stimulusUSER: Would a greyhound be able to outruna greyhound bus?ASSISTANT: Let's think step by step.negative stimulusUSER: Would a greyhound be able to outruna greyhound bus?ASSISTANT:USER: <question>zero-shot CoTASSISTANT: Let's think step by step.USER: <question>ASSISTANT:USER: <n different examples>few-shot CoT<question> ASSISTANT:USER: <question>ASSISTANT:</p>
<p>Table 6 :
6
The stimulus prompting design for CoTstyle methods.</p>
<p>A Chain-of-ThoughtZero-shot CoT.Zero-shot Chain-of-Thought (CoT)(Kojima et al., 2022)is a reasoning approach where a language model generates a step-by-step explanation or thought process to solve a problem without requiring prior examples or specific training.Given a problem Q, we aim to derive the answer using the zero-shot CoT method in a generate function f :(where P = "Let' s think step by step" in this research.The example of zero-shot CoT is shown in Figure5Q: On average Joe throws 25 punches per minute.A fight lasts 5 rounds of 3 minutes.How many punches did he throw?Let's think step by step.... ... Few-shots CoT.Few-shots Chain-of-Thought (CoT)(Wei et al., 2022b) is a prompting technique used in large language models where a few examples of step-by-step reasoning processes are provided.Given a problem Q, we aim to derive the answer using the few-shot CoT method in a generate function f :where T = {t 1 , t 2 , . . ., t n } and t i represents the i-th example in the prompt.The example of zeroshot CoT shown on Figure6, T can be shown in Appendix C.4.Q: Roger has 5 tennis balls.He buys 2 more cans of tennis balls.Each can has 3 tennis balls.How many tennis balls does he have now?A: Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls.5 + 6 = 11.The answer is 11.Q: On average Joe throws 25 punches per minute.A fight lasts 5 rounds of 3 minutes.How many punches did he throw?... ... ...Prompt for GSM8K and SVAMPQ: There are 15 trees in the grove.Grove workers will plant trees in the grove today.After they are done, there will be 21 trees.How many trees did the grove workers plant today?A: There are 15 trees originally.Then there were 21 trees after some more were planted.So there must have been 21 -15 = 6.The answer is 6.Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?A: There are originally 3 cars.2 more cars arrive.3 + 2 = 5.The answer is 5.Q: Leah had 32 chocolates and her sister had 42.If they ate 35, how many pieces do they have left in total?A: Originally, Leah had 32 chocolates.Her sister had 42.So in total they had 32 + 42 = 74.After eating 35, they had 74 -35 = 39.The answer is 39.Q: Jason had 20 lollipops.He gave Denny some lollipops.Now Jason has 12 lollipops.How many lollipops did Jason give to Denny?A: Jason started with 20 lollipops.Then he had 12 after giving some to Denny.So he gave Denny 20 -12 = 8.The answer is 8.Q: Shawn has five toys.For Christmas, he got two toys each from his mom and dad.How many toys does he have now?A: Shawn started with 5 toys.If he got 2 toys each from his mom and dad, then that is 4 more toys.5 + 4 = 9.The answer is 9.Q: There were nine computers in the server room.Five more computers were installed each day, from Monday to Thursday.How many computers are now in the server room?A: There were originally 9 computers.For each of 4 days, 5 more computers were added.So 5 * 4 = 20 computers were added.9 + 20 is 29.The answer is 29.Q: Michael had 58 golf balls.On Tuesday, he lost 23 golf balls.On Wednesday, he lost 2 more.How many golf balls did he have at the end of Wednesday?A: Michael started with 58 golf balls.After losing 23 on Tuesday, he had 58 -23 = 35.After losing 2 more, he had 35 -2 = 33 golf balls.The answer is 33.Q: Olivia has $23.She bought five bagels for $3 each.How much money does she have left?A: Olivia had 23 dollars.5 bagels for 3 dollars each will be 5 x 3 = 15 dollars.So she has 23 -15 dollars left. 23 -15 is 8.The answer is 8.Table8: Few-shot exemplars for full chain of thought prompt for the arithmetic reasoning task.A: There are 9 one-digit numbers from 1 to 9.There are 90 two-digit numbers from 10 to 99.There are 401 three-digit numbers from 100 to 500.9 + 90(2) + 401(3) = 1392.The answer is (b).Table9: Few-shot exemplars for full chain of thought prompt for AQuA.Prompt for StrategyQA Q: Do hamsters provide food for any animals?A: Hamsters are prey animals.Prey are food for predators.Thus, hamsters provide food for some animals.So the answer is yes.Q: Could Brooke Shields succeed at University of Pennsylvania?A: Brooke Shields went to Princeton University.Princeton University is about as academically rigorous as the University of Pennsylvania.Thus, Brooke Shields could also succeed at the University of Pennsylvania.So the answer is yes.Q: Yes or no: Hydrogen's atomic number squared exceeds number of Spice Girls?A: Hydrogen has an atomic number of 1. 1 squared is 1.There are 5 Spice Girls.Thus, Hydrogen's atomic number squared is less than 5.So the answer is no.Q: Yes or no: Is it common to see frost during some college commencements?A: College commencement ceremonies can happen in December, May, and June.December is in the winter, so there can be frost.Thus, there could be frost at some commencements.So the answer is yes.Q: Yes or no: Could a llama birth twice during War inVietnam (1945-46)?A: The War in Vietnam was 6 months.The gestation period for a llama is 11 months, which is more than 6 months.Thus, a llama could not give birth twice during the War in Vietnam.So the answer is no.Q: Yes or no: Would a pear sink in water?A: The density of a pear is about 0.6g/cm 3 , which is less than water.Objects less dense than water float.Thus, a pear would float.So the answer is no.A: The answer must be an item that can absorb ink.Of the above choices, only blotters are used to absorb ink.So the answer is (e).Q: What home entertainment equipment requires cable?Answer Choices: (a) radio shack (b) substation (c) television (d) cabinetA: The answer must require cable.Of the above choices, only television requires cable.So the answer is (c).Q:The fox walked from the city into the forest, what was it looking for?Answer Choices: (a) pretty flowers (b) hen house (c) natural habitat (d) storybookA: The answer must be something in the forest.Of the above choices, only natural habitat is in the forest.So the answer is (b).A: The answer should be the feeling of someone getting divorced who was doing all the work.Of the above choices, the closest feeling is bitterness.So the answer is (c).Table11: Few-shot exemplars for full chain of thought prompt for CSQA.There are newlines between the answer choices that are omitted in the table for space reasons.Prompt for Rondom Letter Concatenation Q: Take the first letters of the words in "James Hickman " and concatenate them.A: The first letter of "James" is "J".The first letter of "Hickman" is "H".Concatenating them is "JH".The answer is JH.Q: Take the second letters of the words in "Carl Mccall Bonilla " and concatenate them.A: The second letter of "Carl" is "a".The second letter of "Mccall" is "c".The second letter of "Bonilla" is "o" Concatenating them is "aco".The answer is aco.Q: Take the third letters of the words in "Randy Tanner " and concatenate them.A: The third letter of "Randy" is "n".The third letter of "Tanner" is "n".Concatenating them is "nn".The answer is nn.Q: Take the first letters of the words in "Kenny Kim " and concatenate them.A: The first letter of "Kenny" is "K".The first letter of "Kim" is "K".Concatenating them is "KK".The answer is KK.Table12: Few-shot exemplars for full chain of thought prompt for the random letter concatenation task.
Two views on the cognitive brain. L David, John W Barack, Krakauer, Nature Reviews Neuroscience. 2262021</p>
<p>Probing classifiers: Promises, shortcomings, and advances. Yonatan Belinkov, 10.1162/COLI_A_00422Comput. Linguistics. 4812022</p>
<p>Towards monosemanticity: Decomposing language models with dictionary learning. Trenton Bricken, Adly Templeton, Joshua Batson, Brian Chen, Adam Jermyn, Tom Conerly, Nicholas L Turner, Cem Anil, Carson Denison, Amanda Askell, 2023Transformer Circuits Thread</p>
<p>Discovering latent knowledge in language models without supervision. Collin Burns, Haotian Ye, Dan Klein, Jacob Steinhardt, Proceedings of ICLR 2023. ICLR 20232023</p>
<p>Keyuan Cheng, Muhammad Asif Ali, Shu Yang, Gang Ling, Yuxuan Zhai, Haoyang Fei, Ke Xu, Lu Yu, Lijie Hu, Di Wang, arXiv:2405.15452Leveraging logical rules in knowledge editing: A cherry on the top. 2024aarXiv preprint</p>
<p>Keyuan Cheng, Gang Lin, Haoyang Fei, Lu Yu, Muhammad Asif Ali, Lijie Hu, Di Wang, arXiv:2404.00492Multi-hop question answering under temporal knowledge editing. 2024barXiv preprint</p>
<p>What does BERT look at? an analysis of bert's attention. Kevin Clark, Urvashi Khandelwal, Omer Levy, Christopher D Manning, 10.18653/V1/W19-4828Proceedings of ACL 2019. ACL 20192019</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, CoRR, abs/2110.141682021</p>
<p>Faithful reasoning using large language models. Antonia Creswell, Murray Shanahan, 10.48550/ARXIV.2208.14271CoRR, abs/2208.142712022</p>
<p>How to think step-by-step: A mechanistic understanding of chain-of-thought reasoning. Subhabrata Dutta, Joykirat Singh, Soumen Chakrabarti, Tanmoy Chakraborty, 10.48550/ARXIV.2402.18312CoRR, abs/2402.183122024</p>
<p>Are large language models bayesian? a martingale perspective on in-context learning. Fabian Falck, Ziyu Wang, Christopher C Holmes, ICLR 2024 Workshop on Secure and Trustworthy Large Language Models. 2024</p>
<p>Net2vec: Quantifying and explaining how concepts are encoded by filters in deep neural networks. Ruth Fong, Andrea Vedaldi, 10.1109/CVPR.2018.00910Proceedings of CVPR 2018. CVPR 20182018</p>
<p>Erasing concepts from diffusion models. Rohit Gandikota, Joanna Materzynska, Jaden Fiotto-Kaufman, David Bau, 10.1109/ICCV51070.2023.00230Proceedings of ICCV 2023. ICCV 20232023</p>
<p>PAL: program-aided language models. Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, Graham Neubig, Proceedings of ICML 2023. ICML 20232023</p>
<p>Did aristotle use a laptop? A question answering benchmark with implicit reasoning strategies. Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, Jonathan Berant, 10.1162/TACL_A_00370Trans. Assoc. Comput. Linguistics. 92021</p>
<p>Rethinking with retrieval: Faithful large language model inference. Hangfeng He, Hongming Zhang, Dan Roth, 10.48550/ARXIV.2301.00303CoRR, abs/2301.003032023</p>
<p>Lijie Hu, Ivan Habernal, Lei Shen, Di Wang, arXiv:2301.09112Differentially private natural language models: Recent advances and future directions. 2023aarXiv preprint</p>
<p>Improving faithfulness for vision transformers. Lijie Hu, Yixin Liu, Ninghao Liu, Mengdi Huai, Lichao Sun, Di Wang, arXiv:2311.179832023barXiv preprint</p>
<p>Seat: stable and explainable attention. Lijie Hu, Yixin Liu, Ninghao Liu, Mengdi Huai, Lichao Sun, Di Wang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2023c37</p>
<p>Lijie Hu, Chenyang Ren, Zhengyu Hu, Cheng-Long Wang, Di Wang, arXiv:2405.15476Editable concept bottleneck models. 2024arXiv preprint</p>
<p>Mathprompter: Mathematical reasoning using large language models. Shima Imani, Liang Du, Harsh Shrivastava, 10.18653/V1/2023.ACL-INDUSTRY.4Proceedings of ACL 2023. ACL 20232023</p>
<p>Attention is not explanation. Sarthak Jain, Byron C Wallace, 10.18653/V1/N19-1357Proceedings of NAACL-HLT 2019. NAACL-HLT 20192019</p>
<p>Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego De Las, Florian Casas, Gianna Bressand, Guillaume Lengyel, Lucile Lample, Saulnier, 10.48550/ARXIV.2310.06825CoRR, abs/2310.06825Mistral 7b. 2023</p>
<p>The impact of reasoning step length on large language models. Mingyu Jin, Qinkai Yu, Dong Shu, Haiyan Zhao, Wenyue Hua, Yanda Meng, Yongfeng Zhang, Mengnan Du, 10.48550/ARXIV.2401.04925CoRR, abs/2401.049252024</p>
<p>Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (TCAV). Been Kim, Martin Wattenberg, Justin Gilmer, Carrie J Cai, James Wexler, Fernanda B Viégas, Rory Sayres, Proceedings of ICML 2018. ICML 20182018</p>
<p>Cotever: Chain of thought prompting annotation toolkit for explanation verification. Seungone Kim, June Se, Yul Joo, Hyungjoo Jang, Jinyoung Chae, Yeo, 10.18653/V1/2023.EACL-DEMO.23Proceedings of EACL 2023. EACL 20232023</p>
<p>Chatgpt: Jack of all trades, master of none. Jan Kocon, Igor Cichecki, Oliwier Kaszyca, Mateusz Kochanek, Dominika Szydlo, Joanna Baran, Julita Bielaniewicz, Marcin Gruza, Arkadiusz Janz, Kamil Kanclerz, 10.1016/J.INFFUS.2023.101861Inf. Fusion. 991018612023</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Proceedings of NeurIPS. NeurIPS2022. 2022</p>
<p>Faithful visionlanguage interpretation via concept bottleneck models. Songning Lai, Lijie Hu, Junxiao Wang, Laure Berti-Equille, Di Wang, The Twelfth International Conference on Learning Representations. 2023</p>
<p>Measuring faithfulness in chain-of-thought reasoning. Tamera Lanham, Anna Chen, Ansh Radhakrishnan, Benoit Steiner, Carson Denison, Danny Hernandez, Dustin Li, Esin Durmus, Evan Hubinger, Jackson Kernion, 10.48550/ARXIV.2307.13702CoRR, abs/2307.137022023</p>
<p>Text guided image editing with automatic concept locating and forgetting. Jia Li, Lijie Hu, Zhixian He, Jingfeng Zhang, Tianhang Zheng, Di Wang, ; Liu, Alisa Liu, Ximing Lu, Sean Welleck, Peter West, Le Ronan, Yejin Bras, Hannaneh Choi, Hajishirzi, 10.18653/V1/2022.ACL-LONG.225arXiv:2405.19708Proceedings of ACL 2022. ACL 20222024. 2022arXiv preprintGenerated knowledge prompting for commonsense reasoning</p>
<p>A survey of deep learning for mathematical reasoning. Pan Lu, Liang Qiu, Wenhao Yu, Sean Welleck, Kai-Wei Chang, 10.18653/V1/2023.ACL-LONG.817Proceedings of ACL 2023. ACL 20232023</p>
<p>Faithful chainof-thought reasoning. Qing Lyu, Shreya Havaldar, Adam Stein, Li Zhang, Delip Rao, Eric Wong, Marianna Apidianaki, Chris Callison-Burch, 10.18653/V1/2023.IJCNLP-MAIN.20Proceedings of IJCNLP 2023. IJCNLP 20232023</p>
<p>Self-refine: Iterative refinement with self-feedback. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Proceedings of NeurIPS 2023. NeurIPS 20232023</p>
<p>Locating and editing factual associations in GPT. Kevin Meng, David Bau, Alex Andonian, Yonatan Belinkov, Proceedings of NeurIPS. NeurIPS2022. 2022</p>
<p>The expressive power of transformers with chain of thought. William Merrill, Ashish Sabharwal, 10.48550/ARXIV.2310.07923CoRR, abs/2310.079232023</p>
<p>Introducing meta llama 3: The most capable openly available llm to date. Meta blog. 2024Meta</p>
<p>A diverse corpus for evaluating and developing english math word problem solvers. Shen-Yun, Chao-Chun Miao, Keh-Yih Liang, Su, 10.18653/V1/2020.ACL-MAIN.92Proceedings of ACL 2020. ACL 20202020</p>
<p>Rethinking the role of demonstrations: What makes in-context learning work. Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, Luke Zettlemoyer, 10.18653/V1/2022.EMNLP-MAIN.759Proceedings of EMNLP 2022. EMNLP 20222022</p>
<p>Cross-task generalization via natural language crowdsourcing instructions. Swaroop Mishra, Daniel Khashabi, Chitta Baral, Hannaneh Hajishirzi, 10.18653/V1/2022.ACL-LONG.244Proceedings of ACL 2022. ACL 20222022</p>
<p>Understanding neural networks via feature visualization: A survey. Anh Nguyen, Jason Yosinski, Jeff Clune, 10.1007/978-3-030-28954-6_4Proceedings of LNCS 2019. LNCS 20192019</p>
<p>Synthesizing the preferred inputs for neurons in neural networks via deep generator networks. Anh Mai Nguyen, Alexey Dosovitskiy, Jason Yosinski, Thomas Brox, Jeff Clune, Proceedings of NeurIPS 2016. NeurIPS 20162016</p>
<p>Show your work: Scratchpads for intermediate computation with language models. I Maxwell, Anders Nye, Guy Johan Andreassen, Henryk Gur-Ari, Jacob Michalewski, David Austin, David Bieber, Aitor Dohan, Maarten Lewkowycz, David Bosma, Luan, CoRR, abs/2112.001142021</p>
<p>Zoom in: An introduction to circuits. Chris Olah, Nick Cammarata, Ludwig Schubert, Gabriel Goh, Michael Petrov, Shan Carter, Distill. 532020</p>
<p>In-context learning and induction heads. Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova Dassarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, 10.48550/ARXIV.2209.11895CoRR, abs/2209.118952022</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Proceedings of NeurIPS. NeurIPS2022. 2022</p>
<p>Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning. Liangming Pan, Alon Albalak, Xinyi Wang, William Wang, 10.18653/V1/2023.FINDINGS-EMNLP.248Proceedings of EMNLP 2023. EMNLP 20232023</p>
<p>The linear representation hypothesis and the geometry of large language models. Kiho Park, Yo Joong Choe, Victor Veitch, 10.48550/ARXIV.2311.03658CoRR, abs/2311.036582023</p>
<p>Are NLP models really able to solve simple math word problems?. Arkil Patel, Satwik Bhattamishra, Navin Goyal, 10.18653/V1/2021.NAACL-MAIN.168Proceedings of NAACL-HLT 2021. NAACL-HLT 20212021</p>
<p>Let's think dot by dot: Hidden computation in transformer language models. Jacob Pfau, William Merrill, Samuel R , 10.48550/ARXIV.2404.15758CoRR, abs/2404.157582024Bowman</p>
<p>Reasoning with language model prompting: A survey. Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang, Huajun Chen, 10.18653/V1/2023.ACL-LONG.294Proceedings of ACL 2023. ACL 20232023</p>
<p>Scaling language models: Methods, analysis &amp; insights from training gopher. Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, H Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, CoRR, abs/2112.114462021</p>
<p>Toolformer: Language models can teach themselves to use tools. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom, Proceedings of NeurIPS 2023. NeurIPS 20232023</p>
<p>Gold doesn't always glitter: Spectral removal of linear and nonlinear guarded attribute information. Shun Shao, Yftah Ziser, Shay B Cohen, 10.18653/V1/2023.EACL-MAIN.118Proceedings of EACL 2023. EACL 20232023</p>
<p>Language models are multilingual chainof-thought reasoners. Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das, Jason Wei, Proceedings of ICLR 2023. ICLR 20232023</p>
<p>Reflexion: an autonomous agent with dynamic memory and self-reflection. Noah Shinn, Beck Labash, Ashwin Gopinath, 10.48550/ARXIV.2303.11366CoRR, abs/2303.113662023</p>
<p>Deep inside convolutional networks: Visualising image classification models and saliency maps. Karen Simonyan, Andrea Vedaldi, Andrew Zisserman, Proceedings of ICLR. ICLR2014. 2014</p>
<p>Smoothgrad: removing noise by adding noise. Daniel Smilkov, Nikhil Thorat, Been Kim, Fernanda B Viégas, Martin Wattenberg, CoRR, abs/1706.038252017</p>
<p>Chain of thoughtlessness: An analysis of cot in planning. Kaya Stechly, Karthik Valmeekam, Subbarao Kambhampati, CoRR, abs/2405.047762024</p>
<p>Intriguing properties of neural networks. Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J Goodfellow, Rob Fergus, Proceedings of ICLR. ICLR2014. 2014</p>
<p>Commonsenseqa: A question answering challenge targeting commonsense knowledge. Alon Talmor, Jonathan Herzig, Nicholas Lourie, Jonathan Berant, 10.18653/V1/N19-1421Proceedings of NAACL-HLT 2019. NAACL-HLT 20192019</p>
<p>Large language models are in-context semantic reasoners rather than symbolic reasoners. Xiaojuan Tang, Zilong Zheng, Jiaqi Li, Fanxu Meng, Song-Chun Zhu, Yitao Liang, Muhan Zhang, 10.48550/ARXIV.2305.14825CoRR, abs/2305.148252023</p>
<p>Scaling monosemanticity: Extracting interpretable features from claude 3 sonnet. Adly Templeton, Tom Conerly, Jonathan Marcus, Jack Lindsey, Trenton Bricken, Brian Chen, Adam Pearce, Craig Citro, Emmanuel Ameisen, Andy Jones, 2024Transformer Circuits Thread</p>
<p>Llama 2: Open foundation and fine-tuned chat models. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, 10.48550/ARXIV.2307.09288CoRR, abs/2307.09288et al. 2023.</p>
<p>Towards understanding chain-of-thought prompting: An empirical study of what matters. Boshi Wang, Sewon Min, Xiang Deng, Jiaming Shen, You Wu, Luke Zettlemoyer, Huan Sun, 10.18653/v1/2023.acl-long.153Proceedings of ACL 2023. ACL 20232023a</p>
<p>Interpretability in the wild: a circuit for indirect object identification in GPT-2 small. Kevin Ro, Wang , Alexandre Variengien, Arthur Conmy, Buck Shlegeris, Jacob Steinhardt, Proceedings of ICLR 2023. ICLR 20232023b</p>
<p>Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models. Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy , Ka-Wei Lee, Ee-Peng Lim, 10.18653/V1/2023.ACL-LONG.147Proceedings of ACL 2023. ACL 20232023c</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, Proceedings of ICLR 2023. ICLR 20232023d</p>
<p>Chain-ofthought reasoning without prompting. Xuezhi Wang, Denny Zhou, 10.48550/ARXIV.2402.10200CoRR, abs/2402.102002024</p>
<p>Finetuned language models are zero-shot learners. Jason Wei, Maarten Bosma, Y Vincent, Kelvin Zhao, Adams Wei Guu, Brian Yu, Nan Lester, Andrew M Du, Dai, V Quoc, Le, Proceedings of ICLR 2022. ICLR 20222022a</p>
<p>Chainof-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H Chi, V Quoc, Denny Le, Zhou, Proceedings of NeurIPS. NeurIPS2022b. 2022</p>
<p>Large language models are better reasoners with self-verification. Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Shengping Liu, Bin Sun, Kang Liu, 10.18653/V1/2023.FINDINGS-EMNLP.167Proceedings of EMNLP 2023. EMNLP 2023Jun Zhao. 2023</p>
<p>Reft: Representation finetuning for language models. Zhengxuan Wu, Aryaman Arora, Zheng Wang, Atticus Geiger, Dan Jurafsky, Christopher D Manning, Christopher Potts, 10.48550/ARXIV.2404.03592CoRR, abs/2404.035922024</p>
<p>An explanation of incontext learning as implicit bayesian inference. Sang Michael Xie, Aditi Raghunathan, Percy Liang, Tengyu Ma, Proceedings of ICLR 2022. ICLR 20222022</p>
<p>Shu Yang, Muhammad Asif Ali, Cheng-Long Wang, Lijie Hu, Di Wang, arXiv:2402.11260Moral: Moe augmented lora for llms' lifelong learning. 2024aarXiv preprint</p>
<p>Human-ai interactions in the communication era. Shu Yang, Lijie Hu, Lu Yu, Muhammad Asif Ali, Di Wang, arXiv:2402.11271Autophagy makes large models achieving local optima. 2024barXiv preprint</p>
<p>Shu Yang, Jiayuan Su, Han Jiang, Mengdi Li, Keyuan Cheng, Muhammad Asif Ali, Lijie Hu, Di Wang, arXiv:2404.00486Dialectical alignment: Resolving the tension of 3h and security threats of llms. 2024carXiv preprint</p>
<p>Logicsolver: Towards interpretable math word problem solving with logical prompt-enhanced learning. Zhicheng Yang, Jinghui Qin, Jiaqi Chen, Liang Lin, Xiaodan Liang, 10.18653/V1/2022.FINDINGS-EMNLP.1Proceedings of EMNLP 2022. EMNLP 20222022</p>
<p>Automatic chain of thought prompting in large language models. Zhuosheng Zhang, Aston Zhang, Mu Li, Alex Smola, Proceedings of ICLR 2023. ICLR 20232023a</p>
<p>A survey on language models for code. Ziyin Zhang, Chaoyu Chen, Bingchang Liu, Cong Liao, Zi Gong, Hang Yu, Jianguo Li, Rui Wang, 10.48550/ARXIV.2311.07989CoRR, abs/2311.079892023b</p>
<p>Verify-and-edit: A knowledge-enhanced chain-of-thought framework. Ruochen Zhao, Xingxuan Li, Shafiq Joty, Chengwei Qin, Lidong Bing, 10.18653/V1/2023.ACL-LONG.320Proceedings of ACL 2023. ACL 20232023</p>
<p>Mquake: Assessing knowledge editing in language models via multi-hop questions. Zexuan Zhong, Zhengxuan Wu, Christopher D Manning, Christopher Potts, Danqi Chen, 10.18653/V1/2023.EMNLP-MAIN.971Proceedings of EMNLP 2023. EMNLP 20232023</p>
<p>Least-to-most prompting enables complex reasoning in large language models. Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc V Le, Ed H Chi, Proceedings of ICLR 2023. ICLR 20232023</p>
<p>Multilingual machine translation with large language models: Empirical results and analysis. Wenhao Zhu, Hongyi Liu, Qingxiu Dong, Jingjing Xu, Lingpeng Kong, Jiajun Chen, Lei Li, Shujian Huang, 10.48550/ARXIV.2304.04675CoRR, abs/2304.046752023</p>
<p>Representation engineering: A top-down approach to AI transparency. Andy Zou, Long Phan, Sarah Chen, James Campbell, Phillip Guo, Richard Ren, Alexander Pan, Xuwang Yin, Mantas Mazeika, Ann-Kathrin Dombrowski, 10.48550/ARXIV.2310.01405CoRR, abs/2310.014052023</p>            </div>
        </div>

    </div>
</body>
</html>